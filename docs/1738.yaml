- en: MOE & MOA for Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16](https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Seeking advice from a panel of specialists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Rahul
    Vir](../Images/3f85d00b0e75540773420ed53ee5da68.png)](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    [Rahul Vir](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    ·4 min read·Jul 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16d3e9edea3d8545cbef99ac81077d6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author (AI Generated leonardo.ai)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) have undoubtedly taken the tech industry by storm.
    Their meteoric rise was fueled by a large corpora of data from Wikipedia, web
    pages, books, troves of research papers and, of course, user content from our
    beloved social media platforms. The data and compute hungry models have been feverishly
    incorporating multi-modal data from audio and video libraries, and have been using
    [10s of thousands](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.)
    of Nvidia GPUs for months to train the state-of-the-art (SOTA) models. All this
    makes us wonder whether this exponential growth can last.
  prefs: []
  type: TYPE_NORMAL
- en: The challenges facing these LLMs are numerous but let’s investigate a few here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost and Scalability**: Larger models can cost tens of millions of dollars
    to to train and serve, becoming a barrier to adoption by the swath of day-to-day
    applications. (See [Cost of training GPT-4](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Data Saturation**: Publicly available datasets will exhaust soon
    enough and may need to rely on slowly generated user content. Only companies and
    agencies that have a steady source of new content will be able to generate improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations**: Models generating false and unsubstantiated information
    is going to be a deterrent with users expecting validation from authoritative
    sources before using for sensitive applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploring unknowns**: LLMs are now being used for applications beyond their
    original intent. For example LLMs have shown great ability in game play, scientific
    discovery and climate modeling. We will need new approaches to solve these complex
    situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we start getting too worried about the future, let’s examine how AI researchers
    are tirelessly working on ways to ensure continued progress. The Mixture-of-Experts
    (MoE) and Mixture-of-Agents (MoA) innovations show that hope is on the horizon.
  prefs: []
  type: TYPE_NORMAL
- en: First introduced in 2017, [Mixture-of-Experts](https://arxiv.org/abs/1701.06538)
    technique showed that multiple experts and a gating network that can pick a sparse
    set of experts can produce a vastly improved outcome with lower computational
    costs. The gating decision allows to turn off large pieces of the network enabling
    conditional computation, and specialization improves performance for language
    modeling and machine translational tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/076aab63ab3201d9a51a7913193f052f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://arxiv.org/pdf/1701.06538): MoE Layer from Outrageously Large
    Neural Networks'
  prefs: []
  type: TYPE_NORMAL
- en: The figure above shows that a Mixture-of-Experts layer is incorporated in a
    recurrent neural network. The gating layer activates only two experts for the
    task and subsequently combines their output.
  prefs: []
  type: TYPE_NORMAL
- en: While this was demonstrated for select benchmarks, conditional computation has
    opened up an avenue to see continued improvements without resorting to ever growing
    model size.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by MOE, [Mixture-of-Agents](https://arxiv.org/html/2406.04692v1) technique
    leverages multiple LLM to improve the outcome. The problem is routed through multiple
    LLMs aka agents that enhance the outcome at each stage and the authors have demonstrated
    that it produces a better outcome with smaller models versus the larger SOTA models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e010a39fddb47fcbc620ad668c47e232.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://arxiv.org/abs/2406.04692): Mixture-of-Agents Enhances Large
    Language Model Capabilities | [license](https://creativecommons.org/licenses/by/4.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: The figure shows 4 Mixture-of-Agents layers with 3 agents in each layer. Selecting
    appropriate LLMs for each layer is important to ensure proper collaboration and
    to produce high quality response. ([Source](https://arxiv.org/html/2406.04692v1/x2.png))
  prefs: []
  type: TYPE_NORMAL
- en: MOA relies on the fact that LLMs collaborating together produce better outputs
    as they can combine responses from other models. The role of LLMs is divided into
    proposers that generate diverse outputs and aggregators that can combine them
    to produce high-quality responses. The multi-stage approach will likely increase
    the Time to First Token (TTFT), so mitigating approaches will need to be developed
    to make them suitable for broad applications.
  prefs: []
  type: TYPE_NORMAL
- en: MOE and MOA have similar foundational elements but behave differently. MOE works
    on the concept of picking a set of experts to complete a job where the gating
    network has the task of picking the right set of experts. MOA works on teams building
    on the work of the previous teams, and improving the outcome at each stage.
  prefs: []
  type: TYPE_NORMAL
- en: Innovations for MOE and MOA have created a path of innovation where a combination
    of specialized components or models, collaborating and exchanging information,
    can continue to provide better outcomes even when linear scaling of model parameters
    and training datasets is no longer trivial.
  prefs: []
  type: TYPE_NORMAL
- en: While it is only with hindsight we will know whether the LLM innovations can
    last, I have been following the research in the field for insights. Seeing what
    is coming out of universities and research institutions, I am extremely bullish
    on what is next to come. I do feel we are just warming up for the onslaught of
    new capabilities and applications that will transform our lives. We don’t know
    what they are but we can be fairly certain that coming days will not fail to surprise
    us.
  prefs: []
  type: TYPE_NORMAL
- en: '*“We tend to overestimate the effect of a technology in the short run and underestimate
    the effect in the long run.”. -Amara’s Law*'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., & Zou, J. (2024). Mixture-of-Agents
    Enhances Large Language Model Capabilities. arXiv [Preprint]. [https://arxiv.org/abs/2406.04692](https://arxiv.org/abs/2406.04692)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer. arXiv preprint arXiv:1701.06538.'
  prefs: []
  type: TYPE_NORMAL
