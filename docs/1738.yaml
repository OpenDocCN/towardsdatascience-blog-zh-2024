- en: MOE & MOA for Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的 MOE 和 MOA
- en: 原文：[https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16](https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16](https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5?source=collection_archive---------8-----------------------#2024-07-16)
- en: Seeking advice from a panel of specialists
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求专家小组的建议
- en: '[](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Rahul
    Vir](../Images/3f85d00b0e75540773420ed53ee5da68.png)](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    [Rahul Vir](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Rahul
    Vir](../Images/3f85d00b0e75540773420ed53ee5da68.png)](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    [Rahul Vir](https://medium.com/@rahulvir?source=post_page---byline--c1cafeffd6a5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    ·4 min read·Jul 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1cafeffd6a5--------------------------------)
    ·阅读时间：4分钟·2024年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/16d3e9edea3d8545cbef99ac81077d6d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16d3e9edea3d8545cbef99ac81077d6d.png)'
- en: Image by the author (AI Generated leonardo.ai)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供（AI生成 leonardo.ai）
- en: Large Language Models (LLMs) have undoubtedly taken the tech industry by storm.
    Their meteoric rise was fueled by a large corpora of data from Wikipedia, web
    pages, books, troves of research papers and, of course, user content from our
    beloved social media platforms. The data and compute hungry models have been feverishly
    incorporating multi-modal data from audio and video libraries, and have been using
    [10s of thousands](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.)
    of Nvidia GPUs for months to train the state-of-the-art (SOTA) models. All this
    makes us wonder whether this exponential growth can last.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）无疑已经掀起了科技行业的风暴。它们的迅猛崛起得益于来自维基百科、网页、书籍、大量研究论文，以及我们喜爱的社交媒体平台的用户内容的大型数据集。这些数据和计算资源密集型的模型一直在热切地整合来自音频和视频库的多模态数据，并且已经使用了[成千上万](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.)的英伟达
    GPU，进行数月的训练，以打造最先进的（SOTA）模型。这一切让我们不禁思考，这种指数级增长是否能够持续下去。
- en: The challenges facing these LLMs are numerous but let’s investigate a few here.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大型语言模型面临的挑战是多方面的，但让我们在此探讨其中的一些。
- en: '**Cost and Scalability**: Larger models can cost tens of millions of dollars
    to to train and serve, becoming a barrier to adoption by the swath of day-to-day
    applications. (See [Cost of training GPT-4](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.))'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本与可扩展性**：更大的模型在训练和部署过程中可能需要数千万美元，这成为了日常应用中广泛采用的障碍。（参见[训练 GPT-4 的成本](https://www.ri.se/en/news/blog/generative-ai-does-not-run-on-thin-air#:~:text=The%20Cost%20of%20Training%20GPT%2D4&text=OpenAI%20has%20revealed%20that%20it,of%20energy%20usage%20during%20training.)）'
- en: '**Training Data Saturation**: Publicly available datasets will exhaust soon
    enough and may need to rely on slowly generated user content. Only companies and
    agencies that have a steady source of new content will be able to generate improvements.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据饱和**：公开可用的数据集很快就会耗尽，可能需要依赖缓慢生成的用户内容。只有那些拥有稳定新内容来源的公司和机构，才能够生成进一步的改进。'
- en: '**Hallucinations**: Models generating false and unsubstantiated information
    is going to be a deterrent with users expecting validation from authoritative
    sources before using for sensitive applications.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**：生成虚假且无根据信息的模型将成为一种障碍，因为用户在使用这些信息进行敏感应用之前，期望从权威来源获得验证。'
- en: '**Exploring unknowns**: LLMs are now being used for applications beyond their
    original intent. For example LLMs have shown great ability in game play, scientific
    discovery and climate modeling. We will need new approaches to solve these complex
    situations.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索未知**：LLMs现在被用于超出其原始目的的应用。例如，LLMs在游戏玩法、科学发现和气候建模方面展现了极大的能力。我们将需要新的方法来解决这些复杂情况。'
- en: Before we start getting too worried about the future, let’s examine how AI researchers
    are tirelessly working on ways to ensure continued progress. The Mixture-of-Experts
    (MoE) and Mixture-of-Agents (MoA) innovations show that hope is on the horizon.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始过于担心未来之前，先来看一下AI研究人员如何不懈努力以确保持续进展。Mixture-of-Experts（MoE）和Mixture-of-Agents（MoA）的创新表明，希望就在眼前。
- en: First introduced in 2017, [Mixture-of-Experts](https://arxiv.org/abs/1701.06538)
    technique showed that multiple experts and a gating network that can pick a sparse
    set of experts can produce a vastly improved outcome with lower computational
    costs. The gating decision allows to turn off large pieces of the network enabling
    conditional computation, and specialization improves performance for language
    modeling and machine translational tasks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首次引入于2017年，[Mixture-of-Experts](https://arxiv.org/abs/1701.06538)技术表明，多个专家和一个能够选择稀疏专家的门控网络能够以更低的计算成本产生显著改善的结果。门控决策使得可以关闭网络的大部分部分，从而启用条件计算，并且专门化提高了语言建模和机器翻译任务的性能。
- en: '![](../Images/076aab63ab3201d9a51a7913193f052f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/076aab63ab3201d9a51a7913193f052f.png)'
- en: '[Source](https://arxiv.org/pdf/1701.06538): MoE Layer from Outrageously Large
    Neural Networks'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/pdf/1701.06538)：来自极大规模神经网络的MoE层'
- en: The figure above shows that a Mixture-of-Experts layer is incorporated in a
    recurrent neural network. The gating layer activates only two experts for the
    task and subsequently combines their output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了Mixture-of-Experts层如何被集成到一个递归神经网络中。门控层仅激活两个专家来处理任务，并随后将它们的输出合并。
- en: While this was demonstrated for select benchmarks, conditional computation has
    opened up an avenue to see continued improvements without resorting to ever growing
    model size.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一点已经在一些选定的基准上得到了验证，但条件计算为我们提供了一个途径，可以在不依赖日益增大的模型规模的情况下看到持续的改进。
- en: Inspired by MOE, [Mixture-of-Agents](https://arxiv.org/html/2406.04692v1) technique
    leverages multiple LLM to improve the outcome. The problem is routed through multiple
    LLMs aka agents that enhance the outcome at each stage and the authors have demonstrated
    that it produces a better outcome with smaller models versus the larger SOTA models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受到MOE的启发，[Mixture-of-Agents](https://arxiv.org/html/2406.04692v1)技术利用多个LLM来改善结果。问题通过多个LLMs（即代理）进行处理，这些代理在每个阶段增强结果，作者已经证明，与更大的SOTA模型相比，这种方法能够用更小的模型产生更好的结果。
- en: '![](../Images/e010a39fddb47fcbc620ad668c47e232.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e010a39fddb47fcbc620ad668c47e232.png)'
- en: '[Source](https://arxiv.org/abs/2406.04692): Mixture-of-Agents Enhances Large
    Language Model Capabilities | [license](https://creativecommons.org/licenses/by/4.0/)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://arxiv.org/abs/2406.04692)：Mixture-of-Agents增强了大语言模型的能力 | [许可](https://creativecommons.org/licenses/by/4.0/)'
- en: The figure shows 4 Mixture-of-Agents layers with 3 agents in each layer. Selecting
    appropriate LLMs for each layer is important to ensure proper collaboration and
    to produce high quality response. ([Source](https://arxiv.org/html/2406.04692v1/x2.png))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了4个Mixture-of-Agents层，每个层中有3个代理。为每一层选择合适的LLMs非常重要，以确保适当的协作并产生高质量的响应。([来源](https://arxiv.org/html/2406.04692v1/x2.png))
- en: MOA relies on the fact that LLMs collaborating together produce better outputs
    as they can combine responses from other models. The role of LLMs is divided into
    proposers that generate diverse outputs and aggregators that can combine them
    to produce high-quality responses. The multi-stage approach will likely increase
    the Time to First Token (TTFT), so mitigating approaches will need to be developed
    to make them suitable for broad applications.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: MOA依赖于这样一个事实：LLMs之间的协作能产生更好的输出，因为它们能够结合其他模型的响应。LLMs的角色分为生成多样输出的提议者和可以将其结合起来生成高质量响应的聚合器。多阶段方法可能会增加“首次令牌时间”（TTFT），因此需要开发缓解方法使其适用于广泛的应用。
- en: MOE and MOA have similar foundational elements but behave differently. MOE works
    on the concept of picking a set of experts to complete a job where the gating
    network has the task of picking the right set of experts. MOA works on teams building
    on the work of the previous teams, and improving the outcome at each stage.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MOE 和 MOA 具有相似的基础元素，但表现不同。MOE 基于选择一组专家来完成任务的概念，门控网络的任务是选择合适的专家集。MOA 则依赖于团队在前一团队工作的基础上进行建设，并在每个阶段改进结果。
- en: Innovations for MOE and MOA have created a path of innovation where a combination
    of specialized components or models, collaborating and exchanging information,
    can continue to provide better outcomes even when linear scaling of model parameters
    and training datasets is no longer trivial.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MOE 和 MOA 的创新开辟了一条创新路径，在这条路径上，专门化的组件或模型的组合通过合作和信息交流，能够持续提供更好的结果，即使在模型参数和训练数据集的线性扩展不再简单时。
- en: While it is only with hindsight we will know whether the LLM innovations can
    last, I have been following the research in the field for insights. Seeing what
    is coming out of universities and research institutions, I am extremely bullish
    on what is next to come. I do feel we are just warming up for the onslaught of
    new capabilities and applications that will transform our lives. We don’t know
    what they are but we can be fairly certain that coming days will not fail to surprise
    us.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们只有回顾时才能知道大语言模型（LLM）创新是否能持久，但我一直在关注该领域的研究以获取见解。从各大高校和研究机构的成果来看，我对未来的进展非常看好。我确实感到，我们正处于新能力和新应用的“热身”阶段，这些将彻底改变我们的生活。我们不知道它们是什么，但可以相当确定，未来的日子不会让我们失望。
- en: '*“We tend to overestimate the effect of a technology in the short run and underestimate
    the effect in the long run.”. -Amara’s Law*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我们往往高估技术在短期内的影响，而低估它在长期内的影响。” - 阿马拉定律*'
- en: '**References**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., & Zou, J. (2024). Mixture-of-Agents
    Enhances Large Language Model Capabilities. arXiv [Preprint]. [https://arxiv.org/abs/2406.04692](https://arxiv.org/abs/2406.04692)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Wang, J., Wang, J., Athiwaratkun, B., Zhang, C., & Zou, J. (2024). 代理混合体增强大语言模型能力。arXiv
    [预印本]。[https://arxiv.org/abs/2406.04692](https://arxiv.org/abs/2406.04692)'
- en: '[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts
    layer. arXiv preprint arXiv:1701.06538.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G.,
    & Dean, J. (2017). 极其庞大的神经网络：稀疏门控专家混合层。arXiv 预印本 arXiv:1701.06538.'
