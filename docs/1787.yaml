- en: 'Function Calling: Fine-Tuning Llama 3 on xLAM'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/function-calling-fine-tuning-llama-3-on-xlam-f9b490d4f063?source=collection_archive---------4-----------------------#2024-07-23](https://towardsdatascience.com/function-calling-fine-tuning-llama-3-on-xlam-f9b490d4f063?source=collection_archive---------4-----------------------#2024-07-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fast and memory-efficient thanks to QLoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--f9b490d4f063--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--f9b490d4f063--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f9b490d4f063--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f9b490d4f063--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--f9b490d4f063--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f9b490d4f063--------------------------------)
    ·8 min read·Jul 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc60aa203270d46fd98b330d5e32fcf4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Recent large language models (LLMs) are highly capable in most language generation
    tasks. However, since they operate based on next-token prediction, they often
    struggle with accurately performing mathematical operations. Additionally, due
    to their knowledge cut-off, they may lack the information needed to answer some
    queries accurately.
  prefs: []
  type: TYPE_NORMAL
- en: One way to alleviate these issues is through function calling. Function calling
    allows LLMs to reliably connect to external tools. It enables interaction with
    external APIs. For example, retrieving information from the Internet and performing
    mathematical operations can be accomplished through function calling by interfacing
    the LLM with a web search engine and a calculator.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how to fine-tune LLMs for function calling. I use
    xLAM, a dataset of 60k entries of function calling released by Salesforce for
    fine-tuning Llama 3\. We will see how to format the dataset and how we can exploit
    the fine-tuned adapters for function calling.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also made this notebook implementing the code described in this article for
    fine-tuning, and some examples of inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#89)](https://kaitchup.substack.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Function Calling for LLMs: How…'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
