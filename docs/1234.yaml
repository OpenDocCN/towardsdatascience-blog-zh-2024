- en: The Math Behind Nadam Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-nadam-optimizer-47dc1970d2cc?source=collection_archive---------2-----------------------#2024-05-16](https://towardsdatascience.com/the-math-behind-nadam-optimizer-47dc1970d2cc?source=collection_archive---------2-----------------------#2024-05-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nadam is one of the most capable optimizers in Deep Learning. Let’s delve into
    its math, and build the algorithm from scratch.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--47dc1970d2cc--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--47dc1970d2cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--47dc1970d2cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--47dc1970d2cc--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--47dc1970d2cc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--47dc1970d2cc--------------------------------)
    ·18 min read·May 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65e6eaad6e0391dc9ba46b0a4e1451f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: In our previous discussion on the Adam optimizer, we explored how Adam has transformed
    the optimization landscape in machine learning with its adept handling of adaptive
    learning rates. Known for its success in various machine learning competitions,
    especially on platforms like Kaggle, Adam has certainly set a high bar for optimization
    techniques. However, the evolution of optimization algorithms didn’t stop there.
    Here’s Nadam — short for Nesterov-accelerated Adaptive Moment Estimation — Adam’s
    advanced successor.
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t need to read my previous article about Adam to understand this one,
    but if you’re interested, here’s the link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-math-behind-adam-optimizer-c41407efe59b?source=post_page-----47dc1970d2cc--------------------------------)
    [## The Math behind Adam Optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: Why is Adam the most popular optimizer in Deep Learning? Let’s understand it
    by diving into its math, and recreating…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-math-behind-adam-optimizer-c41407efe59b?source=post_page-----47dc1970d2cc--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Nadam enhances the Adam optimizer by incorporating Nesterov momentum, which
    introduces a lookahead capability to gradient updates. This adjustment not only
    speeds up the convergence process but also improves the accuracy of the steps
    toward…
  prefs: []
  type: TYPE_NORMAL
