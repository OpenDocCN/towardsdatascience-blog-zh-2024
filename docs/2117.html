<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Simple Framework for RAG Enhanced Visual Question Answering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Simple Framework for RAG Enhanced Visual Question Answering</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30">https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8512" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Empowering Phi-3.5-vision with Wikipedia knowledge for augmented Visual Question Answering.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Gabriele Sgroi, PhD" class="l ep by dd de cx" src="../Images/b81978d35e6238d160457de2affc2b0e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*AcU2TE5e6GUmSp2zdGIe9A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------" rel="noopener follow">Gabriele Sgroi, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/186c503f14d17c4e369edaaa3013e655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HatAi__854URHpzd"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@christianlue?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Christian Lue</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="941a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="beb2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Retrieval Augmented Generation (RAG) is a powerful technique that can improve the accuracy and reliability of the answer generated by Large Language Models (LLMs). It also offers the possibility of checking the sources used by the model during a particular generation, allowing easier fact-checking by human users. Furthermore, RAG makes it possible to keep the model knowledge up-to-date and incorporate topic-specific information without the need for fine-tuning. Overall, RAG provides many benefits and few drawbacks, and its workflow is straightforward to implement. Because of this, it has become the go-to solution for many LLM use cases that require up-to-date and/or specialized knowledge.</p><p id="e33a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Some of the latest developments in the Generative AI field have focused on extending the popular transformer architecture to tackle multiple input and/or output modalities, trying to replicate the huge success of LLMs. There are already several models, both open and closed source, that have demonstrated a remarkable ability to handle multiple modalities. A popular multimodal setting, and one of the first to be tackled, is that of Vision Language Models (VLMs), which has seen interesting open-source contributions with the release of small yet powerful models like LLaVA, Idefics, and Phi-vision. If you want to get started with VLMs and learn more about building a Vision Language Chat Assistant using LLaVA, you can look at my previous post <a class="af nb" rel="noopener" target="_blank" href="/create-your-vision-chat-assistant-with-llava-610b02c3283e">Create your Vision Chat Assistant with LLaVA</a>.</p><p id="1b49" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Designing RAG systems for multimodal models is more challenging than in the text-only case. In fact, the design of RAG systems for LLM is well-established and there is some consensus about the general workflow, as many of the recent developments focus on improving accuracy, reliability, and scalability rather than fundamentally changing the RAG architecture. On the other hand, multimodality opens up multiple ways of retrieving relevant information and, consequentially, there are several different architectural choices that can be made, each with its own advantages and drawbacks. For example, it is possible to use a multimodal embedding model to create a shared vector space for the different modalities or, instead, choose to ground the information in one modality only.</p><p id="9e3b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this blog post, I will discuss a simple framework to extend RAG to Vision Language Models (VLMs), focusing on the Visual Question Answering task. The core idea of the method is to exploit the capabilities of the VLM to understand both text and images to generate a suitable search query that will be used to retrieve external information before answering the user’s prompt.</p><p id="f972" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I will also provide a practical tutorial on how to implement the framework to empower Phi-3.5-vision with access to Wikipedia information, discussing the main points of the implementation and showing some examples. I will leave the details to the full code I shared in the following <a class="af nb" href="https://github.com/GabrieleSgroi/vision_rag" rel="noopener ugc nofollow" target="_blank">Git Hub repo</a>.</p><h1 id="7745" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">RAG for Visual Question Answering</h1><p id="18b0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this section, I will describe the general workflow of the framework mentioned in the introduction. For the sake of exposition, I will discuss the case where there is only one user’s prompt about one image. This is the case, for example, for simple Visual Question Answering (VQA) tasks. The method can be generalized straightforwardly to multiple prompts and images, but the pipeline will become more complex and introduce further complications. Furthermore, I will only consider the case in which the external data consists solely of textual documents. Using a multimodal embedding model for retrieval, or more generally a multimodal search engine, it is possible to include images in the external data as well.</p><p id="fca3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As for the usual RAG workflow, the framework workflow can be divided into two parts: retrieval of the relevant external information and generation conditioned on the provided external data.</p><p id="60f5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">During the retrieval phase, the goal is to retrieve some passages from the external text documents that can provide useful information to answer the user’s prompt. In order to do so effectively, we must ensure that the retrieved passages are relevant to the provided image, the prompt, and, more importantly, the relationship between the two. In fact, even if the retrieved documents contain information about the image, they may not include the specific information needed to provide an answer to the user’s prompt. On the other hand, the prompt may only be correctly understood when paired with the image it refers to. To address these challenges, the framework discussed in this post exploits the multimodal model to generate an appropriate search query, tailored to capture the information needed to answer the user’s prompt in the context of the provided image. A search engine will use the produced query to retrieve the relevant information from the external data.</p><p id="9eb4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In more detail, the multimodal model receives as input both the user’s prompt and the image and it is tasked with creating a search query that is relevant to both of them as a whole. This process can be seen as a special case of a query transformation, designed to consider the multimodal nature of the problem. In fact, the model translates the user’s prompt into a search query while also considering the image it refers to.</p><p id="3101" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The advantage of this approach over other methods that treat each input modality separately, such as using a multimodal embedding model for retrieval or using a generated image caption/description for semantic similarity, is that it can capture the relationships between the prompt and the image more effectively.</p><p id="d7c3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The flowchart for the retrieval phase is sketched below.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj oz"><img src="../Images/bd8841a050d566ca5fe7c3a326d3d9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*vD1vEj_1115aWP8jAAZ26Q.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">During retrieval, the model is tasked to create a query that will be used by a search engine to retrieve the relevant passages. Image by the author.</figcaption></figure><p id="8cb0" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The generation phase is very similar to the standard text-only RAG workflow, the only difference being that the model receives the image in its context in addition to the prompt and the retrieved passages. This process is illustrated below.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pa"><img src="../Images/79f324bd23a63cc7873b6633ece07966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*71qRJxjSEoSpTJ4EXoKe_Q.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">During generation, the model receives the retrieved passages, the prompt, and the image as input. Image by the author.</figcaption></figure><h1 id="9103" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Empowering Phi-3.5-vision with Wikipedia</h1><p id="d979" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this section, I will provide a practical guide on how to apply the discussed framework to enhance a multimodal model by giving it access to Wikipedia. I chose the model <a class="af nb" href="https://huggingface.co/microsoft/Phi-3.5-vision-instruct" rel="noopener ugc nofollow" target="_blank">Phi-3.5-vision</a>, as it is a very powerful yet lightweight open-source Vision Language Model.</p><p id="8810" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this section, I will discuss only the general aspects of the implementation, leaving the details to the code provided in the <a class="af nb" href="https://github.com/GabrieleSgroi/vision_rag" rel="noopener ugc nofollow" target="_blank">GitHub repo</a>.</p><h2 id="37dd" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Retrieval</h2><p id="2878" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The goal of the retrieval phase is to gather some passages from Wikipedia that can provide useful information to answer a user’s question about an image. In the code implementation, I used the Python package <a class="af nb" href="https://pypi.org/project/wikipedia/" rel="noopener ugc nofollow" target="_blank">wikipedia</a> to search and retrieve content from Wikipedia.</p><p id="ff44" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Here are the steps implemented to retrieve the relevant passages:</p><ol class=""><li id="d263" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot ps pt pu bk">Use the multimodal model to generate keywords capturing the meaning of the question about the image.</li><li id="03a7" class="ny nz fq oa b go pv oc od gr pw of og oh px oj ok ol py on oo op pz or os ot ps pt pu bk">Use the generated keywords to search relevant pages on Wikipedia.</li><li id="663d" class="ny nz fq oa b go pv oc od gr pw of og oh px oj ok ol py on oo op pz or os ot ps pt pu bk">Split the content of each retrieved page into chunks.</li><li id="49af" class="ny nz fq oa b go pv oc od gr pw of og oh px oj ok ol py on oo op pz or os ot ps pt pu bk">Select the top chunks by semantic textual similarity to the question and the keywords.</li></ol><p id="09cc" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The first step exploits Phi-3.5-vision to generate an appropriate search query that will be used to retrieve relevant Wikipedia pages. In order to do so, I tasked Phi-3.5-vision to produce keywords relevant to the user’s question and the image. I then used the built-in search function of the wikipedia package to retrieve some pages relevant to the generated keywords.</p><p id="7cca" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The general single-turn single-image chat template for Phi-vision-3.5 has the following structure:</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="1000" class="qe nd fq qb b bg qf qg l qh qi">&lt;|user|&gt;\n<br/>&lt;|image_1|&gt;\n<br/>{prompt}&lt;|end|&gt;\n<br/>&lt;|assistant|&gt;\n</span></pre><p id="ff94" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To generate the keywords I used the following prompt:</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="7ffe" class="qe nd fq qb b bg qf qg l qh qi">Your task is to write a few search keywords to find Wikipedia pages containing<br/>the relevant information to answer the question about the provided image. The <br/>keywords must be as specific as possible and must represent the information <br/>that is needed to answer the question in relation to the provided image. Don't <br/>write more than 3 search keywords.<br/>Question: {question}</span></pre><p id="d2ca" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The tag {question} is substituted with the user question before inference.</p><p id="37b3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">After the keywords have been generated, the built-in search function of the wikipedia package is used to retrieve some pages relevant to the generated keywords. Finally, the selected pages are split into passages, and then the most relevant passages are selected using an embedding model and the LangChain implementation of the FAISS vector store. I used the embedding model <a class="af nb" href="https://huggingface.co/Snowflake/snowflake-arctic-embed-l" rel="noopener ugc nofollow" target="_blank"><em class="qj">snowflake-arctic-embed-l</em></a> to embed the concatenation of the question and the keywords, and the chunks of the retrieved pages. In practice, the retrieval phase is effectively a form of “hybrid search” consisting of two sequential steps: keyword search using the built-in search function of the wikipedia package, and embedding similarity retrieval using an embedding model. In this way, the retrieval operates on the smaller space of the passages of the most relevant pages selected using keyword search, avoiding the need to build an enormous vector store with the embeddings of all the content of Wikipedia. In different settings, the retrieval phase could be remodeled to use similarity retrieval on the whole external corpus or using different combinations of retrieval methods.</p><p id="2720" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Retrieving passages from multiple pages can help reduce the chance of selecting the wrong page and it can also be useful when information from multiple pages is needed to produce an answer.</p><h2 id="fabc" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Generation</h2><p id="72a2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the generation phase, the user’s question, the retrieved passages, and the original images are used as inputs for Phi-3.5-vision to generate an answer.</p><p id="4e0d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I used the following prompt in the general chat template for Phi-3.5-vision:</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="7f9b" class="qe nd fq qb b bg qf qg l qh qi">You are a helpful assistant tasked with answering questions about the provided <br/>image.<br/>Answer the following question: {question}<br/>You can use the following passages retrieved from Wikipedia to provide your <br/>answer:<br/>{passages}</span></pre><p id="004e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">At generation time, the tag {question} is substituted by the user question as before, while the tag {passages} is substituted by the retrieved passages and the names of the corresponding pages with the following format</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="fed9" class="qe nd fq qb b bg qf qg l qh qi">From Wikipedia page {page_name} : "{passage1}"\n\n<br/>From Wikipedia page {page_name} : "{passage2}"\n\n<br/>From Wikipedia page {page_name} : "{passage3}"\n\n<br/>                      ...</span></pre><p id="dc74" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Providing the name of the page from which the passage is extracted can help resolve ambiguities when the content of the latter is not enough to uniquely determine the subject or topic it refers to.</p><h1 id="55dc" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Examples</h1><p id="bfe5" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this section, I will show some examples of answers obtained with the implementation discussed in the previous section, comparing the outputs of the Vision Language Model empowered with RAG with the base version.</p><p id="f503" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">For each example below, I will show the image provided to the model, a block with the question and the answers of both the RAG augmented and base VLM, a block with the search query created by the model, and a block with the passages retrieved from Wikipedia.</p><h2 id="4e53" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Example 1</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/fc32fffd5a80c737c08d79cf43adfc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_wGgo4CvozNw8FON"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@enginakyurt?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">engin akyurt</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="b0b4" class="qe nd fq qb b bg qf qg l qh qi">Question: How tall are the plants that produce this fruit?<br/><br/>Base VLM: Tomatoes are typically grown on plants that can reach heights of 2 to<br/>4 feet, depending on the variety and growing conditions.<br/><br/>RAG VLM: The tomato plants that produce this fruit typically grow to 1-3 meters<br/>(3-10 feet) in height.</span></pre><p id="ed19" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The VLM generated the following search keywords:</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="6ff5" class="qe nd fq qb b bg qf qg l qh qi">Search keywords: tomato plant, tomato plant height</span></pre><p id="e21e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And retrieved the following passages:</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="b2e5" class="qe nd fq qb b bg qf qg l qh qi">From page Tomato (https://en.wikipedia.org/wiki/Tomato): <br/> "throughout all seasons of the year. Tomato plants typically grow to 1–3 <br/>meters (3–10 ft) in height. They are vines that have a weak stem that sprawls <br/>and typically needs support. Indeterminate tomato plants are perennials in <br/>their native habitat, but are cultivated as annuals. (Determinate, or".<br/><br/>From page Tomato (https://en.wikipedia.org/wiki/Tomato): <br/> "Numerous varieties of the tomato plant are widely grown in temperate climates<br/> across the world, with greenhouses allowing for the production of tomatoes <br/>throughout all seasons of the year. Tomato plants typically grow to 1–3 meters<br/>(3–10 ft) in height. They are vines that have a weak stem that".<br/><br/>From page Early Girl (https://en.wikipedia.org/wiki/Early_Girl): <br/> "== See also ==<br/>List of tomato cultivars<br/><br/><br/>== References ==".<br/><br/>From page Black Krim (https://en.wikipedia.org/wiki/Black_Krim): <br/> "== References ==<br/><br/><br/>== See also ==<br/>List of tomato cultivars<br/>Heirloom tomato".<br/><br/>From page Celebrity tomato (https://en.wikipedia.org/wiki/Celebrity_tomato): <br/> "== See also ==<br/>List of tomato cultivars<br/>Tomato production in Florida<br/><br/><br/>== References ==".<br/><br/>From page Fourth of July tomato <br/>(https://en.wikipedia.org/wiki/Fourth_of_July_tomato): <br/> "== See also ==<br/>List of tomato cultivars<br/><br/><br/>== References ==<br/><br/><br/>== External links ==<br/>Picture Gallery from Google Images".<br/><br/>From page Mortgage Lifter (https://en.wikipedia.org/wiki/Mortgage_Lifter): <br/> "== See also ==<br/>List of tomato cultivars<br/><br/><br/>== References ==<br/><br/><br/>== External links ==<br/>Picture Gallery from Google Images".</span></pre><p id="5884" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The model augmented with RAG correctly reported the height range stated on the Wikipedia page ‘Tomato’, while the base model answered with a lower range.</p><h2 id="753d" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Example 2</h2><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/2820d1d705765e00d8da60086af89f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*B5mvljfr2TKuKLSz"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@hansjurgen007?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hans-Jurgen Mager</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="5343" class="qe nd fq qb b bg qf qg l qh qi">Query: How much does this species weight?<br/><br/>Base VLM: Polar bears can weigh between 900 to 1,600 pounds <br/>(408 to 727 kilograms).<br/><br/>RAG VLM:  The weight of this species, the polar bear, ranges from 300-800 kg <br/>(660-1,760 lb) for males and 150-300 kg (330-660 lb) for females.</span></pre><pre class="qm qa qb qc bp qd bb bk"><span id="8757" class="qe nd fq qb b bg qf qg l qh qi">Search keywords: polar bear, weight</span></pre><pre class="qm qa qb qc bp qd bb bk"><span id="7cc3" class="qe nd fq qb b bg qf qg l qh qi">From page Polar bear (https://en.wikipedia.org/wiki/Polar_bear): <br/>"Males are generally 200–250 cm (6.6–8.2 ft) long with a weight of 300–800 kg<br/>(660–1,760 lb). Females are smaller at 180–200 cm (5.9–6.6 ft) with a weight <br/>of 150–300 kg (330–660 lb). Sexual dimorphism in the species is particularly <br/>high compared with most other mammals. Male polar bears also have".<br/><br/>From page Polar bear (https://en.wikipedia.org/wiki/Polar_bear): <br/> "== Notes ==<br/><br/><br/>== References ==<br/><br/><br/>== Bibliography ==<br/><br/><br/>== External links ==<br/>Polar Bears International website<br/>ARKive—images and movies of the polar bear (Ursus maritimus)".<br/><br/>From page Polar bear (https://en.wikipedia.org/wiki/Polar_bear): <br/> "weight of 150–300 kg (330–660 lb). Sexual dimorphism in the species is <br/>particularly high compared with most other mammals. Male polar bears also have <br/>proportionally larger heads than females. The weight of polar bears fluctuates <br/>during the year, as they can bulk up on fat and increase their mass by".<br/><br/>From page List of ursids (https://en.wikipedia.org/wiki/List_of_ursids): <br/> "long, plus a 3–20 cm (1–8 in) tail, though the polar bear is 2.2–2.44 m <br/>(7–8 ft) long, and some subspecies of brown bear can be up to 2.8 m (9 ft). <br/>Weights range greatly from the sun bear, which can be as low as 35 kg (77 lb), <br/>to the polar bear, which can be as high as 726 kg (1,600 lb). Population".<br/><br/>From page Knut (polar bear) (https://en.wikipedia.org/wiki/Knut_(polar_bear)): <br/> "== See also ==<br/>Binky (polar bear)<br/>List of individual bears<br/><br/><br/>== References ==<br/><br/><br/>== External links ==".<br/><br/>From page Polar bear (https://en.wikipedia.org/wiki/Polar_bear): <br/> "The polar bear is the largest living species of bear and land carnivore, <br/>though some brown bear subspecies like the Kodiak bear can rival it in size. <br/>Males are generally 200–250 cm (6.6–8.2 ft) long with a weight of 300–800 kg <br/>(660–1,760 lb). Females are smaller at 180–200 cm (5.9–6.6 ft) with a".<br/><br/>From page Subspecies of brown bear <br/>(https://en.wikipedia.org/wiki/Subspecies_of_brown_bear): <br/> "males, potentially tripling their average weight within three years' time, <br/>and can expect to average between 360 and 545 kg (794 and 1,202 lb). <br/>The reported mean adult body masses for both sexes of the polar bear are very <br/>similar to the peninsular giant and Kodiak bears. Due to their roughly".<br/><br/></span></pre><p id="688b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Again, the RAG model answered correctly with the weights reported in the Wikipedia page ‘Polar bear’, while the base model answered with a slightly different range of weights.</p><h2 id="9db9" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Example 3</h2><p id="b742" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In my previous blog post <a class="af nb" rel="noopener" target="_blank" href="/create-your-vision-chat-assistant-with-llava-610b02c3283e">Create your Vision Chat Assistant with LLaVA</a>, I showed how the LLaVA-1.5 model can be induced to hallucinate when the user’s prompt asks a misleading question about the image. Let’s see what happens in that example using the Phi-3.5-vision model with and without the RAG enhancement. In particular, the model could now be more prone to hallucination if any misleading passages were retrieved.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/1f408a52ca0ef21463de523769cb64d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Gboce6no0ZB70CjM"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Wolfgang Hasselmann</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="67b8" class="qe nd fq qb b bg qf qg l qh qi">Question: Tell me the species of the fish in this image<br/><br/>Base VLM: The image does not contain any fish or aquatic environments, so <br/>I cannot provide a species of fish.<br/><br/>RAG VLM:  The image does not contain any fish or aquatic environments. It is<br/>a photograph of a desert landscape with sand dunes. <br/>Therefore, I cannot provide the species of fish based on this image.</span></pre><p id="e9bd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Neither the base nor the RAG model were induced to hallucinate by the misleading prompt, indicating a possible better robustness of Phi-3.5-vision compared to LLaVA-1.5. In particular, the RAG VLM did not retrieve any page from Wikipedia. In fact, when asked to generate keywords, the model answered</p><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="def6" class="qe nd fq qb b bg qf qg l qh qi">There are no fish visible in the image provided. The image shows a desert <br/>landscape with sand dunes. Therefore, the search keywords related to the image <br/>content would be more appropriate for a desert or dune-related topic.</span></pre><p id="a29b" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This search query did not produce any results with the wikipedia package search function.</p><p id="781d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="qj">Note: in the latter example, in earlier experiments conducted with Idefics 2, both the base VLM and RAG VLM models were affected by hallucinations, answering with the names of fish species not present in the image. In particular, the RAG model retrieved misleading passages related to various fishes. The phenomenon of hallucinations is model-dependent and the RAG framework may not be enough to eliminate it in all instances.</em></p><h1 id="8e58" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Limitations</h1><p id="7b01" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The main limitation of the framework is that it requires the model to have at least a general knowledge of the image and the question provided (while the exact details will be retrieved with RAG) in order to produce a useful search query. In some domain-specific settings, it may be necessary to fine-tune the model for search query generation.</p><p id="07ed" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The framework discussed in this post also suffers from the limitations common to traditional text-only RAG systems. A common source of errors in the generation is the incorrect retrieval of external information. In some cases, the model is not able to correctly identify the failure of the retrieval phase and it produces an answer based on incorrect, irrelevant, or misleading information. In particular, the presence of irrelevant passages can sometimes induce hallucinations that the model wouldn’t have suffered otherwise.</p><p id="2a08" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Another common source of errors is ignoring the retrieved passages or quoting them incorrectly. This can sometimes lead to subtle differences between the retrieved passages and the generated answer that may be difficult to spot at first glance.</p><h2 id="69a0" class="pb nd fq bf ne pc pd pe nh pf pg ph nk oh pi pj pk ol pl pm pn op po pp pq pr bk">Example</h2><p id="d543" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this example, I will show an instance in which the model incorrectly quoted the retrieved passages.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qo"><img src="../Images/7e7e67c1da39dd4d66fda8912926ecf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YWZA7byaMUlE-rrv"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Search keywords: lion, weight, average weightPhoto by <a class="af nb" href="https://unsplash.com/@saluken?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Luke Tanis</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><pre class="ml mm mn mo mp qa qb qc bp qd bb bk"><span id="1f84" class="qe nd fq qb b bg qf qg l qh qi">Query: What's the average weight of this animal?<br/><br/>Base VLM: The average male lion can weigh between 160 to 310 kilograms, while <br/>females typically weigh between 120 to 180 kilograms.<br/><br/>RAG VLM:  Based on the information provided in the passages, the average weight<br/>of a male lion is 187.5 kg (386 lb) in Southern Africa and 174.9 kg (386 lb) in<br/>East Africa.</span></pre><pre class="qm qa qb qc bp qd bb bk"><span id="3ada" class="qe nd fq qb b bg qf qg l qh qi">Search keywords: lion, weight, average weight</span></pre><pre class="qm qa qb qc bp qd bb bk"><span id="1c48" class="qe nd fq qb b bg qf qg l qh qi">From page American lion (https://en.wikipedia.org/wiki/American_lion): <br/> "(518lbs-1153lbs) in males and 175kg to 365 kg (385lbs-805lbs) for females. <br/>In 2008, the American lion was estimated to weigh up to 420 kg (930 lb). A <br/>study in 2009 showed an average weight of 256 kg (564 lb) for males and 351 kg<br/>(774 lb) for the largest specimen analyzed.".<br/><br/>From page Lion (https://en.wikipedia.org/wiki/Lion): <br/> "=== Size ===<br/>Among felids, the lion is second only to the tiger in size. The size and weight<br/>of adult lions vary across its range and habitats. Accounts of a few <br/>individuals that were larger than average exist from Africa and India.<br/><br/><br/>=== Mane ===".<br/><br/>From page Panthera leo leo (https://en.wikipedia.org/wiki/Panthera_leo_leo): <br/> "The lion's fur varies in colour from light buff to dark brown. It has rounded<br/>ears and a black tail tuft. Average head-to-body length of male lions is <br/>2.47–2.84 m (8 ft 1 in – 9 ft 4 in) with a weight of 148.2–190.9 kg <br/>(327–421 lb). Females are smaller and less heavy. Zoological lion specimens".<br/><br/>From page Panthera leo melanochaita <br/>(https://en.wikipedia.org/wiki/Panthera_leo_melanochaita): <br/> "Average head-to-body length of male lions is 2.47–2.84 m (8 ft 1 in – 9 ft <br/>4 in) with a weight ranging from 150–225 kg (331–496 lb) averaging 187.5 kg <br/>(413 lb) in Southern Africa and 145.4–204.7 kg (321–451 lb) averaging 174.9 kg<br/> (386 lb) in East Africa. Females average 83–165 kg (183–364 lb) in".<br/><br/>From page Asiatic lion (https://en.wikipedia.org/wiki/Asiatic_lion): <br/> "An adult male Asiatic lion weighs 160.1 kg (353 lb) on average with the <br/>limit being 190 kg (420 lb); a wild female weighs 100 to 130 kg (220 to 285 lb)<br/>.[1]".<br/><br/>From page List of largest mammals <br/>(https://en.wikipedia.org/wiki/List_of_largest_mammals): <br/> "== See also ==<br/>List of largest land carnivorans<br/>Largest organisms<br/>Largest prehistoric animals<br/>List of largest birds<br/>List of largest cats<br/>List of largest fish<br/>List of largest plants<br/>List of largest reptiles<br/>List of largest insects<br/>List of heaviest land mammals<br/>Smallest organisms<br/><br/><br/>== Notes ==".<br/><br/>From page Ancient Mesopotamian units of measurement <br/>(https://en.wikipedia.org/wiki/Ancient_Mesopotamian_units_of_measurement): <br/> "== See also ==<br/>Assyrian lion weights<br/>Babylonian mathematics<br/>Historical weights and measures<br/>Weights and measures<br/><br/><br/>== References ==<br/><br/><br/>=== Citations ===".</span></pre><p id="c088" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">While the answer stating the weight in kilograms is correct, the model gave a wrong conversion to lbs for the average weight of male lions in Southern Africa, even though the respective passage extracted from Wikipedia reported the correct amount.</p><h1 id="4622" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="dfda" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this post, I illustrated a simple framework that can be used to enhance Visual Question Answering with Retrieval Augmented Generation capabilities. The core idea of the method is to exploit the Vision Language Model to generate queries that will be then used by a standard RAG pipeline to retrieve information from an external corpus. I also presented an implementation of the framework that grants Phi-3.5-vision access to Wikipedia. The full code for this implementation is available in the <a class="af nb" href="https://github.com/GabrieleSgroi/vision_rag" rel="noopener ugc nofollow" target="_blank">GitHub repo</a>.</p><p id="1d18" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">While the discussed method is simple and effective, it is not immune to the limitations common to all RAG systems, and to new challenges posed by the complexity of the multimodal setting. On one hand, retrieving the relevant information for some specific questions can be difficult. Since the search queries are created with the Vision Language Model, the retrieval accuracy is further limited by the ability of the VLM to recognize the image and to understand the details the question refers to. On the other hand, even after the correct information has been retrieved, there is no guarantee that the model won’t hallucinate while producing the answer. In the multimodal setting, this could be exacerbated by the fact that the model has to associate the correct meaning to both the text and the image and also understand the interactions between them.</p><p id="9aad" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The framework I discussed in this post is a straightforward extension of the vanilla RAG pipeline, adapted to the Visual Question Answering task. Standard advanced RAG techniques, such as query transformation, re-ranking the retrieved passages, and <a class="af nb" href="https://arxiv.org/abs/2212.10496" rel="noopener ugc nofollow" target="_blank">Hypothetical Document Embeddings (HyDE)</a> can be easily included to increase the performance. Furthermore, using a multimodal embedding model (like CLIP) new opportunities appear: the image embeddings can be used when searching by similarity for relevant text documents, and it is also possible to retrieve similar and/or relevant images to the original image and the question. The latter could be useful, for example, when a different point of view of the image is needed to answer the prompt. Another direction for improvement is to perform fine-tuning to get more specialized and effective models. Given the role of the multimodal model in the retrieval and generation process, two different fine-tuning processes can be performed: one to get a model specialized in writing search queries, and one to increase the model's performance on the grounded generation task. Finally, the framework could be incorporated into a specialized agentic system to further boost its performance and robustness. An agentic system could, for example, iteratively refine the generated query by giving feedback on the retrieved passages and asking follow-up questions or focusing on searching for information about particular details of the image only when needed. It could also handle multi-hop question-answering tasks for more complicated questions, and decide when the retrieval of further external information is needed to answer the user’s query.</p><p id="e096" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I’d be happy to discuss further improvements and/or different approaches to multimodal RAG in the comment section!</p></div></div></div></div>    
</body>
</html>