- en: Object Detection Basics â€” A Comprehensive Beginnerâ€™s Guide (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹åŸºç¡€ â€” ç»¼åˆåˆå­¦è€…æŒ‡å—ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05](https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05](https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05)
- en: Learn the basics of this advanced computer vision task of object detection in
    an easy to understand multi-part beginnerâ€™s guide
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ˜“äºç†è§£çš„å¤šéƒ¨åˆ†åˆå­¦è€…æŒ‡å—ä¸­ï¼Œå­¦ä¹ è¿™ä¸ªé«˜çº§è®¡ç®—æœºè§†è§‰ä»»åŠ¡â€”â€”ç‰©ä½“æ£€æµ‹çš„åŸºç¡€çŸ¥è¯†
- en: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    Â·9 min readÂ·Feb 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯» Â·2024å¹´2æœˆ5æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8f95d39d6f6a1014031f57234f642646.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f95d39d6f6a1014031f57234f642646.png)'
- en: Photo by [Javier GarcÃ­a](https://unsplash.com/@javigabbo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ª [Javier GarcÃ­a](https://unsplash.com/@javigabbo?utm_source=medium&utm_medium=referral)
    ç”± [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) æä¾›
- en: Driving a car nowadays with the latest drive assist technologies for lane detection,
    blind-spots, traffic signals and so on is pretty common. If we take a step back
    for a minute to appreciate what is happening behind the scenes, the Data Scientist
    in us soon realises that the system is not just *classifying* objects but also
    *locating* them in the scene (in real-time).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œé…å¤‡æœ€æ–°é©¾é©¶è¾…åŠ©æŠ€æœ¯ï¼ˆå¦‚è½¦é“æ£€æµ‹ã€ç›²ç‚¹ç›‘æµ‹ã€äº¤é€šä¿¡å·è¯†åˆ«ç­‰ï¼‰çš„æ±½è½¦å·²ç»ç›¸å½“æ™®éã€‚å¦‚æœæˆ‘ä»¬ç¨å¾®é€€åä¸€æ­¥ï¼Œå»äº†è§£ä¸€ä¸‹å¹•åå‘ç”Ÿäº†ä»€ä¹ˆï¼Œä½œä¸ºæ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘ä»¬å¾ˆå¿«ä¼šæ„è¯†åˆ°ï¼Œç³»ç»Ÿä¸ä»…ä»…æ˜¯åœ¨*åˆ†ç±»*ç‰©ä½“ï¼Œè¿˜åœ¨å®æ—¶åœ°*å®šä½*å®ƒä»¬ã€‚
- en: Such capabilities are prime examples of an **object detection** system in action.
    Drive assist technologies, industrial robots and security systems all make use
    of object detection models to detect objects of interest. **Object detection**
    is an advanced computer vision task which involves both *localisation* [of objects]
    as well as *classification*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åŠŸèƒ½æ˜¯**ç‰©ä½“æ£€æµ‹**ç³»ç»Ÿå®é™…åº”ç”¨çš„å…¸å‹ç¤ºä¾‹ã€‚é©¾é©¶è¾…åŠ©æŠ€æœ¯ã€å·¥ä¸šæœºå™¨äººå’Œå®‰å…¨ç³»ç»Ÿéƒ½åˆ©ç”¨ç‰©ä½“æ£€æµ‹æ¨¡å‹æ¥æ£€æµ‹æ„Ÿå…´è¶£çš„ç‰©ä½“ã€‚**ç‰©ä½“æ£€æµ‹**æ˜¯ä¸€ä¸ªé«˜çº§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œæ¶‰åŠ*ç‰©ä½“çš„å®šä½*å’Œ*åˆ†ç±»*ã€‚
- en: In this article, we will dive deeper into the details of the object detection
    task. We will learn about various concepts associated with it to help us understand
    novel architectures (covered in subsequent articles). We will cover key aspects
    and concepts required to understand object detection models from a Transfer Learning
    standpoint.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ç‰©ä½“æ£€æµ‹ä»»åŠ¡çš„ç»†èŠ‚ã€‚æˆ‘ä»¬å°†å­¦ä¹ ä¸ä¹‹ç›¸å…³çš„å„ç§æ¦‚å¿µï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£æ–°é¢–çš„æ¶æ„ï¼ˆå°†åœ¨åç»­æ–‡ç« ä¸­è®¨è®ºï¼‰ã€‚æˆ‘ä»¬å°†æ¶µç›–ç†è§£ç‰©ä½“æ£€æµ‹æ¨¡å‹æ‰€éœ€çš„å…³é”®æ–¹é¢å’Œæ¦‚å¿µï¼Œå°¤å…¶æ˜¯ä»è¿ç§»å­¦ä¹ çš„è§’åº¦ã€‚
- en: Key Concepts and Building Blocks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³é”®æ¦‚å¿µä¸æ„å»ºæ¨¡å—
- en: 'Object detection consists of two main sub-tasks, *localization* and *classification*.
    Classification of identified objects is straightforward to understand. But how
    do we define localization of objects? Let us cover some key concepts:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦å­ä»»åŠ¡ï¼Œ*å®šä½*å’Œ*åˆ†ç±»*ã€‚ç‰©ä½“çš„åˆ†ç±»æ˜¯å®¹æ˜“ç†è§£çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å¦‚ä½•å®šä¹‰ç‰©ä½“çš„å®šä½å‘¢ï¼Ÿè®©æˆ‘ä»¬äº†è§£ä¸€äº›å…³é”®æ¦‚å¿µï¼š
- en: Bounding Boxes
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾¹ç•Œæ¡†
- en: For the task of object detection, we identify a given objectâ€™s location using
    a rectangular box. This regular box is termed as a *bounding box* and used for
    localization of objects. Typically, the top left corner of the input image is
    set as origin or (0,0). A rectangular bounding box is defined with the help of
    its x and y coordinates for the top-left and bottom right vertices. Let us understand
    this visually. Figure 1(a) depicts a sample image with its origin set at its top
    left corner.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªçŸ©å½¢æ¡†æ¥æ ‡è¯†ç»™å®šç‰©ä½“çš„ä½ç½®ã€‚è¿™ä¸ªè§„åˆ™çš„çŸ©å½¢æ¡†è¢«ç§°ä¸º*è¾¹ç•Œæ¡†*ï¼Œç”¨äºç‰©ä½“çš„å®šä½ã€‚é€šå¸¸ï¼Œè¾“å…¥å›¾åƒçš„å·¦ä¸Šè§’è¢«è®¾ä¸ºåŸç‚¹æˆ–(0,0)ã€‚ä¸€ä¸ªçŸ©å½¢è¾¹ç•Œæ¡†é€šè¿‡å…¶å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„
    x å’Œ y åæ ‡æ¥å®šä¹‰ã€‚è®©æˆ‘ä»¬é€šè¿‡å›¾åƒæ¥ç›´è§‚åœ°ç†è§£è¿™ä¸€ç‚¹ã€‚å›¾1(a)å±•ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹å›¾åƒï¼Œå…¶åŸç‚¹è®¾ç½®åœ¨å·¦ä¸Šè§’ã€‚
- en: '![](../Images/0580171bb1c23766c169aa7f91f7a9a9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0580171bb1c23766c169aa7f91f7a9a9.png)'
- en: 'Figure 1: (a) A sample image with different objects, (b) bounding boxes for
    each of the objects with top-left and bottom-right vertices annotated,(c.)alternate
    way of identifying a bounding box is to use its top-left coordinates along with
    width and height parameters. Source: Author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šï¼ˆaï¼‰åŒ…å«ä¸åŒç‰©ä½“çš„ç¤ºä¾‹å›¾åƒï¼Œï¼ˆbï¼‰æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼Œæ ‡æ³¨äº†å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„é¡¶ç‚¹ï¼Œï¼ˆcï¼‰è¯†åˆ«è¾¹ç•Œæ¡†çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å·¦ä¸Šè§’åæ ‡åŠå…¶å®½åº¦å’Œé«˜åº¦å‚æ•°ã€‚æ¥æºï¼šä½œè€…
- en: Figure 1(b) shows each of the identified objects with their corresponding bounding
    boxes. It is important to note that a bounding box is annotated with its top-left
    and bottom-right coordinates which are relative to the imageâ€™s origin. With 4
    values, we can identify a bounding box uniquely. An alternate method to identify
    a bounding box is to use top-left coordinates along with its width and height
    values. Figure 1(c) shows this alternate way of identifying a bounding box. Different
    solutions may use different methods and it is mostly a matter of preference of
    one over the other.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1(b)æ˜¾ç¤ºäº†æ¯ä¸ªå·²è¯†åˆ«ç‰©ä½“åŠå…¶ç›¸åº”çš„è¾¹ç•Œæ¡†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¾¹ç•Œæ¡†é€šè¿‡å…¶å·¦ä¸Šè§’å’Œå³ä¸‹è§’çš„åæ ‡è¿›è¡Œæ ‡æ³¨ï¼Œè¿™äº›åæ ‡æ˜¯ç›¸å¯¹äºå›¾åƒåŸç‚¹çš„ã€‚é€šè¿‡4ä¸ªå€¼ï¼Œæˆ‘ä»¬å¯ä»¥å”¯ä¸€åœ°è¯†åˆ«ä¸€ä¸ªè¾¹ç•Œæ¡†ã€‚å¦ä¸€ç§è¯†åˆ«è¾¹ç•Œæ¡†çš„æ–¹æ³•æ˜¯ä½¿ç”¨å·¦ä¸Šè§’åæ ‡ä»¥åŠå…¶å®½åº¦å’Œé«˜åº¦å€¼ã€‚å›¾1(c)å±•ç¤ºäº†è¿™ç§è¯†åˆ«è¾¹ç•Œæ¡†çš„æ›¿ä»£æ–¹æ³•ã€‚ä¸åŒçš„è§£å†³æ–¹æ¡ˆå¯èƒ½ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œè¿™é€šå¸¸æ˜¯åŸºäºä¸ªäººçš„åå¥½ã€‚
- en: Object detection models require bounding box coordinates for each object per
    training sample apart from class label. Similarly, an object detection model generates
    *bounding box coordinates* along with *class labels* per identified object during
    inference stage.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ£€æµ‹æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸­éœ€è¦æ¯ä¸ªç‰©ä½“çš„è¾¹ç•Œæ¡†åæ ‡ä»¥åŠç±»åˆ«æ ‡ç­¾ã€‚åŒæ ·ï¼Œåœ¨æ¨ç†é˜¶æ®µï¼Œç›®æ ‡æ£€æµ‹æ¨¡å‹ä¼šä¸ºæ¯ä¸ªè¯†åˆ«å‡ºçš„ç‰©ä½“ç”Ÿæˆ*è¾¹ç•Œæ¡†åæ ‡*å’Œ*ç±»åˆ«æ ‡ç­¾*ã€‚
- en: Anchor Boxes
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é”šæ¡†
- en: Every object detection model scans through a large number of possible regions
    to identify/locate objects for any given image. During the course of training,
    the model learns to determine which of the scanned regions are of interest and
    adjust the coordinates of these regions to match the ground truth bounding boxes.
    Different models may generate these regions of interest differently. Yet, the
    most popular and widely used method is based on *anchor boxes*. For every pixel
    in the given image, multiple bounding boxes of different sizes and aspect ratios
    (ratio of width to height) are generated. These bounding boxes are termed as anchor
    boxes. Figure 2 illustrates different anchor boxes for particular pixel in the
    given image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç›®æ ‡æ£€æµ‹æ¨¡å‹é€šè¿‡æ‰«æå¤§é‡å¯èƒ½çš„åŒºåŸŸæ¥è¯†åˆ«/å®šä½ç»™å®šå›¾åƒä¸­çš„ç‰©ä½“ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å­¦ä¹ ç¡®å®šå“ªäº›æ‰«æåˆ°çš„åŒºåŸŸæ˜¯æ„Ÿå…´è¶£çš„ï¼Œå¹¶è°ƒæ•´è¿™äº›åŒºåŸŸçš„åæ ‡ä»¥åŒ¹é…çœŸå®çš„è¾¹ç•Œæ¡†ã€‚ä¸åŒçš„æ¨¡å‹å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼ç”Ÿæˆè¿™äº›æ„Ÿå…´è¶£åŒºåŸŸã€‚ç„¶è€Œï¼Œæœ€æµè¡Œå’Œå¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•æ˜¯åŸºäº*é”šæ¡†*ã€‚å¯¹äºç»™å®šå›¾åƒä¸­çš„æ¯ä¸ªåƒç´ ï¼Œéƒ½ä¼šç”Ÿæˆå¤šä¸ªä¸åŒå°ºå¯¸å’Œå®½é«˜æ¯”ï¼ˆå®½åº¦ä¸é«˜åº¦çš„æ¯”ç‡ï¼‰çš„è¾¹ç•Œæ¡†ã€‚è¿™äº›è¾¹ç•Œæ¡†è¢«ç§°ä¸ºé”šæ¡†ã€‚å›¾2å±•ç¤ºäº†ç»™å®šå›¾åƒä¸­ç‰¹å®šåƒç´ çš„ä¸åŒé”šæ¡†ã€‚
- en: '![](../Images/ee0dfbb28dcda77b5f25eff7b479d027.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee0dfbb28dcda77b5f25eff7b479d027.png)'
- en: 'Figure 2: Different anchor boxes for a specific pixel (highlighted in red)
    for the given image. Source: Author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šç»™å®šå›¾åƒä¸­ç‰¹å®šåƒç´ ï¼ˆçº¢è‰²æ ‡å‡ºï¼‰å¯¹åº”çš„ä¸åŒé”šæ¡†ã€‚æ¥æºï¼šä½œè€…
- en: 'Anchor box dimensions are controlled using two parameters, *scale* denoted
    as s ğœ– (0,1] and *aspect ratio* denoted as r >0\. As shown in figure 2, for an
    image of height and width h â¨‰ w and specific values of s and r, multiple anchor
    boxes can be generated. Typically, we use the following formulae to get dimensions
    of the anchor boxes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é”šæ¡†çš„ç»´åº¦é€šè¿‡ä¸¤ä¸ªå‚æ•°è¿›è¡Œæ§åˆ¶ï¼Œ*å°ºåº¦*è¡¨ç¤ºä¸º s ğœ– (0,1]ï¼Œ*å®½é«˜æ¯”*è¡¨ç¤ºä¸º r >0ã€‚æ­£å¦‚å›¾2æ‰€ç¤ºï¼Œå¯¹äºé«˜åº¦ä¸º h å’Œå®½åº¦ä¸º w çš„å›¾åƒï¼Œä»¥åŠç‰¹å®šçš„
    s å’Œ r å€¼ï¼Œå¯ä»¥ç”Ÿæˆå¤šä¸ªé”šæ¡†ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å…¬å¼æ¥è®¡ç®—é”šæ¡†çš„ç»´åº¦ï¼š
- en: '**wâ‚=w.sâˆšr**'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**wâ‚=w.sâˆšr**'
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**hâ‚ = h.s / âˆšr**'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**hâ‚ = h.s / âˆšr**'
- en: Where wâ‚and hâ‚are the width and height of the anchor box respectively. Number
    and dimensions of anchor boxes are either predefined or picked up by the model
    during the course of training itself. To put things in perspective, a model generates
    a number of anchor boxes per pixel and learns to adjust/match them with ground
    truth bounding box as the training progresses.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ wâ‚ å’Œ hâ‚ åˆ†åˆ«æ˜¯é”šæ¡†çš„å®½åº¦å’Œé«˜åº¦ã€‚é”šæ¡†çš„æ•°é‡å’Œå°ºå¯¸è¦ä¹ˆæ˜¯é¢„å®šä¹‰çš„ï¼Œè¦ä¹ˆæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”±æ¨¡å‹è‡ªè¡Œé€‰æ‹©çš„ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£ï¼Œæ¨¡å‹åœ¨æ¯ä¸ªåƒç´ ä½ç½®ç”Ÿæˆå¤šä¸ªé”šæ¡†ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ è°ƒæ•´å’ŒåŒ¹é…è¿™äº›é”šæ¡†ä¸çœŸå®è¾¹ç•Œæ¡†ã€‚
- en: 'Bounding boxes and anchor boxes are key concepts to understand the overall
    object detection task. Before we get into the specifics of how such architectures
    work, let us first understand the way we evaluate the performance of such models.
    The following are some of the important evaluation metrics used:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¾¹ç•Œæ¡†å’Œé”šæ¡†æ˜¯ç†è§£æ•´ä¸ªç‰©ä½“æ£€æµ‹ä»»åŠ¡çš„å…³é”®æ¦‚å¿µã€‚åœ¨æ·±å…¥äº†è§£è¿™äº›æ¶æ„çš„å…·ä½“å·¥ä½œæ–¹å¼ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆè¦ç†è§£è¯„ä¼°è¿™äº›æ¨¡å‹æ€§èƒ½çš„æ–¹å¼ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦çš„è¯„ä¼°æŒ‡æ ‡ï¼š
- en: Intersection over union (IOU)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº¤å¹¶æ¯”ï¼ˆIOUï¼‰
- en: An object detection model typically generates a number of anchor boxes which
    are then adjusted to match the ground truth bounding box. But how do we know when
    the match has happened or how well the match is?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹æ¨¡å‹é€šå¸¸ä¼šç”Ÿæˆä¸€äº›é”šæ¡†ï¼Œç„¶åæ ¹æ®å®é™…è¾¹ç•Œæ¡†è¿›è¡Œè°ƒæ•´ã€‚ä½†æˆ‘ä»¬å¦‚ä½•çŸ¥é“ä½•æ—¶åŒ¹é…å‘ç”Ÿï¼Œæˆ–è€…åŒ¹é…çš„æ•ˆæœå¦‚ä½•å‘¢ï¼Ÿ
- en: '*Jaccard Index* is a measure used to determine the similarity between two sets.
    In case of object detection, Jaccard Index is also termed as Intersection Over
    Union or IOU. It is given as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ°å¡å¾·æŒ‡æ•°* æ˜¯ä¸€ç§ç”¨äºç¡®å®šä¸¤ä¸ªé›†åˆä¹‹é—´ç›¸ä¼¼åº¦çš„åº¦é‡ã€‚åœ¨ç‰©ä½“æ£€æµ‹ä¸­ï¼Œæ°å¡å¾·æŒ‡æ•°ä¹Ÿè¢«ç§°ä¸ºäº¤å¹¶æ¯”ï¼ˆIntersection Over Unionï¼Œç®€ç§°IOUï¼‰ã€‚å®ƒçš„è®¡ç®—å…¬å¼ä¸ºï¼š'
- en: '**IOU = | Bâ‚œ âˆ© Bâ‚š | / | Bâ‚œ âˆª Bâ‚š |**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**IOU = | Bâ‚œ âˆ© Bâ‚š | / | Bâ‚œ âˆª Bâ‚š |**'
- en: Where Bâ‚œ is the ground truth bounding box and Bâ‚š is the predicted bounding box.
    In simple terms it is a score between 0 and 1 determined as the ratio of area
    of overlap and area of union between predicted and ground truth bounding box.
    The higher the overlap, the better the score. A score close to 1 depicts near
    perfect match. Figure 3 showcases different scenarios of overlaps between predicted
    and ground truth bounding boxes for a sample image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ Bâ‚œ æ˜¯çœŸå®è¾¹ç•Œæ¡†ï¼ŒBâ‚š æ˜¯é¢„æµ‹è¾¹ç•Œæ¡†ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„åˆ†æ•°ï¼Œè¡¨ç¤ºé¢„æµ‹è¾¹ç•Œæ¡†å’ŒçœŸå®è¾¹ç•Œæ¡†ä¹‹é—´é‡å åŒºåŸŸä¸å¹¶é›†åŒºåŸŸçš„é¢ç§¯æ¯”ã€‚é‡å è¶Šå¤šï¼Œåˆ†æ•°è¶Šé«˜ã€‚æ¥è¿‘
    1 çš„åˆ†æ•°è¡¨ç¤ºå‡ ä¹å®Œç¾çš„åŒ¹é…ã€‚å›¾ 3 å±•ç¤ºäº†é¢„æµ‹è¾¹ç•Œæ¡†ä¸çœŸå®è¾¹ç•Œæ¡†åœ¨æ ·æœ¬å›¾åƒä¸­çš„ä¸åŒé‡å æƒ…å†µã€‚
- en: '![](../Images/691e0c67a0710229f85cb479bfc965a1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/691e0c67a0710229f85cb479bfc965a1.png)'
- en: 'Figure 3: Intersection Over Union (IOU) is a measure of match between the predicted
    and ground-truth bounding box. The higher the overlap, the better is the score.
    Source: Author'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šäº¤å¹¶æ¯”ï¼ˆIOUï¼‰æ˜¯è¡¡é‡é¢„æµ‹è¾¹ç•Œæ¡†ä¸çœŸå®è¾¹ç•Œæ¡†åŒ¹é…ç¨‹åº¦çš„æŒ‡æ ‡ã€‚é‡å è¶Šå¤šï¼Œå¾—åˆ†è¶Šé«˜ã€‚æ¥æºï¼šä½œè€…
- en: Depending upon the problem statement and complexity of the dataset, different
    thresholds for IOU are set to determine which predicted bounding boxes should
    be considered. For instance, an object detection challenge based on [MS-COCO](https://arxiv.org/abs/1405.0312v3)
    uses an IOU threshold of 0.5 to consider a predicted bounding box as true positive.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®é—®é¢˜é™ˆè¿°å’Œæ•°æ®é›†çš„å¤æ‚æ€§ï¼Œè®¾ç½®ä¸åŒçš„ IOU é˜ˆå€¼æ¥ç¡®å®šå“ªäº›é¢„æµ‹è¾¹ç•Œæ¡†åº”è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ã€‚ä¾‹å¦‚ï¼ŒåŸºäº [MS-COCO](https://arxiv.org/abs/1405.0312v3)
    çš„ç‰©ä½“æ£€æµ‹æŒ‘æˆ˜ä½¿ç”¨ 0.5 çš„ IOU é˜ˆå€¼æ¥å°†é¢„æµ‹è¾¹ç•Œæ¡†è§†ä¸ºçœŸæ­£çš„æ­£æ ·æœ¬ã€‚
- en: Mean Average Precision (MAP)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¹³å‡ç²¾åº¦å‡å€¼ï¼ˆMAPï¼‰
- en: 'Precision and Recall are typical metrics used to understand performance of
    classifiers in machine learning context. The following formulae define these metrics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç²¾åº¦å’Œå¬å›ç‡æ˜¯å¸¸ç”¨äºç†è§£åˆ†ç±»å™¨åœ¨æœºå™¨å­¦ä¹ ä¸Šä¸‹æ–‡ä¸­æ€§èƒ½çš„æŒ‡æ ‡ã€‚ä»¥ä¸‹å…¬å¼å®šä¹‰äº†è¿™äº›æŒ‡æ ‡ï¼š
- en: '**Precision = TP / (TP + FP)**'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç²¾åº¦ = TP / (TP + FP)**'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recall = TP/ (TP + FN)
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¬å›ç‡ = TP / (TP + FN)
- en: Where, *TP, FP and FN* stand for *True Positive, False Positive* and *False
    Negative* outcomes respectively. Precision and Recall are typically used together
    to generate Precision-Recall Curve to get a robust quantification of performance.
    This is required due to the opposing nature of precision and recall, i.e. as a
    modelâ€™s recall increases its precision starts decreasing. *PR curves* are used
    to calculate *F1 score*, *Area Under the Curve (AUC)* or *average precision (AP)*
    metrics. Average Precision is calculated as the average of precision at different
    threshold values for recall. Figure 4(a) shows a typical PR curve and figure 4(b)
    depicts how AP is calculated.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ*TPã€FP å’Œ FN* åˆ†åˆ«ä»£è¡¨*çœŸé˜³æ€§ã€å‡é˜³æ€§*å’Œ*å‡é˜´æ€§*çš„ç»“æœã€‚ç²¾åº¦å’Œå¬å›ç‡é€šå¸¸ä¸€èµ·ä½¿ç”¨æ¥ç”Ÿæˆç²¾åº¦-å¬å›æ›²çº¿ï¼Œä»¥å¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œç¨³å¥çš„é‡åŒ–ã€‚è¿™æ˜¯å› ä¸ºç²¾åº¦å’Œå¬å›ç‡çš„å¯¹ç«‹ç‰¹æ€§ï¼Œå³éšç€æ¨¡å‹çš„å¬å›ç‡å¢åŠ ï¼Œç²¾åº¦å¼€å§‹ä¸‹é™ã€‚*PR
    æ›²çº¿*ç”¨äºè®¡ç®—*F1 åˆ†æ•°*ã€*æ›²çº¿ä¸‹é¢ç§¯ (AUC)* æˆ– *å¹³å‡ç²¾åº¦ (AP)* æŒ‡æ ‡ã€‚å¹³å‡ç²¾åº¦æ˜¯é€šè¿‡åœ¨ä¸åŒå¬å›é˜ˆå€¼ä¸‹è®¡ç®—ç²¾åº¦çš„å¹³å‡å€¼æ¥å¾—åˆ°çš„ã€‚å›¾ 4(a)
    æ˜¾ç¤ºäº†ä¸€ä¸ªå…¸å‹çš„ PR æ›²çº¿ï¼Œå›¾ 4(b) å±•ç¤ºäº†å¦‚ä½•è®¡ç®— APã€‚
- en: '![](../Images/b4b6e7d79fc860599b356c1a5d74ebbd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b6e7d79fc860599b356c1a5d74ebbd.png)'
- en: 'Figure 4: a) A typical PR-curve shows modelâ€™s precision at different recall
    values. This is a downward sloping graph due to opposing nature of precision and
    recall metrics; (b) PR-Curve is used to calculate aggregated/combined scores such
    as F1 score, Area Under the Curve (AUC) and Average Precision (AP); (c.) mean
    Average Precision (mAP) is a robust combined metric to understand model performance
    across all classes at different thresholds. Each colored line depicts a different
    PR curve based on specific IOU threshold for each class. Source: Author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼ša) ä¸€ä¸ªå…¸å‹çš„ PR æ›²çº¿å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒå¬å›å€¼ä¸‹çš„ç²¾åº¦ã€‚è¿™æ˜¯ä¸€æ¡å‘ä¸‹å€¾æ–œçš„å›¾è¡¨ï¼Œå› ä¸ºç²¾åº¦å’Œå¬å›ç‡çš„åº¦é‡æ˜¯ç›¸å¯¹çš„ï¼›(b) PR æ›²çº¿ç”¨äºè®¡ç®—èšåˆ/ç»¼åˆåˆ†æ•°ï¼Œå¦‚
    F1 åˆ†æ•°ã€æ›²çº¿ä¸‹é¢ç§¯ (AUC) å’Œå¹³å‡ç²¾åº¦ (AP)ï¼›(c) å¹³å‡ç²¾åº¦å‡å€¼ (mAP) æ˜¯ä¸€ä¸ªç¨³å¥çš„ç»¼åˆæŒ‡æ ‡ï¼Œç”¨äºç†è§£æ¨¡å‹åœ¨ä¸åŒé˜ˆå€¼ä¸‹å¯¹æ‰€æœ‰ç±»åˆ«çš„è¡¨ç°ã€‚æ¯æ¡å½©è‰²æ›²çº¿è¡¨ç¤ºåŸºäºæ¯ä¸ªç±»åˆ«çš„ç‰¹å®š
    IOU é˜ˆå€¼çš„ä¸åŒ PR æ›²çº¿ã€‚æ¥æºï¼šä½œè€…
- en: Figure 4(c) depicts how average precision metric is extended to the object detection
    task. As shown, we calculate PR-Curve at different thresholds of IOU (this is
    done for each class). We then take a mean across all average precision values
    (for each class) to get the final mAP metric. This combined metric is a robust
    quantification of a given modelâ€™s performance. By narrowing down performance to
    just one quantifiable metric makes it easy to compare different modelâ€™s on the
    same test dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4(c) å±•ç¤ºäº†å¹³å‡ç²¾åº¦æŒ‡æ ‡å¦‚ä½•æ‰©å±•åˆ°ç‰©ä½“æ£€æµ‹ä»»åŠ¡ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„ IOU é˜ˆå€¼ä¸‹è®¡ç®— PR æ›²çº¿ï¼ˆè¿™æ˜¯é’ˆå¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œçš„ï¼‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å–æ‰€æœ‰ç±»åˆ«çš„å¹³å‡ç²¾åº¦å€¼çš„å‡å€¼ï¼Œå¾—åˆ°æœ€ç»ˆçš„
    mAP æŒ‡æ ‡ã€‚è¿™ä¸ªç»¼åˆæŒ‡æ ‡æ˜¯å¯¹ç»™å®šæ¨¡å‹åœ¨ä¸åŒç±»åˆ«å’Œé˜ˆå€¼ä¸‹æ€§èƒ½çš„ç¨³å¥é‡åŒ–ã€‚é€šè¿‡å°†æ€§èƒ½ç¼©å°åˆ°ä¸€ä¸ªå¯é‡åŒ–çš„æŒ‡æ ‡ï¼Œå¯ä»¥è½»æ¾åœ°åœ¨ç›¸åŒçš„æµ‹è¯•æ•°æ®é›†ä¸Šæ¯”è¾ƒä¸åŒæ¨¡å‹çš„è¡¨ç°ã€‚
- en: Another metric used to benchmark object detection models is **frames per second
    (FPS)**. This metric points to the number of input images or frames the model
    can analyze for objects per second. This is an important metric for real-time
    use-cases such as security video surveillance, face detection, etc.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªç”¨äºè¯„ä¼°ç‰©ä½“æ£€æµ‹æ¨¡å‹çš„æŒ‡æ ‡æ˜¯**æ¯ç§’å¸§æ•° (FPS)**ã€‚è¯¥æŒ‡æ ‡è¡¨ç¤ºæ¨¡å‹æ¯ç§’å¯ä»¥åˆ†æå¤šå°‘è¾“å…¥å›¾åƒæˆ–å¸§ä»¥æ£€æµ‹ç‰©ä½“ã€‚è¿™æ˜¯å®æ—¶åº”ç”¨åœºæ™¯ï¼ˆå¦‚å®‰å…¨è§†é¢‘ç›‘æ§ã€äººè„¸æ£€æµ‹ç­‰ï¼‰ä¸­ä¸€ä¸ªé‡è¦çš„æŒ‡æ ‡ã€‚
- en: Equipped with these concepts, we are now ready to understand the general framework
    for object detection next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŒæ¡è¿™äº›æ¦‚å¿µï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½ç†è§£ç‰©ä½“æ£€æµ‹çš„ä¸€èˆ¬æ¡†æ¶äº†ã€‚
- en: Object Detection Framework
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹æ¡†æ¶
- en: 'Object detection is an important and active area of research. Over the years,
    a number of different yet effective architectures have been developed and used
    in real-world setting. The task of object detection requires all such architectures
    to tackle a list of sub-tasks. Let us develop an understanding of the general
    framework to tackle object detection before we get to the details of how specific
    models handle them. The general framework comprises of the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹æ˜¯ä¸€ä¸ªé‡è¦ä¸”æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚å¤šå¹´æ¥ï¼Œå·²ç»å¼€å‘å¹¶åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨äº†è®¸å¤šä¸åŒä½†æœ‰æ•ˆçš„æ¶æ„ã€‚ç‰©ä½“æ£€æµ‹çš„ä»»åŠ¡è¦æ±‚æ‰€æœ‰è¿™äº›æ¶æ„è§£å†³ä¸€ç³»åˆ—å­ä»»åŠ¡ã€‚åœ¨æˆ‘ä»¬æ·±å…¥äº†è§£å…·ä½“æ¨¡å‹å¦‚ä½•å¤„ç†è¿™äº›ä»»åŠ¡ä¹‹å‰ï¼Œå…ˆæ¥ç†è§£ä¸€ä¸‹åº”å¯¹ç‰©ä½“æ£€æµ‹çš„ä¸€èˆ¬æ¡†æ¶ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Region Proposal Network
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒºåŸŸå»ºè®®ç½‘ç»œ
- en: Localization and Class Predictions
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®šä½å’Œåˆ†ç±»é¢„æµ‹
- en: Output Optimizations
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å‡ºä¼˜åŒ–
- en: Let us now go through each of these steps in some detail.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è¯¦ç»†äº†è§£ä¸€ä¸‹è¿™äº›æ­¥éª¤ã€‚
- en: Regional Proposal
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŒºåŸŸå»ºè®®
- en: As the name suggests, the first and foremost step in the object detection framework
    is to propose *regions of interest (ROI)*. ROIs are the regions of the input image
    for which the model believes there is a high likelihood of an objectâ€™s presence.
    The likelihood of an objectâ€™s presence or absence is defined using a score called
    objectness score. Regions which have objectness score greater than a certain threshold
    are passed onto the next stage while others are reject.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é¡¾åæ€ä¹‰ï¼Œç‰©ä½“æ£€æµ‹æ¡†æ¶ä¸­çš„ç¬¬ä¸€æ­¥æ˜¯æå‡º*æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰*ã€‚ROIæ˜¯è¾“å…¥å›¾åƒä¸­ï¼Œæ¨¡å‹è®¤ä¸ºç‰©ä½“å­˜åœ¨çš„å¯èƒ½æ€§è¾ƒé«˜çš„åŒºåŸŸã€‚ç‰©ä½“å­˜åœ¨æˆ–ä¸å­˜åœ¨çš„å¯èƒ½æ€§é€šè¿‡ä¸€ä¸ªç§°ä¸ºç‰©ä½“æ€§å¾—åˆ†çš„åˆ†æ•°æ¥å®šä¹‰ã€‚é‚£äº›ç‰©ä½“æ€§å¾—åˆ†å¤§äºæŸä¸ªé˜ˆå€¼çš„åŒºåŸŸä¼šä¼ é€’åˆ°ä¸‹ä¸€é˜¶æ®µï¼Œè€Œå…¶ä»–åŒºåŸŸåˆ™è¢«ä¸¢å¼ƒã€‚
- en: For example, take a look at figure 5 for different ROIs proposed by the model.
    It is important to note that a large number of ROIs are generated at this step.
    Based on the objectness score threshold, the model classifies ROIs as foreground
    or background and only passes foreground regions for further analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒæŸ¥çœ‹å›¾5ï¼Œäº†è§£æ¨¡å‹æå‡ºçš„ä¸åŒæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™ä¸€é˜¶æ®µä¼šç”Ÿæˆå¤§é‡çš„ROIã€‚åŸºäºç‰©ä½“æ€§å¾—åˆ†é˜ˆå€¼ï¼Œæ¨¡å‹ä¼šå°†ROIåˆ†ç±»ä¸ºå‰æ™¯æˆ–èƒŒæ™¯ï¼Œä»…å°†å‰æ™¯åŒºåŸŸä¼ é€’åˆ°ä¸‹ä¸€æ­¥è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚
- en: '![](../Images/c2cc1fcfb109d88d81dd4324de71c252.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2cc1fcfb109d88d81dd4324de71c252.png)'
- en: 'Figure 5: Regional Proposal is the first step in object detection framework.
    Regions of Interest are highlighted as red rectangular boxes. The model marks
    regions with high likelihood of an image (high objectness score) as foreground
    regions and rest as background regions. Source: Author'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šåŒºåŸŸæè®®æ˜¯ç‰©ä½“æ£€æµ‹æ¡†æ¶ä¸­çš„ç¬¬ä¸€æ­¥ã€‚æ„Ÿå…´è¶£åŒºåŸŸä»¥çº¢è‰²çŸ©å½¢æ¡†çš„å½¢å¼çªå‡ºæ˜¾ç¤ºã€‚æ¨¡å‹å°†å›¾åƒä¸­å¯èƒ½æ€§è¾ƒé«˜çš„åŒºåŸŸï¼ˆé«˜ç‰©ä½“æ€§å¾—åˆ†ï¼‰æ ‡è®°ä¸ºå‰æ™¯åŒºåŸŸï¼Œå…¶ä½™æ ‡è®°ä¸ºèƒŒæ™¯åŒºåŸŸã€‚æ¥æºï¼šä½œè€…
- en: There are a number of different ways of generating regions of interest. Earlier
    models used to make use of selective search and related algorithms to generate
    ROIs while newer and more complex models make use of deep learning models to do
    so. We will cover these when we discuss specific architectures in the upcoming
    articles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰æœ‰å¤šç§ä¸åŒçš„æ–¹æ³•ã€‚æ—©æœŸçš„æ¨¡å‹é€šå¸¸ä½¿ç”¨é€‰æ‹©æ€§æœç´¢åŠç›¸å…³ç®—æ³•æ¥ç”ŸæˆROIï¼Œè€Œæ–°å‹æ›´å¤æ‚çš„æ¨¡å‹åˆ™åˆ©ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­è®¨è®ºå…·ä½“æ¶æ„æ—¶è¿›ä¸€æ­¥æ¢è®¨è¿™äº›æ–¹æ³•ã€‚
- en: Localization And Class Predictions
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä½ä¸åˆ†ç±»é¢„æµ‹
- en: 'Object detection models are different from the classification models we typically
    work with. An object detection model generates two outputs for every foreground
    region from the previous step:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰©ä½“æ£€æµ‹æ¨¡å‹ä¸æˆ‘ä»¬é€šå¸¸ä½¿ç”¨çš„åˆ†ç±»æ¨¡å‹æœ‰æ‰€ä¸åŒã€‚ç‰©ä½“æ£€æµ‹æ¨¡å‹ä¼šä¸ºæ¯ä¸ªå‰æ™¯åŒºåŸŸç”Ÿæˆä¸¤ä¸ªè¾“å‡ºï¼Œå‰æ™¯åŒºåŸŸæ¥è‡ªäºä¸Šä¸€é˜¶æ®µçš„ç»“æœï¼š
- en: '**Object Class**: This is the typical classification objective to assign a
    class label to every proposed foreground region. Typically, pre-trained networks
    are used to extract features from the proposed region and then use those features
    to predict the class. State-of-the-art models such as the ones trained on ImageNet
    or MS-COCO with a large number of classes are widely adapted/transfer learnt.
    It is important to note that we generate a class label for every proposed region
    and not just a single label for the whole image (as compared to a typical classification
    task)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç‰©ä½“ç±»åˆ«**ï¼šè¿™æ˜¯å…¸å‹çš„åˆ†ç±»ç›®æ ‡ï¼Œç›®çš„æ˜¯ä¸ºæ¯ä¸ªæè®®çš„å‰æ™¯åŒºåŸŸåˆ†é…ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ã€‚é€šå¸¸ï¼Œä¼šä½¿ç”¨é¢„è®­ç»ƒçš„ç½‘ç»œä»æè®®åŒºåŸŸæå–ç‰¹å¾ï¼Œç„¶ååˆ©ç”¨è¿™äº›ç‰¹å¾æ¥é¢„æµ‹ç±»åˆ«ã€‚åƒåœ¨ImageNetæˆ–MS-COCOä¸Šè®­ç»ƒçš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œæ¶µç›–äº†å¤§é‡ç±»åˆ«å¹¶å¹¿æ³›é‡‡ç”¨è¿ç§»å­¦ä¹ ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªæè®®åŒºåŸŸç”Ÿæˆç±»åˆ«æ ‡ç­¾ï¼Œè€Œä¸æ˜¯åƒå…¸å‹çš„åˆ†ç±»ä»»åŠ¡é‚£æ ·ä¸ºæ•´ä¸ªå›¾åƒç”Ÿæˆä¸€ä¸ªå•ä¸€æ ‡ç­¾ã€‚'
- en: '**Bounding Box Coordinates**: A bounding box is defined a tuple with 4 values
    for x, y, width and height. At this stage the model generates a tuple for every
    proposed foreground region as well (along with the object class).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾¹ç•Œæ¡†åæ ‡**ï¼šè¾¹ç•Œæ¡†å®šä¹‰ä¸ºä¸€ä¸ªåŒ…å«4ä¸ªå€¼çš„å…ƒç»„ï¼Œåˆ†åˆ«è¡¨ç¤ºxã€yã€å®½åº¦å’Œé«˜åº¦ã€‚åœ¨è¿™ä¸€é˜¶æ®µï¼Œæ¨¡å‹ä¼šä¸ºæ¯ä¸ªæè®®çš„å‰æ™¯åŒºåŸŸç”Ÿæˆä¸€ä¸ªå…ƒç»„ï¼ˆåŒæ—¶åŒ…æ‹¬ç‰©ä½“ç±»åˆ«ï¼‰ã€‚'
- en: Output Optimization
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾“å‡ºä¼˜åŒ–
- en: As mentioned earlier, an object detection model proposes a large number of ROIs
    in step one followed by bounding box and class predictions in step two. While
    there is some level of filtering of ROIs in step one (foreground vs background
    regions based on objectness score), there are still a large number of regions
    used for predictions in step two. Generating predictions for such a large number
    of proposed regions ensures good coverage for various objects in the image. Yet,
    there are a number of regions with good amount of overlap for the same region.
    For example, look at the 6 bounding boxes predicted for the same object in figure
    6(a). This potentially can lead to difficulty in getting the exact count of different
    objects in the input image.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œç‰©ä½“æ£€æµ‹æ¨¡å‹åœ¨ç¬¬ä¸€æ­¥æå‡ºäº†å¤§é‡çš„ROIï¼ˆæ„Ÿå…´è¶£åŒºåŸŸï¼‰ï¼Œç„¶ååœ¨ç¬¬äºŒæ­¥è¿›è¡Œè¾¹ç•Œæ¡†å’Œç±»åˆ«é¢„æµ‹ã€‚è™½ç„¶åœ¨ç¬¬ä¸€æ­¥ä¸­å¯¹ROIè¿›è¡Œäº†æŸç§ç¨‹åº¦çš„è¿‡æ»¤ï¼ˆåŸºäºç‰©ä½“å¾—åˆ†åŒºåˆ†å‰æ™¯ä¸èƒŒæ™¯åŒºåŸŸï¼‰ï¼Œä½†åœ¨ç¬¬äºŒæ­¥ä¸­ä»ç„¶æœ‰å¤§é‡åŒºåŸŸç”¨äºé¢„æµ‹ã€‚ä¸ºå¦‚æ­¤å¤§é‡çš„æè®®åŒºåŸŸç”Ÿæˆé¢„æµ‹ç¡®ä¿äº†å¯¹å›¾åƒä¸­å„ç§ç‰©ä½“çš„è‰¯å¥½è¦†ç›–ã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›åŒºåŸŸå¯¹åŒä¸€ç‰©ä½“å­˜åœ¨å¤§é‡é‡å ã€‚ä¾‹å¦‚ï¼Œçœ‹çœ‹å›¾6(a)ä¸­ä¸ºåŒä¸€ä¸ªç‰©ä½“é¢„æµ‹çš„6ä¸ªè¾¹ç•Œæ¡†ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´éš¾ä»¥å‡†ç¡®ç»Ÿè®¡è¾“å…¥å›¾åƒä¸­ä¸åŒç‰©ä½“çš„æ•°é‡ã€‚
- en: '![](../Images/595c3830c4ee85b07a92cc6afacc8f42.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/595c3830c4ee85b07a92cc6afacc8f42.png)'
- en: 'Figure 6 (a)Object detection model generating 6 bounding boxes with good overlap
    for the same object. (b) Output optimized using NMS. Source: Author'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6 (a) ç‰©ä½“æ£€æµ‹æ¨¡å‹ä¸ºåŒä¸€ç‰©ä½“ç”Ÿæˆ6ä¸ªé‡å è¾ƒå¤šçš„è¾¹ç•Œæ¡†ã€‚ (b) ä½¿ç”¨NMSä¼˜åŒ–åçš„è¾“å‡ºã€‚æ¥æºï¼šä½œè€…
- en: Hence, there is a third step in this framework which concerns the optimization
    of the output. This optimization step ensures there is only one bounding box and
    class prediction per object in the input image. There are different ways of performing
    this optimization. By far, the most popular method is called **Non-Maximum Suppression
    (NMS)**. As the name suggests, NMS analyzes all bounding boxes for each object
    to find the one with maximum probability and suppress the rest of them (see figure
    6(b) for optimized output after applying NMS).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨è¿™ä¸ªæ¡†æ¶ä¸­æœ‰ç¬¬ä¸‰ä¸ªæ­¥éª¤ï¼Œæ¶‰åŠè¾“å‡ºçš„ä¼˜åŒ–ã€‚è¿™ä¸ªä¼˜åŒ–æ­¥éª¤ç¡®ä¿æ¯ä¸ªè¾“å…¥å›¾åƒä¸­çš„æ¯ä¸ªç‰©ä½“åªæœ‰ä¸€ä¸ªè¾¹ç•Œæ¡†å’Œç±»åˆ«é¢„æµ‹ã€‚è¿›è¡Œè¿™ç§ä¼˜åŒ–çš„æ–¹æ³•æœ‰å¤šç§ã€‚ç›®å‰ï¼Œæœ€æµè¡Œçš„æ–¹æ³•è¢«ç§°ä¸º**éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰**ã€‚é¡¾åæ€ä¹‰ï¼ŒNMSä¼šåˆ†ææ¯ä¸ªç‰©ä½“çš„æ‰€æœ‰è¾¹ç•Œæ¡†ï¼Œæ‰¾åˆ°å…·æœ‰æœ€å¤§æ¦‚ç‡çš„é‚£ä¸ªï¼Œå¹¶æŠ‘åˆ¶å…¶ä½™çš„è¾¹ç•Œæ¡†ï¼ˆè¯·å‚è§å›¾6(b)ï¼Œå±•ç¤ºäº†åº”ç”¨NMSåçš„ä¼˜åŒ–è¾“å‡ºï¼‰ã€‚
- en: This concludes a high-level understanding of a general object detection framework.
    We discussed about the three major steps involved in localization and classification
    of objects in a given image. In this next article we will use this understanding
    to understand specific implementations and their key contributions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±ç»“æŸäº†å¯¹ä¸€èˆ¬ç‰©ä½“æ£€æµ‹æ¡†æ¶çš„é«˜å±‚æ¬¡ç†è§£ã€‚æˆ‘ä»¬è®¨è®ºäº†å®šä½å’Œåˆ†ç±»å›¾åƒä¸­ç‰©ä½“çš„ä¸‰ä¸ªä¸»è¦æ­¥éª¤ã€‚åœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†åŸºäºè¿™äº›ç†è§£ï¼Œæ¢è®¨å…·ä½“çš„å®ç°æ–¹æ³•åŠå…¶å…³é”®è´¡çŒ®ã€‚
