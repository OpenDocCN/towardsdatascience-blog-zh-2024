- en: Object Detection Basics — A Comprehensive Beginner’s Guide (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测基础 — 综合初学者指南（第一部分）
- en: 原文：[https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05](https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05](https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05)
- en: Learn the basics of this advanced computer vision task of object detection in
    an easy to understand multi-part beginner’s guide
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这篇易于理解的多部分初学者指南中，学习这个高级计算机视觉任务——物体检测的基础知识
- en: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    ·9 min read·Feb 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------)
    ·9 分钟阅读 ·2024年2月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8f95d39d6f6a1014031f57234f642646.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f95d39d6f6a1014031f57234f642646.png)'
- en: Photo by [Javier García](https://unsplash.com/@javigabbo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Javier García](https://unsplash.com/@javigabbo?utm_source=medium&utm_medium=referral)
    由 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Driving a car nowadays with the latest drive assist technologies for lane detection,
    blind-spots, traffic signals and so on is pretty common. If we take a step back
    for a minute to appreciate what is happening behind the scenes, the Data Scientist
    in us soon realises that the system is not just *classifying* objects but also
    *locating* them in the scene (in real-time).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，配备最新驾驶辅助技术（如车道检测、盲点监测、交通信号识别等）的汽车已经相当普遍。如果我们稍微退后一步，去了解一下幕后发生了什么，作为数据科学家，我们很快会意识到，系统不仅仅是在*分类*物体，还在实时地*定位*它们。
- en: Such capabilities are prime examples of an **object detection** system in action.
    Drive assist technologies, industrial robots and security systems all make use
    of object detection models to detect objects of interest. **Object detection**
    is an advanced computer vision task which involves both *localisation* [of objects]
    as well as *classification*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能是**物体检测**系统实际应用的典型示例。驾驶辅助技术、工业机器人和安全系统都利用物体检测模型来检测感兴趣的物体。**物体检测**是一个高级计算机视觉任务，涉及*物体的定位*和*分类*。
- en: In this article, we will dive deeper into the details of the object detection
    task. We will learn about various concepts associated with it to help us understand
    novel architectures (covered in subsequent articles). We will cover key aspects
    and concepts required to understand object detection models from a Transfer Learning
    standpoint.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将深入探讨物体检测任务的细节。我们将学习与之相关的各种概念，以帮助我们理解新颖的架构（将在后续文章中讨论）。我们将涵盖理解物体检测模型所需的关键方面和概念，尤其是从迁移学习的角度。
- en: Key Concepts and Building Blocks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键概念与构建模块
- en: 'Object detection consists of two main sub-tasks, *localization* and *classification*.
    Classification of identified objects is straightforward to understand. But how
    do we define localization of objects? Let us cover some key concepts:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测包括两个主要子任务，*定位*和*分类*。物体的分类是容易理解的。但是，我们如何定义物体的定位呢？让我们了解一些关键概念：
- en: Bounding Boxes
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 边界框
- en: For the task of object detection, we identify a given object’s location using
    a rectangular box. This regular box is termed as a *bounding box* and used for
    localization of objects. Typically, the top left corner of the input image is
    set as origin or (0,0). A rectangular bounding box is defined with the help of
    its x and y coordinates for the top-left and bottom right vertices. Let us understand
    this visually. Figure 1(a) depicts a sample image with its origin set at its top
    left corner.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标检测任务中，我们通过一个矩形框来标识给定物体的位置。这个规则的矩形框被称为*边界框*，用于物体的定位。通常，输入图像的左上角被设为原点或(0,0)。一个矩形边界框通过其左上角和右下角的
    x 和 y 坐标来定义。让我们通过图像来直观地理解这一点。图1(a)展示了一个示例图像，其原点设置在左上角。
- en: '![](../Images/0580171bb1c23766c169aa7f91f7a9a9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0580171bb1c23766c169aa7f91f7a9a9.png)'
- en: 'Figure 1: (a) A sample image with different objects, (b) bounding boxes for
    each of the objects with top-left and bottom-right vertices annotated,(c.)alternate
    way of identifying a bounding box is to use its top-left coordinates along with
    width and height parameters. Source: Author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）包含不同物体的示例图像，（b）每个物体的边界框，标注了左上角和右下角的顶点，（c）识别边界框的另一种方法是使用左上角坐标及其宽度和高度参数。来源：作者
- en: Figure 1(b) shows each of the identified objects with their corresponding bounding
    boxes. It is important to note that a bounding box is annotated with its top-left
    and bottom-right coordinates which are relative to the image’s origin. With 4
    values, we can identify a bounding box uniquely. An alternate method to identify
    a bounding box is to use top-left coordinates along with its width and height
    values. Figure 1(c) shows this alternate way of identifying a bounding box. Different
    solutions may use different methods and it is mostly a matter of preference of
    one over the other.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1(b)显示了每个已识别物体及其相应的边界框。需要注意的是，边界框通过其左上角和右下角的坐标进行标注，这些坐标是相对于图像原点的。通过4个值，我们可以唯一地识别一个边界框。另一种识别边界框的方法是使用左上角坐标以及其宽度和高度值。图1(c)展示了这种识别边界框的替代方法。不同的解决方案可能使用不同的方法，这通常是基于个人的偏好。
- en: Object detection models require bounding box coordinates for each object per
    training sample apart from class label. Similarly, an object detection model generates
    *bounding box coordinates* along with *class labels* per identified object during
    inference stage.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测模型在每个训练样本中需要每个物体的边界框坐标以及类别标签。同样，在推理阶段，目标检测模型会为每个识别出的物体生成*边界框坐标*和*类别标签*。
- en: Anchor Boxes
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 锚框
- en: Every object detection model scans through a large number of possible regions
    to identify/locate objects for any given image. During the course of training,
    the model learns to determine which of the scanned regions are of interest and
    adjust the coordinates of these regions to match the ground truth bounding boxes.
    Different models may generate these regions of interest differently. Yet, the
    most popular and widely used method is based on *anchor boxes*. For every pixel
    in the given image, multiple bounding boxes of different sizes and aspect ratios
    (ratio of width to height) are generated. These bounding boxes are termed as anchor
    boxes. Figure 2 illustrates different anchor boxes for particular pixel in the
    given image.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个目标检测模型通过扫描大量可能的区域来识别/定位给定图像中的物体。在训练过程中，模型学习确定哪些扫描到的区域是感兴趣的，并调整这些区域的坐标以匹配真实的边界框。不同的模型可能会以不同的方式生成这些感兴趣区域。然而，最流行和广泛使用的方法是基于*锚框*。对于给定图像中的每个像素，都会生成多个不同尺寸和宽高比（宽度与高度的比率）的边界框。这些边界框被称为锚框。图2展示了给定图像中特定像素的不同锚框。
- en: '![](../Images/ee0dfbb28dcda77b5f25eff7b479d027.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee0dfbb28dcda77b5f25eff7b479d027.png)'
- en: 'Figure 2: Different anchor boxes for a specific pixel (highlighted in red)
    for the given image. Source: Author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：给定图像中特定像素（红色标出）对应的不同锚框。来源：作者
- en: 'Anchor box dimensions are controlled using two parameters, *scale* denoted
    as s 𝜖 (0,1] and *aspect ratio* denoted as r >0\. As shown in figure 2, for an
    image of height and width h ⨉ w and specific values of s and r, multiple anchor
    boxes can be generated. Typically, we use the following formulae to get dimensions
    of the anchor boxes:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 锚框的维度通过两个参数进行控制，*尺度*表示为 s 𝜖 (0,1]，*宽高比*表示为 r >0。正如图2所示，对于高度为 h 和宽度为 w 的图像，以及特定的
    s 和 r 值，可以生成多个锚框。通常，我们使用以下公式来计算锚框的维度：
- en: '**wₐ=w.s√r**'
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**wₐ=w.s√r**'
- en: ''
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**hₐ = h.s / √r**'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**hₐ = h.s / √r**'
- en: Where wₐand hₐare the width and height of the anchor box respectively. Number
    and dimensions of anchor boxes are either predefined or picked up by the model
    during the course of training itself. To put things in perspective, a model generates
    a number of anchor boxes per pixel and learns to adjust/match them with ground
    truth bounding box as the training progresses.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 wₐ 和 hₐ 分别是锚框的宽度和高度。锚框的数量和尺寸要么是预定义的，要么是在训练过程中由模型自行选择的。为了更好地理解，模型在每个像素位置生成多个锚框，并在训练过程中学习调整和匹配这些锚框与真实边界框。
- en: 'Bounding boxes and anchor boxes are key concepts to understand the overall
    object detection task. Before we get into the specifics of how such architectures
    work, let us first understand the way we evaluate the performance of such models.
    The following are some of the important evaluation metrics used:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 边界框和锚框是理解整个物体检测任务的关键概念。在深入了解这些架构的具体工作方式之前，我们首先要理解评估这些模型性能的方式。以下是一些重要的评估指标：
- en: Intersection over union (IOU)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交并比（IOU）
- en: An object detection model typically generates a number of anchor boxes which
    are then adjusted to match the ground truth bounding box. But how do we know when
    the match has happened or how well the match is?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测模型通常会生成一些锚框，然后根据实际边界框进行调整。但我们如何知道何时匹配发生，或者匹配的效果如何呢？
- en: '*Jaccard Index* is a measure used to determine the similarity between two sets.
    In case of object detection, Jaccard Index is also termed as Intersection Over
    Union or IOU. It is given as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*杰卡德指数* 是一种用于确定两个集合之间相似度的度量。在物体检测中，杰卡德指数也被称为交并比（Intersection Over Union，简称IOU）。它的计算公式为：'
- en: '**IOU = | Bₜ ∩ Bₚ | / | Bₜ ∪ Bₚ |**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**IOU = | Bₜ ∩ Bₚ | / | Bₜ ∪ Bₚ |**'
- en: Where Bₜ is the ground truth bounding box and Bₚ is the predicted bounding box.
    In simple terms it is a score between 0 and 1 determined as the ratio of area
    of overlap and area of union between predicted and ground truth bounding box.
    The higher the overlap, the better the score. A score close to 1 depicts near
    perfect match. Figure 3 showcases different scenarios of overlaps between predicted
    and ground truth bounding boxes for a sample image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 Bₜ 是真实边界框，Bₚ 是预测边界框。简单来说，它是一个介于 0 和 1 之间的分数，表示预测边界框和真实边界框之间重叠区域与并集区域的面积比。重叠越多，分数越高。接近
    1 的分数表示几乎完美的匹配。图 3 展示了预测边界框与真实边界框在样本图像中的不同重叠情况。
- en: '![](../Images/691e0c67a0710229f85cb479bfc965a1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/691e0c67a0710229f85cb479bfc965a1.png)'
- en: 'Figure 3: Intersection Over Union (IOU) is a measure of match between the predicted
    and ground-truth bounding box. The higher the overlap, the better is the score.
    Source: Author'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：交并比（IOU）是衡量预测边界框与真实边界框匹配程度的指标。重叠越多，得分越高。来源：作者
- en: Depending upon the problem statement and complexity of the dataset, different
    thresholds for IOU are set to determine which predicted bounding boxes should
    be considered. For instance, an object detection challenge based on [MS-COCO](https://arxiv.org/abs/1405.0312v3)
    uses an IOU threshold of 0.5 to consider a predicted bounding box as true positive.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题陈述和数据集的复杂性，设置不同的 IOU 阈值来确定哪些预测边界框应被认为是有效的。例如，基于 [MS-COCO](https://arxiv.org/abs/1405.0312v3)
    的物体检测挑战使用 0.5 的 IOU 阈值来将预测边界框视为真正的正样本。
- en: Mean Average Precision (MAP)
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平均精度均值（MAP）
- en: 'Precision and Recall are typical metrics used to understand performance of
    classifiers in machine learning context. The following formulae define these metrics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率是常用于理解分类器在机器学习上下文中性能的指标。以下公式定义了这些指标：
- en: '**Precision = TP / (TP + FP)**'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**精度 = TP / (TP + FP)**'
- en: ''
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recall = TP/ (TP + FN)
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 召回率 = TP / (TP + FN)
- en: Where, *TP, FP and FN* stand for *True Positive, False Positive* and *False
    Negative* outcomes respectively. Precision and Recall are typically used together
    to generate Precision-Recall Curve to get a robust quantification of performance.
    This is required due to the opposing nature of precision and recall, i.e. as a
    model’s recall increases its precision starts decreasing. *PR curves* are used
    to calculate *F1 score*, *Area Under the Curve (AUC)* or *average precision (AP)*
    metrics. Average Precision is calculated as the average of precision at different
    threshold values for recall. Figure 4(a) shows a typical PR curve and figure 4(b)
    depicts how AP is calculated.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*TP、FP 和 FN* 分别代表*真阳性、假阳性*和*假阴性*的结果。精度和召回率通常一起使用来生成精度-召回曲线，以对模型性能进行稳健的量化。这是因为精度和召回率的对立特性，即随着模型的召回率增加，精度开始下降。*PR
    曲线*用于计算*F1 分数*、*曲线下面积 (AUC)* 或 *平均精度 (AP)* 指标。平均精度是通过在不同召回阈值下计算精度的平均值来得到的。图 4(a)
    显示了一个典型的 PR 曲线，图 4(b) 展示了如何计算 AP。
- en: '![](../Images/b4b6e7d79fc860599b356c1a5d74ebbd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4b6e7d79fc860599b356c1a5d74ebbd.png)'
- en: 'Figure 4: a) A typical PR-curve shows model’s precision at different recall
    values. This is a downward sloping graph due to opposing nature of precision and
    recall metrics; (b) PR-Curve is used to calculate aggregated/combined scores such
    as F1 score, Area Under the Curve (AUC) and Average Precision (AP); (c.) mean
    Average Precision (mAP) is a robust combined metric to understand model performance
    across all classes at different thresholds. Each colored line depicts a different
    PR curve based on specific IOU threshold for each class. Source: Author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：a) 一个典型的 PR 曲线展示了模型在不同召回值下的精度。这是一条向下倾斜的图表，因为精度和召回率的度量是相对的；(b) PR 曲线用于计算聚合/综合分数，如
    F1 分数、曲线下面积 (AUC) 和平均精度 (AP)；(c) 平均精度均值 (mAP) 是一个稳健的综合指标，用于理解模型在不同阈值下对所有类别的表现。每条彩色曲线表示基于每个类别的特定
    IOU 阈值的不同 PR 曲线。来源：作者
- en: Figure 4(c) depicts how average precision metric is extended to the object detection
    task. As shown, we calculate PR-Curve at different thresholds of IOU (this is
    done for each class). We then take a mean across all average precision values
    (for each class) to get the final mAP metric. This combined metric is a robust
    quantification of a given model’s performance. By narrowing down performance to
    just one quantifiable metric makes it easy to compare different model’s on the
    same test dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4(c) 展示了平均精度指标如何扩展到物体检测任务。如图所示，我们在不同的 IOU 阈值下计算 PR 曲线（这是针对每个类别进行的）。然后，我们取所有类别的平均精度值的均值，得到最终的
    mAP 指标。这个综合指标是对给定模型在不同类别和阈值下性能的稳健量化。通过将性能缩小到一个可量化的指标，可以轻松地在相同的测试数据集上比较不同模型的表现。
- en: Another metric used to benchmark object detection models is **frames per second
    (FPS)**. This metric points to the number of input images or frames the model
    can analyze for objects per second. This is an important metric for real-time
    use-cases such as security video surveillance, face detection, etc.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于评估物体检测模型的指标是**每秒帧数 (FPS)**。该指标表示模型每秒可以分析多少输入图像或帧以检测物体。这是实时应用场景（如安全视频监控、人脸检测等）中一个重要的指标。
- en: Equipped with these concepts, we are now ready to understand the general framework
    for object detection next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握这些概念，我们现在准备好理解物体检测的一般框架了。
- en: Object Detection Framework
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测框架
- en: 'Object detection is an important and active area of research. Over the years,
    a number of different yet effective architectures have been developed and used
    in real-world setting. The task of object detection requires all such architectures
    to tackle a list of sub-tasks. Let us develop an understanding of the general
    framework to tackle object detection before we get to the details of how specific
    models handle them. The general framework comprises of the following steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测是一个重要且活跃的研究领域。多年来，已经开发并在实际应用中使用了许多不同但有效的架构。物体检测的任务要求所有这些架构解决一系列子任务。在我们深入了解具体模型如何处理这些任务之前，先来理解一下应对物体检测的一般框架。该框架包括以下步骤：
- en: Region Proposal Network
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域建议网络
- en: Localization and Class Predictions
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位和分类预测
- en: Output Optimizations
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出优化
- en: Let us now go through each of these steps in some detail.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细了解一下这些步骤。
- en: Regional Proposal
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区域建议
- en: As the name suggests, the first and foremost step in the object detection framework
    is to propose *regions of interest (ROI)*. ROIs are the regions of the input image
    for which the model believes there is a high likelihood of an object’s presence.
    The likelihood of an object’s presence or absence is defined using a score called
    objectness score. Regions which have objectness score greater than a certain threshold
    are passed onto the next stage while others are reject.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，物体检测框架中的第一步是提出*感兴趣区域（ROI）*。ROI是输入图像中，模型认为物体存在的可能性较高的区域。物体存在或不存在的可能性通过一个称为物体性得分的分数来定义。那些物体性得分大于某个阈值的区域会传递到下一阶段，而其他区域则被丢弃。
- en: For example, take a look at figure 5 for different ROIs proposed by the model.
    It is important to note that a large number of ROIs are generated at this step.
    Based on the objectness score threshold, the model classifies ROIs as foreground
    or background and only passes foreground regions for further analysis.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，查看图5，了解模型提出的不同感兴趣区域（ROI）。需要注意的是，在这一阶段会生成大量的ROI。基于物体性得分阈值，模型会将ROI分类为前景或背景，仅将前景区域传递到下一步进行进一步分析。
- en: '![](../Images/c2cc1fcfb109d88d81dd4324de71c252.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2cc1fcfb109d88d81dd4324de71c252.png)'
- en: 'Figure 5: Regional Proposal is the first step in object detection framework.
    Regions of Interest are highlighted as red rectangular boxes. The model marks
    regions with high likelihood of an image (high objectness score) as foreground
    regions and rest as background regions. Source: Author'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：区域提议是物体检测框架中的第一步。感兴趣区域以红色矩形框的形式突出显示。模型将图像中可能性较高的区域（高物体性得分）标记为前景区域，其余标记为背景区域。来源：作者
- en: There are a number of different ways of generating regions of interest. Earlier
    models used to make use of selective search and related algorithms to generate
    ROIs while newer and more complex models make use of deep learning models to do
    so. We will cover these when we discuss specific architectures in the upcoming
    articles.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成感兴趣区域（ROI）有多种不同的方法。早期的模型通常使用选择性搜索及相关算法来生成ROI，而新型更复杂的模型则利用深度学习模型来完成这一任务。我们将在接下来的文章中讨论具体架构时进一步探讨这些方法。
- en: Localization And Class Predictions
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定位与分类预测
- en: 'Object detection models are different from the classification models we typically
    work with. An object detection model generates two outputs for every foreground
    region from the previous step:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测模型与我们通常使用的分类模型有所不同。物体检测模型会为每个前景区域生成两个输出，前景区域来自于上一阶段的结果：
- en: '**Object Class**: This is the typical classification objective to assign a
    class label to every proposed foreground region. Typically, pre-trained networks
    are used to extract features from the proposed region and then use those features
    to predict the class. State-of-the-art models such as the ones trained on ImageNet
    or MS-COCO with a large number of classes are widely adapted/transfer learnt.
    It is important to note that we generate a class label for every proposed region
    and not just a single label for the whole image (as compared to a typical classification
    task)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体类别**：这是典型的分类目标，目的是为每个提议的前景区域分配一个类别标签。通常，会使用预训练的网络从提议区域提取特征，然后利用这些特征来预测类别。像在ImageNet或MS-COCO上训练的最先进模型，涵盖了大量类别并广泛采用迁移学习。需要注意的是，我们为每个提议区域生成类别标签，而不是像典型的分类任务那样为整个图像生成一个单一标签。'
- en: '**Bounding Box Coordinates**: A bounding box is defined a tuple with 4 values
    for x, y, width and height. At this stage the model generates a tuple for every
    proposed foreground region as well (along with the object class).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界框坐标**：边界框定义为一个包含4个值的元组，分别表示x、y、宽度和高度。在这一阶段，模型会为每个提议的前景区域生成一个元组（同时包括物体类别）。'
- en: Output Optimization
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出优化
- en: As mentioned earlier, an object detection model proposes a large number of ROIs
    in step one followed by bounding box and class predictions in step two. While
    there is some level of filtering of ROIs in step one (foreground vs background
    regions based on objectness score), there are still a large number of regions
    used for predictions in step two. Generating predictions for such a large number
    of proposed regions ensures good coverage for various objects in the image. Yet,
    there are a number of regions with good amount of overlap for the same region.
    For example, look at the 6 bounding boxes predicted for the same object in figure
    6(a). This potentially can lead to difficulty in getting the exact count of different
    objects in the input image.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，物体检测模型在第一步提出了大量的ROI（感兴趣区域），然后在第二步进行边界框和类别预测。虽然在第一步中对ROI进行了某种程度的过滤（基于物体得分区分前景与背景区域），但在第二步中仍然有大量区域用于预测。为如此大量的提议区域生成预测确保了对图像中各种物体的良好覆盖。然而，也有一些区域对同一物体存在大量重叠。例如，看看图6(a)中为同一个物体预测的6个边界框。这可能会导致难以准确统计输入图像中不同物体的数量。
- en: '![](../Images/595c3830c4ee85b07a92cc6afacc8f42.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/595c3830c4ee85b07a92cc6afacc8f42.png)'
- en: 'Figure 6 (a)Object detection model generating 6 bounding boxes with good overlap
    for the same object. (b) Output optimized using NMS. Source: Author'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6 (a) 物体检测模型为同一物体生成6个重叠较多的边界框。 (b) 使用NMS优化后的输出。来源：作者
- en: Hence, there is a third step in this framework which concerns the optimization
    of the output. This optimization step ensures there is only one bounding box and
    class prediction per object in the input image. There are different ways of performing
    this optimization. By far, the most popular method is called **Non-Maximum Suppression
    (NMS)**. As the name suggests, NMS analyzes all bounding boxes for each object
    to find the one with maximum probability and suppress the rest of them (see figure
    6(b) for optimized output after applying NMS).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个框架中有第三个步骤，涉及输出的优化。这个优化步骤确保每个输入图像中的每个物体只有一个边界框和类别预测。进行这种优化的方法有多种。目前，最流行的方法被称为**非极大值抑制（NMS）**。顾名思义，NMS会分析每个物体的所有边界框，找到具有最大概率的那个，并抑制其余的边界框（请参见图6(b)，展示了应用NMS后的优化输出）。
- en: This concludes a high-level understanding of a general object detection framework.
    We discussed about the three major steps involved in localization and classification
    of objects in a given image. In this next article we will use this understanding
    to understand specific implementations and their key contributions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对一般物体检测框架的高层次理解。我们讨论了定位和分类图像中物体的三个主要步骤。在接下来的文章中，我们将基于这些理解，探讨具体的实现方法及其关键贡献。
