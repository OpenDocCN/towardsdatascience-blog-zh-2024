- en: LLMs Pitfalls
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的陷阱
- en: 原文：[https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07](https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07](https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07)
- en: An introduction to some of the key components surrounding LLMs to produce production-grade
    applications
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍一些围绕LLMs的关键组成部分，以生成生产级应用
- en: '[](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    ·9 min read·May 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    ·阅读时间9分钟·2024年5月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ccd96f625ecac5b4f9c80a8b6fe5b2b9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccd96f625ecac5b4f9c80a8b6fe5b2b9.png)'
- en: AI Generated (Image by Author).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成（图像由作者提供）。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Since the rise of ChatGPT, Large Language Models (LLMs) have become more and
    more popular also for non-technical people. Although LLMs on their own cannot
    provide yet a full product ready to be served to a vast audience. As part of this
    article, we will cover some of the key elements that are used to make LLMs production-ready.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自从ChatGPT的崛起以来，大型语言模型（LLMs）变得越来越受欢迎，甚至非技术人员也开始关注它们。尽管LLMs本身尚无法提供一个完整的、可供广大受众使用的产品，但在本文中，我们将探讨一些用于使LLMs具备生产就绪能力的关键元素。
- en: Fine-tuning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Datasets
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'Models like LLAMA are able to predict next tokens in a sequence although this
    doesn’t necessarily make them suited for tasks such as question answering. Therefore
    in order to optimize these models different types of datasets can be used:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像LLAMA这样的模型能够预测序列中的下一个标记，尽管这并不一定使它们适用于诸如问答等任务。因此，为了优化这些模型，可以使用不同类型的数据集：
- en: '**Raw completion**: if the goal is predicting the next token we provide some
    input text and let the model progressively predict the upcoming steps.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始完成**：如果目标是预测下一个标记，我们提供一些输入文本，让模型逐步预测接下来的步骤。'
- en: '**Fill in the middle objective**: in this case we have some starting and ending
    text and the model is learning to fill the gap. This approach is quite popular
    to create code completion models like Codex.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充中间目标**：在这种情况下，我们有一些起始和结束文本，模型正在学习填补空白。这种方法在创建代码补全模型（如Codex）时非常流行。'
- en: '**Instruction datasets**: the goal here is to teach the model how to answer
    questions. We have questions (instructions) as…'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令数据集**：这里的目标是教模型如何回答问题。我们有问题（指令）作为……'
