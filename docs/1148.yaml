- en: LLMs Pitfalls
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07](https://towardsdatascience.com/llms-pitfalls-7a33de009638?source=collection_archive---------6-----------------------#2024-05-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introduction to some of the key components surrounding LLMs to produce production-grade
    applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page---byline--7a33de009638--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7a33de009638--------------------------------)
    ·9 min read·May 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ccd96f625ecac5b4f9c80a8b6fe5b2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: AI Generated (Image by Author).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the rise of ChatGPT, Large Language Models (LLMs) have become more and
    more popular also for non-technical people. Although LLMs on their own cannot
    provide yet a full product ready to be served to a vast audience. As part of this
    article, we will cover some of the key elements that are used to make LLMs production-ready.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Models like LLAMA are able to predict next tokens in a sequence although this
    doesn’t necessarily make them suited for tasks such as question answering. Therefore
    in order to optimize these models different types of datasets can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Raw completion**: if the goal is predicting the next token we provide some
    input text and let the model progressively predict the upcoming steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fill in the middle objective**: in this case we have some starting and ending
    text and the model is learning to fill the gap. This approach is quite popular
    to create code completion models like Codex.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction datasets**: the goal here is to teach the model how to answer
    questions. We have questions (instructions) as…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
