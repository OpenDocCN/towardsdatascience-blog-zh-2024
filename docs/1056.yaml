- en: Robust One-Hot Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/robust-one-hot-encoding-930b5f8943af?source=collection_archive---------4-----------------------#2024-04-26](https://towardsdatascience.com/robust-one-hot-encoding-930b5f8943af?source=collection_archive---------4-----------------------#2024-04-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Production grade one-hot encoding techniques in Python and R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hc.ekne?source=post_page---byline--930b5f8943af--------------------------------)[![Hans
    Christian Ekne](../Images/c85483d8b5dd89584b996b321b7f4a45.png)](https://medium.com/@hc.ekne?source=post_page---byline--930b5f8943af--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--930b5f8943af--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--930b5f8943af--------------------------------)
    [Hans Christian Ekne](https://medium.com/@hc.ekne?source=post_page---byline--930b5f8943af--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--930b5f8943af--------------------------------)
    ·11 min read·Apr 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3b472814a17418f0804ebfbcc8a1e9db.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E / or Dali?;)
  prefs: []
  type: TYPE_NORMAL
- en: Have you faced a crash in your machine learning production environments?
  prefs: []
  type: TYPE_NORMAL
- en: It’s not fun, and especially when it comes to issues that could be avoided.
    One issue that frequently causes problems is one-hot encoding of data. Drawing
    from my own experience, I’ve learned that many of these issues can largely be
    avoided by following a few best practices related to one-hot encoding. In this
    article I will briefly introduce the topic with a few simple examples and share
    some best practices to ensure stability of your machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is one-hot encoding?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One-hot encoding is the practice of turning a factor variable that is stored
    in a column into dummy variables stored over multiple columns and represented
    as 0s and 1s. A simple example illustrates the concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider for example this dataset with some numbers and some columns for colours:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Or more visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de251ce1de486b95f24cf64ab9c324bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Training data with 3 columns / image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The column `color_1_`could also be represented like in the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17f1d9b0fd2a6e5438855cd8aa8dbabf.png)'
  prefs: []
  type: TYPE_IMG
- en: One-hot encoded representation of “color_1_” / image by author
  prefs: []
  type: TYPE_NORMAL
- en: Changing `color_1_` from a one-column compact representation of a categorical
    variable into a multi-column binary representation is what we call one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we use it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple reasons to use one-hot encoding. They could be related to
    avoiding implicit ordering, improving model performance, or just making the data
    compatible with various algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you encode a categorical variable like colour, into a numerical
    structure, (e.g. 1 for black, 2 for green, 3 for red) without converting it to
    dummy variables, a model could mistakenly misinterpret the data to imply an order
    ( black < green < red) when no such order exists.
  prefs: []
  type: TYPE_NORMAL
- en: Also, when training neural nets, it is best practice to normalize the data before
    sending it into the neural net, and with categorical variables, one-hot encoding
    can be a good method. Other linear models, like logistic and linear regression
    assume linear relationships and numerical inputs so for this class of models,
    one-hot encoding can be a good idea as well.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the process of doing one-hot encoding forces us to ensure we don’t
    feed unseen factor levels into our machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, one-hot encoding makes it easier for the machine learning models
    to interpret the data and thus make better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The main reasons why one-hot encoding fails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way we build traditional machine learning models is to first train the models
    on a “training dataset” — typically a dataset of historic values — and then later
    we generate predictions on a new dataset, the “inference dataset.” If the columns
    of the training dataset and the inference dataset don’t match, your machine learning
    algorithm will usually fail. This is primarily due to either missing or new factor
    levels in the inference dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first problem: Missing factors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the following examples, assume that you used the dataset above to train
    your machine learning model. You one-hot encoded the dataset into dummy variables,
    and your fully transformed training data looks like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3838878fe8c718db9f2226fa542dc159.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed training dataset with pd.get_dummies / image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s introduce the inference dataset, this is what you would use for
    making predictions. Let’s say it is given like below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9647cc2c91077b979ff0ab9f045c0c76.png)'
  prefs: []
  type: TYPE_IMG
- en: Inference data with 3 columns / image by author
  prefs: []
  type: TYPE_NORMAL
- en: Using a naive one-hot encoding strategy like we used above (`pd.get_dummies`)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This would transform your inference dataset in the same way, and you obtain
    the dataset below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/beb8f6bd286cd54bd9ff34350c8cefda.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed inference dataset with pd.get_dummies / image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you notice the problems? The first problem is that the inference dataset
    is missing the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you ran this in a model trained with the “training dataset” it would usually
    crash.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second problem: New factors'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The other problem that can occur with one-hot encoding is if your inference
    dataset includes new and unseen factors. Consider again the same datasets as above.
    If you examine closely, you see that the inference dataset now has a new column:
    `color_2__orange.`'
  prefs: []
  type: TYPE_NORMAL
- en: This is the opposite problem as previously, and our inference dataset contains
    new columns which our training dataset didn’t have. This is actually a common
    occurrence and can happen if one of your factor variables had changes. For example,
    if the colours above represent colours of a car, and a car producer suddenly started
    making orange cars, then this data might not be available in the training data,
    but could nonetheless show up in the inference data. In this case you need a robust
    way of dealing with the issue.
  prefs: []
  type: TYPE_NORMAL
- en: One could argue, well why don’t you list all the columns in the transformed
    training dataset as columns that would be needed for your inference dataset? The
    problem here is that you often don’t know what factor levels are in the training
    data upfront.
  prefs: []
  type: TYPE_NORMAL
- en: For example, new levels could be introduced regularly, which could make it difficult
    to maintain. On top of that comes the process of then matching your inference
    dataset with the training data, so you would need to check all actual transformed
    column names that went into the training algorithm, and then match them with the
    transformed inference dataset. If any columns were missing you would need to insert
    new columns with 0 values and if you had extra columns, like the `color_2__orange`
    columns above, those would need to be deleted. This is a rather cumbersome way
    of solving the issue, and thankfully there are better options available.
  prefs: []
  type: TYPE_NORMAL
- en: The solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The solution to this problem is rather straightforward, however many of the
    packages and libraries that attempt to streamline the process of creating prediction
    models fail to implement it well. The key lies in having a function or class that
    is first fitted on the training data, and then use that same instance of the function
    or class to transform both the training dataset and the inference dataset. Below
    we explore how this is done using both Python and R.
  prefs: []
  type: TYPE_NORMAL
- en: In Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python is arguably one the best programming language to use for machine learning,
    largely due to its extensive network of developers and mature package libraries,
    and its ease of use, which promotes rapid development.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding the issues related to one-hot encoding we described above, they can
    be mitigated by using the widely available and tested scikit-learn library, and
    more specifically the `sklearn.preprocessing.OneHotEncoder` class. So, let’s see
    how we can use that on our training and inference datasets to create a robust
    one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a final `DataFrame`of transformed values as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f305e941d89808c5f5ac9eb6977727f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed training dataset with sklearn / image by author
  prefs: []
  type: TYPE_NORMAL
- en: If we break down the code above, we see that the first step is to initialize
    the an instance of the encoder class. We use the option `handle_unknown='ignore'`
    so that we avoid issues with unknow values for the columns when we use the encoder
    to transform on our inference dataset.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we combine a fit and transform action into one step with the `fit_transform`
    method. And finally, we create a new data frame from the encoded data and concatenate
    it with the rest of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now the task remains to use the encoder to transform our inference dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Unlike earlier, when we used the naive `pandas.get_dummies` ,we now see that
    our new `final_inference_df` dataset has the same columns as our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd25ea429be35479ec782fb89827e12e.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed Inference dataset with the correct columns / image by author
  prefs: []
  type: TYPE_NORMAL
- en: In addition to what we showed in the code above, the `OneHotEncoder` class from
    `sklearn.preprocessing` has a lot of other functionality that can be useful as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: For example, it allows you set the `min_frequency` and `max_categories` options.
    As its name implies the `min_frequency` options allow you to specify the minimum
    frequency below which a category will be considered infrequent and then grouped
    together with other infrequent categories, or the `max_categories` option which
    limits the total number of categories. The latter can be especially useful if
    you don’t want to create too many columns in your training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a full overview of the functionality, visit the documentation pages here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?source=post_page-----930b5f8943af--------------------------------#sklearn.preprocessing.OneHotEncoder)
    [## sklearn.preprocessing.OneHotEncoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples using sklearn.preprocessing.OneHotEncoder: Release Highlights for
    scikit-learn 1.4 Release Highlights for…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: scikit-learn.org](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?source=post_page-----930b5f8943af--------------------------------#sklearn.preprocessing.OneHotEncoder)
  prefs: []
  type: TYPE_NORMAL
- en: In R
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Several of my clients use R for running machine learning models in production
    — and it has a lot of great features. Before `polars` came out for Python, R’s
    `data.table` package was superior to what `pandas` could offer in terms of speed
    and efficiency. However, R doesn’t have access to the same type of production
    level packages as `scikit-learn` for python. (There are a few libraries, but they
    are not as mature as `scikit-learn`.) In addition, while some packages might have
    the required functionality, they require loads of other packages to run and can
    introduce dependency conflicts into your code. Consider running the line below
    in a docker container build with the r-base image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes forever to install and takes up a lot of space on your container image.
    Our solution in this case — instead of using functions from a pre-built package
    like `recipes` — is to introduce our own simple function implemented using the
    `data.table` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let’s go through this function and see how it works on our training and inference
    datasets. (R is slightly different from Python and instead of using a class, we
    use a parent function instead, which works in a similar way.)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to create an instance of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Then, just like with the `OneHotEncoder` class from sklearn.preprocessing, we
    also have a fit function inside our `OneHotEncoder`. We use the fit function on
    the training data, supplying both the training dataset and the columns we want
    to one-hot encode.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The fit function simply loops through all the columns we want to use to for
    training and finds all the unique values each of the columns contain. This list
    of columns and their potential values is then used in the transform function.
    We now have a instance of a fitted one-hot encoder function and we can save it
    for later use using a R `.RDS` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the one-hot encoded dataset we need for training, we run the transform
    function on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The transform function is a little bit more complicated than the fit function,
    and the first thing it does is to convert the supplied columns into factors —
    using the original unique values of the columns as factor levels. Then, we loop
    through each of the predictor columns and create `model.matrix` objects of the
    data. These are then added back to the original dataset and the original factor
    column is removed. We also make sure to set any of the missing values to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now get the exact same dataset as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f305e941d89808c5f5ac9eb6977727f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed training dataset using R algorithm / image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, when we need to one-hot encode our inference dataset, we then
    run the same instance of the encoder function on that dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This process ensures we have the same columns in our `transformed_inference_data`
    as we do in our `transformed_training_data`.
  prefs: []
  type: TYPE_NORMAL
- en: Further considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we conclude there are a few extra considerations to mention. As with
    many other things in machine learning there isn’t always an easy answer as to
    when and how to use a specific technique. Even though it clearly mitigates some
    issues, new problems can also arise when doing one-hot encoding. Most commonly,
    these are related to how to deal with high cardinality categorical variables and
    how to deal with memory issues because of increasing the table size.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there are alternative coding techniques such as label encoding,
    embeddings, or target encodings which sometimes could be preferable to one-hot
    encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these topics is rich enough to warrant a dedicated article, so we will
    leave those for the interested reader to explore further.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have shown how naive use of one-hot encoding techniques can lead to mistakes
    and problems with inference data, and we have also seen how to mitigate and resolve
    those issues using both Python and R. If left unresolved, poor management of one-hot
    encoding can potentially lead to crashes and problems with your inference, so
    it is strongly recommended to use more robust techniques—like either sklearn’s
    `OneHotEncoder` or the R function we developed.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*All the code presented and used in the article can be found in the following
    Github repo:* [*https://github.com/hcekne/robust_one_hot_encoding*](https://github.com/hcekne/robust_one_hot_encoding)'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed reading this article and would like to access more content
    from me please feel free to connect with me on LinkedIn at* [*https://www.linkedin.com/in/hans-christian-ekne-1760a259/*](https://www.linkedin.com/in/hans-christian-ekne-1760a259/)
    *or visit my webpage at* [*https://www.ekneconsulting.com/*](https://www.ekneconsulting.com/)
    *to explore some of the services I offer. Don’t hesitate to reach out via email
    at hce@ekneconsulting.com*'
  prefs: []
  type: TYPE_NORMAL
