<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12">https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b401" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="864a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx"><strong class="al">Bell-shaped assumptions for better predictions</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Oct 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3461eb23bbba050007a17f806c580568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVAzQ4TyQJ3bGY2YuXd9bw.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">â›³ï¸ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained: <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a> <br/> â–¶ <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="7b15" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building on our previous article about <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a>, which handles binary data, we now explore Gaussian Naive Bayes for continuous data. Unlike the binary approach, this algorithm assumes each feature follows a normal (Gaussian) distribution.</p><p id="1699" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, weâ€™ll see how Gaussian Naive Bayes handles continuous, bell-shaped data â€” ringing in accurate predictions â€” all <strong class="ne ga">without getting into the intricate math</strong> of Bayesâ€™ Theorem.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c31e0cb8eff429b558a8a46174d03a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqvZ91E05rBiWmD7XwDRLA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="1565" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="afdf" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like other Naive Bayes variants, Gaussian Naive Bayes makes the â€œnaiveâ€ assumption of feature independence. It assumes that the features are conditionally independent given the class label.</p><p id="b462" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, while Bernoulli Naive Bayes is suited for datasets with binary features, Gaussian Naive Bayes assumes that the features follow <strong class="ne ga">a continuous normal (Gaussian)</strong> distribution. Although this assumption may not always hold true in reality, it simplifies the calculations and often leads to surprisingly accurate results.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/0d0e6eec6460b4247cf38d8dc3c8600e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHCHYTdBGEzZ2mBNllC_7A.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Bernoulli NB assumes binary data, Multinomial NB works with discrete counts, and Gaussian NB handles continuous data assuming a normal distribution.</figcaption></figure><h1 id="94e5" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Dataset Used</h1><p id="f27d" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, weâ€™ll use this artificial golf dataset (made by author) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/fc1873cedde634b5e602a6129e33ac2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtAv4irP3epwAVrc49pYtw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: â€˜RainfallAmountâ€™ (in mm), â€˜Temperatureâ€™ (in Celcius), â€˜Humidityâ€™ (in %), â€˜WindSpeedâ€™ (in km/h) and â€˜Playâ€™ (Yes/No, target feature)</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="fb12" class="pm oj fq ob b bg pn po l pp pq"># IMPORTING DATASET #<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np<br/><br/>dataset_dict = {<br/>    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],<br/>    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Set feature matrix X and target vector y<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split the data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/>print(pd.concat([X_train, y_train], axis=1), end='\n\n')<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="ce84" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="7df5" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Gaussian Naive Bayes works with continuous data, assuming each feature follows a Gaussian (normal) distribution.</p><ol class=""><li id="91f7" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Calculate the probability of each class in the training data.</li><li id="c0ce" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">For each feature and class, estimate the mean and variance of the feature values within that class.</li><li id="c52a" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">For a new instance:<br/>a. For each class, calculate the probability density function (PDF) of each feature value under the Gaussian distribution of that feature within the class.<br/>b. Multiply the class probability by the product of the PDF values for all features.</li><li id="d86b" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Predict the class with the highest resulting probability.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a090060bd91a5d5c69880a4926db7175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ypt7TIwcxydGBx5iNx3uSA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Gaussian Naive Bayes uses the normal distribution to model the likelihood of different feature values for each class. It then combines these likelihoods to make a prediction.</figcaption></figure><h2 id="c19a" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Transforming non-Gaussian distributed data</h2><p id="fa8c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Remember that this algorithm naively assume that all the input features are having Gaussian/normal distribution?</p><p id="59d2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we are not really sure about the distribution of our data, especially for features that clearly donâ€™t follow a Gaussian distribution, applying a <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">power transformation </a>(like Box-Cox) before using Gaussian Naive Bayes can be beneficial. This approach can help make the data more Gaussian-like, which aligns better with the assumptions of the algorithm.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/90d0627b51b81e1b87930635d3497adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bkpHzX0b4Df3L3GvyjayZw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All columns are scaled using Power Transformation (Box-Cox Transformation) and then standardized.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="2e03" class="pm oj fq ob b bg pn po l pp pq">from sklearn.preprocessing import PowerTransformer<br/><br/># Initialize and fit the PowerTransformer<br/>pt = PowerTransformer(standardize=True) # Standard Scaling already included<br/>X_train_transformed = pt.fit_transform(X_train)<br/>X_test_transformed = pt.transform(X_test)</span></pre><p id="56a3" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we are ready for the training.</p><h1 id="20bb" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="2ae1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">1.<strong class="ne ga"> Class Probability Calculation</strong>: For each class, calculate its probability: (Number of instances in this class) / (Total number of instances)</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c06266178b47854c1548da3d97e124f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izvUBkz0lQ2CsyfRpoWbjw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="4e2d" class="pm oj fq ob b bg pn po l pp pq">from fractions import Fraction<br/><br/>def calc_target_prob(attr):<br/>    total_counts = attr.value_counts().sum()<br/>    prob_series = attr.value_counts().apply(lambda x: Fraction(x, total_counts).limit_denominator())<br/>    return prob_series<br/><br/>print(calc_target_prob(y_train))</span></pre><p id="dab4" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Feature Probability Calculation</strong> : For each feature and each class, calculate the mean (Î¼) and standard deviation (Ïƒ) of the feature values within that class using the training data. Then, calculate the probability using Gaussian Probability Density Function (PDF) formula.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/b1e80a1082f80606387c89d90f62303c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kg8lJauvIY-0YSGy6vQzHA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For each weather condition, determine the mean and standard deviation for both â€œYESâ€ and â€œNOâ€ instances. Then calculate their PDF using the PDF formula for normal/Gaussian distribution.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a40cfb56adeb49c44ff0899ca2e3dbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ooy5JExmb5DwfUSt6s7hmA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The same process is applied to all of the other features.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="d500" class="pm oj fq ob b bg pn po l pp pq">def calculate_class_probabilities(X_train_transformed, y_train, feature_names):<br/>    classes = y_train.unique()<br/>    equations = pd.DataFrame(index=classes, columns=feature_names)<br/><br/>    for cls in classes:<br/>        X_class = X_train_transformed[y_train == cls]<br/>        mean = X_class.mean(axis=0)<br/>        std = X_class.std(axis=0)<br/>        k1 = 1 / (std * np.sqrt(2 * np.pi))<br/>        k2 = 2 * (std ** 2)<br/><br/>        for i, column in enumerate(feature_names):<br/>            equation = f"{k1[i]:.3f}Â·exp(-(x-({mean[i]:.2f}))Â²/{k2[i]:.3f})"<br/>            equations.loc[cls, column] = equation<br/><br/>    return equations<br/><br/># Use the function with the transformed training data<br/>equation_table = calculate_class_probabilities(X_train_transformed, y_train, X.columns)<br/><br/># Display the equation table<br/>print(equation_table)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qp"><img src="../Images/9b65e3ad3d83feb7d761c151b3593320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5IqP1qhM-SiktbZH1jpIg.png"/></div></div></figure><p id="0379" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Smoothing</strong>: Gaussian Naive Bayes uses a unique smoothing approach. Unlike Laplace smoothing <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">in other variants</a>, it adds a tiny value (0.000000001 times the largest variance) to all variances. This prevents numerical instability from division by zero or very small numbers.</p><h1 id="9034" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Prediction/Classification Step</h1><p id="4886" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Given a new instance with continuous features:</p><p id="98bf" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">1. <strong class="ne ga">Probability Collection</strong>: <br/>For each possible class:<br/> Â· Start with the probability of this class occurring (class probability).<br/> Â· For each feature in the new instance, calculate the probability density function of that feature within the class.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/bf29443203d3ddc521fc69caf88fa8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SdhN0dtAbTmG4LN9iosnw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For ID 14, we calculate the PDF each of the feature for both â€œYESâ€ and â€œNOâ€ instances.</figcaption></figure><p id="f7e4" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Score Calculation &amp; Prediction</strong>: <br/>For each class:<br/> Â· Multiply all the collected PDF values together.<br/> Â· The result is the score for this class.<br/> Â· The class with the highest score is the prediction.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/8c0e57938597cec51337f265f8a137f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*td_42U5Goyj33ng2iUK0QA.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="0d1e" class="pm oj fq ob b bg pn po l pp pq">from scipy.stats import norm<br/><br/>def calculate_class_probability_products(X_train_transformed, y_train, X_new, feature_names, target_name):<br/>    classes = y_train.unique()<br/>    n_features = X_train_transformed.shape[1]<br/>    <br/>    # Create column names using actual feature names<br/>    column_names = [target_name] + list(feature_names) + ['Product']<br/>    <br/>    probability_products = pd.DataFrame(index=classes, columns=column_names)<br/>    <br/>    for cls in classes:<br/>        X_class = X_train_transformed[y_train == cls]<br/>        mean = X_class.mean(axis=0)<br/>        std = X_class.std(axis=0)<br/>        <br/>        prior_prob = np.mean(y_train == cls)<br/>        probability_products.loc[cls, target_name] = prior_prob<br/>        <br/>        feature_probs = []<br/>        for i, feature in enumerate(feature_names):<br/>            prob = norm.pdf(X_new[0, i], mean[i], std[i])<br/>            probability_products.loc[cls, feature] = prob<br/>            feature_probs.append(prob)<br/>        <br/>        product = prior_prob * np.prod(feature_probs)<br/>        probability_products.loc[cls, 'Product'] = product<br/><br/>    return probability_products<br/><br/># Assuming X_new is your new sample reshaped to (1, n_features)<br/>X_new = np.array([-1.28, 1.115, 0.84, 0.68]).reshape(1, -1)<br/><br/># Calculate probability products<br/>prob_products = calculate_class_probability_products(X_train_transformed, y_train, X_new, X.columns, y.name)<br/><br/># Display the probability product table<br/>print(prob_products)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qq"><img src="../Images/9c97ff87d73e99807c7113e900035931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZy-MvjJRPjiyn2q1S6-ag.png"/></div></div></figure><h1 id="cfc9" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Evaluation Step</h1><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/5a17bec3594c45626bd3222026fed233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRjZKLPHCxbHdQ7sjiDQtw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For this particular dataset, this accuracy is considered quite good.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="8679" class="pm oj fq ob b bg pn po l pp pq">from sklearn.naive_bayes import GaussianNB<br/>from sklearn.metrics import accuracy_score<br/><br/># Initialize and train the Gaussian Naive Bayes model<br/>gnb = GaussianNB()<br/>gnb.fit(X_train_transformed, y_train)<br/><br/># Make predictions on the test set<br/>y_pred = gnb.predict(X_test_transformed)<br/><br/># Calculate the accuracy<br/>accuracy = accuracy_score(y_test, y_pred)<br/><br/># Print the accuracy<br/>print(f"Accuracy: {accuracy:.4f}")</span></pre><h1 id="d5c0" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="1aac" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">GaussianNB is known for its simplicity and effectiveness. The main thing to remember about its parameters is:</p><ol class=""><li id="2dd8" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk"><strong class="ne ga">priors</strong>: This is the most notable parameter, <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">similar to Bernoulli Naive Bayes</a>. In most cases, you donâ€™t need to set it manually. By default, itâ€™s calculated from your training data, which often works well.</li><li id="51bb" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">var_smoothing</strong>: This is a stability parameter that you rarely need to adjust. (the default is 0.000000001)</li></ol><p id="e5dc" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The key takeaway is that this algoritm is designed to work well out-of-the-box. In most situations, you can use it without worrying about parameter tuning.</p><h1 id="5302" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><h2 id="f296" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Pros:</h2><ol class=""><li id="6d44" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Simplicity</strong>: Maintains the easy-to-implement and understand trait.</li><li id="a0eb" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Efficiency</strong>: Remains swift in training and prediction, making it suitable for large-scale applications with continuous features.</li><li id="19ab" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Flexibility with Data</strong>: Handles both small and large datasets well, adapting to the scale of the problem at hand.</li><li id="3c19" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Continuous Feature Handling</strong>: Thrives with continuous and real-valued features, making it ideal for tasks like predicting real-valued outputs or working with data where features vary on a continuum.</li></ol><h2 id="e990" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Cons:</h2><ol class=""><li id="66a4" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Independence Assumption</strong>: Still assumes that features are conditionally independent given the class, which might not hold in all real-world scenarios.</li><li id="051d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Gaussian Distribution Assumption</strong>: Works best when feature values truly follow a normal distribution. Non-normal distributions may lead to suboptimal performance (but can be fixed with Power Transformation weâ€™ve discussed)</li><li id="bc02" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Sensitivity to Outliers</strong>: Can be significantly affected by outliers in the training data, as they skew the mean and variance calculations.</li></ol><h1 id="aba6" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="39fd" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Gaussian Naive Bayes stands as an efficient classifier for a wide range of applications involving continuous data. Its ability to handle real-valued features extends its use beyond binary classification tasks, making it a go-to choice for numerous applications.</p><p id="44c2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While it makes some assumptions about data (feature independence and normal distribution), when these conditions are met, it gives robust performance, making it a favorite among both beginners and seasoned data scientists for its balance of simplicity and power.</p><h1 id="951e" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸŒŸ Gaussian Naive Bayes Simplified</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="ce3f" class="pm oj fq ob b bg pn po l pp pq">import pandas as pd<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.preprocessing import PowerTransformer<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],<br/>    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data for model<br/>X, y = df.drop('Play', axis=1), (df['Play'] == 'Yes').astype(int)<br/><br/># Split data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)<br/><br/># Apply PowerTransformer<br/>pt = PowerTransformer(standardize=True)<br/>X_train_transformed = pt.fit_transform(X_train)<br/>X_test_transformed = pt.transform(X_test)<br/><br/># Train the model<br/>nb_clf = GaussianNB()<br/>nb_clf.fit(X_train_transformed, y_train)<br/><br/># Make predictions<br/>y_pred = nb_clf.predict(X_test_transformed)<br/><br/># Check accuracy<br/>accuracy = accuracy_score(y_test, y_pred)<br/>print(f"Accuracy: {accuracy:.4f}")</span></pre></div></div></div><div class="ab cb qr qs qt qu" role="separator"><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="73a4" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Further Reading</h2><p id="7637" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank">GaussianNB</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="82e1" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Technical Environment</h2><p id="bfe1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="c0b4" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">About the Illustrations</h2><p id="ef4f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="2481" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>