<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Gaussian Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12">https://towardsdatascience.com/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c?source=collection_archive---------2-----------------------#2024-10-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b401" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="864a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx"><strong class="al">Bell-shaped assumptions for better predictions</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--04949cef383c--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3461eb23bbba050007a17f806c580568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVAzQ4TyQJ3bGY2YuXd9bw.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">⛳️ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained: <br/> · <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a> <br/> ▶ <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="7b15" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building on our previous article about <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a>, which handles binary data, we now explore Gaussian Naive Bayes for continuous data. Unlike the binary approach, this algorithm assumes each feature follows a normal (Gaussian) distribution.</p><p id="1699" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, we’ll see how Gaussian Naive Bayes handles continuous, bell-shaped data — ringing in accurate predictions — all <strong class="ne ga">without getting into the intricate math</strong> of Bayes’ Theorem.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c31e0cb8eff429b558a8a46174d03a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqvZ91E05rBiWmD7XwDRLA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="1565" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="afdf" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like other Naive Bayes variants, Gaussian Naive Bayes makes the “naive” assumption of feature independence. It assumes that the features are conditionally independent given the class label.</p><p id="b462" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, while Bernoulli Naive Bayes is suited for datasets with binary features, Gaussian Naive Bayes assumes that the features follow <strong class="ne ga">a continuous normal (Gaussian)</strong> distribution. Although this assumption may not always hold true in reality, it simplifies the calculations and often leads to surprisingly accurate results.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/0d0e6eec6460b4247cf38d8dc3c8600e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHCHYTdBGEzZ2mBNllC_7A.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Bernoulli NB assumes binary data, Multinomial NB works with discrete counts, and Gaussian NB handles continuous data assuming a normal distribution.</figcaption></figure><h1 id="94e5" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Dataset Used</h1><p id="f27d" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, we’ll use this artificial golf dataset (made by author) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/fc1873cedde634b5e602a6129e33ac2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtAv4irP3epwAVrc49pYtw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: ‘RainfallAmount’ (in mm), ‘Temperature’ (in Celcius), ‘Humidity’ (in %), ‘WindSpeed’ (in km/h) and ‘Play’ (Yes/No, target feature)</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="fb12" class="pm oj fq ob b bg pn po l pp pq"># IMPORTING DATASET #<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>import pandas as pd<br/>import numpy as np<br/><br/>dataset_dict = {<br/>    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],<br/>    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Set feature matrix X and target vector y<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split the data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/>print(pd.concat([X_train, y_train], axis=1), end='\n\n')<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="ce84" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="7df5" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Gaussian Naive Bayes works with continuous data, assuming each feature follows a Gaussian (normal) distribution.</p><ol class=""><li id="91f7" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Calculate the probability of each class in the training data.</li><li id="c0ce" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">For each feature and class, estimate the mean and variance of the feature values within that class.</li><li id="c52a" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">For a new instance:<br/>a. For each class, calculate the probability density function (PDF) of each feature value under the Gaussian distribution of that feature within the class.<br/>b. Multiply the class probability by the product of the PDF values for all features.</li><li id="d86b" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Predict the class with the highest resulting probability.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a090060bd91a5d5c69880a4926db7175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ypt7TIwcxydGBx5iNx3uSA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Gaussian Naive Bayes uses the normal distribution to model the likelihood of different feature values for each class. It then combines these likelihoods to make a prediction.</figcaption></figure><h2 id="c19a" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Transforming non-Gaussian distributed data</h2><p id="fa8c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Remember that this algorithm naively assume that all the input features are having Gaussian/normal distribution?</p><p id="59d2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we are not really sure about the distribution of our data, especially for features that clearly don’t follow a Gaussian distribution, applying a <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">power transformation </a>(like Box-Cox) before using Gaussian Naive Bayes can be beneficial. This approach can help make the data more Gaussian-like, which aligns better with the assumptions of the algorithm.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/90d0627b51b81e1b87930635d3497adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bkpHzX0b4Df3L3GvyjayZw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All columns are scaled using Power Transformation (Box-Cox Transformation) and then standardized.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="2e03" class="pm oj fq ob b bg pn po l pp pq">from sklearn.preprocessing import PowerTransformer<br/><br/># Initialize and fit the PowerTransformer<br/>pt = PowerTransformer(standardize=True) # Standard Scaling already included<br/>X_train_transformed = pt.fit_transform(X_train)<br/>X_test_transformed = pt.transform(X_test)</span></pre><p id="56a3" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we are ready for the training.</p><h1 id="20bb" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="2ae1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">1.<strong class="ne ga"> Class Probability Calculation</strong>: For each class, calculate its probability: (Number of instances in this class) / (Total number of instances)</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c06266178b47854c1548da3d97e124f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izvUBkz0lQ2CsyfRpoWbjw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="4e2d" class="pm oj fq ob b bg pn po l pp pq">from fractions import Fraction<br/><br/>def calc_target_prob(attr):<br/>    total_counts = attr.value_counts().sum()<br/>    prob_series = attr.value_counts().apply(lambda x: Fraction(x, total_counts).limit_denominator())<br/>    return prob_series<br/><br/>print(calc_target_prob(y_train))</span></pre><p id="dab4" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Feature Probability Calculation</strong> : For each feature and each class, calculate the mean (μ) and standard deviation (σ) of the feature values within that class using the training data. Then, calculate the probability using Gaussian Probability Density Function (PDF) formula.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/b1e80a1082f80606387c89d90f62303c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kg8lJauvIY-0YSGy6vQzHA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For each weather condition, determine the mean and standard deviation for both “YES” and “NO” instances. Then calculate their PDF using the PDF formula for normal/Gaussian distribution.</figcaption></figure><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a40cfb56adeb49c44ff0899ca2e3dbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ooy5JExmb5DwfUSt6s7hmA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">The same process is applied to all of the other features.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="d500" class="pm oj fq ob b bg pn po l pp pq">def calculate_class_probabilities(X_train_transformed, y_train, feature_names):<br/>    classes = y_train.unique()<br/>    equations = pd.DataFrame(index=classes, columns=feature_names)<br/><br/>    for cls in classes:<br/>        X_class = X_train_transformed[y_train == cls]<br/>        mean = X_class.mean(axis=0)<br/>        std = X_class.std(axis=0)<br/>        k1 = 1 / (std * np.sqrt(2 * np.pi))<br/>        k2 = 2 * (std ** 2)<br/><br/>        for i, column in enumerate(feature_names):<br/>            equation = f"{k1[i]:.3f}·exp(-(x-({mean[i]:.2f}))²/{k2[i]:.3f})"<br/>            equations.loc[cls, column] = equation<br/><br/>    return equations<br/><br/># Use the function with the transformed training data<br/>equation_table = calculate_class_probabilities(X_train_transformed, y_train, X.columns)<br/><br/># Display the equation table<br/>print(equation_table)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qp"><img src="../Images/9b65e3ad3d83feb7d761c151b3593320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5IqP1qhM-SiktbZH1jpIg.png"/></div></div></figure><p id="0379" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. <strong class="ne ga">Smoothing</strong>: Gaussian Naive Bayes uses a unique smoothing approach. Unlike Laplace smoothing <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">in other variants</a>, it adds a tiny value (0.000000001 times the largest variance) to all variances. This prevents numerical instability from division by zero or very small numbers.</p><h1 id="9034" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Prediction/Classification Step</h1><p id="4886" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Given a new instance with continuous features:</p><p id="98bf" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">1. <strong class="ne ga">Probability Collection</strong>: <br/>For each possible class:<br/> · Start with the probability of this class occurring (class probability).<br/> · For each feature in the new instance, calculate the probability density function of that feature within the class.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/bf29443203d3ddc521fc69caf88fa8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SdhN0dtAbTmG4LN9iosnw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For ID 14, we calculate the PDF each of the feature for both “YES” and “NO” instances.</figcaption></figure><p id="f7e4" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. <strong class="ne ga">Score Calculation &amp; Prediction</strong>: <br/>For each class:<br/> · Multiply all the collected PDF values together.<br/> · The result is the score for this class.<br/> · The class with the highest score is the prediction.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/8c0e57938597cec51337f265f8a137f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*td_42U5Goyj33ng2iUK0QA.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="0d1e" class="pm oj fq ob b bg pn po l pp pq">from scipy.stats import norm<br/><br/>def calculate_class_probability_products(X_train_transformed, y_train, X_new, feature_names, target_name):<br/>    classes = y_train.unique()<br/>    n_features = X_train_transformed.shape[1]<br/>    <br/>    # Create column names using actual feature names<br/>    column_names = [target_name] + list(feature_names) + ['Product']<br/>    <br/>    probability_products = pd.DataFrame(index=classes, columns=column_names)<br/>    <br/>    for cls in classes:<br/>        X_class = X_train_transformed[y_train == cls]<br/>        mean = X_class.mean(axis=0)<br/>        std = X_class.std(axis=0)<br/>        <br/>        prior_prob = np.mean(y_train == cls)<br/>        probability_products.loc[cls, target_name] = prior_prob<br/>        <br/>        feature_probs = []<br/>        for i, feature in enumerate(feature_names):<br/>            prob = norm.pdf(X_new[0, i], mean[i], std[i])<br/>            probability_products.loc[cls, feature] = prob<br/>            feature_probs.append(prob)<br/>        <br/>        product = prior_prob * np.prod(feature_probs)<br/>        probability_products.loc[cls, 'Product'] = product<br/><br/>    return probability_products<br/><br/># Assuming X_new is your new sample reshaped to (1, n_features)<br/>X_new = np.array([-1.28, 1.115, 0.84, 0.68]).reshape(1, -1)<br/><br/># Calculate probability products<br/>prob_products = calculate_class_probability_products(X_train_transformed, y_train, X_new, X.columns, y.name)<br/><br/># Display the probability product table<br/>print(prob_products)</span></pre><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp qq"><img src="../Images/9c97ff87d73e99807c7113e900035931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZy-MvjJRPjiyn2q1S6-ag.png"/></div></div></figure><h1 id="cfc9" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Evaluation Step</h1><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/5a17bec3594c45626bd3222026fed233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRjZKLPHCxbHdQ7sjiDQtw.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For this particular dataset, this accuracy is considered quite good.</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="8679" class="pm oj fq ob b bg pn po l pp pq">from sklearn.naive_bayes import GaussianNB<br/>from sklearn.metrics import accuracy_score<br/><br/># Initialize and train the Gaussian Naive Bayes model<br/>gnb = GaussianNB()<br/>gnb.fit(X_train_transformed, y_train)<br/><br/># Make predictions on the test set<br/>y_pred = gnb.predict(X_test_transformed)<br/><br/># Calculate the accuracy<br/>accuracy = accuracy_score(y_test, y_pred)<br/><br/># Print the accuracy<br/>print(f"Accuracy: {accuracy:.4f}")</span></pre><h1 id="d5c0" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="1aac" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">GaussianNB is known for its simplicity and effectiveness. The main thing to remember about its parameters is:</p><ol class=""><li id="2dd8" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk"><strong class="ne ga">priors</strong>: This is the most notable parameter, <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">similar to Bernoulli Naive Bayes</a>. In most cases, you don’t need to set it manually. By default, it’s calculated from your training data, which often works well.</li><li id="51bb" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">var_smoothing</strong>: This is a stability parameter that you rarely need to adjust. (the default is 0.000000001)</li></ol><p id="e5dc" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The key takeaway is that this algoritm is designed to work well out-of-the-box. In most situations, you can use it without worrying about parameter tuning.</p><h1 id="5302" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><h2 id="f296" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Pros:</h2><ol class=""><li id="6d44" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Simplicity</strong>: Maintains the easy-to-implement and understand trait.</li><li id="a0eb" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Efficiency</strong>: Remains swift in training and prediction, making it suitable for large-scale applications with continuous features.</li><li id="19ab" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Flexibility with Data</strong>: Handles both small and large datasets well, adapting to the scale of the problem at hand.</li><li id="3c19" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Continuous Feature Handling</strong>: Thrives with continuous and real-valued features, making it ideal for tasks like predicting real-valued outputs or working with data where features vary on a continuum.</li></ol><h2 id="e990" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Cons:</h2><ol class=""><li id="66a4" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Independence Assumption</strong>: Still assumes that features are conditionally independent given the class, which might not hold in all real-world scenarios.</li><li id="051d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Gaussian Distribution Assumption</strong>: Works best when feature values truly follow a normal distribution. Non-normal distributions may lead to suboptimal performance (but can be fixed with Power Transformation we’ve discussed)</li><li id="bc02" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Sensitivity to Outliers</strong>: Can be significantly affected by outliers in the training data, as they skew the mean and variance calculations.</li></ol><h1 id="aba6" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remarks</h1><p id="39fd" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Gaussian Naive Bayes stands as an efficient classifier for a wide range of applications involving continuous data. Its ability to handle real-valued features extends its use beyond binary classification tasks, making it a go-to choice for numerous applications.</p><p id="44c2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While it makes some assumptions about data (feature independence and normal distribution), when these conditions are met, it gives robust performance, making it a favorite among both beginners and seasoned data scientists for its balance of simplicity and power.</p><h1 id="951e" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Gaussian Naive Bayes Simplified</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="ce3f" class="pm oj fq ob b bg pn po l pp pq">import pandas as pd<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.preprocessing import PowerTransformer<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Rainfall': [0.0, 2.0, 7.0, 18.0, 3.0, 3.0, 0.0, 1.0, 0.0, 25.0, 0.0, 18.0, 9.0, 5.0, 0.0, 1.0, 7.0, 0.0, 0.0, 7.0, 5.0, 3.0, 0.0, 2.0, 0.0, 8.0, 4.0, 4.0],<br/>    'Temperature': [29.4, 26.7, 28.3, 21.1, 20.0, 18.3, 17.8, 22.2, 20.6, 23.9, 23.9, 22.2, 27.2, 21.7, 27.2, 23.3, 24.4, 25.6, 27.8, 19.4, 29.4, 22.8, 31.1, 25.0, 26.1, 26.7, 18.9, 28.9],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'WindSpeed': [2.1, 21.2, 1.5, 3.3, 2.0, 17.4, 14.9, 6.9, 2.7, 1.6, 30.3, 10.9, 3.0, 7.5, 10.3, 3.0, 3.9, 21.9, 2.6, 17.3, 9.6, 1.9, 16.0, 4.6, 3.2, 8.3, 3.2, 2.2],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data for model<br/>X, y = df.drop('Play', axis=1), (df['Play'] == 'Yes').astype(int)<br/><br/># Split data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, shuffle=False)<br/><br/># Apply PowerTransformer<br/>pt = PowerTransformer(standardize=True)<br/>X_train_transformed = pt.fit_transform(X_train)<br/>X_test_transformed = pt.transform(X_test)<br/><br/># Train the model<br/>nb_clf = GaussianNB()<br/>nb_clf.fit(X_train_transformed, y_train)<br/><br/># Make predictions<br/>y_pred = nb_clf.predict(X_test_transformed)<br/><br/># Check accuracy<br/>accuracy = accuracy_score(y_test, y_pred)<br/>print(f"Accuracy: {accuracy:.4f}")</span></pre></div></div></div><div class="ab cb qr qs qt qu" role="separator"><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="73a4" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Further Reading</h2><p id="7637" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank">GaussianNB</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="82e1" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">Technical Environment</h2><p id="bfe1" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="c0b4" class="pz oj fq bf ok qa qb qc on qd qe qf oq nl qg qh qi np qj qk ql nt qm qn qo fw bk">About the Illustrations</h2><p id="ef4f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="2481" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="qz ra rb rc rd"><div role="button" tabindex="0" class="ab bx cp kj it re rf bp rg lw ao"><div class="rh l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ri rj cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ri rj em n ay tu"/></div><div class="rk l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rn hp l"><h2 class="bf ga ws ic it wt iv iw wu iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wv vu vv vw vx lj vy vz uf ii wa wb wc uj uk ul ep bm um oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----04949cef383c--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="ww l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="rw dz rx it ab ry il ed"><div class="ed rq bx rr rs"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rq bx kk rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx rv ru"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>