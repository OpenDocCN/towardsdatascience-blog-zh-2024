- en: The Most Simple Way to Set Up ChatGPT Locally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地设置ChatGPT的最简单方法
- en: 原文：[https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17](https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17](https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17)
- en: The secret to running LLMs on consumer hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在消费者硬件上运行LLM的秘诀
- en: '[](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    ·10 min read·Jan 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    ·阅读时长10分钟·2024年1月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4e88f8fdb92a29a3ddbbbae01f561100.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e88f8fdb92a29a3ddbbbae01f561100.png)'
- en: 'Figure 1: Cute tiny little robots are working in a futuristic soap factory
    ([unsplash: Gerard Siderius](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：可爱的小机器人们正在未来派的肥皂工厂工作（[unsplash: Gerard Siderius](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)）。'
- en: As a data scientist, I have dedicated numerous hours delving into the intricacies
    of Large Language Models (LLMs) like [BERT](https://arxiv.org/abs/1810.04805),
    GPT{[2](https://openai.com/research/better-language-models),[3](https://arxiv.org/abs/2005.14165),[4](https://arxiv.org/abs/2303.08774)},
    and [ChatGPT](https://openai.com/blog/chatgpt). These advanced models have significantly
    expanded in scale, making it increasingly challenging to operate the latest high-performance
    models on standard consumer equipment. Regrettably, at my home, I still do not
    have a 8x A100 machine at my disposal.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名数据科学家，我已经花费了大量时间深入研究像[BERT](https://arxiv.org/abs/1810.04805)、GPT{[2](https://openai.com/research/better-language-models),[3](https://arxiv.org/abs/2005.14165),[4](https://arxiv.org/abs/2303.08774)}和[ChatGPT](https://openai.com/blog/chatgpt)这样的超大语言模型（LLMs）的复杂性。这些先进的模型在规模上有了显著扩展，使得在标准消费者设备上运行最新的高性能模型变得越来越具有挑战性。遗憾的是，在我家，我仍然没有8x
    A100机器可供使用。
- en: I do not (yet) have a 8x A100 machine at home
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我（目前）家里没有8x A100机器
- en: 'In the last few years a new technique was used to make models smaller and faster:
    quantization. This method elegantly trims down the once-bulky LLMs to a size more
    *digestible* for consumer-grade hardware. It’s akin to putting these AI giants
    on a digital diet, making them fit comfortably into the more modest confines of
    our home computers. Meanwhile, the open-source community, with trailblazers like
    🤗 HuggingFace and 🦄 Mistral, has been instrumental in democratizing access to
    these models. They’ve essentially turned the exclusive AI club into a ‘come one,
    come all’ tech fest — [no secret handshake required](https://www.youtube.com/watch?v=HoibVdUAYkw)!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，一种新的技术被用来使模型变得更小更快：量化。这种方法巧妙地将曾经庞大的LLM缩小到一个更*易于消化*的大小，适合消费者级硬件。它就像是让这些人工智能巨头进行数字化减肥，使它们能够舒适地适应我们家用计算机更为谦逊的空间。与此同时，开源社区，像🤗
    HuggingFace和🦄 Mistral这样的先行者，在普及这些模型方面发挥了重要作用。他们基本上将这个原本排外的AI俱乐部变成了一个‘人人欢迎’的科技盛会——[无需秘密握手](https://www.youtube.com/watch?v=HoibVdUAYkw)!
- en: While instruction-trained model weights are a significant piece of the puzzle,
    they’re not the whole picture. Think of these weights as the brain of the operation
    —…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管经过指令训练的模型权重是谜题中的重要一环，但它们并不是全部。可以将这些权重看作是操作的“大脑”——…
