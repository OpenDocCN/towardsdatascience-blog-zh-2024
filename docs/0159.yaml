- en: The Most Simple Way to Set Up ChatGPT Locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17](https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The secret to running LLMs on consumer hardware
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    Â·10 min readÂ·Jan 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e88f8fdb92a29a3ddbbbae01f561100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Cute tiny little robots are working in a futuristic soap factory
    ([unsplash: Gerard Siderius](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)).'
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, I have dedicated numerous hours delving into the intricacies
    of Large Language Models (LLMs) like [BERT](https://arxiv.org/abs/1810.04805),
    GPT{[2](https://openai.com/research/better-language-models),[3](https://arxiv.org/abs/2005.14165),[4](https://arxiv.org/abs/2303.08774)},
    and [ChatGPT](https://openai.com/blog/chatgpt). These advanced models have significantly
    expanded in scale, making it increasingly challenging to operate the latest high-performance
    models on standard consumer equipment. Regrettably, at my home, I still do not
    have a 8x A100 machine at my disposal.
  prefs: []
  type: TYPE_NORMAL
- en: I do not (yet) have a 8x A100 machine at home
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the last few years a new technique was used to make models smaller and faster:
    quantization. This method elegantly trims down the once-bulky LLMs to a size more
    *digestible* for consumer-grade hardware. Itâ€™s akin to putting these AI giants
    on a digital diet, making them fit comfortably into the more modest confines of
    our home computers. Meanwhile, the open-source community, with trailblazers like
    ðŸ¤— HuggingFace and ðŸ¦„ Mistral, has been instrumental in democratizing access to
    these models. Theyâ€™ve essentially turned the exclusive AI club into a â€˜come one,
    come allâ€™ tech fest â€” [no secret handshake required](https://www.youtube.com/watch?v=HoibVdUAYkw)!'
  prefs: []
  type: TYPE_NORMAL
- en: While instruction-trained model weights are a significant piece of the puzzle,
    theyâ€™re not the whole picture. Think of these weights as the brain of the operation
    â€”â€¦
  prefs: []
  type: TYPE_NORMAL
