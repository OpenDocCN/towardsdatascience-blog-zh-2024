- en: The Most Simple Way to Set Up ChatGPT Locally
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨æœ¬åœ°è®¾ç½®ChatGPTçš„æœ€ç®€å•æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17](https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17](https://towardsdatascience.com/the-most-simple-way-to-set-up-chatgpt-locally-697a835fa24c?source=collection_archive---------3-----------------------#2024-01-17)
- en: The secret to running LLMs on consumer hardware
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨æ¶ˆè´¹è€…ç¡¬ä»¶ä¸Šè¿è¡ŒLLMçš„ç§˜è¯€
- en: '[](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Dennis
    Bakhuis](../Images/4dc6dca031cdedbb044a1d0a6b142186.png)](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    [Dennis Bakhuis](https://dennisbakhuis.medium.com/?source=post_page---byline--697a835fa24c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    Â·10 min readÂ·Jan 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--697a835fa24c--------------------------------)
    Â·é˜…è¯»æ—¶é•¿10åˆ†é’ŸÂ·2024å¹´1æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4e88f8fdb92a29a3ddbbbae01f561100.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e88f8fdb92a29a3ddbbbae01f561100.png)'
- en: 'Figure 1: Cute tiny little robots are working in a futuristic soap factory
    ([unsplash: Gerard Siderius](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾1ï¼šå¯çˆ±çš„å°æœºå™¨äººä»¬æ­£åœ¨æœªæ¥æ´¾çš„è‚¥çš‚å·¥å‚å·¥ä½œï¼ˆ[unsplash: Gerard Siderius](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)ï¼‰ã€‚'
- en: As a data scientist, I have dedicated numerous hours delving into the intricacies
    of Large Language Models (LLMs) like [BERT](https://arxiv.org/abs/1810.04805),
    GPT{[2](https://openai.com/research/better-language-models),[3](https://arxiv.org/abs/2005.14165),[4](https://arxiv.org/abs/2303.08774)},
    and [ChatGPT](https://openai.com/blog/chatgpt). These advanced models have significantly
    expanded in scale, making it increasingly challenging to operate the latest high-performance
    models on standard consumer equipment. Regrettably, at my home, I still do not
    have a 8x A100 machine at my disposal.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åæ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘å·²ç»èŠ±è´¹äº†å¤§é‡æ—¶é—´æ·±å…¥ç ”ç©¶åƒ[BERT](https://arxiv.org/abs/1810.04805)ã€GPT{[2](https://openai.com/research/better-language-models),[3](https://arxiv.org/abs/2005.14165),[4](https://arxiv.org/abs/2303.08774)}å’Œ[ChatGPT](https://openai.com/blog/chatgpt)è¿™æ ·çš„è¶…å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¤æ‚æ€§ã€‚è¿™äº›å…ˆè¿›çš„æ¨¡å‹åœ¨è§„æ¨¡ä¸Šæœ‰äº†æ˜¾è‘—æ‰©å±•ï¼Œä½¿å¾—åœ¨æ ‡å‡†æ¶ˆè´¹è€…è®¾å¤‡ä¸Šè¿è¡Œæœ€æ–°çš„é«˜æ€§èƒ½æ¨¡å‹å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§ã€‚é—æ†¾çš„æ˜¯ï¼Œåœ¨æˆ‘å®¶ï¼Œæˆ‘ä»ç„¶æ²¡æœ‰8x
    A100æœºå™¨å¯ä¾›ä½¿ç”¨ã€‚
- en: I do not (yet) have a 8x A100 machine at home
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æˆ‘ï¼ˆç›®å‰ï¼‰å®¶é‡Œæ²¡æœ‰8x A100æœºå™¨
- en: 'In the last few years a new technique was used to make models smaller and faster:
    quantization. This method elegantly trims down the once-bulky LLMs to a size more
    *digestible* for consumer-grade hardware. Itâ€™s akin to putting these AI giants
    on a digital diet, making them fit comfortably into the more modest confines of
    our home computers. Meanwhile, the open-source community, with trailblazers like
    ğŸ¤— HuggingFace and ğŸ¦„ Mistral, has been instrumental in democratizing access to
    these models. Theyâ€™ve essentially turned the exclusive AI club into a â€˜come one,
    come allâ€™ tech fest â€” [no secret handshake required](https://www.youtube.com/watch?v=HoibVdUAYkw)!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡å»å‡ å¹´é‡Œï¼Œä¸€ç§æ–°çš„æŠ€æœ¯è¢«ç”¨æ¥ä½¿æ¨¡å‹å˜å¾—æ›´å°æ›´å¿«ï¼šé‡åŒ–ã€‚è¿™ç§æ–¹æ³•å·§å¦™åœ°å°†æ›¾ç»åºå¤§çš„LLMç¼©å°åˆ°ä¸€ä¸ªæ›´*æ˜“äºæ¶ˆåŒ–*çš„å¤§å°ï¼Œé€‚åˆæ¶ˆè´¹è€…çº§ç¡¬ä»¶ã€‚å®ƒå°±åƒæ˜¯è®©è¿™äº›äººå·¥æ™ºèƒ½å·¨å¤´è¿›è¡Œæ•°å­—åŒ–å‡è‚¥ï¼Œä½¿å®ƒä»¬èƒ½å¤Ÿèˆ’é€‚åœ°é€‚åº”æˆ‘ä»¬å®¶ç”¨è®¡ç®—æœºæ›´ä¸ºè°¦é€Šçš„ç©ºé—´ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¼€æºç¤¾åŒºï¼ŒåƒğŸ¤—
    HuggingFaceå’ŒğŸ¦„ Mistralè¿™æ ·çš„å…ˆè¡Œè€…ï¼Œåœ¨æ™®åŠè¿™äº›æ¨¡å‹æ–¹é¢å‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚ä»–ä»¬åŸºæœ¬ä¸Šå°†è¿™ä¸ªåŸæœ¬æ’å¤–çš„AIä¿±ä¹éƒ¨å˜æˆäº†ä¸€ä¸ªâ€˜äººäººæ¬¢è¿â€™çš„ç§‘æŠ€ç››ä¼šâ€”â€”[æ— éœ€ç§˜å¯†æ¡æ‰‹](https://www.youtube.com/watch?v=HoibVdUAYkw)!
- en: While instruction-trained model weights are a significant piece of the puzzle,
    theyâ€™re not the whole picture. Think of these weights as the brain of the operation
    â€”â€¦
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç»è¿‡æŒ‡ä»¤è®­ç»ƒçš„æ¨¡å‹æƒé‡æ˜¯è°œé¢˜ä¸­çš„é‡è¦ä¸€ç¯ï¼Œä½†å®ƒä»¬å¹¶ä¸æ˜¯å…¨éƒ¨ã€‚å¯ä»¥å°†è¿™äº›æƒé‡çœ‹ä½œæ˜¯æ“ä½œçš„â€œå¤§è„‘â€â€”â€”â€¦
