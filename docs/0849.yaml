- en: Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand ✍︎
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Sora的扩散变换器（DiT）手工分析 ✍︎
- en: 原文：[https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02](https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02](https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02)
- en: Explore the secret behind Sora’s state-of-the-art videos
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Sora最先进视频背后的秘密
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    ·12 min read·Apr 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    ·12分钟阅读·2024年4月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5fbe86c67353e44883ee9150b7907a20.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fbe86c67353e44883ee9150b7907a20.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: '*“In the ancient land of DiTharos, there once lived a legend, called Sora.
    A legend that embodied the very essence of unlimited potential, encompassing the
    vastness and the magnificence of the skies.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*“在古老的DiTharos大陆，曾经有一个传说，叫做Sora。一个象征着无限潜力的传说，涵盖了天空的辽阔与宏伟。”*'
- en: '*When it soared high with its iridescent wings spanning vast expanses and light
    bouncing off its striking body, one could hear the words ‘Sora IS Sky’ echoing
    into the heavens. What made it a legend was not just its epic enormity but its
    power to harness the elements of light scattered in the swirling clouds. With
    its mighty strength, the magic that Sora created with a single twirl, was a sight
    to behold!*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*当它展开色彩斑斓的翅膀，飞翔在辽阔的天空，光线在它引人注目的身体上反射时，人们能听到“**Sora 就是天空**”的声音回响在天际。使它成为传说的，不仅仅是它那宏大的体量，还有它驾驭散落在旋云中的光元素的力量。凭借强大的力量，Sora凭一旋所创造的魔法，堪称一绝！*'
- en: '*They say, Sora lives on, honing its skills and getting stronger with each
    passing day, ready to fly when the hour is golden. When you see a splash of crimson
    red in the sky today, you would know it’s a speck of the legend flying into the
    realms of light!”*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们说，Sora依然存在，日复一日磨砺技艺，变得愈加强大，准备在黄金时刻翱翔。当你今天在天空中看到一抹深红，你会知道那是传说的一颗星星正飞入光明的领域！*'
- en: This is a story I told my son about a mythical dragon that lived in a far away
    land. We called it ‘The Legend of Sora’. He really enjoyed it because Sora is
    big and strong, and illuminated the sky. Now of course, he doesn’t understand
    the idea of transformers and diffusion yet, he’s only four, but he does understand
    the idea of a magnanimous dragon that uses the power of light and rules over DiTharos.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我讲给儿子听的一个关于远方土地上神话般的龙的故事。我们称它为“**Sora的传说**”。他非常喜欢这个故事，因为Sora既巨大又强大，能够照亮天空。当然，现在他还不理解变换器和扩散的概念，他才四岁，但他确实明白一个使用光的力量并统治着DiTharos的宏伟龙的概念。
- en: '![](../Images/a3017fb7070ef36c0941deefb160ee3e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3017fb7070ef36c0941deefb160ee3e.png)'
- en: Image by author (The powerful Sora by my son — the color choices and the bold
    strokes are all his work.)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者（强大的Sora由我的儿子创作——色彩选择和大胆的笔触都是他的作品。）
- en: '**Sora by Open AI**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Sora by Open AI**'
- en: And that story very closely resembles how our world’s Sora, Open AI’s text-to-video
    model emerged in the realm of AI and has taken the world by storm. In principle,
    Sora is a diffusion transformer (DiT) developed by [William Peebles](https://www.linkedin.com/in/william-peebles-a980a212a/)
    and [Saining Xie](https://www.linkedin.com/in/sainxie/) in 2023.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事与我们世界上 Sora（Open AI 的文本到视频模型）如何在人工智能领域诞生并席卷全球的过程非常相似。从原则上讲，Sora 是由[William
    Peebles](https://www.linkedin.com/in/william-peebles-a980a212a/) 和 [Saining Xie](https://www.linkedin.com/in/sainxie/)
    在 2023 年开发的扩散变换器（DiT）。
- en: 'In other words, it uses the idea of diffusion for predicting the videos and
    the strength of transformers for next-level scaling. To understand this further,
    let’s try to find the answer to these two questions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它利用扩散的概念来预测视频，并通过变换器的强大功能实现下一阶段的扩展。为了更好地理解这一点，让我们尝试回答这两个问题：
- en: What does Sora do when given a prompt to work on?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Sora 接收到一个提示词时，它会做什么？
- en: How does it combine the diffusion-transformer ideas?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是如何结合扩散变换器的理念的？
- en: Talking about the videos made by Sora, here is my favorite one of an adorable
    Dalmatian in the streets of Italy. How natural is its movement!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 说到 Sora 制作的视频，这是我最喜欢的一部，内容是一只可爱的斑点狗在意大利街头。它的动作多么自然！
- en: 'The prompt used for the video : “The camera directly faces colorful buildings
    in Burano Italy. An adorable dalmation looks through a window on a building on
    the ground floor. Many people are walking and cycling along the canal streets
    in front of the buildings.”'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 视频使用的提示词是：“相机直接面对意大利布拉诺的彩色建筑。一只可爱的斑点狗透过一栋一楼建筑的窗户看外面。许多人在建筑前的运河街道上走路和骑车。”
- en: How did Sora do this?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 是如何做到这一点的？
- en: Without any further ado, let’s dive into the details and look at how Sora creates
    these super-realistic videos based on text-prompts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不再多说，让我们深入了解细节，看看 Sora 如何根据文本提示生成这些超现实的视频。
- en: '**How does Sora work?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Sora 是如何工作的？**'
- en: Thanks once again to Prof. Tom Yeh’s wonderful AI by Hand Series, we have this
    [great piece on Sora](https://www.linkedin.com/feed/update/urn:li:share:7165412130224033793/)
    for our discussion. (All the images below, unless otherwise noted, are by Prof.
    Tom Yeh from the above-mentioned LinkedIn post, which I have edited with his permission.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次感谢 Tom Yeh 教授精彩的《手工AI系列》，我们有了这篇[关于 Sora 的精彩文章](https://www.linkedin.com/feed/update/urn:li:share:7165412130224033793/)作为讨论材料。（以下所有图片，除非另有注明，均来自
    Tom Yeh 教授的上述 LinkedIn 文章，我已获得他的许可进行编辑。）
- en: 'So, here we go:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们开始吧：
- en: '**Our goal** — Generate a video based on a text-prompt.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的目标** — 根据文本提示生成视频。'
- en: 'We are given:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给定了：
- en: Training video
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练视频
- en: Text-prompt
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本提示
- en: Diffusion step *t* = 3
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩散步骤 *t* = 3
- en: For our example, can you guess what our text-prompt is going to be? You are
    right. It is “Sora is sky”. A diffusion step of *t* = 3 means we are adding noise
    or diffusing the model in three steps but for illustration we will stick to one
    in this example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的例子，您能猜到我们的文本提示是什么吗？你猜对了。它是“天空是 Sora”。当扩散步骤 *t* = 3 时，意味着我们在三步内加入噪声或扩散模型，但为了说明方便，我们将在这个例子中只进行一步。
- en: '**What is diffusion?**'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**什么是扩散？**'
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Diffusion mainly refers to the phenomenon of scattering of particles — think
    how we enjoy the soft sun rays making a peek from behind the clouds. This soft
    glow can be attributed to the scattering of sunlight as it passes through the
    cloud layer causing the rays to spread out in different directions.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 扩散主要指的是粒子散射的现象——想象一下我们如何享受阳光透过云层照射出来的柔和光线。这种柔和的光辉可以归因于阳光穿过云层时发生的散射现象，导致光线向不同方向扩散。
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The random motion of the particles drives this diffusion. And that is exactly
    what happens for diffusion models used in image generation. Random noise is added
    to the image causing the elements in the image to deviate from the original and
    thus making way for creating more refined images.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 粒子的随机运动推动了这种扩散。这正是图像生成中扩散模型的工作原理。向图像中添加随机噪声，导致图像中的元素偏离原始状态，从而为创造更精细的图像铺平道路。
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we talk about diffusion in regards to image-models, the key idea to remember
    is ‘noise’.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当我们谈论图像模型中的扩散时，记住的关键概念是“噪声”。
- en: 'The process begins here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 过程从这里开始：
- en: '[1] **Convert video into patches**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] **将视频转换为补丁**'
- en: When working with text-generation, the models break down the large corpus into
    small pieces called tokens and use these tokens for all the calculations. Similarly,
    Sora breaks down the video into smaller elements called visual patches to make
    the work simpler.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文本生成时，模型将大语料库分解成小块，称为 tokens，并使用这些 tokens 进行所有计算。类似地，Sora 将视频分解为更小的元素，称为视觉补丁，以简化工作。
- en: Since we are talking about a video, we are talking about images in multiple
    frames. In our example, we have four frames. Each of the four frames or matrices
    contain the pixels that create the image.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们讨论的是视频，因此涉及的是多帧的图像。在我们的示例中，我们有四个帧。每一个帧或矩阵包含了组成图像的像素。
- en: '![](../Images/f4692e227bf767ec00f7bee6b9feb7e1.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4692e227bf767ec00f7bee6b9feb7e1.png)'
- en: 'The first step here is to convert this training video into 4 spacetime patches
    as below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一步是将这个训练视频转换成如下的 4 个时空补丁：
- en: '![](../Images/f6acaf339dcea5ba8aad7eda0a89d8e8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6acaf339dcea5ba8aad7eda0a89d8e8.png)'
- en: '[2] **Reduce the dimension of these visual patches : Encoder**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] **减少这些视觉补丁的维度：编码器**'
- en: 'Next, dimension reduction. The idea of dimension reduction has existed for
    over a century now *(Trivia : Principal Component Analysis, also known as PCA
    was introduced by Karl Pearson in 1901)*, but its significance hasn’t faded over
    time.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是降维。降维的概念已经存在超过一个世纪了 *(小知识：主成分分析，简称 PCA，是由卡尔·皮尔逊在 1901 年提出的)*，但它的重要性至今未曾褪色。
- en: And Sora uses it too!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 也使用了这个方法！
- en: 'When we talk about Neural Networks, one of the fundamental ideas for dimension
    reduction is the encoder. Encoder, by its design, transforms high-dimensional
    data into lower-dimension by focusing on capturing the most relevant features
    of the data. Win-win on both sides: it increases the efficiency and speed of the
    computations while the algorithm gets useful data to work on.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论神经网络时，降维的一个基本思想是编码器。编码器通过设计，将高维数据转换为低维数据，重点捕捉数据中最相关的特征。这样做是双赢：它提高了计算效率和速度，同时算法也能获得有用的数据进行处理。
- en: Sora uses the same idea for converting the high-dimensional pixels into a lower-dimensional
    latent space. To do so, we multiply the patches with weights and biases, followed
    by ReLU.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 使用相同的思路，将高维像素转化为低维的潜在空间。为此，我们将补丁与权重和偏置相乘，再经过 ReLU 激活函数。
- en: '**Note**:'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意**：'
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Linear transformation : The input embedding vector is multiplied by the weight
    matrix **W** and'
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 线性变换：输入的嵌入向量与权重矩阵 **W** 相乘。
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: then added with the bias vector **b**,
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后加上偏置向量 **b**，
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: z = **W**x+**b**, where **W** is the weight matrix, x is our word embedding
    and **b** is the bias vector.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: z = **W**x + **b**，其中 **W** 是权重矩阵，x 是我们的词嵌入，**b** 是偏置向量。
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ReLU activation function : Next, we apply the ReLU to this intermediate z.'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ReLU 激活函数：接下来，我们将 ReLU 应用到这个中间的 z 上。
- en: ''
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU returns the element-wise maximum of the input and zero. Mathematically,
    h = max{0,z}.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ReLU 返回输入与零的逐元素最大值。数学上表示为 h = max{0, z}。
- en: The weight matrix here is a 2x4 matrix [ [1, 0, -1, 0], [0, 1, 0, 1] ] with
    the bias being [0,1].
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的权重矩阵是一个 2x4 的矩阵 [ [1, 0, -1, 0], [0, 1, 0, 1] ]，偏置为 [0,1]。
- en: The patches matrix here is 4x4.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的补丁矩阵是 4x4。
- en: The product of the transpose of the weight matrix **W** and bias **b** with
    the patches followed by **ReLU** gives us a latent space which is only a 2x4 matrix.
    Thus, by using the visual encoder the dimension of the ‘model’ is reduced from
    4 (2x2x1) to 2 (2x1).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵 **W** 的转置与偏置 **b** 及补丁相乘，再经过 **ReLU**，得到的潜在空间仅是一个 2x4 的矩阵。因此，通过使用视觉编码器，“模型”的维度从
    4（2x2x1）降到 2（2x1）。
- en: '![](../Images/133c33cc9f5459d066951674310d428e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/133c33cc9f5459d066951674310d428e.png)'
- en: In the original DiT paper, this reduction is from 196,608 (256x256x3) to 4096
    (32x32x4), which is huge. Imagine working with 196,608 pixels against working
    with 4096 — a 48 times reduction!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 DiT 论文中，这个降维是从 196,608（256x256x3）降到 4096（32x32x4），这是一个巨大的降维。想象一下，从 196,608
    像素降到 4096 像素——降维了 48 倍！
- en: Right after this dimension reduction, we have one of the most significant steps
    in the entire process — **diffusion**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个降维之后，我们进入整个过程中的一个最重要的步骤——**扩散**。
- en: '[3] **Diffuse the model with noise**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] **通过噪声扩散模型**'
- en: To introduce diffusion, we add sampled noise to the obtained latent features
    in the previous step to find the Noised Latent. The goal here is to **ask the
    model to detect what the noise is**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引入扩散，我们将采样噪声添加到前一步得到的潜在特征中，从而找到噪声潜在特征。这里的目标是**让模型检测噪声是什么**。
- en: '![](../Images/9a4857023e2bc12e838a6824b7b12128.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a4857023e2bc12e838a6824b7b12128.png)'
- en: This is in essence the idea of diffusion for image generation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这是图像生成的扩散思想。
- en: By adding noise to the image, the model is asked to guess what the noise is
    and what it looks like. In return, the model can generate a completely new image
    based on what it guessed and learnt from the noisy image.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向图像添加噪声，模型被要求猜测噪声是什么以及它的样子。作为回报，模型可以根据它的猜测和从噪声图像中学到的东西，生成一个全新的图像。
- en: It can also be seen relative to deleting a word from the language model and
    asking it to guess what the deleted word was.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它也可以看作是从语言模型中删除一个词，并要求模型猜测被删除的词是什么。
- en: Now that the training video has been reduced and diffused with noise, the next
    steps are to make use of the text-prompt to get a video as advocated by the prompt.
    We do this by conditioning with the adaptive norm layer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练视频已被减少并加入了噪声，接下来的步骤是利用文本提示来生成符合提示的视频。我们通过使用自适应归一化层来进行条件化。
- en: '[4]-[6] **Conditioning by Adaptive Norm Layer**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[4]-[6] **通过自适应归一化层进行条件化**'
- en: 'What ‘conditioning’ essentially means is we try to influence the behavior of
    the model using the additional information we have available. For eg: since our
    prompt is ‘Sora is sky’, we would like for the model to focus on elements such
    as sky or clouds rather attaching importance on other concepts like a hat or a
    plant. Thus, an adaptive norm layer massages, in better terms — **dynamically
    scales and shifts the data** in the network based on the input it receives.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: “条件化”本质上意味着我们尝试使用可用的附加信息来影响模型的行为。例如：由于我们的提示是‘Sora是天空’，我们希望模型关注像天空或云朵这样的元素，而不是过多关注其他概念，如帽子或植物。因此，自适应归一化层可以更好地解释——**动态地根据输入数据调整和移动网络中的数据**。
- en: What is scale and shift?
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么是缩放和位移？
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Scale occurs when we multiply, for e.g. we may start with a variable A. When
    we multiply it with 2 suppose, we get 2*A which amplifies or scales the value
    of A up by 2\. If we multiply it by ½, the value is scaled down by 0.5.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 缩放发生在我们进行乘法运算时，例如我们可能从变量A开始。如果我们假设将其与2相乘，那么我们得到2*A，这会将A的值放大2倍。如果我们将其乘以1/2，那么值会缩小到0.5倍。
- en: ''
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shift is denoted by addition, for e.g. we may be walking on the number line.
    We start with 1 and we are asked to shift to 5\. What do we do? We can either
    add 4 and get 1+4=5 or we could add a hundred 0.4s to get to 5, 1+(100*0.04 )=
    5\. It all depends on if we want to take bigger steps (4) or smaller steps (0.04)
    to reach our goal.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 位移用加法表示，例如我们可能在数轴上行走。我们从1开始，需要移动到5。我们该怎么做？我们可以加4，得到1+4=5，或者我们也可以加一百个0.04，得到1+(100*0.04)=5。最终取决于我们是想走大步（4）还是小步（0.04）来达成目标。
- en: '[4] **Encode Conditions**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] **编码条件**'
- en: To make use of the conditions, in our case the information we have for building
    the model, first we translate it into a form the model understands, i.e., **vectors**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用条件，在我们的例子中就是我们用于构建模型的信息，首先我们将其转换为模型能够理解的形式，即**向量**。
- en: The first step in the process is to **translate the prompt into a text embedding
    vector**.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该过程的第一步是**将提示转换为文本嵌入向量**。
- en: The next step is to translate step *t* = 3 into a binary vector.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步是将步骤*t* = 3转换为二进制向量。
- en: The third step is to concatenate these vectors together.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步是将这些向量连接在一起。
- en: '![](../Images/877ebac8fcd40947dfe7735ccbe9ebdc.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/877ebac8fcd40947dfe7735ccbe9ebdc.png)'
- en: '[5] **Estimate Scale/Shift**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] **估算缩放/位移**'
- en: Remember that here we use an ‘adaptive’ layer norm which implies that it adapts
    its values based on what the current conditions of the model are. Thus, to capture
    the correct essence of the data, we need to include the importance of each element
    in the data. And it is done by estimating the scale and shift.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这里我们使用的是“自适应”层归一化，这意味着它根据模型的当前条件调整其值。因此，为了捕捉数据的正确本质，我们需要包括数据中每个元素的重要性。这是通过估算缩放和位移来实现的。
- en: For estimating these values for our model, we multiply the concatenated vector
    of prompt and diffusion step with the weight and add the bias to it. These weights
    and biases are **learnable parameters** which the model learns and updates.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算我们模型的这些值，我们将提示和扩散步骤的连接向量与权重相乘，并添加偏差。这些权重和偏差是**可学习参数**，模型会学习并更新这些参数。
- en: '![](../Images/20a96c4d027e0dcb74a4b43e95edcd51.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20a96c4d027e0dcb74a4b43e95edcd51.png)'
- en: '*(Remark: The third element in the resultant vector, according to me, should
    be 1\. It could be a small error in the original post but as humans we are allowed
    a bit of it, aren’t we? To maintain uniformity, I continue here with the values
    from the original post.)*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*(备注：根据我理解，结果向量中的第三个元素应该是 1。这可能是原文中的一个小错误，但作为人类，我们可以容忍一点错误，不是吗？为了保持一致性，我在这里继续使用原文中的值。)*'
- en: The goal here is to estimate the scale [2,-1] and the shift [-1,5] (since our
    model size is 2, we have two scale and two shift parameters). We keep them under
    ‘X’ and ‘+’ respectively.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是估计尺度 [2,-1] 和偏移 [-1,5]（由于我们的模型大小为 2，因此有两个尺度和两个偏移参数）。我们将它们分别保持为 'X' 和 '+'。
- en: '![](../Images/e1afe750aa12e3c8589b8e306aace95e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1afe750aa12e3c8589b8e306aace95e.png)'
- en: '[6] **Apply Scale/Shift**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] **应用尺度/偏移**'
- en: To apply the scale and shift obtained in the previous step, we multiply the
    noised latent in Step 3 by [2, -1] and shift it by adding [-1,5].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用在上一步骤中获得的尺度和偏移，我们将步骤 3 中的噪声潜在变量乘以 [2, -1] 并通过添加 [-1,5] 来进行偏移。
- en: '![](../Images/b07c1988760b5d313712184f1a634222.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b07c1988760b5d313712184f1a634222.png)'
- en: The result is the **‘conditioned’** noise latent.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 **‘条件化’** 的噪声潜在变量。
- en: '**[7]-[9] Transformer**'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**[7]-[9] 变换器**'
- en: The last three steps consist of adding the transformer element to the above
    diffusion and conditioning steps. This step help us find the noise as predicted
    by the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后三个步骤是将变换器元素添加到上述扩散和条件化步骤中。这一步帮助我们找到模型预测的噪声。
- en: '[7] **Self-Attention**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] **自注意力**'
- en: This is the critical idea behind transformers that make them so phenomenal!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是变换器背后的关键思想，使其如此出色！
- en: What is self-attention?
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 什么是自注意力？
- en: ''
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a mechanism by which each word in a sentence analyzes every other word
    and measures how important they are to each other, making sense of the context
    and relationships in the text.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一种机制，其中句子中的每个单词都会分析其他所有单词，并衡量它们对彼此的重要性，从而理解文本中的上下文和关系。
- en: To enable self-attention, the conditioned noise latent is fed into the Query-Key
    function to obtain a self-attention matrix. The QK-values are omitted here for
    simplicity.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启用自注意力，将条件化噪声潜在变量输入到查询-键（Query-Key）函数中，以获得自注意力矩阵。这里为了简便省略了 QK 值。
- en: '[8] **Attention Pooling**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] **注意力池化**'
- en: Next, we multiply the conditioned noised latent with the self-attention matrix
    to obtain the attention weighted features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将条件化噪声潜在变量与自注意力矩阵相乘，以获得注意力加权特征。
- en: '![](../Images/f1c33717362b9918f946a25cb164dbee.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1c33717362b9918f946a25cb164dbee.png)'
- en: '[9] **Point-wise Feed Forward Network**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] **逐点前馈网络**'
- en: Once again returning back to the basics, we multiply the attention-weighted
    features with weights and biases to obtain the predicted noise.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回到基础，我们将注意力加权特征与权重和偏置相乘，以获得预测的噪声。
- en: '![](../Images/c6887a55e18a4f9d1bf622f42f11cee9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6887a55e18a4f9d1bf622f42f11cee9.png)'
- en: '**Training**'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练**'
- en: The last bit now is to train the model using **Mean Square Error** between the
    **predicted noise** and **the sampled noise** (ground truth).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用 **均方误差**（**MSE**）在 **预测噪声** 和 **采样噪声**（地面真实值）之间训练模型。
- en: '[10] **Calculate the MSE loss gradients and update learnable parameters**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] **计算 MSE 损失梯度并更新可学习参数**'
- en: Using the MSE loss gradients, we use backpropagation to update all the parameters
    that are learnable (for e.g. the weights and biases in the adaptive norm layer).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MSE 损失的梯度，我们通过反向传播来更新所有可学习的参数（例如，自适应归一化层中的权重和偏置）。
- en: '![](../Images/3a7a6dabeab8658d307d4307e6c19845.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a7a6dabeab8658d307d4307e6c19845.png)'
- en: The encoder and decoder parameters are frozen and not learnable.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器的参数被冻结，不可学习。
- en: '*(Remark: The second element in the second row should be -1, a tiny error which
    makes things better).*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*(备注：第二行的第二个元素应该是 -1，这是一个小错误，但它使得事情变得更好)。*'
- en: '[11]-[13] **Generate New Samples**'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[11]-[13] **生成新样本**'
- en: '[11] **Denoise**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] **去噪**'
- en: Now that we are ready to generate new videos (yay!), we first need to remove
    the noise we had introduced. To do so, we subtract the predicted noise from the
    noise-latent to obtain noise-free latent.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备生成新的视频（耶！），首先需要去除我们引入的噪声。为此，我们从噪声潜在变量中减去预测的噪声，以获得无噪声的潜在变量。
- en: '![](../Images/3391e576b58b059eef9272423a126735.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3391e576b58b059eef9272423a126735.png)'
- en: Mind you, this is not the same as our original latent. Reason being we went
    through multiple conditioning and attention steps in between that included the
    context of our problem into the model. Thus, allowing the model a better feel
    for what its target should be while generating the video.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这与我们原始的潜在空间不同。原因是我们在其中经历了多个条件和注意力步骤，这些步骤将我们问题的上下文纳入了模型。因此，使得模型在生成视频时能更好地理解其目标应该是什么。
- en: '[12] **Convert the latent space back to the pixels : Decoder**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] **将潜在空间转换回像素：解码器**'
- en: Just like we did for encoders, we multiply the latent space patches with weight
    and biases while followed by ReLU. We can observe here that after the work of
    the decoder, the model is back to the original dimension of 4 which was lowered
    to 2 when we had used the encoder.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为编码器所做的那样，我们将潜在空间补丁与权重和偏置相乘，之后应用ReLU。在这里，我们可以观察到，经过解码器的处理后，模型已经恢复到原始的4维度，而在使用编码器时，我们将维度降至2。
- en: '![](../Images/ed3cb93df31d1aa408d4a5f3263997b8.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed3cb93df31d1aa408d4a5f3263997b8.png)'
- en: '[13] **Time for the video!**'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] **视频时间！**'
- en: The last step is to arrange the result from the above matrix into a sequence
    of frames which finally gives us our new video. Hooray!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是将上述矩阵的结果排列成一系列帧，最终给我们带来新的视频。太棒了！
- en: '![](../Images/e6535b8b94e1c013d089ba5d25dc378d.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6535b8b94e1c013d089ba5d25dc378d.png)'
- en: And with that we come to the end of this supremely powerful technique. **Congratulations,
    you have created a Sora video!**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一点，我们来到了这一极具力量的技术的结尾。**恭喜你，已经创建了一个Sora视频！**
- en: 'To summarize all that was said and done above, here are the 5 key points:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 总结以上所说的内容，这里有五个关键点：
- en: Converting the videos into visual patches and then reducing their dimension
    is essential. A visual encoder is our friend here.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将视频转换为视觉补丁并减少其维度是至关重要的。视觉编码器在这里是我们的朋友。
- en: As the name suggests, diffusion is the name of the game in this method. Adding
    noise to the video and then working with it at each of the subsequent steps (in
    different ways) is what this technique relies on.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如名字所示，扩散是此方法的核心。在视频中添加噪声，并在后续的每一步（以不同的方式）与之进行处理，是这项技术的基础。
- en: Next up is the transformer architecture that enhances the abilities of the diffusion
    process along with amplifying the scale of the model.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是增强扩散过程能力并放大模型规模的变换器架构。
- en: Once the model is trained and ready to converge to a solution, the two D’s —
    denoiser and decoder come in handy. One by removing the noise and the other by
    projecting the low-dimensional space to its original dimension.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦模型训练完成并准备好收敛到解决方案，两个D——去噪器和解码器便派上用场。一个去除噪声，另一个则将低维空间投射回其原始维度。
- en: Finally, the resultant pixels from the decoder are rearranged to generate the
    desired video.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，来自解码器的结果像素被重新排列以生成所需的视频。
- en: (*Once you are done with the article, I suggest you to read the story at the
    beginning once more. Can you spot the similarities between Sora of DiTharos and
    Sora of our world?*)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (*一旦你完成了文章，我建议你再读一遍开头的故事。你能发现DiTharos中的Sora和我们世界中的Sora之间的相似之处吗？*)
- en: '**The Diffusion-Transformer (DiT) Combo**'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**扩散-变换器（DiT）组合**'
- en: The kind of videos Sora has been able to produce, it is worth saying that the
    Diffusion-Transformer duo is lethal. Along with it, the idea of visual patches
    opens up an avenue for tinkering with a range of image resolutions, aspect ratios
    and durations, which allows for utmost experimentation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Sora能够制作的那种视频，值得一提的是，扩散-变换器（Diffusion-Transformer）二重奏非常强大。与此同时，视觉补丁的概念为我们提供了一条可供尝试不同图像分辨率、纵横比和时长的路径，从而能够进行最广泛的实验。
- en: Overall, it would not be wrong to say that this idea is seminal and without
    a doubt is here to stay. According to this [New York Times article](https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.html)
    , Sora was named after the Japanese word for sky and to evoke the idea of limitless
    potential. And having witnessed its initial promise, it is true that Sora has
    definitely set a new frontier in AI. Now it remains to see how well it stands
    the test of safety and time.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，可以毫不夸张地说，这个想法是开创性的，并且无疑将长期存在。根据这篇[纽约时报的文章](https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.html)，Sora的名字源自日语中的“天空”一词，旨在唤起无限潜力的概念。鉴于我们已经见证了其最初的潜力，确实可以说Sora已经开辟了人工智能的新前沿。现在，剩下的就是看看它如何经得起安全性和时间的考验。
- en: As the legend of **DiTharos** goes — “Sora lives on, honing its skills and getting
    stronger with each passing day, ready to fly when the hour is golden!”
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**DiTharos**的传说——“Sora继续生存，不断磨练技能，随着每一天的过去变得更强，准备在黄金时刻起飞！”
- en: P.S. If you would like to work through this exercise on your own, here is a
    blank template for you to use.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 附言：如果你想自己完成这个练习，这里有一个空白模板供你使用。
- en: '[Blank Template for hand-exercise](https://bit.ly/3vvj6a6)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[手动练习空白模板](https://bit.ly/3vvj6a6)'
- en: Now go have some fun with Sora in **the land of ‘DiTharos’**!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，去和Sora一起在**‘DiTharos’的土地**上玩得开心吧！
