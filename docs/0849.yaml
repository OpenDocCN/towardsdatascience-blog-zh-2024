- en: Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand ✍︎
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02](https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the secret behind Sora’s state-of-the-art videos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------)
    ·12 min read·Apr 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fbe86c67353e44883ee9150b7907a20.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '*“In the ancient land of DiTharos, there once lived a legend, called Sora.
    A legend that embodied the very essence of unlimited potential, encompassing the
    vastness and the magnificence of the skies.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*When it soared high with its iridescent wings spanning vast expanses and light
    bouncing off its striking body, one could hear the words ‘Sora IS Sky’ echoing
    into the heavens. What made it a legend was not just its epic enormity but its
    power to harness the elements of light scattered in the swirling clouds. With
    its mighty strength, the magic that Sora created with a single twirl, was a sight
    to behold!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*They say, Sora lives on, honing its skills and getting stronger with each
    passing day, ready to fly when the hour is golden. When you see a splash of crimson
    red in the sky today, you would know it’s a speck of the legend flying into the
    realms of light!”*'
  prefs: []
  type: TYPE_NORMAL
- en: This is a story I told my son about a mythical dragon that lived in a far away
    land. We called it ‘The Legend of Sora’. He really enjoyed it because Sora is
    big and strong, and illuminated the sky. Now of course, he doesn’t understand
    the idea of transformers and diffusion yet, he’s only four, but he does understand
    the idea of a magnanimous dragon that uses the power of light and rules over DiTharos.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3017fb7070ef36c0941deefb160ee3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author (The powerful Sora by my son — the color choices and the bold
    strokes are all his work.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Sora by Open AI**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And that story very closely resembles how our world’s Sora, Open AI’s text-to-video
    model emerged in the realm of AI and has taken the world by storm. In principle,
    Sora is a diffusion transformer (DiT) developed by [William Peebles](https://www.linkedin.com/in/william-peebles-a980a212a/)
    and [Saining Xie](https://www.linkedin.com/in/sainxie/) in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, it uses the idea of diffusion for predicting the videos and
    the strength of transformers for next-level scaling. To understand this further,
    let’s try to find the answer to these two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What does Sora do when given a prompt to work on?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it combine the diffusion-transformer ideas?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talking about the videos made by Sora, here is my favorite one of an adorable
    Dalmatian in the streets of Italy. How natural is its movement!
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt used for the video : “The camera directly faces colorful buildings
    in Burano Italy. An adorable dalmation looks through a window on a building on
    the ground floor. Many people are walking and cycling along the canal streets
    in front of the buildings.”'
  prefs: []
  type: TYPE_NORMAL
- en: How did Sora do this?
  prefs: []
  type: TYPE_NORMAL
- en: Without any further ado, let’s dive into the details and look at how Sora creates
    these super-realistic videos based on text-prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '**How does Sora work?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks once again to Prof. Tom Yeh’s wonderful AI by Hand Series, we have this
    [great piece on Sora](https://www.linkedin.com/feed/update/urn:li:share:7165412130224033793/)
    for our discussion. (All the images below, unless otherwise noted, are by Prof.
    Tom Yeh from the above-mentioned LinkedIn post, which I have edited with his permission.)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Our goal** — Generate a video based on a text-prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are given:'
  prefs: []
  type: TYPE_NORMAL
- en: Training video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diffusion step *t* = 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For our example, can you guess what our text-prompt is going to be? You are
    right. It is “Sora is sky”. A diffusion step of *t* = 3 means we are adding noise
    or diffusing the model in three steps but for illustration we will stick to one
    in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is diffusion?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Diffusion mainly refers to the phenomenon of scattering of particles — think
    how we enjoy the soft sun rays making a peek from behind the clouds. This soft
    glow can be attributed to the scattering of sunlight as it passes through the
    cloud layer causing the rays to spread out in different directions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The random motion of the particles drives this diffusion. And that is exactly
    what happens for diffusion models used in image generation. Random noise is added
    to the image causing the elements in the image to deviate from the original and
    thus making way for creating more refined images.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we talk about diffusion in regards to image-models, the key idea to remember
    is ‘noise’.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The process begins here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] **Convert video into patches**'
  prefs: []
  type: TYPE_NORMAL
- en: When working with text-generation, the models break down the large corpus into
    small pieces called tokens and use these tokens for all the calculations. Similarly,
    Sora breaks down the video into smaller elements called visual patches to make
    the work simpler.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are talking about a video, we are talking about images in multiple
    frames. In our example, we have four frames. Each of the four frames or matrices
    contain the pixels that create the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4692e227bf767ec00f7bee6b9feb7e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first step here is to convert this training video into 4 spacetime patches
    as below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6acaf339dcea5ba8aad7eda0a89d8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[2] **Reduce the dimension of these visual patches : Encoder**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, dimension reduction. The idea of dimension reduction has existed for
    over a century now *(Trivia : Principal Component Analysis, also known as PCA
    was introduced by Karl Pearson in 1901)*, but its significance hasn’t faded over
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: And Sora uses it too!
  prefs: []
  type: TYPE_NORMAL
- en: 'When we talk about Neural Networks, one of the fundamental ideas for dimension
    reduction is the encoder. Encoder, by its design, transforms high-dimensional
    data into lower-dimension by focusing on capturing the most relevant features
    of the data. Win-win on both sides: it increases the efficiency and speed of the
    computations while the algorithm gets useful data to work on.'
  prefs: []
  type: TYPE_NORMAL
- en: Sora uses the same idea for converting the high-dimensional pixels into a lower-dimensional
    latent space. To do so, we multiply the patches with weights and biases, followed
    by ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Linear transformation : The input embedding vector is multiplied by the weight
    matrix **W** and'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: then added with the bias vector **b**,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: z = **W**x+**b**, where **W** is the weight matrix, x is our word embedding
    and **b** is the bias vector.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'ReLU activation function : Next, we apply the ReLU to this intermediate z.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU returns the element-wise maximum of the input and zero. Mathematically,
    h = max{0,z}.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The weight matrix here is a 2x4 matrix [ [1, 0, -1, 0], [0, 1, 0, 1] ] with
    the bias being [0,1].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The patches matrix here is 4x4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The product of the transpose of the weight matrix **W** and bias **b** with
    the patches followed by **ReLU** gives us a latent space which is only a 2x4 matrix.
    Thus, by using the visual encoder the dimension of the ‘model’ is reduced from
    4 (2x2x1) to 2 (2x1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/133c33cc9f5459d066951674310d428e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the original DiT paper, this reduction is from 196,608 (256x256x3) to 4096
    (32x32x4), which is huge. Imagine working with 196,608 pixels against working
    with 4096 — a 48 times reduction!
  prefs: []
  type: TYPE_NORMAL
- en: Right after this dimension reduction, we have one of the most significant steps
    in the entire process — **diffusion**.
  prefs: []
  type: TYPE_NORMAL
- en: '[3] **Diffuse the model with noise**'
  prefs: []
  type: TYPE_NORMAL
- en: To introduce diffusion, we add sampled noise to the obtained latent features
    in the previous step to find the Noised Latent. The goal here is to **ask the
    model to detect what the noise is**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a4857023e2bc12e838a6824b7b12128.png)'
  prefs: []
  type: TYPE_IMG
- en: This is in essence the idea of diffusion for image generation.
  prefs: []
  type: TYPE_NORMAL
- en: By adding noise to the image, the model is asked to guess what the noise is
    and what it looks like. In return, the model can generate a completely new image
    based on what it guessed and learnt from the noisy image.
  prefs: []
  type: TYPE_NORMAL
- en: It can also be seen relative to deleting a word from the language model and
    asking it to guess what the deleted word was.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the training video has been reduced and diffused with noise, the next
    steps are to make use of the text-prompt to get a video as advocated by the prompt.
    We do this by conditioning with the adaptive norm layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[4]-[6] **Conditioning by Adaptive Norm Layer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What ‘conditioning’ essentially means is we try to influence the behavior of
    the model using the additional information we have available. For eg: since our
    prompt is ‘Sora is sky’, we would like for the model to focus on elements such
    as sky or clouds rather attaching importance on other concepts like a hat or a
    plant. Thus, an adaptive norm layer massages, in better terms — **dynamically
    scales and shifts the data** in the network based on the input it receives.'
  prefs: []
  type: TYPE_NORMAL
- en: What is scale and shift?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Scale occurs when we multiply, for e.g. we may start with a variable A. When
    we multiply it with 2 suppose, we get 2*A which amplifies or scales the value
    of A up by 2\. If we multiply it by ½, the value is scaled down by 0.5.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shift is denoted by addition, for e.g. we may be walking on the number line.
    We start with 1 and we are asked to shift to 5\. What do we do? We can either
    add 4 and get 1+4=5 or we could add a hundred 0.4s to get to 5, 1+(100*0.04 )=
    5\. It all depends on if we want to take bigger steps (4) or smaller steps (0.04)
    to reach our goal.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4] **Encode Conditions**'
  prefs: []
  type: TYPE_NORMAL
- en: To make use of the conditions, in our case the information we have for building
    the model, first we translate it into a form the model understands, i.e., **vectors**.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in the process is to **translate the prompt into a text embedding
    vector**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next step is to translate step *t* = 3 into a binary vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third step is to concatenate these vectors together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/877ebac8fcd40947dfe7735ccbe9ebdc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[5] **Estimate Scale/Shift**'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that here we use an ‘adaptive’ layer norm which implies that it adapts
    its values based on what the current conditions of the model are. Thus, to capture
    the correct essence of the data, we need to include the importance of each element
    in the data. And it is done by estimating the scale and shift.
  prefs: []
  type: TYPE_NORMAL
- en: For estimating these values for our model, we multiply the concatenated vector
    of prompt and diffusion step with the weight and add the bias to it. These weights
    and biases are **learnable parameters** which the model learns and updates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/20a96c4d027e0dcb74a4b43e95edcd51.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Remark: The third element in the resultant vector, according to me, should
    be 1\. It could be a small error in the original post but as humans we are allowed
    a bit of it, aren’t we? To maintain uniformity, I continue here with the values
    from the original post.)*'
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to estimate the scale [2,-1] and the shift [-1,5] (since our
    model size is 2, we have two scale and two shift parameters). We keep them under
    ‘X’ and ‘+’ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1afe750aa12e3c8589b8e306aace95e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[6] **Apply Scale/Shift**'
  prefs: []
  type: TYPE_NORMAL
- en: To apply the scale and shift obtained in the previous step, we multiply the
    noised latent in Step 3 by [2, -1] and shift it by adding [-1,5].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b07c1988760b5d313712184f1a634222.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is the **‘conditioned’** noise latent.
  prefs: []
  type: TYPE_NORMAL
- en: '**[7]-[9] Transformer**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last three steps consist of adding the transformer element to the above
    diffusion and conditioning steps. This step help us find the noise as predicted
    by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[7] **Self-Attention**'
  prefs: []
  type: TYPE_NORMAL
- en: This is the critical idea behind transformers that make them so phenomenal!
  prefs: []
  type: TYPE_NORMAL
- en: What is self-attention?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is a mechanism by which each word in a sentence analyzes every other word
    and measures how important they are to each other, making sense of the context
    and relationships in the text.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To enable self-attention, the conditioned noise latent is fed into the Query-Key
    function to obtain a self-attention matrix. The QK-values are omitted here for
    simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: '[8] **Attention Pooling**'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we multiply the conditioned noised latent with the self-attention matrix
    to obtain the attention weighted features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f1c33717362b9918f946a25cb164dbee.png)'
  prefs: []
  type: TYPE_IMG
- en: '[9] **Point-wise Feed Forward Network**'
  prefs: []
  type: TYPE_NORMAL
- en: Once again returning back to the basics, we multiply the attention-weighted
    features with weights and biases to obtain the predicted noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6887a55e18a4f9d1bf622f42f11cee9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Training**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last bit now is to train the model using **Mean Square Error** between the
    **predicted noise** and **the sampled noise** (ground truth).
  prefs: []
  type: TYPE_NORMAL
- en: '[10] **Calculate the MSE loss gradients and update learnable parameters**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the MSE loss gradients, we use backpropagation to update all the parameters
    that are learnable (for e.g. the weights and biases in the adaptive norm layer).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a7a6dabeab8658d307d4307e6c19845.png)'
  prefs: []
  type: TYPE_IMG
- en: The encoder and decoder parameters are frozen and not learnable.
  prefs: []
  type: TYPE_NORMAL
- en: '*(Remark: The second element in the second row should be -1, a tiny error which
    makes things better).*'
  prefs: []
  type: TYPE_NORMAL
- en: '[11]-[13] **Generate New Samples**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[11] **Denoise**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are ready to generate new videos (yay!), we first need to remove
    the noise we had introduced. To do so, we subtract the predicted noise from the
    noise-latent to obtain noise-free latent.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3391e576b58b059eef9272423a126735.png)'
  prefs: []
  type: TYPE_IMG
- en: Mind you, this is not the same as our original latent. Reason being we went
    through multiple conditioning and attention steps in between that included the
    context of our problem into the model. Thus, allowing the model a better feel
    for what its target should be while generating the video.
  prefs: []
  type: TYPE_NORMAL
- en: '[12] **Convert the latent space back to the pixels : Decoder**'
  prefs: []
  type: TYPE_NORMAL
- en: Just like we did for encoders, we multiply the latent space patches with weight
    and biases while followed by ReLU. We can observe here that after the work of
    the decoder, the model is back to the original dimension of 4 which was lowered
    to 2 when we had used the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed3cb93df31d1aa408d4a5f3263997b8.png)'
  prefs: []
  type: TYPE_IMG
- en: '[13] **Time for the video!**'
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to arrange the result from the above matrix into a sequence
    of frames which finally gives us our new video. Hooray!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6535b8b94e1c013d089ba5d25dc378d.png)'
  prefs: []
  type: TYPE_IMG
- en: And with that we come to the end of this supremely powerful technique. **Congratulations,
    you have created a Sora video!**
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize all that was said and done above, here are the 5 key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting the videos into visual patches and then reducing their dimension
    is essential. A visual encoder is our friend here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the name suggests, diffusion is the name of the game in this method. Adding
    noise to the video and then working with it at each of the subsequent steps (in
    different ways) is what this technique relies on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next up is the transformer architecture that enhances the abilities of the diffusion
    process along with amplifying the scale of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model is trained and ready to converge to a solution, the two D’s —
    denoiser and decoder come in handy. One by removing the noise and the other by
    projecting the low-dimensional space to its original dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the resultant pixels from the decoder are rearranged to generate the
    desired video.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (*Once you are done with the article, I suggest you to read the story at the
    beginning once more. Can you spot the similarities between Sora of DiTharos and
    Sora of our world?*)
  prefs: []
  type: TYPE_NORMAL
- en: '**The Diffusion-Transformer (DiT) Combo**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kind of videos Sora has been able to produce, it is worth saying that the
    Diffusion-Transformer duo is lethal. Along with it, the idea of visual patches
    opens up an avenue for tinkering with a range of image resolutions, aspect ratios
    and durations, which allows for utmost experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, it would not be wrong to say that this idea is seminal and without
    a doubt is here to stay. According to this [New York Times article](https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.html)
    , Sora was named after the Japanese word for sky and to evoke the idea of limitless
    potential. And having witnessed its initial promise, it is true that Sora has
    definitely set a new frontier in AI. Now it remains to see how well it stands
    the test of safety and time.
  prefs: []
  type: TYPE_NORMAL
- en: As the legend of **DiTharos** goes — “Sora lives on, honing its skills and getting
    stronger with each passing day, ready to fly when the hour is golden!”
  prefs: []
  type: TYPE_NORMAL
- en: P.S. If you would like to work through this exercise on your own, here is a
    blank template for you to use.
  prefs: []
  type: TYPE_NORMAL
- en: '[Blank Template for hand-exercise](https://bit.ly/3vvj6a6)'
  prefs: []
  type: TYPE_NORMAL
- en: Now go have some fun with Sora in **the land of ‘DiTharos’**!
  prefs: []
  type: TYPE_NORMAL
