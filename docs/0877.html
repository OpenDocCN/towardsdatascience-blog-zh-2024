<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Improving Generalization in Survival Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Improving Generalization in Survival Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-generalization-in-survival-models-bb7bc045bfc6?source=collection_archive---------8-----------------------#2024-04-05">https://towardsdatascience.com/improving-generalization-in-survival-models-bb7bc045bfc6?source=collection_archive---------8-----------------------#2024-04-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="01c7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Suggestions for estimating and enhancing predictive accuracy for the employee attrition case</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@nicolupi.2?source=post_page---byline--bb7bc045bfc6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nicolas Lupi" class="l ep by dd de cx" src="../Images/7f0735890a77b9ef601dc6cd54a9a861.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9JY1rQbu5HTc7A9LtJQtJw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bb7bc045bfc6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@nicolupi.2?source=post_page---byline--bb7bc045bfc6--------------------------------" rel="noopener follow">Nicolas Lupi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bb7bc045bfc6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/dfd4bd0b1d93bed1196e1eee0d262947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BUe-P4FHQNd6_WeU"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@israelandrxde?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Israel Andrade</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="eb30" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Recently, I’ve come up with a particular issue when dealing with survival analysis: many models I fitted performed well in theory, with strong test metrics, but then failed to predict the true outcomes that were observed in practice. In this article, I want to discuss ways to better estimate the performance of our survival models, and a practical tip to help with extrapolation. Note: the main assumption here is that <strong class="nf fr">we count with several observations of our individuals over time</strong> (e.g., monthly observations for all the employees in a company).</p><p id="ef5e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The problem I was dealing with was the employee attrition case. I had information about several employees in a company, and I was interested in predicting which of them were most likely to leave in the future. If you want to explore the employee attrition topic further, make sure to check out this helpful <a class="af nc" rel="noopener" target="_blank" href="/cracking-the-employee-attrition-problem-with-machine-learning-6ee751ec4aae">article</a>.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f192" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Traditional Approach</h2><p id="2e6a" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">Many existing implementations on survival analysis start off with a dataset containing <strong class="nf fr">one observation per individual</strong> (patients in a health study, employees in the attrition case, clients in the client churn case, and so on). For these individuals we typically have two key variables: one signaling the event of interest (an employee quitting) and another measuring time (how long they’ve been with the company, up to either today or their departure). Together with these two variables, we then have explanatory variables with which we aim to predict the risk of each individual. These features can include the job role, age or compensation of the employee, for example.</p><p id="3253" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Moving on, most implementations out there take a survival model (from simpler estimators such as Kaplan Meier to more complex ones like ensemble models or even neural networks), fit them over a train set and then evaluate over a test set. This train-test split is usually performed over the individual observations, generally making a stratified split.</p><p id="d71e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In my case, I started with a dataset that followed several employees in a company monthly until December 2023 (in case the employee was still at the company), or until the month they left the company — the event date:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/b83d30874eb025f12a3959b6542b2014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x21vbNnKpQKDgVnxk_H3pg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Taking the last record of each employee — Image by author</figcaption></figure><p id="b8c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to adapt my data to the survival case, I took the last observation of each employee as shown in the picture above (the blue dots for active employees, and the red crosses for employees who left). At that point for each employee, I recorded whether the event had occurred at that date or not (if they were active or if they had left), their tenure in months at that time, and all their explanatory variables. I then performed a stratified train-test split over this data, like this:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="f16a" class="pm oi fq pj b bg pn po l pp pq">import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/># We load our dataset with several observations (record_date) per employee (employee_id)<br/># The event column indicates if the employee left on that given month (1) or if the employee was still active (0)<br/>df = pd.read_csv(f'{FILE_NAME}.csv')<br/><br/># Creating a label where positive events have tenure and negative events have negative tenure - required by Random Survival Forest<br/>df_model['label'] = np.where(df_model['event'], df_model['tenure_in_months'], - df_model['tenure_in_months'])<br/><br/>df_train, df_test = train_test_split(df_model, test_size=0.2, stratify=df_model['event'], random_state=42)</span></pre><p id="ed69" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After performing the split, I proceeded to fit a model. In this case, I chose to experiment with a <strong class="nf fr">Random Survival Forest</strong> using the <a class="af nc" href="https://scikit-survival.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">scikit-survival</a> library.</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="2769" class="pm oi fq pj b bg pn po l pp pq">from sklearn.preprocessing import OrdinalEncoder<br/>from sksurv.datasets import get_x_y<br/>from sksurv.ensemble import RandomSurvivalForest<br/><br/>cat_features = [] # list of all the categorical features<br/>features = [] # list of all the features (both categorical and numeric)<br/><br/># Categorical Encoding<br/>encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)<br/>encoder.fit(df_train[cat_features])<br/><br/>df_train[cat_features] = encoder.transform(df_train[cat_features])<br/>df_test[cat_features] = encoder.transform(df_test[cat_features])<br/><br/># X &amp; y<br/>X_train, y_train = get_x_y(df_train, attr_labels=['event','tenure_in_months'], pos_label=1)<br/>X_test, y_test = get_x_y(df_test, attr_labels=['event','tenure_in_months'], pos_label=1)<br/><br/># Fit the model<br/>estimator = RandomSurvivalForest(random_state=RANDOM_STATE)<br/>estimator.fit(X_train[features], y_train)<br/><br/># Store predictions<br/>y_pred = estimator.predict(X_test[features])</span></pre><p id="22d7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After a quick run using the default settings of the model, I was thrilled with the test metrics I saw. First of all, I was getting a <strong class="nf fr">concordance index</strong> above 0.90 in the test set. The concordance index is a measure of how well the model predicts the order of events: it reflects whether employees predicted to be at high risk were indeed the ones leaving the company first. An index of 1 corresponds to perfect prediction accuracy, while an index of 0.5 indicates a prediction no better than random chance.</p><p id="c4f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I was particularly interested in seeing if the employees who left in the test set matched with the most risky employees according to the model. In the case of the Random Survival Forest, the model returns the risk scores of each observation. I took the percentage of employees who left the company in the test set, and used it to filter the most risky employees according to the model. The results were very solid, with the employees flagged with the most risk matching almost perfectly with the actual leavers, with an F1 score above 0.90 in the minority class.</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="e8fb" class="pm oi fq pj b bg pn po l pp pq">from lifelines.utils import concordance_index<br/>from sklearn.metrics import classification_report<br/><br/># Concordance Index<br/>ci_test = concordance_index(df_test['tenure_in_months'], -y_pred, df_test['event'])<br/>print(f'Concordance index:{ci_test:0.5f}\n')<br/><br/># Match the most risky employees (according to the model) with the employees who left<br/>q_test = 1 - df_test['event'].mean()<br/><br/>thr = np.quantile(y_pred, q_test)<br/>risky_employees = (y_pred &gt;= thr) * 1<br/><br/>print(classification_report(df_test['event'], risky_employees))</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pr"><img src="../Images/74122d67e23cd850b485539e7e9f5dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*vDrUBk0Ba512S-AEB-FNyA.png"/></div></figure><p id="9663" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Getting +0.9 metrics on the first run should set off an alarm: was the model really able to predict whether an employee was going to stay or leave with such confidence? Imagine this: we submit our predictions saying which employees are most likely to leave. However, a couple months go by, and HR then reaches us worried, saying that the people who left during the last period, did not exactly match with our predictions, at least at the rate it was expected from our test metrics.</p><p id="437b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We have two main problems here: the first one is that our model isn’t extrapolating quite as well as we thought. The second one, and even worse, is that we weren’t able to measure this lack of performance. First, I’ll show a simple way we can estimate how well our model is truly extrapolating, and then I’ll talk about one potential reason it may be failing to do so, and how to mitigate it.</p><h2 id="72cc" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Estimating Generalization Capabilities</h2><p id="3b07" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">The key here is having access to panel data, that is, several records of our individuals over time, up until the time of event or the time the study ended (the date of our snapshot, in the case of employee attrition). Instead of discarding all this information and keeping only the last record of each employee, we could use it to create a test set that will better reflect how the model performs in the future. The idea is quite simple: suppose we have monthly records of our employees up until December 2023. We could move back, say, 6 months, and pretend we took the snapshot in June instead of December. Then, we would take the last observation for employees who left the company before June 2023 as positive events, and the June 2023 record of employees who survived beyond that date as negative events, even if we already know some of them eventually left afterwards. We are pretending we don’t know this yet.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/357382f4ce299d98ec4b62b23be4ddfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XhLHFEtvKYjNNohcAqFQCw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">We take a snapshot in June 2023 and use the following period as our test set — Image by author</figcaption></figure><p id="e726" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As the picture above shows, I take a snapshot in June, and all employees who were active at that time are taken as active. The test dataset takes all those active employees at June with their explanatory variables as they were on that date, and takes the latest tenure they achieved by December:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="2d04" class="pm oi fq pj b bg pn po l pp pq">test_date = '2023-07-01'<br/><br/># Selecting training data from records before the test date and taking the last observation per employee<br/>df_train = df[df.record_date &lt; test_date].reset_index(drop=True).copy()<br/>df_train = df_train.groupby('employee_id').tail(1).reset_index(drop=True)<br/>df_train['label'] = np.where(df_train['event'], df_train['tenure_in_months'], - df_train['tenure_in_months'])<br/><br/># Preparing test data with records of active employees at the test date<br/>df_test = df[(df.record_date == test_date) &amp; (df['event']==0)].reset_index(drop=True).copy()<br/>df_test = df_test.groupby('employee_id').tail(1).reset_index(drop=True)<br/>df_test = df_test.drop(columns = ['tenure_in_months','event'])<br/><br/># Fetching the last tenure and event status for employees in the test dataset<br/>df_last_tenure = df[df.employee_id.isin(df_test.employee_id.unique())].reset_index(drop=True).copy()<br/>df_last_tenure = df_last_tenure.groupby('employee_id').tail(1).reset_index(drop=True)<br/><br/>df_test = df_test.merge(df_last_tenure[['employee_id','tenure_in_months','event']], how='left')<br/>df_test['label'] = np.where(df_test['event'], df_test['tenure_in_months'], - df_test['tenure_in_months'])</span></pre><p id="eccc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We fit our model again on this new train data, and once we finish we make our predictions for all employees who were active on June. We then compare these predictions to the actual outcome of July — December 2023 — this is our test set. If those employees we marked as having the most risk left during the semester, and those we marked as having the lowest risk didn’t leave, or left rather late in the period, then our model is extrapolating well. By shifting our analysis back in time and leaving the last period for evaluation, we can have a better understanding of how well our model is generalizing. Of course, we could take this one step further and perform some type of time-series cross validation. For example, we could iterate this process many times, each time moving 6 months back in time, and evaluating the model’s accuracy over several time frames.</p><p id="6e22" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After training our model once again, we now see a drastic decrease in performance. First of all, the concordance index is now around 0.5 — equivalent to that of a random predictor. Also, if we try to match the ‘n’ most risky employees according to the model with the ‘n’ employees who left in the test set, we see a very poor classification with a 0.15 F1 for the minority class:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ps"><img src="../Images/71f02778139d87f07c0cdcdbe1f7e231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*sTJ0aIOMq030GYjt4Qh0Tw.png"/></div></figure><p id="2224" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So clearly there is something wrong, but at least we are now able to detect it instead of being misled. The main takeaway here is that our model performs well with a traditional split, but doesn’t extrapolate when doing a time-based split. This is a clear sign that some time bias may be present. In short, time-dependent information is being leaked and our model is overfitting over it. This is common in cases like our employee attrition problem, when the dataset comes from a snapshot taken at some date.</p><h2 id="c207" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Time Bias</h2><p id="d76e" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">The problem cuts down to this: all our positive observations (employees who left) belong to past dates, and all our negative observations (currently active employees) are all measured on the same date — today. If there is a single feature that reveals this to the model, then <strong class="nf fr">instead of predicting risk we will be predicting if an employee was recorded in December 2023 or before</strong>. This could be very subtle. For example, one feature we could be using is the engagement score of the employees. This feature could well show some seasonal patterns, and measuring it at the same time for active employees will surely introduce some bias in the model. Maybe in December, during the holiday season, this engagement score tends to decrease. The model will see a low score associated with all active employees, so it may learn to predict that whenever the engagement runs low, the churn risk also goes down, when in fact it should be the opposite!</p><p id="24e0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By now, a simple yet quite effective solution for this problem should be clear: instead of taking the last observation for each active employee, we could just pick a random month from all their history within the company. This will strongly reduce the chances of the model picking on any temporal patterns that we do not want it to overfit on:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ph"><img src="../Images/91f71a2a045d65eb1a5ea1334e4ac4a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*679In00iCD1FEnYY4nOYWw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">For the active employees, we take random records rather than their last one — Image by author</figcaption></figure><p id="1956" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the picture above we can see that we are now spanning a broader set of dates for the active employees. Instead of using their blue dots at June 2023, we take the random orange dots instead, and record their variables at the time, and the tenure they had so far in the company:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="edba" class="pm oi fq pj b bg pn po l pp pq">np.random.seed(0)<br/><br/># Select training data before the test date<br/>df_train = df[df.record_date &lt; test_date].reset_index(drop=True).copy()<br/><br/># Create an indicator for whether an employee eventually churns within the train set<br/>df_train['indicator'] = df_train.groupby('employee_id').event.transform(max)<br/><br/># Isolate records of employees who left, and store their last observation<br/>churn = df_train[df_train.indicator==1].reset_index(drop=True).copy()<br/>churn = churn.groupby('employee_id').tail(1).reset_index(drop=True)<br/><br/># For employees who stayed, randomly pick one observation from their historic records<br/>stay = df_train[df_train.indicator==0].reset_index(drop=True).copy()<br/>stay = stay.groupby('employee_id').apply(lambda x: x.sample(1)).reset_index(drop=True)<br/><br/># Combine churn and stay samples into the new training dataset<br/>df_train = pd.concat([churn,stay], ignore_index=True).copy()<br/>df_train['label'] = np.where(df_train['event'], df_train['tenure_in_months'], - df_train['tenure_in_months'])<br/>del df_train['indicator']<br/><br/># Prepare the test dataset similarly, using only the snapshot from the test date<br/>df_test = df[(df.record_date == test_date) &amp; (df.event==0)].reset_index(drop=True).copy()<br/>df_test = df_test.groupby('employee_id').tail(1).reset_index(drop=True)<br/>df_test = df_test.drop(columns = ['tenure_in_months','event'])<br/><br/># Get the last known tenure and event status for employees in the test set<br/>df_last_tenure = df[df.employee_id.isin(df_test.employee_id.unique())].reset_index(drop=True).copy()<br/>df_last_tenure = df_last_tenure.groupby('employee_id').tail(1).reset_index(drop=True)<br/><br/>df_test = df_test.merge(df_last_tenure[['employee_id','tenure_in_months','event']], how='left')<br/>df_test['label'] = np.where(df_test['event'], df_test['tenure_in_months'], - df_test['tenure_in_months'])</span></pre><p id="bab7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We then train our model once again, and evaluate it over the same test set we had before. We now see a concordance index of around 0.80. This isn’t the +0.90 we had earlier, but it definitely is a step up from the random-chance level of 0.5. Regarding our interest in classifying employees, we are still very far off the +0.9 F1 we had before, but we do see a slight increase compared to the previous approach, especially for the minority class.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pt"><img src="../Images/20c597ea36649a8d8a34dc2d1c9c132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*cG-79eGyFTBFdfsXUbIhww.png"/></div></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b467" class="oh oi fq bf oj ok ol om on oo op oq or nm os ot ou nq ov ow ox nu oy oz pa pb bk">Closing Remarks</h2><p id="a1e5" class="pw-post-body-paragraph nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny fj bk">To sum up, here are the main takeaways from our discussion:</p><ul class=""><li id="c4e4" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pu pv pw bk">It’s important to pay attention to the dates the observations were recorded — there is a high chance some sort of time bias is present, especially if all the observations of one of the event classes share the same date</li><li id="f766" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk">If we have past observations for our individuals, we can better estimate the performance of our models by setting aside a time period for testing, instead of performing a traditional train-test split over the individual observations</li><li id="8483" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk">If there is a strong performance decrease between the traditional approach and the time-based test split, this could be a sign of time bias</li><li id="fc54" class="nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny pu pv pw bk">One simple way to mitigate this, at least in part, is to randomly choose observations for each individual, instead of taking their last record for each</li></ul><p id="f450" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope this walkthrough was useful. If you’ve been dealing with similar issues in survival analysis, I’d love to hear whether this approach works for you too. Thanks for reading!</p></div></div></div></div>    
</body>
</html>