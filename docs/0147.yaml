- en: 'Graph & Geometric ML in 2024: Where We Are and What’s Next (Part II — Applications)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024年图形与几何机器学习：我们处于何种阶段，未来如何发展（第二部分——应用）
- en: 原文：[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16)
- en: State-of-the-Art Digest
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最前沿摘要
- en: Following the tradition from previous years, we interviewed a cohort of distinguished
    and prolific academic and industrial experts in an attempt to summarise the highlights
    of the past year and predict what is in store for 2024\. Past 2023 was so ripe
    with results that we had to break this post into two parts. This is Part II focusing
    on applications, see also [Part I](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1)
    for theory & new architectures.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延续往年的传统，我们采访了一批杰出且高产的学术和工业专家，旨在总结过去一年的亮点，并预测2024年的发展趋势。2023年成果丰硕，以至于我们不得不将这篇文章分为两部分。这是第二部分，重点介绍应用，欲了解理论与新架构，请参见[第一部分](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1)。
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    ·42 min read·Jan 16, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    ·42分钟阅读·2024年1月16日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
- en: Image by Authors with some help from DALL-E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，并得到DALL-E 3的帮助。
- en: '*The post is written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Dominique Beaini*](https://twitter.com/dom_beaini)*,* [*Nathan
    Benaich*](https://twitter.com/nathanbenaich)*,* [*Joey Bose*](https://twitter.com/bose_joey)*,*
    [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,* [*Bruno Correia*](https://twitter.com/befcorreia)*,*
    [*Ahmed Elhag*](https://twitter.com/Ahmed_AI035)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Leon Klein*](https://twitter.com/leonklein26)*,*
    [*N M Anoop Krishnan*](https://twitter.com/anoopnm007)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Andreas Loukas*](https://twitter.com/loukasa_tweet)*,* [*Santiago Miret*](https://www.linkedin.com/in/santiago-miret)*,*
    [*Luca Naef*](https://twitter.com/NaefLuca)*,* [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,*
    [*Emanuele Rossi*](https://twitter.com/emaros96)*,* [*Hannes Stärk*](https://twitter.com/HannesStaerk)*,*
    [*Alex Tong*](https://twitter.com/AlexanderTong7)*,* [*Anton Tsitsulin*](https://twitter.com/tsitsulin_)*,*
    [*Petar Veličković*](https://twitter.com/PetarV_93)*,* [*Minkai Xu*](https://twitter.com/MinkaiX)*,
    and* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由* [*Michael Galkin*](https://twitter.com/michael_galkin) *和* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *编写和编辑，且有来自* [*Dominique Beaini*](https://twitter.com/dom_beaini)
    *、* [*Nathan Benaich*](https://twitter.com/nathanbenaich) *、* [*Joey Bose*](https://twitter.com/bose_joey)
    *、* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter) *、* [*Bruno
    Correia*](https://twitter.com/befcorreia) *、* [*Ahmed Elhag*](https://twitter.com/Ahmed_AI035)
    *、* [*Kexin Huang*](https://twitter.com/KexinHuang5) *、* [*Chaitanya Joshi*](https://twitter.com/chaitjo)
    *、* [*Leon Klein*](https://twitter.com/leonklein26) *、* [*N M Anoop Krishnan*](https://twitter.com/anoopnm007)
    *、* [*Chen Lin*](https://twitter.com/WillLin1028) *、* [*Andreas Loukas*](https://twitter.com/loukasa_tweet)
    *、* [*Santiago Miret*](https://www.linkedin.com/in/santiago-miret) *、* [*Luca
    Naef*](https://twitter.com/NaefLuca) *、* [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)
    *、* [*Emanuele Rossi*](https://twitter.com/emaros96) *、* [*Hannes Stärk*](https://twitter.com/HannesStaerk)
    *、* [*Alex Tong*](https://twitter.com/AlexanderTong7) *、* [*Anton Tsitsulin*](https://twitter.com/tsitsulin_)
    *、* [*Petar Veličković*](https://twitter.com/PetarV_93) *、* [*Minkai Xu*](https://twitter.com/MinkaiX)
    *和* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng) *的重要贡献。*'
- en: '![](../Images/187024de90ba49fdfd0f708cdc506312.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/187024de90ba49fdfd0f708cdc506312.png)'
- en: 'Geometric ML methods and applications filled the covers of high-profile journals
    in 2023 (Figure sources: the papers by [Wang et al.](https://www.nature.com/articles/s42256-023-00609-5),
    [Viñas et al.](https://www.nature.com/articles/s42256-023-00684-8), [Deng et al.](https://www.nature.com/articles/s42256-023-00716-3),
    [Weiss et al.](https://www.nature.com/articles/s43588-023-00532-0), [Lagemann
    et al.](https://www.nature.com/articles/s42256-023-00744-z), [Duan et al.](https://www.nature.com/articles/s43588-023-00563-7),
    and [Lam et al.](https://www.science.org/doi/10.1126/science.adi2336))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 几何机器学习方法和应用填满了2023年高端期刊的封面（图源：来自[Wang等](https://www.nature.com/articles/s42256-023-00609-5)、[Viñas等](https://www.nature.com/articles/s42256-023-00684-8)、[Deng等](https://www.nature.com/articles/s42256-023-00716-3)、[Weiss等](https://www.nature.com/articles/s43588-023-00532-0)、[Lagemann等](https://www.nature.com/articles/s42256-023-00744-z)、[Duan等](https://www.nature.com/articles/s43588-023-00563-7)
    和[Lam等](https://www.science.org/doi/10.1126/science.adi2336)的论文）
- en: '[Structural Biology (Molecules & Proteins)](#2626)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[结构生物学（分子与蛋白质）](#2626)'
- en: a. [A Structural Biologist’s Perspective](#2f16)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [结构生物学家的视角](#2f16)
- en: b. [Industrial Perspective](#ade6)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [工业视角](#ade6)
- en: c. [Systems Biology](#7a08)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [系统生物学](#7a08)
- en: '[Materials Science (Crystals)](#6211)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[材料科学（晶体）](#6211)'
- en: '[Molecular Dynamics & ML Potentials](#0924)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[分子动力学与机器学习势能](#0924)'
- en: '[Geometric Generative Models (Manifolds)](#34b8)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[几何生成模型（流形）](#34b8)'
- en: '[BIG Graphs, Scalability: When GNNs are too expensive](#3f98)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[大型图谱、可扩展性：当GNN太昂贵时](#3f98)'
- en: '[Algorithmic Reasoning & Alignment](#f6d7)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[算法推理与对齐](#f6d7)'
- en: '[Knowledge Graphs: Inductive Reasoning is Solved?](#5854)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[知识图谱：归纳推理解决了吗？](#5854)'
- en: '[Temporal Graph Learning](#add1)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[时序图学习](#add1)'
- en: '[LLMs + Graphs for Scientific Discovery](#ad3d)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[LLMs + 图谱用于科学发现](#ad3d)'
- en: '[Cool GNN Applications](#3d00)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[酷炫的GNN应用](#3d00)'
- en: '[Geometric Wall Street Bulletin 💸](#986f)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[几何华尔街公告 💸](#986f)'
- en: 'The legend we will be using throughout the text:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在全文中将使用的符号：
- en: 🔥 hot topics
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 热点话题
- en: 💡 year’s highlight
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 今年的亮点
- en: 🏋️ challenges
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋️ 挑战
- en: ➡️ current/next developments
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 当前/下一步发展
- en: 🔮 predictions/speculations
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 预测/推测
- en: 💰 financial transactions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 💰 财务交易
- en: Structural Biology (Molecules & Proteins)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构生物学（分子与蛋白质）
- en: '*Dominique Beaini (Valence), Joey Bose (Mila & Dreamfold), Michael Bronstein
    (Oxford), Bruno Correia (EPFL), Michael Galkin (Intel), Kexin Huang (Stanford),
    Chaitanya Joshi (Cambridge), Andreas Loukas (Genentech), Luca Naef (VantAI), Hannes
    Stärk (MIT), Minkai Xu (Stanford)*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dominique Beaini（Valence）、Joey Bose（Mila & Dreamfold）、Michael Bronstein（Oxford）、Bruno
    Correia（EPFL）、Michael Galkin（Intel）、Kexin Huang（Stanford）、Chaitanya Joshi（Cambridge）、Andreas
    Loukas（Genentech）、Luca Naef（VantAI）、Hannes Stärk（MIT）、Minkai Xu（Stanford）*'
- en: Structural biology was definitely at the forefront of Geometric Deep Learning
    in 2023.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 结构生物学无疑是 2023 年几何深度学习领域的前沿。
- en: Following the 2020 discovery of [halicin](https://pubmed.ncbi.nlm.nih.gov/32084340/)
    as a potential new antibiotic, in 2023, two new antibiotics were discovered with
    the help of GNNs! First, it is [abaucin](https://www.nature.com/articles/s41589-023-01349-8)
    (by McMaster and MIT), which targets a stubborn pathogen resistant to many drugs.
    Second, MIT and Harvard researchers [discovered a new structural class of antibiotics](https://www.nature.com/articles/s41586-023-06887-8)
    where the screening process was supported by [ChemProp](https://github.com/chemprop/chemprop),
    a suite of GNNs for molecular property prediction. We also observe a convergence
    of ML and experimental techniques (“lab-in the-loop”) in the recent work on [autonomous
    molecular discovery](https://www.science.org/doi/10.1126/science.adi1407) (a trend
    we will also see in the Materials Design in the following sections).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 继 2020 年 [halicin](https://pubmed.ncbi.nlm.nih.gov/32084340/) 被发现作为一种潜在的新型抗生素之后，2023
    年通过 GNNs（图神经网络）的帮助，发现了两种新的抗生素！首先是 [abaucin](https://www.nature.com/articles/s41589-023-01349-8)（由麦克马斯特大学和麻省理工学院研究人员发现），它能够靶向一种对多种药物有抗药性的顽固病原体。其次，麻省理工学院和哈佛大学的研究人员
    [发现了一种新的抗生素结构类别](https://www.nature.com/articles/s41586-023-06887-8)，其中筛选过程得到了
    [ChemProp](https://github.com/chemprop/chemprop) 的支持，这是一套用于分子性质预测的 GNN 工具。此外，我们还观察到在最近的[自主分子发现](https://www.science.org/doi/10.1126/science.adi1407)工作中，机器学习和实验技术的融合（“实验室环节”）正在趋于一致（这是我们在后续材料设计部分也将看到的趋势）。
- en: '**Flow Matching** has been one of the biggest generative ML trends of 2023,
    allowing for faster sampling and deterministic sampling trajectories compared
    to diffusion models. The most prominent examples of Flow Matching models we have
    seen in the biological applications are **FoldFlow** ([Bose, Akhound-Sadegh, et
    al](https://arxiv.org/abs/2310.02391).) for protein backbone generation, **FlowSite**
    ([Stärk et al](https://arxiv.org/abs/2310.05764).) for protein binding site design,
    and **EquiFM** ([Song, Gong, et al](https://openreview.net/forum?id=hHUZ5V9XFu).)
    for molecule generation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**流匹配**已经成为 2023 年最大的生成型机器学习趋势之一，相比扩散模型，它能够实现更快的采样和确定性的采样轨迹。在生物学应用中，我们看到的流匹配模型最突出的例子有：**FoldFlow**（[Bose、Akhound-Sadegh
    等人](https://arxiv.org/abs/2310.02391)）用于蛋白质骨架生成，**FlowSite**（[Stärk 等人](https://arxiv.org/abs/2310.05764)）用于蛋白质结合位点设计，以及
    **EquiFM**（[Song、Gong 等人](https://openreview.net/forum?id=hHUZ5V9XFu)）用于分子生成。'
- en: '![](../Images/47b927a05ef39ae2fa830c5ff139d6f7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47b927a05ef39ae2fa830c5ff139d6f7.png)'
- en: '*Conditional probability paths learned by different versions of FoldFlow, visualizing
    the rotation trajectory of a single residue by the action of SO(3) on its homogeneous
    space* 𝕊²*. Figure source:* [*Bose, Akhound-Sadegh, et al*](https://arxiv.org/abs/2310.02391)*.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*FoldFlow 不同版本学习到的条件概率路径，展示了 SO(3) 在其齐次空间上作用下单一残基的旋转轨迹* 𝕊²*。图源：* [*Bose、Akhound-Sadegh
    等人*](https://arxiv.org/abs/2310.02391)*。*'
- en: Efficient Flow Matching on complex geometries with necessary equivariances became
    possible thanks to a handful of theory papers including Riemannian Flow Matching
    ([Chen and Lipman](https://arxiv.org/abs/2302.03660)), Minibatch Optimal Transport
    ([Tong et al](https://arxiv.org/abs/2302.00482)), and Simulation-Free Schrödinger
    bridges ([Tong, Malkin, Fatras, et al](https://arxiv.org/abs/2307.03672)). A great
    resource to learn Flow Matching with code examples and notebooks is the [TorchCFM](https://github.com/atong01/conditional-flow-matching)
    repo on GitHub as well as talks by [Yaron Lipman](https://www.youtube.com/watch?v=5ZSwYogAxYg),
    [Joey Bose](https://www.youtube.com/watch?v=EPxDI0ytfQU), [Hannes Stärk](https://www.youtube.com/watch?v=Xl7YNR1-CN8),
    and [Alex Tong](https://www.youtube.com/watch?v=UhDtH7Ia9Ag).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 高效流匹配在复杂几何体上的应用，结合必要的等变性，得以实现，这得益于几篇理论论文的贡献，包括《黎曼流匹配》（[Chen 和 Lipman](https://arxiv.org/abs/2302.03660)）、《小批量最优传输》（[Tong
    等人](https://arxiv.org/abs/2302.00482)）、以及《无仿真薛定谔桥》（[Tong、Malkin、Fatras 等人](https://arxiv.org/abs/2307.03672)）。一个很好的学习流匹配的资源是
    GitHub 上的 [TorchCFM](https://github.com/atong01/conditional-flow-matching) 仓库，其中包含代码示例和笔记本，以及
    [Yaron Lipman](https://www.youtube.com/watch?v=5ZSwYogAxYg)、[Joey Bose](https://www.youtube.com/watch?v=EPxDI0ytfQU)、[Hannes
    Stärk](https://www.youtube.com/watch?v=Xl7YNR1-CN8) 和 [Alex Tong](https://www.youtube.com/watch?v=UhDtH7Ia9Ag)
    的讲座。
- en: '**Diffusion models** nevertheless continue to be the main workhorse of generative
    modeling in structural biology. In 2023, we saw several landmark works: **FrameDiff**
    ([Yim, Trippe, De Bortoli, Mathieu, et al](https://arxiv.org/abs/2302.02277))
    for protein backbone generation, **EvoDiff** ([Alamdari et al](https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1))
    for generating protein sequences with discrete diffusion, **AbDiffuser** ([Martinkus
    et al](https://arxiv.org/abs/2308.05027)) for full-atom antibody design with frame
    averaging and discrete diffusion (and with successful wet lab experiments), **DiffMaSIF**
    ([Sverrison, Akdel, et al](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf))
    and **DiffDock-PP** ([Ketata, Laue, Mammadov, Stärk, et al](https://arxiv.org/abs/2304.03889))
    for protein-protein docking, **DiffPack** ([Zhang, Zhang, et al](https://arxiv.org/abs/2306.01794))
    for side-chain packing, and the Baker Lab published the **RFDiffusion** **all-atom**
    version ([Krishna, Wang, Ahern, et al](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)).
    Among latent diffusion model (like Stable Diffusion in image generation applications),
    **GeoLDM** ([Xu et al](https://arxiv.org/abs/2305.01140)) was the first for 3D
    molecule conformations, followed by [OmniProt](https://openreview.net/forum?id=DP4NkPZOpD)
    for protein sequence-structure generation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩散模型**仍然是结构生物学中生成建模的主要驱动力。2023年，我们见证了几项具有里程碑意义的工作：**FrameDiff**（[Yim, Trippe,
    De Bortoli, Mathieu等](https://arxiv.org/abs/2302.02277)）用于蛋白质骨架生成，**EvoDiff**（[Alamdari等](https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1)）用于生成具有离散扩散的蛋白质序列，**AbDiffuser**（[Martinkus等](https://arxiv.org/abs/2308.05027)）用于全原子抗体设计，结合框架平均和离散扩散（并通过成功的实验验证），**DiffMaSIF**（[Sverrison,
    Akdel等](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf)）和**DiffDock-PP**（[Ketata,
    Laue, Mammadov, Stärk等](https://arxiv.org/abs/2304.03889)）用于蛋白质-蛋白质对接，**DiffPack**（[Zhang,
    Zhang等](https://arxiv.org/abs/2306.01794)）用于侧链包装，贝克实验室发布了**RFDiffusion** **全原子**版本（[Krishna,
    Wang, Ahern等](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)）。在潜在扩散模型（如图像生成应用中的Stable
    Diffusion）中，**GeoLDM**（[Xu等](https://arxiv.org/abs/2305.01140)）是第一个用于3D分子构象的模型，随后是[OmniProt](https://openreview.net/forum?id=DP4NkPZOpD)用于蛋白质序列-结构生成。'
- en: '![](../Images/67d90c64a0bad7a57857749bd5d60f3d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d90c64a0bad7a57857749bd5d60f3d.png)'
- en: 'FrameDiff: parameterization of the backbone frame with rotation, translation,
    and torsion angle for the oxygen atom. Figure Source: [Yim, Trippe, De Bortoli,
    Mathieu, et al](https://arxiv.org/abs/2302.02277)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: FrameDiff：通过旋转、平移和扭转角度对氧原子的骨架框架进行参数化。图源：[Yim, Trippe, De Bortoli, Mathieu等](https://arxiv.org/abs/2302.02277)
- en: 'Finally, Google DeepMind and Isomorphic Labs [announced](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    **AlphaFold 2.3** — the latest iteration is significantly improving upon the baselines
    in 3 tasks: docking benchmarks (almost 2× better than DiffDock on the new [PoseBusters](https://arxiv.org/abs/2308.05777)
    benchmark), protein-nucleic acid interactions, and antibody-antigen prediction.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，谷歌DeepMind和Isomorphic Labs [宣布](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    **AlphaFold 2.3**——最新版本在三个任务上显著改进了基准：对接基准（比DiffDock在新的[PoseBusters](https://arxiv.org/abs/2308.05777)基准上好近2倍），蛋白质-核酸相互作用，以及抗体-抗原预测。
- en: '***Chaitanya Joshi (Cambridge)***'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***Chaitanya Joshi (剑桥)***'
- en: '💡There have been two emerging trends for biomolecular modeling and design that
    I am very excited about in 2023:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 💡2023年，我非常兴奋地看到生物分子建模和设计领域出现了两个新兴趋势：
- en: 1️⃣ Going from protein structure prediction to conformational ensemble generation.
    There were several interesting approaches to the problem, including [AlphaFold
    with MSA clustering](https://www.nature.com/articles/s41586-023-06832-9), [idpGAN](https://www.nature.com/articles/s41467-023-36443-x),
    [Distributional Graphormer](https://arxiv.org/abs/2306.05445) (a diffusion model),
    and [AlphaFold Meets Flow Matching for Generating Protein Ensembles](https://www.mlsb.io/papers_2023/AlphaFold_Meets_Flow_Matching_for_Generating_Protein_Ensembles.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 从蛋白质结构预测到构象集生成的过渡。针对这个问题有几种有趣的方法，包括 [AlphaFold与MSA聚类](https://www.nature.com/articles/s41586-023-06832-9)，[idpGAN](https://www.nature.com/articles/s41467-023-36443-x)，[Distributional
    Graphormer](https://arxiv.org/abs/2306.05445)（一种扩散模型），以及 [AlphaFold与流匹配结合生成蛋白质集](https://www.mlsb.io/papers_2023/AlphaFold_Meets_Flow_Matching_for_Generating_Protein_Ensembles.pdf)。
- en: '2️⃣ Modelling of biomolecular complexes and design of biomolecular interactions
    among proteins + X: [RFdiffusion all-atom](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)
    and [Ligand MPNN](https://www.biorxiv.org/content/10.1101/2023.12.22.573103v1.full),
    both from the Baker Lab, are representative examples of the trend towards designing
    interactions. The new in-development [AlphaFold report](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    claims that a unified structure prediction model can outperform or match specialised
    models across solo protein and protein complex structure prediction as well as
    protein-ligand and protein-nucleic acid co-folding.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 生物分子复合物建模与蛋白质 + X 的生物分子相互作用设计：[RFdiffusion全原子模型](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)
    和 [Ligand MPNN](https://www.biorxiv.org/content/10.1101/2023.12.22.573103v1.full)，这两者均来自Baker实验室，是朝着设计相互作用趋势的代表性示例。正在开发中的新[AlphaFold报告](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)称，统一的结构预测模型可以在单一蛋白质和蛋白质复合物结构预测以及蛋白质-配体和蛋白质-核酸协同折叠方面，超越或匹配专业化模型的表现。
- en: “However, for all the exciting methodology development in biomolecular modelling
    and design, perhaps the biggest lesson for the ML community this year should be
    to focus more on meaningful **in-silico evaluation** and, if possible, **experimental
    validation**.” — **Chaitanya Joshi** (Cambridge)
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “然而，对于生物分子建模和设计中所有令人兴奋的方法学发展而言，也许今年机器学习社区最大的一课应该是，更加关注有意义的**计算机模拟评估**，如果可能的话，进行**实验验证**。”
    — **Chaitanya Joshi**（剑桥）
- en: 1️⃣ In early 2023, Guolin Ke’s team at DP Technology released two excellent
    re-evaluation papers highlighting how we may have been largely overestimating
    the performance of prominent geometric deep learning-based methods for molecular
    [conformation generation](https://arxiv.org/abs/2302.07061) and [docking](https://arxiv.org/abs/2302.07134)
    w.r.t. traditional baselines.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 在2023年初，Guolin Ke团队在DP Technology发布了两篇出色的再评估论文，突显了我们可能在分子[构象生成](https://arxiv.org/abs/2302.07061)和[对接](https://arxiv.org/abs/2302.07134)方面，相较于传统基准方法，过高估计了以几何深度学习为基础的主流方法的性能。
- en: 2️⃣ [PoseCheck](https://arxiv.org/abs/2308.07413) and [PoseBusters](https://arxiv.org/abs/2308.05777)
    shed further light on the failure modes of current molecular generation and docking
    methods. Critically, generated molecules and their 3D poses are often ‘nonphysical’
    and contain steric clashes, hydrogen placement issues, and high strain energies.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ [PoseCheck](https://arxiv.org/abs/2308.07413) 和 [PoseBusters](https://arxiv.org/abs/2308.05777)
    进一步揭示了当前分子生成和对接方法的失败模式。关键是，生成的分子及其3D姿态往往是“非物理的”，并且包含立体冲突、氢原子位置问题和高应变能。
- en: 3️⃣ Very few papers attempt any experimental validation of new ML ideas. Perhaps
    collaborating with a wet lab is challenging for those focussed on new methodology
    development, but I hope that us ML-ers, as a community, will at least be a lot
    more cautious about the in-silico evaluation metrics we are constantly pushing
    as we create new models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 很少有论文尝试对新的机器学习思想进行实验验证。也许对于那些专注于新方法开发的人来说，与湿实验室的合作比较具有挑战性，但我希望作为一个机器学习社区，我们至少能在不断推动新模型的过程中，更加谨慎地对待我们常用的计算机模拟评估指标。
- en: '***Hannes Stärk (MIT)***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***Hannes Stärk (MIT)***'
- en: 💡I am reading quite some hype here about Flow Matching, stochastic interpolants,
    and Rectified Flows (I will call them “Bridge Matching,” or “BM”). I do not think
    there is much value in just replacing diffusion models with BM in all the existing
    applications. For pure generative modeling, the main BM advantage is simplicity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 💡我看到关于流匹配、随机插值和整流流（我将其称为“桥接匹配”，或“BM”）的讨论颇为火热。我不认为仅仅在所有现有应用中用BM替换扩散模型有什么太大价值。对于纯生成建模，BM的主要优势是简洁性。
- en: I think we should instead be excited about BM for the new capabilities it unlocks.
    For example, training bridges between arbitrary distributions in a simulation-free
    manner (what are the best applications for this? I basically only saw [retrosynthesis](https://arxiv.org/abs/2308.16212)
    so far.) or solving OT problems as in [DSBM](https://arxiv.org/abs/2303.16852)
    that does so for fluid flow downscaling. Maybe a lot of tools emerged in 2023
    (also let us mention [BM with multiple marginals](https://arxiv.org/abs/2310.03695)),
    and in 2024, the community will make good use of them?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们应该更加兴奋的是BM所解锁的新能力。例如，无需仿真即可在任意分布之间训练桥接（这个方法最适合哪些应用呢？到目前为止，我只看到了[逆合成](https://arxiv.org/abs/2308.16212)）。或者解决像[DSBM](https://arxiv.org/abs/2303.16852)这样针对流体流动下采样的OT问题。也许2023年出现了许多工具（我们还可以提到[具有多个边际的BM](https://arxiv.org/abs/2310.03695)），而到2024年，社区将更好地利用它们？
- en: '***Joey Bose (Mila & Dreamfold)***'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***乔伊·博斯 (Mila & Dreamfold)***'
- en: 💡 This year we have really seen the rise of geometric generative models from
    theory to practice. A few standouts for me include [Riemannian Flow Matching](https://arxiv.org/abs/2302.03660)
    — in general any paper by Ricky Chen and Yaron Lipman on these topics is a must-read
    — and FrameDiff from [Yim et. al](https://arxiv.org/abs/2302.02277) which introduced
    a lot of the important machinery for protein backbone generation. Of course, standing
    on the shoulders of both RFM and FrameDIff, we built [FoldFlow](https://arxiv.org/abs/2310.02391),
    a cooler flow-matching approach to protein generative models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 今年我们确实看到了几何生成模型从理论到实践的崛起。几项突出的工作包括[黎曼流匹配](https://arxiv.org/abs/2302.03660)——一般来说，任何由Ricky
    Chen和Yaron Lipman撰写的关于这些主题的论文都值得一读——以及[Yim等人](https://arxiv.org/abs/2302.02277)的FrameDiff，它引入了很多蛋白质骨架生成的重要机制。当然，在RFM和FrameDiff的基础上，我们构建了[FoldFlow](https://arxiv.org/abs/2310.02391)，这是一种更酷的流匹配方法，用于蛋白质生成模型。
- en: “Looking ahead, I foresee a lot **more flow matching**-based approaches coming
    into use. They are better for proteins and longer sequences and can start from
    any source distribution.” — Joey Bose (Mila & Dreamfold)
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “展望未来，我预见到更多基于**流匹配**的方法将会投入使用。它们对于蛋白质和较长的序列更有效，并且可以从任何源分布开始。” — 乔伊·博斯 (Mila
    & Dreamfold)
- en: 🔮 Moreover, I suspect we will soon see **multi-modal generative models** in
    this space, such as discrete + continuous models and also conditional models in
    the same vein as text-conditioned diffusion models for images. Perhaps, we might
    even see **latent generative models** here given that they scale so well!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 此外，我怀疑我们很快将在这个领域看到**多模态生成模型**，例如离散+连续模型，以及类似于图像的文本条件扩散模型的条件模型。也许，考虑到它们的扩展性，我们甚至可能会在这里看到**潜在生成模型**！
- en: '***Minkai Xu (Stanford)***'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***敏凯·徐 (斯坦福大学)***'
- en: “This year, the community has further pushed forward the geometric generative
    models for 3D molecular generation in many perspectives.” — Minkai Xu (Stanford)
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “今年，社区在多个方面进一步推动了几何生成模型在3D分子生成中的应用。” — 敏凯·徐 (斯坦福大学)
- en: '**Flow matching**: Ricky and Yaron proposed the Flow Matching method as an
    alternative to the widely used diffusion models, and EquiFM ([Song et al](https://openreview.net/forum?id=hHUZ5V9XFu)
    and [Klein et al](https://arxiv.org/abs/2306.15030)) realize the variant for 3D
    molecule generation by parameterizing the flow dynamics with equivariant GNNs.
    In the meantime, [FrameFlow](https://arxiv.org/pdf/2310.05297.pdf) and [FoldFlow](https://arxiv.org/abs/2310.02391)
    construct FM models for protein generation.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**流匹配**：Ricky和Yaron提出了流匹配方法，作为广泛使用的扩散模型的替代方案，EquiFM（[Song等人](https://openreview.net/forum?id=hHUZ5V9XFu)
    和 [Klein等人](https://arxiv.org/abs/2306.15030)）通过使用等变GNN来参数化流动动力学，从而实现了3D分子生成的变种。与此同时，[FrameFlow](https://arxiv.org/pdf/2310.05297.pdf)和[FoldFlow](https://arxiv.org/abs/2310.02391)构建了用于蛋白质生成的FM模型。'
- en: 🔮Moving forward similar to vision and text domain, people begin to explore generation
    in the lower-dimensional latent space instead of the complex original data space
    (**latent generative models**). GeoLDM ([Xu et al](https://arxiv.org/abs/2305.01140))
    proposed the first latent diffusion model (like Stable Diffusion in CV) for 3D
    molecule generation, while [Fu et al](https://arxiv.org/abs/2305.04120) enjoys
    similar modeling formulation for large protein generation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 展望未来，类似于视觉和文本领域，人们开始探索在低维潜在空间中进行生成，而不是在复杂的原始数据空间中进行生成（**潜在生成模型**）。GeoLDM（[Xu等人](https://arxiv.org/abs/2305.01140)）提出了第一个潜在扩散模型（类似于CV中的稳定扩散），用于3D分子生成，而[Fu等人](https://arxiv.org/abs/2305.04120)则在大型蛋白质生成中使用了类似的建模方法。
- en: A Structural Biologist’s Perspective
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构生物学家的视角
- en: '*Bruno Correia (EPFL)*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*布鲁诺·科雷亚 (EPFL)*'
- en: “Current generative models still create “garbage” outputs that violate many
    of the physical and chemical properties that molecules are known to have. The
    advantage of current generative models is, of course, their speed which affords
    them the possibility of generating many samples, which then brings to front and
    center the ability to filter the best generated samples, which in the case of
    protein design has benefited immensely from the transformative development of
    AlphaFold2.” — Bruno Correia (EPFL)
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “目前的生成模型仍然会产生‘垃圾’输出，这些输出违反了分子已知的许多物理和化学属性。当然，当前生成模型的优势在于它们的速度，这使得它们能够生成大量样本，从而使得筛选最佳生成样本的能力变得尤为重要，在蛋白质设计的领域，AlphaFold2的变革性发展对此带来了巨大的帮助。”
    — 布鲁诺·科雷亚（EPFL）
- en: ➡️ The next challenge to the community will perhaps be how to infuse generative
    models with **meaningful physical and chemical priors** to enhance sampling performance
    and generalization. Interestingly, we have not seen the same remarkable advances
    (experimentally validated) in applications to small molecule design, which we
    hope to see during 2024.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 对社区的下一个挑战或许是如何将**有意义的物理和化学先验**注入生成模型，以提高采样性能和泛化能力。有趣的是，在小分子设计应用中，我们尚未看到同样显著的进展（经实验验证），但我们希望在2024年能看到这一突破。
- en: ➡️ **The rise of multimodal models.** Generally in biological-related tasks
    data sparsity is a given and as such strategies to extract the most signal out
    of the data are essential. One way to try to overcome such limitations is to improve
    the expressiveness of the data representations and maybe this way obtain more
    performant neural networks. Likely in the short term, we will be able to explore
    architectures that encompass several types of representations of the objects of
    interest and harness the best predictions for the evermore complex tasks we are
    facing as progressively more of the basic problems get solved. This notion of
    multimodality is of course intimately related to the overall aim of having models
    with stronger priors, that in a generative context, honour fundamental constraints
    of the objects of interest.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ **多模态模型的崛起。** 通常在生物学相关任务中，数据稀缺性是一个普遍问题，因此，从数据中提取最大信号的策略至关重要。克服这种局限性的一种方式是提高数据表示的表现力，也许通过这种方式可以获得更高效的神经网络。短期内，我们可能能够探索涵盖多种对象表示的架构，并利用最优预测来解决我们面临的越来越复杂的任务，随着越来越多基础问题的解决，这一点变得尤为重要。多模态性的概念当然与拥有更强先验的模型的总体目标密切相关，在生成任务中，这些模型尊重对象的基本约束。
- en: ➡️ **The models that know everything**. As the power of machine learning models
    improves we clearly tend to request a more multi-objective optimization when it
    comes to attempting to solve real life problems. Taking as an example small molecule
    generation, thinking from a biochemical perspective the drug design problem starts
    by having a target to which a small molecule binds, therefore one of the first
    and most important constraints is that the generative process ought to be conditioned
    to the protein pocket. However, such a constraint may not be enough to create
    real small molecules as many of such chemicals are simply impossible or very hard
    to synthesize, and, therefore, a model that has notions of chemical synthesizability
    and can integrate such constraints in the search space would be much more useful.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ **全知模型。** 随着机器学习模型能力的提升，我们显然倾向于请求更多的多目标优化，以解决现实生活中的问题。以小分子生成为例，从生物化学的角度来看，药物设计问题首先是确定一个靶点，药物分子需要与该靶点结合，因此，生成过程的首要约束条件之一是它应当以蛋白质口袋为条件。然而，这样的约束可能不足以生成真实的小分子，因为许多这样的化学物质根本无法合成或极其难以合成，因此，具备化学合成可行性概念并能将这些约束集成到搜索空间中的模型将更加有用。
- en: ➡️ **From chemotype to phenotype**. On the grounds of data representation, atomic
    graph structures together with vector embeddings have reached remarkable results,
    particularly in the search for new antibiotics. Making accurate predictions of
    which chemical structures have antimicrobial activity, broadly speaking, is an
    exercise of phenotype prediction from chemical structure. Due to the simplicity
    of the approaches used and the impressive results obtained, one would expect that
    more sophisticated data representations on the molecule end and perhaps together
    also with richer phenotype assignment could give critical contributions to such
    an important problem in drug development.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ **从化学型到表型**。在数据表示方面，原子图结构和向量嵌入技术取得了显著成果，尤其是在寻找新抗生素方面。广义上来说，准确预测哪些化学结构具有抗菌活性，是从化学结构到表型预测的一个重要练习。由于所采用方法的简便性和取得的令人印象深刻的成果，可以预期，若能在分子端采用更复杂的数据表示，并且可能结合更丰富的表型分配，这将对药物开发中的这一重要问题作出关键贡献。
- en: Industrial perspective
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工业视角
- en: '***Luca Naef (VantAI)***'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***Luca Naef (VantAI)***'
- en: 🔥*What are the biggest advancements in the field you noticed in 2023?*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥*2023年你注意到的领域里最大的进展是什么？*
- en: '1️⃣ **Increasing multi-modality & modularity** — as shown by the emergence
    of initial co-folding methods for both proteins & small molecules, diffusion and
    non-diffusion-based, to extend on AF2 success: [DiffusionProteinLigand](https://www.biorxiv.org/content/10.1101/2022.12.20.521309v1.full.pdf)
    in the last days of 2022 and [RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1),
    [AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    and [Umol](https://www.biorxiv.org/content/10.1101/2023.11.03.565471v1) by end
    of 2023\. We are also seeing models that have sequence & structure co-trained:
    [SAProt](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2), [ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v1),
    and sequence, structure & surface co-trained with [ProteinINR](https://www.mlsb.io/papers_2023/Pre-training_Sequence_Structure_and_Surface_Features_for_Comprehensive_Protein_Representation_Learning.pdf).
    There is a general revival of surface-based methods after a quieter 2021 and 2022:
    [DiffMasif](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf),
    [SurfDock](https://arxiv.org/abs/2311.17050), and [ShapeProt](https://www.biorxiv.org/content/10.1101/2023.12.03.567710v1).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ **多模态性和模块化的增加** —— 如蛋白质和小分子的初步共折叠方法的出现所示，包括扩展AF2成功的扩散和非扩散基础方法：[DiffusionProteinLigand](https://www.biorxiv.org/content/10.1101/2022.12.20.521309v1.full.pdf)在2022年最后几天发布，和[RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)，[AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)以及[Umol](https://www.biorxiv.org/content/10.1101/2023.11.03.565471v1)预计在2023年底发布。我们也看到了一些模型在序列与结构共同训练的基础上进行训练：[SAProt](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2)，[ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v1)，以及在[ProteinINR](https://www.mlsb.io/papers_2023/Pre-training_Sequence_Structure_and_Surface_Features_for_Comprehensive_Protein_Representation_Learning.pdf)模型下，序列、结构与表面共同训练。经过2021年和2022年相对平静的时期，基于表面的方法重新获得了关注：[DiffMasif](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf)，[SurfDock](https://arxiv.org/abs/2311.17050)，和[ShapeProt](https://www.biorxiv.org/content/10.1101/2023.12.03.567710v1)。
- en: '2️⃣ **Datasets and benchmarks**. Datasets, especially synthetic/computationally
    derived: [ATLAS](https://academic.oup.com/nar/article/52/D1/D384/7438909) and
    the [MDDB](https://mddbr.eu/) for protein dynamics. [MISATO](https://www.biorxiv.org/content/10.1101/2023.05.24.542082v1),
    [SPICE](https://www.nature.com/articles/s41597-022-01882-6), [Splinter](https://www.nature.com/articles/s41597-023-02443-1)
    for protein-ligand complexes, [QM1B](https://arxiv.org/abs/2311.01135) for molecular
    properties. PINDER: large protein-protein docking dataset with matched apo/predicted
    pairs and benchmark suite with retrained docking models. [CryoET data portal](https://chanzuckerberg.github.io/cryoet-data-portal/index.html#)
    for CryoET. And a whole host of welcome benchmarks: PINDER, [PoseBusters](https://arxiv.org/abs/2308.05777),
    and [PoseCheck](https://arxiv.org/abs/2308.07413), with a focus on more rigorous
    and practically relevant settings.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ **数据集和基准测试**。数据集，尤其是合成的/计算生成的数据集：[ATLAS](https://academic.oup.com/nar/article/52/D1/D384/7438909)
    和 [MDDB](https://mddbr.eu/) 用于蛋白质动态学。[MISATO](https://www.biorxiv.org/content/10.1101/2023.05.24.542082v1)，[SPICE](https://www.nature.com/articles/s41597-022-01882-6)，[Splinter](https://www.nature.com/articles/s41597-023-02443-1)
    用于蛋白质-配体复合物，[QM1B](https://arxiv.org/abs/2311.01135) 用于分子属性。PINDER：一个大型蛋白质-蛋白质对接数据集，包含匹配的空载/预测对和基准套件，附带重新训练的对接模型。[CryoET
    数据门户](https://chanzuckerberg.github.io/cryoet-data-portal/index.html#) 用于冷冻电子断层扫描（CryoET）。以及一系列受欢迎的基准测试：PINDER、[PoseBusters](https://arxiv.org/abs/2308.05777)
    和 [PoseCheck](https://arxiv.org/abs/2308.07413)，重点关注更加严格和实际相关的设置。
- en: 3️⃣ **Creative pre-training strategies** to get around the sparsity of diverse
    protein-ligand complexes. Van-der-mers training ([DockGen](https://openreview.net/forum?id=UfBIxpTK10))
    & sidechain training strategies in [RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)
    and pre-training on ligand-only complexes in CCD in [RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1).
    Multi-task pre-training [Unimol](https://openreview.net/forum?id=6K2RM6wVqKu)
    and others.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ **创造性的预训练策略**，以克服蛋白质-配体复合物的稀缺性。范德梅尔训练（[DockGen](https://openreview.net/forum?id=UfBIxpTK10)）和[RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)中的侧链训练策略，以及在[RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)中的配体单独复合物预训练。多任务预训练，如[Unimol](https://openreview.net/forum?id=6K2RM6wVqKu)及其他。
- en: 🏋️ *What are the open challenges that researchers might overlook?*
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋️ *研究人员可能忽视的开放性挑战是什么？*
- en: 1️⃣ **Generalization.** [DockGen](https://openreview.net/forum?id=UfBIxpTK10)showed
    that current state-of-the-art protein-ligand docking models completely lose predictability
    when asked to generalise towards novel protein domains. We see a similar phenomenon
    in the [AlphaFold-lastest report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf),
    where performance on novel proteins & ligands drops heavily to below biophysics-based
    baselines (which have access to holo structures), despite very generous definitions
    of novel protein & ligand. This indicates that existing approaches might still
    largely rely on memorization, an observation that has been extensively argued
    over the [years](https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00487)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ **泛化能力**。[DockGen](https://openreview.net/forum?id=UfBIxpTK10)展示了当前最先进的蛋白质-配体对接模型在试图泛化到新蛋白质领域时完全失去了预测能力。在[AlphaFold最新报告](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf)中，我们看到了类似的现象，新蛋白质和配体的表现显著下降，甚至低于基于生物物理学的基线（这些基线可以访问完整结构），尽管对新蛋白质和配体的定义非常宽泛。这表明现有的方法仍然很大程度上依赖于记忆，这一观察已在[多年](https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00487)的讨论中得到了广泛论证。
- en: '2️⃣ **The curse of (simple) baselines.** A recurring topic over the years,
    2023 has again shown what industry practitioners have long known: in many practical
    problems such as molecular generation, property prediction, docking, and conformer
    prediction, simple baselines or classical approaches often still outperform ML-based
    approaches in practice. This has been documented increasingly in 2023 by [Tripp
    et al.](https://arxiv.org/abs/2310.09267)**,** [Yu et al.](https://arxiv.org/abs/2302.07134),
    [Zhou et al.](https://arxiv.org/abs/2302.07061)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ **（简单）基线的诅咒**。这是一个多年来反复出现的话题，2023年再次证明了业界从业者早已知道的一点：在许多实际问题中，如分子生成、属性预测、对接和构象预测，简单的基线或经典方法往往在实践中仍优于基于机器学习的方法。2023年，[Tripp等人](https://arxiv.org/abs/2310.09267)**，**
    [Yu等人](https://arxiv.org/abs/2302.07134)，[Zhou等人](https://arxiv.org/abs/2302.07061)对此有越来越多的文献记录。
- en: 🔮 *Predictions for 2024!*
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 *2024年的预测！*
- en: “In 2024, data sparsity will remain top of mind and we will see a lot of smart
    ways to use models to generate synthetic training data. Self-distillation in AlphaFold2
    served as a big inspiration, Confidence Bootstrapping in [DockGen](https://openreview.net/forum?id=UfBIxpTK10),
    leveraging the insight that we now have sufficiently powerful models that can
    score poses but not always generate them, first realised in [2022](https://www.biorxiv.org/content/10.1101/2022.03.11.484043v1).”
    — Luca Naef (VantAI)
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在2024年，数据稀缺问题仍将是我们关注的重点，我们将看到很多智能方法来使用模型生成合成训练数据。AlphaFold2中的自蒸馏为我们提供了重要的启示，[DockGen](https://openreview.net/forum?id=UfBIxpTK10)中的置信引导方法，利用了我们现在已经拥有足够强大的模型，这些模型能够评分姿势，但并不总是能够生成姿势，这一发现最早在[2022年](https://www.biorxiv.org/content/10.1101/2022.03.11.484043v1)中提出。”
    — Luca Naef (VantAI)
- en: 2️⃣ We will see more biological/chemical assays purpose-built for ML or only
    making sense in a machine learning context (i.e., might not lead to biological
    insight by themselves but be primarily useful for training models). An example
    from 2023 is the large-scale protein folding experiments by [Tsuboyama et al.](https://www.nature.com/articles/s41586-023-06328-6)
    This move might be driven by techbio startups, where we have seen the first foundation
    models built on such ML-purpose-built assays for structural biology with e.g.
    [ATOM-1](https://www.biorxiv.org/content/10.1101/2023.12.13.571579v1).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 我们将看到更多为机器学习专门设计的生物/化学实验，或仅在机器学习的背景下才有意义（即，这些实验可能本身不会带来生物学上的新见解，但主要用于训练模型）。2023年的一个例子是[Tsuboyama等人](https://www.nature.com/articles/s41586-023-06328-6)的大规模蛋白质折叠实验。这个举措可能受到科技生物初创公司的推动，我们已经看到基于这些专门为结构生物学设计的机器学习实验构建的首个基础模型，例如[ATOM-1](https://www.biorxiv.org/content/10.1101/2023.12.13.571579v1)。
- en: '***Andreas Loukas (Prescient Design, part of Genentech)***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***Andreas Loukas (Prescient Design, 属于基因泰克的一部分)***'
- en: 🔥 *What are the biggest advancements in the field you noticed in 2023?*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 *你在2023年注意到的该领域最大进展是什么？*
- en: “In 2023, we started to see some of the challenges of equivariant generation
    and representation for proteins to be resolved through diffusion models.” — Andreas
    Loukas (Prescient Design)
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在2023年，我们开始看到一些关于蛋白质等变生成和表示的挑战通过扩散模型得以解决。” — Andreas Loukas (Prescient Design)
- en: 1️⃣ We also noticed a **shift towards approaches that model and generate molecular
    systems at higher fidelity**. For instance, the most recent models adopt a fully
    end-to-end approach by generating backbone, sequence and side-chains jointly ([AbDiffuser](https://openreview.net/pdf?id=7GyYpomkEa),
    [dyMEAN](https://arxiv.org/pdf/2302.00203.pdf)) or at least solve the problem
    in two steps but with a partially joint model ([Chroma](https://www.nature.com/articles/s41586-023-06728-8));
    as compared to backbone generation followed by inverse folding as in [RFDiffusion](https://www.nature.com/articles/s41586-023-06415-8)
    and [FrameDiff](https://openreview.net/pdf?id=m8OUBymxwv). Other attempts to improve
    the modelling fidelity can be found in the latest updates to co-folding tools
    like [AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    and [RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)
    which render them sensitive to non-protein components (ligands, prosthetic groups,
    cofactors); as well as in papers that attempt to account for conformational dynamics
    (see discussion above). In my view, this line of work is essential because the
    binding behaviour of molecular systems can be very sensitive to how atoms are
    placed, move, and interact.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 我们还注意到了一种**向更高保真度的分子系统建模与生成方法转变**。例如，最新的模型采用完全端到端的方法，通过联合生成主链、序列和侧链（[AbDiffuser](https://openreview.net/pdf?id=7GyYpomkEa),
    [dyMEAN](https://arxiv.org/pdf/2302.00203.pdf)），或者至少通过一个部分联合的模型在两步中解决问题（[Chroma](https://www.nature.com/articles/s41586-023-06728-8)）；相比之下，主链生成后再进行反向折叠，如在[RF
    Diffusion](https://www.nature.com/articles/s41586-023-06415-8)和[FrameDiff](https://openreview.net/pdf?id=m8OUBymxwv)中所述。其他改善建模保真度的尝试可以在最新的共折叠工具更新中找到，如[AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)和[RF
    Diffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)，它们使模型对非蛋白质成分（配体、辅基、辅因子）更为敏感；同时也可以在一些试图考虑构象动力学的论文中找到（见上文讨论）。在我看来，这一研究方向至关重要，因为分子系统的结合行为对原子如何放置、移动和相互作用非常敏感。
- en: 2️⃣ In 2023, many works also attempted to get a handle on **binding affinity**
    by learning to predict the effect of mutations of a known crystal by pre-training
    on large corpora, such as computationally predicted mutations ([graphinity](https://github.com/oxpig/Graphinity)),
    and on side-tasks, such as [rotamer density estimation](https://openreview.net/pdf?id=_X9Yl1K2mD).
    The obtained results are encouraging as they can significantly outperform semi-empirical
    baselines like Rosetta and FoldX. However, there is still significant work to
    be done to render these models reliable for binding affinity prediction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 在2023年，许多研究也尝试通过学习预测已知晶体突变的效应来掌握**结合亲和力**，通过在大型语料库上进行预训练，如计算预测的突变（[graphinity](https://github.com/oxpig/Graphinity)），以及侧任务，如[旋转体密度估计](https://openreview.net/pdf?id=_X9Yl1K2mD)。所获得的结果令人鼓舞，因为它们可以显著超越如Rosetta和FoldX等半经验基线。然而，仍然有大量工作需要完成，以使这些模型在结合亲和力预测中更加可靠。
- en: '3️⃣ I have further observed a growing recognition of **protein Language Models
    (pLMs)**and specifically [ESM](https://www.science.org/doi/10.1126/science.ade2574)
    as valuable tools, even among those who primarily favour geometric deep learning.
    These embeddings are used to help docking models, allow the construction of simple
    yet competitive predictive models for binding affinity prediction ([Li et al 2023](https://www.nature.com/articles/s41467-023-39022-2)),
    and can generally offer an efficient method to create residue representations
    for GNNs that are informed by the extensive proteome data without the need for
    extensive pretraining ([Jamasb et al 2023](https://www.mlsb.io/papers_2023/Evaluating_Representation_Learning_on_the_Protein_Structure_Universe.pdf)).
    However, I do maintain a concern regarding the use of pLMs: it is unclear whether
    their effectiveness is due to data leakage or genuine generalisation. This is
    particularly pertinent when evaluating models on tasks like amino-acid recovery
    in inverse folding and conditional CDR design, where distinguishing between these
    two factors is crucial.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 我进一步观察到，**蛋白质语言模型（pLMs）**，特别是[ESM](https://www.science.org/doi/10.1126/science.ade2574)，作为有价值的工具，得到了越来越多的认可，即使在那些主要偏好几何深度学习的人群中。这些嵌入被用来帮助对接模型，允许构建简单但具有竞争力的结合亲和力预测模型（[Li
    et al 2023](https://www.nature.com/articles/s41467-023-39022-2)），并且通常能提供一种高效的方式，利用广泛的蛋白质组数据为GNNs创建残基表示，而无需大量的预训练（[Jamasb
    et al 2023](https://www.mlsb.io/papers_2023/Evaluating_Representation_Learning_on_the_Protein_Structure_Universe.pdf)）。然而，我确实对使用pLMs存在担忧：目前尚不清楚它们的有效性是由于数据泄漏还是由真正的泛化能力所致。在评估如逆折叠中的氨基酸恢复和条件CDR设计等任务时，这一点尤其重要，因为区分这两者的因素至关重要。
- en: 🏋️ *What are the open challenges that researchers might overlook?*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋️ *研究人员可能忽视了哪些开放性挑战？*
- en: 1️⃣ Working with **energetically relaxed crystal structures** (and, even worse,
    folded structures) can significantly affect the performance of downstream predictive
    models. This is especially true for the prediction of protein-protein interactions
    (PPIs). In my experience, the performance of PPI predictors severely deteriorates
    when they are given a relaxed structure as opposed to the binding (holo) crystalised
    structure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 与**能量放松的晶体结构**（甚至更糟，折叠结构）一起工作可能会显著影响下游预测模型的性能。对于蛋白质-蛋白质相互作用（PPI）的预测尤为如此。根据我的经验，当预测模型给定的是放松后的结构而非结合（全）晶体结构时，PPI预测器的性能会严重下降。
- en: 2️⃣ Though successful *in silico* antibody design has the capacity to revolutionise
    drug design, **general protein models are not (yet?) as good at folding, docking
    or generating antibodies as antibody-specific models are**. This is perhaps due
    to the low conformational variability of the antibody fold and the distinct binding
    mode between antibodies and antigens (loop-mediated interactions that can involve
    a non-negligible entropic component). Perhaps for the same reasons, the *de novo*
    design of antibody binders (that I define as 0-shot generation of an antibody
    that binds to a previously unseen epitope) remains an open problem. Currently,
    experimentally confirmed cases of *de novo* binders involve mostly stable proteins,
    like [alpha-helical bundles](https://www.nature.com/articles/s41586-023-06415-8),
    that are common in the PDB and harbour interfaces that differ substantially from
    epitope-paratope interactions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 尽管成功的*计算机辅助*抗体设计有潜力彻底改变药物设计，但**通用蛋白质模型（还？）不如专门针对抗体的模型在折叠、对接或生成抗体方面表现得那么好**。这可能是由于抗体折叠的低构象变异性和抗体与抗原之间不同的结合方式（涉及环介导的相互作用，并可能包含不可忽视的熵项）。也许由于同样的原因，抗体结合物的*de
    novo*设计（我定义为零样本生成能与之前未见的表位结合的抗体）仍然是一个未解的难题。目前，经过实验验证的*de novo*结合物案例大多涉及稳定的蛋白质，如[α-螺旋束](https://www.nature.com/articles/s41586-023-06415-8)，它们在PDB中较为常见，且包含的界面与表位-抗表位相互作用有显著不同。
- en: '3️⃣ **We are still lacking a general-purpose proxy for binding free energy***.*
    The main issue here is the lack of high-quality data of sufficient size and diversity
    (esp. co-crystal structures). We should therefore be cognizant of the limitations
    of any such learned proxy for any model evaluation: though predicted binding scores
    that are out of distribution of known binders is a clear signal that something
    is off, we should avoid the typical pitfall of trying to demonstrate the superiority
    of our model in an empirical evaluation by showing how it leads to even higher
    scores.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ **我们仍然缺乏一种通用的结合自由能代理模型***。问题在于缺乏足够大且多样化的高质量数据（特别是共晶结构）。因此，我们应该意识到，任何此类学习代理模型在评估模型时的局限性：尽管预测的结合评分超出已知结合物分布范围是明显的信号，表明某些地方可能出了问题，但我们应避免陷入典型的误区，即通过展示模型如何导致更高的评分来证明我们模型的优越性。
- en: '***Dominique Beaini (Valence Labs, part of Recursion)***'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '***多米尼克·贝阿尼（Valence Labs，Recursion的一部分）***'
- en: “I’m excited to see a very large community being built around the problem of
    drug discovery, and I feel we are on the brink of a new revolution in the speed
    and efficiency of discovering drugs.” — Dominique Beaini (Valence Labs)
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我很高兴看到围绕药物发现问题建立起了一个庞大的社区，并且我感到我们正处于药物发现速度和效率新革命的边缘。” — 多米尼克·贝阿尼（Valence Labs）
- en: '*What work got me excited in 2023?*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*2023年有哪些工作让我感到兴奋？*'
- en: I am confident that machine learning will allow us to tackle rare diseases quickly,
    stop the next COVID-X pandemic before it can spread, and live longer and healthier.
    But there’s a lot of work to be done and there are a lot of challenges ahead,
    some bumps in the road, and some canyons on the way. Speaking of communities,
    you can visit the [Valence Portal](https://portal.valencelabs.com/) to keep up-to-date
    with the 🔥 new in ML for drug discovery.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我相信机器学习将使我们能够快速应对罕见病，阻止下一场COVID-X大流行在蔓延之前爆发，并使我们活得更久、更健康。但仍有大量工作要做，前方有许多挑战，有一些障碍和一些峡谷。说到社区，您可以访问[Valence
    Portal](https://portal.valencelabs.com/)以跟上药物发现领域机器学习中的🔥新进展。
- en: '*What are the hard questions for 2024?*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*2024年有哪些难题？*'
- en: '⚛️ **A new generation of quantum mechanics.** Machine learning force-fields,
    often based on equivariant and invariant GNNs, have been promising us a treasure.
    The treasure of the precision of density functional theory, but thousands of times
    faster and at the scale of entire proteins. Although some steps were made in this
    direction with [Allegro](https://link.springer.com/chapter/10.1007/978-3-031-32041-5_12)
    and [MACE-MP](https://arxiv.org/pdf/2401.00096.pdf), current models do not generalize
    well to unseen settings and very large molecules, and they are still too slow
    to be applicable on the timescale that is needed 🐢. For the generalization, I
    believe that bigger and more diverse datasets are the most important stepping
    stones. For the computation time, I believe we will see models that are less enforcing
    of the equivariance, such as [FAENet](https://arxiv.org/pdf/2305.05577.pdf). But
    efficient sampling methods will play a bigger role: spatial-sampling such as using
    [DiffDock](https://arxiv.org/abs/2210.01776) to get more interesting starting
    points and time-sampling such as [TimeWarp](https://www.microsoft.com/en-us/research/publication/timewarp-transferable-acceleration-of-molecular-dynamics-by-learning-time-coarsened-dynamics/)
    to avoid simulating every frame. I’m really excited by the big STEBS 👣 awaiting
    us in 2024: Spatio-temporal equivariant Boltzmann samplers.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ⚛️ **新一代量子力学。** 基于等变和不变图神经网络（GNNs）的机器学习力场，一直向我们承诺着一笔宝藏。那就是密度泛函理论的精度，但速度要快上千倍，并且适用于整个蛋白质的尺度。虽然[Allegro](https://link.springer.com/chapter/10.1007/978-3-031-32041-5_12)和[MACE-MP](https://arxiv.org/pdf/2401.00096.pdf)等方向上已取得了一些进展，但目前的模型在未见过的情境和超大分子上泛化性较差，而且它们的计算速度仍然太慢，无法满足所需的时间尺度🐢。对于泛化问题，我相信更大且更多样化的数据集是最重要的突破点。对于计算时间，我认为我们将看到一些不那么强求等变性的模型，例如[FAENet](https://arxiv.org/pdf/2305.05577.pdf)。但高效的采样方法将发挥更大的作用：空间采样，比如使用[DiffDock](https://arxiv.org/abs/2210.01776)来获得更有趣的起始点，以及时间采样，如[TimeWarp](https://www.microsoft.com/en-us/research/publication/timewarp-transferable-acceleration-of-molecular-dynamics-by-learning-time-coarsened-dynamics/)来避免模拟每一帧。我对2024年即将到来的重大突破充满期待：时空等变的Boltzmann采样器（STEBS）👣。
- en: '🕸️ **Everything is connected. Biology is inherently multimodal 🙋🐁 🧫🧬🧪.** One
    cannot simply decouple the molecule from the rest of the biological system. Of
    course, that’s how ML for drug discovery was done in the past: simply build a
    model of the molecular graph and fit it to experimental data. But we have reached
    a critical point 🛑, no matter how many trillion parameters are in the GNN model
    is, and how much data are used to train it, and how many experts are mixtured
    together. It is time to bring biology into the mix, and the most straightforward
    way is with multi-modal models. One method is to condition the output of the GNNs
    with the target protein sequences such as [MocFormer](https://www.biorxiv.org/content/10.1101/2023.09.13.557595v4.abstract).
    Another is to use microscopy images or transcriptomics to better inform the model
    of the biological signature of molecules such as [TranSiGen](https://www.biorxiv.org/content/10.1101/2023.11.12.566777v1.full).
    Yet another is to use LLMs to embed contextual information about the tasks such
    as [TwinBooster](https://arxiv.org/pdf/2401.04478.pdf). Or even better, combining
    all of these together 🤯, but this could take years. The main issue for the broader
    community seems to be the availability of large amounts of quality and standardized
    data, but fortunately, this is not an issue for Valence.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 🕸️ **一切都是相互联系的。生物学本质上是多模态的🙋🐁 🧫🧬🧪。** 无法简单地将分子与其他生物系统解耦。当然，这就是过去药物发现中的机器学习方法：仅仅构建分子图模型并将其拟合到实验数据中。但我们已经达到了一个关键点🛑，无论图神经网络（GNN）模型中有多少万亿个参数，使用了多少数据进行训练，或者汇聚了多少专家团队，它都无法解决问题。是时候将生物学融入其中，最直接的方式就是采用多模态模型。一种方法是用目标蛋白质序列来调节GNN的输出，例如[MocFormer](https://www.biorxiv.org/content/10.1101/2023.09.13.557595v4.abstract)。另一种方法是使用显微镜图像或转录组学来更好地为模型提供分子的生物学特征信息，例如[TranSiGen](https://www.biorxiv.org/content/10.1101/2023.11.12.566777v1.full)。还有一种方法是使用大语言模型（LLMs）来嵌入关于任务的上下文信息，例如[TwinBooster](https://arxiv.org/pdf/2401.04478.pdf)。甚至更好的是，将这些方法结合起来🤯，但这可能需要几年时间。对于更广泛的社区而言，主要问题似乎是缺乏大量高质量和标准化的数据，但幸运的是，这对Valence来说并不是问题。
- en: '**🔬 Relating biological knowledge and observables.** Humans have been trying
    to map biology for a long time, building relational maps for genes 🧬, protein-protein
    interactions 🔄, metabolic pathways 🔀, etc. I invite you to read this [review of
    knowledge graphs for drug discovery](https://academic.oup.com/bib/article/23/6/bbac404/6712301).
    But all this knowledge often sits unused and ignored by the ML community. I feel
    that this is an area where GNNs for knowledge graphs could prove very useful,
    especially in 2024, and it could provide another modality for the 🕸️ point above.
    Considering that human knowledge is incomplete, we can instead recover relational
    maps from foundational models. This is the route taken by [Phenom1](https://arxiv.org/abs/2309.16064)
    when trying to recall known genetic relationships. However, having to deal with
    various knowledge databases is an extremely complex task that we can’t expect
    most ML scientists to be able to tackle alone. But with the help of artificial
    assistants like [LOWE](https://www.valencelabs.com/lowe), this can be done in
    a matter of seconds.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**🔬 关联生物学知识和可观察现象。** 人类一直在努力绘制生物学的图谱，构建基因🧬、蛋白质-蛋白质相互作用🔄、代谢通路🔀等的关系图。我邀请你阅读这篇关于[药物发现中的知识图谱综述](https://academic.oup.com/bib/article/23/6/bbac404/6712301)。然而，这些知识常常被机器学习（ML）社区忽视，未被充分利用。我认为这是一个知识图谱领域，特别是在2024年，图神经网络（GNNs）可以发挥巨大作用的地方，它也可能为上述🕸️点提供另一种方式。考虑到人类知识的不完整性，我们可以通过基础模型恢复关系图谱。这也是[Phenom1](https://arxiv.org/abs/2309.16064)在尝试回溯已知遗传关系时所采取的路径。然而，处理各种知识数据库是一项极其复杂的任务，我们不能指望大多数机器学习科学家能够单独应对。但是借助人工助手，如[LOWE](https://www.valencelabs.com/lowe)，这一任务可以在几秒钟内完成。'
- en: '**🏆 Benchmarks, benchmarks, benchmarks.** I can’t repeat the word ***benchmark***
    enough. Alas, benchmarks will stay the unloved kid on the ML block 🫥. But if the
    word benchmark is uncool, its cousin ***competition*** is way cooler 😎! Just as
    the [OGB-LSC](https://ogb.stanford.edu/docs/lsc/) competition and [Open Catalyst](https://opencatalystproject.org/challenge.html)
    challenge played a major role for the GNN community, it is now time for a new
    series of competitions 🥇. We even got the [TGB (Temporal graph benchmark)](https://tgb.complexdatalab.com/)
    recently. If you were at NeurIPS’23, then you probably heard of Polaris coming
    up early 2024 ✨. Polaris is a consortium of multiple pharma and academic groups
    trying to improve the quality of available molecular benchmarks to better represent
    real drug discovery. Perhaps we’ll even see a benchmark suitable for molecular
    graph generation instead of optimizing QED and cLogP, but I wouldn’t hold my breath,
    I have been waiting for years. What kind of new, crazy competition will light
    up the GDL community this year 🤔?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**🏆 基准测试，基准测试，基准测试。** 我无法重复***基准测试***这个词足够多。唉，基准测试依然会是机器学习领域不受宠的存在🫥。但如果“基准测试”这个词不酷，它的表亲***竞赛***就要酷得多了😎！正如[OGB-LSC](https://ogb.stanford.edu/docs/lsc/)竞赛和[Open
    Catalyst](https://opencatalystproject.org/challenge.html)挑战对GNN社区的重大作用一样，现在是时候迎来一系列新的竞赛🥇。我们甚至最近迎来了[TGB（时序图基准测试）](https://tgb.complexdatalab.com/)。如果你参加了NeurIPS''23，可能就听说了Polaris将在2024年初登场✨。Polaris是一个由多个制药公司和学术团体组成的联盟，旨在提高现有分子基准测试的质量，以更好地代表真实的药物发现过程。也许我们会看到适用于分子图生成的基准测试，而不是优化QED和cLogP，但我不会抱太大希望，毕竟我已经等了多年。是什么样的新奇竞赛将点亮今年的GDL社区呢🤔？'
- en: Systems Biology
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统生物学
- en: '***Kexin Huang (Stanford)***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***黄克鑫（斯坦福大学）***'
- en: Biology is an interconnected, multi-scale, and multi-modal system. Effective
    modeling of this system can not only unravel fundamental biological questions
    but also significantly impact therapeutic discovery. The most natural data format
    for encapsulating this system is a relational database or a heterogeneous graph.
    This graph stores data from decades of wet lab experiments across various biological
    modalities, scaling up to billions of data points.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学是一个相互连接、多尺度和多模态的系统。有效的建模不仅可以揭示基础的生物学问题，还可以对治疗发现产生重大影响。最自然的封装这种系统的数据格式是关系型数据库或异构图。这种图存储了几十年来在各种生物学领域进行的湿实验数据，数据量可达数十亿条。
- en: “In 2023, we witnessed a range of innovative applications using GNNs on these
    biological system graphs. These applications have unlocked new biomedical capabilities
    and answered critical biological queries.” — Kexin Huang (Stanford)
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在2023年，我们见证了许多使用GNNs的创新应用，应用于这些生物系统图谱。这些应用解锁了新的生物医学能力，并回答了关键的生物学问题。” — 黄克鑫（斯坦福大学）
- en: 1️⃣ One particularly exciting field is **perturbative biology**. Understanding
    the outcomes of perturbations can lead to advancements in cell reprogramming,
    target discovery, and synthetic lethality, among others. In 2023, [GEARS](https://www.nature.com/articles/s41587-023-01905-6)
    applies GNN to gene perturbational relational graphs and it predicts outcomes
    of genetic perturbations that have not been observed before.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 一个特别令人兴奋的领域是**微扰生物学**。理解微扰的结果可以推动细胞重编程、靶点发现和合成致死等方面的进展。在2023年，[GEARS](https://www.nature.com/articles/s41587-023-01905-6)将图神经网络（GNN）应用于基因扰动关系图，能够预测之前未曾观察到的基因扰动结果。
- en: 2️⃣ Another cool application concerns **protein representation**. While current
    protein representations are fixed and static, we recognize that the same protein
    can exhibit different functions in varying cellular contexts. [PINNACLE](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)
    uses GNN on protein interaction networks to contextualize protein embeddings.
    This approach has shown to enhance 3D structure-based protein representations
    and outperform existing context-free models in identifying therapeutic targets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 另一个酷炫的应用涉及**蛋白质表示**。当前的蛋白质表示是固定且静态的，但我们认识到同一蛋白质在不同的细胞环境中可能表现出不同的功能。[PINNACLE](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)在蛋白质相互作用网络上使用GNN来对蛋白质嵌入进行情境化处理。该方法已被证明能够增强基于3D结构的蛋白质表示，并在识别治疗靶点方面优于现有的无上下文模型。
- en: '![](../Images/2a530ef72f314922fe3608ed1c120dd1.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a530ef72f314922fe3608ed1c120dd1.png)'
- en: 'PINNACLE has protein-, cell type-, and tissue-level attention mechanisms that
    enable the algorithm to generate contextualized representations of proteins, cell
    types, and tissues in a single unified embedding space. Source: [Li et al](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PINNACLE拥有蛋白质、细胞类型和组织层次的注意力机制，使得该算法能够在一个统一的嵌入空间中生成蛋白质、细胞类型和组织的情境化表示。来源：[Li et
    al](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)
- en: 3️⃣ GNNs also have shown a vital role in **diagnosing rare diseases**. [SHEPHERD](https://www.medrxiv.org/content/10.1101/2022.12.07.22283238v1)
    utilizes GNN over massive knowledge graph to encode extensive biological knowledge
    into the ML model and is shown to facilitate causal gene discovery, identify ‘patients-like-me’
    with similar genes or diseases, and provide interpretable insights into novel
    disease manifestations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ GNN在**诊断罕见疾病**方面也发挥了重要作用。[SHEPHERD](https://www.medrxiv.org/content/10.1101/2022.12.07.22283238v1)利用GNN在庞大的知识图谱上对生物学知识进行编码，并已被证明有助于因果基因发现、识别具有相似基因或疾病的‘类似患者’，并为新型疾病表现提供可解释的见解。
- en: ➡️ Moving beyond predictions, understanding the underlying mechanisms of biological
    phenomena is crucial. **Graph XAI** applied to system graphs is a natural fit
    for identifying mechanistic pathways. [TxGNN](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2),
    for example, grounds drug-disease relation predictions in the biological system
    graph, generating multi-hop interpretable paths. These paths rationalize the potential
    of a drug in treating a specific disease. TxGNN designed [visualizations](http://txgnn.org/)
    for these interpretations and conducted user studies, proving their decision-making
    effectiveness for clinicians and biomedical scientists.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 超越预测，理解生物现象背后的机制至关重要。**图神经网络解释性AI（Graph XAI）**应用于系统图是识别机制路径的自然选择。例如，[TxGNN](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)将药物-疾病关系预测与生物系统图结合，生成多跳的可解释路径。这些路径阐明了药物治疗特定疾病的潜力。TxGNN为这些解释设计了[可视化工具](http://txgnn.org/)，并进行用户研究，证明其在临床医生和生物医学科学家决策中的有效性。
- en: '![](../Images/e3f51bc7c5fb00a693d400b266dff34c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f51bc7c5fb00a693d400b266dff34c.png)'
- en: 'A web-based graphical user interface to support clinicians and scientists in
    exploring and analyzing the predictions and explanations generated by TxGNN. The
    ‘Control Panel‘ allows users to select the disease of interest and view the top-ranked
    TXGNN predictions for the query disease. The ‘edge threshold‘ module enables users
    to modify the sparsity of the explanation and thereby control the density of the
    multi-hop paths displayed. The ‘Drug Embedding‘ panel allows users to compare
    the position of a selected drug relative to the entire repurposing candidate library.
    The ‘Path Explanation‘ panel displays the biological relations that have been
    identified as crucial for TXGNN’s predictions regarding therapeutic use. Source:
    [Huang, Chandar, et al](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基于网络的图形用户界面，用于支持临床医生和科学家探索和分析由TxGNN生成的预测和解释。‘控制面板’允许用户选择感兴趣的疾病，并查看该疾病的顶级TXGNN预测结果。‘边缘阈值’模块使用户能够调整解释的稀疏性，从而控制显示的多跳路径的密度。‘药物嵌入’面板允许用户比较所选药物相对于整个药物重定位候选库的位置。‘路径解释’面板展示了被认为对TXGNN预测疗效至关重要的生物学关系。来源：[黄、Chandar等](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)
- en: ➡️ Foundation models in biology have predominantly been unimodal (focused on
    proteins, molecules, diseases, etc.), primarily due to the scarcity of paired
    data. **Bridging across modalities** to answer multi-modal queries is an exciting
    frontier. For example, [BioBridge](https://openreview.net/forum?id=jJCeMiwHdH)
    leverages biological knowledge graphs to learn transformations across unimodal
    foundation models, enabling multi-modal behaviors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 生物学中的基础模型主要是单模态的（专注于蛋白质、分子、疾病等），这主要是由于配对数据的稀缺性。**跨模态桥接**以回答多模态查询是一个令人兴奋的前沿。例如，[BioBridge](https://openreview.net/forum?id=jJCeMiwHdH)利用生物学知识图谱在单模态基础模型之间学习变换，从而实现多模态行为。
- en: 🔮 GNNs applied to system graphs have the potential to (1) encode vast biomedical
    knowledge, (2) bridge biological modalities, (3) provide mechanistic insights,
    and (4) contextualize biological entities. We anticipate even more groundbreaking
    applications of GNN in biology in 2024, addressing some of the most pressing questions
    in the field.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 GNN应用于系统图谱具有以下潜力：（1）编码广泛的生物医学知识，（2）桥接生物学模态，（3）提供机制性洞察，（4）为生物实体提供背景。我们预计2024年GNN在生物学中的应用将带来更多突破性的进展，解决该领域一些最紧迫的问题。
- en: '**Predictions from the 2023 post**'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2023年预测**'
- en: (1) performance improvements of diffusion models such as faster sampling and
    more efficient solvers;
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: （1）扩散模型的性能提升，如更快的采样和更高效的求解器；
- en: ✅ yes, with flow matching
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 是的，使用流匹配
- en: (2) more powerful conditional protein generation models;
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: （2）更强大的条件蛋白质生成模型；
- en: ❌ Chroma and RFDiffusion are still on top
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ Chroma和RFDiffusion仍然领先
- en: (3) more successful applications of [Generative Flow Networks](https://arxiv.org/abs/2111.09266)
    to molecules and proteins
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: （3）[生成流网络](https://arxiv.org/abs/2111.09266)在分子和蛋白质中的更多成功应用；
- en: ❌ yet to be seen
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 尚未确定
- en: Materials Science (Crystals)
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 材料科学（晶体）
- en: '*Michael Galkin (Intel) and Santiago Miret (Intel)*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*迈克尔·高尔金（英特尔）和圣地亚哥·米雷特（英特尔）*'
- en: 'In 2023, for a short period, all scientific news were talking only about [LK-99](https://en.wikipedia.org/wiki/LK-99)
    — a supposed room-temperature superconductor created by a Korean team (spoiler:
    [it did not work as of now](https://www.nature.com/articles/d41586-023-02585-7)).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在2023年，短短一段时间内，所有科学新闻都在讨论[LK-99](https://en.wikipedia.org/wiki/LK-99)——一个由韩国团队创造的假设常温超导体（剧透：[截至目前它并未成功](https://www.nature.com/articles/d41586-023-02585-7)）。
- en: This highlights the huge potential ML has in material science, where perhaps
    the biggest progress of the year has happened — we can now say that materials
    science and materials discovery are first-class citizens in the Geometric DL landscape.
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这突显了机器学习在材料科学中的巨大潜力，或许今年最大的进展就发生在这里——我们现在可以说，材料科学和材料发现已经成为几何深度学习（Geometric DL）领域的核心内容。
- en: 💡The advances of Geometric DL applied to materials science and discovery saw
    significant advances across new modelling methods, creation of new benchmarks
    and datasets, automated design with generative methods, and identifying new research
    questions based on those advances.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 💡应用于材料科学和发现的几何深度学习取得了显著进展，涵盖了新的建模方法、新基准和数据集的创建、生成方法的自动化设计，以及基于这些进展识别出新的研究问题。
- en: 1️⃣ Applications of geometric models as evaluation tools in automated discovery
    workflows. The [Open MatSci ML Toolkit](https://github.com/IntelLabs/matsciml)
    consolidated all open-sourced crystal structures datasets leading to 1.5 million
    data points for ground-state structure calculations that are now easily available
    for model development. The [authors’ initial results](https://arxiv.org/abs/2309.05934)
    seem to indicate that merging datasets seems to improve performance if done attentively.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 几何模型作为自动化发现工作流中评估工具的应用。[Open MatSci ML Toolkit](https://github.com/IntelLabs/matsciml)整合了所有开源的晶体结构数据集，提供了150万个数据点，用于基态结构计算，这些数据现在可以轻松用于模型开发。[作者的初步结果](https://arxiv.org/abs/2309.05934)似乎表明，合并数据集如果处理得当，会提高性能。
- en: 2️⃣ [MatBench Discovery](https://arxiv.org/abs/2308.14920) is another good example
    of this integration of geometric models as an evaluation tool for crystal stability,
    which tests models’ predictions of the **energy above hull** for various crystal
    structures. The energy above hull is the most reliable approximation of crystal
    structure stability and also represents an improvement in metrics compared to
    formation energy or raw energy prediction which have practical limitations as
    stability metrics.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ [MatBench Discovery](https://arxiv.org/abs/2308.14920)是另一个很好例子，展示了几何模型作为晶体稳定性评估工具的应用，它测试了模型对各种晶体结构的**能量高于外壳**的预测。能量高于外壳是晶体结构稳定性的最可靠近似，也代表了比形成能或原始能量预测更有实际意义的度量方法，因为后者作为稳定性度量存在实际限制。
- en: '![](../Images/490c1c14cae95438a36595145e08625f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/490c1c14cae95438a36595145e08625f.png)'
- en: 'Universal potentials are more reliable classifiers because they exit the red
    triangle earliest. These lines show the rolling MAE on the WBM test set as the
    energy to the convex hull of the MP training set is varied, lower is better. The
    red-highlighted ’triangle of peril’ shows where the models are most likely to
    misclassify structures. As long as a model’s rolling MAE remains inside the triangle,
    its mean error is larger than the distance to the convex hull. If the model’s
    error for a given prediction happens to point towards the stability threshold
    at 0 eV from the hull (the plot’s center), its average error will change the stability
    classification of a material from true positive/negative to false negative/positive.
    The width of the ’rolling window’ box indicates the width over which errors hull
    distance prediction errors were averaged. Source: [Riebesell et al](https://arxiv.org/abs/2308.14920)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 普遍势能是更可靠的分类器，因为它们最早会退出红色三角区域。这些线显示了在WBM测试集上的滚动平均绝对误差（MAE），随着MP训练集的能量与凸包的距离变化，数值越低越好。红色突出显示的“危险三角区”表示模型最有可能错误分类结构。只要模型的滚动MAE保持在三角区内，其平均误差大于到凸包的距离。如果模型对某一预测的误差恰好指向位于凸包0
    eV稳定性阈值处（图表的中心），则其平均误差将改变材料的稳定性分类，从真正的正/负类变为假负/假正类。’滚动窗口’框的宽度表示在其内平均的误差对凸包距离预测的宽度。来源：[Riebesell等人](https://arxiv.org/abs/2308.14920)
- en: 3️⃣ In terms of new geometric models for crystal structure prediction, **Crystal
    Hamiltonian Graph neural network** ([CHGNet](https://chgnet.lbl.gov/), [Deng et
    al](https://arxiv.org/abs/2302.14231)) is a new GNN trained on static and relaxation
    trajectories of Materials Project that shows quite competitive performance compared
    to prior methods. The development of CHGNet suggests that finding better training
    objectives will be as (if not more) important than the development of new methods
    as the intersection of materials science and geometric deep learning continues
    to grow.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 在晶体结构预测的新几何模型方面，**晶体哈密顿图神经网络**（[CHGNet](https://chgnet.lbl.gov/)，[邓等人](https://arxiv.org/abs/2302.14231)）是一个新型的图神经网络（GNN），它基于材料项目的静态和松弛轨迹进行训练，与先前的方法相比，表现出相当有竞争力的性能。CHGNet的发展表明，找到更好的训练目标将与开发新方法一样（如果不是更重要的话）变得至关重要，因为材料科学与几何深度学习的交集正在不断增长。
- en: '🔥 The other proof points of the further integration of Geometric DL and materials
    discovery are several massive works by big labs focused on crystal structure discovery
    with generative methods:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 几何深度学习与材料发现进一步整合的其他证明点是几家大实验室在晶体结构发现中采用生成方法的几项庞大工作：
- en: 1️⃣ Google DeepMind released [**GNoME**](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/)
    (Graph Networks for Materials Science by [Merchant et al](https://www.nature.com/articles/s41586-023-06735-9))
    as a successful example of an active learning pipeline for discovering new materials,
    and [UniMat](https://unified-materials.github.io/unimat/) as an *ab initio* crystal
    generation model. Similar to the protein world, we see more examples of automated
    labs for materials science (“lab-in-the-loop”) such as the [A-Lab from UC Berkley](https://www.nature.com/articles/s41586-023-06734-w).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ Google DeepMind 发布了 [**GNoME**](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/)（由
    [Merchant 等人](https://www.nature.com/articles/s41586-023-06735-9) 提出的材料科学图网络）作为发现新材料的成功主动学习流程示例，同时发布了
    [UniMat](https://unified-materials.github.io/unimat/) 作为一个 *从头开始* 的晶体生成模型。类似于蛋白质领域，我们也看到了更多关于材料科学的自动化实验室示例（“实验室-环路”），例如
    [UC 伯克利的 A-Lab](https://www.nature.com/articles/s41586-023-06734-w)。
- en: '![](../Images/bfe512fa260e4fa3114312d89c43d783.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfe512fa260e4fa3114312d89c43d783.png)'
- en: 'The active learning loop of GNoME. Source: [Merchant et al.](https://www.nature.com/articles/s41586-023-06735-9)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: GNoME 的主动学习循环。来源：[Merchant 等人](https://www.nature.com/articles/s41586-023-06735-9)
- en: 2️⃣ Microsoft Research released [MatterGen](https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/),
    a generative model for unconditional and property-guided materials design, and
    [Distributional Graphormer](https://distributionalgraphormer.github.io/), a generative
    model trained to recover the equilibrium energy distribution of a molecule/protein/crystal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 微软研究院发布了 [MatterGen](https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/)，一个用于无条件和属性引导材料设计的生成模型，以及
    [Distributional Graphormer](https://distributionalgraphormer.github.io/)，一个生成模型，旨在恢复分子/蛋白质/晶体的平衡能量分布。
- en: '![](../Images/f9bb59d0a617dbbf35236c7e6d8b146c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9bb59d0a617dbbf35236c7e6d8b146c.png)'
- en: 'Unconditional and conditional generation of MatterGen. Source: [Zeni, Pinsler,
    Zügner, Fowler, Horton, et al.](https://arxiv.org/abs/2312.03687)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MatterGen 的无条件和有条件生成。来源：[Zeni, Pinsler, Zügner, Fowler, Horton 等人](https://arxiv.org/abs/2312.03687)
- en: 3️⃣ Meta AI and CMU released the [Open Catalyst Demo](https://open-catalyst.metademolab.com/)
    where you can play around with relaxations (DFT approximations) of 11.5k catalyst
    materials on 86 adsorbates in 100 different configurations each (making it up
    to 100M combinations). The demo is powered by SOTA geometric models GemNet-OC
    and Equiformer-V2.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ Meta AI 和 CMU 发布了 [Open Catalyst Demo](https://open-catalyst.metademolab.com/)，在这里你可以体验
    11.5k 种催化剂材料在 86 种吸附体上、每种吸附体有 100 种不同配置的弛豫（DFT 近似），总共有多达 100M 种组合。该演示由最先进的几何模型
    GemNet-OC 和 Equiformer-V2 提供支持。
- en: '***Santiago Miret (Intel)***'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '***Santiago Miret (英特尔)***'
- en: While those works represent large-scale deployments of generative methods, there
    is also new work on using reinforcement learning ([Govindarajan et al.](https://openreview.net/forum?id=VbjD8w2ctG),
    [Lacombe et al.](https://openreview.net/forum?id=MNfVMjsL7S)) and GFlowNets ([Mistal
    et al.](https://openreview.net/forum?id=l167FjdPOv), [Nguyen et al.](https://openreview.net/forum?id=dJuDv4MKLE))
    with geometric DL for crystal structure discovery as highlighted in the [AI for
    Accelerated Materials Design (AI4Mat)](https://sites.google.com/view/ai4mat) workshop
    at NeurIPS’23\. AI4Mat-2023 itself saw rapid expansion in participation with a
    2× increase in the number of submitted and accepted papers and almost tripling
    in the number of attendees.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些工作代表了生成方法的大规模应用，但也有新的研究在使用强化学习（[Govindarajan 等人](https://openreview.net/forum?id=VbjD8w2ctG)，[Lacombe
    等人](https://openreview.net/forum?id=MNfVMjsL7S)）和 GFlowNets（[Mistal 等人](https://openreview.net/forum?id=l167FjdPOv)，[Nguyen
    等人](https://openreview.net/forum?id=dJuDv4MKLE)）结合几何深度学习来发现晶体结构，正如在 [AI 加速材料设计
    (AI4Mat)](https://sites.google.com/view/ai4mat) 研讨会中所强调的那样，NeurIPS’23 上的 AI4Mat-2023
    也见证了参与人数的迅速增长，提交和接收的论文数量翻了一番，参会人数几乎增加了三倍。
- en: 💡 Geometric DL and GNNs continue to be a major part of AI4Mat’s research content
    as we saw increased application of methods not only for property prediction but
    also for improving **chemical synthesis** and **material characterization**. One
    such promising example highlighted in the AI4Mat-2023 workshop is **KREED** ([Cheng,
    Lo, et al](https://openreview.net/forum?id=jlZrTCccAb)), which uses equivariant
    diffusion to predict 3D structures of molecules based on incomplete information
    that can be obtained from real laboratory machines.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 几何深度学习（DL）和图神经网络（GNN）仍然是AI4Mat研究内容的重要组成部分，我们看到这些方法的应用不仅限于性质预测，还扩展到了**化学合成**和**材料表征**的改善。AI4Mat-2023工作坊中突出的一个有前景的例子是**KREED**（[Cheng,
    Lo等](https://openreview.net/forum?id=jlZrTCccAb)），它使用等变扩散方法，根据从真实实验室机器获得的不完全信息，预测分子的三维结构。
- en: “Given the importance of structural data in material characterization, the discussions
    at AI4Mat highlighted the opportunities for Geometric DL to enter the space of
    real-world materials modelling in addition to their continued successes in simulations
    including ML-based potentials.” — Santiago Miret (Intel)
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “考虑到结构数据在材料表征中的重要性，AI4Mat的讨论突出了几何深度学习进入真实世界材料建模领域的机会，除了在包括基于机器学习的潜力在内的仿真中的持续成功。”
    — Santiago Miret（英特尔）
- en: '🔮 In 2024, I expect to see multiple developments:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 在2024年，我预计会看到多项发展：
- en: 1️⃣ More discovery architectures and workflows that directly integrate geometric
    models like M3GNet, CHGNet, MACE.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 更多发现新的架构和工作流程，直接整合像M3GNet、CHGNet、MACE这样的几何模型。
- en: 2️⃣ Geometric models might also see increased competition from text-based representations
    and LLMs as [new methods are being proposed](https://openreview.net/forum?id=0r5DE2ZSwJ)
    that directly generate CIF files.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 几何模型可能会面临来自文本表示和大语言模型（LLMs）的竞争，因为[新方法正在被提出](https://openreview.net/forum?id=0r5DE2ZSwJ)，直接生成CIF文件。
- en: 3️⃣ More deployment of geometric models and GNNs into real-world experimental
    data, likely in materials characterization such as KREED, which will likely run
    into regimes with less data compared to simulation-based modeling.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 更多几何模型和GNN应用于真实世界的实验数据，可能用于材料表征，如KREED，预计将面临与基于仿真建模相比数据较少的情况。
- en: Molecular Dynamics & ML Potentials
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分子动力学与机器学习潜力
- en: '*Michael Galkin (Intel), Leon Klein (FU Berlin), N M Anoop Krishnan (IIT Delhi),
    Santiago Miret (Intel)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*Michael Galkin (英特尔)，Leon Klein (柏林自由大学)，N M Anoop Krishnan (印度理工学院德里分校)，Santiago
    Miret (英特尔)*'
- en: One of the pronounced trends of 2023 is going towards foundation models for
    ML potentials that work on a variety of compounds from small molecules to periodic
    crystals
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2023年一个显著的趋势是朝着基础模型发展，这些模型适用于从小分子到周期性晶体等多种化合物的机器学习潜力。
- en: For example, **JMP** ([Shoghi et al](https://arxiv.org/abs/2310.16802)) from
    FAIR and CMU, **DPA-2** ([Zhang, Liu, et al](https://arxiv.org/abs/2312.15492))
    from a large collaboration of Chinese institutions, and **MACE-MP-0** ([Batatia
    et al](https://arxiv.org/abs/2401.00096)) from a collaboration led by Cambridge.
    Practically, those are geometric GNNs pre-trained in the multi-task mode to predict
    the energy (or forces) of a certain atomic structure. Another notable mention
    goes to **Equiformer V2** ([Liao et al](https://arxiv.org/abs/2306.12059)) as
    a strong equivariant transformer that holds SOTA in many tasks including the recent
    [OpenCatalyst 2023 Challenge](https://opencatalystproject.org/challenge.html)
    and [OpenDAC](https://open-dac.github.io/index.html) (Direct Air Capture) challenge.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，FAIR和CMU的**JMP**（[Shoghi等](https://arxiv.org/abs/2310.16802)）、来自中国多个机构的大型合作的**DPA-2**（[Zhang,
    Liu等](https://arxiv.org/abs/2312.15492)），以及剑桥主导的合作中的**MACE-MP-0**（[Batatia等](https://arxiv.org/abs/2401.00096)）。实际上，这些是预训练的几何图神经网络（GNN），用于预测某一原子结构的能量（或力）。另一个值得提及的例子是**Equiformer
    V2**（[Liao等](https://arxiv.org/abs/2306.12059)），这是一种强大的等变变换器，在许多任务中保持了SOTA表现，包括最近的[OpenCatalyst
    2023挑战](https://opencatalystproject.org/challenge.html)和[OpenDAC](https://open-dac.github.io/index.html)（直接空气捕获）挑战。
- en: '![](../Images/82a368628516ed1fc271de109838f4e3.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a368628516ed1fc271de109838f4e3.png)'
- en: 'A foundation model for materials modelling. Trained only on Materials Project
    data which consists primarily of inorganic crystals and is skewed heavily towards
    oxides, MACE-MP-0 is capable of molecular dynamics simulation across a wide variety
    of chemistries in the solid, liquid and gaseous phases. Source: [Batatia et al](https://arxiv.org/abs/2401.00096)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一个材料建模的基础模型。该模型仅在材料项目数据上进行训练，数据主要由无机晶体组成，并且在氧化物方面偏向较重，MACE-MP-0能够在固态、液态和气态的各种化学环境中进行分子动力学仿真。来源：[Batatia等](https://arxiv.org/abs/2401.00096)
- en: ⚛️ A common use case for ML potentials is molecular dynamics (MD) which aims
    to simulate a certain structure on a span of nanoseconds (10ᐨ⁹) to seconds. The
    main problem is that the fundamental timestep in classical methods is a femtosecond
    (10ᐨ¹⁵), that is, you’d need at least 1 million steps to simulate a nanosecond
    and that’s expensive. Modern ML-based methods for MD aim to speed it up by applying
    coarse-graining and other approximation tricks that accelerate simulations by
    large margins (30–1000x). [Fu, Xie, et al](https://openreview.net/forum?id=y8RZoPjEUl)
    (TMLR’23) apply coarse-graining to atomic structures and run a GNN over smaller
    graphs to predict the next-step position. Experimentally, the method brings 1000–10.000x
    speedups compared to classical methods. **TimeWarp** ([Klein, Foong, Fjelde, Mlodozeniec,
    et al](https://arxiv.org/abs/2302.01170), NeurIPS’23) can simulate large timesteps
    (1⁰⁵ — 1⁰⁶ femtoseconds) in a single forward pass by using a conditional normalizing
    flow model that approximates a distribution of next-step positions. A trained
    model is used with MCMC sampling and delivers ~33x speedups.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ⚛️ 机器学习势能的一个常见应用场景是分子动力学（MD），其目的是在纳秒（10ᐨ⁹）到秒的时间范围内模拟某种结构。主要问题在于，经典方法中的基本时间步长是飞秒（10ᐨ¹⁵），也就是说，模拟一个纳秒至少需要1百万个时间步，这非常昂贵。现代基于机器学习的MD方法旨在通过应用粗粒化和其他近似技巧加速模拟，这些方法能够大幅提高模拟速度（30–1000倍）。[Fu,
    Xie, et al](https://openreview.net/forum?id=y8RZoPjEUl)（TMLR'23）将粗粒化应用于原子结构，并在较小的图上运行GNN来预测下一步位置。在实验中，该方法相比经典方法实现了1000–10,000倍的加速。**TimeWarp**（[Klein,
    Foong, Fjelde, Mlodozeniec, et al](https://arxiv.org/abs/2302.01170)，NeurIPS’23）通过使用条件归一化流模型来模拟大时间步长（1⁰⁵
    — 1⁰⁶飞秒），该模型近似下一步位置的分布。在使用MCMC采样的情况下，训练后的模型实现了约33倍的加速。
- en: '![](../Images/b960ce8fc1b4928fe2965be02fbe1f9e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b960ce8fc1b4928fe2965be02fbe1f9e.png)'
- en: '(a) Initial state x(t) (Left) and accepted proposal state x(t+τ) (Right) sampled
    with Timewarp for the dipeptide HT (unseen during training). (b) TICA projections
    of simulation trajectories, showing transitions between metastable states, for
    a short MD simulation (Left) and Timewarp MCMC (Right), both run for 30 minutes
    of wall-clock time. Timewarp MCMC achieves a speed-up factor of ≈ 33 over MD in
    terms of effective sample size per second. Source: [Klein, Foong, Fjelde, Mlodozeniec,
    et al](https://arxiv.org/abs/2302.01170)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 初始状态x(t)（左）和接受的提议状态x(t+τ)（右），通过Timewarp为二肽HT（在训练期间未见过的样本）采样。(b) 模拟轨迹的TICA投影，显示了介稳态之间的转变，分别为短时间MD模拟（左）和Timewarp
    MCMC（右），两者均运行了30分钟的墙钟时间。Timewarp MCMC在有效样本数每秒的速度上，相比MD实现了约33倍的加速。来源：[Klein, Foong,
    Fjelde, Mlodozeniec, et al](https://arxiv.org/abs/2302.01170)
- en: '***Santiago Miret (Intel)***'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '***Santiago Miret（英特尔）***'
- en: 💡As the deployment of geometric models has seen greater success in property
    modelling, researchers have pushed the state-of-the-art by testing these models
    in real-world molecular dynamics simulations. The first work to highlight issues
    with training models on energy and forces alone was [Forces Are Not Enough](https://openreview.net/forum?id=A8pqQipwkt)
    published in TMLR in early 2023\. Nevertheless, advances in neighborhood-based
    methods such as [Allegro](https://arxiv.org/abs/2204.05249) led to the successful
    deployment of large-scale simulations using geometric deep learning models, including
    a [nomination for the Gordon Bell Prize](https://www.hpcwire.com/off-the-wire/sc23-spotlight-gordon-bell-prize-2023-finalists-showcase-diverse-supercomputing-applications/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 💡随着几何模型在属性建模中的成功部署，研究人员通过在现实世界的分子动力学模拟中测试这些模型，推动了最前沿的发展。首个突显仅基于能量和力训练模型存在问题的研究是2023年初在TMLR上发布的[Forces
    Are Not Enough](https://openreview.net/forum?id=A8pqQipwkt)。然而，基于邻域的方法如[Allegro](https://arxiv.org/abs/2204.05249)的进展，导致了使用几何深度学习模型成功部署大规模模拟，其中包括[戈登·贝尔奖提名](https://www.hpcwire.com/off-the-wire/sc23-spotlight-gordon-bell-prize-2023-finalists-showcase-diverse-supercomputing-applications/)。
- en: “Much work still remains in ensuring successful, generalised deployment of machine
    learning potentials across a variety of physical and chemical phenomena.” — Santiago
    Miret (Intel)
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “仍然有大量工作需要做，以确保机器学习势能在各种物理和化学现象中成功、广泛地部署。” — Santiago Miret（英特尔）
- en: ➡️ [EGraffBench](https://arxiv.org/abs/2310.02428) highlights some new challenges,
    such as generalisation across temperatures and materials phase changes (i.e. *solid-to-liquid*
    change), and proposes new metrics for evaluating the performance of machine learning
    potentials in real MD simulations. The AI4Mat-2023 workshop also showcased the
    development of new ML potentials for specialised use cases, such as [solid electrolytes
    for batteries](https://openreview.net/forum?id=jtAXitX6dh).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ [EGraffBench](https://arxiv.org/abs/2310.02428)突出了其中一些新挑战，例如跨温度和材料相变的泛化（即*固体到液体*的变化），并提出了用于评估机器学习势能在实际MD模拟中表现的新指标。AI4Mat-2023研讨会也展示了为特定应用场景开发的新型机器学习势能，例如[用于电池的固态电解质](https://openreview.net/forum?id=jtAXitX6dh)。
- en: '***Leon Klein (FU Berlin)***'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '***Leon Klein (FU Berlin)***'
- en: 💡 A notable constraint in the application of generative models to sample from
    the equilibrium Boltzmann distribution was the requirement for retraining with
    each new system, thereby limiting potential advantages over traditional MD simulations.
    However, recent advancements have seen the emergence of transferable models across
    various domains. Our contribution, [Timewarp](https://arxiv.org/abs/2302.01170),
    presents a transferable model capable of proposing large time steps for MD simulations
    focused on all atom small peptide systems. Similarly, [Fu et al.](https://arxiv.org/abs/2204.10348)
    capture the time-coarsened dynamics of coarse-grained polymers, while [Charron
    et al.](https://arxiv.org/abs/2310.18278) excel in learning a transferable force
    field for coarse-grained proteins.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 将生成模型应用于从平衡玻尔兹曼分布中采样时的一个显著限制是，每次遇到新系统时都需要重新训练，这使得其相较于传统的分子动力学（MD）模拟的潜在优势受限。然而，最近的进展使得跨不同领域的可迁移模型逐渐浮现。我们的贡献，[Timewarp](https://arxiv.org/abs/2302.01170)，提出了一种可迁移模型，能够为聚焦于全原子小肽系统的MD模拟提出大时间步长。类似地，[Fu
    et al.](https://arxiv.org/abs/2204.10348) 捕捉了粗粒化聚合物的时间粗化动力学，而[Charron et al.](https://arxiv.org/abs/2310.18278)则擅长学习用于粗粒化蛋白质的可迁移力场。
- en: “Consequently, this year has demonstrated the feasibility of transferable generative
    models for MD simulations, showcasing their potential to speed up such simulations.”
    — Leon Klein (FU Berlin)
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “因此，今年展示了可迁移生成模型在MD模拟中的可行性，展示了它们加速此类模拟的潜力。” — Leon Klein (FU Berlin)
- en: 🔮 In 2024, I expect that more tailored GNNs are used to improve accuracy for
    the transferable models, with a potential focus on encoding more information about
    the system. For example, Timewarp, while lacking rotational symmetry in its model,
    employs data augmentation. Alternatively, rotational symmetry could be incorporated
    using the recently proposed [SE(3) Equivariant Augmented Coupling Flows](https://arxiv.org/abs/2308.10364)**.**
    Similarly, [Charron et al.](https://arxiv.org/abs/2310.18278) use a SchNet instead
    of a more complex GNN.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 在2024年，我预计会有更多定制化的GNN（图神经网络）被用来提高可迁移模型的准确性，可能会专注于编码更多关于系统的信息。例如，尽管Timewarp在其模型中缺乏旋转对称性，但它通过数据增强来弥补这一点。另一种选择是使用最近提出的[SE(3)等变增强耦合流](https://arxiv.org/abs/2308.10364)来加入旋转对称性。类似地，[Charron
    et al.](https://arxiv.org/abs/2310.18278)使用SchNet，而非更复杂的GNN。
- en: '***N M Anoop Krishnan (IIT Delhi)***'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '***N M Anoop Krishnan (IIT Delhi)***'
- en: “One of the most exciting developments for the year in the realm of ML potentials
    is the development of “universal” interatomic potentials that can span almost
    all the elements of the periodic table.” — N M Anoop Krishnan (IIT Delhi)
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “今年在机器学习势能领域最令人兴奋的进展之一是开发了可以覆盖几乎所有元素的‘通用’原子间势能。” — N M Anoop Krishnan (IIT Delhi)
- en: 💡 Following M3GNet in 2022, this year witnessed the developments of three such
    models based on CHGNet ([Deng et al](https://www.nature.com/articles/s42256-023-00716-3)),
    NequIP ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)),
    and MACE ([Batatia et al](https://arxiv.org/abs/2401.00096)). These models have
    been used to demonstrate several challenging tasks including materials discovery
    ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)), and diverse
    set of MD simulations ([Batatia et al](https://arxiv.org/abs/2401.00096)) such
    as phase transition, amorphization, chemical reaction, 2D materials modeling,
    dissolution, defects, combustion to name a few. These approaches provide promising
    results towards the universality of these potentials, thereby allowing one to
    solve challenging problems including the discovery of crystals from their corresponding
    amorphous structure ([Aykol et al](https://arxiv.org/abs/2310.01117)), a long-standing
    open problem in materials.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 继2022年的M3GNet之后，今年见证了基于CHGNet的三种模型的发展，它们分别是([Deng et al](https://www.nature.com/articles/s42256-023-00716-3))、NequIP
    ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)) 和 MACE
    ([Batatia et al](https://arxiv.org/abs/2401.00096))。这些模型已被用来展示几个具有挑战性的任务，包括材料发现（[Merchant
    et al](https://www.nature.com/articles/s41586-023-06735-9)），以及多种MD模拟（[Batatia
    et al](https://arxiv.org/abs/2401.00096)），如相变、非晶化、化学反应、二维材料建模、溶解、缺陷、燃烧等。这些方法为这些势能的普适性提供了有希望的结果，从而使得解决具有挑战性的问题成为可能，包括从相应的非晶结构中发现晶体（[Aykol
    et al](https://arxiv.org/abs/2310.01117)），这是材料学中一个长期未解的开放问题。
- en: 🏋️ While these potentials do provide a handle to attack some outstanding problems,
    the challenges remain in understanding the scenarios where these potentials can
    fail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 🏋️ 虽然这些势能确实为解决一些突出问题提供了手段，但挑战依然存在，主要是理解这些势能可能失败的情境。
- en: '**1️⃣** Testing these potentials to their limit to understand their capability
    is an important aspect to understand their limitations. This includes modeling
    extreme environments such as **high pressure** and **radiation conditions**, simulating
    complex multicomponent systems such as **glasses or high-entropy alloys**, or
    simulating **different phases** of systems such as water or silica would be interesting
    challenges.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** 测试这些势能的极限以了解它们的能力，是理解其局限性的重要方面。这包括模拟极端环境，如**高压**和**辐射条件**，模拟复杂的多组分系统，如**玻璃或高熵合金**，或者模拟**不同相态**的系统，如水或二氧化硅，这些都是有趣的挑战。'
- en: '**2️⃣** While some of these models have been termed as “foundation” models,
    **emergent behavior** associated with FMs **has not been demonstrated** by them.
    Most of these models simply show extrapolation capability to potentially unseen
    regions in the phase space or to novel compositions. Developing truly foundational
    models in terms of emergent properties would be an interesting challenge.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** 虽然其中一些模型被称为“基础”模型，但它们并未展示与基础模型相关的**涌现行为**。这些模型大多只是展示了对潜在未见区域或新组合物的外推能力。开发真正具有涌现属性的基础模型将是一个有趣的挑战。'
- en: '**3️⃣** A third aspect that has been paid less attention to is the ability
    of these models to **simulate at scale**. While [Allegro](https://arxiv.org/abs/2204.05249)
    has demonstrated some capability in terms of length scales these potentials can
    achieve, simulating at larger time and length scales with stability while respecting
    the “universality” shall still remain an open challenge for these potentials.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣** 第三个较少被关注的方面是这些模型**大规模模拟**的能力。尽管[Allegro](https://arxiv.org/abs/2204.05249)已经展示了这些势能在长度尺度方面的某些能力，但在更大的时间和长度尺度下进行稳定的模拟，同时保持“普适性”，仍然是这些势能面临的一个未解挑战。'
- en: 🔮 **What to expect in 2024?**
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 **2024年会有什么期待？**
- en: '**1️⃣** **Benchmarking suite**: While there exist several benchmarking studies
    on MD simulations, it is expected that 2024 will witness more formalized efforts
    in this direction both in terms of datasets and tasks. A standard set of tasks
    that can automatically evaluate potentials and place them on leaderboards will
    enable easy ranking of potentials targeted for downstream tasks on different materials
    such as metals, polymers, or oxides.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** **基准套件**：虽然已经有一些关于分子动力学（MD）模拟的基准研究，预计2024年将在数据集和任务方面看到更多正式化的努力。一个标准的任务集，可以自动评估势能并将其放置在排行榜上，将便于对不同材料（如金属、聚合物或氧化物）的后续任务进行势能的排名。'
- en: '**2️⃣ Model and dataset development**: Further efforts will be made to make
    ML potentials more compact and efficient in terms of their architectures. Moreover,
    2024 will also witness large-scale dataset development that will provide *ab initio*
    data for training these potentials.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣ 模型与数据集开发**：将进一步努力使机器学习潜力在架构上更紧凑和高效。此外，2024年还将见证大规模数据集的发展，这些数据集将为训练这些潜力提供*从头计算*数据。'
- en: '**3️⃣ Differentiable MD/AIMD**: Further, it is expected that the developments
    in differentiable simulations will become a major area of fusing experiments and
    *ab initio* simulations towards automated development of interatomic potentials
    for targeted applications. This year may also see advances in differentiable AIMD
    with machine learned functionals that may allow economical simulations to scale
    beyond what it has been able to achieve thus far.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣ 可微MD/AIMD**：此外，预计可微模拟的进展将成为融合实验与*从头计算*模拟，朝向自动开发针对特定应用的原子间势能的重要领域。今年可能还会看到可微AIMD的发展，结合机器学习的泛函，可能使得经济型模拟能够超越现有的限制，扩展到更大规模。'
- en: '**Predictions from the 2023 post**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023年后的预测**'
- en: We expect to see a lot more focus on computational efficiency and scalability
    of GNNs. Current GNN-based force-fields are obtaining remarkable accuracy, but
    are still 2–3 orders of magnitude slower than classical force-fields and are typically
    only deployed on a few hundred atoms.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计将更加关注图神经网络（GNN）的计算效率和可扩展性。目前基于GNN的力场在精度上取得了显著成就，但仍然比传统力场慢2到3个数量级，且通常只能部署在几百个原子上。
- en: ✅ Allegro for the Gordon Bell Prize, Large-scale screening with GNoMe
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 阿莱格罗（Allegro）竞逐戈登·贝尔奖，使用GNoMe进行大规模筛选
- en: '🔮**What to expect in 2024**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮**2024年展望**：
- en: '**1️⃣** More deployment of ML potentials into large-scale MD simulations that
    showcase new research opportunities and challenges and provide a better idea of
    what benefits ML potentials provide compared to traditional potentials.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** 更多将机器学习潜力部署到大规模分子动力学（MD）模拟中，展示新的研究机会和挑战，并提供更好的视角，了解机器学习潜力相对于传统势能的优势。'
- en: '**2️⃣** New datasets that outline previously unexplored challenges for ML potentials,
    such as new materials systems and new physical phenomena for those materials such
    as phase changes at various temperatures and pressures.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** 新的数据集将概述机器学习潜力未曾探索的挑战，例如新材料系统和这些材料的新物理现象，如在不同温度和压力下的相变。'
- en: '**3️⃣** Exploration of multi-scale problems that might draw inspiration from
    classical techniques.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣** 探索多尺度问题，可能会从经典技术中获得灵感。'
- en: Geometric Generative Models (Manifolds)
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何生成模型（流形）
- en: '*Joey Bose (Mila & Dreamfold) and Alex Tong (Mila & Dreamfold)*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*乔伊·博斯（Joey Bose）（Mila & Dreamfold）和亚历克斯·汤（Alex Tong）（Mila & Dreamfold）*'
- en: While generative ML continued to dominate the field in 2023, it was the popularization
    of geometric generative models that incorporate geometric priors an interesting
    trend of the year.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然生成型机器学习在2023年继续主导该领域，但将几何先验结合到几何生成模型中的普及成为这一年一个有趣的趋势。
- en: '***Joey Bose (Mila & Dreamfold)***'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '***乔伊·博斯（Joey Bose）（Mila & Dreamfold）***'
- en: “This year we saw the burgeoning subfield of geometric generative generative
    models really take a commanding step forward. With the success of diffusion models
    and flow matching in images we saw more fundamental contributions to enable Generative
    AI for geometric data types.“ — Joey Bose (Mila & Dreamfold)
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “今年我们看到几何生成模型这一新兴子领域迈出了重要的一步。随着扩散模型和流动匹配在图像中的成功应用，我们看到了更多基础性的贡献，为几何数据类型的生成式AI奠定了基础。”——
    乔伊·博斯（Joey Bose）（Mila & Dreamfold）
- en: While diffusion models for manifolds existed, this year we really saw them being
    scaled up with **Scaling Riemannian Diffusion Models** by [Lou et. al](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=54-actIAAAAJ&sortby=pubdate&citation_for_view=54-actIAAAAJ%3A_FxGoFyzp5QC)
    and functional approaches in **Manifold Diffusion Fields** [Elhag et. al.](https://arxiv.org/abs/2305.15586)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然流形的扩散模型早已有所存在，但今年我们确实看到了它们通过**扩展黎曼扩散模型**由[Lou等人](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=54-actIAAAAJ&sortby=pubdate&citation_for_view=54-actIAAAAJ%3A_FxGoFyzp5QC)和**流形扩散场**[Elhag等人](https://arxiv.org/abs/2305.15586)的功能方法得到扩展
- en: '![](../Images/c4c9c5ec0804e2389bd2fc37d0611062.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c9c5ec0804e2389bd2fc37d0611062.png)'
- en: '(Left) Visual depiction of a training iteration for a field on the bunny manifold
    M. (Right) Visual depiction of the sampling process for a field on the bunny manifold.
    Figure source: [Elhag et al.](https://arxiv.org/abs/2305.15586)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: （左）在兔子流形M上的场的训练迭代的可视化表现。（右）在兔子流形上的场的采样过程的可视化表现。图源：[Elhag等人](https://arxiv.org/abs/2305.15586)
- en: For Normalizing flow-based methods, **Riemannian Flow matching** by [Chen and
    Lipman](https://arxiv.org/abs/2302.03660) stands at the top of the sea of papers
    as being the most general framework for FM.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于归一化流的方法，[陈和利普曼](https://arxiv.org/abs/2302.03660)提出的**黎曼流匹配**被认为是FM领域中最通用的框架，位于众多论文之中。
- en: In general, a large theme of geometric generative models involves handling symmetries.
    Equivariant approaches shone this year, from SE(3) models including **EDGI** ([Brehmer,
    Bose et. al](https://arxiv.org/abs/2303.12410)), **SE(3) augmented coupling flows**
    ([Midgley et. al](https://arxiv.org/abs/2308.10364)), to cool theoretical work
    on **Geometric neural diffusion processes** ([Mathieu et. al](https://arxiv.org/abs/2307.05431))
    and important physics-based applications with the paper by [Abbot et. al](https://arxiv.org/abs/2305.02402).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，几何生成模型的一个重要主题是处理对称性。今年，等变方法大放异彩，包括SE(3)模型中的**EDGI**（[Brehmer, Bose等人](https://arxiv.org/abs/2303.12410)）、**SE(3)增强耦合流**（[Midgley等人](https://arxiv.org/abs/2308.10364)），以及关于**几何神经扩散过程**的很酷的理论工作（[Mathieu等人](https://arxiv.org/abs/2307.05431)）和由[Abbot等人](https://arxiv.org/abs/2305.02402)提出的重要基于物理的应用。
- en: '***Alex Tong (Mila & Dreamfold)***'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '***Alex Tong (Mila & Dreamfold)***'
- en: “In 2023 we saw advancement both in terms of modelling and the rise of a new
    application — Protein backbone design. Much work is still needed to understand
    the properties of the SE(3)*ᴺ*₀ type of product manifold, where it is still unclear
    how to best combine modalities” — Alex Tong (Mila & Dreamfold)
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “2023年，我们看到了建模的进展和一个新应用的崛起——蛋白质骨架设计。我们仍然需要更多的工作来理解SE(3)*ᴺ*₀类型的积产品流形的性质，在这一领域，我们仍不清楚如何最好地结合不同的模态”——Alex
    Tong (Mila & Dreamfold)
- en: 2023 saw new models such as [RFDiffusion](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1),
    [FrameDiff](https://arxiv.org/abs/2302.02277), and [FoldFlow](https://arxiv.org/abs/2310.02391)
    which operate over the SE(3)*ᴺ*₀ manifold of protein backbones. This presents
    a new challenge for geometric generative models which I think we will see significant
    progress in the coming year.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年出现了新的模型，如[RFDiffusion](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1)、[FrameDiff](https://arxiv.org/abs/2302.02277)和[FoldFlow](https://arxiv.org/abs/2310.02391)，它们在蛋白质骨架的SE(3)*ᴺ*₀流形上进行操作。这为几何生成模型提出了新的挑战，我认为我们将在未来一年看到在这一领域的重要进展。
- en: On the modelling side, generative modelling with flow and bridge matching models
    in Euclidean domains led to quick succession of Riemannian and equivariant extensions
    with Riemannian Flow Matching by [Chen and Lipman](https://arxiv.org/abs/2302.03660)
    and Equivariant flow matching ([Klein et al.](https://arxiv.org/abs/2306.15030),
    [Song et al.](https://arxiv.org/abs/2312.07168)) on molecule generation tasks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模方面，欧几里得领域中的生成建模与流和桥接匹配模型相结合，迅速继而推出了黎曼流匹配（[陈和利普曼](https://arxiv.org/abs/2302.03660)）和等变流匹配（[Klein等人](https://arxiv.org/abs/2306.15030)、[Song等人](https://arxiv.org/abs/2312.07168)）用于分子生成任务。
- en: '🔮 **What to expect in 2024**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 **2024年展望**：
- en: '**1️⃣** More exploration into modelling the SE(3)*ᴺ*₀ manifold following successes
    in protein backbone design.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** 在蛋白质骨架设计取得成功后，更多地探索SE(3)*ᴺ*₀流形的建模。'
- en: '**2️⃣** Further investigation and theory of how to train generative models
    on multimodal and product manifolds.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** 进一步研究和理论探索如何在多模态和积产品流形上训练生成模型。'
- en: '**3️⃣** Domain-specific models exploiting features of more specific manifold
    and equivariant structures.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**3️⃣** 利用更具体的流形和等变结构特征的领域特定模型。'
- en: 'BIG Graphs, Scalability: When GNNs are too expensive'
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型图，扩展性：当GNNs过于昂贵时
- en: '***Anton Tsitsulin (Google)***'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '***Anton Tsitsulin (Google)***'
- en: This year has been fruitful for large graph fans.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 今年对于大型图形的爱好者来说是丰收的一年。
- en: “Learning on Very Large Graphs has always been a challenge due to the unstructured
    sparsity not being supported by modern accelerators, losing in the [hardware lottery](https://hardwarelottery.github.io/).
    [Tensor Processing Units](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)
    — you can think about them as very fast GPUs with tons (multi-terabyte) of HBM
    memory — were the rescue of 2023.” — **Anton Tsitsulin** (Google)
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在非常大的图形上进行学习一直是一个挑战，因为现代加速器不支持这种非结构化稀疏性，导致在[硬件抽奖](https://hardwarelottery.github.io/)中落败。[张量处理单元](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)——你可以把它们看作是非常快速的
    GPU，配有大量（多 TB）的 HBM 内存——是 2023 年的救星。” —— **Anton Tsitsulin**（Google）
- en: In a KDD paper ([Mayer et al.](https://arxiv.org/abs/2307.14490)), we showed
    that TPUs can solve large-scale node embedding problems more efficiently than
    GPU and CPU systems at a fraction of the cost. Many industrial applications of
    graph machine learning are fully unsupervised; there, it is hard to evaluate embedding
    quality. We wrote a paper ([Tsitsulin et al.](https://arxiv.org/abs/2305.16562))
    that performs **unsupervised embedding analysis** at scale.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇 KDD 论文中（[Mayer 等](https://arxiv.org/abs/2307.14490)），我们展示了 TPUs 如何比 GPU
    和 CPU 系统更高效地解决大规模节点嵌入问题，并且成本大大降低。图形机器学习的许多工业应用是完全无监督的，在这种情况下，很难评估嵌入质量。我们撰写了一篇论文（[Tsitsulin
    等](https://arxiv.org/abs/2305.16562)），在大规模上执行**无监督嵌入分析**。
- en: '![](../Images/c4683b5e9dcf8a260db6773c1db23ad1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4683b5e9dcf8a260db6773c1db23ad1.png)'
- en: 'Scale of TpuGraphs compared to other graph property prediction datasets. Source:
    [Phothilimthana et al.](https://arxiv.org/abs/2308.13490)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TpuGraphs 与其他图形属性预测数据集的规模对比。来源：[Phothilimthana 等](https://arxiv.org/abs/2308.13490)
- en: ➡️ This year, TPUs helped graph machine learning, so it was time to give back.
    We released a new **TpuGraphs** dataset ([Phothilimthana et al.](https://arxiv.org/abs/2308.13490))
    and ran a [Kaggle competition](https://www.kaggle.com/competitions/predict-ai-model-runtime)
    “Google — Fast or Slow? Predict AI Model Runtime” on it that showed [how to improve](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)
    learning models running on TPUs with graph machine learning. It had 792 Competitors,
    616 Teams, and 10,507 Entries. The dataset provides 25x more graphs than the largest
    graph property prediction dataset (with comparable graph sizes), and 770x larger
    graphs on average compared to existing performance prediction datasets on machine
    learning programs. This dataset is so large, a new algorithm for doing graph-level
    predictions on large-scale graphs had to be developed by [Cao et al](https://arxiv.org/abs/2305.12322).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 今年，TPU 帮助了图形机器学习，因此是时候回馈了。我们发布了一个新的**TpuGraphs**数据集（[Phothilimthana 等](https://arxiv.org/abs/2308.13490)），并基于该数据集举办了一个[Kaggle
    竞赛](https://www.kaggle.com/competitions/predict-ai-model-runtime)“Google——快还是慢？预测
    AI 模型运行时间”，该竞赛展示了[如何提升](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)在
    TPU 上运行的图形机器学习学习模型。竞赛共有792名参赛者，616个队伍，和10,507个参赛作品。该数据集提供了比现有的最大图形属性预测数据集（在图形规模相当的情况下）多出25倍的图形，且相比现有的机器学习程序性能预测数据集，平均图形规模大770倍。这个数据集如此庞大，以至于[曹等](https://arxiv.org/abs/2305.12322)不得不开发一种新的算法来进行大规模图形上的图级预测。
- en: ➡️ Large-scale graph clustering has seen significant contributions this year.
    A new approximation algorithm ([Cohen-Addad et al.](https://arxiv.org/abs/2309.17243))
    was proposed for correlation clustering improving the approximation factor from
    1.994 to the whopping 1.73\. **TeraHAC** ([Dhulipala et al](https://arxiv.org/abs/2308.03578))
    is a major improvement over last year’s **ParHAC** (that we covered in the [2023
    post](https://medium.com/towards-data-science/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232#ca19))
    — an approximate (1+𝝐) hierarchical agglomerative clustering algorithm for trillion-edge
    graphs. The largest graph used in the experiments is a massive Web-Query graph
    with 31B nodes and 8.6 trillion edges 👀. Notable mentions also go to the fastest
    (to date) algorithm for Euclidean minimum spanning tree ([Jayaram et al](https://arxiv.org/abs/2308.00503))
    and a new near-linear time algorithm for approximating the Chamfer distance between
    point sets ([Bakshi et al.](https://arxiv.org/abs/2307.03043)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 大规模图聚类在今年取得了显著进展。提出了一种新的近似算法（[Cohen-Addad 等](https://arxiv.org/abs/2309.17243)）用于相关性聚类，将近似因子从1.994提高到惊人的1.73。**TeraHAC**（[Dhulipala
    等](https://arxiv.org/abs/2308.03578)）是对去年**ParHAC**（我们在[2023年的文章](https://medium.com/towards-data-science/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232#ca19)中有提到）的一项重大改进——这是一个适用于万亿边图的近似（1+𝝐）层次聚合聚类算法。实验中使用的最大图是一个庞大的Web-Query图，包含31B节点和8.6万亿边👀。值得一提的还有目前最快的欧几里得最小生成树算法（[Jayaram
    等](https://arxiv.org/abs/2308.00503)）以及一种新的近线性时间算法，用于近似点集之间的Chamfer距离（[Bakshi
    等](https://arxiv.org/abs/2307.03043)）。
- en: '🔮 **What to expect in 2024**:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 **2024年展望**：
- en: '**1️⃣** Algorithmic advances will help scale other popular graph algorithms'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** 算法进展将帮助扩展其他流行的图算法'
- en: '**2️⃣** Novel hardware usage will help scaling up different graph models'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** 新型硬件的使用将有助于扩大不同图模型的规模'
- en: '**Predictions from the 2023 post**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023年文章中的预测**'
- en: (1) further reduction in compute costs and inference time for very large graphs
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 进一步降低大型图的计算成本和推理时间
- en: ✅ We observed order-of-magnitude speedups in clustering and node embedding.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 我们观察到聚类和节点嵌入的加速是数量级的提升。
- en: (2) Perhaps models for OGB LSC graphs could run on commodity machines instead
    of huge clusters?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 或许OGB LSC图的模型可以在普通机器上运行，而不是庞大的集群？
- en: ❌ solid no
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 坚决否定
- en: Algorithmic Reasoning & Alignment
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法推理与对齐
- en: '*Petar Veličković (Google DeepMind) and Liudmila Prokhorenkova (Yandex Research)*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*Petar Veličković（Google DeepMind）和Liudmila Prokhorenkova（Yandex Research）*'
- en: Algorithmic reasoning, a class of ML techniques able to execute algorithmic
    computation, has continued to make stable progress during 2023.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 算法推理，作为一种能够执行算法计算的机器学习技术类别，在2023年持续稳定地进展。
- en: '***Petar Veličković (Google DeepMind)***'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '***Petar Veličković（Google DeepMind）***'
- en: “2023 has been a year of steady progress for neural algorithmic reasoning models
    — it indeed remains one of the areas where GNN development gets most creative
    — probably because it has to be.” — **Petar Veličković** (Google DeepMind)
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “2023年对神经算法推理模型来说是一个稳步发展的年份——这确实是GNN发展中最具创意的领域之一——可能是因为它必须如此。” —— **Petar Veličković**（Google
    DeepMind）
- en: 'Aside from the already discussed [asynchronous algorithmic alignment](https://openreview.net/forum?id=ba4bbZ4KoF)
    work, there are three results we achieved this year that I am personally proudest
    of:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已经讨论过的[异步算法对齐](https://openreview.net/forum?id=ba4bbZ4KoF)工作外，还有三项成果是我在今年最为自豪的：
- en: 1️⃣ [DAR](https://openreview.net/forum?id=tRP0Ydz5nN) showed that pre-trained
    multi-task neural algorithmic reasoners can be scalably deployed to downstream
    graph problems — even if they are 180,000x larger than the synthetic training
    distribution of the NAR. What’s more, we set the state-of-the-art in modelling
    mouse brain vessels 🐁🧠🩸. NAR is **not** a victim of the bitter lesson! 📈
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ [DAR](https://openreview.net/forum?id=tRP0Ydz5nN)证明了预训练的多任务神经算法推理器可以在下游图问题中进行可扩展部署——即使它们比NAR的合成训练分布大180,000倍。更重要的是，我们在小鼠大脑血管建模中达到了最先进的水平
    🐁🧠🩸。NAR **不是**“苦涩教训”的受害者！📈
- en: 2️⃣ [Hint-ReLIC](https://openreview.net/forum?id=kP2p67F4G7) 🗿was our response
    to the rich body of research in [no-hint models](https://openreview.net/forum?id=xkrtvHlp3P).
    We go away from the issue-ridden *hint* *autoregression* and instead model *hint
    invariants* using causal reasoning. We obtain a potent hint-based NAR, which still
    holds state-of-the-art on broad patches of CLRS-30! *“Hints can take you a long
    way, if used in the right way.”*
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ [Hint-ReLIC](https://openreview.net/forum?id=kP2p67F4G7) 🗿是我们对[无提示模型](https://openreview.net/forum?id=xkrtvHlp3P)这一丰富研究领域的回应。我们摒弃了存在众多问题的*提示*
    *自回归*，而是利用因果推理来建模*提示不变性*。我们获得了一个强大的基于提示的NAR，并且在CLRS-30的多个领域仍保持着最先进的技术！*“如果正确使用，提示可以带你走得更远。”*
- en: 3️⃣ Last but not least, we took the plunge and made the first in-depth analysis
    of the [latent space representations of trained NAR models](https://openreview.net/forum?id=tRP0Ydz5nN).
    What we found was not only immensely beautiful to look at 🌺 but it also taught
    us a great deal about how these models work.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 最后但同样重要的是，我们迈出了重要一步，首次对[训练好的NAR模型的潜在空间表示](https://openreview.net/forum?id=tRP0Ydz5nN)进行了深入分析。我们发现的结果不仅非常美观🌺，而且还让我们学到了很多关于这些模型如何工作的知识。
- en: '![](../Images/2f82cfb2cc837b506d2bca4387e8b55c.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f82cfb2cc837b506d2bca4387e8b55c.png)'
- en: 'Left: Trajectory-wise PCA of eight clusters of reweighted graphs showing that
    they all contain a single dominant direction. Different clusters have different
    colors. Middle: Many embedding clusters with dominant directions overlaid in red.
    Right: Step-wise PCA of random graphs with the dominant cluster directions overlaid
    in red. Source: [Mirjanić, Pascanu, Veličković](https://openreview.net/forum?id=tRP0Ydz5nN)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：对八个重加权图的轨迹主成分分析（PCA），显示它们都包含一个单一的主导方向。不同的聚类具有不同的颜色。中图：许多嵌入聚类，主导方向以红色叠加。右图：随机图的逐步主成分分析，主导聚类方向以红色叠加。来源：[Mirjanić,
    Pascanu, Veličković](https://openreview.net/forum?id=tRP0Ydz5nN)
- en: Beyond growing our vibrant community, I find it important to state that many
    of NAR’s foundational ideas are at the crux of important LLM methodologies; to
    name just one example, hint following is directly related to [chain-of-thought](https://arxiv.org/abs/2201.11903)
    prompting.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了壮大我们的充满活力的社区外，我还认为有必要声明，NAR的许多基础性思想处于重要LLM方法论的核心；举一个例子，提示跟随直接与[思维链](https://arxiv.org/abs/2201.11903)提示相关。
- en: 💡 What I am most happy about is that in 2023, this link is getting explicit
    recognition, and ideas from NAR are now directly or indirectly influencing the
    most potent AI systems in use today. Indeed, NAR is listed as a key motivation
    for studying [length generalisation](https://arxiv.org/abs/2310.16028), and more
    broadly [generalisation on the unseen](https://arxiv.org/abs/2301.13105) *(ICML’23
    Best Paper Award)*. CLRS-30, the flagship NAR benchmark, is directly used to evaluate
    capabilities of LLMs in [neural architecture search](https://arxiv.org/abs/2302.14838)
    and [general AI research](https://arxiv.org/abs/2310.03302). And, as a final cherry
    on top, CLRS-30 is recognised as one of only seven reasoning evaluations used
    by [Gemini](https://arxiv.org/abs/2312.11805), a frontier large language model
    from Google DeepMind. I am hopeful that this is a beacon of things to come in
    2024, and that we will see even more ideas from NAR break into the design of frontier
    scalable AI models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 我最开心的是，在2023年，这个链接得到了明确的认可，NAR的想法现在正在直接或间接地影响当今最强大的AI系统。事实上，NAR被列为研究[长度泛化](https://arxiv.org/abs/2310.16028)的关键动机之一，更广泛地说，它对[未见数据的泛化](https://arxiv.org/abs/2301.13105)有重要影响（*ICML’23最佳论文奖*）。CLRS-30，作为NAR的旗舰基准，直接用于评估LLM在[神经架构搜索](https://arxiv.org/abs/2302.14838)和[通用AI研究](https://arxiv.org/abs/2310.03302)中的能力。而作为最后的点睛之笔，CLRS-30被认为是[Gemini](https://arxiv.org/abs/2312.11805)（Google
    DeepMind的前沿大语言模型）使用的仅有七个推理评估之一。我希望这能成为2024年未来的一个指路明灯，并且我们将看到更多来自NAR的想法打破进入前沿可扩展AI模型的设计中。
- en: '***Liudmila Prokhorenkova (Yandex Research)***'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '***Liudmila Prokhorenkova（Yandex研究员）***'
- en: 'Throughout the year, substantial progress has been achieved on the path towards
    endowing models with various algorithmic inductive biases: the use of dual problems
    [(Numeroso et al)](https://arxiv.org/abs/2302.04496), contrastive learning techniques
    ([Bevilacqua et al](https://arxiv.org/abs/2302.10258); [Rodionov et al](https://arxiv.org/abs/2306.13411)),
    augmentation of models with data structures ([Jürß et al](https://arxiv.org/abs/2307.00337);
    [Jain et a](https://arxiv.org/abs/2307.09660)l), and in-depth examination of computational
    models [(Engelmayer et al)](https://arxiv.org/abs/2307.04049). Another important
    direction is evaluating existing models in terms of scalability and data diversity
    [(Minder et al)](https://arxiv.org/abs/2309.12253).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 全年间，在赋予模型各种算法归纳偏见的道路上取得了显著进展：使用双问题[(Numeroso et al)](https://arxiv.org/abs/2302.04496)，对比学习技术([Bevilacqua
    et al](https://arxiv.org/abs/2302.10258); [Rodionov et al](https://arxiv.org/abs/2306.13411))，通过数据结构增强模型([Jürß
    et al](https://arxiv.org/abs/2307.00337); [Jain et al](https://arxiv.org/abs/2307.09660))，以及对计算模型的深入研究[(Engelmayer
    et al)](https://arxiv.org/abs/2307.04049)。另一个重要方向是评估现有模型在可扩展性和数据多样性方面的表现[(Minder
    et al)](https://arxiv.org/abs/2309.12253)。
- en: '“In 2024 it would be great to see more comprehensive analysis and understanding
    of neural reasoners: which operations they learn, how sensitive they are to different
    shifts in data distributions, what types of mistakes they tend to make and why.”
    — **Liudmila Prokhorenkova** (Yandex Research)'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在2024年，能够看到对神经推理器进行更全面的分析和理解会是很好的进展：它们学习了哪些操作，它们对数据分布的不同变化有多敏感，它们倾向于犯哪些类型的错误，以及为何如此。”
    — **Liudmila Prokhorenkova**（Yandex Research）
- en: Gaining such insights may contribute to the development of even more robust
    and scalable models. Furthermore, robust neural reasoners have the potential to
    positively impact combinatorial optimization models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 获得这些见解可能有助于开发更加稳健和可扩展的模型。此外，稳健的神经推理器有可能对组合优化模型产生积极影响。
- en: '**Predictions from the 2023 post**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023年后的预测**'
- en: (1) Algorithmic reasoning tasks are likely to scale to graphs of thousands of
    nodes and practical applications like in code analysis or databases
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 算法推理任务可能会扩展到包含成千上万个节点的图形，并在代码分析或数据库等实际应用中发挥作用。
- en: ✅ yes, [DAR](https://openreview.net/forum?id=tRP0Ydz5nN) scales to the OGB vessel
    size
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 是的，[DAR](https://openreview.net/forum?id=tRP0Ydz5nN) 能够扩展到OGB船舶大小
- en: (2) even more algorithms in the benchmark
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 基准测试中将出现更多算法
- en: ✅ yes, [SALSA-CLRS](https://arxiv.org/abs/2309.12253)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ✅ 是的，[SALSA-CLRS](https://arxiv.org/abs/2309.12253)
- en: (3) most unlikely — there will appear a model capable of solving quickselect
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 最不可能的情况——会出现一个能够解决quickselect问题的模型
- en: ❌ still unsolved ;(
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❌ 仍未解决 ;(
- en: 'Knowledge Graphs: Inductive Reasoning is Solved?'
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 知识图：归纳推理已解决？
- en: '*Michael Galkin (Intel) and Zhaocheng Zhu (Mila & Google)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*Michael Galkin（英特尔）和Zhaocheng Zhu（Mila & Google）*'
- en: 'Since its inception in 2011, the grand challenge of KG representation learning
    was truly inductive reasoning when a **single** model would be able to run inference
    (eg, missing link prediction) on any graph without input features and without
    learning hard-coded entity/relation embedding matrices. [GraIL](https://arxiv.org/abs/1911.06962)
    (ICML’20) and [Neural Bellman-Ford Nets](https://arxiv.org/abs/2106.06935) (NeurIPS’21)
    were instrumental in extending inference to unseen entities, but generalization
    to both new entities and relation types at inference time remained an unsolved
    challenge due to the main question: what can be learned and transferred when the
    whole entity/relation vocabulary can change?'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 自2011年成立以来，知识图表示学习的重大挑战之一就是归纳推理：一个**单一**模型能够在没有输入特征和硬编码的实体/关系嵌入矩阵的情况下，执行任何图上的推理（例如，缺失链接预测）。[GraIL](https://arxiv.org/abs/1911.06962)（ICML'20）和[Neural
    Bellman-Ford Nets](https://arxiv.org/abs/2106.06935)（NeurIPS'21）在扩展推理到未见过的实体方面起到了重要作用，但由于一个关键问题：当整个实体/关系词汇可能发生变化时，如何学习和迁移，推理时对新实体和关系类型的泛化仍然是未解决的挑战。
- en: '🔮 Our prediction for 2023 (an inductive model fully transferable to different
    KGs with new sets of entities and relations, e.g., training on Wikidata, and running
    inference on DBpedia or Freebase) came true in several works:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 我们对2023年的预测（一个完全可转移到不同知识图（KG）的归纳模型，可以处理新的实体和关系集合，例如在Wikidata上进行训练，并在DBpedia或Freebase上运行推理）在多项工作中得以验证：
- en: '[Gao et al](https://arxiv.org/abs/2302.01313) introduced the concept of double
    equivariance that forces the neural net to be equivariant to permutations of both
    node IDs and relation IDs. The proposed ISDEA++ model employs a [DSS-GNN](https://arxiv.org/abs/2110.02910)-like
    aggregation of a relation-induced subgraph and a subgraph induced by all other
    relation types.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gao et al](https://arxiv.org/abs/2302.01313)提出了双重等变性（double equivariance）概念，强制神经网络对节点ID和关系ID的排列保持等变性。所提出的ISDEA++模型采用了类似于[DSS-GNN](https://arxiv.org/abs/2110.02910)的关系诱导子图和由所有其他关系类型诱导的子图的聚合。'
- en: '[ULTRA](https://github.com/DeepGraphLearning/ULTRA) introduced by [Galkin et
    al](https://arxiv.org/abs/2310.04562) learns the invariance of relation interactions
    (captured by a graph of relations) and transfers to absolutely any multi-relational
    graph. ULTRA achieves SOTA results on dozens of transductive and inductive datasets
    even in the zero-shot inference setup. Besides, it enables a foundation model-like
    approach for KG reasoning with generic pre-training, zero-shot inference, and
    task-specific fine-tuning.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ULTRA](https://github.com/DeepGraphLearning/ULTRA)由[Galkin et al](https://arxiv.org/abs/2310.04562)提出，学习关系交互的不变性（通过关系图捕捉）并迁移到任何多关系图。即使在零-shot推理设置下，ULTRA也能在数十个归纳和传导数据集上取得SOTA结果。此外，它还为KG推理提供了类似基础模型的方法，包括通用预训练、零-shot推理和任务特定的微调。'
- en: '![](../Images/849215a4796605a7efc5a4809308de67.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/849215a4796605a7efc5a4809308de67.png)'
- en: 'Three main steps taken by ULTRA: (1) building a relation graph; (2) running
    conditional message passing over the relation graph to get relative relation representations;
    (3) use those representations for inductive link predictor GNN on the entity level.
    Source: [Galkin et al](https://arxiv.org/abs/2310.04562)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRA的三个主要步骤：（1）构建关系图；（2）在关系图上进行条件消息传递以获取相对的关系表示；（3）利用这些表示作为归纳链接预测GNN在实体层面上的输入。来源：[Galkin
    et al](https://arxiv.org/abs/2310.04562)
- en: 'Learn more about inductive reasoning in the recent blog post:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于归纳推理的信息，详见最近的博客文章：
- en: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
    [## ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
    [## ULTRA: 知识图谱推理的基础模型'
- en: One model to rule them all
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个模型统治所有
- en: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
- en: As the grand challenge seems to be solved now, is there anything left for KG
    research, or we should call it a day, throw a party, and move on?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这个重大挑战似乎已经解决，还有什么留给知识图谱（KG）研究的吗？我们是不是该庆祝一下，结束这一天，开个派对，然后继续前进？
- en: '***Michael Galkin (Intel)***'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '***Michael Galkin (Intel)***'
- en: “Indeed, with the grand challenge solved, it feels a bit like an existential
    crisis — everything important is invented, Graph ML enabled things that looked
    impossible just 5 years ago. Perhaps, KG community should re-invent itself and
    focus on practical problems that can be tackled with graph foundation models.
    Otherwise, the subfield would disappear from research radars like Semantic Web”
    — Michael Galkin (Intel)
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “事实上，随着这个重大挑战的解决，感觉有些像是一次生存危机——一切重要的东西都已经发明了，图形机器学习使得五年前看似不可能的事情变成了现实。也许，KG社区应该重新发明自己，专注于那些可以通过图基础模型解决的实际问题。否则，子领域将像语义网那样从研究雷达中消失。”——Michael
    Galkin（Intel）
- en: Transductive and shallow KG embeddings are dead and nobody in 2024 should work
    on them, it is time to retire them for good. ULTRA-like foundation models can
    now work without training on any graph which is a sweet spot for many closed enterprise
    KGs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳性和浅层KG嵌入已经过时，2024年没有人应该再研究它们，是时候永久退休了。类似ULTRA的基础模型现在可以在任何图上运行，而不需要进行训练，这对于许多封闭企业的KG来说是一个理想的选择。
- en: ➡️ The last uncharted territory is inductive reasoning beyond simple link prediction
    ([complex database-like logical queries](https://medium.com/towards-data-science/neural-graph-databases-cc35c9e1d04f))
    and I think it will also be solved in 2024\. Adding temporal aspects, LLM node
    features, or scaling GNNs for larger graphs is a question of time and presents
    more of an engineering task than a research question.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 最后一个未探索的领域是超越简单链接预测的归纳推理（[复杂的类似数据库的逻辑查询](https://medium.com/towards-data-science/neural-graph-databases-cc35c9e1d04f)），我认为它将在2024年解决。加入时间维度、LLM节点特征或扩大GNN以处理更大图的任务，已成为时间问题，并且更多的是工程任务，而非研究问题。
- en: '***Zhaocheng Zhu (Mila & Google)***'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '***Zhaocheng Zhu (Mila & Google)***'
- en: “With the rise of LLMs and numerous prompt-based reasoning techniques, it looks
    like **KG reasoning is coming to an end**. Texts are more expressive and flexible
    than KGs, and meanwhile they are more available in quantity. However, I don’t
    think the reasoning techniques that the KG community developed are in vain.” —
    Zhaocheng Zhu (Mila & Google)
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “随着LLM和众多基于提示的推理技术的兴起，似乎**知识图谱推理已经走到了尽头**。文本比知识图谱更具表现力和灵活性，同时它们在数量上也更加丰富。然而，我不认为知识图谱社区开发的推理技术是徒劳的。”——朱兆诚（Mila
    & 谷歌）
- en: ➡️ We see that many LLM reasoning methods coincide with well-known ideas on
    KGs. For instance, the difference between direct prompting and chain-of-thought
    (CoT) shares much spirit with embedding methods and path-based methods on KGs,
    where the latter ones parameterize smaller steps and thereby generalize better
    to new combinations of steps. In fact, topics like inductive and multi-step generalization
    were explored on KGs several years earlier than on LLMs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 我们看到许多LLM推理方法与知识图谱中的一些著名思想相吻合。例如，直接提示与思维链（CoT）之间的区别，与知识图谱中的嵌入方法和基于路径的方法有很多相似之处，后者通过参数化更小的步骤，从而能够更好地推广到新的步骤组合。事实上，归纳性和多步骤推广等主题，比LLM更早几年就在知识图谱上进行了探讨。
- en: When we develop new techniques for LLMs, it is essential to take a glance at
    similar goals and solutions on KGs. In brief, while the modality of KGs *may fade
    at some point*, the insights we learned from KG reasoning will continue to illuminate
    in the era of LLMs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为LLM开发新技术时，回顾知识图谱（KG）中类似目标和解决方案是至关重要的。简而言之，尽管知识图谱的表现形式*可能会在某个时刻消失*，但我们从知识图谱推理中学到的洞见将继续在LLM时代发挥作用。
- en: Temporal Graph Learning
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序图学习
- en: Shenyang Huang, Emanuele Rossi, Andrea Cini, Ingo Scholtes, and Michael Galkin
    prepared a separate overview post on temporal graph learning!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 黄沈阳、埃马纽埃尔·罗西、安德烈亚·奇尼、因戈·舒尔特斯和迈克尔·高尔金准备了一篇关于时序图学习的独立概述文章！
- en: '[](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
    [## Temporal Graph Learning in 2024'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
    [## 2024年时序图学习'
- en: Continue the journey for evolving networks
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 继续推进网络的演变之旅
- en: towardsdatascience.com](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
- en: LLMs + Graphs for Scientific Discovery
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM + 图用于科学发现
- en: '*Michael Galkin (Intel)*'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*迈克尔·高尔金（英特尔）*'
- en: 💡LLMs were everywhere in 2023 and it’s hard to miss the 🐘 in the room.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 💡2023年LLM无处不在，很难忽视房间里的大象🐘。
- en: “We have seen a flurry of approaches trying to marry graphs with LLMs. The subfield
    is emerging and **making its tiny baby steps** which are important to acknowledge.”
    — Michael Galkin (Intel)
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们已经看到许多方法尝试将图与LLM结合。这个子领域正在发展，并且**迈出了它的小步伐**，这是需要认可的。”——迈克尔·高尔金（英特尔）
- en: We have seen a flurry of approaches trying to marry graphs with LLMs (sometimes
    literally verbalizing the edges in a text prompt) where straightforward prompting
    with edge index does not really work for running graph algorithms with language
    models, so the crux is in the “text linearization” and proper prompting. Among
    the notable mentions, you might be interested in **GraphText** by [Zhao et al](https://arxiv.org/abs/2310.01089)
    that devises a *graph syntax tree* prompt constructed from features and labels
    in the ego-subgraph of a target node — GraphText works for node classification.
    In **Talk Like a Graph** by [Fatemi et al](https://arxiv.org/abs/2310.04560) the
    authors study graph linearization strategies and how they impact LLM performance
    on basic tasks like edge existence, node count, or cycle check.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到许多方法尝试将图与大型语言模型（LLM）结合（有时字面上是在文本提示中表达边缘），其中直接通过边索引进行提示并不真正适用于运行图算法与语言模型，因此关键在于“文本线性化”和合适的提示。在一些值得注意的研究中，你可能会对[赵等人](https://arxiv.org/abs/2310.01089)的**GraphText**感兴趣，该方法设计了一种由特征和标签构成的*图语法树*提示，这些特征和标签来自目标节点的自我子图——GraphText适用于节点分类。在[Fatemi等人](https://arxiv.org/abs/2310.04560)的**Talk
    Like a Graph**中，作者研究了图的线性化策略及其如何影响LLM在基本任务（如边的存在、节点计数或环检查）上的表现。
- en: '![](../Images/4c313a55121a3192694ee0c6d387a706.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c313a55121a3192694ee0c6d387a706.png)'
- en: 'Standard GNNs (left) and GraphText (right). GraphText encodes the graph information
    into text sequences and uses LLM to perform inference. The graph-syntax tree contains
    both node attributes (e.g. feature and label) and relationships (e.g. center-node,
    1st-hop, and 2nd-hop). Source: [Zhao et al](https://arxiv.org/abs/2310.01089)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 标准GNN（左）与GraphText（右）。GraphText将图信息编码成文本序列，并使用LLM进行推理。图语法树包含节点属性（如特征和标签）以及关系（如中心节点、第一跳和第二跳）。来源：[赵等人](https://arxiv.org/abs/2310.01089)
- en: ➡️ Despite the early stage, there exist already 3 recent surveys ([Li et al](https://arxiv.org/abs/2311.12399),
    [Jin et al](https://arxiv.org/abs/2312.02783), [Sun et al](https://arxiv.org/abs/2311.16534))
    covering dozens of prompting approaches for graphs. Generally, it is yet to be
    seen **whether** **LLMs are an appropriate hammer** 🔨 for a specific *graph* nail
    given all the limitations of the autoregressive decoding, small context sizes,
    and permutation-invariant nature of graph tasks. If you are broadly interested
    in LLM reasoning, check out [our recent blog post](/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d)
    covering the main areas and progress made in 2023.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ 尽管处于早期阶段，但已经有三篇近期的综述文章（[Li等人](https://arxiv.org/abs/2311.12399)，[Jin等人](https://arxiv.org/abs/2312.02783)，[Sun等人](https://arxiv.org/abs/2311.16534)）涵盖了多种图的提示方法。总体而言，目前还不清楚**LLM是否适合用作**
    🔨 特定*图*任务的合适工具，因为自回归解码、小上下文窗口和图任务的排列不变性等限制。如果你对LLM推理有广泛兴趣，可以查看[我们最近的博客文章](/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d)，它总结了2023年在这一领域的主要进展。
- en: '➡️ LLMs in applied scientific tasks exhibit more promising, sometimes quite
    unexpected results: **ChemCrow** 🐦‍⬛ by [Bran, Cox, et al](https://arxiv.org/abs/2304.05376)
    is an LLM agent powered with tools that can perform tasks in organic chemistry,
    synthesis, and material design right in natural language (without fancy equivariant
    GNNs). For example, with a query “*Find and synthesize a thiourea organocatalyst
    which accelerates a Diels-Alder reaction*” ChemCrow devises a sequence of actions
    starting from a basic SMILES string and ending up with instructions to a synthesis
    platform.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ➡️ LLM在应用科学任务中展现了更有前景的、有时甚至是意想不到的结果：[Bran, Cox等人](https://arxiv.org/abs/2304.05376)的**ChemCrow**
    🐦‍⬛是一个由工具驱动的LLM代理，可以直接用自然语言执行有机化学、合成和材料设计任务（无需复杂的等变GNN）。例如，查询“*寻找并合成一种能够加速Diels-Alder反应的硫脲有机催化剂*”时，ChemCrow从基本的SMILES字符串开始，设计出一系列操作，最终给出合成平台的操作指令。
- en: Similarly, [Gruver et al](https://openreview.net/forum?id=0r5DE2ZSwJ) fine-tuned
    LLaMA-2 to generate 3D crystal structures as a plain text file with lattice parameters,
    atomic composition, and 3D coordinates and it is surprisingly competitive with
    SOTA geometric diffusion models like CDVAE.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，[Gruver等人](https://openreview.net/forum?id=0r5DE2ZSwJ)对LLaMA-2进行了微调，使其生成包含晶格参数、原子组成和3D坐标的3D晶体结构纯文本文件，令人惊讶的是，它与像CDVAE这样的SOTA几何扩散模型竞争力十足。
- en: '![](../Images/0cf27820c04f943ef14d5fc406665457.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cf27820c04f943ef14d5fc406665457.png)'
- en: 'Experimental validation. a) Example of the script run by a user to initiate
    ChemCrow. b) Query and synthesis of a thiourea organocatalyst. c) The IBM Research
    RoboRXN synthesis platform on which the experiments were executed (pictures reprinted
    courtesy of International Business Machines Corporation). d) Experimentally validated
    compounds. Source: [Bran, Cox, et al](https://arxiv.org/abs/2304.05376)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 实验验证。a) 用户运行脚本以启动ChemCrow的示例。b) 对硫脲有机催化剂的查询与合成。c) 实验执行的IBM研究RoboRXN合成平台（图片转载自国际商业机器公司）。d)
    实验验证的化合物。来源：[Bran, Cox等人](https://arxiv.org/abs/2304.05376)
- en: '🔮 In 2024, scientific applications of LLMs are likely to expand both breadth-wise
    and depth-wise:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 在2024年，LLM的科学应用预计将在广度和深度上都得到扩展：
- en: 1️⃣ Reaching out to more AI4Science areas;
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 扩展到更多的AI4Science领域；
- en: 2️⃣ Integration with geometric foundation models (since multi-modality is the
    main LLM focus for the coming year);
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 与几何基础模型的集成（因为多模态是未来一年LLM的主要关注点）；
- en: '3️⃣ Hot take: LLMs will solve the *quickselect* task in the CLRS-30 benchmark
    before GNNs do 🔥'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 热点话题：LLM将比GNN更快地解决CLRS-30基准中的*quickselect*任务🔥
- en: Cool GNN Applications
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 酷炫的GNN应用
- en: '*Petar Veličković (Google DeepMind)*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*Petar Veličković（Google DeepMind）*'
- en: In my standard deck motivating the use of GNNs to a broader audience, I rely
    on a usual “arsenal” slide of impactful GNN applications over the years. With
    2023 being significantly marked by LLM developments, I was wondering — can I meaningfully
    update this slide, but only using models released this year?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在我用GNN向更广泛的观众展示其应用时，我通常依赖一张“兵器库”幻灯片，展示这些年来影响深远的GNN应用。随着2023年LLM发展的显著标志，我在想——我能否仅使用今年发布的模型，来有意义地更新这张幻灯片呢？
- en: “It was the middle of the year back then, and already I was in for a nice surprise;
    *I did not have enough space to list all the awesome things done with GNNs!” —*
    **Petar Veličković** (Google DeepMind)
  id: totrans-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “那时正是年中的时候，我已经有了一个惊喜；*我没有足够的空间列出所有使用GNN做出的精彩成果！*”——**Petar Veličković**（Google
    DeepMind）
- en: 💡 While it might have gone comparatively under the radar, I confidently claim
    that 2023 was the **most exciting year** for cool GNN applications! The rise of
    LLMs just made it very clear where the limits of text-based autoregressive models
    are, and that for most scientific problems coming from Nature, their graph structure
    cannot be ignored.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 虽然这一点可能相对不太引起注意，但我自信地声称，2023年是**最激动人心的一年**，因为在许多酷炫的GNN应用中！LLM的兴起清楚地表明了基于文本的自回归模型的局限性，而对于大多数来自自然界的科学问题，其图结构是不能被忽视的。
- en: 'Here’s a handful of my personal favourite landmark results — all published
    in top-tier venues:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出的是我个人最喜欢的几个标志性成果——所有这些都发表在顶级期刊上：
- en: '[GraphCast](https://www.science.org/doi/10.1126/science.adi2336) provided us
    a landmark model for medium-range global weather forecasting ⛈️ and with it, more
    accurate foreshadowing of extreme events such as hurricanes. A highly well-deserved
    cover of Science!'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GraphCast](https://www.science.org/doi/10.1126/science.adi2336)为我们提供了一个标志性的模型，用于中范围的全球天气预报⛈️，并借此更加准确地预测极端事件，如飓风。这是《科学》杂志封面上应得的荣誉！'
- en: In an outstanding development in materials science, [GNoME](https://www.nature.com/articles/s41586-023-06735-9)
    uses a GNN-based model to discover *millions* of novel crystal structures 💎 —
    an *“order-of-magnitude expansion in stable materials known to humanity”*. Published
    in Nature.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在材料科学领域的一个杰出发展中，[GNoME](https://www.nature.com/articles/s41586-023-06735-9)使用基于GNN的模型发现了*数百万*种新型晶体结构💎——这被称为*“人类已知稳定材料的数量级扩展”*。已发表在《自然》杂志。
- en: We’ve been treated to not just [one](https://www.nature.com/articles/s41589-023-01349-8),
    but [two](https://www.nature.com/articles/s41586-023-06887-8) new breakthroughs
    in antibiotic discovery 💊 using message passing neural networks — the latter being
    published in Nature!
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不仅迎来了[一项](https://www.nature.com/articles/s41589-023-01349-8)，而且迎来了[两项](https://www.nature.com/articles/s41586-023-06887-8)在抗生素发现方面的突破💊，这两项成果都采用了消息传递神经网络——后者已发表在《自然》杂志！
- en: '[GNNs can smell](https://www.science.org/doi/10.1126/science.ade4401) 👃 by
    observing the molecular structure emitting an odour — a result that may well revolutionise
    many industries, including perfumes! Published in Science.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GNN可以嗅觉](https://www.science.org/doi/10.1126/science.ade4401) 👃 通过观察分子结构发出的气味——这一发现可能会彻底改变多个行业，包括香水行业！已发表在《科学》杂志。'
- en: On the cover of Nature Machine Intelligence, [HYFA](https://www.nature.com/articles/s42256-023-00684-8)
    🍄 shows how to use hypergraph factorisation to make significant progress in gene
    expression imputation 🧬!
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在《自然机器智能》杂志的封面上，[HYFA](https://www.nature.com/articles/s42256-023-00684-8)🍄展示了如何使用超图分解法在基因表达填补问题上取得显著进展🧬！
- en: Last but not least, particle physics ⚛️ remains a natural stronghold of GNN
    applications. In this year’s Nature Physics Review, we have been treated to a
    [fascinating survey](https://www.nature.com/articles/s42254-023-00569-0) elucidating
    the myriad of ways how graph neural networks are deployed for various data analysis
    tasks at the Large Hadron Collider ⚡.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，粒子物理学⚛️仍然是GNN应用的自然强项。在今年的《自然物理学评论》上，我们欣赏到了一篇[精彩的综述](https://www.nature.com/articles/s42254-023-00569-0)，阐明了图神经网络在大型强子对撞机⚡中进行各种数据分析任务的多种应用方式。
- en: ⚽ My own humble contribution to the space of GNN applications this year was
    [TacticAI](https://arxiv.org/abs/2310.10553), the *first full AI system giving
    useful tactical suggestions to (association) football coaches*, developed in partnership
    with our collaborators at Liverpool FC 🔴. TacticAI is capable of both predictive
    modelling (*“what will happen in this tactical scenario?”*), retrieving similar
    tactics, and conditional generative modelling (*“how to modify player positions
    to make a particular outcome happen?”*). In my opinion, the most satisfying part
    of this very fun collaboration was our user study with some of LFC’s top coaching
    staff — directly illustrating that the outputs of our model will be of use to
    coaches in their work 🏃.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ⚽ 我今年在GNN应用领域的谦逊贡献是[TacticAI](https://arxiv.org/abs/2310.10553)，它是*首个为（足球）教练提供有用战术建议的完整AI系统*，与我们在利物浦足球俱乐部的合作伙伴共同开发
    🔴。TacticAI能够进行预测建模（*“这个战术场景中会发生什么？”*）、检索相似战术，并进行条件生成建模（*“如何调整球员位置以实现特定结果？”*）。在我看来，这个非常有趣的合作中，最令人满意的部分是我们与利物浦俱乐部顶级教练团队的用户研究——直接表明我们模型的输出对教练们的工作是有用的
    🏃。
- en: '![](../Images/a05d2b481488396a23817f3106efd65f.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a05d2b481488396a23817f3106efd65f.png)'
- en: 'A “bird’s eye” overview of TacticAI. (A), how corner kick situations are converted
    to a graph representation. Each player is treated as a node in a graph, with node,
    edge and graph features extracted as detailed in the main text. Then, a graph
    neural network operates over this graph by performing message passing; each node’s
    representation is updated using the messages sent to it from its neighbouring
    nodes. (B), how TacticAI processes a given corner kick. To ensure that TacticAI’s
    answers are robust in the face of horizontal or vertical reflections, all possible
    combinations of reflections are applied to the input corner, and these four views
    are then fed to the core TacticAI model, where they are able to interact with
    each other to compute the final player representations — each “internal blue arrow”
    corresponds to a single message passing layer from (A). Once player representations
    are computed, they can be used to predict the corner’s receiver, whether a shot
    has been taken, as well as assistive adjustments to player positions and velocities,
    which increase or decrease the probability of a shot being taken. Source: [Wang,
    Veličković, Hennes et al.](https://arxiv.org/abs/2310.10553)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: TacticAI的“鸟瞰”概览。（A）展示了角球情况如何转化为图表示。每个球员被视为图中的一个节点，节点、边和图的特征如正文中所述提取。然后，图神经网络在这个图上进行消息传递操作；每个节点的表示通过从其邻近节点接收到的消息进行更新。（B）展示了TacticAI如何处理给定的角球。为了确保TacticAI在面对水平或垂直反射时能够提供稳健的答案，所有可能的反射组合都会应用到输入角球上，随后这四种视图被输入到核心TacticAI模型中，在这些视图之间相互作用，以计算最终的球员表示——每个“内部蓝色箭头”对应于（A）中的单个消息传递层。一旦计算出球员表示，它们就可以用来预测角球的接球者，是否已射门，以及对球员位置和速度的辅助调整，这些调整会增加或减少射门的概率。来源：[Wang,
    Veličković, Hennes et al.](https://arxiv.org/abs/2310.10553)
- en: This is what I’m all about — AI systems that significantly augment human abilities.
    I can only hope that, in my home country, Partizan catches on to these methods
    before Red Star does! 😅
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我的追求——显著增强人类能力的AI系统。我只能希望，在我的祖国，帕尔蒂赞能比红星先采纳这些方法！😅
- en: 🔮 What will we see in 2024? Probably more of the same, just accelerated! ⏩
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 2024年我们会看到什么？可能会是相同的内容，只是加速了！⏩
- en: Geometric Wall Street Bulletin 💸
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 几何华尔街公报 💸
- en: '*Nathan Benaich (AirStreet Capital)****,*** *Michael Bronstein (Oxford), and
    Luca Naef (VantAI)*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nathan Benaich (AirStreet Capital)*, *Michael Bronstein (Oxford)* 和 *Luca
    Naef (VantAI)*'
- en: 2023 started with BioNTech (mostly known to the broad public for developing
    mRNA SARS-CoV-2 vaccines) [announcing the acquisition of InstaDeep](https://www.instadeep.com/2023/01/biontech-to-acquire-instadeep-to-strengthen-pioneering-position-in-the-field-of-ai-powered-drug-discovery-design-and-development/),
    a decade-old British company focused on AI-powered drug discovery, design and
    development. In May 2023, Recursion [acquired two startups](https://ir.recursion.com/news-releases/news-release-details/recursion-enters-agreements-acquire-cyclica-and-valence-bolster),
    Cyclica and Valence “to bolster chemistry and generative AI capabilities”. Valence
    ML team is well-known for multiple works in the geometric and graph ML and hosting
    the **Graphs & Geometry and Molecular Modeling** & **Drug Discovery seminars**
    on [YouTube](https://www.youtube.com/@valence_labs).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年开始时，BioNTech（广为人知的是开发了mRNA SARS-CoV-2疫苗）[宣布收购InstaDeep](https://www.instadeep.com/2023/01/biontech-to-acquire-instadeep-to-strengthen-pioneering-position-in-the-field-of-ai-powered-drug-discovery-design-and-development/)，这是一家专注于人工智能驱动药物发现、设计和开发的成立十年的英国公司。2023年5月，Recursion
    [收购了两家初创公司](https://ir.recursion.com/news-releases/news-release-details/recursion-enters-agreements-acquire-cyclica-and-valence-bolster)，Cyclica和Valence，“以增强化学和生成性AI能力”。Valence
    ML团队因在几何和图形机器学习领域的多项工作而著名，并在[YouTube](https://www.youtube.com/@valence_labs)上举办**图形与几何和分子建模**与**药物发现研讨会**。
- en: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)Isomorphic
    Labs started 2024 by announcing small molecule-focused [collaborations](https://www.isomorphiclabs.com/articles/isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations)
    with Eli Lilly and Novartis with upfront payments of $45M and $37.5M, respectively,
    with the potential worth of **$3 billion**.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)Isomorphic
    Labs以宣布与Eli Lilly和Novartis进行小分子药物相关的[合作](https://www.isomorphiclabs.com/articles/isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations)开始了2024年，分别获得4500万美元和3750万美元的预付款，潜在价值为**30亿美元**。'
- en: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)[VantAI
    partnered with Blueprint Medicines](https://www.businesswire.com/news/home/20240108659035/en/VantAI-Secures-Renewed-Support-from-Blueprint-Medicines-to-Chart-New-Frontiers-in-Induced-Proximity-Drug-Discovery)
    on innovative proximity modulating therapeutics, including molecular glue and
    hetero-bifunctional candidates. The deal’s potential worth is $1.25 billion.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)[VantAI与Blueprint
    Medicines](https://www.businesswire.com/news/home/20240108659035/en/VantAI-Secures-Renewed-Support-from-Blueprint-Medicines-to-Chart-New-Frontiers-in-Induced-Proximity-Drug-Discovery)合作，共同开发创新的邻近调节治疗方法，包括分子胶和异二功能候选药物。该交易的潜在价值为12.5亿美元。'
- en: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)CHARM Therapeutics
    raised more funding [from NVIDIA](https://www.businesswire.com/news/home/20230515005172/en/CHARM-Therapeutics-Receives-Investment-for-Deep-Learning-Enabled-Drug-Discovery-Research-from-NVIDIA)
    and [from Bristol Myers Squibb](https://www.businesswire.com/news/home/20230320005101/en/CHARM-Therapeutics-Announces-Collaboration-with-Bristol-Myers-Squibb-to-Enable-and-Accelerate-Small-Molecule-Drug-Discovery-Programs)
    totalling the initial funding round to $70M. The company has developed DragonFold,
    its proprietary algorithm for protein-ligand co-folding.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[💰](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)CHARM Therapeutics获得更多资金，资金来自[NVIDIA](https://www.businesswire.com/news/home/20230515005172/en/CHARM-Therapeutics-Receives-Investment-for-Deep-Learning-Enabled-Drug-Discovery-Research-from-NVIDIA)和[Bristol
    Myers Squibb](https://www.businesswire.com/news/home/20230320005101/en/CHARM-Therapeutics-Announces-Collaboration-with-Bristol-Myers-Squibb-to-Enable-and-Accelerate-Small-Molecule-Drug-Discovery-Programs)，使初始融资总额达到7000万美元。该公司开发了DragonFold，这是其专有的蛋白质-配体共折叠算法。'
- en: 💊 Monte Rosa [announced a successful](https://ir.monterosatx.com/news-releases/news-release-details/monte-rosa-therapeutics-announces-interim-pkpd-and-clinical-data)
    Phase 1 study of MRT-2359 (orally bioavailable investigational molecular glue
    degrader) against MYC-driven tumors like lung cancer and neuroendocrine cancer.
    Monte Rosa is known to [use geometric deep learning](https://ir.monterosatx.com/static-files/8806793a-99fb-4df8-8eb7-3785b39cf210)
    for proteins ([MaSIF](https://www.nature.com/articles/s41592-019-0666-6)).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 💊 Monte Rosa [宣布了MRT-2359的成功](https://ir.monterosatx.com/news-releases/news-release-details/monte-rosa-therapeutics-announces-interim-pkpd-and-clinical-data)1期研究（口服生物可利用的研究性分子胶降解剂），针对MYC驱动的肿瘤，如肺癌和神经内分泌癌。Monte
    Rosa以[使用几何深度学习](https://ir.monterosatx.com/static-files/8806793a-99fb-4df8-8eb7-3785b39cf210)进行蛋白质研究（[MaSIF](https://www.nature.com/articles/s41592-019-0666-6)）而闻名。
- en: '***Nathan Benaich (AirStreet Capital, author of*** [***the State of AI Report***](https://www.stateof.ai/)***)***'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '***Nathan Benaich（AirStreet Capital，《AI现状报告》作者*** [***the State of AI Report***](https://www.stateof.ai/)***)***'
- en: “I have long been optimistic about the potential of AI-first approaches to design
    problems in medicine, biotech, and materials science. Graph-based models had a
    great year in techbio in 2023.” — Nathan Benaich (AirStreet Capital)
  id: totrans-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我一直对AI优先的方法在医学、生物技术和材料科学中解决问题的潜力持乐观态度。基于图的模型在2023年技术生物学领域表现突出。” — Nathan Benaich（AirStreet
    Capital）
- en: '[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8) combines
    diffusion techniques with GNNs to predict protein structures. It denoises blurry
    or corrupted structures from the Protein Data Bank, while tapping into RoseTTAFold’s
    prediction capabilities. DeepMind have continued to further develop AlphaFold
    and build on top of it. Their [AlphaMissense](https://www.science.org/doi/10.1126/science.adg7492)
    uses weak labels, language modeling, and AlphaFold to predict the pathogenicity
    of 71 million human variants. This is an important achievement, as most amino
    acid changes from genetic variation have unknown effects.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8)将扩散技术与图神经网络（GNN）结合，用于预测蛋白质结构。它通过去噪来自蛋白质数据银行（Protein
    Data Bank）的模糊或损坏的结构，同时利用RoseTTAFold的预测能力。DeepMind继续进一步开发AlphaFold，并在其基础上进行扩展。他们的[AlphaMissense](https://www.science.org/doi/10.1126/science.adg7492)使用弱标签、语言建模和AlphaFold来预测7100万个人体变异的致病性。这是一个重要的成就，因为大多数由遗传变异引起的氨基酸变化的影响仍不明确。'
- en: Beyond proteins, graph-based models have been improving our understanding of
    genetics. Stanford’s [GEARS](https://www.nature.com/articles/s41587-023-01905-6.pdf)
    system integrates deep learning with a gene interaction knowledge graph to predict
    gene expression changes from combinatorial perturbations. By leveraging prior
    data on single and double perturbations, GEARS can predict outcomes for thousands
    of gene pairs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 除了蛋白质，基于图的模型也在提升我们对遗传学的理解。斯坦福的[GEARS](https://www.nature.com/articles/s41587-023-01905-6.pdf)系统将深度学习与基因互作知识图谱结合，能够从组合扰动中预测基因表达变化。通过利用单一和双重扰动的先前数据，GEARS能够预测成千上万个基因对的结果。
- en: '![](../Images/34d2a1814ebb3bb913bd1f9cd617b0d2.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34d2a1814ebb3bb913bd1f9cd617b0d2.png)'
- en: 'GEARS can predict new biologically meaningful phenotypes. (a) Workflow for
    predicting all pairwise combinatorial perturbation outcomes of a set of genes.
    (b) Low-dimensional representation of postperturbation gene expression for 102
    one-gene perturbations and 128 two-gene perturbations used to train GEARS. A random
    selection is labeled. (c) GEARS predicts postperturbation gene expression for
    all 5,151 pairwise combinations of the 102 single genes seen experimentally perturbed.
    Predicted postperturbation phenotypes (non-black symbols) are often different
    from phenotypes seen experimentally (black symbols). Colors indicate Leiden clusters
    labeled using marker gene expression. Source: [Roohani et al](https://www.nature.com/articles/s41587-023-01905-6)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: GEARS能够预测新的生物学上有意义的表型。(a) 用于预测一组基因的所有成对组合扰动结果的工作流程。(b) 102种单基因扰动和128种双基因扰动的扰动后基因表达的低维表示，用于训练GEARS。随机选择的数据已标注。(c)
    GEARS预测所有5,151对组合的102种单基因扰动的扰动后基因表达。预测的扰动后表型（非黑色符号）通常与实验中观察到的表型（黑色符号）不同。颜色表示使用标记基因表达进行标记的Leiden簇。来源：[Roohani
    et al](https://www.nature.com/articles/s41587-023-01905-6)
- en: 🔮 In 2024, I put hope in two different developments.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 🔮 在2024年，我对两个不同的发展方向寄予希望。
- en: '**1️⃣** We have seen the first two CRISPR-Cas9 therapies approved in the US
    and the UK. These genome editors were discovered through sequencing and random
    experimentation. I am excited about the use of AI models to design and create
    bespoke editors on demand.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**1️⃣** 我们已经看到了美国和英国批准的首两种CRISPR-Cas9疗法。这些基因组编辑器是通过测序和随机实验发现的。我对利用AI模型按需设计和创建定制化编辑器感到兴奋。'
- en: '**2️⃣** We have started to see multimodality come to the AI bio world — combining
    DNA, RNA, protein, cellular, and imaging data to give us a more holistic understanding
    of biology.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**2️⃣** 我们已经开始看到多模态技术进入AI生物领域 —— 结合DNA、RNA、蛋白质、细胞和成像数据，为我们提供更全面的生物学理解。'
- en: '**Companies to watch in 2024**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**2024年值得关注的公司**'
- en: '[Profluent](https://www.profluent.bio/) — LLMs for protein design'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Profluent](https://www.profluent.bio/) — 用于蛋白质设计的大型语言模型（LLM）'
- en: '[Inceptive.bio](https://inceptive.life/) — founded by one of the authors of
    the Transformers paper.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Inceptive.bio](https://inceptive.life/) — 由《Transformers》论文的作者之一创办。'
- en: '[Enveda Biosciences](https://www.envedabio.com/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Enveda Biosciences](https://www.envedabio.com/)'
- en: '[Orbital Materials](https://orbitalmaterials.com/)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Orbital Materials](https://orbitalmaterials.com/)'
- en: '[Kumo.AI](https://kumo.ai/)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kumo.AI](https://kumo.ai/)'
- en: '[VantAI](https://www.vant.ai/) — we are biased (Michael Bronstein is Vant’s
    Chief Scientist and Luca Naef is a founder and CTO), but this is a cool company
    focused on the rational design of molecular glues using a combination of ML and
    proprietary experimental technology, which we believe to be the right combination
    for success.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VantAI](https://www.vant.ai/) — 我们可能有些偏见（迈克尔·布朗斯坦是Vant的首席科学家，卢卡·纳夫是创始人兼CTO），但这是一家很酷的公司，专注于利用机器学习和专有实验技术的结合，理性设计分子胶，这种组合我们认为是成功的关键。'
- en: '[Future House](https://www.futurehouse.org/articles/announcing-future-house)
    — a new Silicon Valley-based non-profit company in the AI4Science space funded
    by ex-Google CEO Eric Schmidt. Head of Science is Andrew White, known for his
    works on LLMs for chemistry. The self-described mission of the company is a “moonshot
    to build an AI scientist.”'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Future House](https://www.futurehouse.org/articles/announcing-future-house)
    — 一家新成立的位于硅谷的非营利公司，致力于AI4Science领域，由前Google CEO埃里克·施密特资助。科学负责人是安德鲁·怀特，他因在化学领域的LLM工作而闻名。该公司自我描述的使命是“建设一个AI科学家”，一个“登月计划”。'
- en: '*For additional articles about geometric and graph deep learning, see* [*Michael
    Galkin*](https://medium.com/@mgalkin)*’s and* [*Michael Bronstein*](https://medium.com/@michael-bronstein)*’s
    Medium posts and follow the two Michaels (*[*Galkin*](https://twitter.com/michael_galkin)
    *and* [*Bronstein*](https://twitter.com/mmbronstein)*) on Twitter.*'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于几何学和图深度学习的更多文章，请参见* [*Michael Galkin*](https://medium.com/@mgalkin)*和* [*Michael
    Bronstein*](https://medium.com/@michael-bronstein)*的Medium文章，并在Twitter上关注两位迈克尔（*[*Galkin*](https://twitter.com/michael_galkin)
    *和* [*Bronstein*](https://twitter.com/mmbronstein)）*。'
