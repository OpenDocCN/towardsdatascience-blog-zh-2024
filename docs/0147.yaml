- en: 'Graph & Geometric ML in 2024: Where We Are and Whatâ€™s Next (Part II â€” Applications)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024å¹´å›¾å½¢ä¸å‡ ä½•æœºå™¨å­¦ä¹ ï¼šæˆ‘ä»¬å¤„äºä½•ç§é˜¶æ®µï¼Œæœªæ¥å¦‚ä½•å‘å±•ï¼ˆç¬¬äºŒéƒ¨åˆ†â€”â€”åº”ç”¨ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16](https://towardsdatascience.com/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-ii-applications-1ed786f7bf63?source=collection_archive---------3-----------------------#2024-01-16)
- en: State-of-the-Art Digest
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€å‰æ²¿æ‘˜è¦
- en: Following the tradition from previous years, we interviewed a cohort of distinguished
    and prolific academic and industrial experts in an attempt to summarise the highlights
    of the past year and predict what is in store for 2024\. Past 2023 was so ripe
    with results that we had to break this post into two parts. This is Part II focusing
    on applications, see also [Part I](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1)
    for theory & new architectures.
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å»¶ç»­å¾€å¹´çš„ä¼ ç»Ÿï¼Œæˆ‘ä»¬é‡‡è®¿äº†ä¸€æ‰¹æ°å‡ºä¸”é«˜äº§çš„å­¦æœ¯å’Œå·¥ä¸šä¸“å®¶ï¼Œæ—¨åœ¨æ€»ç»“è¿‡å»ä¸€å¹´çš„äº®ç‚¹ï¼Œå¹¶é¢„æµ‹2024å¹´çš„å‘å±•è¶‹åŠ¿ã€‚2023å¹´æˆæœä¸°ç¡•ï¼Œä»¥è‡³äºæˆ‘ä»¬ä¸å¾—ä¸å°†è¿™ç¯‡æ–‡ç« åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚è¿™æ˜¯ç¬¬äºŒéƒ¨åˆ†ï¼Œé‡ç‚¹ä»‹ç»åº”ç”¨ï¼Œæ¬²äº†è§£ç†è®ºä¸æ–°æ¶æ„ï¼Œè¯·å‚è§[ç¬¬ä¸€éƒ¨åˆ†](/graph-geometric-ml-in-2024-where-we-are-and-whats-next-part-i-theory-architectures-3af5d38376e1)ã€‚
- en: '[](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page---byline--1ed786f7bf63--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    Â·42 min readÂ·Jan 16, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ed786f7bf63--------------------------------)
    Â·42åˆ†é’Ÿé˜…è¯»Â·2024å¹´1æœˆ16æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f888e845d4bdb9f34131d8b0544d71fc.png)'
- en: Image by Authors with some help from DALL-E 3.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ï¼Œå¹¶å¾—åˆ°DALL-E 3çš„å¸®åŠ©ã€‚
- en: '*The post is written and edited by* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *and* [*Michael Bronstein*](https://twitter.com/mmbronstein) *with significant
    contributions from* [*Dominique Beaini*](https://twitter.com/dom_beaini)*,* [*Nathan
    Benaich*](https://twitter.com/nathanbenaich)*,* [*Joey Bose*](https://twitter.com/bose_joey)*,*
    [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter)*,* [*Bruno Correia*](https://twitter.com/befcorreia)*,*
    [*Ahmed Elhag*](https://twitter.com/Ahmed_AI035)*,* [*Kexin Huang*](https://twitter.com/KexinHuang5)*,*
    [*Chaitanya Joshi*](https://twitter.com/chaitjo)*,* [*Leon Klein*](https://twitter.com/leonklein26)*,*
    [*N M Anoop Krishnan*](https://twitter.com/anoopnm007)*,* [*Chen Lin*](https://twitter.com/WillLin1028)*,*
    [*Andreas Loukas*](https://twitter.com/loukasa_tweet)*,* [*Santiago Miret*](https://www.linkedin.com/in/santiago-miret)*,*
    [*Luca Naef*](https://twitter.com/NaefLuca)*,* [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)*,*
    [*Emanuele Rossi*](https://twitter.com/emaros96)*,* [*Hannes StÃ¤rk*](https://twitter.com/HannesStaerk)*,*
    [*Alex Tong*](https://twitter.com/AlexanderTong7)*,* [*Anton Tsitsulin*](https://twitter.com/tsitsulin_)*,*
    [*Petar VeliÄkoviÄ‡*](https://twitter.com/PetarV_93)*,* [*Minkai Xu*](https://twitter.com/MinkaiX)*,
    and* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng)*.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”±* [*Michael Galkin*](https://twitter.com/michael_galkin) *å’Œ* [*Michael
    Bronstein*](https://twitter.com/mmbronstein) *ç¼–å†™å’Œç¼–è¾‘ï¼Œä¸”æœ‰æ¥è‡ª* [*Dominique Beaini*](https://twitter.com/dom_beaini)
    *ã€* [*Nathan Benaich*](https://twitter.com/nathanbenaich) *ã€* [*Joey Bose*](https://twitter.com/bose_joey)
    *ã€* [*Johannes Brandstetter*](https://twitter.com/jo_brandstetter) *ã€* [*Bruno
    Correia*](https://twitter.com/befcorreia) *ã€* [*Ahmed Elhag*](https://twitter.com/Ahmed_AI035)
    *ã€* [*Kexin Huang*](https://twitter.com/KexinHuang5) *ã€* [*Chaitanya Joshi*](https://twitter.com/chaitjo)
    *ã€* [*Leon Klein*](https://twitter.com/leonklein26) *ã€* [*N M Anoop Krishnan*](https://twitter.com/anoopnm007)
    *ã€* [*Chen Lin*](https://twitter.com/WillLin1028) *ã€* [*Andreas Loukas*](https://twitter.com/loukasa_tweet)
    *ã€* [*Santiago Miret*](https://www.linkedin.com/in/santiago-miret) *ã€* [*Luca
    Naef*](https://twitter.com/NaefLuca) *ã€* [*Liudmila Prokhorenkova*](https://twitter.com/LProkhorenkova)
    *ã€* [*Emanuele Rossi*](https://twitter.com/emaros96) *ã€* [*Hannes StÃ¤rk*](https://twitter.com/HannesStaerk)
    *ã€* [*Alex Tong*](https://twitter.com/AlexanderTong7) *ã€* [*Anton Tsitsulin*](https://twitter.com/tsitsulin_)
    *ã€* [*Petar VeliÄkoviÄ‡*](https://twitter.com/PetarV_93) *ã€* [*Minkai Xu*](https://twitter.com/MinkaiX)
    *å’Œ* [*Zhaocheng Zhu*](https://twitter.com/zhu_zhaocheng) *çš„é‡è¦è´¡çŒ®ã€‚*'
- en: '![](../Images/187024de90ba49fdfd0f708cdc506312.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/187024de90ba49fdfd0f708cdc506312.png)'
- en: 'Geometric ML methods and applications filled the covers of high-profile journals
    in 2023 (Figure sources: the papers by [Wang et al.](https://www.nature.com/articles/s42256-023-00609-5),
    [ViÃ±as et al.](https://www.nature.com/articles/s42256-023-00684-8), [Deng et al.](https://www.nature.com/articles/s42256-023-00716-3),
    [Weiss et al.](https://www.nature.com/articles/s43588-023-00532-0), [Lagemann
    et al.](https://www.nature.com/articles/s42256-023-00744-z), [Duan et al.](https://www.nature.com/articles/s43588-023-00563-7),
    and [Lam et al.](https://www.science.org/doi/10.1126/science.adi2336))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä½•æœºå™¨å­¦ä¹ æ–¹æ³•å’Œåº”ç”¨å¡«æ»¡äº†2023å¹´é«˜ç«¯æœŸåˆŠçš„å°é¢ï¼ˆå›¾æºï¼šæ¥è‡ª[Wangç­‰](https://www.nature.com/articles/s42256-023-00609-5)ã€[ViÃ±asç­‰](https://www.nature.com/articles/s42256-023-00684-8)ã€[Dengç­‰](https://www.nature.com/articles/s42256-023-00716-3)ã€[Weissç­‰](https://www.nature.com/articles/s43588-023-00532-0)ã€[Lagemannç­‰](https://www.nature.com/articles/s42256-023-00744-z)ã€[Duanç­‰](https://www.nature.com/articles/s43588-023-00563-7)
    å’Œ[Lamç­‰](https://www.science.org/doi/10.1126/science.adi2336)çš„è®ºæ–‡ï¼‰
- en: '[Structural Biology (Molecules & Proteins)](#2626)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç»“æ„ç”Ÿç‰©å­¦ï¼ˆåˆ†å­ä¸è›‹ç™½è´¨ï¼‰](#2626)'
- en: a. [A Structural Biologistâ€™s Perspective](#2f16)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [ç»“æ„ç”Ÿç‰©å­¦å®¶çš„è§†è§’](#2f16)
- en: b. [Industrial Perspective](#ade6)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [å·¥ä¸šè§†è§’](#ade6)
- en: c. [Systems Biology](#7a08)
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. [ç³»ç»Ÿç”Ÿç‰©å­¦](#7a08)
- en: '[Materials Science (Crystals)](#6211)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ææ–™ç§‘å­¦ï¼ˆæ™¶ä½“ï¼‰](#6211)'
- en: '[Molecular Dynamics & ML Potentials](#0924)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[åˆ†å­åŠ¨åŠ›å­¦ä¸æœºå™¨å­¦ä¹ åŠ¿èƒ½](#0924)'
- en: '[Geometric Generative Models (Manifolds)](#34b8)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•ç”Ÿæˆæ¨¡å‹ï¼ˆæµå½¢ï¼‰](#34b8)'
- en: '[BIG Graphs, Scalability: When GNNs are too expensive](#3f98)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å¤§å‹å›¾è°±ã€å¯æ‰©å±•æ€§ï¼šå½“GNNå¤ªæ˜‚è´µæ—¶](#3f98)'
- en: '[Algorithmic Reasoning & Alignment](#f6d7)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç®—æ³•æ¨ç†ä¸å¯¹é½](#f6d7)'
- en: '[Knowledge Graphs: Inductive Reasoning is Solved?](#5854)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[çŸ¥è¯†å›¾è°±ï¼šå½’çº³æ¨ç†è§£å†³äº†å—ï¼Ÿ](#5854)'
- en: '[Temporal Graph Learning](#add1)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ—¶åºå›¾å­¦ä¹ ](#add1)'
- en: '[LLMs + Graphs for Scientific Discovery](#ad3d)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[LLMs + å›¾è°±ç”¨äºç§‘å­¦å‘ç°](#ad3d)'
- en: '[Cool GNN Applications](#3d00)'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é…·ç‚«çš„GNNåº”ç”¨](#3d00)'
- en: '[Geometric Wall Street Bulletin ğŸ’¸](#986f)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•åå°”è¡—å…¬å‘Š ğŸ’¸](#986f)'
- en: 'The legend we will be using throughout the text:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å…¨æ–‡ä¸­å°†ä½¿ç”¨çš„ç¬¦å·ï¼š
- en: ğŸ”¥ hot topics
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ çƒ­ç‚¹è¯é¢˜
- en: ğŸ’¡ yearâ€™s highlight
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä»Šå¹´çš„äº®ç‚¹
- en: ğŸ‹ï¸ challenges
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ï¸ æŒ‘æˆ˜
- en: â¡ï¸ current/next developments
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å½“å‰/ä¸‹ä¸€æ­¥å‘å±•
- en: ğŸ”® predictions/speculations
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® é¢„æµ‹/æ¨æµ‹
- en: ğŸ’° financial transactions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’° è´¢åŠ¡äº¤æ˜“
- en: Structural Biology (Molecules & Proteins)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æ„ç”Ÿç‰©å­¦ï¼ˆåˆ†å­ä¸è›‹ç™½è´¨ï¼‰
- en: '*Dominique Beaini (Valence), Joey Bose (Mila & Dreamfold), Michael Bronstein
    (Oxford), Bruno Correia (EPFL), Michael Galkin (Intel), Kexin Huang (Stanford),
    Chaitanya Joshi (Cambridge), Andreas Loukas (Genentech), Luca Naef (VantAI), Hannes
    StÃ¤rk (MIT), Minkai Xu (Stanford)*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dominique Beainiï¼ˆValenceï¼‰ã€Joey Boseï¼ˆMila & Dreamfoldï¼‰ã€Michael Bronsteinï¼ˆOxfordï¼‰ã€Bruno
    Correiaï¼ˆEPFLï¼‰ã€Michael Galkinï¼ˆIntelï¼‰ã€Kexin Huangï¼ˆStanfordï¼‰ã€Chaitanya Joshiï¼ˆCambridgeï¼‰ã€Andreas
    Loukasï¼ˆGenentechï¼‰ã€Luca Naefï¼ˆVantAIï¼‰ã€Hannes StÃ¤rkï¼ˆMITï¼‰ã€Minkai Xuï¼ˆStanfordï¼‰*'
- en: Structural biology was definitely at the forefront of Geometric Deep Learning
    in 2023.
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç»“æ„ç”Ÿç‰©å­¦æ— ç–‘æ˜¯ 2023 å¹´å‡ ä½•æ·±åº¦å­¦ä¹ é¢†åŸŸçš„å‰æ²¿ã€‚
- en: Following the 2020 discovery of [halicin](https://pubmed.ncbi.nlm.nih.gov/32084340/)
    as a potential new antibiotic, in 2023, two new antibiotics were discovered with
    the help of GNNs! First, it is [abaucin](https://www.nature.com/articles/s41589-023-01349-8)
    (by McMaster and MIT), which targets a stubborn pathogen resistant to many drugs.
    Second, MIT and Harvard researchers [discovered a new structural class of antibiotics](https://www.nature.com/articles/s41586-023-06887-8)
    where the screening process was supported by [ChemProp](https://github.com/chemprop/chemprop),
    a suite of GNNs for molecular property prediction. We also observe a convergence
    of ML and experimental techniques (â€œlab-in the-loopâ€) in the recent work on [autonomous
    molecular discovery](https://www.science.org/doi/10.1126/science.adi1407) (a trend
    we will also see in the Materials Design in the following sections).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ 2020 å¹´ [halicin](https://pubmed.ncbi.nlm.nih.gov/32084340/) è¢«å‘ç°ä½œä¸ºä¸€ç§æ½œåœ¨çš„æ–°å‹æŠ—ç”Ÿç´ ä¹‹åï¼Œ2023
    å¹´é€šè¿‡ GNNsï¼ˆå›¾ç¥ç»ç½‘ç»œï¼‰çš„å¸®åŠ©ï¼Œå‘ç°äº†ä¸¤ç§æ–°çš„æŠ—ç”Ÿç´ ï¼é¦–å…ˆæ˜¯ [abaucin](https://www.nature.com/articles/s41589-023-01349-8)ï¼ˆç”±éº¦å…‹é©¬æ–¯ç‰¹å¤§å­¦å’Œéº»çœç†å·¥å­¦é™¢ç ”ç©¶äººå‘˜å‘ç°ï¼‰ï¼Œå®ƒèƒ½å¤Ÿé¶å‘ä¸€ç§å¯¹å¤šç§è¯ç‰©æœ‰æŠ—è¯æ€§çš„é¡½å›ºç—…åŸä½“ã€‚å…¶æ¬¡ï¼Œéº»çœç†å·¥å­¦é™¢å’Œå“ˆä½›å¤§å­¦çš„ç ”ç©¶äººå‘˜
    [å‘ç°äº†ä¸€ç§æ–°çš„æŠ—ç”Ÿç´ ç»“æ„ç±»åˆ«](https://www.nature.com/articles/s41586-023-06887-8)ï¼Œå…¶ä¸­ç­›é€‰è¿‡ç¨‹å¾—åˆ°äº†
    [ChemProp](https://github.com/chemprop/chemprop) çš„æ”¯æŒï¼Œè¿™æ˜¯ä¸€å¥—ç”¨äºåˆ†å­æ€§è´¨é¢„æµ‹çš„ GNN å·¥å…·ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°åœ¨æœ€è¿‘çš„[è‡ªä¸»åˆ†å­å‘ç°](https://www.science.org/doi/10.1126/science.adi1407)å·¥ä½œä¸­ï¼Œæœºå™¨å­¦ä¹ å’Œå®éªŒæŠ€æœ¯çš„èåˆï¼ˆâ€œå®éªŒå®¤ç¯èŠ‚â€ï¼‰æ­£åœ¨è¶‹äºä¸€è‡´ï¼ˆè¿™æ˜¯æˆ‘ä»¬åœ¨åç»­ææ–™è®¾è®¡éƒ¨åˆ†ä¹Ÿå°†çœ‹åˆ°çš„è¶‹åŠ¿ï¼‰ã€‚
- en: '**Flow Matching** has been one of the biggest generative ML trends of 2023,
    allowing for faster sampling and deterministic sampling trajectories compared
    to diffusion models. The most prominent examples of Flow Matching models we have
    seen in the biological applications are **FoldFlow** ([Bose, Akhound-Sadegh, et
    al](https://arxiv.org/abs/2310.02391).) for protein backbone generation, **FlowSite**
    ([StÃ¤rk et al](https://arxiv.org/abs/2310.05764).) for protein binding site design,
    and **EquiFM** ([Song, Gong, et al](https://openreview.net/forum?id=hHUZ5V9XFu).)
    for molecule generation.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµåŒ¹é…**å·²ç»æˆä¸º 2023 å¹´æœ€å¤§çš„ç”Ÿæˆå‹æœºå™¨å­¦ä¹ è¶‹åŠ¿ä¹‹ä¸€ï¼Œç›¸æ¯”æ‰©æ•£æ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿå®ç°æ›´å¿«çš„é‡‡æ ·å’Œç¡®å®šæ€§çš„é‡‡æ ·è½¨è¿¹ã€‚åœ¨ç”Ÿç‰©å­¦åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„æµåŒ¹é…æ¨¡å‹æœ€çªå‡ºçš„ä¾‹å­æœ‰ï¼š**FoldFlow**ï¼ˆ[Boseã€Akhound-Sadegh
    ç­‰äºº](https://arxiv.org/abs/2310.02391)ï¼‰ç”¨äºè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆï¼Œ**FlowSite**ï¼ˆ[StÃ¤rk ç­‰äºº](https://arxiv.org/abs/2310.05764)ï¼‰ç”¨äºè›‹ç™½è´¨ç»“åˆä½ç‚¹è®¾è®¡ï¼Œä»¥åŠ
    **EquiFM**ï¼ˆ[Songã€Gong ç­‰äºº](https://openreview.net/forum?id=hHUZ5V9XFu)ï¼‰ç”¨äºåˆ†å­ç”Ÿæˆã€‚'
- en: '![](../Images/47b927a05ef39ae2fa830c5ff139d6f7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47b927a05ef39ae2fa830c5ff139d6f7.png)'
- en: '*Conditional probability paths learned by different versions of FoldFlow, visualizing
    the rotation trajectory of a single residue by the action of SO(3) on its homogeneous
    space* ğ•ŠÂ²*. Figure source:* [*Bose, Akhound-Sadegh, et al*](https://arxiv.org/abs/2310.02391)*.*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*FoldFlow ä¸åŒç‰ˆæœ¬å­¦ä¹ åˆ°çš„æ¡ä»¶æ¦‚ç‡è·¯å¾„ï¼Œå±•ç¤ºäº† SO(3) åœ¨å…¶é½æ¬¡ç©ºé—´ä¸Šä½œç”¨ä¸‹å•ä¸€æ®‹åŸºçš„æ—‹è½¬è½¨è¿¹* ğ•ŠÂ²*ã€‚å›¾æºï¼š* [*Boseã€Akhound-Sadegh
    ç­‰äºº*](https://arxiv.org/abs/2310.02391)*ã€‚*'
- en: Efficient Flow Matching on complex geometries with necessary equivariances became
    possible thanks to a handful of theory papers including Riemannian Flow Matching
    ([Chen and Lipman](https://arxiv.org/abs/2302.03660)), Minibatch Optimal Transport
    ([Tong et al](https://arxiv.org/abs/2302.00482)), and Simulation-Free SchrÃ¶dinger
    bridges ([Tong, Malkin, Fatras, et al](https://arxiv.org/abs/2307.03672)). A great
    resource to learn Flow Matching with code examples and notebooks is the [TorchCFM](https://github.com/atong01/conditional-flow-matching)
    repo on GitHub as well as talks by [Yaron Lipman](https://www.youtube.com/watch?v=5ZSwYogAxYg),
    [Joey Bose](https://www.youtube.com/watch?v=EPxDI0ytfQU), [Hannes StÃ¤rk](https://www.youtube.com/watch?v=Xl7YNR1-CN8),
    and [Alex Tong](https://www.youtube.com/watch?v=UhDtH7Ia9Ag).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ•ˆæµåŒ¹é…åœ¨å¤æ‚å‡ ä½•ä½“ä¸Šçš„åº”ç”¨ï¼Œç»“åˆå¿…è¦çš„ç­‰å˜æ€§ï¼Œå¾—ä»¥å®ç°ï¼Œè¿™å¾—ç›Šäºå‡ ç¯‡ç†è®ºè®ºæ–‡çš„è´¡çŒ®ï¼ŒåŒ…æ‹¬ã€Šé»æ›¼æµåŒ¹é…ã€‹ï¼ˆ[Chen å’Œ Lipman](https://arxiv.org/abs/2302.03660)ï¼‰ã€ã€Šå°æ‰¹é‡æœ€ä¼˜ä¼ è¾“ã€‹ï¼ˆ[Tong
    ç­‰äºº](https://arxiv.org/abs/2302.00482)ï¼‰ã€ä»¥åŠã€Šæ— ä»¿çœŸè–›å®šè°”æ¡¥ã€‹ï¼ˆ[Tongã€Malkinã€Fatras ç­‰äºº](https://arxiv.org/abs/2307.03672)ï¼‰ã€‚ä¸€ä¸ªå¾ˆå¥½çš„å­¦ä¹ æµåŒ¹é…çš„èµ„æºæ˜¯
    GitHub ä¸Šçš„ [TorchCFM](https://github.com/atong01/conditional-flow-matching) ä»“åº“ï¼Œå…¶ä¸­åŒ…å«ä»£ç ç¤ºä¾‹å’Œç¬”è®°æœ¬ï¼Œä»¥åŠ
    [Yaron Lipman](https://www.youtube.com/watch?v=5ZSwYogAxYg)ã€[Joey Bose](https://www.youtube.com/watch?v=EPxDI0ytfQU)ã€[Hannes
    StÃ¤rk](https://www.youtube.com/watch?v=Xl7YNR1-CN8) å’Œ [Alex Tong](https://www.youtube.com/watch?v=UhDtH7Ia9Ag)
    çš„è®²åº§ã€‚
- en: '**Diffusion models** nevertheless continue to be the main workhorse of generative
    modeling in structural biology. In 2023, we saw several landmark works: **FrameDiff**
    ([Yim, Trippe, De Bortoli, Mathieu, et al](https://arxiv.org/abs/2302.02277))
    for protein backbone generation, **EvoDiff** ([Alamdari et al](https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1))
    for generating protein sequences with discrete diffusion, **AbDiffuser** ([Martinkus
    et al](https://arxiv.org/abs/2308.05027)) for full-atom antibody design with frame
    averaging and discrete diffusion (and with successful wet lab experiments), **DiffMaSIF**
    ([Sverrison, Akdel, et al](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf))
    and **DiffDock-PP** ([Ketata, Laue, Mammadov, StÃ¤rk, et al](https://arxiv.org/abs/2304.03889))
    for protein-protein docking, **DiffPack** ([Zhang, Zhang, et al](https://arxiv.org/abs/2306.01794))
    for side-chain packing, and the Baker Lab published the **RFDiffusion** **all-atom**
    version ([Krishna, Wang, Ahern, et al](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)).
    Among latent diffusion model (like Stable Diffusion in image generation applications),
    **GeoLDM** ([Xu et al](https://arxiv.org/abs/2305.01140)) was the first for 3D
    molecule conformations, followed by [OmniProt](https://openreview.net/forum?id=DP4NkPZOpD)
    for protein sequence-structure generation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰©æ•£æ¨¡å‹**ä»ç„¶æ˜¯ç»“æ„ç”Ÿç‰©å­¦ä¸­ç”Ÿæˆå»ºæ¨¡çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚2023å¹´ï¼Œæˆ‘ä»¬è§è¯äº†å‡ é¡¹å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„å·¥ä½œï¼š**FrameDiff**ï¼ˆ[Yim, Trippe,
    De Bortoli, Mathieuç­‰](https://arxiv.org/abs/2302.02277)ï¼‰ç”¨äºè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆï¼Œ**EvoDiff**ï¼ˆ[Alamdariç­‰](https://www.biorxiv.org/content/10.1101/2023.09.11.556673v1)ï¼‰ç”¨äºç”Ÿæˆå…·æœ‰ç¦»æ•£æ‰©æ•£çš„è›‹ç™½è´¨åºåˆ—ï¼Œ**AbDiffuser**ï¼ˆ[Martinkusç­‰](https://arxiv.org/abs/2308.05027)ï¼‰ç”¨äºå…¨åŸå­æŠ—ä½“è®¾è®¡ï¼Œç»“åˆæ¡†æ¶å¹³å‡å’Œç¦»æ•£æ‰©æ•£ï¼ˆå¹¶é€šè¿‡æˆåŠŸçš„å®éªŒéªŒè¯ï¼‰ï¼Œ**DiffMaSIF**ï¼ˆ[Sverrison,
    Akdelç­‰](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf)ï¼‰å’Œ**DiffDock-PP**ï¼ˆ[Ketata,
    Laue, Mammadov, StÃ¤rkç­‰](https://arxiv.org/abs/2304.03889)ï¼‰ç”¨äºè›‹ç™½è´¨-è›‹ç™½è´¨å¯¹æ¥ï¼Œ**DiffPack**ï¼ˆ[Zhang,
    Zhangç­‰](https://arxiv.org/abs/2306.01794)ï¼‰ç”¨äºä¾§é“¾åŒ…è£…ï¼Œè´å…‹å®éªŒå®¤å‘å¸ƒäº†**RFDiffusion** **å…¨åŸå­**ç‰ˆæœ¬ï¼ˆ[Krishna,
    Wang, Ahernç­‰](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)ï¼‰ã€‚åœ¨æ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚å›¾åƒç”Ÿæˆåº”ç”¨ä¸­çš„Stable
    Diffusionï¼‰ä¸­ï¼Œ**GeoLDM**ï¼ˆ[Xuç­‰](https://arxiv.org/abs/2305.01140)ï¼‰æ˜¯ç¬¬ä¸€ä¸ªç”¨äº3Dåˆ†å­æ„è±¡çš„æ¨¡å‹ï¼Œéšåæ˜¯[OmniProt](https://openreview.net/forum?id=DP4NkPZOpD)ç”¨äºè›‹ç™½è´¨åºåˆ—-ç»“æ„ç”Ÿæˆã€‚'
- en: '![](../Images/67d90c64a0bad7a57857749bd5d60f3d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d90c64a0bad7a57857749bd5d60f3d.png)'
- en: 'FrameDiff: parameterization of the backbone frame with rotation, translation,
    and torsion angle for the oxygen atom. Figure Source: [Yim, Trippe, De Bortoli,
    Mathieu, et al](https://arxiv.org/abs/2302.02277)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: FrameDiffï¼šé€šè¿‡æ—‹è½¬ã€å¹³ç§»å’Œæ‰­è½¬è§’åº¦å¯¹æ°§åŸå­çš„éª¨æ¶æ¡†æ¶è¿›è¡Œå‚æ•°åŒ–ã€‚å›¾æºï¼š[Yim, Trippe, De Bortoli, Mathieuç­‰](https://arxiv.org/abs/2302.02277)
- en: 'Finally, Google DeepMind and Isomorphic Labs [announced](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    **AlphaFold 2.3** â€” the latest iteration is significantly improving upon the baselines
    in 3 tasks: docking benchmarks (almost 2Ã— better than DiffDock on the new [PoseBusters](https://arxiv.org/abs/2308.05777)
    benchmark), protein-nucleic acid interactions, and antibody-antigen prediction.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè°·æ­ŒDeepMindå’ŒIsomorphic Labs [å®£å¸ƒ](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    **AlphaFold 2.3**â€”â€”æœ€æ–°ç‰ˆæœ¬åœ¨ä¸‰ä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—æ”¹è¿›äº†åŸºå‡†ï¼šå¯¹æ¥åŸºå‡†ï¼ˆæ¯”DiffDockåœ¨æ–°çš„[PoseBusters](https://arxiv.org/abs/2308.05777)åŸºå‡†ä¸Šå¥½è¿‘2å€ï¼‰ï¼Œè›‹ç™½è´¨-æ ¸é…¸ç›¸äº’ä½œç”¨ï¼Œä»¥åŠæŠ—ä½“-æŠ—åŸé¢„æµ‹ã€‚
- en: '***Chaitanya Joshi (Cambridge)***'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***Chaitanya Joshi (å‰‘æ¡¥)***'
- en: 'ğŸ’¡There have been two emerging trends for biomolecular modeling and design that
    I am very excited about in 2023:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡2023å¹´ï¼Œæˆ‘éå¸¸å…´å¥‹åœ°çœ‹åˆ°ç”Ÿç‰©åˆ†å­å»ºæ¨¡å’Œè®¾è®¡é¢†åŸŸå‡ºç°äº†ä¸¤ä¸ªæ–°å…´è¶‹åŠ¿ï¼š
- en: 1ï¸âƒ£ Going from protein structure prediction to conformational ensemble generation.
    There were several interesting approaches to the problem, including [AlphaFold
    with MSA clustering](https://www.nature.com/articles/s41586-023-06832-9), [idpGAN](https://www.nature.com/articles/s41467-023-36443-x),
    [Distributional Graphormer](https://arxiv.org/abs/2306.05445) (a diffusion model),
    and [AlphaFold Meets Flow Matching for Generating Protein Ensembles](https://www.mlsb.io/papers_2023/AlphaFold_Meets_Flow_Matching_for_Generating_Protein_Ensembles.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä»è›‹ç™½è´¨ç»“æ„é¢„æµ‹åˆ°æ„è±¡é›†ç”Ÿæˆçš„è¿‡æ¸¡ã€‚é’ˆå¯¹è¿™ä¸ªé—®é¢˜æœ‰å‡ ç§æœ‰è¶£çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬ [AlphaFoldä¸MSAèšç±»](https://www.nature.com/articles/s41586-023-06832-9)ï¼Œ[idpGAN](https://www.nature.com/articles/s41467-023-36443-x)ï¼Œ[Distributional
    Graphormer](https://arxiv.org/abs/2306.05445)ï¼ˆä¸€ç§æ‰©æ•£æ¨¡å‹ï¼‰ï¼Œä»¥åŠ [AlphaFoldä¸æµåŒ¹é…ç»“åˆç”Ÿæˆè›‹ç™½è´¨é›†](https://www.mlsb.io/papers_2023/AlphaFold_Meets_Flow_Matching_for_Generating_Protein_Ensembles.pdf)ã€‚
- en: '2ï¸âƒ£ Modelling of biomolecular complexes and design of biomolecular interactions
    among proteins + X: [RFdiffusion all-atom](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)
    and [Ligand MPNN](https://www.biorxiv.org/content/10.1101/2023.12.22.573103v1.full),
    both from the Baker Lab, are representative examples of the trend towards designing
    interactions. The new in-development [AlphaFold report](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    claims that a unified structure prediction model can outperform or match specialised
    models across solo protein and protein complex structure prediction as well as
    protein-ligand and protein-nucleic acid co-folding.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ç”Ÿç‰©åˆ†å­å¤åˆç‰©å»ºæ¨¡ä¸è›‹ç™½è´¨ + X çš„ç”Ÿç‰©åˆ†å­ç›¸äº’ä½œç”¨è®¾è®¡ï¼š[RFdiffusionå…¨åŸå­æ¨¡å‹](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1.full)
    å’Œ [Ligand MPNN](https://www.biorxiv.org/content/10.1101/2023.12.22.573103v1.full)ï¼Œè¿™ä¸¤è€…å‡æ¥è‡ªBakerå®éªŒå®¤ï¼Œæ˜¯æœç€è®¾è®¡ç›¸äº’ä½œç”¨è¶‹åŠ¿çš„ä»£è¡¨æ€§ç¤ºä¾‹ã€‚æ­£åœ¨å¼€å‘ä¸­çš„æ–°[AlphaFoldæŠ¥å‘Š](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)ç§°ï¼Œç»Ÿä¸€çš„ç»“æ„é¢„æµ‹æ¨¡å‹å¯ä»¥åœ¨å•ä¸€è›‹ç™½è´¨å’Œè›‹ç™½è´¨å¤åˆç‰©ç»“æ„é¢„æµ‹ä»¥åŠè›‹ç™½è´¨-é…ä½“å’Œè›‹ç™½è´¨-æ ¸é…¸ååŒæŠ˜å æ–¹é¢ï¼Œè¶…è¶Šæˆ–åŒ¹é…ä¸“ä¸šåŒ–æ¨¡å‹çš„è¡¨ç°ã€‚
- en: â€œHowever, for all the exciting methodology development in biomolecular modelling
    and design, perhaps the biggest lesson for the ML community this year should be
    to focus more on meaningful **in-silico evaluation** and, if possible, **experimental
    validation**.â€ â€” **Chaitanya Joshi** (Cambridge)
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œç„¶è€Œï¼Œå¯¹äºç”Ÿç‰©åˆ†å­å»ºæ¨¡å’Œè®¾è®¡ä¸­æ‰€æœ‰ä»¤äººå…´å¥‹çš„æ–¹æ³•å­¦å‘å±•è€Œè¨€ï¼Œä¹Ÿè®¸ä»Šå¹´æœºå™¨å­¦ä¹ ç¤¾åŒºæœ€å¤§çš„ä¸€è¯¾åº”è¯¥æ˜¯ï¼Œæ›´åŠ å…³æ³¨æœ‰æ„ä¹‰çš„**è®¡ç®—æœºæ¨¡æ‹Ÿè¯„ä¼°**ï¼Œå¦‚æœå¯èƒ½çš„è¯ï¼Œè¿›è¡Œ**å®éªŒéªŒè¯**ã€‚â€
    â€” **Chaitanya Joshi**ï¼ˆå‰‘æ¡¥ï¼‰
- en: 1ï¸âƒ£ In early 2023, Guolin Keâ€™s team at DP Technology released two excellent
    re-evaluation papers highlighting how we may have been largely overestimating
    the performance of prominent geometric deep learning-based methods for molecular
    [conformation generation](https://arxiv.org/abs/2302.07061) and [docking](https://arxiv.org/abs/2302.07134)
    w.r.t. traditional baselines.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ åœ¨2023å¹´åˆï¼ŒGuolin Keå›¢é˜Ÿåœ¨DP Technologyå‘å¸ƒäº†ä¸¤ç¯‡å‡ºè‰²çš„å†è¯„ä¼°è®ºæ–‡ï¼Œçªæ˜¾äº†æˆ‘ä»¬å¯èƒ½åœ¨åˆ†å­[æ„è±¡ç”Ÿæˆ](https://arxiv.org/abs/2302.07061)å’Œ[å¯¹æ¥](https://arxiv.org/abs/2302.07134)æ–¹é¢ï¼Œç›¸è¾ƒäºä¼ ç»ŸåŸºå‡†æ–¹æ³•ï¼Œè¿‡é«˜ä¼°è®¡äº†ä»¥å‡ ä½•æ·±åº¦å­¦ä¹ ä¸ºåŸºç¡€çš„ä¸»æµæ–¹æ³•çš„æ€§èƒ½ã€‚
- en: 2ï¸âƒ£ [PoseCheck](https://arxiv.org/abs/2308.07413) and [PoseBusters](https://arxiv.org/abs/2308.05777)
    shed further light on the failure modes of current molecular generation and docking
    methods. Critically, generated molecules and their 3D poses are often â€˜nonphysicalâ€™
    and contain steric clashes, hydrogen placement issues, and high strain energies.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ [PoseCheck](https://arxiv.org/abs/2308.07413) å’Œ [PoseBusters](https://arxiv.org/abs/2308.05777)
    è¿›ä¸€æ­¥æ­ç¤ºäº†å½“å‰åˆ†å­ç”Ÿæˆå’Œå¯¹æ¥æ–¹æ³•çš„å¤±è´¥æ¨¡å¼ã€‚å…³é”®æ˜¯ï¼Œç”Ÿæˆçš„åˆ†å­åŠå…¶3Då§¿æ€å¾€å¾€æ˜¯â€œéç‰©ç†çš„â€ï¼Œå¹¶ä¸”åŒ…å«ç«‹ä½“å†²çªã€æ°¢åŸå­ä½ç½®é—®é¢˜å’Œé«˜åº”å˜èƒ½ã€‚
- en: 3ï¸âƒ£ Very few papers attempt any experimental validation of new ML ideas. Perhaps
    collaborating with a wet lab is challenging for those focussed on new methodology
    development, but I hope that us ML-ers, as a community, will at least be a lot
    more cautious about the in-silico evaluation metrics we are constantly pushing
    as we create new models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å¾ˆå°‘æœ‰è®ºæ–‡å°è¯•å¯¹æ–°çš„æœºå™¨å­¦ä¹ æ€æƒ³è¿›è¡Œå®éªŒéªŒè¯ã€‚ä¹Ÿè®¸å¯¹äºé‚£äº›ä¸“æ³¨äºæ–°æ–¹æ³•å¼€å‘çš„äººæ¥è¯´ï¼Œä¸æ¹¿å®éªŒå®¤çš„åˆä½œæ¯”è¾ƒå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œä½†æˆ‘å¸Œæœ›ä½œä¸ºä¸€ä¸ªæœºå™¨å­¦ä¹ ç¤¾åŒºï¼Œæˆ‘ä»¬è‡³å°‘èƒ½åœ¨ä¸æ–­æ¨åŠ¨æ–°æ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæ›´åŠ è°¨æ…åœ°å¯¹å¾…æˆ‘ä»¬å¸¸ç”¨çš„è®¡ç®—æœºæ¨¡æ‹Ÿè¯„ä¼°æŒ‡æ ‡ã€‚
- en: '***Hannes StÃ¤rk (MIT)***'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '***Hannes StÃ¤rk (MIT)***'
- en: ğŸ’¡I am reading quite some hype here about Flow Matching, stochastic interpolants,
    and Rectified Flows (I will call them â€œBridge Matching,â€ or â€œBMâ€). I do not think
    there is much value in just replacing diffusion models with BM in all the existing
    applications. For pure generative modeling, the main BM advantage is simplicity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡æˆ‘çœ‹åˆ°å…³äºæµåŒ¹é…ã€éšæœºæ’å€¼å’Œæ•´æµæµï¼ˆæˆ‘å°†å…¶ç§°ä¸ºâ€œæ¡¥æ¥åŒ¹é…â€ï¼Œæˆ–â€œBMâ€ï¼‰çš„è®¨è®ºé¢‡ä¸ºç«çƒ­ã€‚æˆ‘ä¸è®¤ä¸ºä»…ä»…åœ¨æ‰€æœ‰ç°æœ‰åº”ç”¨ä¸­ç”¨BMæ›¿æ¢æ‰©æ•£æ¨¡å‹æœ‰ä»€ä¹ˆå¤ªå¤§ä»·å€¼ã€‚å¯¹äºçº¯ç”Ÿæˆå»ºæ¨¡ï¼ŒBMçš„ä¸»è¦ä¼˜åŠ¿æ˜¯ç®€æ´æ€§ã€‚
- en: I think we should instead be excited about BM for the new capabilities it unlocks.
    For example, training bridges between arbitrary distributions in a simulation-free
    manner (what are the best applications for this? I basically only saw [retrosynthesis](https://arxiv.org/abs/2308.16212)
    so far.) or solving OT problems as in [DSBM](https://arxiv.org/abs/2303.16852)
    that does so for fluid flow downscaling. Maybe a lot of tools emerged in 2023
    (also let us mention [BM with multiple marginals](https://arxiv.org/abs/2310.03695)),
    and in 2024, the community will make good use of them?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæˆ‘ä»¬åº”è¯¥æ›´åŠ å…´å¥‹çš„æ˜¯BMæ‰€è§£é”çš„æ–°èƒ½åŠ›ã€‚ä¾‹å¦‚ï¼Œæ— éœ€ä»¿çœŸå³å¯åœ¨ä»»æ„åˆ†å¸ƒä¹‹é—´è®­ç»ƒæ¡¥æ¥ï¼ˆè¿™ä¸ªæ–¹æ³•æœ€é€‚åˆå“ªäº›åº”ç”¨å‘¢ï¼Ÿåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘åªçœ‹åˆ°äº†[é€†åˆæˆ](https://arxiv.org/abs/2308.16212)ï¼‰ã€‚æˆ–è€…è§£å†³åƒ[DSBM](https://arxiv.org/abs/2303.16852)è¿™æ ·é’ˆå¯¹æµä½“æµåŠ¨ä¸‹é‡‡æ ·çš„OTé—®é¢˜ã€‚ä¹Ÿè®¸2023å¹´å‡ºç°äº†è®¸å¤šå·¥å…·ï¼ˆæˆ‘ä»¬è¿˜å¯ä»¥æåˆ°[å…·æœ‰å¤šä¸ªè¾¹é™…çš„BM](https://arxiv.org/abs/2310.03695)ï¼‰ï¼Œè€Œåˆ°2024å¹´ï¼Œç¤¾åŒºå°†æ›´å¥½åœ°åˆ©ç”¨å®ƒä»¬ï¼Ÿ
- en: '***Joey Bose (Mila & Dreamfold)***'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¹”ä¼ŠÂ·åšæ–¯ (Mila & Dreamfold)***'
- en: ğŸ’¡ This year we have really seen the rise of geometric generative models from
    theory to practice. A few standouts for me include [Riemannian Flow Matching](https://arxiv.org/abs/2302.03660)
    â€” in general any paper by Ricky Chen and Yaron Lipman on these topics is a must-read
    â€” and FrameDiff from [Yim et. al](https://arxiv.org/abs/2302.02277) which introduced
    a lot of the important machinery for protein backbone generation. Of course, standing
    on the shoulders of both RFM and FrameDIff, we built [FoldFlow](https://arxiv.org/abs/2310.02391),
    a cooler flow-matching approach to protein generative models.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä»Šå¹´æˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†å‡ ä½•ç”Ÿæˆæ¨¡å‹ä»ç†è®ºåˆ°å®è·µçš„å´›èµ·ã€‚å‡ é¡¹çªå‡ºçš„å·¥ä½œåŒ…æ‹¬[é»æ›¼æµåŒ¹é…](https://arxiv.org/abs/2302.03660)â€”â€”ä¸€èˆ¬æ¥è¯´ï¼Œä»»ä½•ç”±Ricky
    Chenå’ŒYaron Lipmanæ’°å†™çš„å…³äºè¿™äº›ä¸»é¢˜çš„è®ºæ–‡éƒ½å€¼å¾—ä¸€è¯»â€”â€”ä»¥åŠ[Yimç­‰äºº](https://arxiv.org/abs/2302.02277)çš„FrameDiffï¼Œå®ƒå¼•å…¥äº†å¾ˆå¤šè›‹ç™½è´¨éª¨æ¶ç”Ÿæˆçš„é‡è¦æœºåˆ¶ã€‚å½“ç„¶ï¼Œåœ¨RFMå’ŒFrameDiffçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬æ„å»ºäº†[FoldFlow](https://arxiv.org/abs/2310.02391)ï¼Œè¿™æ˜¯ä¸€ç§æ›´é…·çš„æµåŒ¹é…æ–¹æ³•ï¼Œç”¨äºè›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹ã€‚
- en: â€œLooking ahead, I foresee a lot **more flow matching**-based approaches coming
    into use. They are better for proteins and longer sequences and can start from
    any source distribution.â€ â€” Joey Bose (Mila & Dreamfold)
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå±•æœ›æœªæ¥ï¼Œæˆ‘é¢„è§åˆ°æ›´å¤šåŸºäº**æµåŒ¹é…**çš„æ–¹æ³•å°†ä¼šæŠ•å…¥ä½¿ç”¨ã€‚å®ƒä»¬å¯¹äºè›‹ç™½è´¨å’Œè¾ƒé•¿çš„åºåˆ—æ›´æœ‰æ•ˆï¼Œå¹¶ä¸”å¯ä»¥ä»ä»»ä½•æºåˆ†å¸ƒå¼€å§‹ã€‚â€ â€” ä¹”ä¼ŠÂ·åšæ–¯ (Mila
    & Dreamfold)
- en: ğŸ”® Moreover, I suspect we will soon see **multi-modal generative models** in
    this space, such as discrete + continuous models and also conditional models in
    the same vein as text-conditioned diffusion models for images. Perhaps, we might
    even see **latent generative models** here given that they scale so well!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® æ­¤å¤–ï¼Œæˆ‘æ€€ç–‘æˆ‘ä»¬å¾ˆå¿«å°†åœ¨è¿™ä¸ªé¢†åŸŸçœ‹åˆ°**å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹**ï¼Œä¾‹å¦‚ç¦»æ•£+è¿ç»­æ¨¡å‹ï¼Œä»¥åŠç±»ä¼¼äºå›¾åƒçš„æ–‡æœ¬æ¡ä»¶æ‰©æ•£æ¨¡å‹çš„æ¡ä»¶æ¨¡å‹ã€‚ä¹Ÿè®¸ï¼Œè€ƒè™‘åˆ°å®ƒä»¬çš„æ‰©å±•æ€§ï¼Œæˆ‘ä»¬ç”šè‡³å¯èƒ½ä¼šåœ¨è¿™é‡Œçœ‹åˆ°**æ½œåœ¨ç”Ÿæˆæ¨¡å‹**ï¼
- en: '***Minkai Xu (Stanford)***'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ•å‡¯Â·å¾ (æ–¯å¦ç¦å¤§å­¦)***'
- en: â€œThis year, the community has further pushed forward the geometric generative
    models for 3D molecular generation in many perspectives.â€ â€” Minkai Xu (Stanford)
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œä»Šå¹´ï¼Œç¤¾åŒºåœ¨å¤šä¸ªæ–¹é¢è¿›ä¸€æ­¥æ¨åŠ¨äº†å‡ ä½•ç”Ÿæˆæ¨¡å‹åœ¨3Dåˆ†å­ç”Ÿæˆä¸­çš„åº”ç”¨ã€‚â€ â€” æ•å‡¯Â·å¾ (æ–¯å¦ç¦å¤§å­¦)
- en: '**Flow matching**: Ricky and Yaron proposed the Flow Matching method as an
    alternative to the widely used diffusion models, and EquiFM ([Song et al](https://openreview.net/forum?id=hHUZ5V9XFu)
    and [Klein et al](https://arxiv.org/abs/2306.15030)) realize the variant for 3D
    molecule generation by parameterizing the flow dynamics with equivariant GNNs.
    In the meantime, [FrameFlow](https://arxiv.org/pdf/2310.05297.pdf) and [FoldFlow](https://arxiv.org/abs/2310.02391)
    construct FM models for protein generation.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**æµåŒ¹é…**ï¼šRickyå’ŒYaronæå‡ºäº†æµåŒ¹é…æ–¹æ³•ï¼Œä½œä¸ºå¹¿æ³›ä½¿ç”¨çš„æ‰©æ•£æ¨¡å‹çš„æ›¿ä»£æ–¹æ¡ˆï¼ŒEquiFMï¼ˆ[Songç­‰äºº](https://openreview.net/forum?id=hHUZ5V9XFu)
    å’Œ [Kleinç­‰äºº](https://arxiv.org/abs/2306.15030)ï¼‰é€šè¿‡ä½¿ç”¨ç­‰å˜GNNæ¥å‚æ•°åŒ–æµåŠ¨åŠ¨åŠ›å­¦ï¼Œä»è€Œå®ç°äº†3Dåˆ†å­ç”Ÿæˆçš„å˜ç§ã€‚ä¸æ­¤åŒæ—¶ï¼Œ[FrameFlow](https://arxiv.org/pdf/2310.05297.pdf)å’Œ[FoldFlow](https://arxiv.org/abs/2310.02391)æ„å»ºäº†ç”¨äºè›‹ç™½è´¨ç”Ÿæˆçš„FMæ¨¡å‹ã€‚'
- en: ğŸ”®Moving forward similar to vision and text domain, people begin to explore generation
    in the lower-dimensional latent space instead of the complex original data space
    (**latent generative models**). GeoLDM ([Xu et al](https://arxiv.org/abs/2305.01140))
    proposed the first latent diffusion model (like Stable Diffusion in CV) for 3D
    molecule generation, while [Fu et al](https://arxiv.org/abs/2305.04120) enjoys
    similar modeling formulation for large protein generation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® å±•æœ›æœªæ¥ï¼Œç±»ä¼¼äºè§†è§‰å’Œæ–‡æœ¬é¢†åŸŸï¼Œäººä»¬å¼€å§‹æ¢ç´¢åœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆï¼Œè€Œä¸æ˜¯åœ¨å¤æ‚çš„åŸå§‹æ•°æ®ç©ºé—´ä¸­è¿›è¡Œç”Ÿæˆï¼ˆ**æ½œåœ¨ç”Ÿæˆæ¨¡å‹**ï¼‰ã€‚GeoLDMï¼ˆ[Xuç­‰äºº](https://arxiv.org/abs/2305.01140)ï¼‰æå‡ºäº†ç¬¬ä¸€ä¸ªæ½œåœ¨æ‰©æ•£æ¨¡å‹ï¼ˆç±»ä¼¼äºCVä¸­çš„ç¨³å®šæ‰©æ•£ï¼‰ï¼Œç”¨äº3Dåˆ†å­ç”Ÿæˆï¼Œè€Œ[Fuç­‰äºº](https://arxiv.org/abs/2305.04120)åˆ™åœ¨å¤§å‹è›‹ç™½è´¨ç”Ÿæˆä¸­ä½¿ç”¨äº†ç±»ä¼¼çš„å»ºæ¨¡æ–¹æ³•ã€‚
- en: A Structural Biologistâ€™s Perspective
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æ„ç”Ÿç‰©å­¦å®¶çš„è§†è§’
- en: '*Bruno Correia (EPFL)*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¸ƒé²è¯ºÂ·ç§‘é›·äºš (EPFL)*'
- en: â€œCurrent generative models still create â€œgarbageâ€ outputs that violate many
    of the physical and chemical properties that molecules are known to have. The
    advantage of current generative models is, of course, their speed which affords
    them the possibility of generating many samples, which then brings to front and
    center the ability to filter the best generated samples, which in the case of
    protein design has benefited immensely from the transformative development of
    AlphaFold2.â€ â€” Bruno Correia (EPFL)
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œç›®å‰çš„ç”Ÿæˆæ¨¡å‹ä»ç„¶ä¼šäº§ç”Ÿâ€˜åƒåœ¾â€™è¾“å‡ºï¼Œè¿™äº›è¾“å‡ºè¿åäº†åˆ†å­å·²çŸ¥çš„è®¸å¤šç‰©ç†å’ŒåŒ–å­¦å±æ€§ã€‚å½“ç„¶ï¼Œå½“å‰ç”Ÿæˆæ¨¡å‹çš„ä¼˜åŠ¿åœ¨äºå®ƒä»¬çš„é€Ÿåº¦ï¼Œè¿™ä½¿å¾—å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆå¤§é‡æ ·æœ¬ï¼Œä»è€Œä½¿å¾—ç­›é€‰æœ€ä½³ç”Ÿæˆæ ·æœ¬çš„èƒ½åŠ›å˜å¾—å°¤ä¸ºé‡è¦ï¼Œåœ¨è›‹ç™½è´¨è®¾è®¡çš„é¢†åŸŸï¼ŒAlphaFold2çš„å˜é©æ€§å‘å±•å¯¹æ­¤å¸¦æ¥äº†å·¨å¤§çš„å¸®åŠ©ã€‚â€
    â€” å¸ƒé²è¯ºÂ·ç§‘é›·äºšï¼ˆEPFLï¼‰
- en: â¡ï¸ The next challenge to the community will perhaps be how to infuse generative
    models with **meaningful physical and chemical priors** to enhance sampling performance
    and generalization. Interestingly, we have not seen the same remarkable advances
    (experimentally validated) in applications to small molecule design, which we
    hope to see during 2024.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å¯¹ç¤¾åŒºçš„ä¸‹ä¸€ä¸ªæŒ‘æˆ˜æˆ–è®¸æ˜¯å¦‚ä½•å°†**æœ‰æ„ä¹‰çš„ç‰©ç†å’ŒåŒ–å­¦å…ˆéªŒ**æ³¨å…¥ç”Ÿæˆæ¨¡å‹ï¼Œä»¥æé«˜é‡‡æ ·æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨å°åˆ†å­è®¾è®¡åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å°šæœªçœ‹åˆ°åŒæ ·æ˜¾è‘—çš„è¿›å±•ï¼ˆç»å®éªŒéªŒè¯ï¼‰ï¼Œä½†æˆ‘ä»¬å¸Œæœ›åœ¨2024å¹´èƒ½çœ‹åˆ°è¿™ä¸€çªç ´ã€‚
- en: â¡ï¸ **The rise of multimodal models.** Generally in biological-related tasks
    data sparsity is a given and as such strategies to extract the most signal out
    of the data are essential. One way to try to overcome such limitations is to improve
    the expressiveness of the data representations and maybe this way obtain more
    performant neural networks. Likely in the short term, we will be able to explore
    architectures that encompass several types of representations of the objects of
    interest and harness the best predictions for the evermore complex tasks we are
    facing as progressively more of the basic problems get solved. This notion of
    multimodality is of course intimately related to the overall aim of having models
    with stronger priors, that in a generative context, honour fundamental constraints
    of the objects of interest.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ **å¤šæ¨¡æ€æ¨¡å‹çš„å´›èµ·ã€‚** é€šå¸¸åœ¨ç”Ÿç‰©å­¦ç›¸å…³ä»»åŠ¡ä¸­ï¼Œæ•°æ®ç¨€ç¼ºæ€§æ˜¯ä¸€ä¸ªæ™®éé—®é¢˜ï¼Œå› æ­¤ï¼Œä»æ•°æ®ä¸­æå–æœ€å¤§ä¿¡å·çš„ç­–ç•¥è‡³å…³é‡è¦ã€‚å…‹æœè¿™ç§å±€é™æ€§çš„ä¸€ç§æ–¹å¼æ˜¯æé«˜æ•°æ®è¡¨ç¤ºçš„è¡¨ç°åŠ›ï¼Œä¹Ÿè®¸é€šè¿‡è¿™ç§æ–¹å¼å¯ä»¥è·å¾—æ›´é«˜æ•ˆçš„ç¥ç»ç½‘ç»œã€‚çŸ­æœŸå†…ï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿæ¢ç´¢æ¶µç›–å¤šç§å¯¹è±¡è¡¨ç¤ºçš„æ¶æ„ï¼Œå¹¶åˆ©ç”¨æœ€ä¼˜é¢„æµ‹æ¥è§£å†³æˆ‘ä»¬é¢ä¸´çš„è¶Šæ¥è¶Šå¤æ‚çš„ä»»åŠ¡ï¼Œéšç€è¶Šæ¥è¶Šå¤šåŸºç¡€é—®é¢˜çš„è§£å†³ï¼Œè¿™ä¸€ç‚¹å˜å¾—å°¤ä¸ºé‡è¦ã€‚å¤šæ¨¡æ€æ€§çš„æ¦‚å¿µå½“ç„¶ä¸æ‹¥æœ‰æ›´å¼ºå…ˆéªŒçš„æ¨¡å‹çš„æ€»ä½“ç›®æ ‡å¯†åˆ‡ç›¸å…³ï¼Œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¿™äº›æ¨¡å‹å°Šé‡å¯¹è±¡çš„åŸºæœ¬çº¦æŸã€‚
- en: â¡ï¸ **The models that know everything**. As the power of machine learning models
    improves we clearly tend to request a more multi-objective optimization when it
    comes to attempting to solve real life problems. Taking as an example small molecule
    generation, thinking from a biochemical perspective the drug design problem starts
    by having a target to which a small molecule binds, therefore one of the first
    and most important constraints is that the generative process ought to be conditioned
    to the protein pocket. However, such a constraint may not be enough to create
    real small molecules as many of such chemicals are simply impossible or very hard
    to synthesize, and, therefore, a model that has notions of chemical synthesizability
    and can integrate such constraints in the search space would be much more useful.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ **å…¨çŸ¥æ¨¡å‹ã€‚** éšç€æœºå™¨å­¦ä¹ æ¨¡å‹èƒ½åŠ›çš„æå‡ï¼Œæˆ‘ä»¬æ˜¾ç„¶å€¾å‘äºè¯·æ±‚æ›´å¤šçš„å¤šç›®æ ‡ä¼˜åŒ–ï¼Œä»¥è§£å†³ç°å®ç”Ÿæ´»ä¸­çš„é—®é¢˜ã€‚ä»¥å°åˆ†å­ç”Ÿæˆä¸ºä¾‹ï¼Œä»ç”Ÿç‰©åŒ–å­¦çš„è§’åº¦æ¥çœ‹ï¼Œè¯ç‰©è®¾è®¡é—®é¢˜é¦–å…ˆæ˜¯ç¡®å®šä¸€ä¸ªé¶ç‚¹ï¼Œè¯ç‰©åˆ†å­éœ€è¦ä¸è¯¥é¶ç‚¹ç»“åˆï¼Œå› æ­¤ï¼Œç”Ÿæˆè¿‡ç¨‹çš„é¦–è¦çº¦æŸæ¡ä»¶ä¹‹ä¸€æ˜¯å®ƒåº”å½“ä»¥è›‹ç™½è´¨å£è¢‹ä¸ºæ¡ä»¶ã€‚ç„¶è€Œï¼Œè¿™æ ·çš„çº¦æŸå¯èƒ½ä¸è¶³ä»¥ç”ŸæˆçœŸå®çš„å°åˆ†å­ï¼Œå› ä¸ºè®¸å¤šè¿™æ ·çš„åŒ–å­¦ç‰©è´¨æ ¹æœ¬æ— æ³•åˆæˆæˆ–æå…¶éš¾ä»¥åˆæˆï¼Œå› æ­¤ï¼Œå…·å¤‡åŒ–å­¦åˆæˆå¯è¡Œæ€§æ¦‚å¿µå¹¶èƒ½å°†è¿™äº›çº¦æŸé›†æˆåˆ°æœç´¢ç©ºé—´ä¸­çš„æ¨¡å‹å°†æ›´åŠ æœ‰ç”¨ã€‚
- en: â¡ï¸ **From chemotype to phenotype**. On the grounds of data representation, atomic
    graph structures together with vector embeddings have reached remarkable results,
    particularly in the search for new antibiotics. Making accurate predictions of
    which chemical structures have antimicrobial activity, broadly speaking, is an
    exercise of phenotype prediction from chemical structure. Due to the simplicity
    of the approaches used and the impressive results obtained, one would expect that
    more sophisticated data representations on the molecule end and perhaps together
    also with richer phenotype assignment could give critical contributions to such
    an important problem in drug development.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ **ä»åŒ–å­¦å‹åˆ°è¡¨å‹**ã€‚åœ¨æ•°æ®è¡¨ç¤ºæ–¹é¢ï¼ŒåŸå­å›¾ç»“æ„å’Œå‘é‡åµŒå…¥æŠ€æœ¯å–å¾—äº†æ˜¾è‘—æˆæœï¼Œå°¤å…¶æ˜¯åœ¨å¯»æ‰¾æ–°æŠ—ç”Ÿç´ æ–¹é¢ã€‚å¹¿ä¹‰ä¸Šæ¥è¯´ï¼Œå‡†ç¡®é¢„æµ‹å“ªäº›åŒ–å­¦ç»“æ„å…·æœ‰æŠ—èŒæ´»æ€§ï¼Œæ˜¯ä»åŒ–å­¦ç»“æ„åˆ°è¡¨å‹é¢„æµ‹çš„ä¸€ä¸ªé‡è¦ç»ƒä¹ ã€‚ç”±äºæ‰€é‡‡ç”¨æ–¹æ³•çš„ç®€ä¾¿æ€§å’Œå–å¾—çš„ä»¤äººå°è±¡æ·±åˆ»çš„æˆæœï¼Œå¯ä»¥é¢„æœŸï¼Œè‹¥èƒ½åœ¨åˆ†å­ç«¯é‡‡ç”¨æ›´å¤æ‚çš„æ•°æ®è¡¨ç¤ºï¼Œå¹¶ä¸”å¯èƒ½ç»“åˆæ›´ä¸°å¯Œçš„è¡¨å‹åˆ†é…ï¼Œè¿™å°†å¯¹è¯ç‰©å¼€å‘ä¸­çš„è¿™ä¸€é‡è¦é—®é¢˜ä½œå‡ºå…³é”®è´¡çŒ®ã€‚
- en: Industrial perspective
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥ä¸šè§†è§’
- en: '***Luca Naef (VantAI)***'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***Luca Naef (VantAI)***'
- en: ğŸ”¥*What are the biggest advancements in the field you noticed in 2023?*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥*2023å¹´ä½ æ³¨æ„åˆ°çš„é¢†åŸŸé‡Œæœ€å¤§çš„è¿›å±•æ˜¯ä»€ä¹ˆï¼Ÿ*
- en: '1ï¸âƒ£ **Increasing multi-modality & modularity** â€” as shown by the emergence
    of initial co-folding methods for both proteins & small molecules, diffusion and
    non-diffusion-based, to extend on AF2 success: [DiffusionProteinLigand](https://www.biorxiv.org/content/10.1101/2022.12.20.521309v1.full.pdf)
    in the last days of 2022 and [RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1),
    [AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    and [Umol](https://www.biorxiv.org/content/10.1101/2023.11.03.565471v1) by end
    of 2023\. We are also seeing models that have sequence & structure co-trained:
    [SAProt](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2), [ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v1),
    and sequence, structure & surface co-trained with [ProteinINR](https://www.mlsb.io/papers_2023/Pre-training_Sequence_Structure_and_Surface_Features_for_Comprehensive_Protein_Representation_Learning.pdf).
    There is a general revival of surface-based methods after a quieter 2021 and 2022:
    [DiffMasif](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf),
    [SurfDock](https://arxiv.org/abs/2311.17050), and [ShapeProt](https://www.biorxiv.org/content/10.1101/2023.12.03.567710v1).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ **å¤šæ¨¡æ€æ€§å’Œæ¨¡å—åŒ–çš„å¢åŠ ** â€”â€” å¦‚è›‹ç™½è´¨å’Œå°åˆ†å­çš„åˆæ­¥å…±æŠ˜å æ–¹æ³•çš„å‡ºç°æ‰€ç¤ºï¼ŒåŒ…æ‹¬æ‰©å±•AF2æˆåŠŸçš„æ‰©æ•£å’Œéæ‰©æ•£åŸºç¡€æ–¹æ³•ï¼š[DiffusionProteinLigand](https://www.biorxiv.org/content/10.1101/2022.12.20.521309v1.full.pdf)åœ¨2022å¹´æœ€åå‡ å¤©å‘å¸ƒï¼Œå’Œ[RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)ï¼Œ[AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)ä»¥åŠ[Umol](https://www.biorxiv.org/content/10.1101/2023.11.03.565471v1)é¢„è®¡åœ¨2023å¹´åº•å‘å¸ƒã€‚æˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†ä¸€äº›æ¨¡å‹åœ¨åºåˆ—ä¸ç»“æ„å…±åŒè®­ç»ƒçš„åŸºç¡€ä¸Šè¿›è¡Œè®­ç»ƒï¼š[SAProt](https://www.biorxiv.org/content/10.1101/2023.10.01.560349v2)ï¼Œ[ProstT5](https://www.biorxiv.org/content/10.1101/2023.07.23.550085v1)ï¼Œä»¥åŠåœ¨[ProteinINR](https://www.mlsb.io/papers_2023/Pre-training_Sequence_Structure_and_Surface_Features_for_Comprehensive_Protein_Representation_Learning.pdf)æ¨¡å‹ä¸‹ï¼Œåºåˆ—ã€ç»“æ„ä¸è¡¨é¢å…±åŒè®­ç»ƒã€‚ç»è¿‡2021å¹´å’Œ2022å¹´ç›¸å¯¹å¹³é™çš„æ—¶æœŸï¼ŒåŸºäºè¡¨é¢çš„æ–¹æ³•é‡æ–°è·å¾—äº†å…³æ³¨ï¼š[DiffMasif](https://www.mlsb.io/papers_2023/DiffMaSIF_Surface-based_Protein-Protein_Docking_with_Diffusion_Models.pdf)ï¼Œ[SurfDock](https://arxiv.org/abs/2311.17050)ï¼Œå’Œ[ShapeProt](https://www.biorxiv.org/content/10.1101/2023.12.03.567710v1)ã€‚
- en: '2ï¸âƒ£ **Datasets and benchmarks**. Datasets, especially synthetic/computationally
    derived: [ATLAS](https://academic.oup.com/nar/article/52/D1/D384/7438909) and
    the [MDDB](https://mddbr.eu/) for protein dynamics. [MISATO](https://www.biorxiv.org/content/10.1101/2023.05.24.542082v1),
    [SPICE](https://www.nature.com/articles/s41597-022-01882-6), [Splinter](https://www.nature.com/articles/s41597-023-02443-1)
    for protein-ligand complexes, [QM1B](https://arxiv.org/abs/2311.01135) for molecular
    properties. PINDER: large protein-protein docking dataset with matched apo/predicted
    pairs and benchmark suite with retrained docking models. [CryoET data portal](https://chanzuckerberg.github.io/cryoet-data-portal/index.html#)
    for CryoET. And a whole host of welcome benchmarks: PINDER, [PoseBusters](https://arxiv.org/abs/2308.05777),
    and [PoseCheck](https://arxiv.org/abs/2308.07413), with a focus on more rigorous
    and practically relevant settings.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ **æ•°æ®é›†å’ŒåŸºå‡†æµ‹è¯•**ã€‚æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯åˆæˆçš„/è®¡ç®—ç”Ÿæˆçš„æ•°æ®é›†ï¼š[ATLAS](https://academic.oup.com/nar/article/52/D1/D384/7438909)
    å’Œ [MDDB](https://mddbr.eu/) ç”¨äºè›‹ç™½è´¨åŠ¨æ€å­¦ã€‚[MISATO](https://www.biorxiv.org/content/10.1101/2023.05.24.542082v1)ï¼Œ[SPICE](https://www.nature.com/articles/s41597-022-01882-6)ï¼Œ[Splinter](https://www.nature.com/articles/s41597-023-02443-1)
    ç”¨äºè›‹ç™½è´¨-é…ä½“å¤åˆç‰©ï¼Œ[QM1B](https://arxiv.org/abs/2311.01135) ç”¨äºåˆ†å­å±æ€§ã€‚PINDERï¼šä¸€ä¸ªå¤§å‹è›‹ç™½è´¨-è›‹ç™½è´¨å¯¹æ¥æ•°æ®é›†ï¼ŒåŒ…å«åŒ¹é…çš„ç©ºè½½/é¢„æµ‹å¯¹å’ŒåŸºå‡†å¥—ä»¶ï¼Œé™„å¸¦é‡æ–°è®­ç»ƒçš„å¯¹æ¥æ¨¡å‹ã€‚[CryoET
    æ•°æ®é—¨æˆ·](https://chanzuckerberg.github.io/cryoet-data-portal/index.html#) ç”¨äºå†·å†»ç”µå­æ–­å±‚æ‰«æï¼ˆCryoETï¼‰ã€‚ä»¥åŠä¸€ç³»åˆ—å—æ¬¢è¿çš„åŸºå‡†æµ‹è¯•ï¼šPINDERã€[PoseBusters](https://arxiv.org/abs/2308.05777)
    å’Œ [PoseCheck](https://arxiv.org/abs/2308.07413)ï¼Œé‡ç‚¹å…³æ³¨æ›´åŠ ä¸¥æ ¼å’Œå®é™…ç›¸å…³çš„è®¾ç½®ã€‚
- en: 3ï¸âƒ£ **Creative pre-training strategies** to get around the sparsity of diverse
    protein-ligand complexes. Van-der-mers training ([DockGen](https://openreview.net/forum?id=UfBIxpTK10))
    & sidechain training strategies in [RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)
    and pre-training on ligand-only complexes in CCD in [RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1).
    Multi-task pre-training [Unimol](https://openreview.net/forum?id=6K2RM6wVqKu)
    and others.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ **åˆ›é€ æ€§çš„é¢„è®­ç»ƒç­–ç•¥**ï¼Œä»¥å…‹æœè›‹ç™½è´¨-é…ä½“å¤åˆç‰©çš„ç¨€ç¼ºæ€§ã€‚èŒƒå¾·æ¢…å°”è®­ç»ƒï¼ˆ[DockGen](https://openreview.net/forum?id=UfBIxpTK10)ï¼‰å’Œ[RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)ä¸­çš„ä¾§é“¾è®­ç»ƒç­–ç•¥ï¼Œä»¥åŠåœ¨[RF-AA](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)ä¸­çš„é…ä½“å•ç‹¬å¤åˆç‰©é¢„è®­ç»ƒã€‚å¤šä»»åŠ¡é¢„è®­ç»ƒï¼Œå¦‚[Unimol](https://openreview.net/forum?id=6K2RM6wVqKu)åŠå…¶ä»–ã€‚
- en: ğŸ‹ï¸ *What are the open challenges that researchers might overlook?*
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ï¸ *ç ”ç©¶äººå‘˜å¯èƒ½å¿½è§†çš„å¼€æ”¾æ€§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ*
- en: 1ï¸âƒ£ **Generalization.** [DockGen](https://openreview.net/forum?id=UfBIxpTK10)showed
    that current state-of-the-art protein-ligand docking models completely lose predictability
    when asked to generalise towards novel protein domains. We see a similar phenomenon
    in the [AlphaFold-lastest report](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf),
    where performance on novel proteins & ligands drops heavily to below biophysics-based
    baselines (which have access to holo structures), despite very generous definitions
    of novel protein & ligand. This indicates that existing approaches might still
    largely rely on memorization, an observation that has been extensively argued
    over the [years](https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00487)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ **æ³›åŒ–èƒ½åŠ›**ã€‚[DockGen](https://openreview.net/forum?id=UfBIxpTK10)å±•ç¤ºäº†å½“å‰æœ€å…ˆè¿›çš„è›‹ç™½è´¨-é…ä½“å¯¹æ¥æ¨¡å‹åœ¨è¯•å›¾æ³›åŒ–åˆ°æ–°è›‹ç™½è´¨é¢†åŸŸæ—¶å®Œå…¨å¤±å»äº†é¢„æµ‹èƒ½åŠ›ã€‚åœ¨[AlphaFoldæœ€æ–°æŠ¥å‘Š](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/a-glimpse-of-the-next-generation-of-alphafold/alphafold_latest_oct2023.pdf)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ç±»ä¼¼çš„ç°è±¡ï¼Œæ–°è›‹ç™½è´¨å’Œé…ä½“çš„è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œç”šè‡³ä½äºåŸºäºç”Ÿç‰©ç‰©ç†å­¦çš„åŸºçº¿ï¼ˆè¿™äº›åŸºçº¿å¯ä»¥è®¿é—®å®Œæ•´ç»“æ„ï¼‰ï¼Œå°½ç®¡å¯¹æ–°è›‹ç™½è´¨å’Œé…ä½“çš„å®šä¹‰éå¸¸å®½æ³›ã€‚è¿™è¡¨æ˜ç°æœ‰çš„æ–¹æ³•ä»ç„¶å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè®°å¿†ï¼Œè¿™ä¸€è§‚å¯Ÿå·²åœ¨[å¤šå¹´](https://pubs.acs.org/doi/10.1021/acs.jmedchem.2c00487)çš„è®¨è®ºä¸­å¾—åˆ°äº†å¹¿æ³›è®ºè¯ã€‚
- en: '2ï¸âƒ£ **The curse of (simple) baselines.** A recurring topic over the years,
    2023 has again shown what industry practitioners have long known: in many practical
    problems such as molecular generation, property prediction, docking, and conformer
    prediction, simple baselines or classical approaches often still outperform ML-based
    approaches in practice. This has been documented increasingly in 2023 by [Tripp
    et al.](https://arxiv.org/abs/2310.09267)**,** [Yu et al.](https://arxiv.org/abs/2302.07134),
    [Zhou et al.](https://arxiv.org/abs/2302.07061)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ **ï¼ˆç®€å•ï¼‰åŸºçº¿çš„è¯…å’’**ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šå¹´æ¥åå¤å‡ºç°çš„è¯é¢˜ï¼Œ2023å¹´å†æ¬¡è¯æ˜äº†ä¸šç•Œä»ä¸šè€…æ—©å·²çŸ¥é“çš„ä¸€ç‚¹ï¼šåœ¨è®¸å¤šå®é™…é—®é¢˜ä¸­ï¼Œå¦‚åˆ†å­ç”Ÿæˆã€å±æ€§é¢„æµ‹ã€å¯¹æ¥å’Œæ„è±¡é¢„æµ‹ï¼Œç®€å•çš„åŸºçº¿æˆ–ç»å…¸æ–¹æ³•å¾€å¾€åœ¨å®è·µä¸­ä»ä¼˜äºåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•ã€‚2023å¹´ï¼Œ[Trippç­‰äºº](https://arxiv.org/abs/2310.09267)**ï¼Œ**
    [Yuç­‰äºº](https://arxiv.org/abs/2302.07134)ï¼Œ[Zhouç­‰äºº](https://arxiv.org/abs/2302.07061)å¯¹æ­¤æœ‰è¶Šæ¥è¶Šå¤šçš„æ–‡çŒ®è®°å½•ã€‚
- en: ğŸ”® *Predictions for 2024!*
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® *2024å¹´çš„é¢„æµ‹ï¼*
- en: â€œIn 2024, data sparsity will remain top of mind and we will see a lot of smart
    ways to use models to generate synthetic training data. Self-distillation in AlphaFold2
    served as a big inspiration, Confidence Bootstrapping in [DockGen](https://openreview.net/forum?id=UfBIxpTK10),
    leveraging the insight that we now have sufficiently powerful models that can
    score poses but not always generate them, first realised in [2022](https://www.biorxiv.org/content/10.1101/2022.03.11.484043v1).â€
    â€” Luca Naef (VantAI)
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨2024å¹´ï¼Œæ•°æ®ç¨€ç¼ºé—®é¢˜ä»å°†æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¾ˆå¤šæ™ºèƒ½æ–¹æ³•æ¥ä½¿ç”¨æ¨¡å‹ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®ã€‚AlphaFold2ä¸­çš„è‡ªè’¸é¦ä¸ºæˆ‘ä»¬æä¾›äº†é‡è¦çš„å¯ç¤ºï¼Œ[DockGen](https://openreview.net/forum?id=UfBIxpTK10)ä¸­çš„ç½®ä¿¡å¼•å¯¼æ–¹æ³•ï¼Œåˆ©ç”¨äº†æˆ‘ä»¬ç°åœ¨å·²ç»æ‹¥æœ‰è¶³å¤Ÿå¼ºå¤§çš„æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹èƒ½å¤Ÿè¯„åˆ†å§¿åŠ¿ï¼Œä½†å¹¶ä¸æ€»æ˜¯èƒ½å¤Ÿç”Ÿæˆå§¿åŠ¿ï¼Œè¿™ä¸€å‘ç°æœ€æ—©åœ¨[2022å¹´](https://www.biorxiv.org/content/10.1101/2022.03.11.484043v1)ä¸­æå‡ºã€‚â€
    â€” Luca Naef (VantAI)
- en: 2ï¸âƒ£ We will see more biological/chemical assays purpose-built for ML or only
    making sense in a machine learning context (i.e., might not lead to biological
    insight by themselves but be primarily useful for training models). An example
    from 2023 is the large-scale protein folding experiments by [Tsuboyama et al.](https://www.nature.com/articles/s41586-023-06328-6)
    This move might be driven by techbio startups, where we have seen the first foundation
    models built on such ML-purpose-built assays for structural biology with e.g.
    [ATOM-1](https://www.biorxiv.org/content/10.1101/2023.12.13.571579v1).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ æˆ‘ä»¬å°†çœ‹åˆ°æ›´å¤šä¸ºæœºå™¨å­¦ä¹ ä¸“é—¨è®¾è®¡çš„ç”Ÿç‰©/åŒ–å­¦å®éªŒï¼Œæˆ–ä»…åœ¨æœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹æ‰æœ‰æ„ä¹‰ï¼ˆå³ï¼Œè¿™äº›å®éªŒå¯èƒ½æœ¬èº«ä¸ä¼šå¸¦æ¥ç”Ÿç‰©å­¦ä¸Šçš„æ–°è§è§£ï¼Œä½†ä¸»è¦ç”¨äºè®­ç»ƒæ¨¡å‹ï¼‰ã€‚2023å¹´çš„ä¸€ä¸ªä¾‹å­æ˜¯[Tsuboyamaç­‰äºº](https://www.nature.com/articles/s41586-023-06328-6)çš„å¤§è§„æ¨¡è›‹ç™½è´¨æŠ˜å å®éªŒã€‚è¿™ä¸ªä¸¾æªå¯èƒ½å—åˆ°ç§‘æŠ€ç”Ÿç‰©åˆåˆ›å…¬å¸çš„æ¨åŠ¨ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°åŸºäºè¿™äº›ä¸“é—¨ä¸ºç»“æ„ç”Ÿç‰©å­¦è®¾è®¡çš„æœºå™¨å­¦ä¹ å®éªŒæ„å»ºçš„é¦–ä¸ªåŸºç¡€æ¨¡å‹ï¼Œä¾‹å¦‚[ATOM-1](https://www.biorxiv.org/content/10.1101/2023.12.13.571579v1)ã€‚
- en: '***Andreas Loukas (Prescient Design, part of Genentech)***'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***Andreas Loukas (Prescient Design, å±äºåŸºå› æ³°å…‹çš„ä¸€éƒ¨åˆ†)***'
- en: ğŸ”¥ *What are the biggest advancements in the field you noticed in 2023?*
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ *ä½ åœ¨2023å¹´æ³¨æ„åˆ°çš„è¯¥é¢†åŸŸæœ€å¤§è¿›å±•æ˜¯ä»€ä¹ˆï¼Ÿ*
- en: â€œIn 2023, we started to see some of the challenges of equivariant generation
    and representation for proteins to be resolved through diffusion models.â€ â€” Andreas
    Loukas (Prescient Design)
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨2023å¹´ï¼Œæˆ‘ä»¬å¼€å§‹çœ‹åˆ°ä¸€äº›å…³äºè›‹ç™½è´¨ç­‰å˜ç”Ÿæˆå’Œè¡¨ç¤ºçš„æŒ‘æˆ˜é€šè¿‡æ‰©æ•£æ¨¡å‹å¾—ä»¥è§£å†³ã€‚â€ â€” Andreas Loukas (Prescient Design)
- en: 1ï¸âƒ£ We also noticed a **shift towards approaches that model and generate molecular
    systems at higher fidelity**. For instance, the most recent models adopt a fully
    end-to-end approach by generating backbone, sequence and side-chains jointly ([AbDiffuser](https://openreview.net/pdf?id=7GyYpomkEa),
    [dyMEAN](https://arxiv.org/pdf/2302.00203.pdf)) or at least solve the problem
    in two steps but with a partially joint model ([Chroma](https://www.nature.com/articles/s41586-023-06728-8));
    as compared to backbone generation followed by inverse folding as in [RFDiffusion](https://www.nature.com/articles/s41586-023-06415-8)
    and [FrameDiff](https://openreview.net/pdf?id=m8OUBymxwv). Other attempts to improve
    the modelling fidelity can be found in the latest updates to co-folding tools
    like [AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)
    and [RFDiffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)
    which render them sensitive to non-protein components (ligands, prosthetic groups,
    cofactors); as well as in papers that attempt to account for conformational dynamics
    (see discussion above). In my view, this line of work is essential because the
    binding behaviour of molecular systems can be very sensitive to how atoms are
    placed, move, and interact.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æˆ‘ä»¬è¿˜æ³¨æ„åˆ°äº†ä¸€ç§**å‘æ›´é«˜ä¿çœŸåº¦çš„åˆ†å­ç³»ç»Ÿå»ºæ¨¡ä¸ç”Ÿæˆæ–¹æ³•è½¬å˜**ã€‚ä¾‹å¦‚ï¼Œæœ€æ–°çš„æ¨¡å‹é‡‡ç”¨å®Œå…¨ç«¯åˆ°ç«¯çš„æ–¹æ³•ï¼Œé€šè¿‡è”åˆç”Ÿæˆä¸»é“¾ã€åºåˆ—å’Œä¾§é“¾ï¼ˆ[AbDiffuser](https://openreview.net/pdf?id=7GyYpomkEa),
    [dyMEAN](https://arxiv.org/pdf/2302.00203.pdf)ï¼‰ï¼Œæˆ–è€…è‡³å°‘é€šè¿‡ä¸€ä¸ªéƒ¨åˆ†è”åˆçš„æ¨¡å‹åœ¨ä¸¤æ­¥ä¸­è§£å†³é—®é¢˜ï¼ˆ[Chroma](https://www.nature.com/articles/s41586-023-06728-8)ï¼‰ï¼›ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸»é“¾ç”Ÿæˆåå†è¿›è¡Œåå‘æŠ˜å ï¼Œå¦‚åœ¨[RF
    Diffusion](https://www.nature.com/articles/s41586-023-06415-8)å’Œ[FrameDiff](https://openreview.net/pdf?id=m8OUBymxwv)ä¸­æ‰€è¿°ã€‚å…¶ä»–æ”¹å–„å»ºæ¨¡ä¿çœŸåº¦çš„å°è¯•å¯ä»¥åœ¨æœ€æ–°çš„å…±æŠ˜å å·¥å…·æ›´æ–°ä¸­æ‰¾åˆ°ï¼Œå¦‚[AlphaFold2](https://www.isomorphiclabs.com/articles/a-glimpse-of-the-next-generation-of-alphafold)å’Œ[RF
    Diffusion](https://www.biorxiv.org/content/10.1101/2023.10.09.561603v1)ï¼Œå®ƒä»¬ä½¿æ¨¡å‹å¯¹éè›‹ç™½è´¨æˆåˆ†ï¼ˆé…ä½“ã€è¾…åŸºã€è¾…å› å­ï¼‰æ›´ä¸ºæ•æ„Ÿï¼›åŒæ—¶ä¹Ÿå¯ä»¥åœ¨ä¸€äº›è¯•å›¾è€ƒè™‘æ„è±¡åŠ¨åŠ›å­¦çš„è®ºæ–‡ä¸­æ‰¾åˆ°ï¼ˆè§ä¸Šæ–‡è®¨è®ºï¼‰ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä¸€ç ”ç©¶æ–¹å‘è‡³å…³é‡è¦ï¼Œå› ä¸ºåˆ†å­ç³»ç»Ÿçš„ç»“åˆè¡Œä¸ºå¯¹åŸå­å¦‚ä½•æ”¾ç½®ã€ç§»åŠ¨å’Œç›¸äº’ä½œç”¨éå¸¸æ•æ„Ÿã€‚
- en: 2ï¸âƒ£ In 2023, many works also attempted to get a handle on **binding affinity**
    by learning to predict the effect of mutations of a known crystal by pre-training
    on large corpora, such as computationally predicted mutations ([graphinity](https://github.com/oxpig/Graphinity)),
    and on side-tasks, such as [rotamer density estimation](https://openreview.net/pdf?id=_X9Yl1K2mD).
    The obtained results are encouraging as they can significantly outperform semi-empirical
    baselines like Rosetta and FoldX. However, there is still significant work to
    be done to render these models reliable for binding affinity prediction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ åœ¨2023å¹´ï¼Œè®¸å¤šç ”ç©¶ä¹Ÿå°è¯•é€šè¿‡å­¦ä¹ é¢„æµ‹å·²çŸ¥æ™¶ä½“çªå˜çš„æ•ˆåº”æ¥æŒæ¡**ç»“åˆäº²å’ŒåŠ›**ï¼Œé€šè¿‡åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¦‚è®¡ç®—é¢„æµ‹çš„çªå˜ï¼ˆ[graphinity](https://github.com/oxpig/Graphinity)ï¼‰ï¼Œä»¥åŠä¾§ä»»åŠ¡ï¼Œå¦‚[æ—‹è½¬ä½“å¯†åº¦ä¼°è®¡](https://openreview.net/pdf?id=_X9Yl1K2mD)ã€‚æ‰€è·å¾—çš„ç»“æœä»¤äººé¼“èˆï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥æ˜¾è‘—è¶…è¶Šå¦‚Rosettaå’ŒFoldXç­‰åŠç»éªŒåŸºçº¿ã€‚ç„¶è€Œï¼Œä»ç„¶æœ‰å¤§é‡å·¥ä½œéœ€è¦å®Œæˆï¼Œä»¥ä½¿è¿™äº›æ¨¡å‹åœ¨ç»“åˆäº²å’ŒåŠ›é¢„æµ‹ä¸­æ›´åŠ å¯é ã€‚
- en: '3ï¸âƒ£ I have further observed a growing recognition of **protein Language Models
    (pLMs)**and specifically [ESM](https://www.science.org/doi/10.1126/science.ade2574)
    as valuable tools, even among those who primarily favour geometric deep learning.
    These embeddings are used to help docking models, allow the construction of simple
    yet competitive predictive models for binding affinity prediction ([Li et al 2023](https://www.nature.com/articles/s41467-023-39022-2)),
    and can generally offer an efficient method to create residue representations
    for GNNs that are informed by the extensive proteome data without the need for
    extensive pretraining ([Jamasb et al 2023](https://www.mlsb.io/papers_2023/Evaluating_Representation_Learning_on_the_Protein_Structure_Universe.pdf)).
    However, I do maintain a concern regarding the use of pLMs: it is unclear whether
    their effectiveness is due to data leakage or genuine generalisation. This is
    particularly pertinent when evaluating models on tasks like amino-acid recovery
    in inverse folding and conditional CDR design, where distinguishing between these
    two factors is crucial.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æˆ‘è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ï¼Œ**è›‹ç™½è´¨è¯­è¨€æ¨¡å‹ï¼ˆpLMsï¼‰**ï¼Œç‰¹åˆ«æ˜¯[ESM](https://www.science.org/doi/10.1126/science.ade2574)ï¼Œä½œä¸ºæœ‰ä»·å€¼çš„å·¥å…·ï¼Œå¾—åˆ°äº†è¶Šæ¥è¶Šå¤šçš„è®¤å¯ï¼Œå³ä½¿åœ¨é‚£äº›ä¸»è¦åå¥½å‡ ä½•æ·±åº¦å­¦ä¹ çš„äººç¾¤ä¸­ã€‚è¿™äº›åµŒå…¥è¢«ç”¨æ¥å¸®åŠ©å¯¹æ¥æ¨¡å‹ï¼Œå…è®¸æ„å»ºç®€å•ä½†å…·æœ‰ç«äº‰åŠ›çš„ç»“åˆäº²å’ŒåŠ›é¢„æµ‹æ¨¡å‹ï¼ˆ[Li
    et al 2023](https://www.nature.com/articles/s41467-023-39022-2)ï¼‰ï¼Œå¹¶ä¸”é€šå¸¸èƒ½æä¾›ä¸€ç§é«˜æ•ˆçš„æ–¹å¼ï¼Œåˆ©ç”¨å¹¿æ³›çš„è›‹ç™½è´¨ç»„æ•°æ®ä¸ºGNNsåˆ›å»ºæ®‹åŸºè¡¨ç¤ºï¼Œè€Œæ— éœ€å¤§é‡çš„é¢„è®­ç»ƒï¼ˆ[Jamasb
    et al 2023](https://www.mlsb.io/papers_2023/Evaluating_Representation_Learning_on_the_Protein_Structure_Universe.pdf)ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ç¡®å®å¯¹ä½¿ç”¨pLMså­˜åœ¨æ‹…å¿§ï¼šç›®å‰å°šä¸æ¸…æ¥šå®ƒä»¬çš„æœ‰æ•ˆæ€§æ˜¯ç”±äºæ•°æ®æ³„æ¼è¿˜æ˜¯ç”±çœŸæ­£çš„æ³›åŒ–èƒ½åŠ›æ‰€è‡´ã€‚åœ¨è¯„ä¼°å¦‚é€†æŠ˜å ä¸­çš„æ°¨åŸºé…¸æ¢å¤å’Œæ¡ä»¶CDRè®¾è®¡ç­‰ä»»åŠ¡æ—¶ï¼Œè¿™ä¸€ç‚¹å°¤å…¶é‡è¦ï¼Œå› ä¸ºåŒºåˆ†è¿™ä¸¤è€…çš„å› ç´ è‡³å…³é‡è¦ã€‚
- en: ğŸ‹ï¸ *What are the open challenges that researchers might overlook?*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ï¸ *ç ”ç©¶äººå‘˜å¯èƒ½å¿½è§†äº†å“ªäº›å¼€æ”¾æ€§æŒ‘æˆ˜ï¼Ÿ*
- en: 1ï¸âƒ£ Working with **energetically relaxed crystal structures** (and, even worse,
    folded structures) can significantly affect the performance of downstream predictive
    models. This is especially true for the prediction of protein-protein interactions
    (PPIs). In my experience, the performance of PPI predictors severely deteriorates
    when they are given a relaxed structure as opposed to the binding (holo) crystalised
    structure.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¸**èƒ½é‡æ”¾æ¾çš„æ™¶ä½“ç»“æ„**ï¼ˆç”šè‡³æ›´ç³Ÿï¼ŒæŠ˜å ç»“æ„ï¼‰ä¸€èµ·å·¥ä½œå¯èƒ½ä¼šæ˜¾è‘—å½±å“ä¸‹æ¸¸é¢„æµ‹æ¨¡å‹çš„æ€§èƒ½ã€‚å¯¹äºè›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ï¼ˆPPIï¼‰çš„é¢„æµ‹å°¤ä¸ºå¦‚æ­¤ã€‚æ ¹æ®æˆ‘çš„ç»éªŒï¼Œå½“é¢„æµ‹æ¨¡å‹ç»™å®šçš„æ˜¯æ”¾æ¾åçš„ç»“æ„è€Œéç»“åˆï¼ˆå…¨ï¼‰æ™¶ä½“ç»“æ„æ—¶ï¼ŒPPIé¢„æµ‹å™¨çš„æ€§èƒ½ä¼šä¸¥é‡ä¸‹é™ã€‚
- en: 2ï¸âƒ£ Though successful *in silico* antibody design has the capacity to revolutionise
    drug design, **general protein models are not (yet?) as good at folding, docking
    or generating antibodies as antibody-specific models are**. This is perhaps due
    to the low conformational variability of the antibody fold and the distinct binding
    mode between antibodies and antigens (loop-mediated interactions that can involve
    a non-negligible entropic component). Perhaps for the same reasons, the *de novo*
    design of antibody binders (that I define as 0-shot generation of an antibody
    that binds to a previously unseen epitope) remains an open problem. Currently,
    experimentally confirmed cases of *de novo* binders involve mostly stable proteins,
    like [alpha-helical bundles](https://www.nature.com/articles/s41586-023-06415-8),
    that are common in the PDB and harbour interfaces that differ substantially from
    epitope-paratope interactions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å°½ç®¡æˆåŠŸçš„*è®¡ç®—æœºè¾…åŠ©*æŠ—ä½“è®¾è®¡æœ‰æ½œåŠ›å½»åº•æ”¹å˜è¯ç‰©è®¾è®¡ï¼Œä½†**é€šç”¨è›‹ç™½è´¨æ¨¡å‹ï¼ˆè¿˜ï¼Ÿï¼‰ä¸å¦‚ä¸“é—¨é’ˆå¯¹æŠ—ä½“çš„æ¨¡å‹åœ¨æŠ˜å ã€å¯¹æ¥æˆ–ç”ŸæˆæŠ—ä½“æ–¹é¢è¡¨ç°å¾—é‚£ä¹ˆå¥½**ã€‚è¿™å¯èƒ½æ˜¯ç”±äºæŠ—ä½“æŠ˜å çš„ä½æ„è±¡å˜å¼‚æ€§å’ŒæŠ—ä½“ä¸æŠ—åŸä¹‹é—´ä¸åŒçš„ç»“åˆæ–¹å¼ï¼ˆæ¶‰åŠç¯ä»‹å¯¼çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶å¯èƒ½åŒ…å«ä¸å¯å¿½è§†çš„ç†µé¡¹ï¼‰ã€‚ä¹Ÿè®¸ç”±äºåŒæ ·çš„åŸå› ï¼ŒæŠ—ä½“ç»“åˆç‰©çš„*de
    novo*è®¾è®¡ï¼ˆæˆ‘å®šä¹‰ä¸ºé›¶æ ·æœ¬ç”Ÿæˆèƒ½ä¸ä¹‹å‰æœªè§çš„è¡¨ä½ç»“åˆçš„æŠ—ä½“ï¼‰ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£çš„éš¾é¢˜ã€‚ç›®å‰ï¼Œç»è¿‡å®éªŒéªŒè¯çš„*de novo*ç»“åˆç‰©æ¡ˆä¾‹å¤§å¤šæ¶‰åŠç¨³å®šçš„è›‹ç™½è´¨ï¼Œå¦‚[Î±-èºæ—‹æŸ](https://www.nature.com/articles/s41586-023-06415-8)ï¼Œå®ƒä»¬åœ¨PDBä¸­è¾ƒä¸ºå¸¸è§ï¼Œä¸”åŒ…å«çš„ç•Œé¢ä¸è¡¨ä½-æŠ—è¡¨ä½ç›¸äº’ä½œç”¨æœ‰æ˜¾è‘—ä¸åŒã€‚
- en: '3ï¸âƒ£ **We are still lacking a general-purpose proxy for binding free energy***.*
    The main issue here is the lack of high-quality data of sufficient size and diversity
    (esp. co-crystal structures). We should therefore be cognizant of the limitations
    of any such learned proxy for any model evaluation: though predicted binding scores
    that are out of distribution of known binders is a clear signal that something
    is off, we should avoid the typical pitfall of trying to demonstrate the superiority
    of our model in an empirical evaluation by showing how it leads to even higher
    scores.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ **æˆ‘ä»¬ä»ç„¶ç¼ºä¹ä¸€ç§é€šç”¨çš„ç»“åˆè‡ªç”±èƒ½ä»£ç†æ¨¡å‹***ã€‚é—®é¢˜åœ¨äºç¼ºä¹è¶³å¤Ÿå¤§ä¸”å¤šæ ·åŒ–çš„é«˜è´¨é‡æ•°æ®ï¼ˆç‰¹åˆ«æ˜¯å…±æ™¶ç»“æ„ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥æ„è¯†åˆ°ï¼Œä»»ä½•æ­¤ç±»å­¦ä¹ ä»£ç†æ¨¡å‹åœ¨è¯„ä¼°æ¨¡å‹æ—¶çš„å±€é™æ€§ï¼šå°½ç®¡é¢„æµ‹çš„ç»“åˆè¯„åˆ†è¶…å‡ºå·²çŸ¥ç»“åˆç‰©åˆ†å¸ƒèŒƒå›´æ˜¯æ˜æ˜¾çš„ä¿¡å·ï¼Œè¡¨æ˜æŸäº›åœ°æ–¹å¯èƒ½å‡ºäº†é—®é¢˜ï¼Œä½†æˆ‘ä»¬åº”é¿å…é™·å…¥å…¸å‹çš„è¯¯åŒºï¼Œå³é€šè¿‡å±•ç¤ºæ¨¡å‹å¦‚ä½•å¯¼è‡´æ›´é«˜çš„è¯„åˆ†æ¥è¯æ˜æˆ‘ä»¬æ¨¡å‹çš„ä¼˜è¶Šæ€§ã€‚
- en: '***Dominique Beaini (Valence Labs, part of Recursion)***'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¤šç±³å°¼å…‹Â·è´é˜¿å°¼ï¼ˆValence Labsï¼ŒRecursionçš„ä¸€éƒ¨åˆ†ï¼‰***'
- en: â€œIâ€™m excited to see a very large community being built around the problem of
    drug discovery, and I feel we are on the brink of a new revolution in the speed
    and efficiency of discovering drugs.â€ â€” Dominique Beaini (Valence Labs)
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘å¾ˆé«˜å…´çœ‹åˆ°å›´ç»•è¯ç‰©å‘ç°é—®é¢˜å»ºç«‹èµ·äº†ä¸€ä¸ªåºå¤§çš„ç¤¾åŒºï¼Œå¹¶ä¸”æˆ‘æ„Ÿåˆ°æˆ‘ä»¬æ­£å¤„äºè¯ç‰©å‘ç°é€Ÿåº¦å’Œæ•ˆç‡æ–°é©å‘½çš„è¾¹ç¼˜ã€‚â€ â€” å¤šç±³å°¼å…‹Â·è´é˜¿å°¼ï¼ˆValence Labsï¼‰
- en: '*What work got me excited in 2023?*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*2023å¹´æœ‰å“ªäº›å·¥ä½œè®©æˆ‘æ„Ÿåˆ°å…´å¥‹ï¼Ÿ*'
- en: I am confident that machine learning will allow us to tackle rare diseases quickly,
    stop the next COVID-X pandemic before it can spread, and live longer and healthier.
    But thereâ€™s a lot of work to be done and there are a lot of challenges ahead,
    some bumps in the road, and some canyons on the way. Speaking of communities,
    you can visit the [Valence Portal](https://portal.valencelabs.com/) to keep up-to-date
    with the ğŸ”¥ new in ML for drug discovery.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡æœºå™¨å­¦ä¹ å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¿«é€Ÿåº”å¯¹ç½•è§ç—…ï¼Œé˜»æ­¢ä¸‹ä¸€åœºCOVID-Xå¤§æµè¡Œåœ¨è”“å»¶ä¹‹å‰çˆ†å‘ï¼Œå¹¶ä½¿æˆ‘ä»¬æ´»å¾—æ›´ä¹…ã€æ›´å¥åº·ã€‚ä½†ä»æœ‰å¤§é‡å·¥ä½œè¦åšï¼Œå‰æ–¹æœ‰è®¸å¤šæŒ‘æˆ˜ï¼Œæœ‰ä¸€äº›éšœç¢å’Œä¸€äº›å³¡è°·ã€‚è¯´åˆ°ç¤¾åŒºï¼Œæ‚¨å¯ä»¥è®¿é—®[Valence
    Portal](https://portal.valencelabs.com/)ä»¥è·Ÿä¸Šè¯ç‰©å‘ç°é¢†åŸŸæœºå™¨å­¦ä¹ ä¸­çš„ğŸ”¥æ–°è¿›å±•ã€‚
- en: '*What are the hard questions for 2024?*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*2024å¹´æœ‰å“ªäº›éš¾é¢˜ï¼Ÿ*'
- en: 'âš›ï¸ **A new generation of quantum mechanics.** Machine learning force-fields,
    often based on equivariant and invariant GNNs, have been promising us a treasure.
    The treasure of the precision of density functional theory, but thousands of times
    faster and at the scale of entire proteins. Although some steps were made in this
    direction with [Allegro](https://link.springer.com/chapter/10.1007/978-3-031-32041-5_12)
    and [MACE-MP](https://arxiv.org/pdf/2401.00096.pdf), current models do not generalize
    well to unseen settings and very large molecules, and they are still too slow
    to be applicable on the timescale that is needed ğŸ¢. For the generalization, I
    believe that bigger and more diverse datasets are the most important stepping
    stones. For the computation time, I believe we will see models that are less enforcing
    of the equivariance, such as [FAENet](https://arxiv.org/pdf/2305.05577.pdf). But
    efficient sampling methods will play a bigger role: spatial-sampling such as using
    [DiffDock](https://arxiv.org/abs/2210.01776) to get more interesting starting
    points and time-sampling such as [TimeWarp](https://www.microsoft.com/en-us/research/publication/timewarp-transferable-acceleration-of-molecular-dynamics-by-learning-time-coarsened-dynamics/)
    to avoid simulating every frame. Iâ€™m really excited by the big STEBS ğŸ‘£ awaiting
    us in 2024: Spatio-temporal equivariant Boltzmann samplers.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: âš›ï¸ **æ–°ä¸€ä»£é‡å­åŠ›å­¦ã€‚** åŸºäºç­‰å˜å’Œä¸å˜å›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰çš„æœºå™¨å­¦ä¹ åŠ›åœºï¼Œä¸€ç›´å‘æˆ‘ä»¬æ‰¿è¯ºç€ä¸€ç¬”å®è—ã€‚é‚£å°±æ˜¯å¯†åº¦æ³›å‡½ç†è®ºçš„ç²¾åº¦ï¼Œä½†é€Ÿåº¦è¦å¿«ä¸Šåƒå€ï¼Œå¹¶ä¸”é€‚ç”¨äºæ•´ä¸ªè›‹ç™½è´¨çš„å°ºåº¦ã€‚è™½ç„¶[Allegro](https://link.springer.com/chapter/10.1007/978-3-031-32041-5_12)å’Œ[MACE-MP](https://arxiv.org/pdf/2401.00096.pdf)ç­‰æ–¹å‘ä¸Šå·²å–å¾—äº†ä¸€äº›è¿›å±•ï¼Œä½†ç›®å‰çš„æ¨¡å‹åœ¨æœªè§è¿‡çš„æƒ…å¢ƒå’Œè¶…å¤§åˆ†å­ä¸Šæ³›åŒ–æ€§è¾ƒå·®ï¼Œè€Œä¸”å®ƒä»¬çš„è®¡ç®—é€Ÿåº¦ä»ç„¶å¤ªæ…¢ï¼Œæ— æ³•æ»¡è¶³æ‰€éœ€çš„æ—¶é—´å°ºåº¦ğŸ¢ã€‚å¯¹äºæ³›åŒ–é—®é¢˜ï¼Œæˆ‘ç›¸ä¿¡æ›´å¤§ä¸”æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†æ˜¯æœ€é‡è¦çš„çªç ´ç‚¹ã€‚å¯¹äºè®¡ç®—æ—¶é—´ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å°†çœ‹åˆ°ä¸€äº›ä¸é‚£ä¹ˆå¼ºæ±‚ç­‰å˜æ€§çš„æ¨¡å‹ï¼Œä¾‹å¦‚[FAENet](https://arxiv.org/pdf/2305.05577.pdf)ã€‚ä½†é«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•å°†å‘æŒ¥æ›´å¤§çš„ä½œç”¨ï¼šç©ºé—´é‡‡æ ·ï¼Œæ¯”å¦‚ä½¿ç”¨[DiffDock](https://arxiv.org/abs/2210.01776)æ¥è·å¾—æ›´æœ‰è¶£çš„èµ·å§‹ç‚¹ï¼Œä»¥åŠæ—¶é—´é‡‡æ ·ï¼Œå¦‚[TimeWarp](https://www.microsoft.com/en-us/research/publication/timewarp-transferable-acceleration-of-molecular-dynamics-by-learning-time-coarsened-dynamics/)æ¥é¿å…æ¨¡æ‹Ÿæ¯ä¸€å¸§ã€‚æˆ‘å¯¹2024å¹´å³å°†åˆ°æ¥çš„é‡å¤§çªç ´å……æ»¡æœŸå¾…ï¼šæ—¶ç©ºç­‰å˜çš„Boltzmanné‡‡æ ·å™¨ï¼ˆSTEBSï¼‰ğŸ‘£ã€‚
- en: 'ğŸ•¸ï¸ **Everything is connected. Biology is inherently multimodal ğŸ™‹ğŸ ğŸ§«ğŸ§¬ğŸ§ª.** One
    cannot simply decouple the molecule from the rest of the biological system. Of
    course, thatâ€™s how ML for drug discovery was done in the past: simply build a
    model of the molecular graph and fit it to experimental data. But we have reached
    a critical point ğŸ›‘, no matter how many trillion parameters are in the GNN model
    is, and how much data are used to train it, and how many experts are mixtured
    together. It is time to bring biology into the mix, and the most straightforward
    way is with multi-modal models. One method is to condition the output of the GNNs
    with the target protein sequences such as [MocFormer](https://www.biorxiv.org/content/10.1101/2023.09.13.557595v4.abstract).
    Another is to use microscopy images or transcriptomics to better inform the model
    of the biological signature of molecules such as [TranSiGen](https://www.biorxiv.org/content/10.1101/2023.11.12.566777v1.full).
    Yet another is to use LLMs to embed contextual information about the tasks such
    as [TwinBooster](https://arxiv.org/pdf/2401.04478.pdf). Or even better, combining
    all of these together ğŸ¤¯, but this could take years. The main issue for the broader
    community seems to be the availability of large amounts of quality and standardized
    data, but fortunately, this is not an issue for Valence.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ•¸ï¸ **ä¸€åˆ‡éƒ½æ˜¯ç›¸äº’è”ç³»çš„ã€‚ç”Ÿç‰©å­¦æœ¬è´¨ä¸Šæ˜¯å¤šæ¨¡æ€çš„ğŸ™‹ğŸ ğŸ§«ğŸ§¬ğŸ§ªã€‚** æ— æ³•ç®€å•åœ°å°†åˆ†å­ä¸å…¶ä»–ç”Ÿç‰©ç³»ç»Ÿè§£è€¦ã€‚å½“ç„¶ï¼Œè¿™å°±æ˜¯è¿‡å»è¯ç‰©å‘ç°ä¸­çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼šä»…ä»…æ„å»ºåˆ†å­å›¾æ¨¡å‹å¹¶å°†å…¶æ‹Ÿåˆåˆ°å®éªŒæ•°æ®ä¸­ã€‚ä½†æˆ‘ä»¬å·²ç»è¾¾åˆ°äº†ä¸€ä¸ªå…³é”®ç‚¹ğŸ›‘ï¼Œæ— è®ºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ¨¡å‹ä¸­æœ‰å¤šå°‘ä¸‡äº¿ä¸ªå‚æ•°ï¼Œä½¿ç”¨äº†å¤šå°‘æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…æ±‡èšäº†å¤šå°‘ä¸“å®¶å›¢é˜Ÿï¼Œå®ƒéƒ½æ— æ³•è§£å†³é—®é¢˜ã€‚æ˜¯æ—¶å€™å°†ç”Ÿç‰©å­¦èå…¥å…¶ä¸­ï¼Œæœ€ç›´æ¥çš„æ–¹å¼å°±æ˜¯é‡‡ç”¨å¤šæ¨¡æ€æ¨¡å‹ã€‚ä¸€ç§æ–¹æ³•æ˜¯ç”¨ç›®æ ‡è›‹ç™½è´¨åºåˆ—æ¥è°ƒèŠ‚GNNçš„è¾“å‡ºï¼Œä¾‹å¦‚[MocFormer](https://www.biorxiv.org/content/10.1101/2023.09.13.557595v4.abstract)ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨æ˜¾å¾®é•œå›¾åƒæˆ–è½¬å½•ç»„å­¦æ¥æ›´å¥½åœ°ä¸ºæ¨¡å‹æä¾›åˆ†å­çš„ç”Ÿç‰©å­¦ç‰¹å¾ä¿¡æ¯ï¼Œä¾‹å¦‚[TranSiGen](https://www.biorxiv.org/content/10.1101/2023.11.12.566777v1.full)ã€‚è¿˜æœ‰ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥åµŒå…¥å…³äºä»»åŠ¡çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚[TwinBooster](https://arxiv.org/pdf/2401.04478.pdf)ã€‚ç”šè‡³æ›´å¥½çš„æ˜¯ï¼Œå°†è¿™äº›æ–¹æ³•ç»“åˆèµ·æ¥ğŸ¤¯ï¼Œä½†è¿™å¯èƒ½éœ€è¦å‡ å¹´æ—¶é—´ã€‚å¯¹äºæ›´å¹¿æ³›çš„ç¤¾åŒºè€Œè¨€ï¼Œä¸»è¦é—®é¢˜ä¼¼ä¹æ˜¯ç¼ºä¹å¤§é‡é«˜è´¨é‡å’Œæ ‡å‡†åŒ–çš„æ•°æ®ï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œè¿™å¯¹Valenceæ¥è¯´å¹¶ä¸æ˜¯é—®é¢˜ã€‚
- en: '**ğŸ”¬ Relating biological knowledge and observables.** Humans have been trying
    to map biology for a long time, building relational maps for genes ğŸ§¬, protein-protein
    interactions ğŸ”„, metabolic pathways ğŸ”€, etc. I invite you to read this [review of
    knowledge graphs for drug discovery](https://academic.oup.com/bib/article/23/6/bbac404/6712301).
    But all this knowledge often sits unused and ignored by the ML community. I feel
    that this is an area where GNNs for knowledge graphs could prove very useful,
    especially in 2024, and it could provide another modality for the ğŸ•¸ï¸ point above.
    Considering that human knowledge is incomplete, we can instead recover relational
    maps from foundational models. This is the route taken by [Phenom1](https://arxiv.org/abs/2309.16064)
    when trying to recall known genetic relationships. However, having to deal with
    various knowledge databases is an extremely complex task that we canâ€™t expect
    most ML scientists to be able to tackle alone. But with the help of artificial
    assistants like [LOWE](https://www.valencelabs.com/lowe), this can be done in
    a matter of seconds.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ”¬ å…³è”ç”Ÿç‰©å­¦çŸ¥è¯†å’Œå¯è§‚å¯Ÿç°è±¡ã€‚** äººç±»ä¸€ç›´åœ¨åŠªåŠ›ç»˜åˆ¶ç”Ÿç‰©å­¦çš„å›¾è°±ï¼Œæ„å»ºåŸºå› ğŸ§¬ã€è›‹ç™½è´¨-è›‹ç™½è´¨ç›¸äº’ä½œç”¨ğŸ”„ã€ä»£è°¢é€šè·¯ğŸ”€ç­‰çš„å…³ç³»å›¾ã€‚æˆ‘é‚€è¯·ä½ é˜…è¯»è¿™ç¯‡å…³äº[è¯ç‰©å‘ç°ä¸­çš„çŸ¥è¯†å›¾è°±ç»¼è¿°](https://academic.oup.com/bib/article/23/6/bbac404/6712301)ã€‚ç„¶è€Œï¼Œè¿™äº›çŸ¥è¯†å¸¸å¸¸è¢«æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç¤¾åŒºå¿½è§†ï¼Œæœªè¢«å……åˆ†åˆ©ç”¨ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªçŸ¥è¯†å›¾è°±é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨2024å¹´ï¼Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰å¯ä»¥å‘æŒ¥å·¨å¤§ä½œç”¨çš„åœ°æ–¹ï¼Œå®ƒä¹Ÿå¯èƒ½ä¸ºä¸Šè¿°ğŸ•¸ï¸ç‚¹æä¾›å¦ä¸€ç§æ–¹å¼ã€‚è€ƒè™‘åˆ°äººç±»çŸ¥è¯†çš„ä¸å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŸºç¡€æ¨¡å‹æ¢å¤å…³ç³»å›¾è°±ã€‚è¿™ä¹Ÿæ˜¯[Phenom1](https://arxiv.org/abs/2309.16064)åœ¨å°è¯•å›æº¯å·²çŸ¥é—ä¼ å…³ç³»æ—¶æ‰€é‡‡å–çš„è·¯å¾„ã€‚ç„¶è€Œï¼Œå¤„ç†å„ç§çŸ¥è¯†æ•°æ®åº“æ˜¯ä¸€é¡¹æå…¶å¤æ‚çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¸èƒ½æŒ‡æœ›å¤§å¤šæ•°æœºå™¨å­¦ä¹ ç§‘å­¦å®¶èƒ½å¤Ÿå•ç‹¬åº”å¯¹ã€‚ä½†æ˜¯å€ŸåŠ©äººå·¥åŠ©æ‰‹ï¼Œå¦‚[LOWE](https://www.valencelabs.com/lowe)ï¼Œè¿™ä¸€ä»»åŠ¡å¯ä»¥åœ¨å‡ ç§’é’Ÿå†…å®Œæˆã€‚'
- en: '**ğŸ† Benchmarks, benchmarks, benchmarks.** I canâ€™t repeat the word ***benchmark***
    enough. Alas, benchmarks will stay the unloved kid on the ML block ğŸ«¥. But if the
    word benchmark is uncool, its cousin ***competition*** is way cooler ğŸ˜! Just as
    the [OGB-LSC](https://ogb.stanford.edu/docs/lsc/) competition and [Open Catalyst](https://opencatalystproject.org/challenge.html)
    challenge played a major role for the GNN community, it is now time for a new
    series of competitions ğŸ¥‡. We even got the [TGB (Temporal graph benchmark)](https://tgb.complexdatalab.com/)
    recently. If you were at NeurIPSâ€™23, then you probably heard of Polaris coming
    up early 2024 âœ¨. Polaris is a consortium of multiple pharma and academic groups
    trying to improve the quality of available molecular benchmarks to better represent
    real drug discovery. Perhaps weâ€™ll even see a benchmark suitable for molecular
    graph generation instead of optimizing QED and cLogP, but I wouldnâ€™t hold my breath,
    I have been waiting for years. What kind of new, crazy competition will light
    up the GDL community this year ğŸ¤”?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ† åŸºå‡†æµ‹è¯•ï¼ŒåŸºå‡†æµ‹è¯•ï¼ŒåŸºå‡†æµ‹è¯•ã€‚** æˆ‘æ— æ³•é‡å¤***åŸºå‡†æµ‹è¯•***è¿™ä¸ªè¯è¶³å¤Ÿå¤šã€‚å”‰ï¼ŒåŸºå‡†æµ‹è¯•ä¾ç„¶ä¼šæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸä¸å—å® çš„å­˜åœ¨ğŸ«¥ã€‚ä½†å¦‚æœâ€œåŸºå‡†æµ‹è¯•â€è¿™ä¸ªè¯ä¸é…·ï¼Œå®ƒçš„è¡¨äº²***ç«èµ›***å°±è¦é…·å¾—å¤šäº†ğŸ˜ï¼æ­£å¦‚[OGB-LSC](https://ogb.stanford.edu/docs/lsc/)ç«èµ›å’Œ[Open
    Catalyst](https://opencatalystproject.org/challenge.html)æŒ‘æˆ˜å¯¹GNNç¤¾åŒºçš„é‡å¤§ä½œç”¨ä¸€æ ·ï¼Œç°åœ¨æ˜¯æ—¶å€™è¿æ¥ä¸€ç³»åˆ—æ–°çš„ç«èµ›ğŸ¥‡ã€‚æˆ‘ä»¬ç”šè‡³æœ€è¿‘è¿æ¥äº†[TGBï¼ˆæ—¶åºå›¾åŸºå‡†æµ‹è¯•ï¼‰](https://tgb.complexdatalab.com/)ã€‚å¦‚æœä½ å‚åŠ äº†NeurIPS''23ï¼Œå¯èƒ½å°±å¬è¯´äº†Polariså°†åœ¨2024å¹´åˆç™»åœºâœ¨ã€‚Polarisæ˜¯ä¸€ä¸ªç”±å¤šä¸ªåˆ¶è¯å…¬å¸å’Œå­¦æœ¯å›¢ä½“ç»„æˆçš„è”ç›Ÿï¼Œæ—¨åœ¨æé«˜ç°æœ‰åˆ†å­åŸºå‡†æµ‹è¯•çš„è´¨é‡ï¼Œä»¥æ›´å¥½åœ°ä»£è¡¨çœŸå®çš„è¯ç‰©å‘ç°è¿‡ç¨‹ã€‚ä¹Ÿè®¸æˆ‘ä»¬ä¼šçœ‹åˆ°é€‚ç”¨äºåˆ†å­å›¾ç”Ÿæˆçš„åŸºå‡†æµ‹è¯•ï¼Œè€Œä¸æ˜¯ä¼˜åŒ–QEDå’ŒcLogPï¼Œä½†æˆ‘ä¸ä¼šæŠ±å¤ªå¤§å¸Œæœ›ï¼Œæ¯•ç«Ÿæˆ‘å·²ç»ç­‰äº†å¤šå¹´ã€‚æ˜¯ä»€ä¹ˆæ ·çš„æ–°å¥‡ç«èµ›å°†ç‚¹äº®ä»Šå¹´çš„GDLç¤¾åŒºå‘¢ğŸ¤”ï¼Ÿ'
- en: Systems Biology
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç³»ç»Ÿç”Ÿç‰©å­¦
- en: '***Kexin Huang (Stanford)***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***é»„å…‹é‘«ï¼ˆæ–¯å¦ç¦å¤§å­¦ï¼‰***'
- en: Biology is an interconnected, multi-scale, and multi-modal system. Effective
    modeling of this system can not only unravel fundamental biological questions
    but also significantly impact therapeutic discovery. The most natural data format
    for encapsulating this system is a relational database or a heterogeneous graph.
    This graph stores data from decades of wet lab experiments across various biological
    modalities, scaling up to billions of data points.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿç‰©å­¦æ˜¯ä¸€ä¸ªç›¸äº’è¿æ¥ã€å¤šå°ºåº¦å’Œå¤šæ¨¡æ€çš„ç³»ç»Ÿã€‚æœ‰æ•ˆçš„å»ºæ¨¡ä¸ä»…å¯ä»¥æ­ç¤ºåŸºç¡€çš„ç”Ÿç‰©å­¦é—®é¢˜ï¼Œè¿˜å¯ä»¥å¯¹æ²»ç–—å‘ç°äº§ç”Ÿé‡å¤§å½±å“ã€‚æœ€è‡ªç„¶çš„å°è£…è¿™ç§ç³»ç»Ÿçš„æ•°æ®æ ¼å¼æ˜¯å…³ç³»å‹æ•°æ®åº“æˆ–å¼‚æ„å›¾ã€‚è¿™ç§å›¾å­˜å‚¨äº†å‡ åå¹´æ¥åœ¨å„ç§ç”Ÿç‰©å­¦é¢†åŸŸè¿›è¡Œçš„æ¹¿å®éªŒæ•°æ®ï¼Œæ•°æ®é‡å¯è¾¾æ•°åäº¿æ¡ã€‚
- en: â€œIn 2023, we witnessed a range of innovative applications using GNNs on these
    biological system graphs. These applications have unlocked new biomedical capabilities
    and answered critical biological queries.â€ â€” Kexin Huang (Stanford)
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨2023å¹´ï¼Œæˆ‘ä»¬è§è¯äº†è®¸å¤šä½¿ç”¨GNNsçš„åˆ›æ–°åº”ç”¨ï¼Œåº”ç”¨äºè¿™äº›ç”Ÿç‰©ç³»ç»Ÿå›¾è°±ã€‚è¿™äº›åº”ç”¨è§£é”äº†æ–°çš„ç”Ÿç‰©åŒ»å­¦èƒ½åŠ›ï¼Œå¹¶å›ç­”äº†å…³é”®çš„ç”Ÿç‰©å­¦é—®é¢˜ã€‚â€ â€” é»„å…‹é‘«ï¼ˆæ–¯å¦ç¦å¤§å­¦ï¼‰
- en: 1ï¸âƒ£ One particularly exciting field is **perturbative biology**. Understanding
    the outcomes of perturbations can lead to advancements in cell reprogramming,
    target discovery, and synthetic lethality, among others. In 2023, [GEARS](https://www.nature.com/articles/s41587-023-01905-6)
    applies GNN to gene perturbational relational graphs and it predicts outcomes
    of genetic perturbations that have not been observed before.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ ä¸€ä¸ªç‰¹åˆ«ä»¤äººå…´å¥‹çš„é¢†åŸŸæ˜¯**å¾®æ‰°ç”Ÿç‰©å­¦**ã€‚ç†è§£å¾®æ‰°çš„ç»“æœå¯ä»¥æ¨åŠ¨ç»†èƒé‡ç¼–ç¨‹ã€é¶ç‚¹å‘ç°å’Œåˆæˆè‡´æ­»ç­‰æ–¹é¢çš„è¿›å±•ã€‚åœ¨2023å¹´ï¼Œ[GEARS](https://www.nature.com/articles/s41587-023-01905-6)å°†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰åº”ç”¨äºåŸºå› æ‰°åŠ¨å…³ç³»å›¾ï¼Œèƒ½å¤Ÿé¢„æµ‹ä¹‹å‰æœªæ›¾è§‚å¯Ÿåˆ°çš„åŸºå› æ‰°åŠ¨ç»“æœã€‚
- en: 2ï¸âƒ£ Another cool application concerns **protein representation**. While current
    protein representations are fixed and static, we recognize that the same protein
    can exhibit different functions in varying cellular contexts. [PINNACLE](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)
    uses GNN on protein interaction networks to contextualize protein embeddings.
    This approach has shown to enhance 3D structure-based protein representations
    and outperform existing context-free models in identifying therapeutic targets.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å¦ä¸€ä¸ªé…·ç‚«çš„åº”ç”¨æ¶‰åŠ**è›‹ç™½è´¨è¡¨ç¤º**ã€‚å½“å‰çš„è›‹ç™½è´¨è¡¨ç¤ºæ˜¯å›ºå®šä¸”é™æ€çš„ï¼Œä½†æˆ‘ä»¬è®¤è¯†åˆ°åŒä¸€è›‹ç™½è´¨åœ¨ä¸åŒçš„ç»†èƒç¯å¢ƒä¸­å¯èƒ½è¡¨ç°å‡ºä¸åŒçš„åŠŸèƒ½ã€‚[PINNACLE](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)åœ¨è›‹ç™½è´¨ç›¸äº’ä½œç”¨ç½‘ç»œä¸Šä½¿ç”¨GNNæ¥å¯¹è›‹ç™½è´¨åµŒå…¥è¿›è¡Œæƒ…å¢ƒåŒ–å¤„ç†ã€‚è¯¥æ–¹æ³•å·²è¢«è¯æ˜èƒ½å¤Ÿå¢å¼ºåŸºäº3Dç»“æ„çš„è›‹ç™½è´¨è¡¨ç¤ºï¼Œå¹¶åœ¨è¯†åˆ«æ²»ç–—é¶ç‚¹æ–¹é¢ä¼˜äºç°æœ‰çš„æ— ä¸Šä¸‹æ–‡æ¨¡å‹ã€‚
- en: '![](../Images/2a530ef72f314922fe3608ed1c120dd1.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a530ef72f314922fe3608ed1c120dd1.png)'
- en: 'PINNACLE has protein-, cell type-, and tissue-level attention mechanisms that
    enable the algorithm to generate contextualized representations of proteins, cell
    types, and tissues in a single unified embedding space. Source: [Li et al](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PINNACLEæ‹¥æœ‰è›‹ç™½è´¨ã€ç»†èƒç±»å‹å’Œç»„ç»‡å±‚æ¬¡çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿å¾—è¯¥ç®—æ³•èƒ½å¤Ÿåœ¨ä¸€ä¸ªç»Ÿä¸€çš„åµŒå…¥ç©ºé—´ä¸­ç”Ÿæˆè›‹ç™½è´¨ã€ç»†èƒç±»å‹å’Œç»„ç»‡çš„æƒ…å¢ƒåŒ–è¡¨ç¤ºã€‚æ¥æºï¼š[Li et
    al](https://www.biorxiv.org/content/10.1101/2023.07.18.549602v1)
- en: 3ï¸âƒ£ GNNs also have shown a vital role in **diagnosing rare diseases**. [SHEPHERD](https://www.medrxiv.org/content/10.1101/2022.12.07.22283238v1)
    utilizes GNN over massive knowledge graph to encode extensive biological knowledge
    into the ML model and is shown to facilitate causal gene discovery, identify â€˜patients-like-meâ€™
    with similar genes or diseases, and provide interpretable insights into novel
    disease manifestations.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ GNNåœ¨**è¯Šæ–­ç½•è§ç–¾ç—…**æ–¹é¢ä¹Ÿå‘æŒ¥äº†é‡è¦ä½œç”¨ã€‚[SHEPHERD](https://www.medrxiv.org/content/10.1101/2022.12.07.22283238v1)åˆ©ç”¨GNNåœ¨åºå¤§çš„çŸ¥è¯†å›¾è°±ä¸Šå¯¹ç”Ÿç‰©å­¦çŸ¥è¯†è¿›è¡Œç¼–ç ï¼Œå¹¶å·²è¢«è¯æ˜æœ‰åŠ©äºå› æœåŸºå› å‘ç°ã€è¯†åˆ«å…·æœ‰ç›¸ä¼¼åŸºå› æˆ–ç–¾ç—…çš„â€˜ç±»ä¼¼æ‚£è€…â€™ï¼Œå¹¶ä¸ºæ–°å‹ç–¾ç—…è¡¨ç°æä¾›å¯è§£é‡Šçš„è§è§£ã€‚
- en: â¡ï¸ Moving beyond predictions, understanding the underlying mechanisms of biological
    phenomena is crucial. **Graph XAI** applied to system graphs is a natural fit
    for identifying mechanistic pathways. [TxGNN](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2),
    for example, grounds drug-disease relation predictions in the biological system
    graph, generating multi-hop interpretable paths. These paths rationalize the potential
    of a drug in treating a specific disease. TxGNN designed [visualizations](http://txgnn.org/)
    for these interpretations and conducted user studies, proving their decision-making
    effectiveness for clinicians and biomedical scientists.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ è¶…è¶Šé¢„æµ‹ï¼Œç†è§£ç”Ÿç‰©ç°è±¡èƒŒåçš„æœºåˆ¶è‡³å…³é‡è¦ã€‚**å›¾ç¥ç»ç½‘ç»œè§£é‡Šæ€§AIï¼ˆGraph XAIï¼‰**åº”ç”¨äºç³»ç»Ÿå›¾æ˜¯è¯†åˆ«æœºåˆ¶è·¯å¾„çš„è‡ªç„¶é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œ[TxGNN](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)å°†è¯ç‰©-ç–¾ç—…å…³ç³»é¢„æµ‹ä¸ç”Ÿç‰©ç³»ç»Ÿå›¾ç»“åˆï¼Œç”Ÿæˆå¤šè·³çš„å¯è§£é‡Šè·¯å¾„ã€‚è¿™äº›è·¯å¾„é˜æ˜äº†è¯ç‰©æ²»ç–—ç‰¹å®šç–¾ç—…çš„æ½œåŠ›ã€‚TxGNNä¸ºè¿™äº›è§£é‡Šè®¾è®¡äº†[å¯è§†åŒ–å·¥å…·](http://txgnn.org/)ï¼Œå¹¶è¿›è¡Œç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜å…¶åœ¨ä¸´åºŠåŒ»ç”Ÿå’Œç”Ÿç‰©åŒ»å­¦ç§‘å­¦å®¶å†³ç­–ä¸­çš„æœ‰æ•ˆæ€§ã€‚
- en: '![](../Images/e3f51bc7c5fb00a693d400b266dff34c.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f51bc7c5fb00a693d400b266dff34c.png)'
- en: 'A web-based graphical user interface to support clinicians and scientists in
    exploring and analyzing the predictions and explanations generated by TxGNN. The
    â€˜Control Panelâ€˜ allows users to select the disease of interest and view the top-ranked
    TXGNN predictions for the query disease. The â€˜edge thresholdâ€˜ module enables users
    to modify the sparsity of the explanation and thereby control the density of the
    multi-hop paths displayed. The â€˜Drug Embeddingâ€˜ panel allows users to compare
    the position of a selected drug relative to the entire repurposing candidate library.
    The â€˜Path Explanationâ€˜ panel displays the biological relations that have been
    identified as crucial for TXGNNâ€™s predictions regarding therapeutic use. Source:
    [Huang, Chandar, et al](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªåŸºäºç½‘ç»œçš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼Œç”¨äºæ”¯æŒä¸´åºŠåŒ»ç”Ÿå’Œç§‘å­¦å®¶æ¢ç´¢å’Œåˆ†æç”±TxGNNç”Ÿæˆçš„é¢„æµ‹å’Œè§£é‡Šã€‚â€˜æ§åˆ¶é¢æ¿â€™å…è®¸ç”¨æˆ·é€‰æ‹©æ„Ÿå…´è¶£çš„ç–¾ç—…ï¼Œå¹¶æŸ¥çœ‹è¯¥ç–¾ç—…çš„é¡¶çº§TXGNNé¢„æµ‹ç»“æœã€‚â€˜è¾¹ç¼˜é˜ˆå€¼â€™æ¨¡å—ä½¿ç”¨æˆ·èƒ½å¤Ÿè°ƒæ•´è§£é‡Šçš„ç¨€ç–æ€§ï¼Œä»è€Œæ§åˆ¶æ˜¾ç¤ºçš„å¤šè·³è·¯å¾„çš„å¯†åº¦ã€‚â€˜è¯ç‰©åµŒå…¥â€™é¢æ¿å…è®¸ç”¨æˆ·æ¯”è¾ƒæ‰€é€‰è¯ç‰©ç›¸å¯¹äºæ•´ä¸ªè¯ç‰©é‡å®šä½å€™é€‰åº“çš„ä½ç½®ã€‚â€˜è·¯å¾„è§£é‡Šâ€™é¢æ¿å±•ç¤ºäº†è¢«è®¤ä¸ºå¯¹TXGNNé¢„æµ‹ç–—æ•ˆè‡³å…³é‡è¦çš„ç”Ÿç‰©å­¦å…³ç³»ã€‚æ¥æºï¼š[é»„ã€Chandarç­‰](https://www.medrxiv.org/content/10.1101/2023.03.19.23287458v2)
- en: â¡ï¸ Foundation models in biology have predominantly been unimodal (focused on
    proteins, molecules, diseases, etc.), primarily due to the scarcity of paired
    data. **Bridging across modalities** to answer multi-modal queries is an exciting
    frontier. For example, [BioBridge](https://openreview.net/forum?id=jJCeMiwHdH)
    leverages biological knowledge graphs to learn transformations across unimodal
    foundation models, enabling multi-modal behaviors.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ç”Ÿç‰©å­¦ä¸­çš„åŸºç¡€æ¨¡å‹ä¸»è¦æ˜¯å•æ¨¡æ€çš„ï¼ˆä¸“æ³¨äºè›‹ç™½è´¨ã€åˆ†å­ã€ç–¾ç—…ç­‰ï¼‰ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºé…å¯¹æ•°æ®çš„ç¨€ç¼ºæ€§ã€‚**è·¨æ¨¡æ€æ¡¥æ¥**ä»¥å›ç­”å¤šæ¨¡æ€æŸ¥è¯¢æ˜¯ä¸€ä¸ªä»¤äººå…´å¥‹çš„å‰æ²¿ã€‚ä¾‹å¦‚ï¼Œ[BioBridge](https://openreview.net/forum?id=jJCeMiwHdH)åˆ©ç”¨ç”Ÿç‰©å­¦çŸ¥è¯†å›¾è°±åœ¨å•æ¨¡æ€åŸºç¡€æ¨¡å‹ä¹‹é—´å­¦ä¹ å˜æ¢ï¼Œä»è€Œå®ç°å¤šæ¨¡æ€è¡Œä¸ºã€‚
- en: ğŸ”® GNNs applied to system graphs have the potential to (1) encode vast biomedical
    knowledge, (2) bridge biological modalities, (3) provide mechanistic insights,
    and (4) contextualize biological entities. We anticipate even more groundbreaking
    applications of GNN in biology in 2024, addressing some of the most pressing questions
    in the field.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® GNNåº”ç”¨äºç³»ç»Ÿå›¾è°±å…·æœ‰ä»¥ä¸‹æ½œåŠ›ï¼šï¼ˆ1ï¼‰ç¼–ç å¹¿æ³›çš„ç”Ÿç‰©åŒ»å­¦çŸ¥è¯†ï¼Œï¼ˆ2ï¼‰æ¡¥æ¥ç”Ÿç‰©å­¦æ¨¡æ€ï¼Œï¼ˆ3ï¼‰æä¾›æœºåˆ¶æ€§æ´å¯Ÿï¼Œï¼ˆ4ï¼‰ä¸ºç”Ÿç‰©å®ä½“æä¾›èƒŒæ™¯ã€‚æˆ‘ä»¬é¢„è®¡2024å¹´GNNåœ¨ç”Ÿç‰©å­¦ä¸­çš„åº”ç”¨å°†å¸¦æ¥æ›´å¤šçªç ´æ€§çš„è¿›å±•ï¼Œè§£å†³è¯¥é¢†åŸŸä¸€äº›æœ€ç´§è¿«çš„é—®é¢˜ã€‚
- en: '**Predictions from the 2023 post**'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2023å¹´é¢„æµ‹**'
- en: (1) performance improvements of diffusion models such as faster sampling and
    more efficient solvers;
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆ1ï¼‰æ‰©æ•£æ¨¡å‹çš„æ€§èƒ½æå‡ï¼Œå¦‚æ›´å¿«çš„é‡‡æ ·å’Œæ›´é«˜æ•ˆçš„æ±‚è§£å™¨ï¼›
- en: âœ… yes, with flow matching
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æ˜¯çš„ï¼Œä½¿ç”¨æµåŒ¹é…
- en: (2) more powerful conditional protein generation models;
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆ2ï¼‰æ›´å¼ºå¤§çš„æ¡ä»¶è›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹ï¼›
- en: âŒ Chroma and RFDiffusion are still on top
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ Chromaå’ŒRFDiffusionä»ç„¶é¢†å…ˆ
- en: (3) more successful applications of [Generative Flow Networks](https://arxiv.org/abs/2111.09266)
    to molecules and proteins
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆ3ï¼‰[ç”Ÿæˆæµç½‘ç»œ](https://arxiv.org/abs/2111.09266)åœ¨åˆ†å­å’Œè›‹ç™½è´¨ä¸­çš„æ›´å¤šæˆåŠŸåº”ç”¨ï¼›
- en: âŒ yet to be seen
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ å°šæœªç¡®å®š
- en: Materials Science (Crystals)
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ææ–™ç§‘å­¦ï¼ˆæ™¶ä½“ï¼‰
- en: '*Michael Galkin (Intel) and Santiago Miret (Intel)*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”Â·é«˜å°”é‡‘ï¼ˆè‹±ç‰¹å°”ï¼‰å’Œåœ£åœ°äºšå“¥Â·ç±³é›·ç‰¹ï¼ˆè‹±ç‰¹å°”ï¼‰*'
- en: 'In 2023, for a short period, all scientific news were talking only about [LK-99](https://en.wikipedia.org/wiki/LK-99)
    â€” a supposed room-temperature superconductor created by a Korean team (spoiler:
    [it did not work as of now](https://www.nature.com/articles/d41586-023-02585-7)).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨2023å¹´ï¼ŒçŸ­çŸ­ä¸€æ®µæ—¶é—´å†…ï¼Œæ‰€æœ‰ç§‘å­¦æ–°é—»éƒ½åœ¨è®¨è®º[LK-99](https://en.wikipedia.org/wiki/LK-99)â€”â€”ä¸€ä¸ªç”±éŸ©å›½å›¢é˜Ÿåˆ›é€ çš„å‡è®¾å¸¸æ¸©è¶…å¯¼ä½“ï¼ˆå‰§é€ï¼š[æˆªè‡³ç›®å‰å®ƒå¹¶æœªæˆåŠŸ](https://www.nature.com/articles/d41586-023-02585-7)ï¼‰ã€‚
- en: This highlights the huge potential ML has in material science, where perhaps
    the biggest progress of the year has happened â€” we can now say that materials
    science and materials discovery are first-class citizens in the Geometric DL landscape.
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™çªæ˜¾äº†æœºå™¨å­¦ä¹ åœ¨ææ–™ç§‘å­¦ä¸­çš„å·¨å¤§æ½œåŠ›ï¼Œæˆ–è®¸ä»Šå¹´æœ€å¤§çš„è¿›å±•å°±å‘ç”Ÿåœ¨è¿™é‡Œâ€”â€”æˆ‘ä»¬ç°åœ¨å¯ä»¥è¯´ï¼Œææ–™ç§‘å­¦å’Œææ–™å‘ç°å·²ç»æˆä¸ºå‡ ä½•æ·±åº¦å­¦ä¹ ï¼ˆGeometric DLï¼‰é¢†åŸŸçš„æ ¸å¿ƒå†…å®¹ã€‚
- en: ğŸ’¡The advances of Geometric DL applied to materials science and discovery saw
    significant advances across new modelling methods, creation of new benchmarks
    and datasets, automated design with generative methods, and identifying new research
    questions based on those advances.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡åº”ç”¨äºææ–™ç§‘å­¦å’Œå‘ç°çš„å‡ ä½•æ·±åº¦å­¦ä¹ å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œæ¶µç›–äº†æ–°çš„å»ºæ¨¡æ–¹æ³•ã€æ–°åŸºå‡†å’Œæ•°æ®é›†çš„åˆ›å»ºã€ç”Ÿæˆæ–¹æ³•çš„è‡ªåŠ¨åŒ–è®¾è®¡ï¼Œä»¥åŠåŸºäºè¿™äº›è¿›å±•è¯†åˆ«å‡ºæ–°çš„ç ”ç©¶é—®é¢˜ã€‚
- en: 1ï¸âƒ£ Applications of geometric models as evaluation tools in automated discovery
    workflows. The [Open MatSci ML Toolkit](https://github.com/IntelLabs/matsciml)
    consolidated all open-sourced crystal structures datasets leading to 1.5 million
    data points for ground-state structure calculations that are now easily available
    for model development. The [authorsâ€™ initial results](https://arxiv.org/abs/2309.05934)
    seem to indicate that merging datasets seems to improve performance if done attentively.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ å‡ ä½•æ¨¡å‹ä½œä¸ºè‡ªåŠ¨åŒ–å‘ç°å·¥ä½œæµä¸­è¯„ä¼°å·¥å…·çš„åº”ç”¨ã€‚[Open MatSci ML Toolkit](https://github.com/IntelLabs/matsciml)æ•´åˆäº†æ‰€æœ‰å¼€æºçš„æ™¶ä½“ç»“æ„æ•°æ®é›†ï¼Œæä¾›äº†150ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œç”¨äºåŸºæ€ç»“æ„è®¡ç®—ï¼Œè¿™äº›æ•°æ®ç°åœ¨å¯ä»¥è½»æ¾ç”¨äºæ¨¡å‹å¼€å‘ã€‚[ä½œè€…çš„åˆæ­¥ç»“æœ](https://arxiv.org/abs/2309.05934)ä¼¼ä¹è¡¨æ˜ï¼Œåˆå¹¶æ•°æ®é›†å¦‚æœå¤„ç†å¾—å½“ï¼Œä¼šæé«˜æ€§èƒ½ã€‚
- en: 2ï¸âƒ£ [MatBench Discovery](https://arxiv.org/abs/2308.14920) is another good example
    of this integration of geometric models as an evaluation tool for crystal stability,
    which tests modelsâ€™ predictions of the **energy above hull** for various crystal
    structures. The energy above hull is the most reliable approximation of crystal
    structure stability and also represents an improvement in metrics compared to
    formation energy or raw energy prediction which have practical limitations as
    stability metrics.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ [MatBench Discovery](https://arxiv.org/abs/2308.14920)æ˜¯å¦ä¸€ä¸ªå¾ˆå¥½ä¾‹å­ï¼Œå±•ç¤ºäº†å‡ ä½•æ¨¡å‹ä½œä¸ºæ™¶ä½“ç¨³å®šæ€§è¯„ä¼°å·¥å…·çš„åº”ç”¨ï¼Œå®ƒæµ‹è¯•äº†æ¨¡å‹å¯¹å„ç§æ™¶ä½“ç»“æ„çš„**èƒ½é‡é«˜äºå¤–å£³**çš„é¢„æµ‹ã€‚èƒ½é‡é«˜äºå¤–å£³æ˜¯æ™¶ä½“ç»“æ„ç¨³å®šæ€§çš„æœ€å¯é è¿‘ä¼¼ï¼Œä¹Ÿä»£è¡¨äº†æ¯”å½¢æˆèƒ½æˆ–åŸå§‹èƒ½é‡é¢„æµ‹æ›´æœ‰å®é™…æ„ä¹‰çš„åº¦é‡æ–¹æ³•ï¼Œå› ä¸ºåè€…ä½œä¸ºç¨³å®šæ€§åº¦é‡å­˜åœ¨å®é™…é™åˆ¶ã€‚
- en: '![](../Images/490c1c14cae95438a36595145e08625f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/490c1c14cae95438a36595145e08625f.png)'
- en: 'Universal potentials are more reliable classifiers because they exit the red
    triangle earliest. These lines show the rolling MAE on the WBM test set as the
    energy to the convex hull of the MP training set is varied, lower is better. The
    red-highlighted â€™triangle of perilâ€™ shows where the models are most likely to
    misclassify structures. As long as a modelâ€™s rolling MAE remains inside the triangle,
    its mean error is larger than the distance to the convex hull. If the modelâ€™s
    error for a given prediction happens to point towards the stability threshold
    at 0 eV from the hull (the plotâ€™s center), its average error will change the stability
    classification of a material from true positive/negative to false negative/positive.
    The width of the â€™rolling windowâ€™ box indicates the width over which errors hull
    distance prediction errors were averaged. Source: [Riebesell et al](https://arxiv.org/abs/2308.14920)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ™®éåŠ¿èƒ½æ˜¯æ›´å¯é çš„åˆ†ç±»å™¨ï¼Œå› ä¸ºå®ƒä»¬æœ€æ—©ä¼šé€€å‡ºçº¢è‰²ä¸‰è§’åŒºåŸŸã€‚è¿™äº›çº¿æ˜¾ç¤ºäº†åœ¨WBMæµ‹è¯•é›†ä¸Šçš„æ»šåŠ¨å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼Œéšç€MPè®­ç»ƒé›†çš„èƒ½é‡ä¸å‡¸åŒ…çš„è·ç¦»å˜åŒ–ï¼Œæ•°å€¼è¶Šä½è¶Šå¥½ã€‚çº¢è‰²çªå‡ºæ˜¾ç¤ºçš„â€œå±é™©ä¸‰è§’åŒºâ€è¡¨ç¤ºæ¨¡å‹æœ€æœ‰å¯èƒ½é”™è¯¯åˆ†ç±»ç»“æ„ã€‚åªè¦æ¨¡å‹çš„æ»šåŠ¨MAEä¿æŒåœ¨ä¸‰è§’åŒºå†…ï¼Œå…¶å¹³å‡è¯¯å·®å¤§äºåˆ°å‡¸åŒ…çš„è·ç¦»ã€‚å¦‚æœæ¨¡å‹å¯¹æŸä¸€é¢„æµ‹çš„è¯¯å·®æ°å¥½æŒ‡å‘ä½äºå‡¸åŒ…0
    eVç¨³å®šæ€§é˜ˆå€¼å¤„ï¼ˆå›¾è¡¨çš„ä¸­å¿ƒï¼‰ï¼Œåˆ™å…¶å¹³å‡è¯¯å·®å°†æ”¹å˜ææ–™çš„ç¨³å®šæ€§åˆ†ç±»ï¼Œä»çœŸæ­£çš„æ­£/è´Ÿç±»å˜ä¸ºå‡è´Ÿ/å‡æ­£ç±»ã€‚â€™æ»šåŠ¨çª—å£â€™æ¡†çš„å®½åº¦è¡¨ç¤ºåœ¨å…¶å†…å¹³å‡çš„è¯¯å·®å¯¹å‡¸åŒ…è·ç¦»é¢„æµ‹çš„å®½åº¦ã€‚æ¥æºï¼š[Riebesellç­‰äºº](https://arxiv.org/abs/2308.14920)
- en: 3ï¸âƒ£ In terms of new geometric models for crystal structure prediction, **Crystal
    Hamiltonian Graph neural network** ([CHGNet](https://chgnet.lbl.gov/), [Deng et
    al](https://arxiv.org/abs/2302.14231)) is a new GNN trained on static and relaxation
    trajectories of Materials Project that shows quite competitive performance compared
    to prior methods. The development of CHGNet suggests that finding better training
    objectives will be as (if not more) important than the development of new methods
    as the intersection of materials science and geometric deep learning continues
    to grow.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ åœ¨æ™¶ä½“ç»“æ„é¢„æµ‹çš„æ–°å‡ ä½•æ¨¡å‹æ–¹é¢ï¼Œ**æ™¶ä½“å“ˆå¯†é¡¿å›¾ç¥ç»ç½‘ç»œ**ï¼ˆ[CHGNet](https://chgnet.lbl.gov/)ï¼Œ[é‚“ç­‰äºº](https://arxiv.org/abs/2302.14231)ï¼‰æ˜¯ä¸€ä¸ªæ–°å‹çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œå®ƒåŸºäºææ–™é¡¹ç›®çš„é™æ€å’Œæ¾å¼›è½¨è¿¹è¿›è¡Œè®­ç»ƒï¼Œä¸å…ˆå‰çš„æ–¹æ³•ç›¸æ¯”ï¼Œè¡¨ç°å‡ºç›¸å½“æœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚CHGNetçš„å‘å±•è¡¨æ˜ï¼Œæ‰¾åˆ°æ›´å¥½çš„è®­ç»ƒç›®æ ‡å°†ä¸å¼€å‘æ–°æ–¹æ³•ä¸€æ ·ï¼ˆå¦‚æœä¸æ˜¯æ›´é‡è¦çš„è¯ï¼‰å˜å¾—è‡³å…³é‡è¦ï¼Œå› ä¸ºææ–™ç§‘å­¦ä¸å‡ ä½•æ·±åº¦å­¦ä¹ çš„äº¤é›†æ­£åœ¨ä¸æ–­å¢é•¿ã€‚
- en: 'ğŸ”¥ The other proof points of the further integration of Geometric DL and materials
    discovery are several massive works by big labs focused on crystal structure discovery
    with generative methods:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ å‡ ä½•æ·±åº¦å­¦ä¹ ä¸ææ–™å‘ç°è¿›ä¸€æ­¥æ•´åˆçš„å…¶ä»–è¯æ˜ç‚¹æ˜¯å‡ å®¶å¤§å®éªŒå®¤åœ¨æ™¶ä½“ç»“æ„å‘ç°ä¸­é‡‡ç”¨ç”Ÿæˆæ–¹æ³•çš„å‡ é¡¹åºå¤§å·¥ä½œï¼š
- en: 1ï¸âƒ£ Google DeepMind released [**GNoME**](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/)
    (Graph Networks for Materials Science by [Merchant et al](https://www.nature.com/articles/s41586-023-06735-9))
    as a successful example of an active learning pipeline for discovering new materials,
    and [UniMat](https://unified-materials.github.io/unimat/) as an *ab initio* crystal
    generation model. Similar to the protein world, we see more examples of automated
    labs for materials science (â€œlab-in-the-loopâ€) such as the [A-Lab from UC Berkley](https://www.nature.com/articles/s41586-023-06734-w).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ Google DeepMind å‘å¸ƒäº† [**GNoME**](https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/)ï¼ˆç”±
    [Merchant ç­‰äºº](https://www.nature.com/articles/s41586-023-06735-9) æå‡ºçš„ææ–™ç§‘å­¦å›¾ç½‘ç»œï¼‰ä½œä¸ºå‘ç°æ–°ææ–™çš„æˆåŠŸä¸»åŠ¨å­¦ä¹ æµç¨‹ç¤ºä¾‹ï¼ŒåŒæ—¶å‘å¸ƒäº†
    [UniMat](https://unified-materials.github.io/unimat/) ä½œä¸ºä¸€ä¸ª *ä»å¤´å¼€å§‹* çš„æ™¶ä½“ç”Ÿæˆæ¨¡å‹ã€‚ç±»ä¼¼äºè›‹ç™½è´¨é¢†åŸŸï¼Œæˆ‘ä»¬ä¹Ÿçœ‹åˆ°äº†æ›´å¤šå…³äºææ–™ç§‘å­¦çš„è‡ªåŠ¨åŒ–å®éªŒå®¤ç¤ºä¾‹ï¼ˆâ€œå®éªŒå®¤-ç¯è·¯â€ï¼‰ï¼Œä¾‹å¦‚
    [UC ä¼¯å…‹åˆ©çš„ A-Lab](https://www.nature.com/articles/s41586-023-06734-w)ã€‚
- en: '![](../Images/bfe512fa260e4fa3114312d89c43d783.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfe512fa260e4fa3114312d89c43d783.png)'
- en: 'The active learning loop of GNoME. Source: [Merchant et al.](https://www.nature.com/articles/s41586-023-06735-9)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: GNoME çš„ä¸»åŠ¨å­¦ä¹ å¾ªç¯ã€‚æ¥æºï¼š[Merchant ç­‰äºº](https://www.nature.com/articles/s41586-023-06735-9)
- en: 2ï¸âƒ£ Microsoft Research released [MatterGen](https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/),
    a generative model for unconditional and property-guided materials design, and
    [Distributional Graphormer](https://distributionalgraphormer.github.io/), a generative
    model trained to recover the equilibrium energy distribution of a molecule/protein/crystal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å¾®è½¯ç ”ç©¶é™¢å‘å¸ƒäº† [MatterGen](https://www.microsoft.com/en-us/research/blog/mattergen-property-guided-materials-design/)ï¼Œä¸€ä¸ªç”¨äºæ— æ¡ä»¶å’Œå±æ€§å¼•å¯¼ææ–™è®¾è®¡çš„ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠ
    [Distributional Graphormer](https://distributionalgraphormer.github.io/)ï¼Œä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼Œæ—¨åœ¨æ¢å¤åˆ†å­/è›‹ç™½è´¨/æ™¶ä½“çš„å¹³è¡¡èƒ½é‡åˆ†å¸ƒã€‚
- en: '![](../Images/f9bb59d0a617dbbf35236c7e6d8b146c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9bb59d0a617dbbf35236c7e6d8b146c.png)'
- en: 'Unconditional and conditional generation of MatterGen. Source: [Zeni, Pinsler,
    ZÃ¼gner, Fowler, Horton, et al.](https://arxiv.org/abs/2312.03687)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: MatterGen çš„æ— æ¡ä»¶å’Œæœ‰æ¡ä»¶ç”Ÿæˆã€‚æ¥æºï¼š[Zeni, Pinsler, ZÃ¼gner, Fowler, Horton ç­‰äºº](https://arxiv.org/abs/2312.03687)
- en: 3ï¸âƒ£ Meta AI and CMU released the [Open Catalyst Demo](https://open-catalyst.metademolab.com/)
    where you can play around with relaxations (DFT approximations) of 11.5k catalyst
    materials on 86 adsorbates in 100 different configurations each (making it up
    to 100M combinations). The demo is powered by SOTA geometric models GemNet-OC
    and Equiformer-V2.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ Meta AI å’Œ CMU å‘å¸ƒäº† [Open Catalyst Demo](https://open-catalyst.metademolab.com/)ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥ä½“éªŒ
    11.5k ç§å‚¬åŒ–å‰‚ææ–™åœ¨ 86 ç§å¸é™„ä½“ä¸Šã€æ¯ç§å¸é™„ä½“æœ‰ 100 ç§ä¸åŒé…ç½®çš„å¼›è±«ï¼ˆDFT è¿‘ä¼¼ï¼‰ï¼Œæ€»å…±æœ‰å¤šè¾¾ 100M ç§ç»„åˆã€‚è¯¥æ¼”ç¤ºç”±æœ€å…ˆè¿›çš„å‡ ä½•æ¨¡å‹
    GemNet-OC å’Œ Equiformer-V2 æä¾›æ”¯æŒã€‚
- en: '***Santiago Miret (Intel)***'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '***Santiago Miret (è‹±ç‰¹å°”)***'
- en: While those works represent large-scale deployments of generative methods, there
    is also new work on using reinforcement learning ([Govindarajan et al.](https://openreview.net/forum?id=VbjD8w2ctG),
    [Lacombe et al.](https://openreview.net/forum?id=MNfVMjsL7S)) and GFlowNets ([Mistal
    et al.](https://openreview.net/forum?id=l167FjdPOv), [Nguyen et al.](https://openreview.net/forum?id=dJuDv4MKLE))
    with geometric DL for crystal structure discovery as highlighted in the [AI for
    Accelerated Materials Design (AI4Mat)](https://sites.google.com/view/ai4mat) workshop
    at NeurIPSâ€™23\. AI4Mat-2023 itself saw rapid expansion in participation with a
    2Ã— increase in the number of submitted and accepted papers and almost tripling
    in the number of attendees.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™äº›å·¥ä½œä»£è¡¨äº†ç”Ÿæˆæ–¹æ³•çš„å¤§è§„æ¨¡åº”ç”¨ï¼Œä½†ä¹Ÿæœ‰æ–°çš„ç ”ç©¶åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆ[Govindarajan ç­‰äºº](https://openreview.net/forum?id=VbjD8w2ctG)ï¼Œ[Lacombe
    ç­‰äºº](https://openreview.net/forum?id=MNfVMjsL7S)ï¼‰å’Œ GFlowNetsï¼ˆ[Mistal ç­‰äºº](https://openreview.net/forum?id=l167FjdPOv)ï¼Œ[Nguyen
    ç­‰äºº](https://openreview.net/forum?id=dJuDv4MKLE)ï¼‰ç»“åˆå‡ ä½•æ·±åº¦å­¦ä¹ æ¥å‘ç°æ™¶ä½“ç»“æ„ï¼Œæ­£å¦‚åœ¨ [AI åŠ é€Ÿææ–™è®¾è®¡
    (AI4Mat)](https://sites.google.com/view/ai4mat) ç ”è®¨ä¼šä¸­æ‰€å¼ºè°ƒçš„é‚£æ ·ï¼ŒNeurIPSâ€™23 ä¸Šçš„ AI4Mat-2023
    ä¹Ÿè§è¯äº†å‚ä¸äººæ•°çš„è¿…é€Ÿå¢é•¿ï¼Œæäº¤å’Œæ¥æ”¶çš„è®ºæ–‡æ•°é‡ç¿»äº†ä¸€ç•ªï¼Œå‚ä¼šäººæ•°å‡ ä¹å¢åŠ äº†ä¸‰å€ã€‚
- en: ğŸ’¡ Geometric DL and GNNs continue to be a major part of AI4Matâ€™s research content
    as we saw increased application of methods not only for property prediction but
    also for improving **chemical synthesis** and **material characterization**. One
    such promising example highlighted in the AI4Mat-2023 workshop is **KREED** ([Cheng,
    Lo, et al](https://openreview.net/forum?id=jlZrTCccAb)), which uses equivariant
    diffusion to predict 3D structures of molecules based on incomplete information
    that can be obtained from real laboratory machines.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å‡ ä½•æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰å’Œå›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ä»ç„¶æ˜¯AI4Matç ”ç©¶å†…å®¹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™äº›æ–¹æ³•çš„åº”ç”¨ä¸ä»…é™äºæ€§è´¨é¢„æµ‹ï¼Œè¿˜æ‰©å±•åˆ°äº†**åŒ–å­¦åˆæˆ**å’Œ**ææ–™è¡¨å¾**çš„æ”¹å–„ã€‚AI4Mat-2023å·¥ä½œåŠä¸­çªå‡ºçš„ä¸€ä¸ªæœ‰å‰æ™¯çš„ä¾‹å­æ˜¯**KREED**ï¼ˆ[Cheng,
    Loç­‰](https://openreview.net/forum?id=jlZrTCccAb)ï¼‰ï¼Œå®ƒä½¿ç”¨ç­‰å˜æ‰©æ•£æ–¹æ³•ï¼Œæ ¹æ®ä»çœŸå®å®éªŒå®¤æœºå™¨è·å¾—çš„ä¸å®Œå…¨ä¿¡æ¯ï¼Œé¢„æµ‹åˆ†å­çš„ä¸‰ç»´ç»“æ„ã€‚
- en: â€œGiven the importance of structural data in material characterization, the discussions
    at AI4Mat highlighted the opportunities for Geometric DL to enter the space of
    real-world materials modelling in addition to their continued successes in simulations
    including ML-based potentials.â€ â€” Santiago Miret (Intel)
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œè€ƒè™‘åˆ°ç»“æ„æ•°æ®åœ¨ææ–™è¡¨å¾ä¸­çš„é‡è¦æ€§ï¼ŒAI4Matçš„è®¨è®ºçªå‡ºäº†å‡ ä½•æ·±åº¦å­¦ä¹ è¿›å…¥çœŸå®ä¸–ç•Œææ–™å»ºæ¨¡é¢†åŸŸçš„æœºä¼šï¼Œé™¤äº†åœ¨åŒ…æ‹¬åŸºäºæœºå™¨å­¦ä¹ çš„æ½œåŠ›åœ¨å†…çš„ä»¿çœŸä¸­çš„æŒç»­æˆåŠŸã€‚â€
    â€” Santiago Miretï¼ˆè‹±ç‰¹å°”ï¼‰
- en: 'ğŸ”® In 2024, I expect to see multiple developments:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® åœ¨2024å¹´ï¼Œæˆ‘é¢„è®¡ä¼šçœ‹åˆ°å¤šé¡¹å‘å±•ï¼š
- en: 1ï¸âƒ£ More discovery architectures and workflows that directly integrate geometric
    models like M3GNet, CHGNet, MACE.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æ›´å¤šå‘ç°æ–°çš„æ¶æ„å’Œå·¥ä½œæµç¨‹ï¼Œç›´æ¥æ•´åˆåƒM3GNetã€CHGNetã€MACEè¿™æ ·çš„å‡ ä½•æ¨¡å‹ã€‚
- en: 2ï¸âƒ£ Geometric models might also see increased competition from text-based representations
    and LLMs as [new methods are being proposed](https://openreview.net/forum?id=0r5DE2ZSwJ)
    that directly generate CIF files.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ å‡ ä½•æ¨¡å‹å¯èƒ½ä¼šé¢ä¸´æ¥è‡ªæ–‡æœ¬è¡¨ç¤ºå’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„ç«äº‰ï¼Œå› ä¸º[æ–°æ–¹æ³•æ­£åœ¨è¢«æå‡º](https://openreview.net/forum?id=0r5DE2ZSwJ)ï¼Œç›´æ¥ç”ŸæˆCIFæ–‡ä»¶ã€‚
- en: 3ï¸âƒ£ More deployment of geometric models and GNNs into real-world experimental
    data, likely in materials characterization such as KREED, which will likely run
    into regimes with less data compared to simulation-based modeling.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æ›´å¤šå‡ ä½•æ¨¡å‹å’ŒGNNåº”ç”¨äºçœŸå®ä¸–ç•Œçš„å®éªŒæ•°æ®ï¼Œå¯èƒ½ç”¨äºææ–™è¡¨å¾ï¼Œå¦‚KREEDï¼Œé¢„è®¡å°†é¢ä¸´ä¸åŸºäºä»¿çœŸå»ºæ¨¡ç›¸æ¯”æ•°æ®è¾ƒå°‘çš„æƒ…å†µã€‚
- en: Molecular Dynamics & ML Potentials
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ†å­åŠ¨åŠ›å­¦ä¸æœºå™¨å­¦ä¹ æ½œåŠ›
- en: '*Michael Galkin (Intel), Leon Klein (FU Berlin), N M Anoop Krishnan (IIT Delhi),
    Santiago Miret (Intel)*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*Michael Galkin (è‹±ç‰¹å°”)ï¼ŒLeon Klein (æŸæ—è‡ªç”±å¤§å­¦)ï¼ŒN M Anoop Krishnan (å°åº¦ç†å·¥å­¦é™¢å¾·é‡Œåˆ†æ ¡)ï¼ŒSantiago
    Miret (è‹±ç‰¹å°”)*'
- en: One of the pronounced trends of 2023 is going towards foundation models for
    ML potentials that work on a variety of compounds from small molecules to periodic
    crystals
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2023å¹´ä¸€ä¸ªæ˜¾è‘—çš„è¶‹åŠ¿æ˜¯æœç€åŸºç¡€æ¨¡å‹å‘å±•ï¼Œè¿™äº›æ¨¡å‹é€‚ç”¨äºä»å°åˆ†å­åˆ°å‘¨æœŸæ€§æ™¶ä½“ç­‰å¤šç§åŒ–åˆç‰©çš„æœºå™¨å­¦ä¹ æ½œåŠ›ã€‚
- en: For example, **JMP** ([Shoghi et al](https://arxiv.org/abs/2310.16802)) from
    FAIR and CMU, **DPA-2** ([Zhang, Liu, et al](https://arxiv.org/abs/2312.15492))
    from a large collaboration of Chinese institutions, and **MACE-MP-0** ([Batatia
    et al](https://arxiv.org/abs/2401.00096)) from a collaboration led by Cambridge.
    Practically, those are geometric GNNs pre-trained in the multi-task mode to predict
    the energy (or forces) of a certain atomic structure. Another notable mention
    goes to **Equiformer V2** ([Liao et al](https://arxiv.org/abs/2306.12059)) as
    a strong equivariant transformer that holds SOTA in many tasks including the recent
    [OpenCatalyst 2023 Challenge](https://opencatalystproject.org/challenge.html)
    and [OpenDAC](https://open-dac.github.io/index.html) (Direct Air Capture) challenge.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼ŒFAIRå’ŒCMUçš„**JMP**ï¼ˆ[Shoghiç­‰](https://arxiv.org/abs/2310.16802)ï¼‰ã€æ¥è‡ªä¸­å›½å¤šä¸ªæœºæ„çš„å¤§å‹åˆä½œçš„**DPA-2**ï¼ˆ[Zhang,
    Liuç­‰](https://arxiv.org/abs/2312.15492)ï¼‰ï¼Œä»¥åŠå‰‘æ¡¥ä¸»å¯¼çš„åˆä½œä¸­çš„**MACE-MP-0**ï¼ˆ[Batatiaç­‰](https://arxiv.org/abs/2401.00096)ï¼‰ã€‚å®é™…ä¸Šï¼Œè¿™äº›æ˜¯é¢„è®­ç»ƒçš„å‡ ä½•å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œç”¨äºé¢„æµ‹æŸä¸€åŸå­ç»“æ„çš„èƒ½é‡ï¼ˆæˆ–åŠ›ï¼‰ã€‚å¦ä¸€ä¸ªå€¼å¾—æåŠçš„ä¾‹å­æ˜¯**Equiformer
    V2**ï¼ˆ[Liaoç­‰](https://arxiv.org/abs/2306.12059)ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§å¼ºå¤§çš„ç­‰å˜å˜æ¢å™¨ï¼Œåœ¨è®¸å¤šä»»åŠ¡ä¸­ä¿æŒäº†SOTAè¡¨ç°ï¼ŒåŒ…æ‹¬æœ€è¿‘çš„[OpenCatalyst
    2023æŒ‘æˆ˜](https://opencatalystproject.org/challenge.html)å’Œ[OpenDAC](https://open-dac.github.io/index.html)ï¼ˆç›´æ¥ç©ºæ°”æ•è·ï¼‰æŒ‘æˆ˜ã€‚
- en: '![](../Images/82a368628516ed1fc271de109838f4e3.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a368628516ed1fc271de109838f4e3.png)'
- en: 'A foundation model for materials modelling. Trained only on Materials Project
    data which consists primarily of inorganic crystals and is skewed heavily towards
    oxides, MACE-MP-0 is capable of molecular dynamics simulation across a wide variety
    of chemistries in the solid, liquid and gaseous phases. Source: [Batatia et al](https://arxiv.org/abs/2401.00096)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªææ–™å»ºæ¨¡çš„åŸºç¡€æ¨¡å‹ã€‚è¯¥æ¨¡å‹ä»…åœ¨ææ–™é¡¹ç›®æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ•°æ®ä¸»è¦ç”±æ— æœºæ™¶ä½“ç»„æˆï¼Œå¹¶ä¸”åœ¨æ°§åŒ–ç‰©æ–¹é¢åå‘è¾ƒé‡ï¼ŒMACE-MP-0èƒ½å¤Ÿåœ¨å›ºæ€ã€æ¶²æ€å’Œæ°”æ€çš„å„ç§åŒ–å­¦ç¯å¢ƒä¸­è¿›è¡Œåˆ†å­åŠ¨åŠ›å­¦ä»¿çœŸã€‚æ¥æºï¼š[Batatiaç­‰](https://arxiv.org/abs/2401.00096)
- en: âš›ï¸ A common use case for ML potentials is molecular dynamics (MD) which aims
    to simulate a certain structure on a span of nanoseconds (10á¨â¹) to seconds. The
    main problem is that the fundamental timestep in classical methods is a femtosecond
    (10á¨Â¹âµ), that is, youâ€™d need at least 1 million steps to simulate a nanosecond
    and thatâ€™s expensive. Modern ML-based methods for MD aim to speed it up by applying
    coarse-graining and other approximation tricks that accelerate simulations by
    large margins (30â€“1000x). [Fu, Xie, et al](https://openreview.net/forum?id=y8RZoPjEUl)
    (TMLRâ€™23) apply coarse-graining to atomic structures and run a GNN over smaller
    graphs to predict the next-step position. Experimentally, the method brings 1000â€“10.000x
    speedups compared to classical methods. **TimeWarp** ([Klein, Foong, Fjelde, Mlodozeniec,
    et al](https://arxiv.org/abs/2302.01170), NeurIPSâ€™23) can simulate large timesteps
    (1â°âµ â€” 1â°â¶ femtoseconds) in a single forward pass by using a conditional normalizing
    flow model that approximates a distribution of next-step positions. A trained
    model is used with MCMC sampling and delivers ~33x speedups.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: âš›ï¸ æœºå™¨å­¦ä¹ åŠ¿èƒ½çš„ä¸€ä¸ªå¸¸è§åº”ç”¨åœºæ™¯æ˜¯åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰ï¼Œå…¶ç›®çš„æ˜¯åœ¨çº³ç§’ï¼ˆ10á¨â¹ï¼‰åˆ°ç§’çš„æ—¶é—´èŒƒå›´å†…æ¨¡æ‹ŸæŸç§ç»“æ„ã€‚ä¸»è¦é—®é¢˜åœ¨äºï¼Œç»å…¸æ–¹æ³•ä¸­çš„åŸºæœ¬æ—¶é—´æ­¥é•¿æ˜¯é£ç§’ï¼ˆ10á¨Â¹âµï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªçº³ç§’è‡³å°‘éœ€è¦1ç™¾ä¸‡ä¸ªæ—¶é—´æ­¥ï¼Œè¿™éå¸¸æ˜‚è´µã€‚ç°ä»£åŸºäºæœºå™¨å­¦ä¹ çš„MDæ–¹æ³•æ—¨åœ¨é€šè¿‡åº”ç”¨ç²—ç²’åŒ–å’Œå…¶ä»–è¿‘ä¼¼æŠ€å·§åŠ é€Ÿæ¨¡æ‹Ÿï¼Œè¿™äº›æ–¹æ³•èƒ½å¤Ÿå¤§å¹…æé«˜æ¨¡æ‹Ÿé€Ÿåº¦ï¼ˆ30â€“1000å€ï¼‰ã€‚[Fu,
    Xie, et al](https://openreview.net/forum?id=y8RZoPjEUl)ï¼ˆTMLR'23ï¼‰å°†ç²—ç²’åŒ–åº”ç”¨äºåŸå­ç»“æ„ï¼Œå¹¶åœ¨è¾ƒå°çš„å›¾ä¸Šè¿è¡ŒGNNæ¥é¢„æµ‹ä¸‹ä¸€æ­¥ä½ç½®ã€‚åœ¨å®éªŒä¸­ï¼Œè¯¥æ–¹æ³•ç›¸æ¯”ç»å…¸æ–¹æ³•å®ç°äº†1000â€“10,000å€çš„åŠ é€Ÿã€‚**TimeWarp**ï¼ˆ[Klein,
    Foong, Fjelde, Mlodozeniec, et al](https://arxiv.org/abs/2302.01170)ï¼ŒNeurIPSâ€™23ï¼‰é€šè¿‡ä½¿ç”¨æ¡ä»¶å½’ä¸€åŒ–æµæ¨¡å‹æ¥æ¨¡æ‹Ÿå¤§æ—¶é—´æ­¥é•¿ï¼ˆ1â°âµ
    â€” 1â°â¶é£ç§’ï¼‰ï¼Œè¯¥æ¨¡å‹è¿‘ä¼¼ä¸‹ä¸€æ­¥ä½ç½®çš„åˆ†å¸ƒã€‚åœ¨ä½¿ç”¨MCMCé‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒåçš„æ¨¡å‹å®ç°äº†çº¦33å€çš„åŠ é€Ÿã€‚
- en: '![](../Images/b960ce8fc1b4928fe2965be02fbe1f9e.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b960ce8fc1b4928fe2965be02fbe1f9e.png)'
- en: '(a) Initial state x(t) (Left) and accepted proposal state x(t+Ï„) (Right) sampled
    with Timewarp for the dipeptide HT (unseen during training). (b) TICA projections
    of simulation trajectories, showing transitions between metastable states, for
    a short MD simulation (Left) and Timewarp MCMC (Right), both run for 30 minutes
    of wall-clock time. Timewarp MCMC achieves a speed-up factor of â‰ˆ 33 over MD in
    terms of effective sample size per second. Source: [Klein, Foong, Fjelde, Mlodozeniec,
    et al](https://arxiv.org/abs/2302.01170)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) åˆå§‹çŠ¶æ€x(t)ï¼ˆå·¦ï¼‰å’Œæ¥å—çš„æè®®çŠ¶æ€x(t+Ï„)ï¼ˆå³ï¼‰ï¼Œé€šè¿‡Timewarpä¸ºäºŒè‚½HTï¼ˆåœ¨è®­ç»ƒæœŸé—´æœªè§è¿‡çš„æ ·æœ¬ï¼‰é‡‡æ ·ã€‚(b) æ¨¡æ‹Ÿè½¨è¿¹çš„TICAæŠ•å½±ï¼Œæ˜¾ç¤ºäº†ä»‹ç¨³æ€ä¹‹é—´çš„è½¬å˜ï¼Œåˆ†åˆ«ä¸ºçŸ­æ—¶é—´MDæ¨¡æ‹Ÿï¼ˆå·¦ï¼‰å’ŒTimewarp
    MCMCï¼ˆå³ï¼‰ï¼Œä¸¤è€…å‡è¿è¡Œäº†30åˆ†é’Ÿçš„å¢™é’Ÿæ—¶é—´ã€‚Timewarp MCMCåœ¨æœ‰æ•ˆæ ·æœ¬æ•°æ¯ç§’çš„é€Ÿåº¦ä¸Šï¼Œç›¸æ¯”MDå®ç°äº†çº¦33å€çš„åŠ é€Ÿã€‚æ¥æºï¼š[Klein, Foong,
    Fjelde, Mlodozeniec, et al](https://arxiv.org/abs/2302.01170)
- en: '***Santiago Miret (Intel)***'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '***Santiago Miretï¼ˆè‹±ç‰¹å°”ï¼‰***'
- en: ğŸ’¡As the deployment of geometric models has seen greater success in property
    modelling, researchers have pushed the state-of-the-art by testing these models
    in real-world molecular dynamics simulations. The first work to highlight issues
    with training models on energy and forces alone was [Forces Are Not Enough](https://openreview.net/forum?id=A8pqQipwkt)
    published in TMLR in early 2023\. Nevertheless, advances in neighborhood-based
    methods such as [Allegro](https://arxiv.org/abs/2204.05249) led to the successful
    deployment of large-scale simulations using geometric deep learning models, including
    a [nomination for the Gordon Bell Prize](https://www.hpcwire.com/off-the-wire/sc23-spotlight-gordon-bell-prize-2023-finalists-showcase-diverse-supercomputing-applications/).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡éšç€å‡ ä½•æ¨¡å‹åœ¨å±æ€§å»ºæ¨¡ä¸­çš„æˆåŠŸéƒ¨ç½²ï¼Œç ”ç©¶äººå‘˜é€šè¿‡åœ¨ç°å®ä¸–ç•Œçš„åˆ†å­åŠ¨åŠ›å­¦æ¨¡æ‹Ÿä¸­æµ‹è¯•è¿™äº›æ¨¡å‹ï¼Œæ¨åŠ¨äº†æœ€å‰æ²¿çš„å‘å±•ã€‚é¦–ä¸ªçªæ˜¾ä»…åŸºäºèƒ½é‡å’ŒåŠ›è®­ç»ƒæ¨¡å‹å­˜åœ¨é—®é¢˜çš„ç ”ç©¶æ˜¯2023å¹´åˆåœ¨TMLRä¸Šå‘å¸ƒçš„[Forces
    Are Not Enough](https://openreview.net/forum?id=A8pqQipwkt)ã€‚ç„¶è€Œï¼ŒåŸºäºé‚»åŸŸçš„æ–¹æ³•å¦‚[Allegro](https://arxiv.org/abs/2204.05249)çš„è¿›å±•ï¼Œå¯¼è‡´äº†ä½¿ç”¨å‡ ä½•æ·±åº¦å­¦ä¹ æ¨¡å‹æˆåŠŸéƒ¨ç½²å¤§è§„æ¨¡æ¨¡æ‹Ÿï¼Œå…¶ä¸­åŒ…æ‹¬[æˆˆç™»Â·è´å°”å¥–æå](https://www.hpcwire.com/off-the-wire/sc23-spotlight-gordon-bell-prize-2023-finalists-showcase-diverse-supercomputing-applications/)ã€‚
- en: â€œMuch work still remains in ensuring successful, generalised deployment of machine
    learning potentials across a variety of physical and chemical phenomena.â€ â€” Santiago
    Miret (Intel)
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œä»ç„¶æœ‰å¤§é‡å·¥ä½œéœ€è¦åšï¼Œä»¥ç¡®ä¿æœºå™¨å­¦ä¹ åŠ¿èƒ½åœ¨å„ç§ç‰©ç†å’ŒåŒ–å­¦ç°è±¡ä¸­æˆåŠŸã€å¹¿æ³›åœ°éƒ¨ç½²ã€‚â€ â€” Santiago Miretï¼ˆè‹±ç‰¹å°”ï¼‰
- en: â¡ï¸ [EGraffBench](https://arxiv.org/abs/2310.02428) highlights some new challenges,
    such as generalisation across temperatures and materials phase changes (i.e. *solid-to-liquid*
    change), and proposes new metrics for evaluating the performance of machine learning
    potentials in real MD simulations. The AI4Mat-2023 workshop also showcased the
    development of new ML potentials for specialised use cases, such as [solid electrolytes
    for batteries](https://openreview.net/forum?id=jtAXitX6dh).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ [EGraffBench](https://arxiv.org/abs/2310.02428)çªå‡ºäº†å…¶ä¸­ä¸€äº›æ–°æŒ‘æˆ˜ï¼Œä¾‹å¦‚è·¨æ¸©åº¦å’Œææ–™ç›¸å˜çš„æ³›åŒ–ï¼ˆå³*å›ºä½“åˆ°æ¶²ä½“*çš„å˜åŒ–ï¼‰ï¼Œå¹¶æå‡ºäº†ç”¨äºè¯„ä¼°æœºå™¨å­¦ä¹ åŠ¿èƒ½åœ¨å®é™…MDæ¨¡æ‹Ÿä¸­è¡¨ç°çš„æ–°æŒ‡æ ‡ã€‚AI4Mat-2023ç ”è®¨ä¼šä¹Ÿå±•ç¤ºäº†ä¸ºç‰¹å®šåº”ç”¨åœºæ™¯å¼€å‘çš„æ–°å‹æœºå™¨å­¦ä¹ åŠ¿èƒ½ï¼Œä¾‹å¦‚[ç”¨äºç”µæ± çš„å›ºæ€ç”µè§£è´¨](https://openreview.net/forum?id=jtAXitX6dh)ã€‚
- en: '***Leon Klein (FU Berlin)***'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '***Leon Klein (FU Berlin)***'
- en: ğŸ’¡ A notable constraint in the application of generative models to sample from
    the equilibrium Boltzmann distribution was the requirement for retraining with
    each new system, thereby limiting potential advantages over traditional MD simulations.
    However, recent advancements have seen the emergence of transferable models across
    various domains. Our contribution, [Timewarp](https://arxiv.org/abs/2302.01170),
    presents a transferable model capable of proposing large time steps for MD simulations
    focused on all atom small peptide systems. Similarly, [Fu et al.](https://arxiv.org/abs/2204.10348)
    capture the time-coarsened dynamics of coarse-grained polymers, while [Charron
    et al.](https://arxiv.org/abs/2310.18278) excel in learning a transferable force
    field for coarse-grained proteins.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ å°†ç”Ÿæˆæ¨¡å‹åº”ç”¨äºä»å¹³è¡¡ç»å°”å…¹æ›¼åˆ†å¸ƒä¸­é‡‡æ ·æ—¶çš„ä¸€ä¸ªæ˜¾è‘—é™åˆ¶æ˜¯ï¼Œæ¯æ¬¡é‡åˆ°æ–°ç³»ç»Ÿæ—¶éƒ½éœ€è¦é‡æ–°è®­ç»ƒï¼Œè¿™ä½¿å¾—å…¶ç›¸è¾ƒäºä¼ ç»Ÿçš„åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰æ¨¡æ‹Ÿçš„æ½œåœ¨ä¼˜åŠ¿å—é™ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„è¿›å±•ä½¿å¾—è·¨ä¸åŒé¢†åŸŸçš„å¯è¿ç§»æ¨¡å‹é€æ¸æµ®ç°ã€‚æˆ‘ä»¬çš„è´¡çŒ®ï¼Œ[Timewarp](https://arxiv.org/abs/2302.01170)ï¼Œæå‡ºäº†ä¸€ç§å¯è¿ç§»æ¨¡å‹ï¼Œèƒ½å¤Ÿä¸ºèšç„¦äºå…¨åŸå­å°è‚½ç³»ç»Ÿçš„MDæ¨¡æ‹Ÿæå‡ºå¤§æ—¶é—´æ­¥é•¿ã€‚ç±»ä¼¼åœ°ï¼Œ[Fu
    et al.](https://arxiv.org/abs/2204.10348) æ•æ‰äº†ç²—ç²’åŒ–èšåˆç‰©çš„æ—¶é—´ç²—åŒ–åŠ¨åŠ›å­¦ï¼Œè€Œ[Charron et al.](https://arxiv.org/abs/2310.18278)åˆ™æ“…é•¿å­¦ä¹ ç”¨äºç²—ç²’åŒ–è›‹ç™½è´¨çš„å¯è¿ç§»åŠ›åœºã€‚
- en: â€œConsequently, this year has demonstrated the feasibility of transferable generative
    models for MD simulations, showcasing their potential to speed up such simulations.â€
    â€” Leon Klein (FU Berlin)
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œå› æ­¤ï¼Œä»Šå¹´å±•ç¤ºäº†å¯è¿ç§»ç”Ÿæˆæ¨¡å‹åœ¨MDæ¨¡æ‹Ÿä¸­çš„å¯è¡Œæ€§ï¼Œå±•ç¤ºäº†å®ƒä»¬åŠ é€Ÿæ­¤ç±»æ¨¡æ‹Ÿçš„æ½œåŠ›ã€‚â€ â€” Leon Klein (FU Berlin)
- en: ğŸ”® In 2024, I expect that more tailored GNNs are used to improve accuracy for
    the transferable models, with a potential focus on encoding more information about
    the system. For example, Timewarp, while lacking rotational symmetry in its model,
    employs data augmentation. Alternatively, rotational symmetry could be incorporated
    using the recently proposed [SE(3) Equivariant Augmented Coupling Flows](https://arxiv.org/abs/2308.10364)**.**
    Similarly, [Charron et al.](https://arxiv.org/abs/2310.18278) use a SchNet instead
    of a more complex GNN.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® åœ¨2024å¹´ï¼Œæˆ‘é¢„è®¡ä¼šæœ‰æ›´å¤šå®šåˆ¶åŒ–çš„GNNï¼ˆå›¾ç¥ç»ç½‘ç»œï¼‰è¢«ç”¨æ¥æé«˜å¯è¿ç§»æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå¯èƒ½ä¼šä¸“æ³¨äºç¼–ç æ›´å¤šå…³äºç³»ç»Ÿçš„ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå°½ç®¡Timewarpåœ¨å…¶æ¨¡å‹ä¸­ç¼ºä¹æ—‹è½¬å¯¹ç§°æ€§ï¼Œä½†å®ƒé€šè¿‡æ•°æ®å¢å¼ºæ¥å¼¥è¡¥è¿™ä¸€ç‚¹ã€‚å¦ä¸€ç§é€‰æ‹©æ˜¯ä½¿ç”¨æœ€è¿‘æå‡ºçš„[SE(3)ç­‰å˜å¢å¼ºè€¦åˆæµ](https://arxiv.org/abs/2308.10364)æ¥åŠ å…¥æ—‹è½¬å¯¹ç§°æ€§ã€‚ç±»ä¼¼åœ°ï¼Œ[Charron
    et al.](https://arxiv.org/abs/2310.18278)ä½¿ç”¨SchNetï¼Œè€Œéæ›´å¤æ‚çš„GNNã€‚
- en: '***N M Anoop Krishnan (IIT Delhi)***'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '***N M Anoop Krishnan (IIT Delhi)***'
- en: â€œOne of the most exciting developments for the year in the realm of ML potentials
    is the development of â€œuniversalâ€ interatomic potentials that can span almost
    all the elements of the periodic table.â€ â€” N M Anoop Krishnan (IIT Delhi)
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œä»Šå¹´åœ¨æœºå™¨å­¦ä¹ åŠ¿èƒ½é¢†åŸŸæœ€ä»¤äººå…´å¥‹çš„è¿›å±•ä¹‹ä¸€æ˜¯å¼€å‘äº†å¯ä»¥è¦†ç›–å‡ ä¹æ‰€æœ‰å…ƒç´ çš„â€˜é€šç”¨â€™åŸå­é—´åŠ¿èƒ½ã€‚â€ â€” N M Anoop Krishnan (IIT Delhi)
- en: ğŸ’¡ Following M3GNet in 2022, this year witnessed the developments of three such
    models based on CHGNet ([Deng et al](https://www.nature.com/articles/s42256-023-00716-3)),
    NequIP ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)),
    and MACE ([Batatia et al](https://arxiv.org/abs/2401.00096)). These models have
    been used to demonstrate several challenging tasks including materials discovery
    ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)), and diverse
    set of MD simulations ([Batatia et al](https://arxiv.org/abs/2401.00096)) such
    as phase transition, amorphization, chemical reaction, 2D materials modeling,
    dissolution, defects, combustion to name a few. These approaches provide promising
    results towards the universality of these potentials, thereby allowing one to
    solve challenging problems including the discovery of crystals from their corresponding
    amorphous structure ([Aykol et al](https://arxiv.org/abs/2310.01117)), a long-standing
    open problem in materials.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ç»§2022å¹´çš„M3GNetä¹‹åï¼Œä»Šå¹´è§è¯äº†åŸºäºCHGNetçš„ä¸‰ç§æ¨¡å‹çš„å‘å±•ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯([Deng et al](https://www.nature.com/articles/s42256-023-00716-3))ã€NequIP
    ([Merchant et al](https://www.nature.com/articles/s41586-023-06735-9)) å’Œ MACE
    ([Batatia et al](https://arxiv.org/abs/2401.00096))ã€‚è¿™äº›æ¨¡å‹å·²è¢«ç”¨æ¥å±•ç¤ºå‡ ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ææ–™å‘ç°ï¼ˆ[Merchant
    et al](https://www.nature.com/articles/s41586-023-06735-9)ï¼‰ï¼Œä»¥åŠå¤šç§MDæ¨¡æ‹Ÿï¼ˆ[Batatia
    et al](https://arxiv.org/abs/2401.00096)ï¼‰ï¼Œå¦‚ç›¸å˜ã€éæ™¶åŒ–ã€åŒ–å­¦ååº”ã€äºŒç»´ææ–™å»ºæ¨¡ã€æº¶è§£ã€ç¼ºé™·ã€ç‡ƒçƒ§ç­‰ã€‚è¿™äº›æ–¹æ³•ä¸ºè¿™äº›åŠ¿èƒ½çš„æ™®é€‚æ€§æä¾›äº†æœ‰å¸Œæœ›çš„ç»“æœï¼Œä»è€Œä½¿å¾—è§£å†³å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜æˆä¸ºå¯èƒ½ï¼ŒåŒ…æ‹¬ä»ç›¸åº”çš„éæ™¶ç»“æ„ä¸­å‘ç°æ™¶ä½“ï¼ˆ[Aykol
    et al](https://arxiv.org/abs/2310.01117)ï¼‰ï¼Œè¿™æ˜¯ææ–™å­¦ä¸­ä¸€ä¸ªé•¿æœŸæœªè§£çš„å¼€æ”¾é—®é¢˜ã€‚
- en: ğŸ‹ï¸ While these potentials do provide a handle to attack some outstanding problems,
    the challenges remain in understanding the scenarios where these potentials can
    fail.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‹ï¸ è™½ç„¶è¿™äº›åŠ¿èƒ½ç¡®å®ä¸ºè§£å†³ä¸€äº›çªå‡ºé—®é¢˜æä¾›äº†æ‰‹æ®µï¼Œä½†æŒ‘æˆ˜ä¾ç„¶å­˜åœ¨ï¼Œä¸»è¦æ˜¯ç†è§£è¿™äº›åŠ¿èƒ½å¯èƒ½å¤±è´¥çš„æƒ…å¢ƒã€‚
- en: '**1ï¸âƒ£** Testing these potentials to their limit to understand their capability
    is an important aspect to understand their limitations. This includes modeling
    extreme environments such as **high pressure** and **radiation conditions**, simulating
    complex multicomponent systems such as **glasses or high-entropy alloys**, or
    simulating **different phases** of systems such as water or silica would be interesting
    challenges.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** æµ‹è¯•è¿™äº›åŠ¿èƒ½çš„æé™ä»¥äº†è§£å®ƒä»¬çš„èƒ½åŠ›ï¼Œæ˜¯ç†è§£å…¶å±€é™æ€§çš„é‡è¦æ–¹é¢ã€‚è¿™åŒ…æ‹¬æ¨¡æ‹Ÿæç«¯ç¯å¢ƒï¼Œå¦‚**é«˜å‹**å’Œ**è¾å°„æ¡ä»¶**ï¼Œæ¨¡æ‹Ÿå¤æ‚çš„å¤šç»„åˆ†ç³»ç»Ÿï¼Œå¦‚**ç»ç’ƒæˆ–é«˜ç†µåˆé‡‘**ï¼Œæˆ–è€…æ¨¡æ‹Ÿ**ä¸åŒç›¸æ€**çš„ç³»ç»Ÿï¼Œå¦‚æ°´æˆ–äºŒæ°§åŒ–ç¡…ï¼Œè¿™äº›éƒ½æ˜¯æœ‰è¶£çš„æŒ‘æˆ˜ã€‚'
- en: '**2ï¸âƒ£** While some of these models have been termed as â€œfoundationâ€ models,
    **emergent behavior** associated with FMs **has not been demonstrated** by them.
    Most of these models simply show extrapolation capability to potentially unseen
    regions in the phase space or to novel compositions. Developing truly foundational
    models in terms of emergent properties would be an interesting challenge.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** è™½ç„¶å…¶ä¸­ä¸€äº›æ¨¡å‹è¢«ç§°ä¸ºâ€œåŸºç¡€â€æ¨¡å‹ï¼Œä½†å®ƒä»¬å¹¶æœªå±•ç¤ºä¸åŸºç¡€æ¨¡å‹ç›¸å…³çš„**æ¶Œç°è¡Œä¸º**ã€‚è¿™äº›æ¨¡å‹å¤§å¤šåªæ˜¯å±•ç¤ºäº†å¯¹æ½œåœ¨æœªè§åŒºåŸŸæˆ–æ–°ç»„åˆç‰©çš„å¤–æ¨èƒ½åŠ›ã€‚å¼€å‘çœŸæ­£å…·æœ‰æ¶Œç°å±æ€§çš„åŸºç¡€æ¨¡å‹å°†æ˜¯ä¸€ä¸ªæœ‰è¶£çš„æŒ‘æˆ˜ã€‚'
- en: '**3ï¸âƒ£** A third aspect that has been paid less attention to is the ability
    of these models to **simulate at scale**. While [Allegro](https://arxiv.org/abs/2204.05249)
    has demonstrated some capability in terms of length scales these potentials can
    achieve, simulating at larger time and length scales with stability while respecting
    the â€œuniversalityâ€ shall still remain an open challenge for these potentials.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£** ç¬¬ä¸‰ä¸ªè¾ƒå°‘è¢«å…³æ³¨çš„æ–¹é¢æ˜¯è¿™äº›æ¨¡å‹**å¤§è§„æ¨¡æ¨¡æ‹Ÿ**çš„èƒ½åŠ›ã€‚å°½ç®¡[Allegro](https://arxiv.org/abs/2204.05249)å·²ç»å±•ç¤ºäº†è¿™äº›åŠ¿èƒ½åœ¨é•¿åº¦å°ºåº¦æ–¹é¢çš„æŸäº›èƒ½åŠ›ï¼Œä½†åœ¨æ›´å¤§çš„æ—¶é—´å’Œé•¿åº¦å°ºåº¦ä¸‹è¿›è¡Œç¨³å®šçš„æ¨¡æ‹Ÿï¼ŒåŒæ—¶ä¿æŒâ€œæ™®é€‚æ€§â€ï¼Œä»ç„¶æ˜¯è¿™äº›åŠ¿èƒ½é¢ä¸´çš„ä¸€ä¸ªæœªè§£æŒ‘æˆ˜ã€‚'
- en: ğŸ”® **What to expect in 2024?**
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® **2024å¹´ä¼šæœ‰ä»€ä¹ˆæœŸå¾…ï¼Ÿ**
- en: '**1ï¸âƒ£** **Benchmarking suite**: While there exist several benchmarking studies
    on MD simulations, it is expected that 2024 will witness more formalized efforts
    in this direction both in terms of datasets and tasks. A standard set of tasks
    that can automatically evaluate potentials and place them on leaderboards will
    enable easy ranking of potentials targeted for downstream tasks on different materials
    such as metals, polymers, or oxides.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** **åŸºå‡†å¥—ä»¶**ï¼šè™½ç„¶å·²ç»æœ‰ä¸€äº›å…³äºåˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰æ¨¡æ‹Ÿçš„åŸºå‡†ç ”ç©¶ï¼Œé¢„è®¡2024å¹´å°†åœ¨æ•°æ®é›†å’Œä»»åŠ¡æ–¹é¢çœ‹åˆ°æ›´å¤šæ­£å¼åŒ–çš„åŠªåŠ›ã€‚ä¸€ä¸ªæ ‡å‡†çš„ä»»åŠ¡é›†ï¼Œå¯ä»¥è‡ªåŠ¨è¯„ä¼°åŠ¿èƒ½å¹¶å°†å…¶æ”¾ç½®åœ¨æ’è¡Œæ¦œä¸Šï¼Œå°†ä¾¿äºå¯¹ä¸åŒææ–™ï¼ˆå¦‚é‡‘å±ã€èšåˆç‰©æˆ–æ°§åŒ–ç‰©ï¼‰çš„åç»­ä»»åŠ¡è¿›è¡ŒåŠ¿èƒ½çš„æ’åã€‚'
- en: '**2ï¸âƒ£ Model and dataset development**: Further efforts will be made to make
    ML potentials more compact and efficient in terms of their architectures. Moreover,
    2024 will also witness large-scale dataset development that will provide *ab initio*
    data for training these potentials.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£ æ¨¡å‹ä¸æ•°æ®é›†å¼€å‘**ï¼šå°†è¿›ä¸€æ­¥åŠªåŠ›ä½¿æœºå™¨å­¦ä¹ æ½œåŠ›åœ¨æ¶æ„ä¸Šæ›´ç´§å‡‘å’Œé«˜æ•ˆã€‚æ­¤å¤–ï¼Œ2024å¹´è¿˜å°†è§è¯å¤§è§„æ¨¡æ•°æ®é›†çš„å‘å±•ï¼Œè¿™äº›æ•°æ®é›†å°†ä¸ºè®­ç»ƒè¿™äº›æ½œåŠ›æä¾›*ä»å¤´è®¡ç®—*æ•°æ®ã€‚'
- en: '**3ï¸âƒ£ Differentiable MD/AIMD**: Further, it is expected that the developments
    in differentiable simulations will become a major area of fusing experiments and
    *ab initio* simulations towards automated development of interatomic potentials
    for targeted applications. This year may also see advances in differentiable AIMD
    with machine learned functionals that may allow economical simulations to scale
    beyond what it has been able to achieve thus far.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£ å¯å¾®MD/AIMD**ï¼šæ­¤å¤–ï¼Œé¢„è®¡å¯å¾®æ¨¡æ‹Ÿçš„è¿›å±•å°†æˆä¸ºèåˆå®éªŒä¸*ä»å¤´è®¡ç®—*æ¨¡æ‹Ÿï¼Œæœå‘è‡ªåŠ¨å¼€å‘é’ˆå¯¹ç‰¹å®šåº”ç”¨çš„åŸå­é—´åŠ¿èƒ½çš„é‡è¦é¢†åŸŸã€‚ä»Šå¹´å¯èƒ½è¿˜ä¼šçœ‹åˆ°å¯å¾®AIMDçš„å‘å±•ï¼Œç»“åˆæœºå™¨å­¦ä¹ çš„æ³›å‡½ï¼Œå¯èƒ½ä½¿å¾—ç»æµå‹æ¨¡æ‹Ÿèƒ½å¤Ÿè¶…è¶Šç°æœ‰çš„é™åˆ¶ï¼Œæ‰©å±•åˆ°æ›´å¤§è§„æ¨¡ã€‚'
- en: '**Predictions from the 2023 post**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023å¹´åçš„é¢„æµ‹**'
- en: We expect to see a lot more focus on computational efficiency and scalability
    of GNNs. Current GNN-based force-fields are obtaining remarkable accuracy, but
    are still 2â€“3 orders of magnitude slower than classical force-fields and are typically
    only deployed on a few hundred atoms.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¢„è®¡å°†æ›´åŠ å…³æ³¨å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰çš„è®¡ç®—æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚ç›®å‰åŸºäºGNNçš„åŠ›åœºåœ¨ç²¾åº¦ä¸Šå–å¾—äº†æ˜¾è‘—æˆå°±ï¼Œä½†ä»ç„¶æ¯”ä¼ ç»ŸåŠ›åœºæ…¢2åˆ°3ä¸ªæ•°é‡çº§ï¼Œä¸”é€šå¸¸åªèƒ½éƒ¨ç½²åœ¨å‡ ç™¾ä¸ªåŸå­ä¸Šã€‚
- en: âœ… Allegro for the Gordon Bell Prize, Large-scale screening with GNoMe
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… é˜¿è±æ ¼ç½—ï¼ˆAllegroï¼‰ç«é€æˆˆç™»Â·è´å°”å¥–ï¼Œä½¿ç”¨GNoMeè¿›è¡Œå¤§è§„æ¨¡ç­›é€‰
- en: 'ğŸ”®**What to expect in 2024**:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”®**2024å¹´å±•æœ›**ï¼š
- en: '**1ï¸âƒ£** More deployment of ML potentials into large-scale MD simulations that
    showcase new research opportunities and challenges and provide a better idea of
    what benefits ML potentials provide compared to traditional potentials.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** æ›´å¤šå°†æœºå™¨å­¦ä¹ æ½œåŠ›éƒ¨ç½²åˆ°å¤§è§„æ¨¡åˆ†å­åŠ¨åŠ›å­¦ï¼ˆMDï¼‰æ¨¡æ‹Ÿä¸­ï¼Œå±•ç¤ºæ–°çš„ç ”ç©¶æœºä¼šå’ŒæŒ‘æˆ˜ï¼Œå¹¶æä¾›æ›´å¥½çš„è§†è§’ï¼Œäº†è§£æœºå™¨å­¦ä¹ æ½œåŠ›ç›¸å¯¹äºä¼ ç»ŸåŠ¿èƒ½çš„ä¼˜åŠ¿ã€‚'
- en: '**2ï¸âƒ£** New datasets that outline previously unexplored challenges for ML potentials,
    such as new materials systems and new physical phenomena for those materials such
    as phase changes at various temperatures and pressures.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** æ–°çš„æ•°æ®é›†å°†æ¦‚è¿°æœºå™¨å­¦ä¹ æ½œåŠ›æœªæ›¾æ¢ç´¢çš„æŒ‘æˆ˜ï¼Œä¾‹å¦‚æ–°ææ–™ç³»ç»Ÿå’Œè¿™äº›ææ–™çš„æ–°ç‰©ç†ç°è±¡ï¼Œå¦‚åœ¨ä¸åŒæ¸©åº¦å’Œå‹åŠ›ä¸‹çš„ç›¸å˜ã€‚'
- en: '**3ï¸âƒ£** Exploration of multi-scale problems that might draw inspiration from
    classical techniques.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£** æ¢ç´¢å¤šå°ºåº¦é—®é¢˜ï¼Œå¯èƒ½ä¼šä»ç»å…¸æŠ€æœ¯ä¸­è·å¾—çµæ„Ÿã€‚'
- en: Geometric Generative Models (Manifolds)
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡ ä½•ç”Ÿæˆæ¨¡å‹ï¼ˆæµå½¢ï¼‰
- en: '*Joey Bose (Mila & Dreamfold) and Alex Tong (Mila & Dreamfold)*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä¹”ä¼ŠÂ·åšæ–¯ï¼ˆJoey Boseï¼‰ï¼ˆMila & Dreamfoldï¼‰å’Œäºšå†å…‹æ–¯Â·æ±¤ï¼ˆAlex Tongï¼‰ï¼ˆMila & Dreamfoldï¼‰*'
- en: While generative ML continued to dominate the field in 2023, it was the popularization
    of geometric generative models that incorporate geometric priors an interesting
    trend of the year.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç”Ÿæˆå‹æœºå™¨å­¦ä¹ åœ¨2023å¹´ç»§ç»­ä¸»å¯¼è¯¥é¢†åŸŸï¼Œä½†å°†å‡ ä½•å…ˆéªŒç»“åˆåˆ°å‡ ä½•ç”Ÿæˆæ¨¡å‹ä¸­çš„æ™®åŠæˆä¸ºè¿™ä¸€å¹´ä¸€ä¸ªæœ‰è¶£çš„è¶‹åŠ¿ã€‚
- en: '***Joey Bose (Mila & Dreamfold)***'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¹”ä¼ŠÂ·åšæ–¯ï¼ˆJoey Boseï¼‰ï¼ˆMila & Dreamfoldï¼‰***'
- en: â€œThis year we saw the burgeoning subfield of geometric generative generative
    models really take a commanding step forward. With the success of diffusion models
    and flow matching in images we saw more fundamental contributions to enable Generative
    AI for geometric data types.â€œ â€” Joey Bose (Mila & Dreamfold)
  id: totrans-188
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œä»Šå¹´æˆ‘ä»¬çœ‹åˆ°å‡ ä½•ç”Ÿæˆæ¨¡å‹è¿™ä¸€æ–°å…´å­é¢†åŸŸè¿ˆå‡ºäº†é‡è¦çš„ä¸€æ­¥ã€‚éšç€æ‰©æ•£æ¨¡å‹å’ŒæµåŠ¨åŒ¹é…åœ¨å›¾åƒä¸­çš„æˆåŠŸåº”ç”¨ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æ›´å¤šåŸºç¡€æ€§çš„è´¡çŒ®ï¼Œä¸ºå‡ ä½•æ•°æ®ç±»å‹çš„ç”Ÿæˆå¼AIå¥ å®šäº†åŸºç¡€ã€‚â€â€”â€”
    ä¹”ä¼ŠÂ·åšæ–¯ï¼ˆJoey Boseï¼‰ï¼ˆMila & Dreamfoldï¼‰
- en: While diffusion models for manifolds existed, this year we really saw them being
    scaled up with **Scaling Riemannian Diffusion Models** by [Lou et. al](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=54-actIAAAAJ&sortby=pubdate&citation_for_view=54-actIAAAAJ%3A_FxGoFyzp5QC)
    and functional approaches in **Manifold Diffusion Fields** [Elhag et. al.](https://arxiv.org/abs/2305.15586)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æµå½¢çš„æ‰©æ•£æ¨¡å‹æ—©å·²æœ‰æ‰€å­˜åœ¨ï¼Œä½†ä»Šå¹´æˆ‘ä»¬ç¡®å®çœ‹åˆ°äº†å®ƒä»¬é€šè¿‡**æ‰©å±•é»æ›¼æ‰©æ•£æ¨¡å‹**ç”±[Louç­‰äºº](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=54-actIAAAAJ&sortby=pubdate&citation_for_view=54-actIAAAAJ%3A_FxGoFyzp5QC)å’Œ**æµå½¢æ‰©æ•£åœº**[Elhagç­‰äºº](https://arxiv.org/abs/2305.15586)çš„åŠŸèƒ½æ–¹æ³•å¾—åˆ°æ‰©å±•
- en: '![](../Images/c4c9c5ec0804e2389bd2fc37d0611062.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c9c5ec0804e2389bd2fc37d0611062.png)'
- en: '(Left) Visual depiction of a training iteration for a field on the bunny manifold
    M. (Right) Visual depiction of the sampling process for a field on the bunny manifold.
    Figure source: [Elhag et al.](https://arxiv.org/abs/2305.15586)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆå·¦ï¼‰åœ¨å…”å­æµå½¢Mä¸Šçš„åœºçš„è®­ç»ƒè¿­ä»£çš„å¯è§†åŒ–è¡¨ç°ã€‚ï¼ˆå³ï¼‰åœ¨å…”å­æµå½¢ä¸Šçš„åœºçš„é‡‡æ ·è¿‡ç¨‹çš„å¯è§†åŒ–è¡¨ç°ã€‚å›¾æºï¼š[Elhagç­‰äºº](https://arxiv.org/abs/2305.15586)
- en: For Normalizing flow-based methods, **Riemannian Flow matching** by [Chen and
    Lipman](https://arxiv.org/abs/2302.03660) stands at the top of the sea of papers
    as being the most general framework for FM.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŸºäºå½’ä¸€åŒ–æµçš„æ–¹æ³•ï¼Œ[é™ˆå’Œåˆ©æ™®æ›¼](https://arxiv.org/abs/2302.03660)æå‡ºçš„**é»æ›¼æµåŒ¹é…**è¢«è®¤ä¸ºæ˜¯FMé¢†åŸŸä¸­æœ€é€šç”¨çš„æ¡†æ¶ï¼Œä½äºä¼—å¤šè®ºæ–‡ä¹‹ä¸­ã€‚
- en: In general, a large theme of geometric generative models involves handling symmetries.
    Equivariant approaches shone this year, from SE(3) models including **EDGI** ([Brehmer,
    Bose et. al](https://arxiv.org/abs/2303.12410)), **SE(3) augmented coupling flows**
    ([Midgley et. al](https://arxiv.org/abs/2308.10364)), to cool theoretical work
    on **Geometric neural diffusion processes** ([Mathieu et. al](https://arxiv.org/abs/2307.05431))
    and important physics-based applications with the paper by [Abbot et. al](https://arxiv.org/abs/2305.02402).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œå‡ ä½•ç”Ÿæˆæ¨¡å‹çš„ä¸€ä¸ªé‡è¦ä¸»é¢˜æ˜¯å¤„ç†å¯¹ç§°æ€§ã€‚ä»Šå¹´ï¼Œç­‰å˜æ–¹æ³•å¤§æ”¾å¼‚å½©ï¼ŒåŒ…æ‹¬SE(3)æ¨¡å‹ä¸­çš„**EDGI**ï¼ˆ[Brehmer, Boseç­‰äºº](https://arxiv.org/abs/2303.12410)ï¼‰ã€**SE(3)å¢å¼ºè€¦åˆæµ**ï¼ˆ[Midgleyç­‰äºº](https://arxiv.org/abs/2308.10364)ï¼‰ï¼Œä»¥åŠå…³äº**å‡ ä½•ç¥ç»æ‰©æ•£è¿‡ç¨‹**çš„å¾ˆé…·çš„ç†è®ºå·¥ä½œï¼ˆ[Mathieuç­‰äºº](https://arxiv.org/abs/2307.05431)ï¼‰å’Œç”±[Abbotç­‰äºº](https://arxiv.org/abs/2305.02402)æå‡ºçš„é‡è¦åŸºäºç‰©ç†çš„åº”ç”¨ã€‚
- en: '***Alex Tong (Mila & Dreamfold)***'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '***Alex Tong (Mila & Dreamfold)***'
- en: â€œIn 2023 we saw advancement both in terms of modelling and the rise of a new
    application â€” Protein backbone design. Much work is still needed to understand
    the properties of the SE(3)*á´º*â‚€ type of product manifold, where it is still unclear
    how to best combine modalitiesâ€ â€” Alex Tong (Mila & Dreamfold)
  id: totrans-195
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œ2023å¹´ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å»ºæ¨¡çš„è¿›å±•å’Œä¸€ä¸ªæ–°åº”ç”¨çš„å´›èµ·â€”â€”è›‹ç™½è´¨éª¨æ¶è®¾è®¡ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦æ›´å¤šçš„å·¥ä½œæ¥ç†è§£SE(3)*á´º*â‚€ç±»å‹çš„ç§¯äº§å“æµå½¢çš„æ€§è´¨ï¼Œåœ¨è¿™ä¸€é¢†åŸŸï¼Œæˆ‘ä»¬ä»ä¸æ¸…æ¥šå¦‚ä½•æœ€å¥½åœ°ç»“åˆä¸åŒçš„æ¨¡æ€â€â€”â€”Alex
    Tong (Mila & Dreamfold)
- en: 2023 saw new models such as [RFDiffusion](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1),
    [FrameDiff](https://arxiv.org/abs/2302.02277), and [FoldFlow](https://arxiv.org/abs/2310.02391)
    which operate over the SE(3)*á´º*â‚€ manifold of protein backbones. This presents
    a new challenge for geometric generative models which I think we will see significant
    progress in the coming year.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 2023å¹´å‡ºç°äº†æ–°çš„æ¨¡å‹ï¼Œå¦‚[RFDiffusion](https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1)ã€[FrameDiff](https://arxiv.org/abs/2302.02277)å’Œ[FoldFlow](https://arxiv.org/abs/2310.02391)ï¼Œå®ƒä»¬åœ¨è›‹ç™½è´¨éª¨æ¶çš„SE(3)*á´º*â‚€æµå½¢ä¸Šè¿›è¡Œæ“ä½œã€‚è¿™ä¸ºå‡ ä½•ç”Ÿæˆæ¨¡å‹æå‡ºäº†æ–°çš„æŒ‘æˆ˜ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å°†åœ¨æœªæ¥ä¸€å¹´çœ‹åˆ°åœ¨è¿™ä¸€é¢†åŸŸçš„é‡è¦è¿›å±•ã€‚
- en: On the modelling side, generative modelling with flow and bridge matching models
    in Euclidean domains led to quick succession of Riemannian and equivariant extensions
    with Riemannian Flow Matching by [Chen and Lipman](https://arxiv.org/abs/2302.03660)
    and Equivariant flow matching ([Klein et al.](https://arxiv.org/abs/2306.15030),
    [Song et al.](https://arxiv.org/abs/2312.07168)) on molecule generation tasks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å»ºæ¨¡æ–¹é¢ï¼Œæ¬§å‡ é‡Œå¾—é¢†åŸŸä¸­çš„ç”Ÿæˆå»ºæ¨¡ä¸æµå’Œæ¡¥æ¥åŒ¹é…æ¨¡å‹ç›¸ç»“åˆï¼Œè¿…é€Ÿç»§è€Œæ¨å‡ºäº†é»æ›¼æµåŒ¹é…ï¼ˆ[é™ˆå’Œåˆ©æ™®æ›¼](https://arxiv.org/abs/2302.03660)ï¼‰å’Œç­‰å˜æµåŒ¹é…ï¼ˆ[Kleinç­‰äºº](https://arxiv.org/abs/2306.15030)ã€[Songç­‰äºº](https://arxiv.org/abs/2312.07168)ï¼‰ç”¨äºåˆ†å­ç”Ÿæˆä»»åŠ¡ã€‚
- en: 'ğŸ”® **What to expect in 2024**:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® **2024å¹´å±•æœ›**ï¼š
- en: '**1ï¸âƒ£** More exploration into modelling the SE(3)*á´º*â‚€ manifold following successes
    in protein backbone design.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** åœ¨è›‹ç™½è´¨éª¨æ¶è®¾è®¡å–å¾—æˆåŠŸåï¼Œæ›´å¤šåœ°æ¢ç´¢SE(3)*á´º*â‚€æµå½¢çš„å»ºæ¨¡ã€‚'
- en: '**2ï¸âƒ£** Further investigation and theory of how to train generative models
    on multimodal and product manifolds.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** è¿›ä¸€æ­¥ç ”ç©¶å’Œç†è®ºæ¢ç´¢å¦‚ä½•åœ¨å¤šæ¨¡æ€å’Œç§¯äº§å“æµå½¢ä¸Šè®­ç»ƒç”Ÿæˆæ¨¡å‹ã€‚'
- en: '**3ï¸âƒ£** Domain-specific models exploiting features of more specific manifold
    and equivariant structures.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**3ï¸âƒ£** åˆ©ç”¨æ›´å…·ä½“çš„æµå½¢å’Œç­‰å˜ç»“æ„ç‰¹å¾çš„é¢†åŸŸç‰¹å®šæ¨¡å‹ã€‚'
- en: 'BIG Graphs, Scalability: When GNNs are too expensive'
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§å‹å›¾ï¼Œæ‰©å±•æ€§ï¼šå½“GNNsè¿‡äºæ˜‚è´µæ—¶
- en: '***Anton Tsitsulin (Google)***'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '***Anton Tsitsulin (Google)***'
- en: This year has been fruitful for large graph fans.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¹´å¯¹äºå¤§å‹å›¾å½¢çš„çˆ±å¥½è€…æ¥è¯´æ˜¯ä¸°æ”¶çš„ä¸€å¹´ã€‚
- en: â€œLearning on Very Large Graphs has always been a challenge due to the unstructured
    sparsity not being supported by modern accelerators, losing in the [hardware lottery](https://hardwarelottery.github.io/).
    [Tensor Processing Units](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)
    â€” you can think about them as very fast GPUs with tons (multi-terabyte) of HBM
    memory â€” were the rescue of 2023.â€ â€” **Anton Tsitsulin** (Google)
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨éå¸¸å¤§çš„å›¾å½¢ä¸Šè¿›è¡Œå­¦ä¹ ä¸€ç›´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºç°ä»£åŠ é€Ÿå™¨ä¸æ”¯æŒè¿™ç§éç»“æ„åŒ–ç¨€ç–æ€§ï¼Œå¯¼è‡´åœ¨[ç¡¬ä»¶æŠ½å¥–](https://hardwarelottery.github.io/)ä¸­è½è´¥ã€‚[å¼ é‡å¤„ç†å•å…ƒ](https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains)â€”â€”ä½ å¯ä»¥æŠŠå®ƒä»¬çœ‹ä½œæ˜¯éå¸¸å¿«é€Ÿçš„
    GPUï¼Œé…æœ‰å¤§é‡ï¼ˆå¤š TBï¼‰çš„ HBM å†…å­˜â€”â€”æ˜¯ 2023 å¹´çš„æ•‘æ˜Ÿã€‚â€ â€”â€” **Anton Tsitsulin**ï¼ˆGoogleï¼‰
- en: In a KDD paper ([Mayer et al.](https://arxiv.org/abs/2307.14490)), we showed
    that TPUs can solve large-scale node embedding problems more efficiently than
    GPU and CPU systems at a fraction of the cost. Many industrial applications of
    graph machine learning are fully unsupervised; there, it is hard to evaluate embedding
    quality. We wrote a paper ([Tsitsulin et al.](https://arxiv.org/abs/2305.16562))
    that performs **unsupervised embedding analysis** at scale.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ç¯‡ KDD è®ºæ–‡ä¸­ï¼ˆ[Mayer ç­‰](https://arxiv.org/abs/2307.14490)ï¼‰ï¼Œæˆ‘ä»¬å±•ç¤ºäº† TPUs å¦‚ä½•æ¯” GPU
    å’Œ CPU ç³»ç»Ÿæ›´é«˜æ•ˆåœ°è§£å†³å¤§è§„æ¨¡èŠ‚ç‚¹åµŒå…¥é—®é¢˜ï¼Œå¹¶ä¸”æˆæœ¬å¤§å¤§é™ä½ã€‚å›¾å½¢æœºå™¨å­¦ä¹ çš„è®¸å¤šå·¥ä¸šåº”ç”¨æ˜¯å®Œå…¨æ— ç›‘ç£çš„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¾ˆéš¾è¯„ä¼°åµŒå…¥è´¨é‡ã€‚æˆ‘ä»¬æ’°å†™äº†ä¸€ç¯‡è®ºæ–‡ï¼ˆ[Tsitsulin
    ç­‰](https://arxiv.org/abs/2305.16562)ï¼‰ï¼Œåœ¨å¤§è§„æ¨¡ä¸Šæ‰§è¡Œ**æ— ç›‘ç£åµŒå…¥åˆ†æ**ã€‚
- en: '![](../Images/c4683b5e9dcf8a260db6773c1db23ad1.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4683b5e9dcf8a260db6773c1db23ad1.png)'
- en: 'Scale of TpuGraphs compared to other graph property prediction datasets. Source:
    [Phothilimthana et al.](https://arxiv.org/abs/2308.13490)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: TpuGraphs ä¸å…¶ä»–å›¾å½¢å±æ€§é¢„æµ‹æ•°æ®é›†çš„è§„æ¨¡å¯¹æ¯”ã€‚æ¥æºï¼š[Phothilimthana ç­‰](https://arxiv.org/abs/2308.13490)
- en: â¡ï¸ This year, TPUs helped graph machine learning, so it was time to give back.
    We released a new **TpuGraphs** dataset ([Phothilimthana et al.](https://arxiv.org/abs/2308.13490))
    and ran a [Kaggle competition](https://www.kaggle.com/competitions/predict-ai-model-runtime)
    â€œGoogle â€” Fast or Slow? Predict AI Model Runtimeâ€ on it that showed [how to improve](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)
    learning models running on TPUs with graph machine learning. It had 792 Competitors,
    616 Teams, and 10,507 Entries. The dataset provides 25x more graphs than the largest
    graph property prediction dataset (with comparable graph sizes), and 770x larger
    graphs on average compared to existing performance prediction datasets on machine
    learning programs. This dataset is so large, a new algorithm for doing graph-level
    predictions on large-scale graphs had to be developed by [Cao et al](https://arxiv.org/abs/2305.12322).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä»Šå¹´ï¼ŒTPU å¸®åŠ©äº†å›¾å½¢æœºå™¨å­¦ä¹ ï¼Œå› æ­¤æ˜¯æ—¶å€™å›é¦ˆäº†ã€‚æˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªæ–°çš„**TpuGraphs**æ•°æ®é›†ï¼ˆ[Phothilimthana ç­‰](https://arxiv.org/abs/2308.13490)ï¼‰ï¼Œå¹¶åŸºäºè¯¥æ•°æ®é›†ä¸¾åŠäº†ä¸€ä¸ª[Kaggle
    ç«èµ›](https://www.kaggle.com/competitions/predict-ai-model-runtime)â€œGoogleâ€”â€”å¿«è¿˜æ˜¯æ…¢ï¼Ÿé¢„æµ‹
    AI æ¨¡å‹è¿è¡Œæ—¶é—´â€ï¼Œè¯¥ç«èµ›å±•ç¤ºäº†[å¦‚ä½•æå‡](https://blog.research.google/2023/12/advancements-in-machine-learning-for.html)åœ¨
    TPU ä¸Šè¿è¡Œçš„å›¾å½¢æœºå™¨å­¦ä¹ å­¦ä¹ æ¨¡å‹ã€‚ç«èµ›å…±æœ‰792åå‚èµ›è€…ï¼Œ616ä¸ªé˜Ÿä¼ï¼Œå’Œ10,507ä¸ªå‚èµ›ä½œå“ã€‚è¯¥æ•°æ®é›†æä¾›äº†æ¯”ç°æœ‰çš„æœ€å¤§å›¾å½¢å±æ€§é¢„æµ‹æ•°æ®é›†ï¼ˆåœ¨å›¾å½¢è§„æ¨¡ç›¸å½“çš„æƒ…å†µä¸‹ï¼‰å¤šå‡º25å€çš„å›¾å½¢ï¼Œä¸”ç›¸æ¯”ç°æœ‰çš„æœºå™¨å­¦ä¹ ç¨‹åºæ€§èƒ½é¢„æµ‹æ•°æ®é›†ï¼Œå¹³å‡å›¾å½¢è§„æ¨¡å¤§770å€ã€‚è¿™ä¸ªæ•°æ®é›†å¦‚æ­¤åºå¤§ï¼Œä»¥è‡³äº[æ›¹ç­‰](https://arxiv.org/abs/2305.12322)ä¸å¾—ä¸å¼€å‘ä¸€ç§æ–°çš„ç®—æ³•æ¥è¿›è¡Œå¤§è§„æ¨¡å›¾å½¢ä¸Šçš„å›¾çº§é¢„æµ‹ã€‚
- en: â¡ï¸ Large-scale graph clustering has seen significant contributions this year.
    A new approximation algorithm ([Cohen-Addad et al.](https://arxiv.org/abs/2309.17243))
    was proposed for correlation clustering improving the approximation factor from
    1.994 to the whopping 1.73\. **TeraHAC** ([Dhulipala et al](https://arxiv.org/abs/2308.03578))
    is a major improvement over last yearâ€™s **ParHAC** (that we covered in the [2023
    post](https://medium.com/towards-data-science/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232#ca19))
    â€” an approximate (1+ğ) hierarchical agglomerative clustering algorithm for trillion-edge
    graphs. The largest graph used in the experiments is a massive Web-Query graph
    with 31B nodes and 8.6 trillion edges ğŸ‘€. Notable mentions also go to the fastest
    (to date) algorithm for Euclidean minimum spanning tree ([Jayaram et al](https://arxiv.org/abs/2308.00503))
    and a new near-linear time algorithm for approximating the Chamfer distance between
    point sets ([Bakshi et al.](https://arxiv.org/abs/2307.03043)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å¤§è§„æ¨¡å›¾èšç±»åœ¨ä»Šå¹´å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚æå‡ºäº†ä¸€ç§æ–°çš„è¿‘ä¼¼ç®—æ³•ï¼ˆ[Cohen-Addad ç­‰](https://arxiv.org/abs/2309.17243)ï¼‰ç”¨äºç›¸å…³æ€§èšç±»ï¼Œå°†è¿‘ä¼¼å› å­ä»1.994æé«˜åˆ°æƒŠäººçš„1.73ã€‚**TeraHAC**ï¼ˆ[Dhulipala
    ç­‰](https://arxiv.org/abs/2308.03578)ï¼‰æ˜¯å¯¹å»å¹´**ParHAC**ï¼ˆæˆ‘ä»¬åœ¨[2023å¹´çš„æ–‡ç« ](https://medium.com/towards-data-science/graph-ml-in-2023-the-state-of-affairs-1ba920cb9232#ca19)ä¸­æœ‰æåˆ°ï¼‰çš„ä¸€é¡¹é‡å¤§æ”¹è¿›â€”â€”è¿™æ˜¯ä¸€ä¸ªé€‚ç”¨äºä¸‡äº¿è¾¹å›¾çš„è¿‘ä¼¼ï¼ˆ1+ğï¼‰å±‚æ¬¡èšåˆèšç±»ç®—æ³•ã€‚å®éªŒä¸­ä½¿ç”¨çš„æœ€å¤§å›¾æ˜¯ä¸€ä¸ªåºå¤§çš„Web-Queryå›¾ï¼ŒåŒ…å«31BèŠ‚ç‚¹å’Œ8.6ä¸‡äº¿è¾¹ğŸ‘€ã€‚å€¼å¾—ä¸€æçš„è¿˜æœ‰ç›®å‰æœ€å¿«çš„æ¬§å‡ é‡Œå¾—æœ€å°ç”Ÿæˆæ ‘ç®—æ³•ï¼ˆ[Jayaram
    ç­‰](https://arxiv.org/abs/2308.00503)ï¼‰ä»¥åŠä¸€ç§æ–°çš„è¿‘çº¿æ€§æ—¶é—´ç®—æ³•ï¼Œç”¨äºè¿‘ä¼¼ç‚¹é›†ä¹‹é—´çš„Chamferè·ç¦»ï¼ˆ[Bakshi
    ç­‰](https://arxiv.org/abs/2307.03043)ï¼‰ã€‚
- en: 'ğŸ”® **What to expect in 2024**:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® **2024å¹´å±•æœ›**ï¼š
- en: '**1ï¸âƒ£** Algorithmic advances will help scale other popular graph algorithms'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** ç®—æ³•è¿›å±•å°†å¸®åŠ©æ‰©å±•å…¶ä»–æµè¡Œçš„å›¾ç®—æ³•'
- en: '**2ï¸âƒ£** Novel hardware usage will help scaling up different graph models'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** æ–°å‹ç¡¬ä»¶çš„ä½¿ç”¨å°†æœ‰åŠ©äºæ‰©å¤§ä¸åŒå›¾æ¨¡å‹çš„è§„æ¨¡'
- en: '**Predictions from the 2023 post**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023å¹´æ–‡ç« ä¸­çš„é¢„æµ‹**'
- en: (1) further reduction in compute costs and inference time for very large graphs
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (1) è¿›ä¸€æ­¥é™ä½å¤§å‹å›¾çš„è®¡ç®—æˆæœ¬å’Œæ¨ç†æ—¶é—´
- en: âœ… We observed order-of-magnitude speedups in clustering and node embedding.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æˆ‘ä»¬è§‚å¯Ÿåˆ°èšç±»å’ŒèŠ‚ç‚¹åµŒå…¥çš„åŠ é€Ÿæ˜¯æ•°é‡çº§çš„æå‡ã€‚
- en: (2) Perhaps models for OGB LSC graphs could run on commodity machines instead
    of huge clusters?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: (2) æˆ–è®¸OGB LSCå›¾çš„æ¨¡å‹å¯ä»¥åœ¨æ™®é€šæœºå™¨ä¸Šè¿è¡Œï¼Œè€Œä¸æ˜¯åºå¤§çš„é›†ç¾¤ï¼Ÿ
- en: âŒ solid no
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ åšå†³å¦å®š
- en: Algorithmic Reasoning & Alignment
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®—æ³•æ¨ç†ä¸å¯¹é½
- en: '*Petar VeliÄkoviÄ‡ (Google DeepMind) and Liudmila Prokhorenkova (Yandex Research)*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*Petar VeliÄkoviÄ‡ï¼ˆGoogle DeepMindï¼‰å’ŒLiudmila Prokhorenkovaï¼ˆYandex Researchï¼‰*'
- en: Algorithmic reasoning, a class of ML techniques able to execute algorithmic
    computation, has continued to make stable progress during 2023.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•æ¨ç†ï¼Œä½œä¸ºä¸€ç§èƒ½å¤Ÿæ‰§è¡Œç®—æ³•è®¡ç®—çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ç±»åˆ«ï¼Œåœ¨2023å¹´æŒç»­ç¨³å®šåœ°è¿›å±•ã€‚
- en: '***Petar VeliÄkoviÄ‡ (Google DeepMind)***'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '***Petar VeliÄkoviÄ‡ï¼ˆGoogle DeepMindï¼‰***'
- en: â€œ2023 has been a year of steady progress for neural algorithmic reasoning models
    â€” it indeed remains one of the areas where GNN development gets most creative
    â€” probably because it has to be.â€ â€” **Petar VeliÄkoviÄ‡** (Google DeepMind)
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œ2023å¹´å¯¹ç¥ç»ç®—æ³•æ¨ç†æ¨¡å‹æ¥è¯´æ˜¯ä¸€ä¸ªç¨³æ­¥å‘å±•çš„å¹´ä»½â€”â€”è¿™ç¡®å®æ˜¯GNNå‘å±•ä¸­æœ€å…·åˆ›æ„çš„é¢†åŸŸä¹‹ä¸€â€”â€”å¯èƒ½æ˜¯å› ä¸ºå®ƒå¿…é¡»å¦‚æ­¤ã€‚â€ â€”â€” **Petar VeliÄkoviÄ‡**ï¼ˆGoogle
    DeepMindï¼‰
- en: 'Aside from the already discussed [asynchronous algorithmic alignment](https://openreview.net/forum?id=ba4bbZ4KoF)
    work, there are three results we achieved this year that I am personally proudest
    of:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å·²ç»è®¨è®ºè¿‡çš„[å¼‚æ­¥ç®—æ³•å¯¹é½](https://openreview.net/forum?id=ba4bbZ4KoF)å·¥ä½œå¤–ï¼Œè¿˜æœ‰ä¸‰é¡¹æˆæœæ˜¯æˆ‘åœ¨ä»Šå¹´æœ€ä¸ºè‡ªè±ªçš„ï¼š
- en: 1ï¸âƒ£ [DAR](https://openreview.net/forum?id=tRP0Ydz5nN) showed that pre-trained
    multi-task neural algorithmic reasoners can be scalably deployed to downstream
    graph problems â€” even if they are 180,000x larger than the synthetic training
    distribution of the NAR. Whatâ€™s more, we set the state-of-the-art in modelling
    mouse brain vessels ğŸğŸ§ ğŸ©¸. NAR is **not** a victim of the bitter lesson! ğŸ“ˆ
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ [DAR](https://openreview.net/forum?id=tRP0Ydz5nN)è¯æ˜äº†é¢„è®­ç»ƒçš„å¤šä»»åŠ¡ç¥ç»ç®—æ³•æ¨ç†å™¨å¯ä»¥åœ¨ä¸‹æ¸¸å›¾é—®é¢˜ä¸­è¿›è¡Œå¯æ‰©å±•éƒ¨ç½²â€”â€”å³ä½¿å®ƒä»¬æ¯”NARçš„åˆæˆè®­ç»ƒåˆ†å¸ƒå¤§180,000å€ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨å°é¼ å¤§è„‘è¡€ç®¡å»ºæ¨¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ°´å¹³
    ğŸğŸ§ ğŸ©¸ã€‚NAR **ä¸æ˜¯**â€œè‹¦æ¶©æ•™è®­â€çš„å—å®³è€…ï¼ğŸ“ˆ
- en: 2ï¸âƒ£ [Hint-ReLIC](https://openreview.net/forum?id=kP2p67F4G7) ğŸ—¿was our response
    to the rich body of research in [no-hint models](https://openreview.net/forum?id=xkrtvHlp3P).
    We go away from the issue-ridden *hint* *autoregression* and instead model *hint
    invariants* using causal reasoning. We obtain a potent hint-based NAR, which still
    holds state-of-the-art on broad patches of CLRS-30! *â€œHints can take you a long
    way, if used in the right way.â€*
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ [Hint-ReLIC](https://openreview.net/forum?id=kP2p67F4G7) ğŸ—¿æ˜¯æˆ‘ä»¬å¯¹[æ— æç¤ºæ¨¡å‹](https://openreview.net/forum?id=xkrtvHlp3P)è¿™ä¸€ä¸°å¯Œç ”ç©¶é¢†åŸŸçš„å›åº”ã€‚æˆ‘ä»¬æ‘’å¼ƒäº†å­˜åœ¨ä¼—å¤šé—®é¢˜çš„*æç¤º*
    *è‡ªå›å½’*ï¼Œè€Œæ˜¯åˆ©ç”¨å› æœæ¨ç†æ¥å»ºæ¨¡*æç¤ºä¸å˜æ€§*ã€‚æˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªå¼ºå¤§çš„åŸºäºæç¤ºçš„NARï¼Œå¹¶ä¸”åœ¨CLRS-30çš„å¤šä¸ªé¢†åŸŸä»ä¿æŒç€æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼*â€œå¦‚æœæ­£ç¡®ä½¿ç”¨ï¼Œæç¤ºå¯ä»¥å¸¦ä½ èµ°å¾—æ›´è¿œã€‚â€*
- en: 3ï¸âƒ£ Last but not least, we took the plunge and made the first in-depth analysis
    of the [latent space representations of trained NAR models](https://openreview.net/forum?id=tRP0Ydz5nN).
    What we found was not only immensely beautiful to look at ğŸŒº but it also taught
    us a great deal about how these models work.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬è¿ˆå‡ºäº†é‡è¦ä¸€æ­¥ï¼Œé¦–æ¬¡å¯¹[è®­ç»ƒå¥½çš„NARæ¨¡å‹çš„æ½œåœ¨ç©ºé—´è¡¨ç¤º](https://openreview.net/forum?id=tRP0Ydz5nN)è¿›è¡Œäº†æ·±å…¥åˆ†æã€‚æˆ‘ä»¬å‘ç°çš„ç»“æœä¸ä»…éå¸¸ç¾è§‚ğŸŒºï¼Œè€Œä¸”è¿˜è®©æˆ‘ä»¬å­¦åˆ°äº†å¾ˆå¤šå…³äºè¿™äº›æ¨¡å‹å¦‚ä½•å·¥ä½œçš„çŸ¥è¯†ã€‚
- en: '![](../Images/2f82cfb2cc837b506d2bca4387e8b55c.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f82cfb2cc837b506d2bca4387e8b55c.png)'
- en: 'Left: Trajectory-wise PCA of eight clusters of reweighted graphs showing that
    they all contain a single dominant direction. Different clusters have different
    colors. Middle: Many embedding clusters with dominant directions overlaid in red.
    Right: Step-wise PCA of random graphs with the dominant cluster directions overlaid
    in red. Source: [MirjaniÄ‡, Pascanu, VeliÄkoviÄ‡](https://openreview.net/forum?id=tRP0Ydz5nN)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å·¦å›¾ï¼šå¯¹å…«ä¸ªé‡åŠ æƒå›¾çš„è½¨è¿¹ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œæ˜¾ç¤ºå®ƒä»¬éƒ½åŒ…å«ä¸€ä¸ªå•ä¸€çš„ä¸»å¯¼æ–¹å‘ã€‚ä¸åŒçš„èšç±»å…·æœ‰ä¸åŒçš„é¢œè‰²ã€‚ä¸­å›¾ï¼šè®¸å¤šåµŒå…¥èšç±»ï¼Œä¸»å¯¼æ–¹å‘ä»¥çº¢è‰²å åŠ ã€‚å³å›¾ï¼šéšæœºå›¾çš„é€æ­¥ä¸»æˆåˆ†åˆ†æï¼Œä¸»å¯¼èšç±»æ–¹å‘ä»¥çº¢è‰²å åŠ ã€‚æ¥æºï¼š[MirjaniÄ‡,
    Pascanu, VeliÄkoviÄ‡](https://openreview.net/forum?id=tRP0Ydz5nN)
- en: Beyond growing our vibrant community, I find it important to state that many
    of NARâ€™s foundational ideas are at the crux of important LLM methodologies; to
    name just one example, hint following is directly related to [chain-of-thought](https://arxiv.org/abs/2201.11903)
    prompting.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å£®å¤§æˆ‘ä»¬çš„å……æ»¡æ´»åŠ›çš„ç¤¾åŒºå¤–ï¼Œæˆ‘è¿˜è®¤ä¸ºæœ‰å¿…è¦å£°æ˜ï¼ŒNARçš„è®¸å¤šåŸºç¡€æ€§æ€æƒ³å¤„äºé‡è¦LLMæ–¹æ³•è®ºçš„æ ¸å¿ƒï¼›ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œæç¤ºè·Ÿéšç›´æ¥ä¸[æ€ç»´é“¾](https://arxiv.org/abs/2201.11903)æç¤ºç›¸å…³ã€‚
- en: ğŸ’¡ What I am most happy about is that in 2023, this link is getting explicit
    recognition, and ideas from NAR are now directly or indirectly influencing the
    most potent AI systems in use today. Indeed, NAR is listed as a key motivation
    for studying [length generalisation](https://arxiv.org/abs/2310.16028), and more
    broadly [generalisation on the unseen](https://arxiv.org/abs/2301.13105) *(ICMLâ€™23
    Best Paper Award)*. CLRS-30, the flagship NAR benchmark, is directly used to evaluate
    capabilities of LLMs in [neural architecture search](https://arxiv.org/abs/2302.14838)
    and [general AI research](https://arxiv.org/abs/2310.03302). And, as a final cherry
    on top, CLRS-30 is recognised as one of only seven reasoning evaluations used
    by [Gemini](https://arxiv.org/abs/2312.11805), a frontier large language model
    from Google DeepMind. I am hopeful that this is a beacon of things to come in
    2024, and that we will see even more ideas from NAR break into the design of frontier
    scalable AI models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘æœ€å¼€å¿ƒçš„æ˜¯ï¼Œåœ¨2023å¹´ï¼Œè¿™ä¸ªé“¾æ¥å¾—åˆ°äº†æ˜ç¡®çš„è®¤å¯ï¼ŒNARçš„æƒ³æ³•ç°åœ¨æ­£åœ¨ç›´æ¥æˆ–é—´æ¥åœ°å½±å“å½“ä»Šæœ€å¼ºå¤§çš„AIç³»ç»Ÿã€‚äº‹å®ä¸Šï¼ŒNARè¢«åˆ—ä¸ºç ”ç©¶[é•¿åº¦æ³›åŒ–](https://arxiv.org/abs/2310.16028)çš„å…³é”®åŠ¨æœºä¹‹ä¸€ï¼Œæ›´å¹¿æ³›åœ°è¯´ï¼Œå®ƒå¯¹[æœªè§æ•°æ®çš„æ³›åŒ–](https://arxiv.org/abs/2301.13105)æœ‰é‡è¦å½±å“ï¼ˆ*ICMLâ€™23æœ€ä½³è®ºæ–‡å¥–*ï¼‰ã€‚CLRS-30ï¼Œä½œä¸ºNARçš„æ——èˆ°åŸºå‡†ï¼Œç›´æ¥ç”¨äºè¯„ä¼°LLMåœ¨[ç¥ç»æ¶æ„æœç´¢](https://arxiv.org/abs/2302.14838)å’Œ[é€šç”¨AIç ”ç©¶](https://arxiv.org/abs/2310.03302)ä¸­çš„èƒ½åŠ›ã€‚è€Œä½œä¸ºæœ€åçš„ç‚¹ç›ä¹‹ç¬”ï¼ŒCLRS-30è¢«è®¤ä¸ºæ˜¯[Gemini](https://arxiv.org/abs/2312.11805)ï¼ˆGoogle
    DeepMindçš„å‰æ²¿å¤§è¯­è¨€æ¨¡å‹ï¼‰ä½¿ç”¨çš„ä»…æœ‰ä¸ƒä¸ªæ¨ç†è¯„ä¼°ä¹‹ä¸€ã€‚æˆ‘å¸Œæœ›è¿™èƒ½æˆä¸º2024å¹´æœªæ¥çš„ä¸€ä¸ªæŒ‡è·¯æ˜ç¯ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†çœ‹åˆ°æ›´å¤šæ¥è‡ªNARçš„æƒ³æ³•æ‰“ç ´è¿›å…¥å‰æ²¿å¯æ‰©å±•AIæ¨¡å‹çš„è®¾è®¡ä¸­ã€‚
- en: '***Liudmila Prokhorenkova (Yandex Research)***'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '***Liudmila Prokhorenkovaï¼ˆYandexç ”ç©¶å‘˜ï¼‰***'
- en: 'Throughout the year, substantial progress has been achieved on the path towards
    endowing models with various algorithmic inductive biases: the use of dual problems
    [(Numeroso et al)](https://arxiv.org/abs/2302.04496), contrastive learning techniques
    ([Bevilacqua et al](https://arxiv.org/abs/2302.10258); [Rodionov et al](https://arxiv.org/abs/2306.13411)),
    augmentation of models with data structures ([JÃ¼rÃŸ et al](https://arxiv.org/abs/2307.00337);
    [Jain et a](https://arxiv.org/abs/2307.09660)l), and in-depth examination of computational
    models [(Engelmayer et al)](https://arxiv.org/abs/2307.04049). Another important
    direction is evaluating existing models in terms of scalability and data diversity
    [(Minder et al)](https://arxiv.org/abs/2309.12253).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨å¹´é—´ï¼Œåœ¨èµ‹äºˆæ¨¡å‹å„ç§ç®—æ³•å½’çº³åè§çš„é“è·¯ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ï¼šä½¿ç”¨åŒé—®é¢˜[(Numeroso et al)](https://arxiv.org/abs/2302.04496)ï¼Œå¯¹æ¯”å­¦ä¹ æŠ€æœ¯([Bevilacqua
    et al](https://arxiv.org/abs/2302.10258); [Rodionov et al](https://arxiv.org/abs/2306.13411))ï¼Œé€šè¿‡æ•°æ®ç»“æ„å¢å¼ºæ¨¡å‹([JÃ¼rÃŸ
    et al](https://arxiv.org/abs/2307.00337); [Jain et al](https://arxiv.org/abs/2307.09660))ï¼Œä»¥åŠå¯¹è®¡ç®—æ¨¡å‹çš„æ·±å…¥ç ”ç©¶[(Engelmayer
    et al)](https://arxiv.org/abs/2307.04049)ã€‚å¦ä¸€ä¸ªé‡è¦æ–¹å‘æ˜¯è¯„ä¼°ç°æœ‰æ¨¡å‹åœ¨å¯æ‰©å±•æ€§å’Œæ•°æ®å¤šæ ·æ€§æ–¹é¢çš„è¡¨ç°[(Minder
    et al)](https://arxiv.org/abs/2309.12253)ã€‚
- en: 'â€œIn 2024 it would be great to see more comprehensive analysis and understanding
    of neural reasoners: which operations they learn, how sensitive they are to different
    shifts in data distributions, what types of mistakes they tend to make and why.â€
    â€” **Liudmila Prokhorenkova** (Yandex Research)'
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œåœ¨2024å¹´ï¼Œèƒ½å¤Ÿçœ‹åˆ°å¯¹ç¥ç»æ¨ç†å™¨è¿›è¡Œæ›´å…¨é¢çš„åˆ†æå’Œç†è§£ä¼šæ˜¯å¾ˆå¥½çš„è¿›å±•ï¼šå®ƒä»¬å­¦ä¹ äº†å“ªäº›æ“ä½œï¼Œå®ƒä»¬å¯¹æ•°æ®åˆ†å¸ƒçš„ä¸åŒå˜åŒ–æœ‰å¤šæ•æ„Ÿï¼Œå®ƒä»¬å€¾å‘äºçŠ¯å“ªäº›ç±»å‹çš„é”™è¯¯ï¼Œä»¥åŠä¸ºä½•å¦‚æ­¤ã€‚â€
    â€” **Liudmila Prokhorenkova**ï¼ˆYandex Researchï¼‰
- en: Gaining such insights may contribute to the development of even more robust
    and scalable models. Furthermore, robust neural reasoners have the potential to
    positively impact combinatorial optimization models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: è·å¾—è¿™äº›è§è§£å¯èƒ½æœ‰åŠ©äºå¼€å‘æ›´åŠ ç¨³å¥å’Œå¯æ‰©å±•çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç¨³å¥çš„ç¥ç»æ¨ç†å™¨æœ‰å¯èƒ½å¯¹ç»„åˆä¼˜åŒ–æ¨¡å‹äº§ç”Ÿç§¯æå½±å“ã€‚
- en: '**Predictions from the 2023 post**'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023å¹´åçš„é¢„æµ‹**'
- en: (1) Algorithmic reasoning tasks are likely to scale to graphs of thousands of
    nodes and practical applications like in code analysis or databases
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: (1) ç®—æ³•æ¨ç†ä»»åŠ¡å¯èƒ½ä¼šæ‰©å±•åˆ°åŒ…å«æˆåƒä¸Šä¸‡ä¸ªèŠ‚ç‚¹çš„å›¾å½¢ï¼Œå¹¶åœ¨ä»£ç åˆ†ææˆ–æ•°æ®åº“ç­‰å®é™…åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ã€‚
- en: âœ… yes, [DAR](https://openreview.net/forum?id=tRP0Ydz5nN) scales to the OGB vessel
    size
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æ˜¯çš„ï¼Œ[DAR](https://openreview.net/forum?id=tRP0Ydz5nN) èƒ½å¤Ÿæ‰©å±•åˆ°OGBèˆ¹èˆ¶å¤§å°
- en: (2) even more algorithms in the benchmark
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: (2) åŸºå‡†æµ‹è¯•ä¸­å°†å‡ºç°æ›´å¤šç®—æ³•
- en: âœ… yes, [SALSA-CLRS](https://arxiv.org/abs/2309.12253)
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: âœ… æ˜¯çš„ï¼Œ[SALSA-CLRS](https://arxiv.org/abs/2309.12253)
- en: (3) most unlikely â€” there will appear a model capable of solving quickselect
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (3) æœ€ä¸å¯èƒ½çš„æƒ…å†µâ€”â€”ä¼šå‡ºç°ä¸€ä¸ªèƒ½å¤Ÿè§£å†³quickselecté—®é¢˜çš„æ¨¡å‹
- en: âŒ still unsolved ;(
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: âŒ ä»æœªè§£å†³ ;(
- en: 'Knowledge Graphs: Inductive Reasoning is Solved?'
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ¥è¯†å›¾ï¼šå½’çº³æ¨ç†å·²è§£å†³ï¼Ÿ
- en: '*Michael Galkin (Intel) and Zhaocheng Zhu (Mila & Google)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*Michael Galkinï¼ˆè‹±ç‰¹å°”ï¼‰å’ŒZhaocheng Zhuï¼ˆMila & Googleï¼‰*'
- en: 'Since its inception in 2011, the grand challenge of KG representation learning
    was truly inductive reasoning when a **single** model would be able to run inference
    (eg, missing link prediction) on any graph without input features and without
    learning hard-coded entity/relation embedding matrices. [GraIL](https://arxiv.org/abs/1911.06962)
    (ICMLâ€™20) and [Neural Bellman-Ford Nets](https://arxiv.org/abs/2106.06935) (NeurIPSâ€™21)
    were instrumental in extending inference to unseen entities, but generalization
    to both new entities and relation types at inference time remained an unsolved
    challenge due to the main question: what can be learned and transferred when the
    whole entity/relation vocabulary can change?'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª2011å¹´æˆç«‹ä»¥æ¥ï¼ŒçŸ¥è¯†å›¾è¡¨ç¤ºå­¦ä¹ çš„é‡å¤§æŒ‘æˆ˜ä¹‹ä¸€å°±æ˜¯å½’çº³æ¨ç†ï¼šä¸€ä¸ª**å•ä¸€**æ¨¡å‹èƒ½å¤Ÿåœ¨æ²¡æœ‰è¾“å…¥ç‰¹å¾å’Œç¡¬ç¼–ç çš„å®ä½“/å…³ç³»åµŒå…¥çŸ©é˜µçš„æƒ…å†µä¸‹ï¼Œæ‰§è¡Œä»»ä½•å›¾ä¸Šçš„æ¨ç†ï¼ˆä¾‹å¦‚ï¼Œç¼ºå¤±é“¾æ¥é¢„æµ‹ï¼‰ã€‚[GraIL](https://arxiv.org/abs/1911.06962)ï¼ˆICML'20ï¼‰å’Œ[Neural
    Bellman-Ford Nets](https://arxiv.org/abs/2106.06935)ï¼ˆNeurIPS'21ï¼‰åœ¨æ‰©å±•æ¨ç†åˆ°æœªè§è¿‡çš„å®ä½“æ–¹é¢èµ·åˆ°äº†é‡è¦ä½œç”¨ï¼Œä½†ç”±äºä¸€ä¸ªå…³é”®é—®é¢˜ï¼šå½“æ•´ä¸ªå®ä½“/å…³ç³»è¯æ±‡å¯èƒ½å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå¦‚ä½•å­¦ä¹ å’Œè¿ç§»ï¼Œæ¨ç†æ—¶å¯¹æ–°å®ä½“å’Œå…³ç³»ç±»å‹çš„æ³›åŒ–ä»ç„¶æ˜¯æœªè§£å†³çš„æŒ‘æˆ˜ã€‚
- en: 'ğŸ”® Our prediction for 2023 (an inductive model fully transferable to different
    KGs with new sets of entities and relations, e.g., training on Wikidata, and running
    inference on DBpedia or Freebase) came true in several works:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® æˆ‘ä»¬å¯¹2023å¹´çš„é¢„æµ‹ï¼ˆä¸€ä¸ªå®Œå…¨å¯è½¬ç§»åˆ°ä¸åŒçŸ¥è¯†å›¾ï¼ˆKGï¼‰çš„å½’çº³æ¨¡å‹ï¼Œå¯ä»¥å¤„ç†æ–°çš„å®ä½“å’Œå…³ç³»é›†åˆï¼Œä¾‹å¦‚åœ¨Wikidataä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨DBpediaæˆ–Freebaseä¸Šè¿è¡Œæ¨ç†ï¼‰åœ¨å¤šé¡¹å·¥ä½œä¸­å¾—ä»¥éªŒè¯ï¼š
- en: '[Gao et al](https://arxiv.org/abs/2302.01313) introduced the concept of double
    equivariance that forces the neural net to be equivariant to permutations of both
    node IDs and relation IDs. The proposed ISDEA++ model employs a [DSS-GNN](https://arxiv.org/abs/2110.02910)-like
    aggregation of a relation-induced subgraph and a subgraph induced by all other
    relation types.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gao et al](https://arxiv.org/abs/2302.01313)æå‡ºäº†åŒé‡ç­‰å˜æ€§ï¼ˆdouble equivarianceï¼‰æ¦‚å¿µï¼Œå¼ºåˆ¶ç¥ç»ç½‘ç»œå¯¹èŠ‚ç‚¹IDå’Œå…³ç³»IDçš„æ’åˆ—ä¿æŒç­‰å˜æ€§ã€‚æ‰€æå‡ºçš„ISDEA++æ¨¡å‹é‡‡ç”¨äº†ç±»ä¼¼äº[DSS-GNN](https://arxiv.org/abs/2110.02910)çš„å…³ç³»è¯±å¯¼å­å›¾å’Œç”±æ‰€æœ‰å…¶ä»–å…³ç³»ç±»å‹è¯±å¯¼çš„å­å›¾çš„èšåˆã€‚'
- en: '[ULTRA](https://github.com/DeepGraphLearning/ULTRA) introduced by [Galkin et
    al](https://arxiv.org/abs/2310.04562) learns the invariance of relation interactions
    (captured by a graph of relations) and transfers to absolutely any multi-relational
    graph. ULTRA achieves SOTA results on dozens of transductive and inductive datasets
    even in the zero-shot inference setup. Besides, it enables a foundation model-like
    approach for KG reasoning with generic pre-training, zero-shot inference, and
    task-specific fine-tuning.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ULTRA](https://github.com/DeepGraphLearning/ULTRA)ç”±[Galkin et al](https://arxiv.org/abs/2310.04562)æå‡ºï¼Œå­¦ä¹ å…³ç³»äº¤äº’çš„ä¸å˜æ€§ï¼ˆé€šè¿‡å…³ç³»å›¾æ•æ‰ï¼‰å¹¶è¿ç§»åˆ°ä»»ä½•å¤šå…³ç³»å›¾ã€‚å³ä½¿åœ¨é›¶-shotæ¨ç†è®¾ç½®ä¸‹ï¼ŒULTRAä¹Ÿèƒ½åœ¨æ•°åä¸ªå½’çº³å’Œä¼ å¯¼æ•°æ®é›†ä¸Šå–å¾—SOTAç»“æœã€‚æ­¤å¤–ï¼Œå®ƒè¿˜ä¸ºKGæ¨ç†æä¾›äº†ç±»ä¼¼åŸºç¡€æ¨¡å‹çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬é€šç”¨é¢„è®­ç»ƒã€é›¶-shotæ¨ç†å’Œä»»åŠ¡ç‰¹å®šçš„å¾®è°ƒã€‚'
- en: '![](../Images/849215a4796605a7efc5a4809308de67.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/849215a4796605a7efc5a4809308de67.png)'
- en: 'Three main steps taken by ULTRA: (1) building a relation graph; (2) running
    conditional message passing over the relation graph to get relative relation representations;
    (3) use those representations for inductive link predictor GNN on the entity level.
    Source: [Galkin et al](https://arxiv.org/abs/2310.04562)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ULTRAçš„ä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼šï¼ˆ1ï¼‰æ„å»ºå…³ç³»å›¾ï¼›ï¼ˆ2ï¼‰åœ¨å…³ç³»å›¾ä¸Šè¿›è¡Œæ¡ä»¶æ¶ˆæ¯ä¼ é€’ä»¥è·å–ç›¸å¯¹çš„å…³ç³»è¡¨ç¤ºï¼›ï¼ˆ3ï¼‰åˆ©ç”¨è¿™äº›è¡¨ç¤ºä½œä¸ºå½’çº³é“¾æ¥é¢„æµ‹GNNåœ¨å®ä½“å±‚é¢ä¸Šçš„è¾“å…¥ã€‚æ¥æºï¼š[Galkin
    et al](https://arxiv.org/abs/2310.04562)
- en: 'Learn more about inductive reasoning in the recent blog post:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤šå…³äºå½’çº³æ¨ç†çš„ä¿¡æ¯ï¼Œè¯¦è§æœ€è¿‘çš„åšå®¢æ–‡ç« ï¼š
- en: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
    [## ULTRA: Foundation Models for Knowledge Graph Reasoning'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
    [## ULTRA: çŸ¥è¯†å›¾è°±æ¨ç†çš„åŸºç¡€æ¨¡å‹'
- en: One model to rule them all
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹ç»Ÿæ²»æ‰€æœ‰
- en: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/ultra-foundation-models-for-knowledge-graph-reasoning-9f8f4a0d7f09?source=post_page-----1ed786f7bf63--------------------------------)
- en: As the grand challenge seems to be solved now, is there anything left for KG
    research, or we should call it a day, throw a party, and move on?
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è¿™ä¸ªé‡å¤§æŒ‘æˆ˜ä¼¼ä¹å·²ç»è§£å†³ï¼Œè¿˜æœ‰ä»€ä¹ˆç•™ç»™çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ç ”ç©¶çš„å—ï¼Ÿæˆ‘ä»¬æ˜¯ä¸æ˜¯è¯¥åº†ç¥ä¸€ä¸‹ï¼Œç»“æŸè¿™ä¸€å¤©ï¼Œå¼€ä¸ªæ´¾å¯¹ï¼Œç„¶åç»§ç»­å‰è¿›ï¼Ÿ
- en: '***Michael Galkin (Intel)***'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '***Michael Galkin (Intel)***'
- en: â€œIndeed, with the grand challenge solved, it feels a bit like an existential
    crisis â€” everything important is invented, Graph ML enabled things that looked
    impossible just 5 years ago. Perhaps, KG community should re-invent itself and
    focus on practical problems that can be tackled with graph foundation models.
    Otherwise, the subfield would disappear from research radars like Semantic Webâ€
    â€” Michael Galkin (Intel)
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œäº‹å®ä¸Šï¼Œéšç€è¿™ä¸ªé‡å¤§æŒ‘æˆ˜çš„è§£å†³ï¼Œæ„Ÿè§‰æœ‰äº›åƒæ˜¯ä¸€æ¬¡ç”Ÿå­˜å±æœºâ€”â€”ä¸€åˆ‡é‡è¦çš„ä¸œè¥¿éƒ½å·²ç»å‘æ˜äº†ï¼Œå›¾å½¢æœºå™¨å­¦ä¹ ä½¿å¾—äº”å¹´å‰çœ‹ä¼¼ä¸å¯èƒ½çš„äº‹æƒ…å˜æˆäº†ç°å®ã€‚ä¹Ÿè®¸ï¼ŒKGç¤¾åŒºåº”è¯¥é‡æ–°å‘æ˜è‡ªå·±ï¼Œä¸“æ³¨äºé‚£äº›å¯ä»¥é€šè¿‡å›¾åŸºç¡€æ¨¡å‹è§£å†³çš„å®é™…é—®é¢˜ã€‚å¦åˆ™ï¼Œå­é¢†åŸŸå°†åƒè¯­ä¹‰ç½‘é‚£æ ·ä»ç ”ç©¶é›·è¾¾ä¸­æ¶ˆå¤±ã€‚â€â€”â€”Michael
    Galkinï¼ˆIntelï¼‰
- en: Transductive and shallow KG embeddings are dead and nobody in 2024 should work
    on them, it is time to retire them for good. ULTRA-like foundation models can
    now work without training on any graph which is a sweet spot for many closed enterprise
    KGs.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: å½’çº³æ€§å’Œæµ…å±‚KGåµŒå…¥å·²ç»è¿‡æ—¶ï¼Œ2024å¹´æ²¡æœ‰äººåº”è¯¥å†ç ”ç©¶å®ƒä»¬ï¼Œæ˜¯æ—¶å€™æ°¸ä¹…é€€ä¼‘äº†ã€‚ç±»ä¼¼ULTRAçš„åŸºç¡€æ¨¡å‹ç°åœ¨å¯ä»¥åœ¨ä»»ä½•å›¾ä¸Šè¿è¡Œï¼Œè€Œä¸éœ€è¦è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯¹äºè®¸å¤šå°é—­ä¼ä¸šçš„KGæ¥è¯´æ˜¯ä¸€ä¸ªç†æƒ³çš„é€‰æ‹©ã€‚
- en: â¡ï¸ The last uncharted territory is inductive reasoning beyond simple link prediction
    ([complex database-like logical queries](https://medium.com/towards-data-science/neural-graph-databases-cc35c9e1d04f))
    and I think it will also be solved in 2024\. Adding temporal aspects, LLM node
    features, or scaling GNNs for larger graphs is a question of time and presents
    more of an engineering task than a research question.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æœ€åä¸€ä¸ªæœªæ¢ç´¢çš„é¢†åŸŸæ˜¯è¶…è¶Šç®€å•é“¾æ¥é¢„æµ‹çš„å½’çº³æ¨ç†ï¼ˆ[å¤æ‚çš„ç±»ä¼¼æ•°æ®åº“çš„é€»è¾‘æŸ¥è¯¢](https://medium.com/towards-data-science/neural-graph-databases-cc35c9e1d04f)ï¼‰ï¼Œæˆ‘è®¤ä¸ºå®ƒå°†åœ¨2024å¹´è§£å†³ã€‚åŠ å…¥æ—¶é—´ç»´åº¦ã€LLMèŠ‚ç‚¹ç‰¹å¾æˆ–æ‰©å¤§GNNä»¥å¤„ç†æ›´å¤§å›¾çš„ä»»åŠ¡ï¼Œå·²æˆä¸ºæ—¶é—´é—®é¢˜ï¼Œå¹¶ä¸”æ›´å¤šçš„æ˜¯å·¥ç¨‹ä»»åŠ¡ï¼Œè€Œéç ”ç©¶é—®é¢˜ã€‚
- en: '***Zhaocheng Zhu (Mila & Google)***'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '***Zhaocheng Zhu (Mila & Google)***'
- en: â€œWith the rise of LLMs and numerous prompt-based reasoning techniques, it looks
    like **KG reasoning is coming to an end**. Texts are more expressive and flexible
    than KGs, and meanwhile they are more available in quantity. However, I donâ€™t
    think the reasoning techniques that the KG community developed are in vain.â€ â€”
    Zhaocheng Zhu (Mila & Google)
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œéšç€LLMå’Œä¼—å¤šåŸºäºæç¤ºçš„æ¨ç†æŠ€æœ¯çš„å…´èµ·ï¼Œä¼¼ä¹**çŸ¥è¯†å›¾è°±æ¨ç†å·²ç»èµ°åˆ°äº†å°½å¤´**ã€‚æ–‡æœ¬æ¯”çŸ¥è¯†å›¾è°±æ›´å…·è¡¨ç°åŠ›å’Œçµæ´»æ€§ï¼ŒåŒæ—¶å®ƒä»¬åœ¨æ•°é‡ä¸Šä¹Ÿæ›´åŠ ä¸°å¯Œã€‚ç„¶è€Œï¼Œæˆ‘ä¸è®¤ä¸ºçŸ¥è¯†å›¾è°±ç¤¾åŒºå¼€å‘çš„æ¨ç†æŠ€æœ¯æ˜¯å¾’åŠ³çš„ã€‚â€â€”â€”æœ±å…†è¯šï¼ˆMila
    & è°·æ­Œï¼‰
- en: â¡ï¸ We see that many LLM reasoning methods coincide with well-known ideas on
    KGs. For instance, the difference between direct prompting and chain-of-thought
    (CoT) shares much spirit with embedding methods and path-based methods on KGs,
    where the latter ones parameterize smaller steps and thereby generalize better
    to new combinations of steps. In fact, topics like inductive and multi-step generalization
    were explored on KGs several years earlier than on LLMs.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æˆ‘ä»¬çœ‹åˆ°è®¸å¤šLLMæ¨ç†æ–¹æ³•ä¸çŸ¥è¯†å›¾è°±ä¸­çš„ä¸€äº›è‘—åæ€æƒ³ç›¸å»åˆã€‚ä¾‹å¦‚ï¼Œç›´æ¥æç¤ºä¸æ€ç»´é“¾ï¼ˆCoTï¼‰ä¹‹é—´çš„åŒºåˆ«ï¼Œä¸çŸ¥è¯†å›¾è°±ä¸­çš„åµŒå…¥æ–¹æ³•å’ŒåŸºäºè·¯å¾„çš„æ–¹æ³•æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ï¼Œåè€…é€šè¿‡å‚æ•°åŒ–æ›´å°çš„æ­¥éª¤ï¼Œä»è€Œèƒ½å¤Ÿæ›´å¥½åœ°æ¨å¹¿åˆ°æ–°çš„æ­¥éª¤ç»„åˆã€‚äº‹å®ä¸Šï¼Œå½’çº³æ€§å’Œå¤šæ­¥éª¤æ¨å¹¿ç­‰ä¸»é¢˜ï¼Œæ¯”LLMæ›´æ—©å‡ å¹´å°±åœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œäº†æ¢è®¨ã€‚
- en: When we develop new techniques for LLMs, it is essential to take a glance at
    similar goals and solutions on KGs. In brief, while the modality of KGs *may fade
    at some point*, the insights we learned from KG reasoning will continue to illuminate
    in the era of LLMs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä¸ºLLMå¼€å‘æ–°æŠ€æœ¯æ—¶ï¼Œå›é¡¾çŸ¥è¯†å›¾è°±ï¼ˆKGï¼‰ä¸­ç±»ä¼¼ç›®æ ‡å’Œè§£å†³æ–¹æ¡ˆæ˜¯è‡³å…³é‡è¦çš„ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå°½ç®¡çŸ¥è¯†å›¾è°±çš„è¡¨ç°å½¢å¼*å¯èƒ½ä¼šåœ¨æŸä¸ªæ—¶åˆ»æ¶ˆå¤±*ï¼Œä½†æˆ‘ä»¬ä»çŸ¥è¯†å›¾è°±æ¨ç†ä¸­å­¦åˆ°çš„æ´è§å°†ç»§ç»­åœ¨LLMæ—¶ä»£å‘æŒ¥ä½œç”¨ã€‚
- en: Temporal Graph Learning
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—¶åºå›¾å­¦ä¹ 
- en: Shenyang Huang, Emanuele Rossi, Andrea Cini, Ingo Scholtes, and Michael Galkin
    prepared a separate overview post on temporal graph learning!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: é»„æ²ˆé˜³ã€åŸƒé©¬çº½åŸƒå°”Â·ç½—è¥¿ã€å®‰å¾·çƒˆäºšÂ·å¥‡å°¼ã€å› æˆˆÂ·èˆ’å°”ç‰¹æ–¯å’Œè¿ˆå…‹å°”Â·é«˜å°”é‡‘å‡†å¤‡äº†ä¸€ç¯‡å…³äºæ—¶åºå›¾å­¦ä¹ çš„ç‹¬ç«‹æ¦‚è¿°æ–‡ç« ï¼
- en: '[](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
    [## Temporal Graph Learning in 2024'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
    [## 2024å¹´æ—¶åºå›¾å­¦ä¹ '
- en: Continue the journey for evolving networks
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç»§ç»­æ¨è¿›ç½‘ç»œçš„æ¼”å˜ä¹‹æ—…
- en: towardsdatascience.com](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/temporal-graph-learning-in-2024-feaa9371b8e2?source=post_page-----1ed786f7bf63--------------------------------)
- en: LLMs + Graphs for Scientific Discovery
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM + å›¾ç”¨äºç§‘å­¦å‘ç°
- en: '*Michael Galkin (Intel)*'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¿ˆå…‹å°”Â·é«˜å°”é‡‘ï¼ˆè‹±ç‰¹å°”ï¼‰*'
- en: ğŸ’¡LLMs were everywhere in 2023 and itâ€™s hard to miss the ğŸ˜ in the room.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡2023å¹´LLMæ— å¤„ä¸åœ¨ï¼Œå¾ˆéš¾å¿½è§†æˆ¿é—´é‡Œçš„å¤§è±¡ğŸ˜ã€‚
- en: â€œWe have seen a flurry of approaches trying to marry graphs with LLMs. The subfield
    is emerging and **making its tiny baby steps** which are important to acknowledge.â€
    â€” Michael Galkin (Intel)
  id: totrans-272
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘ä»¬å·²ç»çœ‹åˆ°è®¸å¤šæ–¹æ³•å°è¯•å°†å›¾ä¸LLMç»“åˆã€‚è¿™ä¸ªå­é¢†åŸŸæ­£åœ¨å‘å±•ï¼Œå¹¶ä¸”**è¿ˆå‡ºäº†å®ƒçš„å°æ­¥ä¼**ï¼Œè¿™æ˜¯éœ€è¦è®¤å¯çš„ã€‚â€â€”â€”è¿ˆå…‹å°”Â·é«˜å°”é‡‘ï¼ˆè‹±ç‰¹å°”ï¼‰
- en: We have seen a flurry of approaches trying to marry graphs with LLMs (sometimes
    literally verbalizing the edges in a text prompt) where straightforward prompting
    with edge index does not really work for running graph algorithms with language
    models, so the crux is in the â€œtext linearizationâ€ and proper prompting. Among
    the notable mentions, you might be interested in **GraphText** by [Zhao et al](https://arxiv.org/abs/2310.01089)
    that devises a *graph syntax tree* prompt constructed from features and labels
    in the ego-subgraph of a target node â€” GraphText works for node classification.
    In **Talk Like a Graph** by [Fatemi et al](https://arxiv.org/abs/2310.04560) the
    authors study graph linearization strategies and how they impact LLM performance
    on basic tasks like edge existence, node count, or cycle check.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°è®¸å¤šæ–¹æ³•å°è¯•å°†å›¾ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»“åˆï¼ˆæœ‰æ—¶å­—é¢ä¸Šæ˜¯åœ¨æ–‡æœ¬æç¤ºä¸­è¡¨è¾¾è¾¹ç¼˜ï¼‰ï¼Œå…¶ä¸­ç›´æ¥é€šè¿‡è¾¹ç´¢å¼•è¿›è¡Œæç¤ºå¹¶ä¸çœŸæ­£é€‚ç”¨äºè¿è¡Œå›¾ç®—æ³•ä¸è¯­è¨€æ¨¡å‹ï¼Œå› æ­¤å…³é”®åœ¨äºâ€œæ–‡æœ¬çº¿æ€§åŒ–â€å’Œåˆé€‚çš„æç¤ºã€‚åœ¨ä¸€äº›å€¼å¾—æ³¨æ„çš„ç ”ç©¶ä¸­ï¼Œä½ å¯èƒ½ä¼šå¯¹[èµµç­‰äºº](https://arxiv.org/abs/2310.01089)çš„**GraphText**æ„Ÿå…´è¶£ï¼Œè¯¥æ–¹æ³•è®¾è®¡äº†ä¸€ç§ç”±ç‰¹å¾å’Œæ ‡ç­¾æ„æˆçš„*å›¾è¯­æ³•æ ‘*æç¤ºï¼Œè¿™äº›ç‰¹å¾å’Œæ ‡ç­¾æ¥è‡ªç›®æ ‡èŠ‚ç‚¹çš„è‡ªæˆ‘å­å›¾â€”â€”GraphTexté€‚ç”¨äºèŠ‚ç‚¹åˆ†ç±»ã€‚åœ¨[Fatemiç­‰äºº](https://arxiv.org/abs/2310.04560)çš„**Talk
    Like a Graph**ä¸­ï¼Œä½œè€…ç ”ç©¶äº†å›¾çš„çº¿æ€§åŒ–ç­–ç•¥åŠå…¶å¦‚ä½•å½±å“LLMåœ¨åŸºæœ¬ä»»åŠ¡ï¼ˆå¦‚è¾¹çš„å­˜åœ¨ã€èŠ‚ç‚¹è®¡æ•°æˆ–ç¯æ£€æŸ¥ï¼‰ä¸Šçš„è¡¨ç°ã€‚
- en: '![](../Images/4c313a55121a3192694ee0c6d387a706.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c313a55121a3192694ee0c6d387a706.png)'
- en: 'Standard GNNs (left) and GraphText (right). GraphText encodes the graph information
    into text sequences and uses LLM to perform inference. The graph-syntax tree contains
    both node attributes (e.g. feature and label) and relationships (e.g. center-node,
    1st-hop, and 2nd-hop). Source: [Zhao et al](https://arxiv.org/abs/2310.01089)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†GNNï¼ˆå·¦ï¼‰ä¸GraphTextï¼ˆå³ï¼‰ã€‚GraphTextå°†å›¾ä¿¡æ¯ç¼–ç æˆæ–‡æœ¬åºåˆ—ï¼Œå¹¶ä½¿ç”¨LLMè¿›è¡Œæ¨ç†ã€‚å›¾è¯­æ³•æ ‘åŒ…å«èŠ‚ç‚¹å±æ€§ï¼ˆå¦‚ç‰¹å¾å’Œæ ‡ç­¾ï¼‰ä»¥åŠå…³ç³»ï¼ˆå¦‚ä¸­å¿ƒèŠ‚ç‚¹ã€ç¬¬ä¸€è·³å’Œç¬¬äºŒè·³ï¼‰ã€‚æ¥æºï¼š[èµµç­‰äºº](https://arxiv.org/abs/2310.01089)
- en: â¡ï¸ Despite the early stage, there exist already 3 recent surveys ([Li et al](https://arxiv.org/abs/2311.12399),
    [Jin et al](https://arxiv.org/abs/2312.02783), [Sun et al](https://arxiv.org/abs/2311.16534))
    covering dozens of prompting approaches for graphs. Generally, it is yet to be
    seen **whether** **LLMs are an appropriate hammer** ğŸ”¨ for a specific *graph* nail
    given all the limitations of the autoregressive decoding, small context sizes,
    and permutation-invariant nature of graph tasks. If you are broadly interested
    in LLM reasoning, check out [our recent blog post](/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d)
    covering the main areas and progress made in 2023.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å°½ç®¡å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†å·²ç»æœ‰ä¸‰ç¯‡è¿‘æœŸçš„ç»¼è¿°æ–‡ç« ï¼ˆ[Liç­‰äºº](https://arxiv.org/abs/2311.12399)ï¼Œ[Jinç­‰äºº](https://arxiv.org/abs/2312.02783)ï¼Œ[Sunç­‰äºº](https://arxiv.org/abs/2311.16534)ï¼‰æ¶µç›–äº†å¤šç§å›¾çš„æç¤ºæ–¹æ³•ã€‚æ€»ä½“è€Œè¨€ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥š**LLMæ˜¯å¦é€‚åˆç”¨ä½œ**
    ğŸ”¨ ç‰¹å®š*å›¾*ä»»åŠ¡çš„åˆé€‚å·¥å…·ï¼Œå› ä¸ºè‡ªå›å½’è§£ç ã€å°ä¸Šä¸‹æ–‡çª—å£å’Œå›¾ä»»åŠ¡çš„æ’åˆ—ä¸å˜æ€§ç­‰é™åˆ¶ã€‚å¦‚æœä½ å¯¹LLMæ¨ç†æœ‰å¹¿æ³›å…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[æˆ‘ä»¬æœ€è¿‘çš„åšå®¢æ–‡ç« ](/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d)ï¼Œå®ƒæ€»ç»“äº†2023å¹´åœ¨è¿™ä¸€é¢†åŸŸçš„ä¸»è¦è¿›å±•ã€‚
- en: 'â¡ï¸ LLMs in applied scientific tasks exhibit more promising, sometimes quite
    unexpected results: **ChemCrow** ğŸ¦â€â¬› by [Bran, Cox, et al](https://arxiv.org/abs/2304.05376)
    is an LLM agent powered with tools that can perform tasks in organic chemistry,
    synthesis, and material design right in natural language (without fancy equivariant
    GNNs). For example, with a query â€œ*Find and synthesize a thiourea organocatalyst
    which accelerates a Diels-Alder reaction*â€ ChemCrow devises a sequence of actions
    starting from a basic SMILES string and ending up with instructions to a synthesis
    platform.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ LLMåœ¨åº”ç”¨ç§‘å­¦ä»»åŠ¡ä¸­å±•ç°äº†æ›´æœ‰å‰æ™¯çš„ã€æœ‰æ—¶ç”šè‡³æ˜¯æ„æƒ³ä¸åˆ°çš„ç»“æœï¼š[Bran, Coxç­‰äºº](https://arxiv.org/abs/2304.05376)çš„**ChemCrow**
    ğŸ¦â€â¬›æ˜¯ä¸€ä¸ªç”±å·¥å…·é©±åŠ¨çš„LLMä»£ç†ï¼Œå¯ä»¥ç›´æ¥ç”¨è‡ªç„¶è¯­è¨€æ‰§è¡Œæœ‰æœºåŒ–å­¦ã€åˆæˆå’Œææ–™è®¾è®¡ä»»åŠ¡ï¼ˆæ— éœ€å¤æ‚çš„ç­‰å˜GNNï¼‰ã€‚ä¾‹å¦‚ï¼ŒæŸ¥è¯¢â€œ*å¯»æ‰¾å¹¶åˆæˆä¸€ç§èƒ½å¤ŸåŠ é€ŸDiels-Alderååº”çš„ç¡«è„²æœ‰æœºå‚¬åŒ–å‰‚*â€æ—¶ï¼ŒChemCrowä»åŸºæœ¬çš„SMILESå­—ç¬¦ä¸²å¼€å§‹ï¼Œè®¾è®¡å‡ºä¸€ç³»åˆ—æ“ä½œï¼Œæœ€ç»ˆç»™å‡ºåˆæˆå¹³å°çš„æ“ä½œæŒ‡ä»¤ã€‚
- en: Similarly, [Gruver et al](https://openreview.net/forum?id=0r5DE2ZSwJ) fine-tuned
    LLaMA-2 to generate 3D crystal structures as a plain text file with lattice parameters,
    atomic composition, and 3D coordinates and it is surprisingly competitive with
    SOTA geometric diffusion models like CDVAE.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œ[Gruverç­‰äºº](https://openreview.net/forum?id=0r5DE2ZSwJ)å¯¹LLaMA-2è¿›è¡Œäº†å¾®è°ƒï¼Œä½¿å…¶ç”ŸæˆåŒ…å«æ™¶æ ¼å‚æ•°ã€åŸå­ç»„æˆå’Œ3Dåæ ‡çš„3Dæ™¶ä½“ç»“æ„çº¯æ–‡æœ¬æ–‡ä»¶ï¼Œä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®ƒä¸åƒCDVAEè¿™æ ·çš„SOTAå‡ ä½•æ‰©æ•£æ¨¡å‹ç«äº‰åŠ›åè¶³ã€‚
- en: '![](../Images/0cf27820c04f943ef14d5fc406665457.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cf27820c04f943ef14d5fc406665457.png)'
- en: 'Experimental validation. a) Example of the script run by a user to initiate
    ChemCrow. b) Query and synthesis of a thiourea organocatalyst. c) The IBM Research
    RoboRXN synthesis platform on which the experiments were executed (pictures reprinted
    courtesy of International Business Machines Corporation). d) Experimentally validated
    compounds. Source: [Bran, Cox, et al](https://arxiv.org/abs/2304.05376)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: å®éªŒéªŒè¯ã€‚a) ç”¨æˆ·è¿è¡Œè„šæœ¬ä»¥å¯åŠ¨ChemCrowçš„ç¤ºä¾‹ã€‚b) å¯¹ç¡«è„²æœ‰æœºå‚¬åŒ–å‰‚çš„æŸ¥è¯¢ä¸åˆæˆã€‚c) å®éªŒæ‰§è¡Œçš„IBMç ”ç©¶RoboRXNåˆæˆå¹³å°ï¼ˆå›¾ç‰‡è½¬è½½è‡ªå›½é™…å•†ä¸šæœºå™¨å…¬å¸ï¼‰ã€‚d)
    å®éªŒéªŒè¯çš„åŒ–åˆç‰©ã€‚æ¥æºï¼š[Bran, Coxç­‰äºº](https://arxiv.org/abs/2304.05376)
- en: 'ğŸ”® In 2024, scientific applications of LLMs are likely to expand both breadth-wise
    and depth-wise:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® åœ¨2024å¹´ï¼ŒLLMçš„ç§‘å­¦åº”ç”¨é¢„è®¡å°†åœ¨å¹¿åº¦å’Œæ·±åº¦ä¸Šéƒ½å¾—åˆ°æ‰©å±•ï¼š
- en: 1ï¸âƒ£ Reaching out to more AI4Science areas;
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æ‰©å±•åˆ°æ›´å¤šçš„AI4Scienceé¢†åŸŸï¼›
- en: 2ï¸âƒ£ Integration with geometric foundation models (since multi-modality is the
    main LLM focus for the coming year);
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä¸å‡ ä½•åŸºç¡€æ¨¡å‹çš„é›†æˆï¼ˆå› ä¸ºå¤šæ¨¡æ€æ˜¯æœªæ¥ä¸€å¹´LLMçš„ä¸»è¦å…³æ³¨ç‚¹ï¼‰ï¼›
- en: '3ï¸âƒ£ Hot take: LLMs will solve the *quickselect* task in the CLRS-30 benchmark
    before GNNs do ğŸ”¥'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ çƒ­ç‚¹è¯é¢˜ï¼šLLMå°†æ¯”GNNæ›´å¿«åœ°è§£å†³CLRS-30åŸºå‡†ä¸­çš„*quickselect*ä»»åŠ¡ğŸ”¥
- en: Cool GNN Applications
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é…·ç‚«çš„GNNåº”ç”¨
- en: '*Petar VeliÄkoviÄ‡ (Google DeepMind)*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*Petar VeliÄkoviÄ‡ï¼ˆGoogle DeepMindï¼‰*'
- en: In my standard deck motivating the use of GNNs to a broader audience, I rely
    on a usual â€œarsenalâ€ slide of impactful GNN applications over the years. With
    2023 being significantly marked by LLM developments, I was wondering â€” can I meaningfully
    update this slide, but only using models released this year?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ç”¨GNNå‘æ›´å¹¿æ³›çš„è§‚ä¼—å±•ç¤ºå…¶åº”ç”¨æ—¶ï¼Œæˆ‘é€šå¸¸ä¾èµ–ä¸€å¼ â€œå…µå™¨åº“â€å¹»ç¯ç‰‡ï¼Œå±•ç¤ºè¿™äº›å¹´æ¥å½±å“æ·±è¿œçš„GNNåº”ç”¨ã€‚éšç€2023å¹´LLMå‘å±•çš„æ˜¾è‘—æ ‡å¿—ï¼Œæˆ‘åœ¨æƒ³â€”â€”æˆ‘èƒ½å¦ä»…ä½¿ç”¨ä»Šå¹´å‘å¸ƒçš„æ¨¡å‹ï¼Œæ¥æœ‰æ„ä¹‰åœ°æ›´æ–°è¿™å¼ å¹»ç¯ç‰‡å‘¢ï¼Ÿ
- en: â€œIt was the middle of the year back then, and already I was in for a nice surprise;
    *I did not have enough space to list all the awesome things done with GNNs!â€ â€”*
    **Petar VeliÄkoviÄ‡** (Google DeepMind)
  id: totrans-288
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œé‚£æ—¶æ­£æ˜¯å¹´ä¸­çš„æ—¶å€™ï¼Œæˆ‘å·²ç»æœ‰äº†ä¸€ä¸ªæƒŠå–œï¼›*æˆ‘æ²¡æœ‰è¶³å¤Ÿçš„ç©ºé—´åˆ—å‡ºæ‰€æœ‰ä½¿ç”¨GNNåšå‡ºçš„ç²¾å½©æˆæœï¼*â€â€”â€”**Petar VeliÄkoviÄ‡**ï¼ˆGoogle
    DeepMindï¼‰
- en: ğŸ’¡ While it might have gone comparatively under the radar, I confidently claim
    that 2023 was the **most exciting year** for cool GNN applications! The rise of
    LLMs just made it very clear where the limits of text-based autoregressive models
    are, and that for most scientific problems coming from Nature, their graph structure
    cannot be ignored.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ è™½ç„¶è¿™ä¸€ç‚¹å¯èƒ½ç›¸å¯¹ä¸å¤ªå¼•èµ·æ³¨æ„ï¼Œä½†æˆ‘è‡ªä¿¡åœ°å£°ç§°ï¼Œ2023å¹´æ˜¯**æœ€æ¿€åŠ¨äººå¿ƒçš„ä¸€å¹´**ï¼Œå› ä¸ºåœ¨è®¸å¤šé…·ç‚«çš„GNNåº”ç”¨ä¸­ï¼LLMçš„å…´èµ·æ¸…æ¥šåœ°è¡¨æ˜äº†åŸºäºæ–‡æœ¬çš„è‡ªå›å½’æ¨¡å‹çš„å±€é™æ€§ï¼Œè€Œå¯¹äºå¤§å¤šæ•°æ¥è‡ªè‡ªç„¶ç•Œçš„ç§‘å­¦é—®é¢˜ï¼Œå…¶å›¾ç»“æ„æ˜¯ä¸èƒ½è¢«å¿½è§†çš„ã€‚
- en: 'Hereâ€™s a handful of my personal favourite landmark results â€” all published
    in top-tier venues:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œåˆ—å‡ºçš„æ˜¯æˆ‘ä¸ªäººæœ€å–œæ¬¢çš„å‡ ä¸ªæ ‡å¿—æ€§æˆæœâ€”â€”æ‰€æœ‰è¿™äº›éƒ½å‘è¡¨åœ¨é¡¶çº§æœŸåˆŠä¸Šï¼š
- en: '[GraphCast](https://www.science.org/doi/10.1126/science.adi2336) provided us
    a landmark model for medium-range global weather forecasting â›ˆï¸ and with it, more
    accurate foreshadowing of extreme events such as hurricanes. A highly well-deserved
    cover of Science!'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GraphCast](https://www.science.org/doi/10.1126/science.adi2336)ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ ‡å¿—æ€§çš„æ¨¡å‹ï¼Œç”¨äºä¸­èŒƒå›´çš„å…¨çƒå¤©æ°”é¢„æŠ¥â›ˆï¸ï¼Œå¹¶å€Ÿæ­¤æ›´åŠ å‡†ç¡®åœ°é¢„æµ‹æç«¯äº‹ä»¶ï¼Œå¦‚é£“é£ã€‚è¿™æ˜¯ã€Šç§‘å­¦ã€‹æ‚å¿—å°é¢ä¸Šåº”å¾—çš„è£èª‰ï¼'
- en: In an outstanding development in materials science, [GNoME](https://www.nature.com/articles/s41586-023-06735-9)
    uses a GNN-based model to discover *millions* of novel crystal structures ğŸ’ â€”
    an *â€œorder-of-magnitude expansion in stable materials known to humanityâ€*. Published
    in Nature.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ææ–™ç§‘å­¦é¢†åŸŸçš„ä¸€ä¸ªæ°å‡ºå‘å±•ä¸­ï¼Œ[GNoME](https://www.nature.com/articles/s41586-023-06735-9)ä½¿ç”¨åŸºäºGNNçš„æ¨¡å‹å‘ç°äº†*æ•°ç™¾ä¸‡*ç§æ–°å‹æ™¶ä½“ç»“æ„ğŸ’â€”â€”è¿™è¢«ç§°ä¸º*â€œäººç±»å·²çŸ¥ç¨³å®šææ–™çš„æ•°é‡çº§æ‰©å±•â€*ã€‚å·²å‘è¡¨åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ã€‚
- en: Weâ€™ve been treated to not just [one](https://www.nature.com/articles/s41589-023-01349-8),
    but [two](https://www.nature.com/articles/s41586-023-06887-8) new breakthroughs
    in antibiotic discovery ğŸ’Š using message passing neural networks â€” the latter being
    published in Nature!
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä»…è¿æ¥äº†[ä¸€é¡¹](https://www.nature.com/articles/s41589-023-01349-8)ï¼Œè€Œä¸”è¿æ¥äº†[ä¸¤é¡¹](https://www.nature.com/articles/s41586-023-06887-8)åœ¨æŠ—ç”Ÿç´ å‘ç°æ–¹é¢çš„çªç ´ğŸ’Šï¼Œè¿™ä¸¤é¡¹æˆæœéƒ½é‡‡ç”¨äº†æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œâ€”â€”åè€…å·²å‘è¡¨åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ï¼
- en: '[GNNs can smell](https://www.science.org/doi/10.1126/science.ade4401) ğŸ‘ƒ by
    observing the molecular structure emitting an odour â€” a result that may well revolutionise
    many industries, including perfumes! Published in Science.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GNNå¯ä»¥å—…è§‰](https://www.science.org/doi/10.1126/science.ade4401) ğŸ‘ƒ é€šè¿‡è§‚å¯Ÿåˆ†å­ç»“æ„å‘å‡ºçš„æ°”å‘³â€”â€”è¿™ä¸€å‘ç°å¯èƒ½ä¼šå½»åº•æ”¹å˜å¤šä¸ªè¡Œä¸šï¼ŒåŒ…æ‹¬é¦™æ°´è¡Œä¸šï¼å·²å‘è¡¨åœ¨ã€Šç§‘å­¦ã€‹æ‚å¿—ã€‚'
- en: On the cover of Nature Machine Intelligence, [HYFA](https://www.nature.com/articles/s42256-023-00684-8)
    ğŸ„ shows how to use hypergraph factorisation to make significant progress in gene
    expression imputation ğŸ§¬!
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ã€Šè‡ªç„¶æœºå™¨æ™ºèƒ½ã€‹æ‚å¿—çš„å°é¢ä¸Šï¼Œ[HYFA](https://www.nature.com/articles/s42256-023-00684-8)ğŸ„å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨è¶…å›¾åˆ†è§£æ³•åœ¨åŸºå› è¡¨è¾¾å¡«è¡¥é—®é¢˜ä¸Šå–å¾—æ˜¾è‘—è¿›å±•ğŸ§¬ï¼
- en: Last but not least, particle physics âš›ï¸ remains a natural stronghold of GNN
    applications. In this yearâ€™s Nature Physics Review, we have been treated to a
    [fascinating survey](https://www.nature.com/articles/s42254-023-00569-0) elucidating
    the myriad of ways how graph neural networks are deployed for various data analysis
    tasks at the Large Hadron Collider âš¡.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯ï¼Œç²’å­ç‰©ç†å­¦âš›ï¸ä»ç„¶æ˜¯GNNåº”ç”¨çš„è‡ªç„¶å¼ºé¡¹ã€‚åœ¨ä»Šå¹´çš„ã€Šè‡ªç„¶ç‰©ç†å­¦è¯„è®ºã€‹ä¸Šï¼Œæˆ‘ä»¬æ¬£èµåˆ°äº†ä¸€ç¯‡[ç²¾å½©çš„ç»¼è¿°](https://www.nature.com/articles/s42254-023-00569-0)ï¼Œé˜æ˜äº†å›¾ç¥ç»ç½‘ç»œåœ¨å¤§å‹å¼ºå­å¯¹æ’æœºâš¡ä¸­è¿›è¡Œå„ç§æ•°æ®åˆ†æä»»åŠ¡çš„å¤šç§åº”ç”¨æ–¹å¼ã€‚
- en: âš½ My own humble contribution to the space of GNN applications this year was
    [TacticAI](https://arxiv.org/abs/2310.10553), the *first full AI system giving
    useful tactical suggestions to (association) football coaches*, developed in partnership
    with our collaborators at Liverpool FC ğŸ”´. TacticAI is capable of both predictive
    modelling (*â€œwhat will happen in this tactical scenario?â€*), retrieving similar
    tactics, and conditional generative modelling (*â€œhow to modify player positions
    to make a particular outcome happen?â€*). In my opinion, the most satisfying part
    of this very fun collaboration was our user study with some of LFCâ€™s top coaching
    staff â€” directly illustrating that the outputs of our model will be of use to
    coaches in their work ğŸƒ.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: âš½ æˆ‘ä»Šå¹´åœ¨GNNåº”ç”¨é¢†åŸŸçš„è°¦é€Šè´¡çŒ®æ˜¯[TacticAI](https://arxiv.org/abs/2310.10553)ï¼Œå®ƒæ˜¯*é¦–ä¸ªä¸ºï¼ˆè¶³çƒï¼‰æ•™ç»ƒæä¾›æœ‰ç”¨æˆ˜æœ¯å»ºè®®çš„å®Œæ•´AIç³»ç»Ÿ*ï¼Œä¸æˆ‘ä»¬åœ¨åˆ©ç‰©æµ¦è¶³çƒä¿±ä¹éƒ¨çš„åˆä½œä¼™ä¼´å…±åŒå¼€å‘
    ğŸ”´ã€‚TacticAIèƒ½å¤Ÿè¿›è¡Œé¢„æµ‹å»ºæ¨¡ï¼ˆ*â€œè¿™ä¸ªæˆ˜æœ¯åœºæ™¯ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿâ€*ï¼‰ã€æ£€ç´¢ç›¸ä¼¼æˆ˜æœ¯ï¼Œå¹¶è¿›è¡Œæ¡ä»¶ç”Ÿæˆå»ºæ¨¡ï¼ˆ*â€œå¦‚ä½•è°ƒæ•´çƒå‘˜ä½ç½®ä»¥å®ç°ç‰¹å®šç»“æœï¼Ÿâ€*ï¼‰ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä¸ªéå¸¸æœ‰è¶£çš„åˆä½œä¸­ï¼Œæœ€ä»¤äººæ»¡æ„çš„éƒ¨åˆ†æ˜¯æˆ‘ä»¬ä¸åˆ©ç‰©æµ¦ä¿±ä¹éƒ¨é¡¶çº§æ•™ç»ƒå›¢é˜Ÿçš„ç”¨æˆ·ç ”ç©¶â€”â€”ç›´æ¥è¡¨æ˜æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºå¯¹æ•™ç»ƒä»¬çš„å·¥ä½œæ˜¯æœ‰ç”¨çš„
    ğŸƒã€‚
- en: '![](../Images/a05d2b481488396a23817f3106efd65f.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a05d2b481488396a23817f3106efd65f.png)'
- en: 'A â€œbirdâ€™s eyeâ€ overview of TacticAI. (A), how corner kick situations are converted
    to a graph representation. Each player is treated as a node in a graph, with node,
    edge and graph features extracted as detailed in the main text. Then, a graph
    neural network operates over this graph by performing message passing; each nodeâ€™s
    representation is updated using the messages sent to it from its neighbouring
    nodes. (B), how TacticAI processes a given corner kick. To ensure that TacticAIâ€™s
    answers are robust in the face of horizontal or vertical reflections, all possible
    combinations of reflections are applied to the input corner, and these four views
    are then fed to the core TacticAI model, where they are able to interact with
    each other to compute the final player representations â€” each â€œinternal blue arrowâ€
    corresponds to a single message passing layer from (A). Once player representations
    are computed, they can be used to predict the cornerâ€™s receiver, whether a shot
    has been taken, as well as assistive adjustments to player positions and velocities,
    which increase or decrease the probability of a shot being taken. Source: [Wang,
    VeliÄkoviÄ‡, Hennes et al.](https://arxiv.org/abs/2310.10553)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: TacticAIçš„â€œé¸Ÿç°â€æ¦‚è§ˆã€‚ï¼ˆAï¼‰å±•ç¤ºäº†è§’çƒæƒ…å†µå¦‚ä½•è½¬åŒ–ä¸ºå›¾è¡¨ç¤ºã€‚æ¯ä¸ªçƒå‘˜è¢«è§†ä¸ºå›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹ã€è¾¹å’Œå›¾çš„ç‰¹å¾å¦‚æ­£æ–‡ä¸­æ‰€è¿°æå–ã€‚ç„¶åï¼Œå›¾ç¥ç»ç½‘ç»œåœ¨è¿™ä¸ªå›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’æ“ä½œï¼›æ¯ä¸ªèŠ‚ç‚¹çš„è¡¨ç¤ºé€šè¿‡ä»å…¶é‚»è¿‘èŠ‚ç‚¹æ¥æ”¶åˆ°çš„æ¶ˆæ¯è¿›è¡Œæ›´æ–°ã€‚ï¼ˆBï¼‰å±•ç¤ºäº†TacticAIå¦‚ä½•å¤„ç†ç»™å®šçš„è§’çƒã€‚ä¸ºäº†ç¡®ä¿TacticAIåœ¨é¢å¯¹æ°´å¹³æˆ–å‚ç›´åå°„æ—¶èƒ½å¤Ÿæä¾›ç¨³å¥çš„ç­”æ¡ˆï¼Œæ‰€æœ‰å¯èƒ½çš„åå°„ç»„åˆéƒ½ä¼šåº”ç”¨åˆ°è¾“å…¥è§’çƒä¸Šï¼Œéšåè¿™å››ç§è§†å›¾è¢«è¾“å…¥åˆ°æ ¸å¿ƒTacticAIæ¨¡å‹ä¸­ï¼Œåœ¨è¿™äº›è§†å›¾ä¹‹é—´ç›¸äº’ä½œç”¨ï¼Œä»¥è®¡ç®—æœ€ç»ˆçš„çƒå‘˜è¡¨ç¤ºâ€”â€”æ¯ä¸ªâ€œå†…éƒ¨è“è‰²ç®­å¤´â€å¯¹åº”äºï¼ˆAï¼‰ä¸­çš„å•ä¸ªæ¶ˆæ¯ä¼ é€’å±‚ã€‚ä¸€æ—¦è®¡ç®—å‡ºçƒå‘˜è¡¨ç¤ºï¼Œå®ƒä»¬å°±å¯ä»¥ç”¨æ¥é¢„æµ‹è§’çƒçš„æ¥çƒè€…ï¼Œæ˜¯å¦å·²å°„é—¨ï¼Œä»¥åŠå¯¹çƒå‘˜ä½ç½®å’Œé€Ÿåº¦çš„è¾…åŠ©è°ƒæ•´ï¼Œè¿™äº›è°ƒæ•´ä¼šå¢åŠ æˆ–å‡å°‘å°„é—¨çš„æ¦‚ç‡ã€‚æ¥æºï¼š[Wang,
    VeliÄkoviÄ‡, Hennes et al.](https://arxiv.org/abs/2310.10553)
- en: This is what Iâ€™m all about â€” AI systems that significantly augment human abilities.
    I can only hope that, in my home country, Partizan catches on to these methods
    before Red Star does! ğŸ˜…
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘çš„è¿½æ±‚â€”â€”æ˜¾è‘—å¢å¼ºäººç±»èƒ½åŠ›çš„AIç³»ç»Ÿã€‚æˆ‘åªèƒ½å¸Œæœ›ï¼Œåœ¨æˆ‘çš„ç¥–å›½ï¼Œå¸•å°”è’‚èµèƒ½æ¯”çº¢æ˜Ÿå…ˆé‡‡çº³è¿™äº›æ–¹æ³•ï¼ğŸ˜…
- en: ğŸ”® What will we see in 2024? Probably more of the same, just accelerated! â©
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® 2024å¹´æˆ‘ä»¬ä¼šçœ‹åˆ°ä»€ä¹ˆï¼Ÿå¯èƒ½ä¼šæ˜¯ç›¸åŒçš„å†…å®¹ï¼Œåªæ˜¯åŠ é€Ÿäº†ï¼â©
- en: Geometric Wall Street Bulletin ğŸ’¸
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‡ ä½•åå°”è¡—å…¬æŠ¥ ğŸ’¸
- en: '*Nathan Benaich (AirStreet Capital)****,*** *Michael Bronstein (Oxford), and
    Luca Naef (VantAI)*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nathan Benaich (AirStreet Capital)*, *Michael Bronstein (Oxford)* å’Œ *Luca
    Naef (VantAI)*'
- en: 2023 started with BioNTech (mostly known to the broad public for developing
    mRNA SARS-CoV-2 vaccines) [announcing the acquisition of InstaDeep](https://www.instadeep.com/2023/01/biontech-to-acquire-instadeep-to-strengthen-pioneering-position-in-the-field-of-ai-powered-drug-discovery-design-and-development/),
    a decade-old British company focused on AI-powered drug discovery, design and
    development. In May 2023, Recursion [acquired two startups](https://ir.recursion.com/news-releases/news-release-details/recursion-enters-agreements-acquire-cyclica-and-valence-bolster),
    Cyclica and Valence â€œto bolster chemistry and generative AI capabilitiesâ€. Valence
    ML team is well-known for multiple works in the geometric and graph ML and hosting
    the **Graphs & Geometry and Molecular Modeling** & **Drug Discovery seminars**
    on [YouTube](https://www.youtube.com/@valence_labs).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 2023å¹´å¼€å§‹æ—¶ï¼ŒBioNTechï¼ˆå¹¿ä¸ºäººçŸ¥çš„æ˜¯å¼€å‘äº†mRNA SARS-CoV-2ç–«è‹—ï¼‰[å®£å¸ƒæ”¶è´­InstaDeep](https://www.instadeep.com/2023/01/biontech-to-acquire-instadeep-to-strengthen-pioneering-position-in-the-field-of-ai-powered-drug-discovery-design-and-development/)ï¼Œè¿™æ˜¯ä¸€å®¶ä¸“æ³¨äºäººå·¥æ™ºèƒ½é©±åŠ¨è¯ç‰©å‘ç°ã€è®¾è®¡å’Œå¼€å‘çš„æˆç«‹åå¹´çš„è‹±å›½å…¬å¸ã€‚2023å¹´5æœˆï¼ŒRecursion
    [æ”¶è´­äº†ä¸¤å®¶åˆåˆ›å…¬å¸](https://ir.recursion.com/news-releases/news-release-details/recursion-enters-agreements-acquire-cyclica-and-valence-bolster)ï¼ŒCyclicaå’ŒValenceï¼Œâ€œä»¥å¢å¼ºåŒ–å­¦å’Œç”Ÿæˆæ€§AIèƒ½åŠ›â€ã€‚Valence
    MLå›¢é˜Ÿå› åœ¨å‡ ä½•å’Œå›¾å½¢æœºå™¨å­¦ä¹ é¢†åŸŸçš„å¤šé¡¹å·¥ä½œè€Œè‘—åï¼Œå¹¶åœ¨[YouTube](https://www.youtube.com/@valence_labs)ä¸Šä¸¾åŠ**å›¾å½¢ä¸å‡ ä½•å’Œåˆ†å­å»ºæ¨¡**ä¸**è¯ç‰©å‘ç°ç ”è®¨ä¼š**ã€‚
- en: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)Isomorphic
    Labs started 2024 by announcing small molecule-focused [collaborations](https://www.isomorphiclabs.com/articles/isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations)
    with Eli Lilly and Novartis with upfront payments of $45M and $37.5M, respectively,
    with the potential worth of **$3 billion**.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)Isomorphic
    Labsä»¥å®£å¸ƒä¸Eli Lillyå’ŒNovartisè¿›è¡Œå°åˆ†å­è¯ç‰©ç›¸å…³çš„[åˆä½œ](https://www.isomorphiclabs.com/articles/isomorphic-labs-kicks-off-2024-with-two-pharmaceutical-collaborations)å¼€å§‹äº†2024å¹´ï¼Œåˆ†åˆ«è·å¾—4500ä¸‡ç¾å…ƒå’Œ3750ä¸‡ç¾å…ƒçš„é¢„ä»˜æ¬¾ï¼Œæ½œåœ¨ä»·å€¼ä¸º**30äº¿ç¾å…ƒ**ã€‚'
- en: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)[VantAI
    partnered with Blueprint Medicines](https://www.businesswire.com/news/home/20240108659035/en/VantAI-Secures-Renewed-Support-from-Blueprint-Medicines-to-Chart-New-Frontiers-in-Induced-Proximity-Drug-Discovery)
    on innovative proximity modulating therapeutics, including molecular glue and
    hetero-bifunctional candidates. The dealâ€™s potential worth is $1.25 billion.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)[VantAIä¸Blueprint
    Medicines](https://www.businesswire.com/news/home/20240108659035/en/VantAI-Secures-Renewed-Support-from-Blueprint-Medicines-to-Chart-New-Frontiers-in-Induced-Proximity-Drug-Discovery)åˆä½œï¼Œå…±åŒå¼€å‘åˆ›æ–°çš„é‚»è¿‘è°ƒèŠ‚æ²»ç–—æ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å­èƒ¶å’Œå¼‚äºŒåŠŸèƒ½å€™é€‰è¯ç‰©ã€‚è¯¥äº¤æ˜“çš„æ½œåœ¨ä»·å€¼ä¸º12.5äº¿ç¾å…ƒã€‚'
- en: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)CHARM Therapeutics
    raised more funding [from NVIDIA](https://www.businesswire.com/news/home/20230515005172/en/CHARM-Therapeutics-Receives-Investment-for-Deep-Learning-Enabled-Drug-Discovery-Research-from-NVIDIA)
    and [from Bristol Myers Squibb](https://www.businesswire.com/news/home/20230320005101/en/CHARM-Therapeutics-Announces-Collaboration-with-Bristol-Myers-Squibb-to-Enable-and-Accelerate-Small-Molecule-Drug-Discovery-Programs)
    totalling the initial funding round to $70M. The company has developed DragonFold,
    its proprietary algorithm for protein-ligand co-folding.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[ğŸ’°](https://apps.timwhitlock.info/emoji/tables/unicode#emoji-modal)CHARM Therapeuticsè·å¾—æ›´å¤šèµ„é‡‘ï¼Œèµ„é‡‘æ¥è‡ª[NVIDIA](https://www.businesswire.com/news/home/20230515005172/en/CHARM-Therapeutics-Receives-Investment-for-Deep-Learning-Enabled-Drug-Discovery-Research-from-NVIDIA)å’Œ[Bristol
    Myers Squibb](https://www.businesswire.com/news/home/20230320005101/en/CHARM-Therapeutics-Announces-Collaboration-with-Bristol-Myers-Squibb-to-Enable-and-Accelerate-Small-Molecule-Drug-Discovery-Programs)ï¼Œä½¿åˆå§‹èèµ„æ€»é¢è¾¾åˆ°7000ä¸‡ç¾å…ƒã€‚è¯¥å…¬å¸å¼€å‘äº†DragonFoldï¼Œè¿™æ˜¯å…¶ä¸“æœ‰çš„è›‹ç™½è´¨-é…ä½“å…±æŠ˜å ç®—æ³•ã€‚'
- en: ğŸ’Š Monte Rosa [announced a successful](https://ir.monterosatx.com/news-releases/news-release-details/monte-rosa-therapeutics-announces-interim-pkpd-and-clinical-data)
    Phase 1 study of MRT-2359 (orally bioavailable investigational molecular glue
    degrader) against MYC-driven tumors like lung cancer and neuroendocrine cancer.
    Monte Rosa is known to [use geometric deep learning](https://ir.monterosatx.com/static-files/8806793a-99fb-4df8-8eb7-3785b39cf210)
    for proteins ([MaSIF](https://www.nature.com/articles/s41592-019-0666-6)).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’Š Monte Rosa [å®£å¸ƒäº†MRT-2359çš„æˆåŠŸ](https://ir.monterosatx.com/news-releases/news-release-details/monte-rosa-therapeutics-announces-interim-pkpd-and-clinical-data)1æœŸç ”ç©¶ï¼ˆå£æœç”Ÿç‰©å¯åˆ©ç”¨çš„ç ”ç©¶æ€§åˆ†å­èƒ¶é™è§£å‰‚ï¼‰ï¼Œé’ˆå¯¹MYCé©±åŠ¨çš„è‚¿ç˜¤ï¼Œå¦‚è‚ºç™Œå’Œç¥ç»å†…åˆ†æ³Œç™Œã€‚Monte
    Rosaä»¥[ä½¿ç”¨å‡ ä½•æ·±åº¦å­¦ä¹ ](https://ir.monterosatx.com/static-files/8806793a-99fb-4df8-8eb7-3785b39cf210)è¿›è¡Œè›‹ç™½è´¨ç ”ç©¶ï¼ˆ[MaSIF](https://www.nature.com/articles/s41592-019-0666-6)ï¼‰è€Œé—»åã€‚
- en: '***Nathan Benaich (AirStreet Capital, author of*** [***the State of AI Report***](https://www.stateof.ai/)***)***'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '***Nathan Benaichï¼ˆAirStreet Capitalï¼Œã€ŠAIç°çŠ¶æŠ¥å‘Šã€‹ä½œè€…*** [***the State of AI Report***](https://www.stateof.ai/)***)***'
- en: â€œI have long been optimistic about the potential of AI-first approaches to design
    problems in medicine, biotech, and materials science. Graph-based models had a
    great year in techbio in 2023.â€ â€” Nathan Benaich (AirStreet Capital)
  id: totrans-310
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œæˆ‘ä¸€ç›´å¯¹AIä¼˜å…ˆçš„æ–¹æ³•åœ¨åŒ»å­¦ã€ç”Ÿç‰©æŠ€æœ¯å’Œææ–™ç§‘å­¦ä¸­è§£å†³é—®é¢˜çš„æ½œåŠ›æŒä¹è§‚æ€åº¦ã€‚åŸºäºå›¾çš„æ¨¡å‹åœ¨2023å¹´æŠ€æœ¯ç”Ÿç‰©å­¦é¢†åŸŸè¡¨ç°çªå‡ºã€‚â€ â€” Nathan Benaichï¼ˆAirStreet
    Capitalï¼‰
- en: '[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8) combines
    diffusion techniques with GNNs to predict protein structures. It denoises blurry
    or corrupted structures from the Protein Data Bank, while tapping into RoseTTAFoldâ€™s
    prediction capabilities. DeepMind have continued to further develop AlphaFold
    and build on top of it. Their [AlphaMissense](https://www.science.org/doi/10.1126/science.adg7492)
    uses weak labels, language modeling, and AlphaFold to predict the pathogenicity
    of 71 million human variants. This is an important achievement, as most amino
    acid changes from genetic variation have unknown effects.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[RFdiffusion](https://www.nature.com/articles/s41586-023-06415-8)å°†æ‰©æ•£æŠ€æœ¯ä¸å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ç»“åˆï¼Œç”¨äºé¢„æµ‹è›‹ç™½è´¨ç»“æ„ã€‚å®ƒé€šè¿‡å»å™ªæ¥è‡ªè›‹ç™½è´¨æ•°æ®é“¶è¡Œï¼ˆProtein
    Data Bankï¼‰çš„æ¨¡ç³Šæˆ–æŸåçš„ç»“æ„ï¼ŒåŒæ—¶åˆ©ç”¨RoseTTAFoldçš„é¢„æµ‹èƒ½åŠ›ã€‚DeepMindç»§ç»­è¿›ä¸€æ­¥å¼€å‘AlphaFoldï¼Œå¹¶åœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ã€‚ä»–ä»¬çš„[AlphaMissense](https://www.science.org/doi/10.1126/science.adg7492)ä½¿ç”¨å¼±æ ‡ç­¾ã€è¯­è¨€å»ºæ¨¡å’ŒAlphaFoldæ¥é¢„æµ‹7100ä¸‡ä¸ªäººä½“å˜å¼‚çš„è‡´ç—…æ€§ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æˆå°±ï¼Œå› ä¸ºå¤§å¤šæ•°ç”±é—ä¼ å˜å¼‚å¼•èµ·çš„æ°¨åŸºé…¸å˜åŒ–çš„å½±å“ä»ä¸æ˜ç¡®ã€‚'
- en: Beyond proteins, graph-based models have been improving our understanding of
    genetics. Stanfordâ€™s [GEARS](https://www.nature.com/articles/s41587-023-01905-6.pdf)
    system integrates deep learning with a gene interaction knowledge graph to predict
    gene expression changes from combinatorial perturbations. By leveraging prior
    data on single and double perturbations, GEARS can predict outcomes for thousands
    of gene pairs.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è›‹ç™½è´¨ï¼ŒåŸºäºå›¾çš„æ¨¡å‹ä¹Ÿåœ¨æå‡æˆ‘ä»¬å¯¹é—ä¼ å­¦çš„ç†è§£ã€‚æ–¯å¦ç¦çš„[GEARS](https://www.nature.com/articles/s41587-023-01905-6.pdf)ç³»ç»Ÿå°†æ·±åº¦å­¦ä¹ ä¸åŸºå› äº’ä½œçŸ¥è¯†å›¾è°±ç»“åˆï¼Œèƒ½å¤Ÿä»ç»„åˆæ‰°åŠ¨ä¸­é¢„æµ‹åŸºå› è¡¨è¾¾å˜åŒ–ã€‚é€šè¿‡åˆ©ç”¨å•ä¸€å’ŒåŒé‡æ‰°åŠ¨çš„å…ˆå‰æ•°æ®ï¼ŒGEARSèƒ½å¤Ÿé¢„æµ‹æˆåƒä¸Šä¸‡ä¸ªåŸºå› å¯¹çš„ç»“æœã€‚
- en: '![](../Images/34d2a1814ebb3bb913bd1f9cd617b0d2.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34d2a1814ebb3bb913bd1f9cd617b0d2.png)'
- en: 'GEARS can predict new biologically meaningful phenotypes. (a) Workflow for
    predicting all pairwise combinatorial perturbation outcomes of a set of genes.
    (b) Low-dimensional representation of postperturbation gene expression for 102
    one-gene perturbations and 128 two-gene perturbations used to train GEARS. A random
    selection is labeled. (c) GEARS predicts postperturbation gene expression for
    all 5,151 pairwise combinations of the 102 single genes seen experimentally perturbed.
    Predicted postperturbation phenotypes (non-black symbols) are often different
    from phenotypes seen experimentally (black symbols). Colors indicate Leiden clusters
    labeled using marker gene expression. Source: [Roohani et al](https://www.nature.com/articles/s41587-023-01905-6)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: GEARSèƒ½å¤Ÿé¢„æµ‹æ–°çš„ç”Ÿç‰©å­¦ä¸Šæœ‰æ„ä¹‰çš„è¡¨å‹ã€‚(a) ç”¨äºé¢„æµ‹ä¸€ç»„åŸºå› çš„æ‰€æœ‰æˆå¯¹ç»„åˆæ‰°åŠ¨ç»“æœçš„å·¥ä½œæµç¨‹ã€‚(b) 102ç§å•åŸºå› æ‰°åŠ¨å’Œ128ç§åŒåŸºå› æ‰°åŠ¨çš„æ‰°åŠ¨ååŸºå› è¡¨è¾¾çš„ä½ç»´è¡¨ç¤ºï¼Œç”¨äºè®­ç»ƒGEARSã€‚éšæœºé€‰æ‹©çš„æ•°æ®å·²æ ‡æ³¨ã€‚(c)
    GEARSé¢„æµ‹æ‰€æœ‰5,151å¯¹ç»„åˆçš„102ç§å•åŸºå› æ‰°åŠ¨çš„æ‰°åŠ¨ååŸºå› è¡¨è¾¾ã€‚é¢„æµ‹çš„æ‰°åŠ¨åè¡¨å‹ï¼ˆéé»‘è‰²ç¬¦å·ï¼‰é€šå¸¸ä¸å®éªŒä¸­è§‚å¯Ÿåˆ°çš„è¡¨å‹ï¼ˆé»‘è‰²ç¬¦å·ï¼‰ä¸åŒã€‚é¢œè‰²è¡¨ç¤ºä½¿ç”¨æ ‡è®°åŸºå› è¡¨è¾¾è¿›è¡Œæ ‡è®°çš„Leidenç°‡ã€‚æ¥æºï¼š[Roohani
    et al](https://www.nature.com/articles/s41587-023-01905-6)
- en: ğŸ”® In 2024, I put hope in two different developments.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”® åœ¨2024å¹´ï¼Œæˆ‘å¯¹ä¸¤ä¸ªä¸åŒçš„å‘å±•æ–¹å‘å¯„äºˆå¸Œæœ›ã€‚
- en: '**1ï¸âƒ£** We have seen the first two CRISPR-Cas9 therapies approved in the US
    and the UK. These genome editors were discovered through sequencing and random
    experimentation. I am excited about the use of AI models to design and create
    bespoke editors on demand.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**1ï¸âƒ£** æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†ç¾å›½å’Œè‹±å›½æ‰¹å‡†çš„é¦–ä¸¤ç§CRISPR-Cas9ç–—æ³•ã€‚è¿™äº›åŸºå› ç»„ç¼–è¾‘å™¨æ˜¯é€šè¿‡æµ‹åºå’Œéšæœºå®éªŒå‘ç°çš„ã€‚æˆ‘å¯¹åˆ©ç”¨AIæ¨¡å‹æŒ‰éœ€è®¾è®¡å’Œåˆ›å»ºå®šåˆ¶åŒ–ç¼–è¾‘å™¨æ„Ÿåˆ°å…´å¥‹ã€‚'
- en: '**2ï¸âƒ£** We have started to see multimodality come to the AI bio world â€” combining
    DNA, RNA, protein, cellular, and imaging data to give us a more holistic understanding
    of biology.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**2ï¸âƒ£** æˆ‘ä»¬å·²ç»å¼€å§‹çœ‹åˆ°å¤šæ¨¡æ€æŠ€æœ¯è¿›å…¥AIç”Ÿç‰©é¢†åŸŸ â€”â€” ç»“åˆDNAã€RNAã€è›‹ç™½è´¨ã€ç»†èƒå’Œæˆåƒæ•°æ®ï¼Œä¸ºæˆ‘ä»¬æä¾›æ›´å…¨é¢çš„ç”Ÿç‰©å­¦ç†è§£ã€‚'
- en: '**Companies to watch in 2024**'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '**2024å¹´å€¼å¾—å…³æ³¨çš„å…¬å¸**'
- en: '[Profluent](https://www.profluent.bio/) â€” LLMs for protein design'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Profluent](https://www.profluent.bio/) â€” ç”¨äºè›‹ç™½è´¨è®¾è®¡çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰'
- en: '[Inceptive.bio](https://inceptive.life/) â€” founded by one of the authors of
    the Transformers paper.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Inceptive.bio](https://inceptive.life/) â€” ç”±ã€ŠTransformersã€‹è®ºæ–‡çš„ä½œè€…ä¹‹ä¸€åˆ›åŠã€‚'
- en: '[Enveda Biosciences](https://www.envedabio.com/)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Enveda Biosciences](https://www.envedabio.com/)'
- en: '[Orbital Materials](https://orbitalmaterials.com/)'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Orbital Materials](https://orbitalmaterials.com/)'
- en: '[Kumo.AI](https://kumo.ai/)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kumo.AI](https://kumo.ai/)'
- en: '[VantAI](https://www.vant.ai/) â€” we are biased (Michael Bronstein is Vantâ€™s
    Chief Scientist and Luca Naef is a founder and CTO), but this is a cool company
    focused on the rational design of molecular glues using a combination of ML and
    proprietary experimental technology, which we believe to be the right combination
    for success.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[VantAI](https://www.vant.ai/) â€” æˆ‘ä»¬å¯èƒ½æœ‰äº›åè§ï¼ˆè¿ˆå…‹å°”Â·å¸ƒæœ—æ–¯å¦æ˜¯Vantçš„é¦–å¸­ç§‘å­¦å®¶ï¼Œå¢å¡Â·çº³å¤«æ˜¯åˆ›å§‹äººå…¼CTOï¼‰ï¼Œä½†è¿™æ˜¯ä¸€å®¶å¾ˆé…·çš„å…¬å¸ï¼Œä¸“æ³¨äºåˆ©ç”¨æœºå™¨å­¦ä¹ å’Œä¸“æœ‰å®éªŒæŠ€æœ¯çš„ç»“åˆï¼Œç†æ€§è®¾è®¡åˆ†å­èƒ¶ï¼Œè¿™ç§ç»„åˆæˆ‘ä»¬è®¤ä¸ºæ˜¯æˆåŠŸçš„å…³é”®ã€‚'
- en: '[Future House](https://www.futurehouse.org/articles/announcing-future-house)
    â€” a new Silicon Valley-based non-profit company in the AI4Science space funded
    by ex-Google CEO Eric Schmidt. Head of Science is Andrew White, known for his
    works on LLMs for chemistry. The self-described mission of the company is a â€œmoonshot
    to build an AI scientist.â€'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Future House](https://www.futurehouse.org/articles/announcing-future-house)
    â€” ä¸€å®¶æ–°æˆç«‹çš„ä½äºç¡…è°·çš„éè¥åˆ©å…¬å¸ï¼Œè‡´åŠ›äºAI4Scienceé¢†åŸŸï¼Œç”±å‰Google CEOåŸƒé‡Œå…‹Â·æ–½å¯†ç‰¹èµ„åŠ©ã€‚ç§‘å­¦è´Ÿè´£äººæ˜¯å®‰å¾·é²Â·æ€€ç‰¹ï¼Œä»–å› åœ¨åŒ–å­¦é¢†åŸŸçš„LLMå·¥ä½œè€Œé—»åã€‚è¯¥å…¬å¸è‡ªæˆ‘æè¿°çš„ä½¿å‘½æ˜¯â€œå»ºè®¾ä¸€ä¸ªAIç§‘å­¦å®¶â€ï¼Œä¸€ä¸ªâ€œç™»æœˆè®¡åˆ’â€ã€‚'
- en: '*For additional articles about geometric and graph deep learning, see* [*Michael
    Galkin*](https://medium.com/@mgalkin)*â€™s and* [*Michael Bronstein*](https://medium.com/@michael-bronstein)*â€™s
    Medium posts and follow the two Michaels (*[*Galkin*](https://twitter.com/michael_galkin)
    *and* [*Bronstein*](https://twitter.com/mmbronstein)*) on Twitter.*'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºå‡ ä½•å­¦å’Œå›¾æ·±åº¦å­¦ä¹ çš„æ›´å¤šæ–‡ç« ï¼Œè¯·å‚è§* [*Michael Galkin*](https://medium.com/@mgalkin)*å’Œ* [*Michael
    Bronstein*](https://medium.com/@michael-bronstein)*çš„Mediumæ–‡ç« ï¼Œå¹¶åœ¨Twitterä¸Šå…³æ³¨ä¸¤ä½è¿ˆå…‹å°”ï¼ˆ*[*Galkin*](https://twitter.com/michael_galkin)
    *å’Œ* [*Bronstein*](https://twitter.com/mmbronstein)ï¼‰*ã€‚'
