- en: 'Towards Monosemanticity: A Step Towards Understanding Large Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走向单一语义性：理解大型语言模型的一步
- en: 原文：[https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11](https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11](https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3?source=collection_archive---------8-----------------------#2024-07-11)
- en: Understanding the mechanistic interpretability research problem and reverse-engineering
    these large language models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解机械解释性研究问题并逆向工程这些大型语言模型
- en: '[](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)[![Anish
    Dubey](../Images/f85f17fb79718c819b4bd1c9a16338a7.png)](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)
    [Anish Dubey](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)[![Anish
    Dubey](../Images/f85f17fb79718c819b4bd1c9a16338a7.png)](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)
    [Anish Dubey](https://medium.com/@anishdubey?source=post_page---byline--e7b88380d7b3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)
    ·10 min read·Jul 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e7b88380d7b3--------------------------------)
    ·10分钟阅读·2024年7月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Context
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: One of AI researchers' main burning questions is understanding how these large
    language models work. Mathematically, we have a good answer on how different neural
    network weights interact and produce a final answer. But, understanding them intuitively
    is one of the core questions AI researchers aim to answer. It is important because
    unless we understand how these LLMs work, it is very difficult to solve problems
    like LLM alignment and AI safety or to model the LLM to solve specific problems.
    This problem of understanding how large language models work is defined as a mechanistic
    interpretability research problem and the core idea is how we can reverse-engineer
    these large language models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能研究人员的一个主要迫切问题是理解这些大型语言模型如何工作。从数学角度来看，我们已经有了一个很好的答案，了解不同神经网络权重如何相互作用并产生最终答案。但直观地理解这些模型是人工智能研究人员要回答的核心问题之一。这非常重要，因为除非我们理解这些大型语言模型的工作原理，否则很难解决诸如LLM对齐和人工智能安全性等问题，或将LLM建模以解决特定问题。理解大型语言模型如何工作的这一问题被定义为一种机械解释性研究问题，核心思想是我们如何逆向工程这些大型语言模型。
- en: 'Anthropic is one of the companies that has made great strides in understanding
    these large models. The main question is how these models work apart from a mathematical
    point of view. In Oct ’23, they published this paper: *Towards Monosemanticity:
    Decomposing Language models with dictionary learning (*[*link*](https://transformer-circuits.pub/2023/monosemantic-features)*)*.
    This paper aims to solve this problem and build a basic understanding of how these
    models work.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 是在理解这些大型模型方面取得重大进展的公司之一。主要问题是，除了数学角度，如何理解这些模型的工作原理。2023年10月，他们发布了这篇论文：*走向单一语义性：通过字典学习分解语言模型*（[*链接*](https://transformer-circuits.pub/2023/monosemantic-features)）。这篇论文旨在解决这一问题，并建立对这些模型工作原理的基本理解。
- en: 'The below post aims to capture high-level basic concepts and build a solid
    foundation to understand the *“Towards Monosemanticity: Decomposing Language Models
    with dictionary learning”* paper.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文章旨在捕捉高层次的基本概念，并为理解 *“走向单一语义性：通过字典学习分解语言模型”* 论文打下坚实的基础。
- en: The paper starts with a loaded term, “*Towards Monosemanticity*”. Let’s dive
    straight into it to understand what this means.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 论文开篇提出了一个复杂的术语，“*走向单一语义性*”。让我们直接深入探讨，理解这意味着什么。
- en: What is Monosemanticity vs Polysemanticity?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是单一语义性与多重语义性？
- en: 'The basic unit of a large language model is a neural network which is made
    of neurons. So, neurons are the basic unit of the entire LLM’s. However, on inspection,
    we find that neurons fire for unrelated concepts in neural networks. For example:
    For vision models, a single neuron responds to *“faces of cats”* as well as *“fronts
    of cars”*. This concept is called *“polysemantic”*. This means neurons can respond
    to mixtures of unrelated inputs. This makes this problem very hard since the neuron
    itself cannot be used to analyze the behavior of the model. It would be nice if
    one neuron responds to the faces of cats while another neuron responds to the
    front of cars. If a neuron only fires for one feature, this property would have
    been called “*monosemanticity*”.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的基本单元是由神经元组成的神经网络。因此，神经元是整个大型语言模型（LLM）的基本单元。然而，经过检查，我们发现神经元在神经网络中会对不相关的概念发生反应。例如：在视觉模型中，一个神经元同时响应*“猫的脸”*和*“汽车的前脸”*。这个概念被称为*“多义性”*。这意味着神经元可以对不相关的输入的混合产生反应。这使得问题变得非常复杂，因为神经元本身不能用来分析模型的行为。如果一个神经元仅响应猫的脸，而另一个神经元响应汽车的前脸，那就好了。如果一个神经元只对一个特征发生反应，这种特性将被称为“*单义性*”。
- en: Hence the first section of the paper, *“Towards Monosemanticity,”* means if
    we can move from polysemanticity towards monosemanticity, this can help us understand
    neural networks with better depth.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，论文的第一部分，“*朝向单义性*”，意味着如果我们能从多义性转向单义性，这将有助于我们更深入地理解神经网络。
- en: How to go further beyond neurons
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何超越神经元进一步发展？
- en: 'Now, the key question is if neurons fire for unrelated concepts, it means there
    needs to be a more fundamental representation of data that the network learns.
    Let’s take an example: “Cats” and “Cars”. Cats can be represented as a combination
    of “animal, fur, eyes, legs, moving” while Cars can be a combination of “wheels,
    seats, rectangle, headlight”. This is 1st level representation. These can be further
    broken down into abstract concepts. Let’s take “eyes” and “headlight”. Eyes can
    be represented as “round, black, white” while headlight can be represented as
    “round, white, light”. As you can see, we can further build this abstract representation
    and notice that two very unrelated things (Cat and Car) start to share some representations.
    This is only 2 layers deep and can be imagined if we represent 8x, 16x, or 256x
    layers deep. A lot of things will be represented with very basic abstract concepts
    (difficult to interpret for humans) but concepts will be shared among different
    entities.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关键问题是，如果神经元对不相关的概念发生反应，这意味着网络需要学习一个更基础的数据表示。我们来看一个例子：“猫”和“汽车”。猫可以表示为“动物、毛发、眼睛、四肢、移动”的组合，而汽车则可以表示为“轮子、座椅、矩形、前灯”的组合。这是第一层表示。这些表示可以进一步细分为抽象概念。我们来看“眼睛”和“前灯”。眼睛可以表示为“圆形、黑色、白色”，而前灯可以表示为“圆形、白色、光”。正如你所看到的，我们可以进一步构建这个抽象表示，并注意到两个非常不相关的事物（猫和汽车）开始共享一些表示。这只有两层深，如果我们假设表示深度为8x、16x或256x层，很多事物将用非常基础的抽象概念表示（这些概念对人类来说难以解释），但这些概念将在不同的实体之间共享。
- en: The author uses terminology called “*features*” to represent this concept. According
    to the paper, each neuron can store many unrelated features and hence fires for
    completely unrelated inputs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了术语“*特征*”来表示这个概念。根据论文，每个神经元可以存储许多不相关的特征，因此会对完全不相关的输入产生反应。
- en: If a neuron is storing many features, how to get feature-level representation?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果一个神经元存储许多特征，如何获得特征级别的表示？
- en: The answer is always to scale more. If we think about this as if a neuron is
    storing, let’s say, 5 different features, can we break the neuron into 5 individual
    neurons and have each sub-neuron represent features? ***This is the core idea
    behind the paper***.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 答案总是要扩展规模。如果我们认为一个神经元存储了5个不同的特征，那么我们能否将这个神经元拆分成5个独立的神经元，每个子神经元代表一个特征？***这就是论文的核心思想***。
- en: The below image is a representation of the core idea of the paper. The *“Observed
    model”* is the actual model that stores multiple features of information. It is
    called the low-dimensional projection of some hypothetical larger network. The
    larger network is a hypothetical disentangled model that represents each neuron
    mapping to one feature and showing *“monosemanticity”* behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了论文核心思想的表示。*“观察到的模型”*是实际存储多个特征信息的模型。它被称为某个假设的大型网络的低维投影。大型网络是一个假设的解耦模型，表示每个神经元映射到一个特征并显示出*“单义性”*行为。
- en: With this, we can say whatever model we trained on, there will always be a bigger
    model that can contain 1:1 mapping between data and feature and hence we need
    to learn this bigger model for moving towards monosemanticity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们可以说，无论我们训练的是哪个模型，总会有一个更大的模型可以实现数据和特征之间的1:1映射，因此我们需要学习这个更大的模型，以便向单一语义性迈进。
- en: '![](../Images/92f12d711a49296b7e0f2a8173c3adad.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92f12d711a49296b7e0f2a8173c3adad.png)'
- en: Image from [https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)
- en: Now, before moving to technical implementation, let’s review all the information
    so far. The neuron is the basic unit in neural networks but contains multiple
    features of data. When data (tokens) are broken down into smaller abstract concepts,
    these are called features. If a neuron is storing multiple features, we need a
    way to represent each feature with its neuron so that only one neuron fires for
    each feature. This approach will help us move towards “*monosemanticity*”. Mathematically,
    it means we need to scale more as we need more neurons to represent the data into
    features.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在进入技术实现之前，让我们回顾一下至今为止的信息。神经元是神经网络中的基本单元，但它包含多个数据特征。当数据（令牌）被分解成更小的抽象概念时，这些被称为特征。如果一个神经元存储多个特征，我们需要一种方式将每个特征与其神经元关联，以确保每个特征只有一个神经元激活。这种方法将帮助我们向“*单一语义性*”迈进。数学上，这意味着我们需要更多的扩展，因为我们需要更多的神经元将数据转化为特征。
- en: With the basic and core idea under our grasp, let’s move to the technical implementation
    of how such things can be built.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们掌握了基本的核心思想后，接下来让我们探讨一下如何构建这些技术实现。
- en: Technical setup
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术设置
- en: Since we have established we need more scaling, the idea is to scale up the
    output after multi-layer perceptron (MLP). Before moving on to how to scale, let’s
    quickly review how the LLM model works with transformer and MLP blocks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经确定需要更多的扩展，目标是在多层感知机（MLP）之后扩展输出。在讨论如何扩展之前，让我们快速回顾一下LLM模型如何与Transformer和MLP模块一起工作。
- en: The below image is a representation of how the LLM model works with a transformer
    and MLP block. The idea is that each token is represented in terms of embeddings
    (vector) and is passed to the attention block which computes attention across
    different tokens. The output of the attention block is the same dimension as the
    input of each token. Now the output of each token from the attention block is
    parsed through a multi-layer perceptron (MLP) which scales up and then scales
    down the token to the same size as the input token. This step is repeated multiple
    times before the final output. In the case of chat-GPT-3, 96 layers do this operation.
    This is the same as how the transformer architecture works. Refer to the *“Attention
    is all you need”* paper for more details. [Link](https://arxiv.org/abs/1706.03762)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图片展示了LLM模型如何与Transformer和MLP模块协同工作。核心思想是每个令牌通过嵌入（向量）表示，并传递到注意力模块，该模块计算不同令牌之间的注意力。注意力模块的输出维度与每个令牌的输入维度相同。然后，来自注意力模块的每个令牌输出经过多层感知机（MLP）进行扩展，再缩小到与输入令牌相同的大小。这个过程会重复多次，直到产生最终输出。在chat-GPT-3的情况下，有96层执行这个操作。这与Transformer架构的工作原理相同。详情请参阅*“Attention
    is all you need”*论文。[链接](https://arxiv.org/abs/1706.03762)
- en: '![](../Images/0dbc3d5d4e7a8ad848675f81d57a79e5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dbc3d5d4e7a8ad848675f81d57a79e5.png)'
- en: Image by Author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now with the basic architecture laid out, let’s delve deeper into what sparse
    autoencoders are. The authors used *“sparse autoencoders”* to do up and down scaling
    and hence these have become a fundamental block to understand.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在基本架构已经搭建完成后，让我们更深入地了解什么是稀疏自编码器。作者使用了*“稀疏自编码器”*来进行放大和缩小，因此这些成为了理解的基础模块。
- en: Sparse auto-encoders
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Sparse auto-encoders are the neural network itself but contain 3 stages of the
    neural network (encoder, sparse activation in the middle, and decoder). The idea
    is that the auto-encoder takes, let’s say, 512-dimensional input, scales to a
    4096 middle layer, and then reduces to 512-dimensional output. Now once an input
    of 512 dimensions comes, it goes through an encoder whose job is to isolate features
    from data. After this, it is mapped into high dimensional space (sparse autoencoder
    activations) where only a few non-zero values are allowed and hence considered
    sparse. The idea here is to force the model to learn a few features in high-dimensional
    space. Finally, the matrix is forced to map back into the decoder (512 size) to
    reconstruct the same size and values as the encoder input.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自动编码器本身就是神经网络，但包含三个阶段（编码器、居中的稀疏激活和解码器）。其思想是，自动编码器接收一个512维的输入，将其缩放到一个4096的中间层，然后再压缩为512维的输出。当一个512维的输入到达时，它会通过编码器，编码器的任务是从数据中提取特征。接下来，它被映射到一个高维空间（稀疏自动编码器激活层），在这个空间中，只允许少量非零值，因此被认为是稀疏的。这里的想法是强迫模型在高维空间中学习少量特征。最后，矩阵被强制映射回解码器（512维），以重构与编码器输入相同的大小和值。
- en: The below image represents the sparse auto-encoder (SAE) architecture.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下图表示了稀疏自动编码器（SAE）架构。
- en: '![](../Images/1d4511be30d058b2a25bd3a1c12f88c8.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d4511be30d058b2a25bd3a1c12f88c8.png)'
- en: Image by Author
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: With basic transformer architecture and SAE explained, let’s try to understand
    how SAE’s are integrated with transformer blocks for interpretability.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本的变换器架构和稀疏自动编码器解释完毕后，让我们尝试理解稀疏自动编码器如何与变换器块集成以实现可解释性。
- en: How is sparse auto-encoder (SAE) integrated with an LLM?
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自动编码器（SAE）是如何与大语言模型（LLM）集成的？
- en: 'An LLM is based on a transformer block which is an attention mechanism followed
    by an MLP (multi-layer perceptron) block. The idea is to take output from MLP
    and feed it into the sparse auto-encoder block. Let’s take an example: “Golden
    Gate was built in 1937”. Golden is the 1st token which gets parsed through the
    attention block and then the MLP block. The output after the MLP block will be
    the same dimension as input but it will contain context from other words in the
    sentence due to attention mechanisms. Now, the same output vector from the MLP
    block becomes the input of the sparse auto-encoder itself. Each token has its
    own MLP output dimensions which can be fed into SAE as well. The below diagram
    conveys this information and how it is integrated with the transformer block.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）基于一个变换器块，这是一个注意力机制，后面接着一个MLP（多层感知机）块。其思路是从MLP获取输出并将其输入稀疏自动编码器块。举个例子：“Golden
    Gate was built in 1937”。“Golden”是第一个词元，它通过注意力块解析，然后进入MLP块。经过MLP块后的输出与输入的维度相同，但由于注意力机制，它将包含句子中其他词的上下文。现在，来自MLP块的相同输出向量成为稀疏自动编码器的输入。每个词元都有自己的MLP输出维度，也可以输入到SAE中。下图传达了这一信息，以及它如何与变换器块集成。
- en: '![](../Images/58ae4085c440c0a87255fd454c0efead.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58ae4085c440c0a87255fd454c0efead.png)'
- en: Image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: 'Side note: The below image is very famous in the paper and conveys the same
    information as the above section. It takes input from the activation vector from
    the MLP layer and feeding into SAE for feature scaling. Hopefully, the image below
    will make a lot more sense with the above explanation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 旁注：下面的图片在论文中非常著名，传达的信息与上述部分相同。它接收来自MLP层的激活向量，并将其输入稀疏自动编码器进行特征缩放。希望通过上面的解释，下面的图片会更容易理解。
- en: '![](../Images/d2d070357f015e3ee90b176a2738ebe5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2d070357f015e3ee90b176a2738ebe5.png)'
- en: Image from [https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)
- en: Now, that we understand the architecture and integration of SAE with LLM’s,
    the basic question is how are these SAE trained? Since these are also neural networks,
    these models need to be trained as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经理解了稀疏自动编码器与大语言模型（LLM）集成的架构，基本问题是这些稀疏自动编码器是如何训练的？由于这些也是神经网络，这些模型也需要进行训练。
- en: How are autoencoders trained?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器是如何训练的？
- en: The dataset for autoencoders comes from the main LLM itself. When training an
    LLM model with a token, the output after every MLP layer called activation vectors
    is stored for each token. So we have an input of tokens (512 size) and an output
    from MLP activation layer (512 size). We can collect different activations for
    the same tokens in different contexts. In the paper, the author collected different
    activations for 256 contexts for the same token. This gives a good representation
    of a token in different context settings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的数据集来自于 LLM 本身。当使用一个词汇训练 LLM 模型时，每个词汇在每个 MLP 层后的输出，即激活向量，会被存储。因此，我们有一个来自词汇的输入（512
    大小）和来自 MLP 激活层的输出（512 大小）。我们可以在不同上下文中收集相同词汇的不同激活。在论文中，作者为相同的词汇收集了 256 个不同上下文的激活。这为相同的词汇在不同上下文设置中的表示提供了良好的基础。
- en: Once the input is selected, SAE is trained for input and output (input is same
    as output from MLP activation layer (512 size), output is same as input). Since
    input is equal to output, the job of SAE is to enhance the information of 512
    size to 4096 size with sparse activation (1–2 non-zero values) and then convert
    back to 512 size. Since it is upscaling but with a penalty to reconstruct the
    information with 1–2 non-zero values, this is where the learning happens and the
    model is forced to learn 1–2 features for a specific data/token.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了输入，SAE 就会针对输入和输出进行训练（输入与来自 MLP 激活层（512 大小）的输出相同，输出与输入相同）。由于输入等于输出，SAE 的任务是通过稀疏激活（1–2
    个非零值）将 512 大小的信息提升至 4096 大小，然后再转换回 512 大小。由于这是一个上升过程，但在重构信息时需要对 1–2 个非零值进行惩罚，这就是学习发生的地方，模型被迫为特定数据/词汇学习
    1–2 个特征。
- en: Intuitively, this is a very simple problem for a model to learn. The input is
    the same as output and the middle layer is larger than the input and output layer.
    The model can learn the same mapping but we introduce a penalty for only a few
    values in the middle layer that are non-zero. Now it becomes a difficult problem
    since input is the same as output but the middle layer has only 1–2 non-zero values.
    So the model has to explicitly learn in the middle layer of what the data represents.
    This is where data gets broken down into features and features are learned in
    the middle layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，这是一个非常简单的问题，模型很容易学习。输入与输出相同，中间层比输入和输出层更大。模型可以学习相同的映射，但我们对中间层中的少数非零值引入了惩罚。现在，它变成了一个困难的问题，因为输入与输出相同，但中间层只有
    1–2 个非零值。因此，模型必须显式地在中间层学习数据代表了什么。这就是数据被分解为特征，并且特征在中间层中被学习的地方。
- en: With all this understanding, we are ready to tackle feature interpretability
    now.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上理解，我们现在准备好处理特征可解释性的问题了。
- en: Feature interpretability
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征可解释性
- en: Since training is done, let’s move to the inference phase now. This is where
    the interpretation begins now. The output from the MLP layer of the LLM’s model
    is fed into the SAE. In the SAE, only a few (1–2) blocks become activated. This
    is the middle layer of the SAE. Here, human inspection is required to see what
    neuron in the middle layer gets activated.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们现在进入推理阶段。这就是解释开始的地方。LLM 模型的 MLP 层输出被输入到 SAE 中。在 SAE 中，只有少数（1–2）块被激活。这是
    SAE 的中间层。在这里，需要人工检查以查看中间层中哪个神经元被激活。
- en: 'Example: Let’s say there are 2 types of context given to LLM and our job is
    to figure out when “*Golden*” is triggered. Context 1: “Golden Gate was built
    in 1937”, Context 2: “Golden Gate is in San Francisco”. When both the contexts
    are fed into LLM and the output of context 1 and context 2 for the “*Golden*”
    token is taken and fed into SAE, there should be only 1–2 features fired in the
    middle layer of SAE. Let’s say this feature number is 1345(a random number assigned
    out of 4096). This will denote that the 1345 feature gets triggered when Golden
    Gate is mentioned in the token input list. This means feature 1345 represents
    the *“Golden Gate”* context.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：假设有 2 种上下文被输入到 LLM 中，我们的任务是确定何时触发“*Golden*”这一词汇。上下文 1：“Golden Gate 是 1937
    年建成的”，上下文 2：“Golden Gate 位于旧金山”。当两个上下文同时输入到 LLM 中，并且提取“*Golden*”词汇在上下文 1 和上下文
    2 中的输出，并将其输入到 SAE 时，SAE 的中间层应该只有 1–2 个特征被激活。假设这个特征编号是 1345（从 4096 中随机分配的数字）。这表示当在输入列表中提到
    Golden Gate 时，特征 1345 被触发。这意味着特征 1345 代表了*“Golden Gate”*的上下文。
- en: Hence, this is one way to interpret features from SAE.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是解释 SAE 特征的一种方式。
- en: Limitations of the current approach
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当前方法的局限性
- en: '**Measurement**: The main bottleneck comes around the interpretation of the
    features. In the above example, human judgment is required to see if 1345 belongs
    to Golden Gate and is tested with multiple contexts. No mathematical loss function
    formulation helps answer this question quantitatively. This is one of the main
    bottlenecks that mechanistic interpretability faces in determining how to measure
    whether the progress of machines is interpretable or not.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**测量**：主要瓶颈在于特征的解释。在上述例子中，需要人工判断是否1345属于金门大桥，并通过多个上下文进行测试。没有任何数学损失函数公式能够定量地回答这个问题。这是机械可解释性领域在确定如何衡量机器进展是否可解释时面临的主要瓶颈之一。'
- en: '**Scaling**: Another aspect is scaling, since training SAE on each layer with
    4x more parameters is extremely memory and computation-intensive. As main models
    increase their parameters, it becomes even more difficult to scale SAE and hence
    there are concerns around the scaling aspect of using SAE as well.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩展性**：另一个方面是扩展性，因为在每一层训练稀疏自编码器（SAE）时，参数是原来的四倍，这对内存和计算资源要求极高。随着主模型参数的增加，扩展稀疏自编码器变得更加困难，因此也存在关于使用稀疏自编码器的扩展性问题。'
- en: But overall, this has been a fascinating journey. We started from a model and
    understood their nuances around interpretability and why neurons, despite being
    a basic unit, are still not the fundamental unit to understand. We went deeper
    to understand how data is made up of features and if there is a way to learn features.
    We learned how sparse auto-encoders help learn the sparse representation of the
    features and can be the building block of feature representation. Finally, we
    learned how to train sparse auto-encoders and after training, how SAE’s can be
    used to interpret the features in the inference phase.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，这是一次令人着迷的旅程。我们从一个模型出发，理解了其可解释性方面的细微差别，并探讨了为什么神经元尽管是一个基本单元，但仍不是理解的根本单元。我们进一步深入探讨了数据是如何由特征构成的，以及是否有方法学习这些特征。我们了解了稀疏自编码器如何帮助学习特征的稀疏表示，并且它可以成为特征表示的构建块。最后，我们学习了如何训练稀疏自编码器，并在训练后，如何在推理阶段使用稀疏自编码器来解释特征。
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The field of mechanistic interpretability has a long way to go. However current
    research from Anthropic in terms of introducing sparse auto-encoders is a big
    step towards interpretability. The field still suffers from limitations around
    measurement and scaling challenges but so far has been one of the best and most
    advanced research in the field of mechanistic interpretability.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 机械可解释性领域仍有很长的路要走。然而，Anthropic在引入稀疏自编码器方面的当前研究，已是朝着可解释性迈出的重要一步。该领域仍然面临测量和扩展性挑战，但迄今为止，已是机械可解释性领域中最优秀、最先进的研究之一。
- en: References
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://transformer-circuits.pub/2023/monosemantic-features](https://transformer-circuits.pub/2023/monosemantic-features)'
- en: '[https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for](https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for](https://www.lesswrong.com/posts/CJPqwXoFtgkKPRay8/an-intuitive-explanation-of-sparse-autoencoders-for)'
- en: '[https://www.dwarkeshpatel.com/p/dario-amodei](https://www.dwarkeshpatel.com/p/dario-amodei)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.dwarkeshpatel.com/p/dario-amodei](https://www.dwarkeshpatel.com/p/dario-amodei)'
