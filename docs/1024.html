<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1?source=collection_archive---------0-----------------------#2024-04-23">https://towardsdatascience.com/the-landscape-of-emerging-ai-agent-architectures-for-reasoning-planning-and-tool-calling-a-a95214b743c1?source=collection_archive---------0-----------------------#2024-04-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sandibesen?source=post_page---byline--a95214b743c1--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Sandi Besen" class="l ep by dd de cx" src="../Images/97361d97f50269f70b6621da2256bc29.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*gHEvwZHf-nDi0QXwnsUeFg.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a95214b743c1--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@sandibesen?source=post_page---byline--a95214b743c1--------------------------------" rel="noopener follow">Sandi Besen</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a95214b743c1--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">6</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ma mb mc md me mf lx ly paragraph-image"><div role="button" tabindex="0" class="mg mh ed mi bh mj"><div class="lx ly lz"><img src="../Images/f40135ab16836eb9faae12e98ef3805d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uwa8bfgtFhBRJ-Nrv9t-_w.png"/></div></div><figcaption class="ml mm mn lx ly mo mp bf b bg z dx">Image by Author</figcaption></figure><p id="2898" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr"><em class="no">My team and I (</em></strong><a class="af np" href="https://www.linkedin.com/in/sandibesen/" rel="noopener ugc nofollow" target="_blank"><strong class="ms fr"><em class="no">Sandi Besen</em></strong></a><strong class="ms fr"><em class="no">, </em></strong><a class="af np" href="https://www.linkedin.com/in/tula-masterman/" rel="noopener ugc nofollow" target="_blank"><strong class="ms fr"><em class="no">Tula Masterman</em></strong></a><strong class="ms fr"><em class="no">, </em></strong><a class="af np" href="https://www.linkedin.com/in/mason-sawtell/" rel="noopener ugc nofollow" target="_blank"><strong class="ms fr"><em class="no">Mason Sawtell</em></strong></a><strong class="ms fr"><em class="no">, and </em></strong><a class="af np" href="https://www.linkedin.com/in/alexchao56/" rel="noopener ugc nofollow" target="_blank"><strong class="ms fr"><em class="no">Alex Chao</em></strong></a><strong class="ms fr"><em class="no">) recently published a survey research paper that offers a comprehensive look at the current state of AI agent architectures. As co-authors of this work, we set out to uncover the key design elements that enable these autonomous systems to effectively execute complex goals.</em></strong></p><p id="5125" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This paper serves as a resource for researchers, developers, and anyone interested in staying updated on the cutting-edge progress in the field of AI agent technologies.</p><p id="12dc" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Read the full meta-analysis on <a class="af np" href="https://arxiv.org/abs/2404.11584" rel="noopener ugc nofollow" target="_blank">Arxiv</a></p></div></div></div><div class="ab cb nq nr ns nt" role="separator"><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0e4b" class="ny nz fq bf oa ob oc od oe of og oh oi oj ok ol om on oo op oq or os ot ou ov bk">A Shift Towards Agents</h1><p id="b56e" class="pw-post-body-paragraph mq mr fq ms b mt ow mv mw mx ox mz na nb oy nd ne nf oz nh ni nj pa nl nm nn fj bk">Since the launch of ChatGPT, the initial wave of generative AI applications has largely revolved around chatbots that utilize the Retrieval Augmented Generation (RAG) pattern to respond to user prompts. While there is ongoing work to enhance the robustness of these RAG-based systems, the research community is now exploring the next generation of AI applications — a common theme being the development of autonomous AI agents.</p><p id="9f95" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Agentic systems incorporate advanced capabilities like planning, iteration, and reflection, which leverage the model’s inherent reasoning abilities to accomplish tasks end-to-end. Paired with the ability to use tools, plugins, and function calls — agents are empowered to tackle a wider range of general-purpose work.</p><h1 id="8fd9" class="ny nz fq bf oa ob pb od oe of pc oh oi oj pd ol om on pe op oq or pf ot ou ov bk">The Importance of Reasoning, Planning, and Effective Tool Calling for Agents</h1><p id="1e62" class="pw-post-body-paragraph mq mr fq ms b mt ow mv mw mx ox mz na nb oy nd ne nf oz nh ni nj pa nl nm nn fj bk">Reasoning is a foundational building block of the human mind. Without reasoning one would not be able to make decisions, solve problems, or refine plans when new information is learned — essentially misunderstanding the world around us.<strong class="ms fr"> If agents don’t have strong reasoning skills then they might misunderstand their task, generate nonsensical answers, or fail to consider multi-step implications.</strong></p><p id="5c01" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">We find that most agent implementations contain a planning phase which invokes one of the following techniques to create a plan: task decomposition, multi-plan selection, external module-aided planning, reflection and refinement and memory-augmented planning [1].</p><p id="fb48" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Another benefit of utilizing an agent implementation over just a base language model is the agent’s ability to solve complex problems by calling tools. Tools can enable an agent to execute actions such as interacting with APIs, writing to third party applications, and more<strong class="ms fr">.</strong> Reasoning and tool calling are closely intertwined and effective tool calling has a dependency on adequate reasoning. Put simply, you can’t expect an agent with poor reasoning abilities to understand when is the appropriate time to call its tools.</p><h1 id="2f4f" class="ny nz fq bf oa ob pb od oe of pc oh oi oj pd ol om on pe op oq or pf ot ou ov bk"><strong class="al">Single vs Multi Agent Architecture</strong></h1><p id="5aaa" class="pw-post-body-paragraph mq mr fq ms b mt ow mv mw mx ox mz na nb oy nd ne nf oz nh ni nj pa nl nm nn fj bk"><strong class="ms fr">Our findings emphasize that both single-agent and multi-agent architectures can be used to solve challenging tasks by employing reasoning and tool calling steps.</strong></p><p id="d8e9" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">For single agent implementations, we find that successful goal execution is contingent upon proper planning and self-correction</strong> [1, 2, 3, 4]. Without the ability to self-evaluate and create effective plans, single agents may get stuck in an endless execution loop and never accomplish a given task or return a result that does not meet user expectations [2]. We find that single agent architectures are especially useful when the task requires straightforward function calling and does not need feedback from another agent.</p><p id="ef11" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, we note that single agent patterns often struggle to complete a long sequence of sub tasks or tool calls [5, 6]. Multi-agent patterns can address the issues of parallel tasks and robustness since multiple agents within the architecture can work on individual subproblems. Many multi-agent patterns start by taking a complex problem and breaking it down into several smaller tasks. Then, each agent works independently on solving each task using their own independent set of tools.</p><p id="f95c" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Architectures involving multiple agents present an opportunity for intelligent labor division based on capabilities as well as valuable feedback from diverse agent personas. Numerous multi-agent architectures operate in stages where teams of agents are dynamically formed and reorganized for each planning, execution, and evaluation phase [7, 8, 9]. This reorganization yields superior outcomes because specialized agents are utilized for specific tasks and removed when no longer required. By matching agent roles and skills to the task at hand, agent teams can achieve greater accuracy and reduce the time needed to accomplish the goal. Crucial features of effective multi-agent architectures include clear leadership within agent teams, dynamic team construction, and efficient information sharing among team members to prevent important information from getting lost amidst superfluous communication.</p><p id="4856" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="no">Our research highlights notable single agent methods such as ReAct, RAISE, Reflexion, AutoGPT + P, LATS, and multi agent implementations such as DyLAN, AgentVerse, and MetaGPT, which are explained more in depth in the </em><a class="af np" href="https://arxiv.org/abs/2404.11584" rel="noopener ugc nofollow" target="_blank"><em class="no">full text</em></a><em class="no">.</em></p><h1 id="53a4" class="ny nz fq bf oa ob pb od oe of pc oh oi oj pd ol om on pe op oq or pf ot ou ov bk">Our Key Findings</h1><p id="3c01" class="pw-post-body-paragraph mq mr fq ms b mt ow mv mw mx ox mz na nb oy nd ne nf oz nh ni nj pa nl nm nn fj bk"><strong class="ms fr">Single Agent Patterns:</strong></p><p id="9b3b" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Single agent patterns are generally best suited for tasks with a narrowly defined list of tools and where processes are well-defined. They don’t face poor feedback from other agents or distracting and unrelated chatter from other team members. However, single agents may get stuck in an execution loop and fail to make progress towards their goal if their reasoning and refinement capabilities aren’t robust.</p><p id="5f50" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Multi Agent Patterns:</strong></p><p id="909a" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Multi agent patterns are well-suited for tasks where feedback from multiple personas is beneficial in accomplishing the task. They are useful when parallelization across distinct tasks or workflows is required, allowing individual agents to proceed with their next steps without being hindered by the state of tasks handled by others.</p><p id="0123" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Feedback and Human in the Loop</strong></p><p id="61ef" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Language models tend to commit to an answer earlier in their response, which can cause a ‘snowball effect’ of increasing diversion from their goal state [10]. By implementing feedback, agents are much more likely to correct their course and reach their goal. Human oversight improves the immediate outcome by aligning the agent’s responses more closely with human expectations, yielding more reliable and trustworthy results [11, 8]. Agents can be susceptible to feedback from other agents, even if the feedback is not sound. This can lead the agent team to generate a faulty plan which diverts them from their objective [12].</p><p id="f261" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Information Sharing and Communication</strong></p><p id="e8d0" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Multi-agent patterns have a greater tendency to get caught up in niceties and ask one another things like “how are you”, while single agent patterns tend to stay focused on the task at hand since there is no team dynamic to manage. This can be mitigated by robust prompting. In vertical architectures, agents can fail to send critical information to their supporting agents not realizing the other agents aren’t privy to necessary information to complete their task. This failure can lead to confusion in the team or hallucination in the results. One approach to address this issue is to explicitly include information about access rights in the system prompt so that the agents have contextually appropriate interactions.</p><p id="67c7" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Impact of Role Definition and Dynamic Teams</strong></p><p id="4be2" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Clear role definition is critical for both single and multi-agent architectures. Role definition ensures that the agents understands their assigned role, stay focused on the provided task, execute the proper tools, and minimizes hallucination of other capabilities. Establishing a clear group leader improves the overall performance of multi-agent teams by streamlining task assignment. Dynamic teams where agents are brought in and out of the system based on need have also been shown to be effective. This ensures that all agents participating in the tasks are strong contributors.</p><p id="9c1e" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr">Summary of Key Insights</strong></p><p id="0b00" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The key insights discussed suggest that the best agent architecture varies based on use case. <strong class="ms fr">Regardless of the architecture selected, the best performing agent systems tend to incorporate at least one of the following approaches: well defined system prompts, clear leadership and task division, dedicated reasoning / planning- execution — evaluation phases, dynamic team structures, human or agentic feedback, and intelligent message filtering</strong>. Architectures that leverage these techniques are more effective across a variety of benchmarks and problem types.</p><h1 id="453e" class="ny nz fq bf oa ob pb od oe of pc oh oi oj pd ol om on pe op oq or pf ot ou ov bk">Conclusion</h1><p id="ae2e" class="pw-post-body-paragraph mq mr fq ms b mt ow mv mw mx ox mz na nb oy nd ne nf oz nh ni nj pa nl nm nn fj bk">Our meta-analysis aims to provide a holistic understanding of the current AI agent landscape and offer insight for those building with existing agent architectures or developing custom agent architectures. There are notable limitations and areas for future improvement in the design and development of autonomous AI agents such as a lack of comprehensive agent benchmarks, real world applicability, and the mitigation of harmful language model biases. These areas will need to be addressed in the near-term to enable reliable agents.</p></div></div></div><div class="ab cb nq nr ns nt" role="separator"><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="21a9" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Note: The opinions expressed both in this article and paper are solely those of the authors and do not necessarily reflect the views or policies of their respective employers.</p><p id="2ec3" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">If you still have questions or think that something needs to be further clarified? Drop me a DM on <a class="af np" href="https://www.linkedin.com/in/sandibesen/" rel="noopener ugc nofollow" target="_blank">Linkedin</a>! I‘m always eager to engage in food for thought and iterate on my work.</p></div></div></div><div class="ab cb nq nr ns nt" role="separator"><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw nx"/><span class="nu by bm nv nw"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="afff" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="ms fr"><em class="no">References</em></strong></p><p id="84ed" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[1] Timo Birr et al. AutoGPT+P: Affordance-based Task Planning with Large Language Models. arXiv:2402.10778 [cs] version: 1. Feb. 2024. URL: <a class="af np" href="https://arxiv.org/abs/2402.10778" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2402.10778.</a></p><p id="029e" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[2] Shunyu Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs]. Mar. 2023. URL: <a class="af np" href="https://arxiv.org/abs/2210.03629" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2210.03629.</a></p><p id="ecb4" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[3] Na Liu et al. From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs]. Jan. 2024. URL:<a class="af np" href="https://arxiv.org/abs/2401.02777" rel="noopener ugc nofollow" target="_blank"> http://arxiv.org/abs/2401.02777.</a></p><p id="4280" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[4] Noah Shinn et al. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs]. Oct. 2023. URL: <a class="af np" href="https://arxiv.org/abs/2303.11366" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2303.11366</a></p><p id="8dd5" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[5]Zhengliang Shi et al. Learning to Use Tools via Cooperative and Interactive Agents. arXiv:2403.03031 [cs]. Mar. 2024. URL: <a class="af np" href="https://arxiv.org/abs/2403.03031" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2403.03031</a></p><p id="f484" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[6] Silin Gao et al. Efficient Tool Use with Chain-of-Abstraction Reasoning. arXiv:2401.17464 [cs]. Feb. 2024. URL: <a class="af np" href="https://arxiv.org/abs/2401.17464" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2401.17464</a></p><p id="e0ff" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[7] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: <a class="af np" href="https://arxiv.org/abs/2308.10848" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2308.10848</a>.</p><p id="65c5" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[8] Xudong Guo et al. Embodied LLM Agents Learn to Cooperate in Organized Teams. 2024. arXiv: 2403.12482 [cs.AI]. URL: <a class="af np" href="https://arxiv.org/abs/2403.12482" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2403.12482</a></p><p id="a40e" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[9] Zijun Liu et al. Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization. 2023. arXiv: 2310.02170 [cs.CL]. URL: <a class="af np" href="https://arxiv.org/abs/2310.02170" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2310.02170</a></p><p id="ac4d" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[10] Muru Zhang et al. How Language Model Hallucinations Can Snowball. arXiv:2305.13534 [cs]. May 2023. URL: <a class="af np" href="https://arxiv.org/abs/2305.13534" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2305.13534</a>.</p><p id="b7b7" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[11] Xueyang Feng et al. Large Language Model-based Human-Agent Collaboration for Complex Task Solving. 2024. arXiv: 2402.12914 [cs.CL].</p><p id="0ebe" class="pw-post-body-paragraph mq mr fq ms b mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">[12] Weize Chen et al. AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. arXiv:2308.10848 [cs]. Oct. 2023. URL: <a class="af np" href="https://arxiv.org/abs/2308.10848" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2308.10848</a>.</p></div></div></div></div>    
</body>
</html>