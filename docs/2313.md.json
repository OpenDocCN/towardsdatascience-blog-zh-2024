["```py\nimport pdfkit\nimport os\nimport google.generativeai as genai\nfrom dotenv import load_dotenv\n\nload_dotenv(\"../genai_agents/keys.env\")\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n```", "```py\nARTICLE_URL = \"https://lakshmanok.medium....\"\npdfkit.from_url(ARTICLE_URL, \"article.pdf\")\npdf_file = genai.upload_file(\"article.pdf\")\n```", "```py\nclass Slide(BaseModel):\n    title: str\n    key_points: List[str]\n    lecture_notes: str\n\nclass Lecture(BaseModel):\n    slides: List[Slide]\n    lecture_title: str\n    based_on_article_by: str\n```", "```py\nlecture_prompt = \"\"\"\nYou are a university professor who needs to create a lecture to\na class of undergraduate students.\n\n* Create a 10-slide lecture based on the following article.\n* Each slide should contain the following information:\n  - title: a single sentence that summarizes the main point\n  - key_points: a list of between 2 and 5 bullet points. Use phrases, not full sentences.\n  - lecture_notes: 3-10 sentences explaining the key points in easy-to-understand language. Expand on the points using other information from the article.\n* Also, create a title for the lecture and attribute the original article's author.\n\"\"\"\n```", "```py\n model = genai.GenerativeModel(\n    \"gemini-1.5-flash-001\",\n    system_instruction=[lecture_prompt]\n)\ngeneration_config={\n    \"temperature\": 0.7,\n    \"response_mime_type\": \"application/json\",\n    \"response_schema\": Lecture\n}\nresponse = model.generate_content(\n    [pdf_file],\n    generation_config=generation_config,\n    stream=False\n)\n```", "```py\nlecture = json.loads(response.text)\n```", "```py\n{'key_points': [\n    'Silver layer cleans, structures, and prepares data for self-service analytics.',\n    'Data is denormalized and organized for easier use.',\n    'Type 2 slowly changing dimensions are handled in this layer.',\n    'Governance responsibility lies with the source team.'\n  ],\n 'lecture_notes': 'The silver layer takes data from the bronze layer and transforms it into a usable format for self-service analytics. This involves cleaning, structuring, and organizing the data. Type 2 slowly changing dimensions, which track changes over time, are also handled in this layer. The governance of the silver layer rests with the source team, which is typically the data engineering team responsible for the source system.',\n 'title': 'The Silver Layer: Data Transformation and Preparation'\n}\n```", "```py\nfor slidejson in lecture['slides']:\n    slide = presentation.slides.add_slide(presentation.slide_layouts[1])\n    title = slide.shapes.title\n    title.text = slidejson['title']\n    # bullets\n    textframe = slide.placeholders[1].text_frame\n    for key_point in slidejson['key_points']:\n        p = textframe.add_paragraph()\n        p.text = key_point\n        p.level = 1\n    # notes\n    notes_frame = slide.notes_slide.notes_text_frame\n    notes_frame.text = slidejson['lecture_notes']\n```", "```py\nfrom google.cloud import texttospeech\n\ndef convert_text_audio(text, audio_mp3file):\n    \"\"\"Synthesizes speech from the input string of text.\"\"\"\n    tts_client = texttospeech.TextToSpeechClient()    \n    input_text = texttospeech.SynthesisInput(text=text)\n\n    voice = texttospeech.VoiceSelectionParams(\n        language_code=\"en-US\",\n        name=\"en-US-Standard-C\",\n        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,\n    )\n    audio_config = texttospeech.AudioConfig(\n        audio_encoding=texttospeech.AudioEncoding.MP3\n    )\n\n    response = tts_client.synthesize_speech(\n        request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n    )\n\n    # The response's audio_content is binary.\n    with open(audio_mp3file, \"wb\") as out:\n        out.write(response.audio_content)\n        print(f\"{audio_mp3file} written.\")\n```", "```py\nfor slideno, slide in enumerate(lecture['slides']):\n        text = f\"On to {slide['title']} \\n\"\n        text += slide['lecture_notes'] + \"\\n\\n\"\n        filename = os.path.join(outdir, f\"audio_{slideno+1:02}.mp3\")\n        convert_text_audio(text, filename)\n        filenames.append(filename)\n```", "```py\ncombined = pydub.AudioSegment.empty()\nfor audio_file in audio_files:\n    audio = pydub.AudioSegment.from_file(audio_file)\n    combined += audio\n    # pause for 4 seconds\n    silence = pydub.AudioSegment.silent(duration=4000)\n    combined += silence\ncombined.export(\"lecture.wav\", format=\"wav\")\n```", "```py\ndef text_to_image(output_path, title, keypoints):\n    image = Image.new(\"RGB\", (1000, 750), \"black\")\n    draw = ImageDraw.Draw(image)\n    title_font = ImageFont.truetype(\"Coval-Black.ttf\", size=42)\n    draw.multiline_text((10, 25), wrap(title, 50), font=title_font)\n    text_font = ImageFont.truetype(\"Coval-Light.ttf\", size=36)\n    for ptno, keypoint in enumerate(keypoints):\n        draw.multiline_text((10, (ptno+2)*100), wrap(keypoint, 60), font=text_font) \n    image.save(output_path)\n```", "```py\nclips = []\nfor slide, audio in zip(slide_files, audio_files):\n    audio_clip = AudioFileClip(f\"article_audio/{audio}\")\n    slide_clip = ImageClip(f\"article_slides/{slide}\").set_duration(audio_clip.duration)\n    slide_clip = slide_clip.set_audio(audio_clip)\n    clips.append(slide_clip)\nfull_video = concatenate_videoclips(clips)\n```", "```py\nfull_video.write_videofile(\"lecture.mp4\", fps=24, codec=\"mpeg4\", \n                           temp_audiofile='temp-audio.mp4', remove_temp=True)\n```", "```py\nlecture.json  lecture.mp4  lecture.pptx  lecture.wav\n```"]