- en: What We Still Don’t Understand About Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们仍然不理解的机器学习问题
- en: 原文：[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)
- en: Machine Learning unknowns that researchers struggle to understand — from Batch
    Norm to what SGD hides
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的未知问题，研究人员难以理解——从批量归一化到SGD隐藏的奥秘
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    ·12 min read·Jul 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    ·阅读时间12分钟·2024年7月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)'
- en: What We Still Don’t Understand About Machine Learning. (by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然不理解的机器学习问题。（作者）
- en: If you’re not a member, [read for free here](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9)
    👈
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你还不是会员，[点击这里免费阅读](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9)
    👈
- en: It is surprising how some of the basic subjects in machine learning are still
    unknown by researchers and despite being fundamental and common to use, seem to
    be mysterious. It’s a fun thing about machine learning that we build things that
    work and then figure out why they work at all!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，机器学习中的一些基础主题至今仍然是研究人员未解之谜，尽管它们是基本的且常用的，但似乎仍然神秘莫测。机器学习的一个有趣之处在于，我们构建了有效的系统，然后才去弄清楚它们为何有效！
- en: Here, I aim to investigate the unknown territory in some machine learning concepts
    in order to show while these ideas can seem basic, in reality, they are constructed
    by layers upon layers of abstraction. This helps us to practice questioning the
    **depth of our knowledge**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我旨在探讨一些机器学习概念中的未知领域，以展示尽管这些想法看起来基础，实际上它们是通过一层层抽象构建而成的。这有助于我们练习质疑**我们知识的深度**。
- en: In this article, we explore several key phenomena in deep learning that challenge
    our traditional understanding of neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了深度学习中的几个关键现象，这些现象挑战了我们对神经网络的传统理解。
- en: We start with **Batch Normalization** and its underlying mechanisms that remain
    not fully understood.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从**批量归一化**开始，并讨论其尚未完全理解的基本机制。
- en: We examine the counterintuitive observation that **overparameterized** models
    often **generalize better**, contradicting the classical machine learning theories.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们考察了一个反直觉的观察结果，即**过参数化**模型往往**具有更好的泛化能力**，这与经典的机器学习理论相矛盾。
- en: We explore the implicit regularization effects of **gradient descent**, which
    seem to naturally bias neural networks towards simpler, more generalizable solutions.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探索了**梯度下降**的隐式正则化效应，这似乎自然地使神经网络倾向于更简单、更具普适性的解决方案。
- en: Finally, we touch on the Lottery Ticket Hypothesis, which proposes that large
    neural networks contain smaller subnetworks capable of achieving **comparable
    performance** when trained in isolation.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们讨论了彩票票假说（Lottery Ticket Hypothesis），该假说提出，大型神经网络包含较小的子网络，当它们被单独训练时，能够实现**相当的性能**。
- en: '*This article contains 4 parts, they are not connected so feel free to skip
    to the ones you like more.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文包含4个部分，它们之间没有直接关联，所以可以随意跳到你更感兴趣的部分。*'
- en: · [1\. Batch Normalization](#de8b)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: · [1\. 批量归一化](#de8b)
- en: ∘ [What We Don’t Understand](#95c8)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [我们不理解的东西](#95c8)
- en: ∘ [Batch Norm Considerations](#ec44)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [批量归一化的注意事项](#ec44)
- en: · [2\. Over-Parameterization and Generalization](#82b3)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: · [2\. 过度参数化与泛化](#82b3)
- en: ∘ [Understanding Deep Learning Requires Rethinking Generalization](#6185)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [理解深度学习需要重新思考泛化](#6185)
- en: · [3\. Implicit Regularization in Neural Networks](#f626)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: · [3\. 神经网络中的隐式正则化](#f626)
- en: ∘ [Experiment 1](#2783)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [实验 1](#2783)
- en: ∘ [Experiment 2](#ff53)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [实验 2](#ff53)
- en: ∘ [Gradient Descent as a natural Reguralizer](#85f8)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [梯度下降作为一种自然正则化器](#85f8)
- en: · [4\. The Lottery Ticket Hypothesis](#b35e)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: · [4\. 彩票票假设](#b35e)
- en: · [Final Word.](#821e)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: · [最后的话。](#821e)
- en: ∘ [Let’s Connect!](#03ee)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [让我们连接！](#03ee)
- en: · [Further Reads](#397f)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: · [进一步阅读](#397f)
- en: · [References](#f3e3)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#f3e3)
- en: 1\. Batch Normalization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 批量归一化
- en: Introduced by *Sergey Ioffe* and *Christian Szegedy* in 2015 [1], batch normalization
    is a method to train neural networks faster and more stable. It was previously
    known that transforming the input data to have their mean set to zero and variance
    to one would result in a faster convergence. The authors used this idea further
    and introduced **Batch Normalization** to transform the input of hidden layers
    to have a mean of zero and a variance of one.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由*谢尔盖·约夫*和*克里斯蒂安·谢格迪*于2015年提出的[1]，批量归一化是一种加速神经网络训练并提高其稳定性的方法。之前已知，将输入数据的均值调整为零、方差调整为一，可以实现更快的收敛。作者进一步使用这一思想，提出了**批量归一化**，以使隐藏层的输入具有零均值和单位方差。
- en: '![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)'
- en: An outline of the Residual Block used in Resnet and the use of Batch Norm in
    it. (by author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个概述ResNet中使用的残差块以及其中批量归一化的应用。（作者提供）
- en: Since its introduction, the batch norm has become common in neural networks.
    One such example, among many, is its use in the famous ResNet architecture. So
    we could confidently say we are certain of how effective it could be.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自其提出以来，批量归一化在神经网络中变得十分常见。其中一个例子，就是它在著名的ResNet架构中的应用。因此我们可以自信地说，它的有效性是毋庸置疑的。
- en: An interesting research [2] on the effectiveness of Batch Norm showed that while
    training a full ResNet-866 network yielded **93** percent accuracy, freezing all
    the parameters and training **only** the parameters of the Batch Norm layers resulted
    in **83** percent accuracy — only a 10% difference.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一项有趣的研究[2]表明，尽管训练完整的ResNet-866网络达到了**93**%的准确率，但冻结所有参数，仅训练批量归一化层的参数，结果只达到了**83**%的准确率——仅有10%的差异。
- en: 'Batch Norm is beneficial in three ways:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化有三个方面的好处：
- en: By normalizing the inputs to each layer, it **accelerates** the training process.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对每一层的输入进行归一化，它**加速**了训练过程。
- en: It reduces the sensitivity to **initial conditions**, meaning we need less careful
    weight initialization.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了对**初始条件**的敏感性，这意味着我们不需要非常小心地初始化权重。
- en: Batch Norm also acts as a **regularizer**. It improves model generalization
    and in some cases, reduces the need for other regularization techniques.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化还充当了**正则化器**。它提高了模型的泛化能力，在某些情况下，减少了对其他正则化技术的需求。
- en: What We Don’t Understand
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们不理解的东西
- en: While the positive effect of the Batch Norm is evident, nobody quite understands
    the reason behind its effectiveness. Originally, the authors of the Batch Normalization
    paper introduced it as a method to solve the problem of **internal covariate shift**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管批量归一化的正面效果显而易见，但没人真正理解其背后的有效性原因。最初，批量归一化论文的作者将其提出作为解决**内部协变量偏移**问题的方法。
- en: '*A heads-up about internal covariate shift is that you will find various definitions
    of it that don’t seem to be related at first.* ***Here is how I would like to
    define it.***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*关于内部协变量偏移的一个提示是，你会发现它有各种不同的定义，乍一看似乎没有关联。* ***这是我想要定义它的方式。***'
- en: The layers of a neural network are updated during the backpropagation from the
    finish (output layer) to the start (input layer). Internal covariate shift refers
    to the phenomenon where the distribution of inputs to a layer changes during training
    as the parameters of the previous layers are updated.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的层是在反向传播过程中从输出层（结束）更新到输入层（开始）的。内部协变量偏移指的是在训练过程中，由于前一层参数的更新，输入层的分布发生变化的现象。
- en: As we change the parameters of the earlier layers, we also change the input
    distribution to the later layers that have been updated to better fit the older
    distribution.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当我们改变前面几层的参数时，也会改变后续层的输入分布，而这些后续层已经更新，以更好地适应较旧的分布。
- en: Internal covariate shift slows down the training and makes it harder for the
    network to converge, as each layer must continuously adapt to the changing distribution
    of its inputs introduced by the update of previous layers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 内部协变量偏移（internal covariate shift）会减缓训练过程，并使网络更难以收敛，因为每一层必须不断适应前一层更新引入的输入分布的变化。
- en: The authors of the original Batch Normalization paper believed that the reason
    behind its effectiveness is because of mitigating the problem of internal covariate
    shift. However, a later paper [3] argued that the success of Batch Norm has nothing
    to do with internal covariate shift, but is due to **smoothing the optimization
    landscape**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 原始批量归一化论文的作者认为，其有效性的原因在于缓解了内部协变量偏移问题。然而，后来的一篇论文[3]认为，批量归一化的成功与内部协变量偏移无关，而是由于**平滑优化景观**的作用。
- en: '![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)'
- en: 'Comparison of two loss landscapes: a highly rough and sharp loss surface (left)
    vs. a smoother loss surface (right). ([Source](https://arxiv.org/abs/1712.09913))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 两种损失景观的比较：一个是高度粗糙和陡峭的损失表面（左），另一个是较为平滑的损失表面（右）。([Source](https://arxiv.org/abs/1712.09913))
- en: The figure above is taken from [4], which is not actually about Batch Normalization,
    but is a good visualization of how a smooth loss landscape looks like. However,
    the theory that the Batch Norm is effective due to smoothing the loss landscape
    has challenges and questions of its own.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上图来自[4]，该文并非关于批量归一化的内容，但很好地展示了平滑损失景观的样子。然而，批量归一化通过平滑损失景观而有效的理论本身也存在挑战和疑问。
- en: Batch Norm Considerations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化的注意事项
- en: 'Due to our limited understanding of how Batch Normalization works, here is
    what to consider when it comes to using them in your network:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对批量归一化工作原理的理解有限，以下是在网络中使用它时需要考虑的事项：
- en: There is a difference between **train** and **inference** modes when using the
    Batch Norm. Using the wrong mode can lead to unexpected behavior that is tricky
    to identify. [5]
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批量归一化时，**训练（train）**和**推理（inference）**模式是不同的。使用错误的模式可能导致难以识别的意外行为。[5]
- en: Batch Norm relies heavily on the size of your **minibatch**. So while it makes
    the need for a careful weight initialization less significant, choosing the right
    size of minibatch becomes more crucial. [5]
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化（Batch Norm）在很大程度上依赖于**小批量（minibatch）**的大小。因此，虽然它减少了对谨慎初始化权重的需求，但选择合适的小批量大小变得更加关键。[5]
- en: There is still an ongoing debate about whether to use Batch Norm before the
    activation function or after it. [6]
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于在激活函数之前还是之后使用批量归一化，目前仍存在争论。[6]
- en: While Batch Norm has a **regularizer** effect, its interaction with other regularizations
    such as *dropout* or *weight decay* is not clearly known.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然批量归一化具有**正则化（regularizer）**的效果，但它与其他正则化方法（如*丢弃法（dropout）*或*权重衰减（weight decay）*）的相互作用尚不完全明确。
- en: There are many questions to answer when it comes to Batch Norms, but research
    is still ongoing to uncover how these layers affect a neural network.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于批量归一化仍有许多问题待解答，但研究仍在进行，以揭示这些层如何影响神经网络。
- en: 2\. Over-Parameterization and Generalization
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 过度参数化与泛化
- en: '![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)'
- en: Face recognition experiments showing that the optimal number of weights in a
    network can be much larger than the number of data points.. ([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 面部识别实验表明，网络中权重的最优数量可以远大于数据点的数量。([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))
- en: Big networks have challenged our old beliefs of how neural networks work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 大型网络挑战了我们过去对神经网络工作原理的理解。
- en: It was traditionally believed using over-parametrized models would result in
    overfitting. Thus the solution would be either to limit the size of the network
    or to add regularization to prevent overfitting to the training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上认为，使用过度参数化的模型会导致过拟合。因此，解决方案通常是限制网络的大小，或添加正则化以防止过拟合训练数据。
- en: Surprisingly, in the case of neural networks, using bigger networks could improve
    the **generalization error** (|train error - test error|). In other words, bigger
    networks generalize better. [7] This is in contradiction to what traditional complexity
    metrics such as [VC dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    — a metric to quantify the difficulty of learning from examples, have promised.
    [8]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 出人意料的是，在神经网络的情况下，使用更大的网络可能会改善**泛化误差**（|训练误差 - 测试误差|）。换句话说，更大的网络有更好的泛化能力。[7]
    这与传统的复杂度度量标准（如[VC维度](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)——一个量化从样本中学习难度的指标）所宣称的内容相矛盾。[8]
- en: This theory also challenges a debate about whether or not deep neural networks
    (DNNs) achieve their performance by *memorizing* training data or by learning
    patterns. [9] If they memorize the data, how could they possibly generalize to
    predict the unseen data? And if they don’t memorize the data but only learn the
    underlying pattern, how do they predict correct labels even when we introduce
    a certain amount of noise to the labels?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这一理论还挑战了一个关于深度神经网络（DNN）是否通过*记忆*训练数据来实现其性能的辩论。[9] 如果它们是通过记忆数据来做的，那它们怎么可能泛化到预测未见过的数据呢？如果它们不记忆数据，而只是学习数据中的潜在模式，那它们又是如何预测正确的标签，即使我们给标签引入了一定的噪声？
- en: '![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)'
- en: overfitting of a classifier. (upscaled — [source](https://commons.wikimedia.org/wiki/File:Overfitting.svg))
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的过拟合。（放大图——[来源](https://commons.wikimedia.org/wiki/File:Overfitting.svg)）
- en: '*Understanding Deep Learning Requires Rethinking Generalization*'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*理解深度学习需要重新思考泛化能力*'
- en: An interesting paper on this subject is *Understanding deep learning requires
    rethinking generalization* [10]. The authors argue that traditional approaches
    fail to explain why larger networks generalize well and at the same time, these
    networks can fit even random data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一主题的有趣论文是*理解深度学习需要重新思考泛化能力*[10]。作者认为，传统的方法无法解释为什么更大的网络能够良好泛化，同时这些网络也能拟合随机数据。
- en: 'A notable part of this paper explains the role of **explicit regularizations**
    such as weight decay, dropout, and data augmentation on the generalization error:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的一部分重点解释了**显式正则化**（如权重衰减、丢弃法和数据增强）在泛化误差中的作用：
- en: Explicit regularization may improve generalization performance, but is neither
    necessary nor by itself sufficient for controlling generalization error. L2-regularization
    (weight decay) sometimes even helps optimization, illustrating its poorly understood
    nature in deep learning. [10]
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显式正则化可能会改善泛化性能，但既不是必要的，也不是单独足以控制泛化误差。L2正则化（权重衰减）有时甚至有助于优化，说明它在深度学习中的理解仍然不完全。[10]
- en: Even with dropout and weight decay, InceptionV3 was still able to fit the **random**
    training set very well beyond expectation. This implication is not to devalue
    regularization, but more to emphasize that bigger gains could be achieved from
    changing the model **architecture**.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用了丢弃法和权重衰减，InceptionV3仍然能够非常好地拟合**随机**训练集，超出了预期。这一含义并不是贬低正则化，而是更强调通过改变模型**架构**可以获得更大的收益。
- en: '![](../Images/e203216819fb3accb40cefdf6557f216.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e203216819fb3accb40cefdf6557f216.png)'
- en: The effect of regularization on generalization. [10]
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化对泛化能力的影响。[10]
- en: 'So what makes a neural network that generalizes well, different from those
    that generalize poorly? It seems like a rabbit hole. We yet need to rethink a
    few things:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让一个能够良好泛化的神经网络与那些泛化不良的网络不同呢？这似乎是一个“兔子洞”。我们仍然需要重新思考一些问题：
- en: Our understanding of a model’s **effective capacity**.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对模型**有效容量**的理解。
- en: Our measurement of a model’s complexityand size. Are model parameters or FLOPs
    simply good metrics? Obviously not.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对模型复杂度和大小的衡量。模型参数或FLOP是否仅仅是好的度量标准？显然不是。
- en: The definition of generalization and how to measure it.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化能力的定义以及如何衡量它。
- en: '![](../Images/cd130cd6128053c768b899942736cb1f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cd130cd6128053c768b899942736cb1f.png)'
- en: As the size of the networks (H) keeps increasing, train and test errors keep
    decreasing and overfitting does not happen. [11]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 随着网络规模（H）的不断增大，训练误差和测试误差持续下降，并且没有发生过拟合。[11]
- en: When it comes to big networks and the effect of parameter count on generalization
    you can find numerous papers and blog posts, some even contradicting others.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于大规模网络和参数数量对泛化能力的影响，已有大量论文和博客文章，其中一些甚至互相矛盾。
- en: Our current understanding could suggest that larger networks can generalize
    well despite their tendency to overfit. This could be due to their depth, allowing
    learning of more complex patterns when compared to shallow networks. This is mostly
    domain-dependant — certain data types may benefit from smaller models and by following
    Occam’s razor principle (don’t miss this post for a further read👇).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前的理解可能表明，尽管较大的网络倾向于过拟合，但它们仍能较好地泛化。这可能是由于它们的深度，使得它们能够学习比浅层网络更复杂的模式。这主要取决于领域——某些数据类型可能更适合使用较小的模型，并遵循奥卡姆剃刀原理（不要错过这篇文章以进一步阅读👇）。
- en: 3\. Implicit Regularization in Neural Networks
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 神经网络中的隐式正则化
- en: At the heart of machine learning lies Gradient Descent — the steps we take to
    find the local minima in a loss landscape. Gradient Descent (GD), along with Stochastic
    Gradient Descent (SGD) is one of the first that is digested by anyone starting
    to learn machine learning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的核心是梯度下降——我们为了在损失景观中找到局部最小值而采取的步骤。梯度下降（GD）以及随机梯度下降（SGD）是任何开始学习机器学习的人首先理解的算法。
- en: As the algorithm seems straightforward, one might expect that it does not have
    much depth. However, you can never find the button of the pool in machine learning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法看起来简单，可能有人认为它没有太多深度。然而，在机器学习中，你永远无法找到池塘的底部。
- en: Do neural networks benefit from an implicit regularization by Gradient Descent
    that pushes them to find *simpler* and *more general* solutions? Could this be
    the reason why over-parametrized networks generalize as shown in the previous
    part?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是否从梯度下降的隐式正则化中受益，这种正则化推动它们找到*更简单*和*更通用*的解决方案？这是否是之前提到的过参数化网络能够泛化的原因？
- en: '![](../Images/304be222467e499f2e5c6c5d56844e50.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/304be222467e499f2e5c6c5d56844e50.png)'
- en: 'Gradient Descent in 2D. (Source: [Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 二维中的梯度下降。（来源：[Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm)）
- en: 'There are two experiments you need to pay close attention to:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个实验需要特别注意：
- en: Experiment 1
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验1
- en: When the authors of [11] trained models on CIFAR-10 and MNIST datasets using
    SGD and no explicit regularization, they concluded that as the size of the network
    increases, the test and training errors keep decreasing. This goes against the
    belief that bigger networks have a higher test error because of overfitting. Even
    after adding more and more parameters to the network, the generalization error
    does not increase. Then they forced the network to overfit by adding random label
    noise. As shown in the figure below, even with 5% random labels, the test error
    decreases further and there are no significant signs of overfitting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当[11]的作者使用SGD并且没有显式正则化，训练CIFAR-10和MNIST数据集的模型时，他们得出结论：随着网络大小的增加，测试和训练误差不断减少。这与认为较大的网络由于过拟合而有更高测试误差的观点相悖。即使在网络中添加更多的参数，泛化误差也没有增加。随后，他们通过添加随机标签噪声强制网络过拟合。如下面的图所示，即使标签随机噪声达到5%，测试误差仍然进一步降低，而且没有明显的过拟合迹象。
- en: '![](../Images/99f78afda3879ffd5d8b592f2199a524.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99f78afda3879ffd5d8b592f2199a524.png)'
- en: Test and Train error of a network with increasing size (H) and 5% noise to the
    labels. Left is MNIST and right is CIFAR-10\. [11]
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 网络随着大小（H）增加并且标签噪声为5%的测试和训练误差。左侧是MNIST，右侧是CIFAR-10。[11]
- en: Experiment 2
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验2
- en: An important paper, *In Search of the Real Inductive Bias* [12], experiments
    by fitting a predictor using linearly separable datasets. The authors show how
    logistic regression, using gradient descent and no regularization, inherently
    biases the solution towards the maximum-margin separator (also known as hard margin
    SVM). This is an interesting and surprising behavior of gradient descent. Because
    even though the loss and the optimization **don’t directly involve** any terms
    that encourage margin maximization (like those you find in Support Vector Machines),
    gradient descent inherently biases the solution towards a max-margin classifier.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇重要的论文，*寻找真实的归纳偏置* [12]，通过拟合一个使用线性可分数据集的预测器进行实验。作者展示了如何在没有正则化的情况下，使用梯度下降的逻辑回归本能地将解偏向于最大间隔分离器（也称为硬间隔SVM）。这是梯度下降的一个有趣且令人惊讶的行为。因为即使损失和优化**并未直接涉及**任何鼓励最大化间隔的项（比如在支持向量机中找到的那些项），梯度下降本能地将解偏向于最大间隔分类器。
- en: '![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)'
- en: 'H3 represents how a hard-margin SVM would classify the dataset. (Source: [Wikimedia
    Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg))'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: H3表示硬间隔支持向量机（SVM）如何对数据集进行分类。（来源：[Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg)）
- en: Gradient Descent as a natural Reguralizer
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降作为一种自然的正则化方法
- en: The experiments suggest an implicit regularization effect as if the optimization
    process favors simpler and more stable solutions. This implies that GD has a preference
    for simpler models, often converging to a special type of local minima referred
    to as “flat” minima, which tends to have **lower generalization error** compared
    to sharper minima. This helps explain why deep learning models often perform well
    on real-world tasks beyond the training data. This suggests that the optimization
    process itself can be considered a form of implicit regularization, leading to
    models that are not only minimal in error on the training data, but also robust
    in their prediction of unseen data. A full theoretical explanation remains an
    active area of research.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明存在一种隐式正则化效应，优化过程似乎偏向更简单和更稳定的解决方案。这意味着梯度下降（GD）更倾向于简单的模型，通常会收敛到一种特殊类型的局部最小值，即“平坦”最小值，相比于更尖锐的最小值，平坦最小值通常具有**较低的泛化误差**。这有助于解释为何深度学习模型在实际任务中往往表现良好，即使在超出训练数据的情况下。这表明优化过程本身可以视为一种隐式正则化，导致模型不仅在训练数据上最小化误差，而且在预测未见数据时也表现出鲁棒性。对此的完整理论解释仍然是一个活跃的研究领域。
- en: 'Perhaps this article could also be interesting to you, on how and why deep
    neural networks are converging into a unified representation of reality:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这篇文章也会对你有趣，讲述了深度神经网络是如何以及为何趋向于统一的现实表示：
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
    [## 柏拉图表示法：AI深度网络模型是否正在趋同？'
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representation…
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工智能模型是否正在朝向统一的现实表示发展？柏拉图表示法……
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)'
- en: 4\. The Lottery Ticket Hypothesis
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 彩票票据假说
- en: Model pruning can reduce the parameters of a trained neural network by 90%.
    If done correctly, this could be achieved without dropping the accuracy. But you
    can only prune your model **after** it has been trained. If we could manage to
    remove the excess parameters **before** training, this would mean using much less
    time and resources.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 模型剪枝可以通过减少训练好的神经网络的参数达到90%。如果操作得当，可以在不降低准确度的情况下实现这一点。但你只能在模型训练完成后**进行**剪枝。如果我们能够在训练前移除多余的参数，这意味着可以使用更少的时间和资源。
- en: The Lottery Ticket Hypothesis [13] argues that a neural network contains subnetworks
    that when trained in isolation, can reach test accuracy comparable to the original
    network. These subnetworks — *the winning tickets,* have the initial weights that
    make their training successful — *the lottery*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 彩票票据假说[13]认为，神经网络包含一些子网络，当它们单独训练时，可以达到与原始网络相当的测试准确度。这些子网络——*中奖的票据*，拥有使其训练成功的初始权重——*彩票*。
- en: 'The authors find these subnetworks through an **iterative pruning** method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过**迭代剪枝**方法找到了这些子网络：
- en: '**Training the Network**: First, they train the original unpruned network.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练网络**：首先，训练原始的未剪枝网络。'
- en: '**Pruning**: After training, they prune **p%** of the weights.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：训练后，剪枝**p%**的权重。'
- en: '**Resetting Weights**: The remaining weights are set to their original values
    from the initial initialization.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重置权重**：剩余的权重被重置为初始初始化时的原始值。'
- en: '**Retraining**: The pruned network is retrained to see if it can reach the
    same or higher performance than the previous network.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**再训练**：剪枝后的网络被重新训练，以查看它是否能达到与之前的网络相同或更高的性能。'
- en: '**Repeat**: Until a desired sparsity of the original network is achieved, or
    the pruned network can no longer match the performance of the unpruned network,
    this process is repeated.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复**：直到达到原始网络期望的稀疏度，或者剪枝后的网络无法再与未剪枝网络匹敌时，才会停止此过程。'
- en: '![](../Images/40897b86443a85999f980a18cdfaea41.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40897b86443a85999f980a18cdfaea41.png)'
- en: Iterative Pruning used in the Lottery Ticket Hypothesis paper. (by author)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代剪枝在《彩票票据假说》论文中的应用。（作者）
- en: The proposed method of iterative training is computationally expensive, requiring
    training a network 15 times or more on multiple experiments.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的迭代训练方法在计算上非常昂贵，要求在多个实验中训练一个网络15次或更多次。
- en: It remains an area of research why we have such phenomena in neural networks.
    Could it be true that SGD only focuses on the winning tickets when training the
    network and not the full body of the network? Why do certain random initializations
    contain such highly effective sub-networks? If you want to dive deep into this
    theory, don’t miss out on [13] and [14].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在神经网络中会出现这种现象仍是一个研究领域。是否有可能是SGD在训练网络时只关注成功的网络部分，而不是网络的全部？为什么某些随机初始化会包含如此高效的子网络？如果你想深入探讨这个理论，不要错过[13]和[14]。
- en: Final Word.
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的话。
- en: '*Thank you for reading through the article!* I have tried my best to provide
    an accurate article, however, please share your opinions and suggestions if you
    think any modifications are required.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢您阅读本文！* 我已经尽力提供一篇准确的文章，但如果您认为需要修改，请分享您的意见和建议。'
- en: Let’s Connect!
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们保持联系！
- en: '*Subscribe for FREE to be notified of new articles! You can also find me on*
    [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *and* [*Twitter*](https://x.com/itsHesamSheikh)*.*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*免费订阅以接收新文章通知！你还可以在* [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/)
    *和* [*Twitter*](https://x.com/itsHesamSheikh)* 上找到我。*'
- en: Further Reads
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'If you have reached so far, you might also find these articles interesting:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经看到这里，你可能也会对以下文章感兴趣：
- en: '[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [## Learn Anything with AI and the Feynman Technique'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [## 利用AI和费曼技巧学习任何东西'
- en: study any concept in four easy steps, by applying AI and a Noble Prize winner
    approach
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过应用AI和诺贝尔奖得主的方法，分四个简单步骤学习任何概念
- en: pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [## A Comprehensive Guide to Collaborative AI Agents in Practice
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [## 实践中的协作AI代理全面指南'
- en: the definition, and building a team of agents that refine your CV and Cover
    Letter for job applications
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义，并建立一个代理团队，精炼你的简历和求职信
- en: towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
    [## I Played Flappy Bird in ChatGPT
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
    [## 我在ChatGPT中玩了Flappy Bird'
- en: GPT-4 is fantastic, but is it good enough to be a Game Engine? I tried this
    with Flappy Bird, using a simple LangChain…
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4非常棒，但它足够好，能够作为游戏引擎吗？我用一个简单的LangChain尝试了这个，做了一个Flappy Bird的游戏……
- en: pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)'
- en: References
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. [arXiv](https://arxiv.org/abs/1502.03167).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Ioffe, S., & Szegedy, C. (2015)。 《批量归一化：通过减少内部协方差偏移加速深度网络训练》。 [arXiv](https://arxiv.org/abs/1502.03167)。'
- en: '[2] [Outside the Norm](https://www.deeplearning.ai/the-batch/outside-the-norm/),
    DeepLearning.AI'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [超出常规](https://www.deeplearning.ai/the-batch/outside-the-norm/)，DeepLearning.AI'
- en: '[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander
    (29 May 2018). “How Does Batch Normalization Help Optimization?”. arXiv:[1805.11604](https://arxiv.org/abs/1805.11604)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander
    (2018年5月29日)。 “批量归一化如何帮助优化？” arXiv：[1805.11604](https://arxiv.org/abs/1805.11604)'
- en: '[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). Visualizing
    the Loss Landscape of Neural Nets. [arXiv](https://arxiv.org/abs/1712.09913)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). 可视化神经网络的损失景观。[arXiv](https://arxiv.org/abs/1712.09913)'
- en: '[5] [On The Perils of Batch Norm](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [批归一化的危险](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)'
- en: '[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)'
- en: '[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018).
    Towards Understanding the Role of Over-Parametrization in Generalization of Neural
    Networks. [arXiv](https://arxiv.org/abs/1805.12076).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018).
    朝着理解过度参数化在神经网络泛化中的作用迈进。[arXiv](https://arxiv.org/abs/1805.12076)'
- en: '[8] [Why is deep learning hyped despite bad VC dimension?](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [为什么深度学习在糟糕的VC维度下仍然被炒作？](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)'
- en: '[9] [DEEP NETS DON’T LEARN VIA MEMORIZATION](https://openreview.net/pdf?id=rJv6ZgHYg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [深度网络不是通过记忆化学习的](https://openreview.net/pdf?id=rJv6ZgHYg)'
- en: '[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding
    Deep Learning Requires Rethinking Generalization. [*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). 理解深度学习需要重新思考泛化问题。[*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)'
- en: '[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). In Search of the Real
    Inductive Bias: On the Role of Implicit Regularization in Deep Learning. *arXiv:1412.6614*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). 寻找真实的归纳偏差：关于隐式正则化在深度学习中的作用。*arXiv:1412.6614*'
- en: '[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017).
    The Implicit Bias of Gradient Descent on Separable Data. *arXiv:1710.10345*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017).
    梯度下降在可分数据上的隐式偏差。*arXiv:1710.10345*'
- en: '[13] Frankle, J., & Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding
    Sparse, Trainable Neural Networks. *arXiv:1803.03635*'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Frankle, J., & Carbin, M. (2019). 彩票票假设：寻找稀疏的、可训练的神经网络。*arXiv:1803.03635*'
- en: '[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)'
