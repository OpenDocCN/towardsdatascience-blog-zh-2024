- en: What We Still Don’t Understand About Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26](https://towardsdatascience.com/what-we-still-dont-understand-about-machine-learning-699e0002a057?source=collection_archive---------1-----------------------#2024-07-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Machine Learning unknowns that researchers struggle to understand — from Batch
    Norm to what SGD hides
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--699e0002a057--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--699e0002a057--------------------------------)
    ·12 min read·Jul 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a58ae922fc1a99ffd5a8d87485bad57.png)'
  prefs: []
  type: TYPE_IMG
- en: What We Still Don’t Understand About Machine Learning. (by author)
  prefs: []
  type: TYPE_NORMAL
- en: If you’re not a member, [read for free here](/what-we-still-dont-understand-about-machine-learning-699e0002a057?sk=e2498900bd4ada1e8abe37e056f6ebc9)
    👈
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is surprising how some of the basic subjects in machine learning are still
    unknown by researchers and despite being fundamental and common to use, seem to
    be mysterious. It’s a fun thing about machine learning that we build things that
    work and then figure out why they work at all!
  prefs: []
  type: TYPE_NORMAL
- en: Here, I aim to investigate the unknown territory in some machine learning concepts
    in order to show while these ideas can seem basic, in reality, they are constructed
    by layers upon layers of abstraction. This helps us to practice questioning the
    **depth of our knowledge**.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explore several key phenomena in deep learning that challenge
    our traditional understanding of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We start with **Batch Normalization** and its underlying mechanisms that remain
    not fully understood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We examine the counterintuitive observation that **overparameterized** models
    often **generalize better**, contradicting the classical machine learning theories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explore the implicit regularization effects of **gradient descent**, which
    seem to naturally bias neural networks towards simpler, more generalizable solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we touch on the Lottery Ticket Hypothesis, which proposes that large
    neural networks contain smaller subnetworks capable of achieving **comparable
    performance** when trained in isolation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*This article contains 4 parts, they are not connected so feel free to skip
    to the ones you like more.*'
  prefs: []
  type: TYPE_NORMAL
- en: · [1\. Batch Normalization](#de8b)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [What We Don’t Understand](#95c8)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Batch Norm Considerations](#ec44)
  prefs: []
  type: TYPE_NORMAL
- en: · [2\. Over-Parameterization and Generalization](#82b3)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Understanding Deep Learning Requires Rethinking Generalization](#6185)
  prefs: []
  type: TYPE_NORMAL
- en: · [3\. Implicit Regularization in Neural Networks](#f626)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Experiment 1](#2783)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Experiment 2](#ff53)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Gradient Descent as a natural Reguralizer](#85f8)
  prefs: []
  type: TYPE_NORMAL
- en: · [4\. The Lottery Ticket Hypothesis](#b35e)
  prefs: []
  type: TYPE_NORMAL
- en: · [Final Word.](#821e)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Let’s Connect!](#03ee)
  prefs: []
  type: TYPE_NORMAL
- en: · [Further Reads](#397f)
  prefs: []
  type: TYPE_NORMAL
- en: · [References](#f3e3)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Batch Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduced by *Sergey Ioffe* and *Christian Szegedy* in 2015 [1], batch normalization
    is a method to train neural networks faster and more stable. It was previously
    known that transforming the input data to have their mean set to zero and variance
    to one would result in a faster convergence. The authors used this idea further
    and introduced **Batch Normalization** to transform the input of hidden layers
    to have a mean of zero and a variance of one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d64e4c77a25558d72ba330536db8ba1.png)'
  prefs: []
  type: TYPE_IMG
- en: An outline of the Residual Block used in Resnet and the use of Batch Norm in
    it. (by author)
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction, the batch norm has become common in neural networks.
    One such example, among many, is its use in the famous ResNet architecture. So
    we could confidently say we are certain of how effective it could be.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting research [2] on the effectiveness of Batch Norm showed that while
    training a full ResNet-866 network yielded **93** percent accuracy, freezing all
    the parameters and training **only** the parameters of the Batch Norm layers resulted
    in **83** percent accuracy — only a 10% difference.
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch Norm is beneficial in three ways:'
  prefs: []
  type: TYPE_NORMAL
- en: By normalizing the inputs to each layer, it **accelerates** the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces the sensitivity to **initial conditions**, meaning we need less careful
    weight initialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch Norm also acts as a **regularizer**. It improves model generalization
    and in some cases, reduces the need for other regularization techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What We Don’t Understand
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the positive effect of the Batch Norm is evident, nobody quite understands
    the reason behind its effectiveness. Originally, the authors of the Batch Normalization
    paper introduced it as a method to solve the problem of **internal covariate shift**.
  prefs: []
  type: TYPE_NORMAL
- en: '*A heads-up about internal covariate shift is that you will find various definitions
    of it that don’t seem to be related at first.* ***Here is how I would like to
    define it.***'
  prefs: []
  type: TYPE_NORMAL
- en: The layers of a neural network are updated during the backpropagation from the
    finish (output layer) to the start (input layer). Internal covariate shift refers
    to the phenomenon where the distribution of inputs to a layer changes during training
    as the parameters of the previous layers are updated.
  prefs: []
  type: TYPE_NORMAL
- en: As we change the parameters of the earlier layers, we also change the input
    distribution to the later layers that have been updated to better fit the older
    distribution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Internal covariate shift slows down the training and makes it harder for the
    network to converge, as each layer must continuously adapt to the changing distribution
    of its inputs introduced by the update of previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the original Batch Normalization paper believed that the reason
    behind its effectiveness is because of mitigating the problem of internal covariate
    shift. However, a later paper [3] argued that the success of Batch Norm has nothing
    to do with internal covariate shift, but is due to **smoothing the optimization
    landscape**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f88c5bb3a672aa66fcd7373c0c431a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparison of two loss landscapes: a highly rough and sharp loss surface (left)
    vs. a smoother loss surface (right). ([Source](https://arxiv.org/abs/1712.09913))'
  prefs: []
  type: TYPE_NORMAL
- en: The figure above is taken from [4], which is not actually about Batch Normalization,
    but is a good visualization of how a smooth loss landscape looks like. However,
    the theory that the Batch Norm is effective due to smoothing the loss landscape
    has challenges and questions of its own.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Norm Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to our limited understanding of how Batch Normalization works, here is
    what to consider when it comes to using them in your network:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a difference between **train** and **inference** modes when using the
    Batch Norm. Using the wrong mode can lead to unexpected behavior that is tricky
    to identify. [5]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch Norm relies heavily on the size of your **minibatch**. So while it makes
    the need for a careful weight initialization less significant, choosing the right
    size of minibatch becomes more crucial. [5]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is still an ongoing debate about whether to use Batch Norm before the
    activation function or after it. [6]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While Batch Norm has a **regularizer** effect, its interaction with other regularizations
    such as *dropout* or *weight decay* is not clearly known.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many questions to answer when it comes to Batch Norms, but research
    is still ongoing to uncover how these layers affect a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Over-Parameterization and Generalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c4c6c3f08203f8d9473bd7ce0197c417.png)'
  prefs: []
  type: TYPE_IMG
- en: Face recognition experiments showing that the optimal number of weights in a
    network can be much larger than the number of data points.. ([source](https://clgiles.ist.psu.edu/papers/UMD-CS-TR-3617.what.size.neural.net.to.use.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: Big networks have challenged our old beliefs of how neural networks work.
  prefs: []
  type: TYPE_NORMAL
- en: It was traditionally believed using over-parametrized models would result in
    overfitting. Thus the solution would be either to limit the size of the network
    or to add regularization to prevent overfitting to the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly, in the case of neural networks, using bigger networks could improve
    the **generalization error** (|train error - test error|). In other words, bigger
    networks generalize better. [7] This is in contradiction to what traditional complexity
    metrics such as [VC dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    — a metric to quantify the difficulty of learning from examples, have promised.
    [8]
  prefs: []
  type: TYPE_NORMAL
- en: This theory also challenges a debate about whether or not deep neural networks
    (DNNs) achieve their performance by *memorizing* training data or by learning
    patterns. [9] If they memorize the data, how could they possibly generalize to
    predict the unseen data? And if they don’t memorize the data but only learn the
    underlying pattern, how do they predict correct labels even when we introduce
    a certain amount of noise to the labels?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8dbde067c84e6356a774b2dcf646f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: overfitting of a classifier. (upscaled — [source](https://commons.wikimedia.org/wiki/File:Overfitting.svg))
  prefs: []
  type: TYPE_NORMAL
- en: '*Understanding Deep Learning Requires Rethinking Generalization*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An interesting paper on this subject is *Understanding deep learning requires
    rethinking generalization* [10]. The authors argue that traditional approaches
    fail to explain why larger networks generalize well and at the same time, these
    networks can fit even random data.
  prefs: []
  type: TYPE_NORMAL
- en: 'A notable part of this paper explains the role of **explicit regularizations**
    such as weight decay, dropout, and data augmentation on the generalization error:'
  prefs: []
  type: TYPE_NORMAL
- en: Explicit regularization may improve generalization performance, but is neither
    necessary nor by itself sufficient for controlling generalization error. L2-regularization
    (weight decay) sometimes even helps optimization, illustrating its poorly understood
    nature in deep learning. [10]
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Even with dropout and weight decay, InceptionV3 was still able to fit the **random**
    training set very well beyond expectation. This implication is not to devalue
    regularization, but more to emphasize that bigger gains could be achieved from
    changing the model **architecture**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e203216819fb3accb40cefdf6557f216.png)'
  prefs: []
  type: TYPE_IMG
- en: The effect of regularization on generalization. [10]
  prefs: []
  type: TYPE_NORMAL
- en: 'So what makes a neural network that generalizes well, different from those
    that generalize poorly? It seems like a rabbit hole. We yet need to rethink a
    few things:'
  prefs: []
  type: TYPE_NORMAL
- en: Our understanding of a model’s **effective capacity**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our measurement of a model’s complexityand size. Are model parameters or FLOPs
    simply good metrics? Obviously not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of generalization and how to measure it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/cd130cd6128053c768b899942736cb1f.png)'
  prefs: []
  type: TYPE_IMG
- en: As the size of the networks (H) keeps increasing, train and test errors keep
    decreasing and overfitting does not happen. [11]
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to big networks and the effect of parameter count on generalization
    you can find numerous papers and blog posts, some even contradicting others.
  prefs: []
  type: TYPE_NORMAL
- en: Our current understanding could suggest that larger networks can generalize
    well despite their tendency to overfit. This could be due to their depth, allowing
    learning of more complex patterns when compared to shallow networks. This is mostly
    domain-dependant — certain data types may benefit from smaller models and by following
    Occam’s razor principle (don’t miss this post for a further read👇).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implicit Regularization in Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the heart of machine learning lies Gradient Descent — the steps we take to
    find the local minima in a loss landscape. Gradient Descent (GD), along with Stochastic
    Gradient Descent (SGD) is one of the first that is digested by anyone starting
    to learn machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As the algorithm seems straightforward, one might expect that it does not have
    much depth. However, you can never find the button of the pool in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Do neural networks benefit from an implicit regularization by Gradient Descent
    that pushes them to find *simpler* and *more general* solutions? Could this be
    the reason why over-parametrized networks generalize as shown in the previous
    part?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/304be222467e499f2e5c6c5d56844e50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gradient Descent in 2D. (Source: [Wikimedia Commons](https://en.wikipedia.org/wiki/File:Gradient_Descent_in_2D.webm))'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two experiments you need to pay close attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the authors of [11] trained models on CIFAR-10 and MNIST datasets using
    SGD and no explicit regularization, they concluded that as the size of the network
    increases, the test and training errors keep decreasing. This goes against the
    belief that bigger networks have a higher test error because of overfitting. Even
    after adding more and more parameters to the network, the generalization error
    does not increase. Then they forced the network to overfit by adding random label
    noise. As shown in the figure below, even with 5% random labels, the test error
    decreases further and there are no significant signs of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99f78afda3879ffd5d8b592f2199a524.png)'
  prefs: []
  type: TYPE_IMG
- en: Test and Train error of a network with increasing size (H) and 5% noise to the
    labels. Left is MNIST and right is CIFAR-10\. [11]
  prefs: []
  type: TYPE_NORMAL
- en: Experiment 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important paper, *In Search of the Real Inductive Bias* [12], experiments
    by fitting a predictor using linearly separable datasets. The authors show how
    logistic regression, using gradient descent and no regularization, inherently
    biases the solution towards the maximum-margin separator (also known as hard margin
    SVM). This is an interesting and surprising behavior of gradient descent. Because
    even though the loss and the optimization **don’t directly involve** any terms
    that encourage margin maximization (like those you find in Support Vector Machines),
    gradient descent inherently biases the solution towards a max-margin classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67d2bde1fbdf0706ff73b0f3baeddee9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'H3 represents how a hard-margin SVM would classify the dataset. (Source: [Wikimedia
    Commons](https://commons.wikimedia.org/wiki/File:Svm_separating_hyperplanes_(SVG).svg))'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent as a natural Reguralizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The experiments suggest an implicit regularization effect as if the optimization
    process favors simpler and more stable solutions. This implies that GD has a preference
    for simpler models, often converging to a special type of local minima referred
    to as “flat” minima, which tends to have **lower generalization error** compared
    to sharper minima. This helps explain why deep learning models often perform well
    on real-world tasks beyond the training data. This suggests that the optimization
    process itself can be considered a form of implicit regularization, leading to
    models that are not only minimal in error on the training data, but also robust
    in their prediction of unseen data. A full theoretical explanation remains an
    active area of research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perhaps this article could also be interesting to you, on how and why deep
    neural networks are converging into a unified representation of reality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
    [## Platonic Representation: Are AI Deep Network Models Converging?'
  prefs: []
  type: TYPE_NORMAL
- en: Are Artificial Intelligence models evolving towards a unified representation
    of reality? The Platonic Representation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/platonic-representation-hypothesis-c812813d7248?source=post_page-----699e0002a057--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The Lottery Ticket Hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model pruning can reduce the parameters of a trained neural network by 90%.
    If done correctly, this could be achieved without dropping the accuracy. But you
    can only prune your model **after** it has been trained. If we could manage to
    remove the excess parameters **before** training, this would mean using much less
    time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: The Lottery Ticket Hypothesis [13] argues that a neural network contains subnetworks
    that when trained in isolation, can reach test accuracy comparable to the original
    network. These subnetworks — *the winning tickets,* have the initial weights that
    make their training successful — *the lottery*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors find these subnetworks through an **iterative pruning** method:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training the Network**: First, they train the original unpruned network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning**: After training, they prune **p%** of the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resetting Weights**: The remaining weights are set to their original values
    from the initial initialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retraining**: The pruned network is retrained to see if it can reach the
    same or higher performance than the previous network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repeat**: Until a desired sparsity of the original network is achieved, or
    the pruned network can no longer match the performance of the unpruned network,
    this process is repeated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/40897b86443a85999f980a18cdfaea41.png)'
  prefs: []
  type: TYPE_IMG
- en: Iterative Pruning used in the Lottery Ticket Hypothesis paper. (by author)
  prefs: []
  type: TYPE_NORMAL
- en: The proposed method of iterative training is computationally expensive, requiring
    training a network 15 times or more on multiple experiments.
  prefs: []
  type: TYPE_NORMAL
- en: It remains an area of research why we have such phenomena in neural networks.
    Could it be true that SGD only focuses on the winning tickets when training the
    network and not the full body of the network? Why do certain random initializations
    contain such highly effective sub-networks? If you want to dive deep into this
    theory, don’t miss out on [13] and [14].
  prefs: []
  type: TYPE_NORMAL
- en: Final Word.
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Thank you for reading through the article!* I have tried my best to provide
    an accurate article, however, please share your opinions and suggestions if you
    think any modifications are required.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s Connect!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Subscribe for FREE to be notified of new articles! You can also find me on*
    [*LinkedIn*](https://www.linkedin.com/in/hesamsheikh/) *and* [*Twitter*](https://x.com/itsHesamSheikh)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you have reached so far, you might also find these articles interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [## Learn Anything with AI and the Feynman Technique'
  prefs: []
  type: TYPE_NORMAL
- en: study any concept in four easy steps, by applying AI and a Noble Prize winner
    approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/learn-anything-with-ai-and-the-feynman-technique-00a33f6a02bc?source=post_page-----699e0002a057--------------------------------)
    [](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [## A Comprehensive Guide to Collaborative AI Agents in Practice
  prefs: []
  type: TYPE_NORMAL
- en: the definition, and building a team of agents that refine your CV and Cover
    Letter for job applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-comprehensive-guide-to-collaborative-ai-agents-in-practice-1f4048947d9c?source=post_page-----699e0002a057--------------------------------)
    [](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
    [## I Played Flappy Bird in ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 is fantastic, but is it good enough to be a Game Engine? I tried this
    with Flappy Bird, using a simple LangChain…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/chatgpt-as-a-game-engine-to-play-flappy-bird-ee4adff46f48?source=post_page-----699e0002a057--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. [arXiv](https://arxiv.org/abs/1502.03167).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Outside the Norm](https://www.deeplearning.ai/the-batch/outside-the-norm/),
    DeepLearning.AI'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Santurkar, Shibani; Tsipras, Dimitris; Ilyas, Andrew; Madry, Aleksander
    (29 May 2018). “How Does Batch Normalization Help Optimization?”. arXiv:[1805.11604](https://arxiv.org/abs/1805.11604)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2018). Visualizing
    the Loss Landscape of Neural Nets. [arXiv](https://arxiv.org/abs/1712.09913)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [On The Perils of Batch Norm](https://www.alexirpan.com/2017/04/26/perils-batch-norm.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [https://x.com/svpino/status/1588501331316121601](https://x.com/svpino/status/1588501331316121601)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., & Srebro, N. (2018).
    Towards Understanding the Role of Over-Parametrization in Generalization of Neural
    Networks. [arXiv](https://arxiv.org/abs/1805.12076).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [Why is deep learning hyped despite bad VC dimension?](https://cs.stackexchange.com/questions/75327/why-is-deep-learning-hyped-despite-bad-vc-dimension)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [DEEP NETS DON’T LEARN VIA MEMORIZATION](https://openreview.net/pdf?id=rJv6ZgHYg)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2017). Understanding
    Deep Learning Requires Rethinking Generalization. [*arXiv:1611.03530*](https://arxiv.org/abs/1611.03530)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Neyshabur, B., Tomioka, R., & Srebro, N. (2015). In Search of the Real
    Inductive Bias: On the Role of Implicit Regularization in Deep Learning. *arXiv:1412.6614*'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2017).
    The Implicit Bias of Gradient Descent on Separable Data. *arXiv:1710.10345*'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Frankle, J., & Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding
    Sparse, Trainable Neural Networks. *arXiv:1803.03635*'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] [https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis](https://www.lesswrong.com/posts/Z7R6jFjce3J2Ryj44/exploring-the-lottery-ticket-hypothesis)'
  prefs: []
  type: TYPE_NORMAL
