["```py\nclass Attention(nn.Module):\n    def __init__(self, \n                dim: int,\n                chan: int,\n                num_heads: int=1,\n                qkv_bias: bool=False,\n                qk_scale: NoneFloat=None):\n\n        \"\"\" Attention Module\n\n            Args:\n                dim (int): input size of a single token\n                chan (int): resulting size of a single token (channels)\n                num_heads(int): number of attention heads in MSA\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Constants\n        self.num_heads = num_heads\n        self.chan = chan\n        self.head_dim = self.chan // self.num_heads\n        self.scale = qk_scale or self.head_dim ** -0.5\n        assert self.chan % self.num_heads == 0, '\"Chan\" must be evenly divisible by \"num_heads\".'\n\n        ## Define Layers\n        self.qkv = nn.Linear(dim, chan * 3, bias=qkv_bias)\n        #### Each token gets projected from starting length (dim) to channel length (chan) 3 times (for each Q, K, V)\n        self.proj = nn.Linear(chan, chan)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        ## Dimensions: (batch, num_tokens, token_len)\n\n        ## Calcuate QKVs\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        #### Dimensions: (3, batch, heads, num_tokens, chan/num_heads = head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        ## Calculate Attention\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        #### Dimensions: (batch, heads, num_tokens, num_tokens)\n\n        ## Attention Layer\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.chan)\n        #### Dimensions: (batch, heads, num_tokens, chan)\n\n        ## Projection Layers\n        x = self.proj(x)\n\n        ## Skip Connection Layer\n        v = v.transpose(1, 2).reshape(B, N, self.chan)\n        x = v + x     \n        #### Because the original x has different size with current x, use v to do skip connection\n\n        return x\n```", "```py\n# Define an Input\ntoken_len = 7*7\nchannels = 64\nnum_tokens = 100\nbatch = 13\nx = torch.rand(batch, num_tokens, token_len)\nB, N, C = x.shape\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n\n# Define the Module\nA = Attention(dim=token_len, chan=channels, num_heads=1, qkv_bias=False, qk_scale=None)\nA.eval();\n```", "```py\nInput dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 49\n```", "```py\nqkv = A.qkv(x).reshape(B, N, 3, A.num_heads, A.head_dim).permute(2, 0, 3, 1, 4)\nq, k, v = qkv[0], qkv[1], qkv[2]\nprint('Dimensions for Queries are\\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\nprint('See that the dimensions for queries, keys, and values are all the same:')\nprint('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)\n```", "```py\nDimensions for Queries are\n   batchsize: 13 \n   attention heads: 1 \n   number of tokens: 100 \n   new length of tokens: 64\nSee that the dimensions for queries, keys, and values are all the same:\n   Shape of Q: torch.Size([13, 1, 100, 64]) \n   Shape of K: torch.Size([13, 1, 100, 64]) \n   Shape of V: torch.Size([13, 1, 100, 64])\n```", "```py\nattn = (q * A.scale) @ k.transpose(-2, -1)\nprint('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n```", "```py\nDimensions for Attn are\n   batchsize: 13 \n   attention heads: 1 \n   number of tokens: 100 \n   number of tokens: 100\n```", "```py\nattn = attn.softmax(dim=-1)\nprint('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n```", "```py\nDimensions for Attn are\n   batchsize: 13 \n   attention heads: 1 \n   number of tokens: 100 \n   number of tokens: 100\n```", "```py\nx = attn @ v\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   attention heads: 1 \n   number of tokens: 100 \n   length of tokens: 64\n```", "```py\nx = x.transpose(1, 2).reshape(B, N, A.chan)\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\n```", "```py\nx = A.proj(x)\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\n```", "```py\norig_shape = (batch, num_tokens, token_len)\ncurr_shape = (x.shape[0], x.shape[1], x.shape[2])\nv = v.transpose(1, 2).reshape(B, N, A.chan)\nv_shape = (v.shape[0], v.shape[1], v.shape[2])\nprint('Original shape of input x:', orig_shape)\nprint('Current shape of x:', curr_shape)\nprint('Shape of V:', v_shape)\nx = v + x     \nprint('After skip connection, dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n```", "```py\nOriginal shape of input x: (13, 100, 49)\nCurrent shape of x: (13, 100, 64)\nShape of V: (13, 100, 64)\nAfter skip connection, dimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\n```", "```py\n# Define an Input\ntoken_len = 7*7\nchannels = 64\nnum_tokens = 100\nbatch = 13\nnum_heads = 4\nx = torch.rand(batch, num_tokens, token_len)\nB, N, C = x.shape\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n\n# Define the Module\nMSA = Attention(dim=token_len, chan=channels, num_heads=num_heads, qkv_bias=False, qk_scale=None)\nMSA.eval();\n```", "```py\nInput dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 49\n```", "```py\nqkv = MSA.qkv(x).reshape(B, N, 3, MSA.num_heads, MSA.head_dim).permute(2, 0, 3, 1, 4)\nq, k, v = qkv[0], qkv[1], qkv[2]\nprint('Head Dimension = chan / num_heads =', MSA.chan, '/', MSA.num_heads, '=', MSA.head_dim)\nprint('Dimensions for Queries are\\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\nprint('See that the dimensions for queries, keys, and values are all the same:')\nprint('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)\n```", "```py\nHead Dimension = chan / num_heads = 64 / 4 = 16\nDimensions for Queries are\n   batchsize: 13 \n   attention heads: 4 \n   number of tokens: 100 \n   new length of tokens: 16\nSee that the dimensions for queries, keys, and values are all the same:\n   Shape of Q: torch.Size([13, 4, 100, 16]) \n   Shape of K: torch.Size([13, 4, 100, 16]) \n   Shape of V: torch.Size([13, 4, 100, 16])\n```", "```py\nattn = (q * MSA.scale) @ k.transpose(-2, -1)\nprint('Dimensions for Attn are\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n```", "```py\nDimensions for Attn are\n   batchsize: 13 \n   attention heads: 4 \n   number of tokens: 100 \n   number of tokens: 100\n```", "```py\nattn = attn.softmax(dim=-1)\n\nx = attn @ v\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   attention heads: 4 \n   number of tokens: 100 \n   length of tokens: 16\n```", "```py\nx = x.transpose(1, 2).reshape(B, N, MSA.chan)\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\n```", "```py\nx = MSA.proj(x)\nprint('Dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n\norig_shape = (batch, num_tokens, token_len)\ncurr_shape = (x.shape[0], x.shape[1], x.shape[2])\nv = v.transpose(1, 2).reshape(B, N, A.chan)\nv_shape = (v.shape[0], v.shape[1], v.shape[2])\nprint('Original shape of input x:', orig_shape)\nprint('Current shape of x:', curr_shape)\nprint('Shape of V:', v_shape)\nx = v + x     \nprint('After skip connection, dimensions for x are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n```", "```py\nDimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\nOriginal shape of input x: (13, 100, 49)\nCurrent shape of x: (13, 100, 64)\nShape of V: (13, 100, 64)\nAfter skip connection, dimensions for x are\n   batchsize: 13 \n   number of tokens: 100 \n   length of tokens: 64\n```"]