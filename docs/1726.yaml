- en: Topic Modeling Open-Source Research with the OpenAlex API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15](https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Open-source intelligence (OSINT) is something that can add tremendous value
    to organizations. Insight gained from analyzing social media data, web data, or
    global research, can be great in supporting all kinds of analyses. This article
    will give an overview of using topic modeling to help us make sense of thousands
    of pieces of open-source research.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------)
    ·9 min read·Jul 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee2aac991c4193c160d128a71f553a42.png)'
  prefs: []
  type: TYPE_IMG
- en: '“resorting to paper… #research# #proposal” by [catherinecronin](https://www.flickr.com/photos/catherinecronin/14326101068)
    is licensed under CC BY-SA 2.0\. To view a copy of this license, visit [https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.](https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.)'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Topic Modeling?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is an unsupervised machine learning technique used to analyze
    documents and identity ‘topics’ using semantic similarity. This is similar to
    clustering, but not every document is exclusive to one topic. It is more about
    grouping the content found in a corpus. Topic modeling has many different applications
    but is mainly used to better understand large amounts of text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae6dbdf009e015f0e89bfb45c5c4be55.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: For example, a retail chain may model customer surveys and reviews to identify
    negative reviews and drill down into the key issues outlined by their customers.
    In this case, we will import a large amount of articles and abstracts to understand
    the key topics in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Topic modeling can be computationally expensive at scale. In this example,
    I used the Amazon Sagemaker environment to take advantage of their CPU.*'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAlex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAlex is a free to use catalogue system of global research. They have indexed
    over 250 million pieces of news, articles, abstracts, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '[## OpenAlex'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: openalex.org](https://openalex.org/?source=post_page-----5191c7db9156--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Luckily for us, they have a free (but limited) and flexible API that will allow
    us to quickly ingest tens of thousands of articles while also applying filters,
    such as year, type of media, keywords, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While we ingest the data from the API, we will apply some criteria. First, we
    will only ingest documents where the year is between 2016 and 2022\. We want fairly
    recent language as terms and taxonomy of certain subjects can change over long
    periods of time.
  prefs: []
  type: TYPE_NORMAL
- en: We will also add key terms and conduct multiple searches. While normally we
    would likely ingest random subject areas, we will use key terms to narrow our
    search. This way, we will have an idea of how may high-level topics we have, and
    can compare that to the output of the model. Below, we create a function where
    we can add key terms and conduct searches through the API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We conduct 5 different searches, each being a different technology area. These
    technology areas are inspired by the DoD “Critical Technology Areas”. See more
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)
    [## USD(R&E) Strategic Vision and Critical Technology Areas - DoD Research & Engineering,
    OUSD(R&E)'
  prefs: []
  type: TYPE_NORMAL
- en: The OUSD(R&E) works closely with the Military Services, Combatant Commands,
    industry, academia, and other stakeholders…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.cto.mil](https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a search using the required OpenAlex syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After compiling our searches and dropping duplicate documents, we must clean
    the data to prepare it for our topic model. There are 2 main issues with our current
    output.
  prefs: []
  type: TYPE_NORMAL
- en: The abstracts are returned as an inverted index (due to legal reasons). However,
    we can use these to return the original text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we obtain the original text, it will be raw and unprocessed, creating noise
    and hurting our model. We will conduct traditional NLP preprocessing to get it
    ready for the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Below is a function to return original text from an inverted index.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the raw text, we can conduct our traditional preprocessing
    steps, such as standardization, removing stop words, lemmatization, etc. Below
    are functions that can be mapped to a list or series of documents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a preprocessed series of documents, we can create our first
    topic model!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Topic Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our topic model, we will use gensim to create a Latent Dirichlet Allocation
    (LDA) model. LDA is the most common model for topic modeling, as it is very effective
    in identifying high-level themes within a corpus. Below are the packages used
    to create the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Before we create our model, we must prepare our corpus and ID mappings. This
    can be done with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we can create a topic model. As you will see below, there are many different
    parameters that will affect the model’s performance. You can read about the many
    parameters in gensim’s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The most import parameter will be the number of topics. Here, we set an arbitrary
    10\. Since we don’t know how many topics there *should* be, this parameter should
    definitely be optimized. But how do we measure the quality of our model?
  prefs: []
  type: TYPE_NORMAL
- en: This is where coherence scores come in. The coherence score is a measure from
    0–1\. Coherence scores measure the quality of our topics by making sure they are
    sound and distinct. We want clear boundaries between well-defined topics. While
    this is a bit subjective in the end, it gives us a great idea of the quality of
    our results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we get a coherence score of about 0.48, which isn’t too bad! But not ready
    for production.
  prefs: []
  type: TYPE_NORMAL
- en: Visualize Our Topic Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic models can be difficult to visualize. Lucky for us, there is a great module
    ‘pyLDAvis’ that can automatically produce an interactive visualization that allows
    us to view our topics in a vector space and drill down into each topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see below, this produces a great visualization where we can get a
    quick idea of how our model performed. By looking into the vector space, we see
    some topics are distinct and well-defined. However, we also have some overlapping
    topics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/528452b69c598401246c2e00ad1dcb42.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We can click on a topic to few the most relevant tokens. As we adjust the relevance
    metric (lambda), we can see topic-specific tokens by sliding it left, and seeing
    relevant but less topic-specific tokens by sliding it to the right.
  prefs: []
  type: TYPE_NORMAL
- en: When clicking into each topic, I can vaguely see the topics that I originally
    searched for. For example, topic 5 seems to align with my ‘human-machine interfaces’
    search. There is also a cluster of topics that seem to be related to biotechnology,
    but some are more clear than others.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize the Topic Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the pyLDAvis interface and our coherence score of 0.48, there is definitely
    room for improvement. For our final step, lets write a function where we can loop
    through values for different parameters and try to optimize our coherence score.
    Below, is a function that tests different values of the number of topics and the
    decay rate. The function computes the coherence score for every combination of
    parameters and saves them in a data frame.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Just by passing a couple of small ranges through two parameters, we identified
    parameters that increased our coherence score from 0.48 to 0.55, a sizable improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To continue to build a production-level model, there is plenty of experimentation
    to be had with the parameters. Because LDA is so computationally expensive, I
    kept the experiment above limited and only compared about 20 different models.
    But with more time and power, we can compare hundreds of models.
  prefs: []
  type: TYPE_NORMAL
- en: As well, there are improvements to be made with our data pipeline. I noticed
    several words that may need to be added to our stop word list. Words like ‘use’
    and ‘department’ are not adding any semantic value, especially for documents about
    different technologies. As well, there are technical terms that do not get processed
    correctly, resulting in a single letter or a group of letters. We could spend
    some time doing a bag-of-words analysis to identify those stop word opportunities.
    This would eliminate noise in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Got an introduction to topic modeling and the OpenAlex data source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Built a data pipeline to ingest data from an API and prepare it for an NLP model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constructed an LDA model and visualized the results using pyLDAvis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrote code to help us find the optimal parameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discussed next steps for model improvement
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*This is my first Medium article, so I hope you enjoyed it. Please feel free
    to leave feedback, ask questions, or request other topics!*'
  prefs: []
  type: TYPE_NORMAL
