- en: 'From Local to Cloud: Estimating GPU Resources for Open-Source LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18](https://towardsdatascience.com/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f?source=collection_archive---------4-----------------------#2024-11-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Estimating GPU memory for deploying the latest open-source LLMs*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[![Maxime
    Jabarian](../Images/d6c2198e2e3259ae98b5bbe0e3079768.png)](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    [Maxime Jabarian](https://medium.com/@maximejabarian?source=post_page---byline--b4a015a0174f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4a015a0174f--------------------------------)
    ·4 min read·Nov 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcea4003ded428dc0835ca4f0ec07375.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Source](https://unsplash.com/fr/photos/gros-plan-dune-carte-video-sur-fond-jaune-ipVMl4H6g6o)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re like me, you probably get excited about the latest and greatest open-source
    LLMs — from models like Llama 3 to the more compact Phi-3 Mini. But before you
    jump into deploying your language model, there’s one crucial factor you need to
    plan for: **GPU memory**. Misjudge this, and your shiny new web app might choke,
    run sluggishly, or rack up hefty cloud bills. To make things easier, I explain
    to you what’s quantization, and I’ve prepared for you a ***GPU Memory Planning
    Cheat Sheet in 2024***— a handy summary of the latest open-source LLMs on the
    market and what you need to know before deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are not a member, [**read here**](https://medium.com/towards-data-science/from-local-to-cloud-estimating-gpu-resources-for-open-source-llms-b4a015a0174f)**.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Bother Estimating GPU Memory?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When deploying LLMs, guessing how much GPU memory you need is risky. Too little,
    and your model crashes. Too much, and you’re burning money for no reason.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding these memory requirements upfront is like knowing how much luggage
    you can fit in your car before a road trip — it saves headaches and keeps things
    efficient.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
