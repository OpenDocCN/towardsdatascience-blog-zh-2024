- en: Deep Dive into LSTMs and xLSTMs by Hand ✍️
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动深入探讨LSTMs和xLSTMs ✍️
- en: 原文：[https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09](https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09](https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09)
- en: Explore the wisdom of LSTM leading into xLSTMs — a probable competition to the
    present-day LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索LSTM的智慧，通向xLSTMs——一个可能成为当今LLMs竞争对手的技术
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    [Srijanie Dey, 博士](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    ·12 min read·Jul 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    ·阅读时间：12分钟·2024年7月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/49ffa3c9ea9d92e16c715ce00259c89f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49ffa3c9ea9d92e16c715ce00259c89f.png)'
- en: Image by author (The ancient wizard as created by my 4-year old)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片（古老的巫师，由我的四岁孩子创作）
- en: '*“In the enchanted realm of Serentia, where ancient forests whispered secrets
    of spells long forgotten, there dwelled the Enigmastrider — a venerable wizard,
    guardian of timeless wisdom.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*“在塞伦蒂亚的神秘领域中，那里古老的森林低语着久已遗忘的咒语，居住着谜行者——一位尊敬的巫师，永恒智慧的守护者。”*'
- en: '*One pivotal day as Serentia faced dire peril, the Enigmastrider wove a mystical
    ritual using the Essence Stones, imbued with the essence of past, present, and
    future. Drawing upon ancient magic he conjured the LSTM, a conduit of knowledge
    capable of preserving Serentia’s history and foreseeing its destiny. Like a river
    of boundless wisdom, the LSTM flowed transcending the present and revealing what
    lay beyond the horizon.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*“有一天，当塞伦蒂亚面临巨大危险时，谜行者使用赋予过去、现在与未来精华的本质之石，编织了一场神秘的仪式。借助古老的魔法，他召唤出了LSTM，一个知识的传导体，能够保存塞伦蒂亚的历史并预见其命运。犹如一条无尽的智慧之河，LSTM流动着，超越了当下，揭示了地平线之外的未来。”*'
- en: '*From his secluded abode the Enigmastrider observed as Serentia was reborn,
    ascending to new heights. He knew that his arcane wisdom and tireless efforts
    had once again safeguarded a legacy in this magical realm.”*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*“从他隐居的住所，谜行者观察着塞伦蒂亚的重生，向新的高峰攀升。他知道，正是他那深奥的智慧和不懈的努力，再一次在这个神奇的世界中守护了一个遗产。”*'
- en: And with that story we begin our expedition to the depths of one of the most
    appealing Recurrent Neural Networks — the Long Short-Term Memory Networks, very
    popularly known as the LSTMs. Why do we revisit this classic? Because they may
    once again become useful as longer context-lengths in language modeling grow in
    importance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这个故事，我们开始了对最吸引人的递归神经网络之一——长短期记忆网络（LSTMs）的深入探讨。为什么我们要再次回顾这个经典？因为随着语言建模中长上下文长度的重要性日益增长，它们可能再次变得非常有用。
- en: Can LSTMs once again get an edge over LLMs?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTMs能否再次超越LLMs？
- en: A short while ago, researchers in Austria came up with a promising initiative
    to revive the lost glory of LSTMs — by giving way to the more evolved Extended
    Long-short Term Memory, also called xLSTM. It would not be wrong to say that before
    Transformers, LSTMs had worn the throne for innumerous deep-learning successes.
    Now the question stands, with their abilities maximized and drawbacks minimized,
    can they compete with the present-day LLMs?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，奥地利的研究人员提出了一个有前景的举措，旨在复兴LSTM的失落辉煌——通过发展更为进化的扩展长短期记忆网络（Extended Long-short
    Term Memory，简称xLSTM）。可以说，在变压器（Transformers）出现之前，LSTM曾经在深度学习的成功中独占鳌头。现在的问题是，随着其能力的最大化和缺点的最小化，LSTM能否与当今的大型语言模型（LLM）竞争？
- en: 'To learn the answer, let’s move back in time a bit and revise what LSTMs were
    and what made them so special:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到答案，我们不妨回顾一下LSTM的历史，并回顾一下它们的独特之处：
- en: Long Short Term Memory Networks were first introduced in the year 1997 by [Hochreiter
    and Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf) — to address
    the long-term dependency problem faced by RNNs. With around 106518 citations on
    the paper, it is no wonder that LSTMs are a classic.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）最早由[Hochreiter 和 Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf)于1997年提出——旨在解决循环神经网络（RNN）面临的长期依赖问题。该论文已被引用约106518次，LSTM成为经典也就不足为奇了。
- en: The key idea in an LSTM is the ability to learn when to remember and when to
    forget relevant information over arbitrary time intervals. Just like us humans.
    Rather than starting every idea from scratch — we rely on much older information
    and are able to very aptly connect the dots. Of course, when talking about LSTMs,
    the question arises — don’t RNNs do the same thing?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的核心思想是能够学习在任意时间间隔内，何时记住和何时忘记相关信息。这就像我们人类一样。我们不会从零开始每次想法——我们依赖于更久远的信息，并能非常巧妙地将它们联系起来。当然，说到LSTM时，大家会问——RNN不也做一样的事情吗？
- en: The short answer is yes, they do. However, there is a big difference. The RNN
    architecture does not support delving too much in the past — only up to the immediate
    past. And that is not very helpful.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简短的答案是：可以。然而，存在一个很大的区别。RNN架构并不支持过多关注过去的内容——只能关注即时的过去。而这并不太有帮助。
- en: 'As an example, let’s consider these line John Keats wrote in ‘To Autumn’:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们来看约翰·济慈在《秋天》中的这些诗句：
- en: '*“Season of mists and mellow fruitfulness,*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*“Season of mists and mellow fruitfulness,*'
- en: '*Close bosom-friend of the maturing sun;”*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*Close bosom-friend of the maturing sun;”*'
- en: As humans, we understand that words “mists” and “mellow fruitfulness” are conceptually
    related to the season of autumn, evoking ideas of a specific time of year. Similarly,
    LSTMs can capture this notion and use it to understand the context further when
    the words “maturing sun” comes in. Despite the separation between these words
    in the sequence, LSTM networks can learn to associate and keep the previous connections
    intact. And this is the big contrast when compared with the original Recurrent
    Neural Network framework.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们理解“mists”（雾气）和“mellow fruitfulness”（丰盈的果实）这两个词与秋季的季节是概念上相关的，唤起了特定时节的想法。同样，LSTM可以捕捉到这种概念，并利用它进一步理解“maturing
    sun”（成熟的太阳）一词的上下文。尽管这些词在序列中分隔开，LSTM网络依然能够学习到它们之间的关联，并保持前面的联系不变。这与原始的循环神经网络（RNN）框架有着明显的区别。
- en: 'And the way LSTMs do it is with the help of a gating mechanism. If we consider
    the architecture of an RNN vs an LSTM, the difference is very evident. The RNN
    has a very simple architecture — the past state and present input pass through
    an activation function to output the next state. An LSTM block, on the other hand,
    adds three more gates on top of an RNN block: the input gate, the forget gate
    and output gate which together handle the past state along with the present input.
    This idea of gating is what makes all the difference.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM之所以能做到这一点，是通过一个门控机制。如果我们对比RNN和LSTM的架构，差异非常明显。RNN的架构非常简单——过去的状态和当前的输入通过一个激活函数传递，输出下一个状态。另一方面，LSTM块在RNN块的基础上添加了三个门控：输入门、遗忘门和输出门，它们共同处理过去的状态和当前的输入。这种门控机制正是所有区别的所在。
- en: To understand things further, let’s dive into the details with these incredible
    works on [LSTMs](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-deeplearning-activity-7206573043425447936-IYqt?utm_source=share&utm_medium=member_desktop)
    and [xLSTMs](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-activity-7194319329536978945-VP6W?utm_source=share&utm_medium=member_desktop)
    by the amazing [Prof. Tom Yeh](https://www.linkedin.com/in/tom-yeh/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解，让我们深入探讨这些令人惊叹的关于[LSTM](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-deeplearning-activity-7206573043425447936-IYqt?utm_source=share&utm_medium=member_desktop)和[xLSTM](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-activity-7194319329536978945-VP6W?utm_source=share&utm_medium=member_desktop)的杰出作品，作者是[Tom
    Yeh教授](https://www.linkedin.com/in/tom-yeh/)。
- en: First, let’s understand the mathematical cogs and wheels behind LSTMs before
    exploring their newer version.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解LSTM背后的数学机制，然后再探讨其更新版本。
- en: (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the
    above-mentioned LinkedIn posts, which I have edited with his permission. )
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: （下方的所有图片，除非另有说明，均来自Tom Yeh教授的LinkedIn帖子，我已获得他的许可进行编辑。）
- en: 'So, here we go:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，接下来我们开始：
- en: How does an LSTM work?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM是如何工作的？
- en: '[1] Initialize'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[1] 初始化'
- en: The first step begins with randomly assigning values to the previous hidden
    state h0 and memory cells C0\. Keeping it in sync with the diagrams, we set
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步从随机分配先前的隐藏状态h0和记忆单元C0的值开始。为了与图示同步，我们设置：
- en: h0 → [1,1]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: h0 → [1,1]
- en: C0 → [0.3, -0.5]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: C0 → [0.3, -0.5]
- en: '![](../Images/8dfd7b032b37f204451c17d02584e40b.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dfd7b032b37f204451c17d02584e40b.png)'
- en: '[2] Linear Transform'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2] 线性变换'
- en: In the next step, we perform a linear transform by multiplying the four weight
    matrices (*Wf*, *Wc*, *Wi* and *Wo*) with the concatenated current input X1 and
    the previous hidden state that we assigned in the previous step.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们通过将四个权重矩阵（*Wf*，*Wc*，*Wi*和*Wo*）与当前输入X1和先前隐藏状态进行拼接后相乘，来执行线性变换。
- en: The resultant values are called **feature values** obtained as the combination
    of the current input and the hidden state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果值称为**特征值**，它们是当前输入和隐藏状态的组合。
- en: '![](../Images/6dccf0a8b5c7eba2afac97f64c89e09d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dccf0a8b5c7eba2afac97f64c89e09d.png)'
- en: '[3] Non-linear Transform'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[3] 非线性变换'
- en: This step is crucial in the LSTM process. It is a non-linear transform with
    two parts — a **sigmoid σ** and **tanh**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步在LSTM过程中至关重要。它是一个非线性变换，包含两个部分——**sigmoid σ**和**tanh**。
- en: The sigmoid is used to obtain gate values between 0 and 1\. This layer essentially
    determines what information to retain and what to forget. The values always range
    between 0 and 1 — a ‘0’ implies completely eliminating the information whereas
    a ‘1’ implies keeping it in place.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid用于获取介于0和1之间的门值。这个层实际上决定了保留哪些信息，遗忘哪些信息。值始终在0和1之间——‘0’表示完全删除信息，而‘1’表示保留信息。
- en: 'Forget gate (f1): [-4, -6] → [0, 0]'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门 (f1)：[-4, -6] → [0, 0]
- en: 'Input gate (i1): [6, 4] → [1, 1]'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门 (i1)：[6, 4] → [1, 1]
- en: 'Output gate (o1): [4, -5] → [1, 0]'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门 (o1)：[4, -5] → [1, 0]
- en: In the next part, tanh is applied to obtain new candidate memory values that
    could be added on top of the previous information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，应用tanh来获得新的候选记忆值，这些值可以添加到先前的信息上。
- en: 'Candidate memory (C’1): [1, -6] → [0.8, -1]'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选记忆 (C’1)：[1, -6] → [0.8, -1]
- en: '![](../Images/b76632ff806ae0df8b13d605cc6f0421.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b76632ff806ae0df8b13d605cc6f0421.png)'
- en: '[4] Update Memory'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[4] 更新记忆'
- en: Once the above values are obtained, it is time to update the current state using
    these values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得上述值，就可以使用这些值来更新当前状态。
- en: '**The previous step made the decision on what needs to be done, in this step
    we implement that decision.**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**前一步已经决定了需要做什么，在这一步我们执行那个决定。**'
- en: 'We do so in two parts:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其分为两部分：
- en: '**Forget** : Multiply the current memory values (C0) element-wise with the
    obtained forget-gate values. What it does is it updates in the current state the
    values that were decided could be forgotten. → C0 .* f1'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**忘记**：将当前记忆值 (C0) 与获得的忘记门值按元素相乘。其作用是更新当前状态中被决定遗忘的值。→ C0 .* f1'
- en: '**Input** : Multiply the updated memory values (C’1) element-wise with the
    input gate values to obtain ‘input-scaled’ the memory values. → C’1 .* i1'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入**：将更新后的记忆值 (C’1) 与输入门值按元素相乘，得到‘输入缩放’后的记忆值。→ C’1 .* i1'
- en: Finally, we add these two terms above to get the updated memory C1, i.e. **C0
    .* f1 + C’1 .* i1 = C1**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将上述两项相加，得到更新后的记忆C1，即**C0 .* f1 + C’1 .* i1 = C1**
- en: '![](../Images/eea4538ecb24d07770a0f06e8e1fba75.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea4538ecb24d07770a0f06e8e1fba75.png)'
- en: '[5] Candidate Output'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[5] 候选输出'
- en: 'Finally, we make the decision on how the output is going to look like:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们决定输出结果的形式：
- en: To begin, we first apply tanh as before to the new memory C1 to obtain a candidate
    output o’1\. This pushes the values between -1 and 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们像之前一样对新的记忆 C1 应用 tanh，得到候选输出 o’1。这样会将值压缩到 -1 到 1 之间。
- en: '![](../Images/c234f20db05f4d7c603a7c44bdd4ce92.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c234f20db05f4d7c603a7c44bdd4ce92.png)'
- en: '[6] Update Hidden State'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[6] 更新隐藏状态'
- en: To get the final output, we multiply the candidate output o’1 obtained in the
    previous step with the sigmoid of the output gate o1 obtained in Step 3\. The
    result obtained is the first output of the network and is the updated hidden state
    h1, i.e. **o’1 * o1 = h1.**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终输出，我们将之前步骤中获得的候选输出 o’1 与步骤 3 中输出门 o1 的 sigmoid 结果相乘，得到的结果就是网络的第一个输出，也是更新后的隐藏状态
    h1，即 **o’1 * o1 = h1**。
- en: '![](../Images/b95c5a03295b502387b9d2a982d62e9c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b95c5a03295b502387b9d2a982d62e9c.png)'
- en: — — Process t = 2 — -
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: — — 过程 t = 2 — -
- en: 'We continue with the subsequent iterations below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行以下的后续迭代：
- en: '[7] Initialize'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[7] 初始化'
- en: First, we copy the updates from the previous steps i.e. updated hidden state
    h1 and memory C1.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们复制来自前面步骤的更新结果，即更新后的隐藏状态 h1 和记忆 C1。
- en: '![](../Images/142061e2ce495b94eaeaf251c8aae472.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/142061e2ce495b94eaeaf251c8aae472.png)'
- en: '[8] Linear Transform'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[8] 线性变换'
- en: We repeat Step [2] which is element-wise weight and bias matrix multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤 [2]，即逐元素的权重和偏置矩阵乘法。
- en: '![](../Images/2ad7d59f612d08f21401728ca8af4702.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ad7d59f612d08f21401728ca8af4702.png)'
- en: '[9] Update Memory (C2)'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[9] 更新记忆 (C2)'
- en: We repeat steps [3] and [4] which are the non-linear transforms using sigmoid
    and tanh layers, followed by the decision on forgetting the relevant parts and
    introducing new information — this gives us the updated memory C2.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 重复步骤 [3] 和 [4]，即使用 sigmoid 和 tanh 层进行非线性变换，随后决定遗忘相关部分并引入新信息——这将给我们更新后的记忆 C2。
- en: '![](../Images/30b79ad19fe3908c5867691cc98137b7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30b79ad19fe3908c5867691cc98137b7.png)'
- en: '[10] Update Hidden State (h2)'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[10] 更新隐藏状态 (h2)'
- en: Finally, we repeat steps [5] and [6] which adds up to give us the second hidden
    state h2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们重复步骤 [5] 和 [6]，这会将结果加起来得到第二个隐藏状态 h2。
- en: '![](../Images/e81ce3f9aea26b308f1da056116e47fd.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e81ce3f9aea26b308f1da056116e47fd.png)'
- en: Next, we have the final iteration.
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来，我们进入最后的迭代。
- en: — — Process t = 3 — -
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: — — 过程 t = 3 — -
- en: '[11] Initialize'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[11] 初始化'
- en: Once again we copy the hidden state and memory from the previous iteration i.e.
    h2 and C2.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们复制来自上一迭代的隐藏状态和记忆值，即 h2 和 C2。
- en: '![](../Images/9199f1c501b328aa1dbd243a632507a0.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9199f1c501b328aa1dbd243a632507a0.png)'
- en: '[12] Linear Transform'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[12] 线性变换'
- en: We perform the same linear-transform as we do in Step 2.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行与步骤 2 相同的线性变换。
- en: '![](../Images/91822b2527646253a536451432811b8c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91822b2527646253a536451432811b8c.png)'
- en: '[13] Update Memory (C3)'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[13] 更新记忆 (C3)'
- en: Next, we perform the non-linear transforms and perform the memory updates based
    on the values obtained during the transform.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们执行非线性变换，并根据变换中获得的值执行记忆更新。
- en: '![](../Images/dc328875f1265c3352b7b0d2c6054e1e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc328875f1265c3352b7b0d2c6054e1e.png)'
- en: '[14] Update Hidden State (h3)'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[14] 更新隐藏状态 (h3)'
- en: Once done, we use those values to obtain the final hidden state h3.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们使用这些值来获得最终的隐藏状态 h3。
- en: '![](../Images/528e2b7e2eb1ac3137fda08e5f46b73b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/528e2b7e2eb1ac3137fda08e5f46b73b.png)'
- en: 'Summary:'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结：
- en: 'To summarize the working above, the key thing to remember is that LSTM depends
    on three main gates : **input, forget and output**. And these gates as can be
    inferred from the names, control what part of the information and how much of
    it is relevant and which parts can be discarded.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 总结上面的工作，关键点是记住 LSTM 依赖于三个主要的门控：**输入门、遗忘门和输出门**。正如这些名称所示，这些门控决定了哪些部分的信息是相关的，以及要保留多少信息，哪些部分可以被丢弃。
- en: 'Very briefly, the steps to do so are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 简要来说，执行的步骤如下：
- en: Initialize the hidden state and memory values from the previous state.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前一状态初始化隐藏状态和记忆值。
- en: Perform linear-transform to help the network start looking at the hidden state
    and memory values.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行线性变换，帮助网络开始关注隐藏状态和记忆值。
- en: Apply non-linear transform (sigmoid and tanh) to determine what values to retain
    /discard and to obtain new candidate memory values.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据应用非线性变换（sigmoid 和 tanh），以确定保留/丢弃哪些值，并获得新的候选记忆值。
- en: Based on the decision (values obtained) in Step 3, we perform memory updates.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于步骤 3 中获得的决策（值），我们进行记忆更新。
- en: Next, we determine what the output is going to look like based on the memory
    update obtained in the previous step. We obtain a candidate output here.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们根据前一步骤中获得的记忆更新来确定输出的样子。我们在这里获得一个候选输出。
- en: We combine the candidate output with the gated output value obtained in Step
    3 to finally reach the intermediate hidden state.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将候选输出与步骤3中获得的门控输出值结合，最终得出中间隐藏状态。
- en: This loop continues for as many iterations as needed.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这个循环将根据需要继续进行多次迭代。
- en: Extended Long-Short Term Memory (xLSTM)
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展长短期记忆（xLSTM）
- en: The need for xLSTMs
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: xLSTM的需求
- en: When LSTMs emerged, they definitely set the platform for doing something that
    was not done previously. Recurrent Neural Networks could have memory but it was
    very limited and hence the birth of LSTM — to support long-term dependencies.
    However, it was not enough. Because analyzing inputs as sequences obstructed the
    use of parallel computation and moreover, led to drops in performance due to long
    dependencies.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当LSTM出现时，它们无疑为做一些之前无法做到的事情奠定了基础。循环神经网络可以具有记忆，但它的记忆非常有限，因此诞生了LSTM——为了支持长期依赖。然而，这还不够。因为将输入分析为序列阻碍了并行计算的使用，而且由于长期依赖，还导致了性能下降。
- en: Thus, as a solution to it all were born the transformers. But the question still
    remained — can we once again use LSTMs by addressing their limitations to achieve
    what Transformers do? To answer that question, came the xLSTM architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为所有问题的解决方案，transformers诞生了。但问题仍然存在——我们是否可以通过解决LSTM的局限性，再次利用LSTM实现transformers的功能？为了解答这个问题，xLSTM架构应运而生。
- en: How is xLSTM different from LSTM?
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: xLSTM与LSTM有何不同？
- en: xLSTMs can be seen as a very evolved version of LSTMs. The underlying structure
    of LSTMs are preserved in xLSTM, however new elements have been introduced which
    help handle the drawbacks of the original form.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: xLSTM可以看作是LSTM的一个非常进化的版本。xLSTM保留了LSTM的基础结构，但引入了新的元素，这些元素有助于处理原始形式的缺点。
- en: Exponential Gating & Scalar Memory Mixing — sLSTM
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指数门控与标量记忆混合——sLSTM
- en: The most crucial difference is the introduction of **exponential gating**. In
    LSTMs, when we perform Step [3], we induce a sigmoid gating to all gates, while
    for xLSTMs it has been replaced by exponential gating.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最关键的区别是引入了**指数门控**。在LSTM中，当我们执行步骤[3]时，会对所有门进行sigmoid门控，而对于xLSTM，它已经被指数门控所替代。
- en: 'For eg: For the input gate i1-'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：对于输入门i1-
- en: '![](../Images/04def2fa390328178528101562981194.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04def2fa390328178528101562981194.png)'
- en: is now,
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，
- en: '![](../Images/36ccf4af44d1978c697aabd5314a8320.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36ccf4af44d1978c697aabd5314a8320.png)'
- en: Images by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: With a bigger range that exponential gating provides, xLSTMs are able to handle
    updates better as compared to the sigmoid function which compresses inputs to
    the range of (0, 1). There is a catch though — exponential values may grow up
    to be very large. To mitigate that problem, xLSTMs incorporate normalization and
    the logarithm function seen in the equations below plays an important role here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于指数门控提供了更大的范围，xLSTM能够比sigmoid函数更好地处理更新，因为sigmoid函数将输入压缩到（0, 1）的范围内。然而，有一个问题——指数值可能会变得非常大。为了缓解这个问题，xLSTM引入了归一化，并且下面方程中出现的对数函数在这里发挥了重要作用。
- en: '![](../Images/7a79124479a3fd526ba17241e252b6d8.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a79124479a3fd526ba17241e252b6d8.png)'
- en: Image from Reference [1]
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源于参考文献[1]
- en: Now, logarithm does reverse the effect of the exponential but their combined
    application, as the [xLSTM paper](https://arxiv.org/abs/2405.04517) claims, leads
    the way for balanced states.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对数确实逆转了指数的效果，但正如[xLSTM论文](https://arxiv.org/abs/2405.04517)所声称的，它们的结合应用为平衡状态指明了方向。
- en: This exponential gating along with memory mixing among the different gates (as
    in the original LSTM) forms the **sLSTM** block.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这种指数门控以及不同门之间的记忆混合（如原始LSTM中所示）形成了**sLSTM**块。
- en: Matrix Memory Cell — mLSTM
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵记忆单元——mLSTM
- en: The other new aspect of the xLSTM architecture is the increase from a scalar
    memory to matrix memory which allows it to process more information in parallel.
    It also draws semblance to the transformer architecture by introducing the key,
    query and value vectors and using them in the normalizer state as the weighted
    sum of key vectors, where each key vector is weighted by the input and forget
    gates.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: xLSTM架构的另一个新特点是将标量记忆扩展为矩阵记忆，这使得它能够并行处理更多信息。它还通过引入键、查询和值向量，并将它们作为加权的键向量之和使用在归一化器状态中，从而与transformer架构相似，其中每个键向量的权重由输入门和遗忘门确定。
- en: Once the sLSTM and mLSTM blocks are ready, they are stacked one over the other
    using residual connections to yield xLSTM blocks and finally the xLSTM architecture.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 sLSTM 和 mLSTM 模块准备好后，它们会通过残差连接一个个堆叠在一起，形成 xLSTM 模块，最终构成 xLSTM 架构。
- en: Thus, the introduction of exponential gating (with appropriate normalization)
    along with newer memory structures establish a strong pedestal for the xLSTMs
    to achieve results similar to the transformers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，指数门控（结合适当的归一化）和更新的记忆结构的引入为 xLSTM 提供了一个强有力的基础，使其能够实现类似于变换器的结果。
- en: '**To summarize:**'
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**总结：**'
- en: An **LSTM** is a special Recurrent Neural Network (RNN) that allows connecting
    previous information to the current state just as us humans do with persistence
    of our thoughts. LSTMs became incredibly popular because of their ability to look
    far into the past rather than depending only on the immediate past. What made
    it possible was the introduction of special gating elements into the RNN architecture-
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LSTM** 是一种特殊的循环神经网络（RNN），它允许像人类一样将先前的信息与当前状态连接，保持思维的持续性。LSTM 因其能够回顾较远的过去，而不仅仅依赖于眼前的即时过去，变得非常流行。使这一切成为可能的是特殊门控元素的引入到
    RNN 架构中——'
- en: '**Forget Gate**: Determines what information from the previous cell state should
    be kept or forgotten. By selectively forgetting irrelevant past information, the
    LSTM maintains long-term dependencies.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：决定应当保留或忘记前一个单元状态中的哪些信息。通过有选择地遗忘不相关的过去信息，LSTM 保持了长期依赖关系。'
- en: '**Input Gate** : Determines what new information should be stored in the cell
    state. By controlling how the cell state is updated, it incorporates new information
    important for predicting the current output.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：决定应当存储在单元状态中的新信息。通过控制单元状态的更新方式，它融合了预测当前输出所需的新信息。'
- en: '**Output Gate** : Determines what information should be the output as the hidden
    state. By selectively exposing parts of the cell state as the output, the LSTM
    can provide relevant information to subsequent layers while suppressing the non-pertinent
    details and thus propagating only the important information over longer sequences.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：决定应当作为隐藏状态输出的信息。通过选择性地将单元状态的部分内容作为输出，LSTM 可以向后续层提供相关信息，同时抑制不相关的细节，从而只传递重要信息到更长的序列中。'
- en: '2\. An **xLSTM** is an evolved version of the LSTM that addresses the drawbacks
    faced by the LSTM. It is true that LSTMs are capable of handling long-term dependencies,
    however the information is processed sequentially and thus doesn’t incorporate
    the power of parallelism that today’s transformers capitalize on. To address that,
    xLSTMs bring in:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **xLSTM** 是 LSTM 的一种进化版本，旨在解决 LSTM 面临的缺陷。虽然 LSTM 能够处理长期依赖问题，但信息是顺序处理的，因此没有利用当前变换器所强调的并行性。为了解决这个问题，xLSTM
    引入了：
- en: '**sLSTM** : Exponential gating that helps to include larger ranges as compared
    to sigmoid activation.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sLSTM**：指数门控，帮助包括比 sigmoid 激活函数更广的范围。'
- en: '**mLSTM** : New memory structures with matrix memory to enhance memory capacity
    and enhance more efficient information retrieval.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mLSTM**：新型的记忆结构，采用矩阵记忆以增强记忆容量并提高信息检索效率。'
- en: Will LSTMs make their comeback?
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 会重返舞台吗？
- en: LSTMs overall are part of the Recurrent Neural Network family that process information
    in a sequential manner recursively. The advent of Transformers completely obliterated
    the application of recurrence however, their struggle to handle extremely long
    sequences still remains a burning problem. Research suggests that quadratic time
    is pertinent for long-ranges or long contexts.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 总体上属于循环神经网络家族，它们以递归的方式按顺序处理信息。变换器的出现彻底消除了递归应用，但它们在处理极长序列时依然面临严峻问题。研究表明，对于长范围或长上下文，二次时间复杂度是相关的。
- en: Thus, it does seem worthwhile to explore options that could at least enlighten
    a solution path and a good starting point would be going back to LSTMs — in short,
    LSTMs have a good chance of making a comeback. The present xLSTM results definitely
    look promising. And then, to round it all up — the use of recurrence by [Mamba](https://arxiv.org/pdf/2312.00752)
    stands as a good testimony that this could be a lucrative path to explore.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，探索能够至少启发解决方案路径的选项似乎是值得的，一个好的起点可能是回到 LSTM——简而言之，LSTM 很有可能卷土重来。当前的 xLSTM 结果无疑看起来很有前景。最后，作为总结——[Mamba](https://arxiv.org/pdf/2312.00752)
    对递归的使用作为一个良好的例证，表明这可能是一个值得探索的有利路径。
- en: So, let’s follow along in this journey and see it unfold while keeping in mind
    the power of recurrence!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们一起跟随这个旅程，看到它的展开，同时牢记递归的强大力量！
- en: '*P.S. If you would like to work through this exercise on your own, here is
    a link to a blank template for your use.*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*附言：如果你想自己完成这个练习，这里有一个空白模板的链接供你使用。*'
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1zfj9TgKP52fOu75HLDs-cooHkhOZAbfU/view?usp=sharing)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[空白模板用于手动练习](https://drive.google.com/file/d/1zfj9TgKP52fOu75HLDs-cooHkhOZAbfU/view?usp=sharing)'
- en: Now go have fun and create some Long Short-Term effect!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在去玩得开心，创造一些长短期效应吧！
- en: '![](../Images/eefcdf7e690e6ecece1242a79f2acd5d.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eefcdf7e690e6ecece1242a79f2acd5d.png)'
- en: Image by author
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'References:'
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: 'xLSTM: Extended Long Short-Term Memory, Maximilian et al. May 2024 [https://arxiv.org/abs/2405.04517](https://arxiv.org/abs/2405.04517)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: xLSTM：扩展的长短期记忆，Maximilian 等，2024年5月 [https://arxiv.org/abs/2405.04517](https://arxiv.org/abs/2405.04517)
- en: Long Short-Term Memory, Sepp Hochreiter and Jürgen Schmidhuber, 1997, Neural
    Comput. 9, 8 (November 15, 1997), 1735–1780\. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 长短期记忆，Sepp Hochreiter 和 Jürgen Schmidhuber，1997年，Neural Comput. 9, 8（1997年11月15日），1735–1780。
    [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
