- en: Deep Dive into LSTMs and xLSTMs by Hand ✍️
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09](https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explore the wisdom of LSTM leading into xLSTMs — a probable competition to the
    present-day LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------)
    ·12 min read·Jul 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49ffa3c9ea9d92e16c715ce00259c89f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author (The ancient wizard as created by my 4-year old)
  prefs: []
  type: TYPE_NORMAL
- en: '*“In the enchanted realm of Serentia, where ancient forests whispered secrets
    of spells long forgotten, there dwelled the Enigmastrider — a venerable wizard,
    guardian of timeless wisdom.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*One pivotal day as Serentia faced dire peril, the Enigmastrider wove a mystical
    ritual using the Essence Stones, imbued with the essence of past, present, and
    future. Drawing upon ancient magic he conjured the LSTM, a conduit of knowledge
    capable of preserving Serentia’s history and foreseeing its destiny. Like a river
    of boundless wisdom, the LSTM flowed transcending the present and revealing what
    lay beyond the horizon.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*From his secluded abode the Enigmastrider observed as Serentia was reborn,
    ascending to new heights. He knew that his arcane wisdom and tireless efforts
    had once again safeguarded a legacy in this magical realm.”*'
  prefs: []
  type: TYPE_NORMAL
- en: And with that story we begin our expedition to the depths of one of the most
    appealing Recurrent Neural Networks — the Long Short-Term Memory Networks, very
    popularly known as the LSTMs. Why do we revisit this classic? Because they may
    once again become useful as longer context-lengths in language modeling grow in
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: Can LSTMs once again get an edge over LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A short while ago, researchers in Austria came up with a promising initiative
    to revive the lost glory of LSTMs — by giving way to the more evolved Extended
    Long-short Term Memory, also called xLSTM. It would not be wrong to say that before
    Transformers, LSTMs had worn the throne for innumerous deep-learning successes.
    Now the question stands, with their abilities maximized and drawbacks minimized,
    can they compete with the present-day LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn the answer, let’s move back in time a bit and revise what LSTMs were
    and what made them so special:'
  prefs: []
  type: TYPE_NORMAL
- en: Long Short Term Memory Networks were first introduced in the year 1997 by [Hochreiter
    and Schmidhuber](https://www.bioinf.jku.at/publications/older/2604.pdf) — to address
    the long-term dependency problem faced by RNNs. With around 106518 citations on
    the paper, it is no wonder that LSTMs are a classic.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea in an LSTM is the ability to learn when to remember and when to
    forget relevant information over arbitrary time intervals. Just like us humans.
    Rather than starting every idea from scratch — we rely on much older information
    and are able to very aptly connect the dots. Of course, when talking about LSTMs,
    the question arises — don’t RNNs do the same thing?
  prefs: []
  type: TYPE_NORMAL
- en: The short answer is yes, they do. However, there is a big difference. The RNN
    architecture does not support delving too much in the past — only up to the immediate
    past. And that is not very helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let’s consider these line John Keats wrote in ‘To Autumn’:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Season of mists and mellow fruitfulness,*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Close bosom-friend of the maturing sun;”*'
  prefs: []
  type: TYPE_NORMAL
- en: As humans, we understand that words “mists” and “mellow fruitfulness” are conceptually
    related to the season of autumn, evoking ideas of a specific time of year. Similarly,
    LSTMs can capture this notion and use it to understand the context further when
    the words “maturing sun” comes in. Despite the separation between these words
    in the sequence, LSTM networks can learn to associate and keep the previous connections
    intact. And this is the big contrast when compared with the original Recurrent
    Neural Network framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the way LSTMs do it is with the help of a gating mechanism. If we consider
    the architecture of an RNN vs an LSTM, the difference is very evident. The RNN
    has a very simple architecture — the past state and present input pass through
    an activation function to output the next state. An LSTM block, on the other hand,
    adds three more gates on top of an RNN block: the input gate, the forget gate
    and output gate which together handle the past state along with the present input.
    This idea of gating is what makes all the difference.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand things further, let’s dive into the details with these incredible
    works on [LSTMs](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-deeplearning-activity-7206573043425447936-IYqt?utm_source=share&utm_medium=member_desktop)
    and [xLSTMs](https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-activity-7194319329536978945-VP6W?utm_source=share&utm_medium=member_desktop)
    by the amazing [Prof. Tom Yeh](https://www.linkedin.com/in/tom-yeh/).
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s understand the mathematical cogs and wheels behind LSTMs before
    exploring their newer version.
  prefs: []
  type: TYPE_NORMAL
- en: (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the
    above-mentioned LinkedIn posts, which I have edited with his permission. )
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: How does an LSTM work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Initialize'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step begins with randomly assigning values to the previous hidden
    state h0 and memory cells C0\. Keeping it in sync with the diagrams, we set
  prefs: []
  type: TYPE_NORMAL
- en: h0 → [1,1]
  prefs: []
  type: TYPE_NORMAL
- en: C0 → [0.3, -0.5]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8dfd7b032b37f204451c17d02584e40b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[2] Linear Transform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next step, we perform a linear transform by multiplying the four weight
    matrices (*Wf*, *Wc*, *Wi* and *Wo*) with the concatenated current input X1 and
    the previous hidden state that we assigned in the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: The resultant values are called **feature values** obtained as the combination
    of the current input and the hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dccf0a8b5c7eba2afac97f64c89e09d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[3] Non-linear Transform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This step is crucial in the LSTM process. It is a non-linear transform with
    two parts — a **sigmoid σ** and **tanh**.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid is used to obtain gate values between 0 and 1\. This layer essentially
    determines what information to retain and what to forget. The values always range
    between 0 and 1 — a ‘0’ implies completely eliminating the information whereas
    a ‘1’ implies keeping it in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forget gate (f1): [-4, -6] → [0, 0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input gate (i1): [6, 4] → [1, 1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output gate (o1): [4, -5] → [1, 0]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next part, tanh is applied to obtain new candidate memory values that
    could be added on top of the previous information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Candidate memory (C’1): [1, -6] → [0.8, -1]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b76632ff806ae0df8b13d605cc6f0421.png)'
  prefs: []
  type: TYPE_IMG
- en: '[4] Update Memory'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the above values are obtained, it is time to update the current state using
    these values.
  prefs: []
  type: TYPE_NORMAL
- en: '**The previous step made the decision on what needs to be done, in this step
    we implement that decision.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We do so in two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget** : Multiply the current memory values (C0) element-wise with the
    obtained forget-gate values. What it does is it updates in the current state the
    values that were decided could be forgotten. → C0 .* f1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Input** : Multiply the updated memory values (C’1) element-wise with the
    input gate values to obtain ‘input-scaled’ the memory values. → C’1 .* i1'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we add these two terms above to get the updated memory C1, i.e. **C0
    .* f1 + C’1 .* i1 = C1**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea4538ecb24d07770a0f06e8e1fba75.png)'
  prefs: []
  type: TYPE_IMG
- en: '[5] Candidate Output'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we make the decision on how the output is going to look like:'
  prefs: []
  type: TYPE_NORMAL
- en: To begin, we first apply tanh as before to the new memory C1 to obtain a candidate
    output o’1\. This pushes the values between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c234f20db05f4d7c603a7c44bdd4ce92.png)'
  prefs: []
  type: TYPE_IMG
- en: '[6] Update Hidden State'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get the final output, we multiply the candidate output o’1 obtained in the
    previous step with the sigmoid of the output gate o1 obtained in Step 3\. The
    result obtained is the first output of the network and is the updated hidden state
    h1, i.e. **o’1 * o1 = h1.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b95c5a03295b502387b9d2a982d62e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: — — Process t = 2 — -
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We continue with the subsequent iterations below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Initialize'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we copy the updates from the previous steps i.e. updated hidden state
    h1 and memory C1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/142061e2ce495b94eaeaf251c8aae472.png)'
  prefs: []
  type: TYPE_IMG
- en: '[8] Linear Transform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeat Step [2] which is element-wise weight and bias matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ad7d59f612d08f21401728ca8af4702.png)'
  prefs: []
  type: TYPE_IMG
- en: '[9] Update Memory (C2)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We repeat steps [3] and [4] which are the non-linear transforms using sigmoid
    and tanh layers, followed by the decision on forgetting the relevant parts and
    introducing new information — this gives us the updated memory C2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30b79ad19fe3908c5867691cc98137b7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[10] Update Hidden State (h2)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we repeat steps [5] and [6] which adds up to give us the second hidden
    state h2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e81ce3f9aea26b308f1da056116e47fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we have the final iteration.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: — — Process t = 3 — -
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[11] Initialize'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once again we copy the hidden state and memory from the previous iteration i.e.
    h2 and C2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9199f1c501b328aa1dbd243a632507a0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[12] Linear Transform'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We perform the same linear-transform as we do in Step 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91822b2527646253a536451432811b8c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[13] Update Memory (C3)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we perform the non-linear transforms and perform the memory updates based
    on the values obtained during the transform.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc328875f1265c3352b7b0d2c6054e1e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[14] Update Hidden State (h3)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once done, we use those values to obtain the final hidden state h3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/528e2b7e2eb1ac3137fda08e5f46b73b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Summary:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize the working above, the key thing to remember is that LSTM depends
    on three main gates : **input, forget and output**. And these gates as can be
    inferred from the names, control what part of the information and how much of
    it is relevant and which parts can be discarded.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Very briefly, the steps to do so are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the hidden state and memory values from the previous state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform linear-transform to help the network start looking at the hidden state
    and memory values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply non-linear transform (sigmoid and tanh) to determine what values to retain
    /discard and to obtain new candidate memory values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the decision (values obtained) in Step 3, we perform memory updates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we determine what the output is going to look like based on the memory
    update obtained in the previous step. We obtain a candidate output here.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We combine the candidate output with the gated output value obtained in Step
    3 to finally reach the intermediate hidden state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This loop continues for as many iterations as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Extended Long-Short Term Memory (xLSTM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The need for xLSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When LSTMs emerged, they definitely set the platform for doing something that
    was not done previously. Recurrent Neural Networks could have memory but it was
    very limited and hence the birth of LSTM — to support long-term dependencies.
    However, it was not enough. Because analyzing inputs as sequences obstructed the
    use of parallel computation and moreover, led to drops in performance due to long
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, as a solution to it all were born the transformers. But the question still
    remained — can we once again use LSTMs by addressing their limitations to achieve
    what Transformers do? To answer that question, came the xLSTM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: How is xLSTM different from LSTM?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: xLSTMs can be seen as a very evolved version of LSTMs. The underlying structure
    of LSTMs are preserved in xLSTM, however new elements have been introduced which
    help handle the drawbacks of the original form.
  prefs: []
  type: TYPE_NORMAL
- en: Exponential Gating & Scalar Memory Mixing — sLSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most crucial difference is the introduction of **exponential gating**. In
    LSTMs, when we perform Step [3], we induce a sigmoid gating to all gates, while
    for xLSTMs it has been replaced by exponential gating.
  prefs: []
  type: TYPE_NORMAL
- en: 'For eg: For the input gate i1-'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04def2fa390328178528101562981194.png)'
  prefs: []
  type: TYPE_IMG
- en: is now,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36ccf4af44d1978c697aabd5314a8320.png)'
  prefs: []
  type: TYPE_IMG
- en: Images by author
  prefs: []
  type: TYPE_NORMAL
- en: With a bigger range that exponential gating provides, xLSTMs are able to handle
    updates better as compared to the sigmoid function which compresses inputs to
    the range of (0, 1). There is a catch though — exponential values may grow up
    to be very large. To mitigate that problem, xLSTMs incorporate normalization and
    the logarithm function seen in the equations below plays an important role here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a79124479a3fd526ba17241e252b6d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Reference [1]
  prefs: []
  type: TYPE_NORMAL
- en: Now, logarithm does reverse the effect of the exponential but their combined
    application, as the [xLSTM paper](https://arxiv.org/abs/2405.04517) claims, leads
    the way for balanced states.
  prefs: []
  type: TYPE_NORMAL
- en: This exponential gating along with memory mixing among the different gates (as
    in the original LSTM) forms the **sLSTM** block.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Memory Cell — mLSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other new aspect of the xLSTM architecture is the increase from a scalar
    memory to matrix memory which allows it to process more information in parallel.
    It also draws semblance to the transformer architecture by introducing the key,
    query and value vectors and using them in the normalizer state as the weighted
    sum of key vectors, where each key vector is weighted by the input and forget
    gates.
  prefs: []
  type: TYPE_NORMAL
- en: Once the sLSTM and mLSTM blocks are ready, they are stacked one over the other
    using residual connections to yield xLSTM blocks and finally the xLSTM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the introduction of exponential gating (with appropriate normalization)
    along with newer memory structures establish a strong pedestal for the xLSTMs
    to achieve results similar to the transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '**To summarize:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An **LSTM** is a special Recurrent Neural Network (RNN) that allows connecting
    previous information to the current state just as us humans do with persistence
    of our thoughts. LSTMs became incredibly popular because of their ability to look
    far into the past rather than depending only on the immediate past. What made
    it possible was the introduction of special gating elements into the RNN architecture-
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forget Gate**: Determines what information from the previous cell state should
    be kept or forgotten. By selectively forgetting irrelevant past information, the
    LSTM maintains long-term dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input Gate** : Determines what new information should be stored in the cell
    state. By controlling how the cell state is updated, it incorporates new information
    important for predicting the current output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Gate** : Determines what information should be the output as the hidden
    state. By selectively exposing parts of the cell state as the output, the LSTM
    can provide relevant information to subsequent layers while suppressing the non-pertinent
    details and thus propagating only the important information over longer sequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2\. An **xLSTM** is an evolved version of the LSTM that addresses the drawbacks
    faced by the LSTM. It is true that LSTMs are capable of handling long-term dependencies,
    however the information is processed sequentially and thus doesn’t incorporate
    the power of parallelism that today’s transformers capitalize on. To address that,
    xLSTMs bring in:'
  prefs: []
  type: TYPE_NORMAL
- en: '**sLSTM** : Exponential gating that helps to include larger ranges as compared
    to sigmoid activation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mLSTM** : New memory structures with matrix memory to enhance memory capacity
    and enhance more efficient information retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will LSTMs make their comeback?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs overall are part of the Recurrent Neural Network family that process information
    in a sequential manner recursively. The advent of Transformers completely obliterated
    the application of recurrence however, their struggle to handle extremely long
    sequences still remains a burning problem. Research suggests that quadratic time
    is pertinent for long-ranges or long contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, it does seem worthwhile to explore options that could at least enlighten
    a solution path and a good starting point would be going back to LSTMs — in short,
    LSTMs have a good chance of making a comeback. The present xLSTM results definitely
    look promising. And then, to round it all up — the use of recurrence by [Mamba](https://arxiv.org/pdf/2312.00752)
    stands as a good testimony that this could be a lucrative path to explore.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s follow along in this journey and see it unfold while keeping in mind
    the power of recurrence!
  prefs: []
  type: TYPE_NORMAL
- en: '*P.S. If you would like to work through this exercise on your own, here is
    a link to a blank template for your use.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1zfj9TgKP52fOu75HLDs-cooHkhOZAbfU/view?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: Now go have fun and create some Long Short-Term effect!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eefcdf7e690e6ecece1242a79f2acd5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'xLSTM: Extended Long Short-Term Memory, Maximilian et al. May 2024 [https://arxiv.org/abs/2405.04517](https://arxiv.org/abs/2405.04517)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Long Short-Term Memory, Sepp Hochreiter and Jürgen Schmidhuber, 1997, Neural
    Comput. 9, 8 (November 15, 1997), 1735–1780\. [https://doi.org/10.1162/neco.1997.9.8.1735](https://doi.org/10.1162/neco.1997.9.8.1735)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
