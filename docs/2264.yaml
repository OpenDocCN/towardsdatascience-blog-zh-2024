- en: 'OpenAI o1: Is This the Enigmatic Force That Will Reshape Every Knowledge Sector
    We Know?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/openai-o1-the-enigmatic-force-that-will-reshape-every-knowledge-sector-that-we-know-of-or-99396d641fff?source=collection_archive---------6-----------------------#2024-09-16](https://towardsdatascience.com/openai-o1-the-enigmatic-force-that-will-reshape-every-knowledge-sector-that-we-know-of-or-99396d641fff?source=collection_archive---------6-----------------------#2024-09-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: My first encounters with the o1 model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@abhinavp_41237?source=post_page---byline--99396d641fff--------------------------------)[![Abhinav
    Prasad Yasaswi](../Images/22731615708560c0826c9a17365e3bb9.png)](https://medium.com/@abhinavp_41237?source=post_page---byline--99396d641fff--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99396d641fff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99396d641fff--------------------------------)
    [Abhinav Prasad Yasaswi](https://medium.com/@abhinavp_41237?source=post_page---byline--99396d641fff--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99396d641fff--------------------------------)
    ·6 min read·Sep 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5942dac6b543e469c097f7a8e02b6d2.png)'
  prefs: []
  type: TYPE_IMG
- en: An image generated by DALL-E with a prompt precisely the same as the blog title.
  prefs: []
  type: TYPE_NORMAL
- en: My first encounters with the o1 model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: On the 12th of September at 10:00 a.m., I was in the class “**Frontier Topics
    in Generative AI**,” a graduate-level course at Arizona State University. A day
    before this, on the 11th of September, I submitted a team assignment that involved
    trying to identify flaws and erroneous outputs generated by GPT-4 (essentially
    trying to prompt GPT-4 to see if it makes mistakes on trivial questions or high-school-level
    reasoning questions) as part of another graduate-level class “**Topics in Natural
    Language Processing.”** We identified several trivial mistakes that GPT-4 made,
    one of them being *unable to count the number of r’s in the word strawberry*.
    Before submitting this assignment, I researched several peer-reviewed papers on
    the internet that identified where and why GPT -4 made mistakes and how you could
    rectify them. Most of the documents I came across identified two main domains
    where GPT-4 erred, and they dealt with **planning and reasoning**.
  prefs: []
  type: TYPE_NORMAL
- en: This paper¹ (although almost a year old) goes in depth through several cases
    where GPT-4 fails to answer trivial questions that involve simple counting, simple
    arithmetic, elementary logic, and even common sense. The paper¹ reasons that these
    questions require some level of reasoning and that because GPT-4 is utterly incapable
    of reasoning, it almost always gets these questions wrong. The author also states
    that reasoning is a (very) computationally hard problem. Although GPT-4 is very
    compute-intensive, its **compute-intensive nature is not geared towards involving
    reasoning** in solving the questions that it’s prompted with. Several other papers
    echo this notion of GPT-4 being unable to reason or plan²³.
  prefs: []
  type: TYPE_NORMAL
- en: Well, let’s get back to the 12th of September. My class ends at around 10:15
    a.m., and I come back straight home from class and open up YouTube on my phone
    as I dig into my morning brunch. The first recommendation on my YouTube homepage
    was a video from OpenAI announcing the release of GPT-o1 named “[Building OpenAI
    o1](https://youtu.be/3k89FMJhZ00?feature=shared)”. They announced that this model
    is a straight-up a **reasoning model** and that it would take more time to reason
    and answer your questions providing more accurate answers. They state that they
    have put more compute time into RL (Reinforcement Learning) than previous models
    to generate coherent ***chains-of-thoughts⁴***. Essentially, they have trained
    the chain of thought generation process using Reinforcement learning (to generate
    and hone its own generated chain of thought process). In the o1 models, the engineers
    were able to ask the model questions as to why it was wrong (whenever it was wrong)
    in its chain-of-thought process and it could identify the mistakes and correct
    itself from them. The model could question itself and have to reflect (see “Reflection
    in LLMs”) on its outputs and correct itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'In another video “[Reasoning with OpenAI o1](https://youtu.be/3BkQI3nIiB8?feature=shared)”,
    [Jerry Tworek](https://www.linkedin.com/in/jerry-tworek-b5b9aa56/) demonstrates
    how previous OpenAI and most other LLMs in the market tend to fail on the following
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: “Assume the laws of physics on earth. A small strawberry is put into a normal
    cup and the cup is placed upside down on a table. Someone then takes the cup and
    puts it inside the microwave. Where is the strawberry now? Explain your reasoning
    step by step.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Legacy GPT-4 answers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb0dcbe3c0b129811486cf62d8eead58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GPT-4 getting the strawberry in a cup question wrong'
  prefs: []
  type: TYPE_NORMAL
- en: 'The relatively newer GPT-4o also gets it wrong:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc13c3a9bd503719a5b4d33cbdff2240.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: GPT-4 o getting the strawberry in a cup question wrong'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT o1 gets the answer right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e05827f6ff9242c81afa26fb4d112eae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: GPT o1 getting the answer to the strawberry in a cup question right'
  prefs: []
  type: TYPE_NORMAL
- en: If you click on the dropdown at the beginning of the model’s response (see Figure
    4), you know that it elicits its thought process (chain-of-thought), and the researchers
    at OpenAI claim that the o1 model has been trained with reinforcement learning
    to get this chain of thought better. Also, it’s interesting to note that [Jason
    Wei](https://www.linkedin.com/in/jason-wei-5a7323b0/) (you can see him sitting
    third from the right on the bottom row in the video “[Building OpenAI o1](https://youtu.be/3k89FMJhZ00?feature=shared)”),
    the author of the chain-of-thought paper⁴ that he published back at Google, is
    now an OpenAI employee that is working on the o1 model to integrate the chain-of-thought
    (that he discovered at Google) process into this model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/782b733ada9d6169722aa4ce2703e984.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Chain-of-thought elicitation by GPT o1'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get back to the counting question that my team found out about as
    part of my assignment.
  prefs: []
  type: TYPE_NORMAL
- en: How many r’s are in the word strawberry?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s run this question on GPT-4o:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5804ab99e4ef3d95165148948f10df5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: GPT4o getting the number of r’s in strawberry question wrong.'
  prefs: []
  type: TYPE_NORMAL
- en: A very simple counting problem that it get’s wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run this on the new GPT o1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bab8fe662094f3e72eb9d9a886e6daa7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: GPT o1 getting the number of r’s in strawberry question right.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT o1 gets the answer right by thinking for a couple of seconds. The researchers
    at OpenAI say that it goes through its response repeatedly and thinks its way
    to the right answer. There does seem to be really significant improvements in
    terms of the model’s ability to solve a lot of academic exam questions.
  prefs: []
  type: TYPE_NORMAL
- en: Anyways after I opened up X.com (Formerly Twitter), I came across several people
    showcasing their attempts at trying to make the o1 model fail. This is a fascinating
    one I came across (this [tweet](https://x.com/creeoer/status/1834588136749388035)
    by @creeor) where the model fails to answer a trivial question in which the answer
    lies in the question itself. So I tried the exact same prompt on my account and
    it gave me the wrong answer (see Figure 7).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a308a9ce3906f01eddd5001ca4a82df6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: OpenAI o1 is still getting simple riddles wrong when you tweak the
    riddle. Shows that the models still rely on a lot of the memorization that happened
    during it’s training and isn’t fully using it’s reasoning capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: When I ask it what this classic riddle is that it is talking about it tells
    me about a riddle that it memorized from the internet. It’s interesting to see
    how these models can sometimes fall back on memorized content rather than truly
    reasoning through a problem. Despite the significant advancements and [benchmark](https://cdn.openai.com/o1-system-card.pdf)
    improvements, there are still areas where AI models struggle, especially with
    tasks that require deeper reasoning or understanding context in a nuanced way.
    While benchmarks can show progress, real-world applications often reveal the limitations.
    It’s through continuous testing, feedback, and real-world use cases that these
    models can be further refined.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b25f1703d94524a7f99069f24231a57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: o1 blindly answers from a riddle it memorized. It doesn’t read the
    question that was given to it and try to answer it as presented.'
  prefs: []
  type: TYPE_NORMAL
- en: There was a [compilation](https://medium.com/@aliborji/a-categorical-archive-of-chatgpt-failures-2c888805d3c3)
    of ChatGPT failures done about a year and a half back. Compilations of model errors
    are invaluable for understanding and improving AI systems. I’m sure people will
    come up with another compilation of errors for the o1 model soon.
  prefs: []
  type: TYPE_NORMAL
- en: Although I agree entirely that the chain-of-thought process benefits both AI
    and human learning, real learning indeed comes from experience and making mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: I will continue to keep posting my findings on the o1 model on my Medium page.
    Follow my account to keep posted. And thank you for taking the time to read my
    Medium post.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Arkoudas, Konstantine. “GPT-4 can’t reason.” *arXiv preprint arXiv:2308.03762*
    (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Aghzal, Mohamed, Erion Plaku, and Ziyu Yao. “Look Further Ahead: Testing
    the Limits of GPT-4 in Path Planning.” *arXiv preprint arXiv:2406.12000* (2024).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Kambhampati, Subbarao, et al. “LLMs Can’t Plan, But Can Help Planning in
    LLM-Modulo Frameworks.” *arXiv preprint arXiv:2402.01817* (2024).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Wei, Jason, et al. “Chain-of-thought prompting elicits reasoning in large
    language models.” *Advances in neural information processing systems* 35 (2022):
    24824–24837.'
  prefs: []
  type: TYPE_NORMAL
