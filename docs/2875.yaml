- en: How to Prune LLaMA 3.2 and Similar Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-prune-llama-3-2-and-similar-large-language-models-cf18e9a2afb6?source=collection_archive---------6-----------------------#2024-11-27](https://towardsdatascience.com/how-to-prune-llama-3-2-and-similar-large-language-models-cf18e9a2afb6?source=collection_archive---------6-----------------------#2024-11-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article presents a structured pruning technique for state-of-the-art models,
    that uses a GLU architecture, enabling the creation of smaller and more efficient
    large language models.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@peremartra?source=post_page---byline--cf18e9a2afb6--------------------------------)[![Pere
    Martra](../Images/18e937d347f4f726847c356d9ea5fbdc.png)](https://medium.com/@peremartra?source=post_page---byline--cf18e9a2afb6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf18e9a2afb6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf18e9a2afb6--------------------------------)
    [Pere Martra](https://medium.com/@peremartra?source=post_page---byline--cf18e9a2afb6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf18e9a2afb6--------------------------------)
    ·14 min read·Nov 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: This article was originally written in Spanish and translated
    into English using AI tools as support to ensure accuracy and consistency. You
    can find the original Spanish version* [*here*](https://martra.uadla.com/creando-small-models-eficientes-con-llama-3-2-y-pruning/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: As large language models continue to grow in size to achieve greater capabilities,
    the demand for more efficient, smaller versions has become more necessary than
    ever. However, reducing a model’s size without losing its core functionality is
    a delicate balancing act.
  prefs: []
  type: TYPE_NORMAL
- en: Techniques such as quantization and pruning are commonly used to decrease size,
    while methods like knowledge distillation or transfer learning help retain or
    recover the capabilities lost during the reduction process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/deb89dfb2377d7025fa02d43207ccc47.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author with GPT 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among these, **pruning** stands out as one of the most effective strategies
    for reducing model size. Unlike quantization, which simplifies numerical representations,
    pruning involves removing specific parts of the model, such as neurons or entire
    layers. But this effectiveness comes at a cost: pruning…'
  prefs: []
  type: TYPE_NORMAL
