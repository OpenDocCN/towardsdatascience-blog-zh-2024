- en: MultiChoice Question Answering In HuggingFace
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HuggingFace中的多选问答
- en: 原文：[https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07](https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07](https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07)
- en: Unveiling the power of question answering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示问答任务的强大能力
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    ·15 min read·Feb 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    ·阅读时长15分钟·2024年2月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/53031ba3d97b0658a995789648fe0416.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53031ba3d97b0658a995789648fe0416.png)'
- en: Image from unsplash.com
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自unsplash.com
- en: Natural language processing techniques are demonstrating immense capability
    on **question answering (QA) tasks**. In this post, we leverage the HuggingFace
    library to tackle a multiple choice question answering challenge.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理技术在**问答（QA）任务**中展现出了巨大的能力。在这篇文章中，我们利用HuggingFace库来解决一个多选问答挑战。
- en: Specifically, **we fine-tune a pre-trained BERT model on a multi-choice question
    dataset using the Trainer API**. This allows adapting the powerful bidirectional
    representations from pre-trained BERT to our target task. By adding a **classification
    head**, the model learns textual patterns that help determine the correct choice
    out of a set of answer options per question. **We then evaluate performance using
    accuracy across the held-out test set**.
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 具体来说，**我们在一个多选问题数据集上微调一个预训练的BERT模型，使用Trainer API**。这使得我们能够将预训练BERT的强大双向表示适配到我们的目标任务中。通过添加**分类头**，模型可以学习帮助从多个选项中选择正确答案的文本模式。**然后，我们通过在保留的测试集上使用准确率来评估模型性能**。
- en: The Transformer framework allows quickly experimenting with different model
    architectures, tokenizer options, and training approaches. In this analysis, we
    demonstrate a step by step recipe for achieving competitive performance on multiple
    choice QA through HuggingFace Transformers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer框架允许快速实验不同的模型架构、分词器选项和训练方法。在本次分析中，我们将展示一个逐步的过程，用于通过HuggingFace Transformers实现多选QA任务的竞争性性能。
- en: 'First step: Install and Import libraries'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：安装并导入库
- en: 'The first step is to install and import the libraries. To install the libraries
    use `pip install` command as following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装和导入相关库。要安装库，请使用以下`pip install`命令：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
