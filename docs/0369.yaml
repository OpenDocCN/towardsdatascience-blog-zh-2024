- en: MultiChoice Question Answering In HuggingFace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07](https://towardsdatascience.com/multichoice-question-answering-in-huggingface-96f61eb88d18?source=collection_archive---------9-----------------------#2024-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unveiling the power of question answering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--96f61eb88d18--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--96f61eb88d18--------------------------------)
    ·15 min read·Feb 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53031ba3d97b0658a995789648fe0416.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing techniques are demonstrating immense capability
    on **question answering (QA) tasks**. In this post, we leverage the HuggingFace
    library to tackle a multiple choice question answering challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, **we fine-tune a pre-trained BERT model on a multi-choice question
    dataset using the Trainer API**. This allows adapting the powerful bidirectional
    representations from pre-trained BERT to our target task. By adding a **classification
    head**, the model learns textual patterns that help determine the correct choice
    out of a set of answer options per question. **We then evaluate performance using
    accuracy across the held-out test set**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Transformer framework allows quickly experimenting with different model
    architectures, tokenizer options, and training approaches. In this analysis, we
    demonstrate a step by step recipe for achieving competitive performance on multiple
    choice QA through HuggingFace Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First step: Install and Import libraries'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step is to install and import the libraries. To install the libraries
    use `pip install` command as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
