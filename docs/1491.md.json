["```py\n{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128009,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.40.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n```", "```py\nimport transformers\nimport torch\nimport numpy as np\nfrom transformers import AutoConfig, LlamaModel\nfrom safetensors import safe_open\nimport os\nimport matplotlib.pyplot as plt\n```", "```py\nMODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n!huggingface-cli download {MODEL_ID} --quiet --local-dir /tmp/{MODEL_ID}\n```", "```py\ndef load_specific_layers_safetensors(model, model_name, layer_to_load):\n    state_dict = {}\n    files = [f for f in os.listdir(model_name) if f.endswith('.safetensors')]\n    for file in files:\n        filepath = os.path.join(model_name, file)\n        with safe_open(filepath, framework=\"pt\") as f:\n            for key in f.keys():\n                if f\"layers.{layer_to_load}.\" in key:\n                    new_key = key.replace(f\"model.layers.{layer_to_load}.\", 'layers.0.')\n                    state_dict[new_key] = f.get_tensor(key)\n\n    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n    if missing_keys:\n        print(f\"Missing keys: {missing_keys}\")\n    if unexpected_keys:\n        print(f\"Unexpected keys: {unexpected_keys}\")\n```", "```py\ndef get_singular_values(model_path, matrix_type, layer_number, head_number):\n    \"\"\"\n    Computes the singular values of the specified matrix in the Llama-3 model.\n\n    Parameters:\n    model_path (str): Path to the model\n    matrix_type (str): Type of matrix ('q', 'k', 'v', 'o')\n    layer_number (int): Layer number (0 to 31)\n    head_number (int): Head number (0 to 31)\n\n    Returns:\n    np.array: Array of singular values\n    \"\"\"\n    assert matrix_type in ['q', 'k', 'v', 'o'], \"Invalid matrix type\"\n    assert 0 <= layer_number < 32, \"Invalid layer number\"\n    assert 0 <= head_number < 32, \"Invalid head number\"\n\n    # Load the model only for that specific layer since we have limited RAM even after using fp16\n    config = AutoConfig.from_pretrained(model_path)\n    config.num_hidden_layers = 1\n    model = LlamaModel(config)\n    load_specific_layers_safetensors(model, model_path, layer_number)\n\n    # Access the specified layer\n    # Always index 0 since we have loaded for the specific layer\n    layer = model.layers[0]\n\n    # Determine the size of each head\n    num_heads = layer.self_attn.num_heads\n    head_dim = layer.self_attn.head_dim\n\n    # Access the specified matrix\n    weight_matrix = getattr(layer.self_attn, f\"{matrix_type}_proj\").weight.detach().numpy()\n    if matrix_type in ['q','o']:\n        start = head_number * head_dim\n        end = (head_number + 1) * head_dim\n    else:  # 'k', 'v' matrices\n        # Adjust the head_number based on num_key_value_heads\n        # This is done since llama3-8b use Grouped Query Attention\n        num_key_value_groups = num_heads // config.num_key_value_heads\n        head_number_kv = head_number // num_key_value_groups\n        start = head_number_kv * head_dim\n        end = (head_number_kv + 1) * head_dim\n\n    # Extract the weights for the specified head\n    if matrix_type in ['q', 'k', 'v']:\n        weight_matrix = weight_matrix[start:end, :]\n    else:  # 'o' matrix\n        weight_matrix = weight_matrix[:, start:end]\n\n    # Compute singular values\n    singular_values = np.linalg.svd(weight_matrix, compute_uv=False)\n\n    del model, config\n\n    return list(singular_values)\n```"]