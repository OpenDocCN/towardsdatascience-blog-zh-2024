- en: How Much Energy Do LLMs Consume?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大语言模型消耗多少能源？
- en: 原文：[https://towardsdatascience.com/how-much-energy-do-llms-consume-23c09fe3f545?source=collection_archive---------4-----------------------#2024-07-02](https://towardsdatascience.com/how-much-energy-do-llms-consume-23c09fe3f545?source=collection_archive---------4-----------------------#2024-07-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-much-energy-do-llms-consume-23c09fe3f545?source=collection_archive---------4-----------------------#2024-07-02](https://towardsdatascience.com/how-much-energy-do-llms-consume-23c09fe3f545?source=collection_archive---------4-----------------------#2024-07-02)
- en: We use EnergyMeter, a Python tool, to measure the energy consumption of different
    LLMs including Llama, Dolly, and BLOOM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们使用 EnergyMeter 这一 Python 工具来测量不同 LLM（大语言模型）的能耗，包括 Llama、Dolly 和 BLOOM。
- en: '[](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)[![Mauricio
    Fadel Argerich](../Images/0e84f1a168a436821894c0e13b4e0c53.png)](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)
    [Mauricio Fadel Argerich](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)[![Mauricio
    Fadel Argerich](../Images/0e84f1a168a436821894c0e13b4e0c53.png)](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)
    [Mauricio Fadel Argerich](https://medium.com/@mauriciofadelargerich?source=post_page---byline--23c09fe3f545--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)
    ·12 min read·Jul 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--23c09fe3f545--------------------------------)
    ·12 分钟阅读·2024 年 7 月 2 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5630ac576e53b591e0392028054cfb64.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5630ac576e53b591e0392028054cfb64.png)'
- en: Replying to all those questions requires a lot of energy! [made with AI by [Designer](https://www.bing.com/images/create?form=SBCATT).]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 回答所有这些问题需要大量的能源！[由 AI 制作 [Designer](https://www.bing.com/images/create?form=SBCATT)。]
- en: 'Large Language Models (LLMs) are becoming the new mainstream for several tasks
    we perform everyday: searching answers to our everyday questions, summarizing
    texts, creating slides or creating images for this article; LLMs are powerful
    tools and its safe to say their use will increase in the years to come. Their
    amazing power was achieved in part thanks to the (really) large number of trainable
    parameters they have; just as an example, the model behind ChatGPT (GPT-3) has
    more than 175 billion parameters. Training such large models requires incredibly
    powerful computing resources (in particular GPUs), which is translated into really
    high costs for buying or even renting the resources and, in turn, this is translated
    into a high energy demand, needed to power those resources. For instance, it is
    estimated that training GPT-3 required 1287 MWh, equivalent to the mean yearly
    energy consumption of 420 persons [1].'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）正在成为我们日常执行的几个任务的新主流：搜索日常问题的答案、总结文本、制作幻灯片或为本文创建图像；LLM 是强大的工具，可以说它们的使用将在未来几年增加。它们惊人的能力部分得益于它们拥有的（非常）大量可训练参数；举个例子，ChatGPT
    背后的模型（GPT-3）有超过 1750 亿个参数。训练如此庞大的模型需要极其强大的计算资源（特别是 GPU），这意味着购买或租用这些资源的成本非常高，进而带来很高的能源需求，以供给这些资源的动力。例如，据估计，训练
    GPT-3 需要 1287 MWh，相当于 420 人的年平均能源消耗[1]。
- en: But even more energy is used to keep these LLMs running, for their inference.
    In fact, OpenAI announced that 100 million people are using ChatGPT weekly [2],
    so even if we assume each user only asks four question per week, this means that
    ChatGPT has at least…
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为了保持这些 LLM 的运行，进行推理时消耗的能量更多。实际上，OpenAI 宣布每周有 1 亿人使用 ChatGPT[2]，即使我们假设每个用户每周只提问四个问题，这意味着
    ChatGPT 至少…
