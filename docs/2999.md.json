["```py\npip install gymnasium torch\n```", "```py\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim):\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.mean = nn.Linear(hidden_dim, action_dim)\n        self.log_std = nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        mean = self.mean(x)\n        log_std = torch.clamp(self.log_std(x), -20, 2)  # Limit log_std to prevent instability\n        return mean, log_std\n```", "```py\nclass SoftQNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim):\n        super(SoftQNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state, action):\n        x = torch.cat([state, action], dim=-1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.out(x)\n```", "```py\nclass ValueNetwork(nn.Module):\n    def __init__(self, state_dim, hidden_dim):\n        super(ValueNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return self.out(x)\n```", "```py\ndef update(batch_size, reward_scale, gamma=0.99, soft_tau=1e-2):\n    # Sample a batch\n    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n    state, next_state, action, reward, done = map(lambda x: torch.FloatTensor(x).to(device), \n                                                  [state, next_state, action, reward, done])\n\n    # Update Q-networks\n    target_value = target_value_net(next_state)\n    target_q = reward + (1 - done) * gamma * target_value\n    q1_loss = F.mse_loss(soft_q_net1(state, action), target_q.detach())\n    q2_loss = F.mse_loss(soft_q_net2(state, action), target_q.detach())\n\n    soft_q_optimizer1.zero_grad()\n    q1_loss.backward()\n    soft_q_optimizer1.step()\n\n    soft_q_optimizer2.zero_grad()\n    q2_loss.backward()\n    soft_q_optimizer2.step()\n\n    # Update Value Network\n    predicted_q = torch.min(soft_q_net1(state, action), soft_q_net2(state, action))\n    value_loss = F.mse_loss(value_net(state), predicted_q - alpha * log_prob)\n    value_optimizer.zero_grad()\n    value_loss.backward()\n    value_optimizer.step()\n\n    # Update Policy Network\n    new_action, log_prob, _, _, _ = policy_net.evaluate(state)\n    policy_loss = (alpha * log_prob - predicted_q).mean()\n    policy_optimizer.zero_grad()\n    policy_loss.backward()\n    policy_optimizer.step()\n\n    # Soft Update Target Network\n    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n        target_param.data.copy_(soft_tau * param.data + (1 - soft_tau) * target_param.data)\n```", "```py\npython sac.py --train\npython sac.py --test\n```"]