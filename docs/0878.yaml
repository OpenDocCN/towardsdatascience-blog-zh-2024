- en: 'Deep Learning At Scale: Parallel Model Training'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05](https://towardsdatascience.com/deep-learning-at-scale-parallel-model-training-d7c22904b5a4?source=collection_archive---------9-----------------------#2024-04-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Concept and a Pytorch Lightning example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[![Caroline
    Arnold](../Images/fb13ba36e302d8161b67c4888d0601e4.png)](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    [Caroline Arnold](https://medium.com/@caroline.arnold_63207?source=post_page---byline--d7c22904b5a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7c22904b5a4--------------------------------)
    ·7 min read·Apr 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9889f32b020f8d3fbe4ebe73db1d32fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author using Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel training on a large number of GPUs is state of the art in deep learning.
    The open source image generation algorithm Stable Diffusion [was trained](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/)
    on a cluster of 256 GPUs. Meta’s [AI Research SuperCluster](https://www.siliconrepublic.com/machines/meta-is-using-two-nvidia-gpu-clusters-to-train-llama-3)
    contains more than 24,000 NVIDIA H100 GPUs that are used to train models such
    as Llama 3.
  prefs: []
  type: TYPE_NORMAL
- en: By using multiple GPUs, machine learning experts reduce the *wall time* of their
    training runs. Training Stable Diffusion [took 150,000 GPU hours](https://the-decoder.com/training-cost-for-stable-diffusion-was-just-600000-and-that-is-a-good-sign-for-ai-progress/),
    or more than 17 years. Parallel training reduced that to 25 days.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of [parallel deep learning](https://arxiv.org/abs/1802.09941):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data parallelism*, where a large dataset is distributed across multiple GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model parallelism*, where a deep learning model that is too large to fit on
    a single GPU is distributed across multiple devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will focus here on data parallelism, as model parallelism [only becomes relevant](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html)
    for very large models beyond 500M parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond reducing wall time, there is an economic argument for parallel training:
    Cloud compute providers such as [AWS offer single machines](https://docs.aws.amazon.com/batch/latest/userguide/gpu-jobs.html)
    with up to 16 GPUs. Parallel training can take advantage of all available GPUs,
    and you get more value for your money.'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Parallelism is the dominant paradigm](https://en.wikipedia.org/wiki/Parallel_computing)
    in high performance computing. Programmers identify tasks that can be executed
    independently of each other and distribute them across a large number of devices.
    The serial parts of the program distribute the tasks and gather the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computing reduces computation time. A program parallelized across four
    devices could ideally run four times faster than the same program running on a
    single device. In practice, communication overhead limits this scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '*As an analogy, think of a group of painters painting a wall. The communication
    overhead occurs when the foreman tells everyone what color to use and what area
    to paint. Painting can be done in parallel, and only the finishing is again a
    serial task.*'
  prefs: []
  type: TYPE_NORMAL
- en: Training deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training a deep learning algorithm requires data and an optimization routine
    like [stochastic gradient descent](/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79).
  prefs: []
  type: TYPE_NORMAL
- en: The training data is shuffled and split into batches of fixed size. Batch by
    batch, the training routine calculates the loss between the actual and predicted
    labels. The model parameters are adjusted according to the gradient of the loss
    with respect to the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f2d4393572ca20cfa46824bd8c9d985.png)'
  prefs: []
  type: TYPE_IMG
- en: Stochastic gradient descent. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: An *epoch* of training is complete when the model has seen all training data
    batches once.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed data parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following diagram describes data parallelism for training a deep learning
    model using 3 GPUs. A replica of the untrained model is copied to each GPU. The
    data set is split into 3 parts, each part is processed in parallel on a separate
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f7994943c709cc560310884b328f8f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Distributed data, same model. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Note that each model replica sees different subsets of the training data. After
    completing the epoch, the weights and biases in each model replica are different.
  prefs: []
  type: TYPE_NORMAL
- en: The three model replicas are harmonized by averaging the weights across all
    model instances. The updated model is broadcast across all 3 GPUs, and training
    continues with the next epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training changes the effective batch size to `number of devices
    * original batch size` . This can affect training, convergence, and model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Pytorch Lightning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need several ingredients for data parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: A dataloader that can handle distributed training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An *all-reduce* function that harmonizes the model replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A framework for the different parallel parts to communicate with each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In PyTorch Lightning, the [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html)
    handles the entire training process. It can perform distributed data parallel
    training *out of the box* by specifying the following keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of nodes — the machines in the cloud compute cluster that you want to
    use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of devices (GPUs) *per node*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training strategy, here distributed data parallelism. Different [strategies](https://lightning.ai/docs/pytorch/stable/api_references.html#strategies)
    are available for distributed training, they take care of adjusting the training
    procedure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For distributed data parallelism (*ddp*) training on 2 nodes with 2 GPUs each,
    use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By default, the Lightning Trainer uses all available GPUs in the context of
    the computing environment. This implies that you need to specify explicitly if
    you want to use only a single GPU on a multi-GPU machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment: Parallel training with the Country211 dataset'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [Country211 dataset](https://github.com/openai/CLIP/blob/main/data/country211.md)
    consists of geo-tagged images from 211 countries. In my experiment, I use a pre-trained
    vision transformer and fine tune this model on the Country211 dataset. The code
    is available on [GitHub](https://github.com/crlna16/pretrained-vision-transformer).
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
    [## How to Fine-Tune a Pretrained Vision Transformer on Satellite Data'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step tutorial in PyTorch Lightning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=post_page-----d7c22904b5a4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: There are 31,650 training images. I used a batch size of 256 throughout and
    [experimented with the number of GPUs](https://lightning.ai/docs/fabric/stable/guide/multi_node/cloud.html),
    going from a single GPU on a single node up to eight GPUs distributed across four
    nodes. I applied an early stopping condition, where training stops if the validation
    accuracy does not improve for five epochs.
  prefs: []
  type: TYPE_NORMAL
- en: The training wall time, which is the total time it took to train the model,
    decreases with the number of GPUs used. On a single GPU, the wall time is 45 minutes,
    which drops to 11 minutes when I use eight GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal scaling would be 45 minutes / 8 GPUs = 5.6 minutes, but communication
    overhead and a larger number of epochs when training on more GPUs keep us from
    reaching this optimum.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e288058bf56d71bc57ef28cd0c8acbe3.png)'
  prefs: []
  type: TYPE_IMG
- en: Wall time for different numbers of devices. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the generalization capabilities, I calculated the test set accuracy
    with all trained models. The best test set accuracy was obtained when using a
    single GPU on a single device, but even there, the model had issues with generalization:
    The test set accuracy is only 15%.'
  prefs: []
  type: TYPE_NORMAL
- en: The panel shows the decrease in accuracy, relative to the optimal result, for
    different numbers of GPUs. The more devices were used in training, the less accurate
    the model became when applied to the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28b987cf516ae749486725807cc19f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Test set accuracy. Image created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The changing accuracy could be due to the changing effective batch size — note
    that we do not get exactly the same model depending on the training strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In this experiment, there is a tradeoff between generalization accuracy and
    speed. Training on 8 GPUs was four times faster, but 11% less accurate than training
    on a single device.
  prefs: []
  type: TYPE_NORMAL
- en: A look at parallel training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the training log file, you will see lines like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The parallel training processes must be able to communicate with each other.
    The communication backend, here the NVIDIA Collective Communications Library (*nccl*),
    is automatically initialized depending on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: We have requested two nodes with two GPUs each, for a total of four parallel
    processes. They are enumerated by their *global rank* (0–3). On each node, the
    *local rank* identifies the GPU that is used (0 or 1).
  prefs: []
  type: TYPE_NORMAL
- en: When the model is large, the [*all-reduce* operation](https://en.wikipedia.org/wiki/Collective_operation#All-Reduce)
    can take a long time. Compute clusters connect the GPUs with efficient networks,
    but sending a full model replica out puts a strain on communication.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Lightning provides strategies](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html)
    for model parallelism. Note that this should only be explored for very large models
    with billions of parameters, where the model parameters and gradients exceed GPU
    memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel training can speed up the training process and help you make the most
    of your computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: We distinguish between data parallelism, where replicas of the model see subsets
    of the data during training, and model parallelism, where the model itself is
    distributed across multiple devices.
  prefs: []
  type: TYPE_NORMAL
- en: With PyTorch Lightning, data parallel training is as simple as specifying the
    *distributed data parallelism* strategy and the number of nodes and devices (GPUs)
    per node.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallel training reduces the wall time and changes the effective batch
    size. Models trained on different numbers of devices may perform differently,
    and careful evaluation is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallel training should only be considered for very large models exceeding
    500M parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to replicate the experiment can be found on my GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)
    [## GitHub - crlna16/pretrained-vision-transformer: Pretrained Vision Transformer
    with PyTorch…'
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Vision Transformer with PyTorch Lightning - crlna16/pretrained-vision-transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d7c22904b5a4--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tal Ben-Nun *et al*, “Demystifying Parallel and Distributed Deep Learning:
    An In-Depth Concurrency Analysis”, 2018, [https://arxiv.org/abs/1802.09941](https://arxiv.org/abs/1802.09941)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI Country211 dataset: [source](https://github.com/openai/CLIP/blob/main/data/country211.md)
    (subset of [YFCC100M](https://dx.doi.org/10.1145/2812802)) and [license](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/#yfcc100m)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vision transformer model: Dosovitskiy et al, *An Image is Worth 16x16 Words:
    Transformers for Image Recognition at Scale*, ICLR, 2021\. [Paper](https://arxiv.org/pdf/2010.11929.pdf)
    and code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch Lightning project for data-parallel training: [docs](https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will Falcon on distributed training [(TDS, 2019)](/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
