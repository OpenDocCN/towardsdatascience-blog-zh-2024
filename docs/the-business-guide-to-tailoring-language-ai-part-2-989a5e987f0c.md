# 定制语言AI的商业指南 第二部分

> 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19)

## 向ChatGPT和其他基于聊天的语言AI发出提示——以及为什么您应该（不）关心它

[](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[![Georg Ruile博士](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------) [Georg Ruile博士](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------) ·12分钟阅读·2024年4月19日

--

![](../Images/d38c05848cc0a25ec1ad62765d9fe3e1.png)

## 前言

本文阐明了如何“与”旨在进行对话交互的超大语言模型（LLM）进行沟通，如ChatGPT、Claude等，从而确保您从中获得的答案对于当前任务尽可能有用。人类与语言聊天机器人之间的这种沟通通常被称为提示。在本文中，我旨在为没有计算机科学背景的人提供关于该主题的简明概述，以便每个人都能理解。它也可以帮助企业理解在定制LLM过程中应当期待什么（或不应期待什么）。

提示是您在为企业定制语言模型时可以采取的四个步骤中的第一个。我在[上一篇文章](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838)中介绍了**四步框架**，旨在解锁定制LLM。如果您还没有阅读过，先读一下可能会对您有所帮助，这样您就能将本文中的思想放入更大的背景中。

[](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------) [## 定制语言AI的商业指南

### 解锁您将理解的定制LLM解决方案框架

towardsdatascience.com](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)

# 引言

在ChatGPT广泛推出后不久，一个新的热门职业进入了AI领域：**提示工程**。这些AI“耳语者”，即那些具有特定“提示”技能的人，能够与语言AI对话，让它以有用的方式回应，已成为炙手可热的职位（[并且薪水丰厚](https://www.forbes.com/sites/jackkelly/2024/03/06/the-hot-new-high-paying-career-is-an-ai-prompt-engineer/)）。考虑到正确提示的一个主要构建块仅仅是（或不那么简单地）提供准确的指令（见下文），我必须承认这一发展让我感到惊讶（尽管提示*工程*无疑不仅仅是“耳语”）：精确简洁地沟通不是我们每个人都应具备的基本职业技能吗？但我又反思到，在软件开发中拥有精心设计的需求是多么重要，而“需求工程”角色已经成为成功软件开发项目的重要组成部分。

在LLM和提示的主题中，我观察到一种不确定性和“最佳猜测”，甚至是矛盾，这在我以往接触的任何IT相关主题中都没有经历过。这与AI模型的类型和规模以及它们的**随机特性**有关，而这些超出了本文的讨论范围。考虑到像GPT-4这样模型的[1.76万亿参数](https://en.wikipedia.org/wiki/GPT-4)，从输入（你的“提示”）到输出（模型回应）的可能组合和路径数量几乎是无限且非确定性的。因此，应用程序主要将这些模型视为黑箱，而相关研究则侧重于经验方法，比如基准测试它们的性能。

不幸的是，我不能提供一个完美的、适用于所有情况的提示解决方案，来永远解决你的LLM需求。再者，**不同的模型行为不同**，你可能会理解我的困境。不过，也有一些好消息：一方面，你可以且应该始终考虑一些**基本原则和概念**，这些会帮助你优化与机器的互动。精心设计的提示能比糟糕的提示更进一步，这也是为什么深入探讨这个话题是非常值得的。另一方面，**甚至可能根本不需要过多担心提示问题**，这将节省你宝贵的计算时间（字面意义上的CPU/GPU时间，以及比喻意义上你自己大脑的时间）。

## 从“为什么”开始

这里我并不是指Simon Sinek的经典TEDx商业建议。相反，我鼓励你去好奇地思考**技术为什么会这样做**。我坚信，如果你至少理解一点软件的内在工作原理，它将极大地帮助你在应用中使用它。

那么，从原则上来说，输入（提示）是如何与输出（响应）相关的？为什么正确的提示会导致更合适的响应？要弄清楚这一点，我们至少需要对模型架构及其训练和微调有一个粗略的了解，而不需要理解像臭名昭著的 Transformer 架构和注意力机制等深奥的概念，这些概念最终促成了我们今天所知的 ChatGPT 类生成 AI 的突破。

从我们的角度来看，可以从两个方面进行考虑：

*模型是如何* ***检索知识并生成响应的？***与此密切相关

*模型是如何被* ***训练和微调的？***

重要的是要理解，LLM 本质上是一个**深度神经网络**，因此，它是基于**统计和概率**来工作的。简单来说，模型生成的输出反映了与上下文最接近的匹配，基于它从大量训练数据中学习到的知识。这里的一个构建块是所谓的**嵌入**，其中相似的词义（在数学上）彼此接近，尽管模型实际上并不“理解”这些词义。如果这听起来很花哨，它确实有点复杂，但同时，它“仅仅”是数学而已，所以不要害怕。

![](../Images/bdb1b66d57538d9d96891eb1aea06b47.png)

一个简单的词向量嵌入示例——相似的词“含义”彼此接近

在查看训练过程时，考虑语言模型所经过的训练数据和过程是有帮助的。模型不仅看过大量的文本数据，它还学会了什么构成了对特定问题的高质量回答，例如在像 StackOverflow 这样的站点上，或者在为模型训练和微调编写的高质量 Q&A 助手文档中。此外，在微调阶段，它还基于**人类反馈**学习并迭代地调整其最佳响应。如果没有这些强大的训练和微调工作，模型可能会像回答“你叫什么名字”这样的问题时，直接回答“你姓什么”，因为它在互联网上的表单中经常看到这种情况[1]。

我想表达的是：与自然语言 AI 交互时，始终要记住模型是如何学习的，以及它如何根据你的输入生成输出。尽管没有人能确切知道这一点，但考虑可能的相关性是很有用的：模型之前在哪些地方和什么上下文中可能见过与你相似的输入？在预训练阶段，模型有哪些数据可用，且数据的质量和数量如何？举个例子：有没有想过，为什么大型语言模型（LLM）能够解决数学方程（尽管不总是可靠，但有时仍然令人惊讶），而没有内建的计算能力？LLM 不进行计算，它们匹配模式！

# 提示入门 101

有许多提示技巧，以及大量的科学文献对其效果进行了基准测试。在这里，我只是想介绍一些知名的概念。我相信，一旦你理解了这些基本概念，你就能扩展自己的提示技巧库，甚至自己开发和测试新的技巧。

## 提问，它会给你答案

在深入具体的提示概念之前，我想强调一个我认为无法过度强调的普遍观点：

***你的提示质量决定了模型的响应。***

这里说的质量，我不一定指复杂的提示构建。我指的是提出精确问题或给出结构良好的指令并提供必要背景的基本思路。我在[上一篇文章](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838)中提到过这个概念。当我们遇到萨姆——钢琴演奏者时，你会发现如果你让酒吧钢琴师演奏一首随机的爵士乐，他可能不会演奏你心中的那首曲子。而如果你准确地告诉他你想听的是什么，那么你对结果的满意度可能会更高。

类似地，如果你曾经有机会，假设雇人做些家务，而你的合同说明仅仅写着“浴室翻新”，你可能会感到惊讶，最后的浴室根本没有达到你预期的效果。承包商，就像模型一样，只会参考他学到的翻新和浴室装修的知识，并会按照这些经验来完成工作。

所以，以下是一些**提示的通用指导原则**：

· 要清晰具体。

· 要完整。

· 提供背景信息。

· 指定期望的输出风格、长度等。

这样，模型在生成响应时就能根据你的提示，拥有足够且匹配的参考数据。

## 角色扮演提示——简单，但被高估了

在 ChatGPT 的早期，角色扮演提示的想法随处可见：与其要求助手立即给出答案（即简单查询），你首先给它指定一个特定角色，如“教师”或“顾问”等。这样的提示可能看起来像这样[2]：

***从现在起，你是一位优秀的数学老师，总是正确地教授你的学生数学问题。我是你的学生之一。***

已经证明，这一概念能够带来更优的结果。一个[研究论文](https://arxiv.org/abs/2308.07702)报告指出，通过这种角色扮演，模型会隐式地触发逐步推理过程，这正是你希望它在应用CoT技巧时所做的，如下所示。然而，这种方法也已被证明**有时会表现不尽如人意**，因此需要精心设计。

根据我的经验，单纯分配一个角色并不能奏效。我曾尝试过上面提到的论文中的示例任务。与这项研究不同，GPT3.5（截至今天是OpenAI的ChatGPT的免费版本，你可以自己尝试）通过简单查询得出了正确的结果：

![](../Images/89a418b3d6837d5d59787565537487e2.png)

一个使用简单查询的示例，而不是[2]建议的角色扮演提示，依然得出了正确的回答。

我还尝试过用简单查询和角色扮演进行不同的逻辑挑战，使用了类似上面的提示。在我的实验中，发生了两种情况：

*要么* ***简单查询在第一次尝试时给出了正确答案***，*要么*

*无论是* ***简单查询还是角色扮演都得出了错误的，但不同的答案***

**角色扮演在我的任何简单（非科学严谨）实验中并未超越查询**。因此，我得出结论，模型最近一定有所改进，而**角色扮演提示的影响正在减弱**。

通过查看不同的研究，在没有进行广泛的进一步实验的情况下，我认为为了超越简单查询，角色扮演提示需要嵌入到**合理且深思熟虑的设计**中，以超越最基本的方法——否则完全没有价值。

我很高兴在下面的评论中看到你们在这方面的经验。

## 少量样本（Few-Shot）即上下文学习

另一个直观且相对简单的概念是所谓的少量样本提示，也称为上下文学习。与零样本提示不同，我们不仅要求模型执行某项任务并期望其给出结果，还额外**提供（“少量”）解决方案的示例**。尽管你可能认为提供示例会带来更好的表现，但这实际上是一项非常显著的能力：这些大语言模型能够进行上下文学习，即通过仅凭推理，在少量输入-标签对的条件下执行新任务，并为新输入做出预测[3]。

设置少量样本提示包括：

*(1)* ***收集所需回答的示例***，并且

(2) 编写你的提示，并* ***指示如何处理这些示例***。

让我们来看一个典型的分类示例。这里，模型被给出几个判断为正面、负面或中立的陈述。模型的任务是对最终的陈述进行评分：

![](../Images/964b159019f7b64cc3c4b8f5ea1fa594.png)

一个典型的少量样本提示的分类示例。模型需要将陈述分类为给定的类别（正面/负面）。

再次强调，尽管这是一种简单直观的方法，我对它在最先进的语言模型中的价值持怀疑态度。在我的（再次，非科学严谨的）实验中，**少样本（Few-Shot）提示在任何情况下都没有优于零样本（Zero-Shot）**。（模型已经知道，不守时的鼓手是一种糟糕的体验，而我并没有教它这个……）。我的发现似乎与最近的研究一致，甚至有研究显示相反的效果（**零样本优于少样本**）[4]。

在我看来，根据这一经验背景，值得考虑的是，这种方法的设计成本、计算成本、API成本和延迟成本是否值得投资。

## CoT提示或“让我们一步步思考”

思维链（CoT）提示旨在让我们的模型更擅长解决复杂的多步骤推理问题。它可以像在输入查询中添加CoT指令“让我们一步步思考”一样简单，从而显著提高准确性[5][6]。

与其像少样本（Few-Shot）方法那样仅提供最终查询或在提示中添加一个或几个示例，不如提示模型**将其推理过程分解成一系列中间步骤**。这类似于人类如何（理想情况下）解决一个具有挑战性的问题。

记得你在学校的数学考试吗？通常，在更高级的课程中，你不仅被要求解答一个数学方程式，还要写下你是如何推导出最终解答的逻辑步骤。即使答案不对，你可能也会因为数学上合理的解题步骤而获得一些分数。就像你在学校的老师一样，你期望模型将任务分解成子任务，进行中间推理，并得出最终答案。

再次强调，我自己也进行过不少CoT实验。并且，大多数时候，仅仅添加**“让我们一步步思考”并没有改善回答的质量**。事实上，似乎**CoT方法已经成为最近经过微调的基于聊天的语言模型（如ChatGPT）的隐式标准**，即使没有明确的指令，回应也经常被分解为推理的块。

然而，我遇到过一个例子，在这个例子中，**明确的CoT指令确实显著改善了答案**。我使用了来自[这篇文章](https://medium.com/@thomasczerny/chain-of-thought-cot-prompting-9ee4967e927c)的CoT示例，但将其改编成了一个trick question。在这里，你可以看到ChatGPT是如何陷入我的陷阱的，当它没有明确要求使用CoT方法时（尽管回应中展示了逐步推理）：

![](../Images/d66d445d5c947e3d7919d1c8553ca5da.png)

这是一个通过简单查询而不是CoT提示提出的 trick question。尽管回答被“逐步”分解，但它并不完全正确。

当我在相同的提示中添加“让我们一步步思考”时，它正确地解答了这个trick question（嗯，它实际上是无法解决的，ChatGPT也正确指出了这一点）：

![](../Images/286f905eaa9ca7c70dc194b2da1c2fd3.png)

使用明确的思维链提示，得到正确回答的同一个陷阱问题

总结来说，思维链提示（Chain of Thought prompting）旨在建立推理能力，而这些能力对于语言模型来说是很难隐性地获取的。它鼓励模型阐述并完善其推理过程，而不是试图直接从问题跳跃到答案。

再次强调，我的实验结果揭示了**简单的思维链方法的效益有限**（添加“让我们一步步思考”）。**思维链在某些情况下确实比简单查询表现更好**，同时添加思维链指令的额外努力也很少。这种**成本效益比**是我喜欢这种方法的原因之一。另一个原因是，个人而言，我喜欢这种方法，它不仅能帮助模型，还能**帮助我们人类进行反思**，甚至在构建提示时，逐步考虑必要的推理步骤。

如前所述，当模型变得越来越精细调整并习惯于这种推理过程时，这种简单的思维链方法的效果可能会逐渐减弱。

# 结论

在本文中，我们深入探讨了基于聊天的大型语言模型的提示方法。与其仅仅向你介绍最流行的提示技术，我更鼓励你从“为什么提示如此重要”的问题开始这段旅程。在这段旅程中，我们发现，随着模型的演变，提示的重要性正在逐渐减弱。目前不断发展的模型架构可能会进一步减少提示技巧的相关性。**基于代理的框架**就是其中之一，它通过在处理特定查询和任务时采取不同的“路线”来实现。

然而，这并不意味着**在提示中清晰、具体并提供必要的上下文**就不值得付出努力。相反，我是这方面的坚定倡导者，因为这不仅能帮助模型，还能帮助你自己弄清楚你究竟想要达到什么目的。

就像在人类交流中一样，多个因素决定了达成预期结果的合适方法。通常，结合并反复尝试不同的方法，能在特定情境下获得最佳效果。尝试、测试、迭代！

最后，与人类交互不同，你几乎可以无限次地在个人的试验和错误提示旅程中进行测试。享受这段旅程吧！

*笔记与参考资料

所有插图均由作者亲手精心绘制 :)*

[1]: 大型语言模型如何工作：从零到ChatGPT

[https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)

[2]: 通过角色扮演提示实现更好的零样本推理

[https://arxiv.org/abs/2308.07702](https://arxiv.org/abs/2308.07702)

[3]: 重新思考演示的角色：是什么让上下文学习有效？

[https://arxiv.org/abs/2202.12837](https://arxiv.org/abs/2202.12837)

[4]: 大型语言模型的提示编程：超越少样本范式

[https://dl.acm.org/doi/abs/10.1145/3411763.3451760](https://dl.acm.org/doi/abs/10.1145/3411763.3451760)。

[5]: 什么时候需要为 ChatGPT 提供链式思维提示？

[https://arxiv.org/abs/2304.03262](https://arxiv.org/abs/2304.03262)

[6]: 大型语言模型是零-shot 推理者

[https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)
