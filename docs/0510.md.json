["```py\nimport torch\nimport torch.nn as nn\nimport math\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_dim=256, num_heads=4):\n        \"\"\"\n        input_dim: Dimensionality of the input.\n        num_heads: The number of attention heads to split the input into.\n        \"\"\"\n        super(MultiHeadAttention, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.num_heads = num_heads\n        assert hidden_dim % num_heads == 0, \"Hidden dim must be divisible by num heads\"\n        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part\n        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part\n        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part\n        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output layer\n\n    def check_sdpa_inputs(self, x):\n        assert x.size(1) == self.num_heads, f\"Expected size of x to be ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}), got {x.size()}\"\n        assert x.size(3) == self.hidden_dim // self.num_heads\n\n    def scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n        self.check_sdpa_inputs(query)\n        self.check_sdpa_inputs(key)\n        self.check_sdpa_inputs(value)\n\n        d_k = query.size(-1)\n        tgt_len, src_len = query.size(-2), key.size(-2)\n\n        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)\n        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) \n\n        # Attention mask here\n        if attention_mask is not None:\n            if attention_mask.dim() == 2:\n                assert attention_mask.size() == (tgt_len, src_len)\n                attention_mask = attention_mask.unsqueeze(0)\n                logits = logits + attention_mask\n            else:\n                raise ValueError(f\"Attention mask size {attention_mask.size()}\")\n\n        # Key mask here\n        if key_padding_mask is not None:\n            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads\n            logits = logits + key_padding_mask\n\n        attention = torch.softmax(logits, dim=-1)\n        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n\n        return output, attention\n\n    def split_into_heads(self, x, num_heads):\n        batch_size, seq_length, hidden_dim = x.size()\n        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)\n\n        return x.transpose(1, 2) # Final dim will be (batch_size, num_heads, seq_length, , hidden_dim // num_heads)\n\n    def combine_heads(self, x):\n        batch_size, num_heads, seq_length, head_hidden_dim = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)\n\n    def forward(\n            self, \n            q, \n            k, \n            v, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        q : tensor of shape (batch_size, query_sequence_length, hidden_dim)\n        k : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        v : tensor of shape (batch_size, key_sequence_length, hidden_dim)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, self.num_heads)\n        k = self.split_into_heads(k, self.num_heads)\n        v = self.split_into_heads(v, self.num_heads)\n\n        # attn_values, attn_weights = self.multihead_attn(q, k, v, attn_mask=attention_mask)\n        attn_values, attn_weights  = self.scaled_dot_product_attention(\n            query=q, \n            key=k, \n            value=v, \n            attention_mask=attention_mask,\n            key_padding_mask=key_padding_mask,\n        )\n        grouped = self.combine_heads(attn_values)\n        output = self.Wo(grouped)\n\n        self.attention_weigths = attn_weights\n\n        return output\n```", "```py\nd = {\"panther\": 1, \"bear\": 10, \"dog\":3}\nd[\"wolf\"] = 0.2*d[\"panther\"] + 0.7*d[\"dog\"] + 0.1*d[\"bear\"]\n```", "```py\nlogits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # we compute the weights of attention\n```", "```py\nattention = torch.softmax(logits, dim=-1)\noutput = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)\n```", "```py\nif attention_mask is not None:\n    if attention_mask.dim() == 2:\n        assert attention_mask.size() == (tgt_len, src_len)\n        attention_mask = attention_mask.unsqueeze(0)\n        logits = logits + attention_mask\n```", "```py\n# Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return x\n```", "```py\nclass PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model: int, d_ff: int):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))\n```", "```py\nclass EncoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(EncoderBlock, self).__init__()\n        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm2 = nn.LayerNorm(n_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, src_padding_mask=None):\n        assert x.ndim==3, \"Expected input to be 3-dim, got {}\".format(x.ndim)\n        att_output = self.mha(x, x, x, key_padding_mask=src_padding_mask)\n        x = x + self.dropout(self.norm1(att_output))\n\n        ff_output = self.ff(x)\n        output = x + self.norm2(ff_output)\n\n        return output\n```", "```py\nclass Encoder(nn.Module):\n    def __init__(\n            self, \n            vocab_size: int, \n            n_dim: int, \n            dropout: float, \n            n_encoder_blocks: int,\n            n_heads: int):\n\n        super(Encoder, self).__init__()\n        self.n_dim = n_dim\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )    \n        self.encoder_blocks = nn.ModuleList([\n            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)\n        ])\n\n    def forward(self, x, padding_mask=None):\n        x = self.embedding(x) * math.sqrt(self.n_dim)\n        x = self.positional_encoding(x)\n        for block in self.encoder_blocks:\n            x = block(x=x, src_padding_mask=padding_mask)\n        return x\n```", "```py\n # Stuff before\n\nself.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\nmasked_att_output = self.self_attention(\n    q=tgt, \n    k=tgt, \n    v=tgt, \n    attention_mask=tgt_mask, <-- HERE IS THE CAUSAL MASK\n    key_padding_mask=tgt_padding_mask)\n\n# Stuff after\n```", "```py\ndef scaled_dot_product_attention(\n            self, \n            query, \n            key, \n            value, \n            attention_mask=None, \n            key_padding_mask=None):\n        \"\"\"\n        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)\n        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)\n        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)\n        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)\n\n        \"\"\"\n```", "```py\n# Stuff before\nself.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\ncross_att_output = self.cross_attention(\n        q=x1, \n        k=memory, \n        v=memory, \n        attention_mask=None,  <-- NO CAUSAL MASK HERE\n        key_padding_mask=memory_padding_mask) <-- WE NEED TO USE THE PADDING OF THE SOURCE\n# Stuff after\n```", "```py\nclass DecoderBlock(nn.Module):\n    def __init__(self, n_dim: int, dropout: float, n_heads: int):\n        super(DecoderBlock, self).__init__()\n\n        # The first Multi-Head Attention has a mask to avoid looking at the future\n        self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm1 = nn.LayerNorm(n_dim)\n\n        # The second Multi-Head Attention will take inputs from the encoder as key/value inputs\n        self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)\n        self.norm2 = nn.LayerNorm(n_dim)\n\n        self.ff = PositionWiseFeedForward(n_dim, n_dim)\n        self.norm3 = nn.LayerNorm(n_dim)\n        # self.dropout = nn.Dropout(dropout)\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n\n        masked_att_output = self.self_attention(\n            q=tgt, k=tgt, v=tgt, attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)\n        x1 = tgt + self.norm1(masked_att_output)\n\n        cross_att_output = self.cross_attention(\n            q=x1, k=memory, v=memory, attention_mask=None, key_padding_mask=memory_padding_mask)\n        x2 = x1 + self.norm2(cross_att_output)\n\n        ff_output = self.ff(x2)\n        output = x2 + self.norm3(ff_output)\n\n        return output\n\nclass Decoder(nn.Module):\n    def __init__(\n        self, \n        vocab_size: int, \n        n_dim: int, \n        dropout: float, \n        n_decoder_blocks: int,\n        n_heads: int):\n\n        super(Decoder, self).__init__()\n\n        self.embedding = nn.Embedding(\n            num_embeddings=vocab_size, \n            embedding_dim=n_dim,\n            padding_idx=0\n        )\n        self.positional_encoding = PositionalEncoding(\n            d_model=n_dim, \n            dropout=dropout\n        )\n\n        self.decoder_blocks = nn.ModuleList([\n            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)\n        ])\n\n    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):\n        x = self.embedding(tgt)\n        x = self.positional_encoding(x)\n\n        for block in self.decoder_blocks:\n            x = block(\n                x, \n                memory, \n                tgt_mask=tgt_mask, \n                tgt_padding_mask=tgt_padding_mask, \n                memory_padding_mask=memory_padding_mask)\n        return x\n```", "```py\npadding_mask = (x == PAD_IDX)\n```", "```py\ndef generate_square_subsequent_mask(size: int):\n      \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n      mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n      return mask\n```", "```py\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom .encoder import Encoder\nfrom .decoder import Decoder\n\nclass Transformer(nn.Module):\n    def __init__(self, **kwargs):\n        super(Transformer, self).__init__()\n\n        for k, v in kwargs.items():\n            print(f\" * {k}={v}\")\n\n        self.vocab_size = kwargs.get('vocab_size')\n        self.model_dim = kwargs.get('model_dim')\n        self.dropout = kwargs.get('dropout')\n        self.n_encoder_layers = kwargs.get('n_encoder_layers')\n        self.n_decoder_layers = kwargs.get('n_decoder_layers')\n        self.n_heads = kwargs.get('n_heads')\n        self.batch_size = kwargs.get('batch_size')\n        self.PAD_IDX = kwargs.get('pad_idx', 0)\n\n        self.encoder = Encoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_encoder_layers, self.n_heads)\n        self.decoder = Decoder(\n            self.vocab_size, self.model_dim, self.dropout, self.n_decoder_layers, self.n_heads)\n        self.fc = nn.Linear(self.model_dim, self.vocab_size)\n\n    @staticmethod    \n    def generate_square_subsequent_mask(size: int):\n            \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n            mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()\n            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n            return mask\n\n    def encode(\n            self, \n            x: torch.Tensor, \n        ) -> torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, S) with elements in (0, C) where C is num_classes\n        Output\n            (B, S, E) embedding\n        \"\"\"\n\n        mask = (x == self.PAD_IDX).float()\n        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        # (B, S, E)\n        encoder_output = self.encoder(\n            x, \n            padding_mask=encoder_padding_mask\n        )  \n\n        return encoder_output, encoder_padding_mask\n\n    def decode(\n            self, \n            tgt: torch.Tensor, \n            memory: torch.Tensor, \n            memory_padding_mask=None\n        ) -> torch.Tensor:\n        \"\"\"\n        B = Batch size\n        S = Source sequence length\n        L = Target sequence length\n        E = Model dimension\n\n        Input\n            encoded_x: (B, S, E)\n            y: (B, L) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        mask = (tgt == self.PAD_IDX).float()\n        tgt_padding_mask = mask.masked_fill(mask == 1, float('-inf'))\n\n        decoder_output = self.decoder(\n            tgt=tgt, \n            memory=memory, \n            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)), \n            tgt_padding_mask=tgt_padding_mask, \n            memory_padding_mask=memory_padding_mask,\n        )  \n        output = self.fc(decoder_output)  # shape (B, L, C)\n        return output\n\n    def forward(\n            self, \n            x: torch.Tensor, \n            y: torch.Tensor, \n        ) -> torch.Tensor:\n        \"\"\"\n        Input\n            x: (B, Sx) with elements in (0, C) where C is num_classes\n            y: (B, Sy) with elements in (0, C) where C is num_classes\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        # Encoder output shape (B, S, E)\n        encoder_output, encoder_padding_mask = self.encode(x)  \n\n        # Decoder output shape (B, L, C)\n        decoder_output = self.decode(\n            tgt=y, \n            memory=encoder_output, \n            memory_padding_mask=encoder_padding_mask\n        )  \n\n        return decoder_output\n```", "```py\ndef predict(\n            self,\n            x: torch.Tensor,\n            sos_idx: int=1,\n            eos_idx: int=2,\n            max_length: int=None\n        ) -> torch.Tensor:\n        \"\"\"\n        Method to use at inference time. Predict y from x one token at a time. This method is greedy\n        decoding. Beam search can be used instead for a potential accuracy boost.\n\n        Input\n            x: str\n        Output\n            (B, L, C) logits\n        \"\"\"\n\n        # Pad the tokens with beginning and end of sentence tokens\n        x = torch.cat([\n            torch.tensor([sos_idx]), \n            x, \n            torch.tensor([eos_idx])]\n        ).unsqueeze(0)\n\n        encoder_output, mask = self.transformer.encode(x) # (B, S, E)\n\n        if not max_length:\n            max_length = x.size(1)\n\n        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * sos_idx\n        for step in range(1, max_length):\n            y = outputs[:, :step]\n            probs = self.transformer.decode(y, encoder_output)\n            output = torch.argmax(probs, dim=-1)\n\n            # Uncomment if you want to see step by step predicitons\n            # print(f\"Knowing {y} we output {output[:, -1]}\")\n\n            if output[:, -1].detach().numpy() in (eos_idx, sos_idx):\n                break\n            outputs[:, step] = output[:, -1]\n\n        return outputs\n```", "```py\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\nnp.random.seed(0)\n\ndef generate_random_string():\n    len = np.random.randint(10, 20)\n    return \"\".join([chr(x) for x in np.random.randint(97, 97+26, len)])\n\nclass ReverseDataset(Dataset):\n    def __init__(self, n_samples, pad_idx, sos_idx, eos_idx):\n        super(ReverseDataset, self).__init__()\n        self.pad_idx = pad_idx\n        self.sos_idx = sos_idx\n        self.eos_idx = eos_idx\n        self.values = [generate_random_string() for _ in range(n_samples)]\n        self.labels = [x[::-1] for x in self.values]\n\n    def __len__(self):\n        return len(self.values)  # number of samples in the dataset\n\n    def __getitem__(self, index):\n        return self.text_transform(self.values[index].rstrip(\"\\n\")), \\\n            self.text_transform(self.labels[index].rstrip(\"\\n\"))\n\n    def text_transform(self, x):\n        return torch.tensor([self.sos_idx] + [ord(z)-97+3 for z in x] + [self.eos_idx])\n```", "```py\nPAD_IDX = 0\nSOS_IDX = 1\nEOS_IDX = 2\n\ndef train(model, optimizer, loader, loss_fn, epoch):\n    model.train()\n    losses = 0\n    acc = 0\n    history_loss = []\n    history_acc = [] \n\n    with tqdm(loader, position=0, leave=True) as tepoch:\n        for x, y in tepoch:\n            tepoch.set_description(f\"Epoch {epoch}\")\n\n            optimizer.zero_grad()\n            logits = model(x, y[:, :-1])\n            loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n            loss.backward()\n            optimizer.step()\n            losses += loss.item()\n\n            preds = logits.argmax(dim=-1)\n            masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n            accuracy = (masked_pred == y[:, 1:]).float().mean()\n            acc += accuracy.item()\n\n            history_loss.append(loss.item())\n            history_acc.append(accuracy.item())\n            tepoch.set_postfix(loss=loss.item(), accuracy=100\\. * accuracy.item())\n\n    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc\n\ndef evaluate(model, loader, loss_fn):\n    model.eval()\n    losses = 0\n    acc = 0\n    history_loss = []\n    history_acc = [] \n\n    for x, y in tqdm(loader, position=0, leave=True):\n\n        logits = model(x, y[:, :-1])\n        loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))\n        losses += loss.item()\n\n        preds = logits.argmax(dim=-1)\n        masked_pred = preds * (y[:, 1:]!=PAD_IDX)\n        accuracy = (masked_pred == y[:, 1:]).float().mean()\n        acc += accuracy.item()\n\n        history_loss.append(loss.item())\n        history_acc.append(accuracy.item())\n\n    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc\n```", "```py\nimport torch\nimport time\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\ndef collate_fn(batch):\n    \"\"\" \n    This function pads inputs with PAD_IDX to have batches of equal length\n    \"\"\"\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(src_sample)\n        tgt_batch.append(tgt_sample)\n\n    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n    return src_batch, tgt_batch\n\n# Model hyperparameters\nargs = {\n    'vocab_size': 128,\n    'model_dim': 128,\n    'dropout': 0.1,\n    'n_encoder_layers': 1,\n    'n_decoder_layers': 1,\n    'n_heads': 4\n}\n\n# Define model here\nmodel = Transformer(**args)\n\n# Instantiate datasets\ntrain_iter = ReverseDataset(50000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)\neval_iter = ReverseDataset(10000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)\ndataloader_train = DataLoader(train_iter, batch_size=256, collate_fn=collate_fn)\ndataloader_val = DataLoader(eval_iter, batch_size=256, collate_fn=collate_fn)\n\n# During debugging, we ensure sources and targets are indeed reversed\n# s, t = next(iter(dataloader_train))\n# print(s[:4, ...])\n# print(t[:4, ...])\n# print(s.size())\n\n# Initialize model parameters\nfor p in model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\n# Define loss function : we ignore logits which are padding tokens\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n\n# Save history to dictionnary\nhistory = {\n    'train_loss': [],\n    'eval_loss': [],\n    'train_acc': [],\n    'eval_acc': []\n}\n\n# Main loop\nfor epoch in range(1, 4):\n    start_time = time.time()\n    train_loss, train_acc, hist_loss, hist_acc = train(model, optimizer, dataloader_train, loss_fn, epoch)\n    history['train_loss'] += hist_loss\n    history['train_acc'] += hist_acc\n    end_time = time.time()\n    val_loss, val_acc, hist_loss, hist_acc = evaluate(model, dataloader_val, loss_fn)\n    history['eval_loss'] += hist_loss\n    history['eval_acc'] += hist_acc\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, Val acc: {val_acc:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n```", "```py\nfig = plt.figure(figsize=(10., 10.))\nimages = model.decoder.decoder_blocks[0].cross_attention.attention_weigths[0,...].detach().numpy()\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                nrows_ncols=(2, 2),  # creates 2x2 grid of axes\n                axes_pad=0.1,  # pad between axes in inch.\n                )\n\nfor ax, im in zip(grid, images):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n```", "```py\nclass Translator(nn.Module):\n    def __init__(self, transformer):\n        super(Translator, self).__init__()\n        self.transformer = transformer\n\n    @staticmethod\n    def str_to_tokens(s):\n        return [ord(z)-97+3 for z in s]\n\n    @staticmethod\n    def tokens_to_str(tokens):\n        return \"\".join([chr(x+94) for x in tokens])\n\n    def __call__(self, sentence, max_length=None, pad=False):\n\n        x = torch.tensor(self.str_to_tokens(sentence))\n        x = torch.cat([torch.tensor([SOS_IDX]), x, torch.tensor([EOS_IDX])]).unsqueeze(0)\n\n        encoder_output, mask = self.transformer.encode(x) # (B, S, E)\n\n        if not max_length:\n            max_length = x.size(1)\n\n        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * SOS_IDX\n\n        for step in range(1, max_length):\n            y = outputs[:, :step]\n            probs = self.transformer.decode(y, encoder_output)\n            output = torch.argmax(probs, dim=-1)\n            print(f\"Knowing {y} we output {output[:, -1]}\")\n            if output[:, -1].detach().numpy() in (EOS_IDX, SOS_IDX):\n                break\n            outputs[:, step] = output[:, -1]\n\n        return self.tokens_to_str(outputs[0])\n\ntranslator = Translator(model)\n```", "```py\nfig = plt.figure()\nimages = model.decoder.decoder_blocks[0].cross_attention.attention_weigths[0,...].detach().numpy().mean(axis=0)\n\nfig, ax = plt.subplots(1,1, figsize=(10., 10.))\n# Iterating over the grid returs the Axes.\nax.set_yticks(range(len(out)))\nax.set_xticks(range(len(sentence)))\n\nax.xaxis.set_label_position('top') \n\nax.set_xticklabels(iter(sentence))\nax.set_yticklabels([f\"step {i}\" for i in range(len(out))])\nax.imshow(images)\n```"]