["```py\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n```", "```py\nimport pandas as pd\n\n# Load data\ndata = pd.read_excel(\"Boston_Housing.xlsx\")\n```", "```py\ndata.shape\n# (506, 14)\n```", "```py\n# Split up predictors and target\ny = data['MEDV']\nX = data.drop(columns=['MEDV'])\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Distribution of predictors and relationship with target\nfor col in X.columns:\n    fig, ax = plt.subplots(1, 2, figsize=(6,2))\n    ax[0].hist(X[col])\n    ax[1].scatter(X[col], y)\n    fig.suptitle(col)\n    plt.show()\n```", "```py\ndef compute_cost(X, y, w, b): \n    m = X.shape[0] \n\n    f_wb = np.dot(X, w) + b\n    cost = np.sum(np.power(f_wb - y, 2))\n\n    total_cost = 1 / (2 * m) * cost\n\n    return total_cost\n```", "```py\ndef compute_gradient(X, y, w, b):\n    m, n = X.shape\n    dj_dw = np.zeros((n,))\n    dj_db = 0.\n\n    err = (np.dot(X, w) + b) - y\n    dj_dw = np.dot(X.T, err)    # dimension: (n,m)*(m,1)=(n,1)\n    dj_db = np.sum(err)\n\n    dj_dw = dj_dw / m\n    dj_db = dj_db / m\n\n    return dj_db, dj_dw\n```", "```py\ndef gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n    J_history = []\n    w = copy.deepcopy(w_in)\n    b = b_in\n\n    for i in range(num_iters):\n        dj_db, dj_dw = gradient_function(X, y, w, b)\n\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        cost = cost_function(X, y, w, b)\n        J_history.append(cost)\n\n        if i % math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n\n    return w, b, J_history\n```", "```py\niterations = 1000\nalpha = 1.0e-6\n\nw_out, b_out, J_hist = gradient_descent(X_train, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, iterations)\n```", "```py\nIteration    0: Cost   169.76\nIteration  100: Cost   106.96\nIteration  200: Cost   101.11\nIteration  300: Cost    95.90\nIteration  400: Cost    91.26\nIteration  500: Cost    87.12\nIteration  600: Cost    83.44\nIteration  700: Cost    80.15\nIteration  800: Cost    77.21\nIteration  900: Cost    74.58\n```", "```py\ndef plot_cost(data, cost_type):\n    plt.figure(figsize=(4,2))\n    plt.plot(data)\n    plt.xlabel(\"Iteration Step\")\n    plt.ylabel(cost_type)\n    plt.title(\"Cost vs. Iteration\")\n    plt.show() \n```", "```py\ndef predict(X, w, b):\n    p = np.dot(X, w) + b\n    return p\n```", "```py\ny_pred = predict(X_test, w_out, b_out)\n```", "```py\ndef compute_mse(y1, y2):\n    return np.mean(np.power((y1 - y2),2))\n```", "```py\nmse = compute_mse(y_test, y_pred)\nprint(mse)\n```", "```py\n132.83636802687786\n```", "```py\ndef plot_pred_actual(y_actual, y_pred):\n    x_ul = int(math.ceil(max(y_actual.max(), y_pred.max()) / 10.0)) * 10\n    y_ul = x_ul\n\n    plt.figure(figsize=(4,4))\n    plt.scatter(y_actual, y_pred)\n    plt.xlim(0, x_ul)\n    plt.ylim(0, y_ul)\n    plt.xlabel(\"Actual values\")\n    plt.ylabel(\"Predicted values\")\n    plt.title(\"Predicted vs Actual values\")\n    plt.show()\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\nX_train_norm = standard_scaler.fit_transform(X_train)\nX_test_norm = standard_scaler.transform(X_test)\n```", "```py\niterations = 1000\nalpha = 1.0e-2\n\nw_out, b_out, J_hist = gradient_descent(X_train_norm, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, iterations)\n\nprint(f\"Training result: w = {w_out}, b = {b_out}\")\nprint(f\"Training MSE = {J_hist[-1]}\")\n```", "```py\nTraining result: w = [-0.87200786  0.83235112 -0.35656148  0.70462672 -1.44874782  2.69272839\n -0.12111304 -2.55104665  0.89855827 -0.93374049 -2.151963   -3.7142413 ], b = 22.61090500500162\nTraining MSE = 9.95513733581214\n```", "```py\nmse = compute_mse(y_test, y_pred)\nprint(f\"Test MSE = {mse}\")\n```", "```py\nTest MSE = 35.66317674147827\n```", "```py\ndef compute_cost_ridge(X, y, w, b, lambda_ = 1): \n    m = X.shape[0] \n\n    f_wb = np.dot(X, w) + b\n    cost = np.sum(np.power(f_wb - y, 2))    \n\n    reg_cost = np.sum(np.power(w, 2))\n\n    total_cost = 1 / (2 * m) * cost + (lambda_ / (2 * m)) * reg_cost\n\n    return total_cost\n```", "```py\ndef compute_gradient_ridge(X, y, w, b, lambda_):\n    m = X.shape[0]\n\n    err = np.dot(X, w) + b - y\n    dj_dw = np.dot(X.T, err) / m + (lambda_ / m) * w\n    dj_db = np.sum(err) / m\n\n    return dj_db, dj_dw\n```", "```py\ndef gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, lambda_=0.7, num_iters=1000):\n    J_history = []\n    w = copy.deepcopy(w_in)\n    b = b_in\n\n    for i in range(num_iters):\n        dj_db, dj_dw = gradient_function(X, y, w, b, lambda_)\n\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        cost = cost_function(X, y, w, b, lambda_)\n        J_history.append(cost)\n\n        if i % math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n\n    return w, b, J_history\n```", "```py\niterations = 1000\nalpha = 1.0e-2\nlambda_ = 1\n\nw_out, b_out, J_hist = gradient_descent(X_train_norm, y_train, w_init, b_init, compute_cost_ridge, compute_gradient_ridge, alpha, lambda_, iterations)\n```", "```py\nprint(f\"Training result: w = {w_out}, b = {b_out}\")\nprint(f\"Training MSE = {J_hist[-1]}\")\n```", "```py\nTraining result: w = [-0.86996629  0.82769399 -0.35944104  0.7051097  -1.43568137  2.69434668\n -0.12306667 -2.53197524  0.88587909 -0.92817437 -2.14746836 -3.70146378], b = 22.61090500500162\nTraining MSE = 10.005991756561285\n```", "```py\ndef soft_threshold(rho, lamda_):\n    if rho < - lamda_:\n        return (rho + lamda_)\n    elif rho >  lamda_:\n        return (rho - lamda_)\n    else: \n        return 0\n```", "```py\ndef compute_residuals(X, y, w, b):\n    return y - (np.dot(X, w) + b)\n```", "```py\ndef compute_rho_j(X, y, w, b, j):\n    X_k = np.delete(X, j, axis=1)    # remove the jth element\n    w_k = np.delete(w, j)    # remove the jth element\n\n    err = compute_residuals(X_k, y, w_k, b)\n\n    X_j = X[:,j]\n    rho_j = np.dot(X_j, err)\n\n    return rho_j\n```", "```py\ndef coordinate_descent_lasso(X, y, w_in, b_in, cost_function, lambda_, num_iters=1000, tolerance=1e-4):\n    J_history = []\n    w = copy.deepcopy(w_in)\n    b = b_in\n    n = X.shape[1]\n\n    for i in range(num_iters):\n        # Update weights\n        for j in range(n):\n            X_j = X[:,j]\n            rho_j = compute_rho_j(X, y, w, b, j)\n            w[j] = soft_threshold(rho_j, lambda_) / np.sum(X_j ** 2)\n\n        # Update bias\n        b = np.mean(y - np.dot(X, w))\n        err = compute_residuals(X, y, w, b)\n\n        # Calculate total cost\n        cost = cost_function(X, y, w, b, lambda_)\n        J_history.append(cost)\n\n        if i % math.ceil(num_iters/10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n\n        # Check convergence\n        if np.max(np.abs(err)) < tolerance:\n            break\n\n    return w, b, J_history\n```", "```py\niterations = 1000\nlambda_ = 1e-4\ntolerance = 1e-4\n\nw_out, b_out, J_hist = coordinate_descent_lasso(X_train_norm, y_train, w_init, b_init, compute_cost_lasso, lambda_, iterations, tolerance)\n```", "```py\nprint(f\"Training result: w = {w_out}, b = {b_out}\")\n```", "```py\nTraining result: w = [-0.86643384  0.82700157 -0.35437324  0.70320366 -1.44112303  2.69451013\n -0.11649385 -2.53543865  0.88170899 -0.92308699 -2.15014264 -3.71479811], b = 22.61090500500162\n```", "```py\nprint(standard_scaler.scale_)\n```", "```py\n[8.12786482e+00 2.36076347e+01 6.98435113e+00 2.53975353e-01\n 1.15057872e-01 6.93831576e-01 2.80721481e+01 2.07800639e+00\n 8.65042138e+00 1.70645434e+02 2.19210336e+00 7.28999160e+00]\n```"]