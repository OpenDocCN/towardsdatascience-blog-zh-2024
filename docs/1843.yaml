- en: A Simple Regularization for Your GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的GAN正则化方法
- en: 原文：[https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29](https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29](https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29)
- en: How to capture data distributions effectively with GANs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用GANs有效捕捉数据分布
- en: '[](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)[![Shashank
    Sharma](../Images/61adaaf21b57375419eaa5341c322368.png)](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)
    [Shashank Sharma](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)[![Shashank
    Sharma](../Images/61adaaf21b57375419eaa5341c322368.png)](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)
    [Shashank Sharma](https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)
    ·14 min read·Jul 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------)
    ·14分钟阅读·2024年7月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In 2018, I had the privilege of orally presenting my paper at the AAAI conference.
    A common feedback was that the insights were clearer in the presentation than
    in the paper. Although some time has passed since then, I believe there’s still
    value in sharing the core insights and intuitions.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，我有幸在AAAI会议上做口头报告。一个常见的反馈是，报告中的见解比论文中更加清晰。尽管从那时起已经过去了一段时间，我认为分享这些核心见解和直觉仍然很有价值。
- en: The paper addressed a significant problem of reliably capturing modes in a dataset
    with Generative Adversarial Networks (GANs). This article is formulated around
    my intuitions of GANs and derives the proposed approach from those intuitions.
    Finally, I present a copy-paste solution for those who want to try it out. If
    you are familiar with GANs, feel free to skip to the next section.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文解决了使用生成对抗网络（GANs）可靠捕捉数据集中的模式的一个重要问题。本文是围绕我对GANs的直觉进行阐述的，并从这些直觉中推导出提出的方法。最后，我为那些想尝试的人提供了一个现成的解决方案。如果你已经熟悉GANs，可以直接跳到下一节。
- en: 'Paper: [Sharma, S. and Namboodiri, V., 2018, April. No modes left behind: Capturing
    the data distribution effectively using gans. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*]([paper](https://arxiv.org/abs/1802.00771), [github](https://github.com/shashank879/logan))'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 论文：[Sharma, S. 和 Namboodiri, V., 2018年4月. 不遗漏任何模式：使用GANs有效捕捉数据分布。在*人工智能学会会议论文集*]([paper](https://arxiv.org/abs/1802.00771),
    [github](https://github.com/shashank879/logan))
- en: '***A quick intro to Generative Adversarial Networks***'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***生成对抗网络简介***'
- en: GANs are used to learn Generators for a given distribution. This means that
    if we are given a dataset of images, say of birds, we have to learn a function
    that generates images that look like birds. The Generator function is usually
    deterministic, so it relies on a random number as input for stochasticity to produce
    a variety of images. Thus, the function takes a *n*-dimensional number as input
    and outputs an image. The input number *z* is typically, low-dimensional and randomly
    sampled from a uniform or a normal distribution. This distribution is called the
    latent distribution *Pz*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GANs用于学习给定分布的生成器。这意味着，如果我们得到一组鸟类图像的数据集，我们需要学习一个函数来生成看起来像鸟的图像。生成器函数通常是确定性的，因此它依赖于一个随机数作为输入，通过随机性生成各种图像。因此，该函数接受一个*n*维的数字作为输入，并输出一张图像。输入数字*z*通常是低维的，并从均匀分布或正态分布中随机抽取。这个分布被称为潜在分布*Pz*。
- en: We refer to the space of “all possible” images as the data space *X,* the set
    of bird images as real *R*, and their distribution as *Pr*. The Generator at optimality,
    maps each value of *z* to some image that has a high likelihood of belonging to
    *R*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将“所有可能”图像的空间称为数据空间 *X*，将鸟类图像的集合称为真实 *R*，其分布为 *Pr*。在最优状态下，生成器将每个 *z* 的值映射到某个高概率属于
    *R* 的图像。
- en: 'GANs solve this problem using two learned functions: a Generator (*G)* and
    a Discriminator (*D)*. *G* takes the number *z* as input to produce a sample from
    data space, *x = G(z)*. At any point, we call the set of all images generated
    by *G* as fake *F*, and their distribution *Pg*. The Discriminator takes a sample
    *x* from the data space and outputs a scalar *D(x),* predicting its probability
    of belonging to the real or fake distribution.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 通过两个学习的函数来解决这个问题：生成器（*G*）和鉴别器（*D*）。*G* 以数字 *z* 为输入，从数据空间中生成一个样本，*x = G(z)*。在任何时候，我们将
    *G* 生成的所有图像集合称为假样本 *F*，它们的分布为 *Pg*。鉴别器从数据空间中取样一个样本 *x*，并输出一个标量 *D(x)*，预测它属于真实分布还是假分布的概率。
- en: Initially, neither *G* nor *D* is well-trained. We sample some random numbers
    at each training step and pass them through *G* to get some fake samples. Similarly,
    we take an equal number of random samples from the real subset. *D* is trained
    to output 0 for fake, and 1 for real samples via cross-entropic loss. *G* is trained
    to fool *D* such that the output of *D(G(z))* becomes 1\. In other words, increase
    the probability of generating samples that score high (produce more), and decrease
    it for those that score low. The gradients flow from the loss function through
    *D* and then through *G*. Please refer to the original GAN paper for the loss
    equations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，*G* 和 *D* 都没有经过充分的训练。在每个训练步骤中，我们随机抽取一些数字并将其传递给 *G*，以获得一些假样本。同样，我们从真实子集里取出相等数量的随机样本。*D*
    被训练通过交叉熵损失输出 0 表示假样本，输出 1 表示真实样本。*G* 被训练来欺骗 *D*，使得 *D(G(z))* 的输出变为 1。换句话说，增加生成高评分样本的概率（生成更多样本），减少低评分样本的概率。梯度从损失函数流经
    *D*，然后流经 *G*。有关损失方程，请参阅原始的 GAN 论文。
- en: '![](../Images/09f1ea274890b28f6499cb33aa5c0adc.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09f1ea274890b28f6499cb33aa5c0adc.png)'
- en: '[Fig 1.] *Image taken from the presentation “Generative Adversarial Networks”
    at NIPS Workshop on Perturbation, Optimization, and Statistics, Montreal, 2014\.
    [Note: We refer to Pd as Pr in this article].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.] *图片取自2014年蒙特利尔NIPS研讨会《生成对抗网络》上的演示文稿《Generative Adversarial Networks》。[注：在本文中，我们将
    Pd 称为 Pr]*'
- en: The above figure illustrates how a GAN learns for a 1-dimensional space *X*.
    The black dotted line represents the real distribution, which we refer to as *Pr*.
    The green line represents the fake samples' distribution *Pg*. The blue dotted
    line represents the Discriminator output *D(x)* for a data sample. In the beginning,
    neither *D* nor *G* performs correctly. First, *D* is updated to correctly classify
    real and fake samples. Next, *G* is updated to follow the local gradients of the
    Discriminator values for the generated samples *D(G(z))*, making *Pg* come closer
    to *Pr*. In other words, *G* slightly improves each sample based on *D*’s feedback.
    The last illustration shows the final equilibrium state.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了 GAN 如何在一维空间 *X* 中进行学习。黑色虚线代表真实分布，我们称之为 *Pr*。绿色线条代表假样本的分布 *Pg*。蓝色虚线代表鉴别器输出
    *D(x)* 对数据样本的判断。开始时，*D* 和 *G* 都没有正确地执行任务。首先，*D* 被更新，以正确分类真实样本和假样本。接着，*G* 被更新，以跟随鉴别器值对生成样本
    *D(G(z))* 的局部梯度，使得 *Pg* 更接近 *Pr*。换句话说，*G* 根据 *D* 的反馈轻微改善每一个样本。最后的插图展示了最终的平衡状态。
- en: This can be thought of as a frequentist approach. If *G* produces more samples
    from a mode than what occurs in *Pr*, even though the sample might look flawless,
    *D* begins to classify them as fake, discouraging *G* from generating such samples.
    Conversely, when *G* produces fewer samples, *D* begins to classify them a real,
    encouraging *G* to generate more of them. This continues till the frequency of
    generation of an element matches the frequency of its occurrence in *Pr.* Or,
    the element is equally likely in *Pg* and *Pr*. When the distributions exactly
    match, *D* outputs 0.5 at all points, indicating it cannot distinguish between
    real and fake samples. Then the loss reaches a minimum, and neither *G* nor *D*
    can improve further; this state is called the Nash equilibrium.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被看作是一种频率主义的方法。如果 *G* 从一个模式生成的样本超过了 *Pr* 中该模式的出现频率，尽管这些样本可能看起来无瑕疵，*D* 会开始将它们分类为假样本，从而阻止
    *G* 生成这样的样本。相反，当 *G* 生成的样本较少时，*D* 开始将它们分类为真实样本，鼓励 *G* 生成更多此类样本。这个过程一直持续，直到某个元素的生成频率与其在
    *Pr* 中的出现频率匹配。或者，该元素在 *Pg* 和 *Pr* 中的出现概率相等。当分布完全匹配时，*D* 在所有点的输出为 0.5，表示它无法区分真实和假样本。此时损失达到最小值，*G*
    和 *D* 都无法进一步改进；这种状态被称为纳什均衡。
- en: Later Wasserstein GANs modified this objective a bit. *D* is trained to increase
    for real samples and decrease for fake unboundedly. They refer to it as a Critic.
    Rather than computing a frequency-based loss, they modified *G*'s objective to
    move *Pg* in the direction that improves *D(G(z))* directly. Please refer to the
    original paper for the equilibrium guarantee and other details of the method.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的 Wasserstein GAN 对这个目标进行了一些修改。*D* 被训练为对真实样本增加值，对假样本则无限减少。它们称之为 Critic。与其计算基于频率的损失，它们将
    *G* 的目标修改为直接调整 *Pg* 的方向，以提升 *D(G(z))* 的值。有关平衡保证和该方法其他细节，请参阅原始论文。
- en: In my experience with GANs, I’ve found it more productive to view them not as
    competition between *G* and *D*, but a cooperative interaction. The Discriminator's
    objective is to establish a gradient of ‘realness’ between *Pg* and *Pr,* like
    a soft boundary. *G* then uses this feedback to move *Pg* closer to *Pr*. The
    smoother this boundary, the easier it is for *G* to improve. Viewing the GAN setup
    as competitive is disadvantageous because the loss of either networks, *D* or
    *G*, means failure of the final objective. However, the perspective of a joint
    objective aligns directly with the desired behavior.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我在 GAN 方面的经验，我发现将 GAN 看作是 *G* 和 *D* 之间的合作互动，而非竞争，能带来更好的效果。鉴别器的目标是建立 *Pg* 和
    *Pr* 之间的“真实性”梯度，就像一个软边界。然后，*G* 使用这个反馈将 *Pg* 移向 *Pr*。这个边界越平滑，*G* 改进的难度就越小。将 GAN
    设定为竞争模式是不利的，因为无论是 *D* 还是 *G* 网络的损失，都意味着最终目标的失败。然而，联合目标的视角直接与所期望的行为对齐。
- en: '***The problem of mode loss***'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***模式丢失问题***'
- en: A frequently occurring problem in GANs is the losing of minor modes by the Generator.
    *G* can receive feedback by *D* only for the samples it generates. If *G* misses
    a mode because it initially aims for the larger modes, it never improves for it.
    *G* only improves at a mode as long as it produces samples ‘nearby’ that mode.
    Technically speaking, the Generator follows the local gradients from the Discriminator
    to shift the modes in *Pg* to match those of *Pr*. Once *G* loses the local gradients
    to a minor mode, it never faces a penalty for not generating samples from that
    mode. It is a problem with real-world datasets, which are usually sparse, and
    many minor modes occur.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 中，一个常见的问题是生成器丢失了次要模式。*G* 只能通过 *D* 对其生成的样本获得反馈。如果 *G* 因为最初专注于较大的模式而错过了一个模式，它就无法对此进行改进。*G*
    只有在生成接近该模式的样本时，才会在该模式上进行改进。技术上讲，生成器跟随鉴别器的局部梯度，将 *Pg* 的模式移向与 *Pr* 相匹配的方向。一旦 *G*
    失去了次要模式的局部梯度，它就永远不会因没有从该模式生成样本而受到惩罚。这个问题在现实世界的数据集中尤为突出，因为这些数据集通常是稀疏的，并且包含许多次要模式。
- en: '![](../Images/96b908831c603de92228004f47cdcf9d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/96b908831c603de92228004f47cdcf9d.png)'
- en: '[Fig 2.] In the illustration, the numbers indicate the D(x) contours’ values,
    and the dashed boundary indicates the fake distribution F. The arrows indicate
    the gradients (orthogonal to the contours) experienced by G. There are two modes,
    major M1 and minor M2\. Although, the Discriminator has marked M2 as real; Still,
    since the Generator distribution does not receive the gradients that lead to M2,
    it is missed.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.] 在插图中，数字表示 D(x) 等高线的值，虚线边界表示假分布 F。箭头表示 *G* 所经历的梯度（与等高线垂直）。有两个模式，主模式 M1
    和次要模式 M2。尽管鉴别器已将 M2 标记为真实，但由于生成器分布没有接收到导致 M2 的梯度，因此它错过了该模式。'
- en: 'This can be seen in the differential equation that is used to compute the gradients.
    Given the loss function, gradients for learning *G* are computed as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以从用于计算梯度的微分方程中看出。给定损失函数，学习*G*的梯度计算如下：
- en: '![](../Images/e81588ac922291fdf7073b6074b1658d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e81588ac922291fdf7073b6074b1658d.png)'
- en: The middle term relies on seeing an improvement in *D(G(z))* wrt the data sample
    *G(z)* for the generated samples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 中间项依赖于在生成样本的数据样本 *G(z)* 上，*D(G(z))* 相对于数据样本 *G(z)* 的改进。
- en: '***Our Method***'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们的方法***'
- en: In our paper, we proposed a reliable approach to solving this problem. We test
    it with generated toy datasets and a real-world image dataset with a massive single
    mode. We also test the quality of learned representations by evaluating the CIFAR
    score and qualitative analysis using the CelebA face dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的论文中，我们提出了解决这个问题的可靠方法。我们通过生成的玩具数据集和一个具有大单峰的真实世界图像数据集进行测试。我们还通过评估 CIFAR 分数和使用
    CelebA 面部数据集的定性分析来测试学习表示的质量。
- en: The following sections explain the underlying intuitions behind our approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了我们方法背后的基本直觉。
- en: '***The inverted Generator or Encoder***'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***反向生成器或编码器***'
- en: Let’s explore the opposite problem; given a dataset of images, we need to learn
    a mapping from the image to the latent distribution. Let’s assume the latent distribution
    is a 10-dimensional Uniform[0, 1] distribution. Thus, we construct a GAN where
    G is a function that takes images as input and outputs a 10-dimensional number
    with values in the range [0, 1]. *D* takes numbers from this space and outputs
    their “realness,” which indicates how likely it is to come from the Uniform distribution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨相反的问题；给定一个图像数据集，我们需要学习从图像到潜在分布的映射。假设潜在分布是一个 10 维的均匀分布 [0, 1]。因此，我们构建一个
    GAN，其中 G 是一个将图像作为输入并输出一个 10 维的数值，数值范围在 [0, 1] 之间的函数。*D* 从这个空间中取数值并输出它们的“真实度”，表示它来自均匀分布的可能性。
- en: In this scenario, the Generator is called an Encoder *(E)*. This can be because
    it learns to compress information. But is it useful?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，生成器被称为编码器 *(E)*。这是因为它学习压缩信息。但这有用吗？
- en: We can visualize the Encoder’s task as assigning 10 floating numbers in the
    range [0, 1] to each image. This effectively places all the given images along
    a line of length 1, repeated for 10 different lines. Since we specify the Real
    distribution as Uniform, at equilibrium, the Encoder will match this distribution.
    Or, all the images will be uniformly spread along these 10 lines.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将编码器的任务视为为每个图像分配 10 个浮动数字，范围在 [0, 1] 之间。这实际上将所有给定的图像沿着长度为 1 的直线排布，并为 10
    条不同的直线重复此过程。由于我们将真实分布指定为均匀分布，因此在平衡时，编码器将与该分布匹配。或者，所有图像将均匀分布在这 10 条直线之上。
- en: Assuming *E* has a finite capacity, meaning it cannot memorize all the patterns
    in the features and it is regularized such that there is continuity in outputs
    for inputs. Meaning, that the weights are finite and outputs cannot abruptly change
    for small changes in inputs. It will cause *E* to bring images with similar features
    into meaningful groups that can help it complete the task with these constraints.
    Thus, placing semantically closer images together in the feature space. While
    the features might be entangled, they yield meaningful representations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*E*具有有限的容量，这意味着它无法记住特征中的所有模式，并且它被正则化，以确保输入输出之间的连续性。也就是说，权重是有限的，并且对于输入的微小变化，输出不能突然改变。这将导致*E*将具有相似特征的图像归为有意义的组，这将帮助它在这些约束下完成任务。因此，将语义上更接近的图像放在特征空间中一起。尽管这些特征可能是交织在一起的，但它们仍然能够产生有意义的表示。
- en: Now let’s look at the problem of mode loss from this perspective. We chose a
    Uniform distribution as *Pr*. Since it is a unimodal distribution, there is no
    weaker mode to lose. If *E* misses a region within the mode, it experiences gradients
    at the edge of *Pg* towards this region. If the Discriminator is regularized,
    its output will gradually change at the boundary of the missed region. Technically,
    *D* is differentiable wrt *X* at the boundary of this region. Then, *E* will follow
    the increasing *D* values to improve. Any region missed by *E* will eventually
    be captured. Thus, there can be no problem of mode loss in this case!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从这个角度来看模式丢失问题。我们选择均匀分布作为*Pr*。由于它是单峰分布，因此没有更弱的模式可供丢失。如果*E*遗漏了模式中的某个区域，它会在*Pg*的边缘体验到指向该区域的梯度。如果判别器被正则化，它的输出将在遗漏区域的边界逐渐发生变化。从技术上讲，*D*
    在该区域的边界对*X*是可微的。然后，*E* 将跟随增加的 *D* 值进行改进。*E*遗漏的任何区域最终都会被捕捉到。因此，在这种情况下不会出现模式丢失问题！
- en: Since the entire region is connected, the Encoder will experience corrective
    gradients for any differences between *Pg* and *Pr*. There will only be a global
    optimum, and the network won’t get stuck in a local optimum. Thus, given enough
    capacity, an Encoder can perfectly encode any data distribution to a unimodal
    distribution. We show this for a uniform distribution here via an illustration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于整个区域是连通的，编码器将在*Pg*和*Pr*之间的任何差异中体验到修正梯度。只有全局最优解，网络不会陷入局部最优解。因此，给定足够的容量，编码器可以完美地将任何数据分布编码为单峰分布。我们通过插图展示了这一点，以均匀分布为例。
- en: From here onwards, we refer to the distribution of images as *Pr*, the latent
    distribution as *Pz*. The image samples will be denoted as *x* and the latent
    samples as *z*. The Generator takes *z* as input to produce images *G(z),* and
    the Encoder takes *x* as input to yield latent representations *E(x)*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将图像的分布称为*Pr*，潜在分布称为*Pz*。图像样本将表示为*x*，潜在样本表示为*z*。生成器以*z*为输入生成图像*G(z)*，编码器以*x*为输入生成潜在表示*E(x)*。
- en: '![](../Images/d7e8a63f87f4fd90dd9029bd2ac1e2c7.png)![](../Images/b2f9d7128fd1433511dc1af3be08b2b7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7e8a63f87f4fd90dd9029bd2ac1e2c7.png)![](../Images/b2f9d7128fd1433511dc1af3be08b2b7.png)'
- en: '[Fig 3.] Mode loss in Encoder with a uniform latent distribution and with a
    distribution with disconnected modes.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3.] 编码器在均匀潜在分布与具有不连通模态的分布中的模式损失。'
- en: '***BIGAN (Combined training of Encoder & Generator)***'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***BIGAN（编码器与生成器联合训练）***'
- en: BIGAN was introduced by Donahue et al. in 2017\. It simultaneously trains a
    Generator (*G*) and an Encoder (*E*) with a shared Discriminator (*D*). While
    the Encoder and Generator operate the same as before, the Discriminator takes
    both, *x* and *z*, as input and produces a scalar output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BIGAN由Donahue等人于2017年提出。它同时训练一个生成器（*G*）和一个编码器（*E*），并共享一个判别器（*D*）。虽然编码器和生成器的操作与之前相同，但判别器将*x*和*z*作为输入并产生一个标量输出。
- en: The objective for *D* is to assign 1 to the tuples *(x, E(x))* and assign 0
    to *(G(z), z)*. Thus, it tries to establish a boundary between the distributions
    of *(x, E(x))* and *(G(z), z)*. The Generator traverses this boundary gradient
    upwards to generate more samples labeled as 1 by the Discriminator, and the Encoder
    cascades down this boundary similarly. The objective of *D* here is to help the
    distributions of *(x, E(x))* and *(G(z), z)* merge.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*D*的目标是将元组*(x, E(x))*标记为1，将*(G(z), z)*标记为0。因此，它试图在*(x, E(x))*和*(G(z), z)*的分布之间建立一个边界。生成器沿着这个边界梯度向上移动，生成更多被判别器标记为1的样本，编码器则类似地沿着这个边界向下传播。这里的*D*的目标是帮助*(x,
    E(x))*和*(G(z), z)*的分布融合。'
- en: So what is the significance of these distributions merging? This can happen
    only when the distribution of *G(z)* matches the data distribution *Pr*, and the
    distribution of *E(x)* matches the latent distribution *Pz*. Thus, each latent
    variable maps to an image, and each image is mapped to a latent variable. Another
    inherent important feature is that this mapping is reversible, ie. *G(E(x))=x*
    and *E(G(z))=z*. Please refer to the original paper for more details.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些分布融合的意义何在？只有当*G(z)*的分布与数据分布*Pr*匹配，且*E(x)*的分布与潜在分布*Pz*匹配时，这种情况才会发生。因此，每个潜在变量都映射到一张图像，每张图像也都映射到一个潜在变量。另一个固有的重要特性是，这种映射是可逆的，即*G(E(x))=x*且*E(G(z))=z*。更多细节请参考原论文。
- en: Let’s visualize what it looks like — the Discriminator functions in the joint
    space of *x* and *z*. The illustration below shows the starting and equilibrium
    states of *G* and *E,* for a *1-*dimensional *X* and a *1-*dimensional *Z*. *Pz*
    is a uniform distribution and *Pr* is a sparse distribution with 5 point modes.
    Consequently, modes of *Pr* (*{x1, x2, x3, x4, x5}*)appear as ‘spots,’ while the
    latent variable's distribution appears continuous. The green points represent
    the *(G(z), z)* tuples and the yellow points represent the *(x, E(x))* tuples.
    Modifying *E* moves the yellow spots along the *Z-axis,* and modifying *G* moves
    the green points along the *X-axis*. Thus, for the distributions to match, *E*
    has to spread the yellow points along the *Z-axis* to approximate a uniform distribution.
    And, *G* must move the green points horizontally to resemble the distribution
    of given data, *Pr*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来可视化一下它是什么样子——判别器在*x*和*z*的联合空间中工作。下面的插图展示了*G*和*E*的起始状态和均衡状态，对于*1*维的*X*和*1*维的*Z*。*Pz*是均匀分布，*Pr*是一个具有5个点模式的稀疏分布。因此，*Pr*的模式（*{x1,
    x2, x3, x4, x5}*）表现为“点”，而潜在变量的分布表现为连续分布。绿色点表示*(G(z), z)*元组，黄色点表示*(x, E(x))*元组。修改*E*会沿着*Z轴*移动黄色点，而修改*G*会沿着*X轴*移动绿色点。因此，为了使分布匹配，*E*必须沿着*Z轴*扩展黄色点，以逼近均匀分布。而*G*必须水平地移动绿色点，以使其类似于给定数据的分布*Pr*。
- en: '![](../Images/f8d3be67ac9edfb91b3b26f37f1dc57d.png)![](../Images/12cfbc3c95830043bbc11d7f77053aec.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d3be67ac9edfb91b3b26f37f1dc57d.png)![](../Images/12cfbc3c95830043bbc11d7f77053aec.png)'
- en: '[Fig 4.] In the beginning, G maps all values of z to a random x, and E maps
    all values of x, {x1,x2,x3,x4,x5}, to a random z. At equilibrium, yellow points
    are spread uniformly along the Z-axis. And the green points align against the
    possible modes in Pr. The real samples (x, E(x)) are shown ‘concentrated’ because
    the Encoder''s limited capacity cannot spread the point mode. The generations
    (G(z), z) are shown ‘stringy’ because z is sampled from a continuous distribution.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4.] 一开始，G将所有的z值映射到一个随机的x，而E将所有的x值{ x1,x2,x3,x4,x5 }映射到一个随机的z。在均衡状态下，黄色点沿着Z轴均匀分布。绿色点则对齐到*Pr*中的可能模式。真实样本*(x,
    E(x))*显示为“集中”状态，因为编码器的有限容量无法扩展点模式。生成的样本*(G(z), z)*显示为“线状”，因为z是从连续分布中采样的。'
- en: '![](../Images/98ba74ff29c38d71ed9962bae75e7730.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98ba74ff29c38d71ed9962bae75e7730.png)'
- en: '[Fig 5.] Notice that had the Generator and Encoder been trained separately
    using separate Discriminators, this would also have been a valid configuration.
    This meets the criterion of matching the distributions but does not allow the
    invertibility of G and E. This is NOT the objective of BIGAN.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5.] 请注意，如果生成器和编码器分别使用不同的判别器单独训练，这也是一个有效的配置。这符合匹配分布的标准，但不允许G和E的可逆性。这不是BIGAN的目标。'
- en: '![](../Images/e686ca5f075b04376a2279d9ddd19fb8.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e686ca5f075b04376a2279d9ddd19fb8.png)'
- en: '[Fig 6.] If the data modes are not points but slightly spread, the Encoder
    can spread them with limited capacity against a uniform distribution. The Generator
    with limited capacity is continuous; thus, there are some values of z for which
    G can output intermodal values of the data.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6.] 如果数据模式不是点而是略微分散的，编码器可以在有限容量下将其扩展为均匀分布。具有有限容量的生成器是连续的；因此，对于某些z值，G可以输出数据的跨模式值。'
- en: It’s important to note that *G* and *E* do not directly interact with each other,
    but only via *D*. As a result, their objectives or loss functions are independent
    of the other's performance. For example, the Encoder's objective is to make the
    distribution of *E(x)* match *Pz* regardless of how *G* is performing. This is
    because in matching the tuples *(x, E(x))* with *(G(z), z)*, the Encoder has control
    over *E(x)* only, and *E(x)* has to match *Pz* regardless of *G(z)* matching *Pr*.
    The same argument goes for the Generator. Thus, the Encoder will still perform
    perfectly for a unimodal distribution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，*G*和*E*并不直接相互作用，而是通过*D*进行交互。因此，它们的目标或损失函数是相互独立的。例如，编码器的目标是使*E(x)*的分布匹配*Pz*，而不考虑*G*的表现如何。这是因为在将元组*(x,
    E(x))*与*(G(z), z)*匹配时，编码器只能控制*E(x)*，而*E(x)*必须与*Pz*匹配，无论*G(z)*是否匹配*Pr*。对生成器来说也是同样的道理。因此，编码器对于单峰分布仍然能够完美地执行。
- en: '***What does the problem of mode loss look like in BIGANs?***'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***在BIGAN中，模式丧失的问题是什么样的？***'
- en: If the Generator loses the gradients to the weaker modes, they can still be
    lost, even if they are well Encoded.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生成器失去对较弱模式的梯度，它们仍然可能会丢失，即使这些梯度已经被很好地编码。
- en: '![](../Images/00a301c7c01841d53ede934e64059353.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00a301c7c01841d53ede934e64059353.png)'
- en: '[Fig 7.] A collapsed Generator that outputs x3 for all values of z.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7.] 一个塌陷的生成器，对于所有z的值输出x3。'
- en: In the illustration above, *G* has collapsed to the mode *x3*. *G* experiences
    the gradients along the *X-axis* to the nearby modes *x2* and *x4,* shown with
    blue arrows. However, the distant modes *x1* and *x5* may get neglected and left
    behind.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的插图中，*G*已经塌陷到模式*x3*。*G*经历了沿*X轴*指向邻近模式*x2*和*x4*的梯度，蓝色箭头表示。但远离的模式*x1*和*x5*可能会被忽视并被落下。
- en: '***Finally, our solution!***'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***最后，我们的解决方案！***'
- en: An idea was proposed to stabilize Wasserstein GANs by Gulrajani et al. in the
    paper ‘Improved Training of Wasserstein GANs’. Since the Discriminator in WGANs
    is unbounded, the loss can spike if it is not regularized. This can be seen in
    the loss equation via expansion using the chain rule again.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Gulrajani 等人在论文《改进的 Wasserstein GAN 训练》中提出了通过一种方法来稳定 Wasserstein GAN。由于 WGAN
    中的鉴别器是无界的，如果没有正则化，损失可能会突然增加。这可以通过链式法则扩展后的损失方程中看到。
- en: '![](../Images/2487b0cedc533a62c228076c750bd49e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2487b0cedc533a62c228076c750bd49e.png)'
- en: Here the term *∂D/∂G* should always be finite or, *D(x)* should be differentiable
    everywhere wrt *x*. The original method placed a bound on the weights to achieve
    this. However, Gulrajani et al. suggested placing a penalty on the gradients directly
    via an additional loss for the Discriminator. For this, points were randomly sampled
    between the real and generated samples from the current batch. And the magnitude
    of the gradients, *∂D/∂x*, at those points was forced to be 1 via a mean squared
    loss.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，术语*∂D/∂G*应始终是有限的，或者说*D(x)*应在相对于*x*的每个位置都是可微的。原始方法通过对权重施加边界来实现这一点。然而，Gulrajani
    等人建议通过额外的损失直接对梯度施加惩罚。为此，在当前批次中，点被随机采样于真实样本和生成样本之间。并且，在这些点处，梯度*∂D/∂x*的幅度被强制为1，通过均方误差来实现。
- en: The message to take away was that modeling the Discriminator landscape directly
    is also a viable solution. Inspired by the technique to directly model the landscape
    of the Discriminator, we can use something similar. Let’s have a look at *Fig
    7* again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要传达的信息是，直接建模鉴别器的景观也是一种可行的解决方案。受到直接建模鉴别器景观技术的启发，我们可以使用类似的方法。让我们再看一眼*图 7*。
- en: '![](../Images/359ea2cd16720070cbc966ec91005ecb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/359ea2cd16720070cbc966ec91005ecb.png)'
- en: '[Fig 8.] Here the generated points {g1, g2, g3, g4, g5} were supposed to reach
    the marked modes but failed because of missing gradients.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.] 在这里，生成的点 {g1, g2, g3, g4, g5} 应该到达标记的模式，但由于缺少梯度，它们未能成功到达。'
- en: The points *{g1, g2, g3, g4, g5}* are the generations *G(z)* for the encodings
    *E(x)* of the data points in *{x1, x2, x3, x4, x5}* respectively or, *gi = G(E(xi))*.
    These are the reconstructions of the points *xi*. We need to model gradients *∂D/∂x*
    such that the points *gi* start moving towards their respective target points
    *xi*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 点*{g1, g2, g3, g4, g5}*是数据点*{x1, x2, x3, x4, x5}*的编码*E(x)*的生成*G(z)*，或者*gi =
    G(E(xi))*。这些是点*xi*的重建。我们需要建模梯度*∂D/∂x*，使得点*gi*开始向它们各自的目标点*xi*移动。
- en: To do this, we sample some points uniformly along the line segments connecting
    *xi* to their reconstructions *gi*. We then force the gradients *∂D/∂x* at all
    those points to be unity and directed towards *xi* via a mean squared error. We
    call this pair-wise gradient penalty, and it is added as an additional loss for
    the Discriminator.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们沿着连接*xi*与其重建*gi*的线段均匀地采样一些点。然后我们强制所有这些点的梯度*∂D/∂x*为单位值，并且指向*xi*，通过均方误差来实现。我们称之为成对梯度惩罚，并将其作为额外的损失加到鉴别器中。
- en: '![](../Images/4cf992bc3ad4058976fa95771299007d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cf992bc3ad4058976fa95771299007d.png)'
- en: 'The first term in the loss is the unit vector pointing in the right direction,
    and the second term is the gradient of the discriminator wrt x at the sampled
    point. [Note: gi are referred to as x-hat here.]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 损失中的第一项是指向正确方向的单位向量，第二项是鉴别器在采样点处相对于 x 的梯度。[注：此处 gi 被称为 x-hat。]
- en: One might consider using the mean squared error between *xi* and its reconstruction
    *gi* as an additional loss term for the Generator, aiming for a similar effect.
    However, we found it difficult to balance the reconstruction loss with the adversarial
    loss for the Generator. This is because the adversarial and reconstruction losses
    are completely different in behavior and scale, making it difficult to find a
    constant weight that balances them effectively across datasets, toy and real.
    In contrast, the gradient penalty does not constrain *D(x)* directly but only
    *∂D/∂x*; thus, it is not a directly competing objective for the adversarial loss
    and only has a regularizing effect. We found a single constant (**λ**=1) to work
    in all cases.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有人会考虑将*xi*与其重建*gi*之间的均方误差作为生成器的附加损失项，以期达到类似的效果。然而，我们发现很难平衡生成器的重建损失和对抗损失。这是因为对抗损失和重建损失在行为和规模上完全不同，难以找到一个常数权重来有效地平衡它们，适用于玩具数据集和真实数据集。相比之下，梯度惩罚并不直接约束*D(x)*，而是约束*∂D/∂x*；因此，它不是对抗损失的直接竞争目标，而仅起到正则化作用。我们发现一个常数（**λ**=1）在所有情况下都有效。
- en: '***Does it work?***'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '***它有效吗？***'
- en: We train simple networks like DCGAN and MLPs with different losses. We use toy
    datasets to visualize the solution better and use an image dataset with a heavy
    central mode to check mode loss.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用不同的损失函数训练简单的网络，如DCGAN和MLP。我们使用玩具数据集来更好地可视化解决方案，并使用具有重中心模式的图像数据集来检查模式丢失。
- en: '**A. Toy Dataset** We synthesize (2-dim *X* and 1-dim *Z*) datasets with multiple
    sparse modes using a mixture of Normal distributions. These modes are arranged
    in circles and girds. It can be seen that the default BIGAN easily misses modes,
    but our method captured all modes in all cases.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**A. 玩具数据集** 我们使用正态分布的混合生成具有多个稀疏模式的（二维*X*和一维*Z*）数据集。这些模式排列成圆形和网格状。可以看出，默认的BIGAN很容易错过模式，但我们的方法在所有情况下都能捕捉到所有模式。'
- en: '![](../Images/be4a97655b354a732af0d37a23ea12e4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be4a97655b354a732af0d37a23ea12e4.png)'
- en: '[Fig 9.] Results from training a BIGAN network using the original method and
    our proposal on a toy dataset with sparse modes. The first column shows results
    from the original GAN and the second from our proposal.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.] 使用原始方法和我们提出的方法在一个具有稀疏模式的玩具数据集上训练BIGAN网络的结果。第一列展示了原始GAN的结果，第二列展示了我们提议方法的结果。'
- en: '**B. Heavy central mode** We extracted snapshots at regular intervals from
    footage of a traffic intersection (ref. [5]). The background remains static, and
    there is very little activity at certain times at certain locations in the frame.
    The dataset has a huge mode as the background only, without vehicles. While the
    original GAN and WGAN fail consistently at the task, our method shows significant
    learning.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**B. 重中心模式** 我们从交通交叉口的视频中定期提取快照（参考文献 [5]）。背景保持静止，在某些时间和某些位置，画面中的活动非常少。该数据集具有一个巨大的模式，仅包含背景，没有车辆。虽然原始的GAN和WGAN在这个任务上始终失败，但我们的方法展示了显著的学习效果。'
- en: '![](../Images/8d1804e00fef4e95b26d9b7f9ecb9fba.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d1804e00fef4e95b26d9b7f9ecb9fba.png)'
- en: '[Fig 10.] Generations and Reconstructions from the original GAN. Notice it
    collapses to the most frequent sample.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.] 来自原始GAN的生成和重建。注意它会收敛到最频繁的样本。'
- en: '![](../Images/b077956a40426e862f44fe9289a909dc.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b077956a40426e862f44fe9289a909dc.png)'
- en: '[Fig 11.] Generations and Reconstructions from our method. The generator can
    capture the minor modes.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.] 我们方法的生成和重建。生成器能够捕捉到较小的模式。'
- en: '**C. Latent interpolations** We also tested our method with the CelebA face
    dataset and found that the model learned minor features that occurred only in
    some frames like hats, glasses, extreme face angles, etc. Please refer to the
    paper for the complete results.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**C. 潜在插值** 我们还使用CelebA人脸数据集测试了我们的方法，发现模型学习到了一些只在部分帧中出现的细节特征，如帽子、眼镜、极端的面部角度等。完整结果请参见论文。'
- en: '![](../Images/92eb174a03a9493a2161a7cb80c67eb1.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92eb174a03a9493a2161a7cb80c67eb1.png)'
- en: '[Fig 12.] Generations from interpolations in the latent space.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12.] 潜在空间插值的生成结果。'
- en: '***Try it out***'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '***试试吧***'
- en: For those using a BIGAN or any other method where *E* and *G* are invertible,
    feel free to try it out. Just add the output of the following function to the
    Discriminator loss. The approach should work for all network architectures. As
    for others using traditional GANs, BIGANs could be a valuable consideration.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用BIGAN或其他任何方法，其中*E*和*G*是可逆的，尽管尝试。只需将以下函数的输出添加到鉴别器损失中。该方法应该适用于所有网络架构。至于其他使用传统GAN的用户，BIGAN可能是一个有价值的考虑。
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Conclusion***'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '***结论***'
- en: If the Encoder and Discriminator have enough capacity, the Encoder can map any
    distribution to a unimodal latent distribution accurately. When this is achieved
    (and the Generator and Encoder are invertible), the Generator can also learn the
    real distribution perfectly via pair-wise gradient penalty. The penalty effectively
    regularizes the Discriminator, eliminating the need to balance the three networks.
    The method benefits from increasing the capacity of any one of the networks independently.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果编码器和鉴别器具有足够的容量，编码器可以准确地将任何分布映射到单峰潜在分布。当实现这一点时（生成器和编码器是可逆的），生成器也可以通过成对梯度惩罚完美地学习真实分布。惩罚有效地对鉴别器进行正则化，消除了平衡三个网络的需要。该方法通过独立增加任何一个网络的容量而受益。
- en: I hope this helps people get insights into GANs and maybe help with mode loss
    :)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能帮助人们深入了解GAN，并可能有助于模式损失 :)
- en: '***References***'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '***参考文献***'
- en: '*[Note: Unless otherwise noted, all images are by the author]*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*[注：除非另有说明，所有图片均为作者所拍摄]*'
- en: '[1] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A. and Bengio, Y., 2014\. Generative adversarial nets. *Advances
    in neural information processing systems*, *27*.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A. and Bengio, Y., 2014\. 生成对抗网络。*神经信息处理系统进展*, *27*。'
- en: '[2] Arjovsky, M., Chintala, S. and Bottou, L., 2017, July. Wasserstein generative
    adversarial networks. In International conference on machine learning (pp. 214–223).
    PMLR.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Arjovsky, M., Chintala, S. and Bottou, L., 2017年7月。Wasserstein生成对抗网络。在机器学习国际会议上（第214-223页）。PMLR。'
- en: '[3] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. and Courville, A.C.,
    2017\. Improved training of wasserstein gans. *Advances in neural information
    processing systems*, *30*.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. and Courville, A.C.,
    2017\. 改进的Wasserstein GAN训练。*神经信息处理系统进展*, *30*。'
- en: '[4] Donahue, J., Krähenbühl, P. and Darrell, T., 2017\. Adversarial Feature
    Learning. In: 5th International Conference on Learning Representations (ICLR),
    Toulon, France, 24–26 April 2017.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Donahue, J., Krähenbühl, P. and Darrell, T., 2017\. 对抗特征学习。在：第5届国际学习表示会议（ICLR），法国图伦，2017年4月24-26日。'
- en: '[5] (*Traffic dataset*): Varadarajan, J. and Odobez, J.M., 2009, September.
    Topic models for scene analysis and abnormality detection. In *2009 IEEE 12th
    International Conference on Computer Vision Workshops, ICCV Workshops* (pp. 1338–1345).
    IEEE.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]（*交通数据集*）：Varadarajan, J. and Odobez, J.M., 2009年9月。用于场景分析和异常检测的主题模型。在*2009年IEEE第12届国际计算机视觉研讨会，ICCV研讨会*（第1338-1345页）。IEEE。'
