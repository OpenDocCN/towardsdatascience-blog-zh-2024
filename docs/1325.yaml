- en: 'Fine-Tune Smaller Transformer Models: Text Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28](https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Working with Smaller Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Microsoft’s Phi-3 to generate synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    ·18 min read·May 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6d2531f8cfeba1957c01af183353d13.png)'
  prefs: []
  type: TYPE_IMG
- en: Build a smaller model from a bigger model to perform on a use case | Image by
    author
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’re not a member but want to read this article, see this friend link*
    [*here.*](/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?sk=e71e980957ac03cbb3676a33252c61ad)'
  prefs: []
  type: TYPE_NORMAL
- en: Text classification models aren’t new, but the bar for how quickly they can
    be built and how well they perform has improved.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer-based model I will fine-tune here is more than 1000 times smaller
    than GPT-3.5 Turbo. It will perform consistently better for this use case because
    it will be specifically trained for it.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to optimize AI workflows where smaller models excel, particularly
    in handling redundant tasks where larger models are simply overkill.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/782d09ae6a26ffdc3592b605481710ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplified demonstration of model sizes for fun | Image by author
  prefs: []
  type: TYPE_NORMAL
- en: I’ve previously talked about [this](https://medium.com/gitconnected/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc),
    where I built a slightly larger [keyword extractor](https://huggingface.co/ilsilfverskiold/tech-keywords-extractor)
    for tech-focused content using a sequence-to-sequence transformer model. I also
    went through the different [models](https://github.com/ilsilfverskiold/smaller-models-docs/tree/main/nlp/docs)
    and what they excelled at.
  prefs: []
  type: TYPE_NORMAL
- en: For this piece, I’m diving into text classification with transformers, where
    encoder models do well. I’ll train a…
  prefs: []
  type: TYPE_NORMAL
