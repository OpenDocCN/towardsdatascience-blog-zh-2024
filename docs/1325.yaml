- en: 'Fine-Tune Smaller Transformer Models: Text Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调更小的 Transformer 模型：文本分类
- en: 原文：[https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28](https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28](https://towardsdatascience.com/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?source=collection_archive---------0-----------------------#2024-05-28)
- en: Working with Smaller Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用更小的语言模型
- en: Using Microsoft’s Phi-3 to generate synthetic data
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用微软的 Phi-3 生成合成数据
- en: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Ida
    Silfverskiöld](../Images/a2c0850bc0198688f70a5eca858cf8b5.png)](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    [Ida Silfverskiöld](https://medium.com/@ilsilfverskiold?source=post_page---byline--77cbbd3bf02b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    ·18 min read·May 28, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--77cbbd3bf02b--------------------------------)
    ·18 分钟阅读·2024年5月28日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d6d2531f8cfeba1957c01af183353d13.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6d2531f8cfeba1957c01af183353d13.png)'
- en: Build a smaller model from a bigger model to perform on a use case | Image by
    author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 从更大的模型构建更小的模型来执行某个应用场景 | 图片来自作者
- en: '*If you’re not a member but want to read this article, see this friend link*
    [*here.*](/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?sk=e71e980957ac03cbb3676a33252c61ad)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你不是会员，但想阅读这篇文章，可以通过这个朋友链接查看* [*点击这里。*](/fine-tune-smaller-transformer-models-text-classification-77cbbd3bf02b?sk=e71e980957ac03cbb3676a33252c61ad)'
- en: Text classification models aren’t new, but the bar for how quickly they can
    be built and how well they perform has improved.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类模型并不新鲜，但其构建速度和性能水平已有显著提升。
- en: The transformer-based model I will fine-tune here is more than 1000 times smaller
    than GPT-3.5 Turbo. It will perform consistently better for this use case because
    it will be specifically trained for it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里微调的基于 Transformer 的模型，比 GPT-3.5 Turbo 小超过 1000 倍。由于它将专门针对这个应用场景进行训练，因此在此用例上会表现得更好。
- en: The idea is to optimize AI workflows where smaller models excel, particularly
    in handling redundant tasks where larger models are simply overkill.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是优化 AI 工作流，使得较小的模型在某些场景下表现更好，特别是在处理冗余任务时，较大的模型则显得有些过于强大。
- en: '![](../Images/782d09ae6a26ffdc3592b605481710ca.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/782d09ae6a26ffdc3592b605481710ca.png)'
- en: Simplified demonstration of model sizes for fun | Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的模型大小演示 | 图片来自作者
- en: I’ve previously talked about [this](https://medium.com/gitconnected/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc),
    where I built a slightly larger [keyword extractor](https://huggingface.co/ilsilfverskiold/tech-keywords-extractor)
    for tech-focused content using a sequence-to-sequence transformer model. I also
    went through the different [models](https://github.com/ilsilfverskiold/smaller-models-docs/tree/main/nlp/docs)
    and what they excelled at.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前曾讨论过 [这篇文章](https://medium.com/gitconnected/fine-tune-smaller-nlp-models-with-hugging-face-for-specific-use-cases-1745813471dc)，其中我为技术类内容构建了一个稍大的
    [关键词提取器](https://huggingface.co/ilsilfverskiold/tech-keywords-extractor)，使用的是序列到序列的
    Transformer 模型。我还介绍了不同的 [模型](https://github.com/ilsilfverskiold/smaller-models-docs/tree/main/nlp/docs)
    以及它们的优势。
- en: For this piece, I’m diving into text classification with transformers, where
    encoder models do well. I’ll train a…
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将深入探讨使用 Transformer 进行文本分类，特别是编码器模型在此方面的优势。我将训练一个…
