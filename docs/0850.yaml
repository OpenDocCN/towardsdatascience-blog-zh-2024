- en: Clone the Abilities of Powerful LLMs into Small Local Models Using Knowledge
    Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02](https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Boost the performance of local LLMs using supervision from larger ones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    ·7 min read·Apr 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75ff2d16cad0eb78895e199d33f7552f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [matthew Feeney](https://unsplash.com/@matt__feeney?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/person-wearing-black-and-gray-jacket-in-front-of-bookshelf-Nwkh-n6l25w?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of Natural Language Processing (NLP), cutting-edge Large Language
    Models (LLMs) offer remarkable few-shot learning and reasoning capabilities. However,
    the computational demands and latency associated with these models can sometimes
    render them impractical for certain applications. If your goal, for instance,
    is to develop a translation service, you probably don’t require your back-end
    LLM to possess the ability to crack jokes or explain quantum physics to a kindergartner.
    This highlights the demand for specialized, smaller-scale models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A viable solution to this challenge is to construct tailored LLMs that cater
    precisely to your specific use case. This involves annotating significant volumes
    of data and then fine-tuning a more compact model like Tiny-llama to suit your
    requirements. Such an approach not only ensures that the model aligns closely
    with your needs but also mitigates the computational and deployment expenses associated
    with larger LLMs. However, one must acknowledge the downside of this method: the
    process of data annotation is often laborious and time-consuming.'
  prefs: []
  type: TYPE_NORMAL
