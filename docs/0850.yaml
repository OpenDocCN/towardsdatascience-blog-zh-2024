- en: Clone the Abilities of Powerful LLMs into Small Local Models Using Knowledge
    Distillation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用知识蒸馏将强大LLM的能力克隆到小型本地模型中
- en: 原文：[https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02](https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02](https://towardsdatascience.com/clone-the-abilities-of-powerful-llms-into-small-local-models-using-knowledge-distillation-12e954d256c2?source=collection_archive---------4-----------------------#2024-04-02)
- en: Boost the performance of local LLMs using supervision from larger ones
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用大规模模型的监督提升本地LLM的性能
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--12e954d256c2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    ·7 min read·Apr 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--12e954d256c2--------------------------------)
    ·7分钟阅读·2024年4月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/75ff2d16cad0eb78895e199d33f7552f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75ff2d16cad0eb78895e199d33f7552f.png)'
- en: Photo by [matthew Feeney](https://unsplash.com/@matt__feeney?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/person-wearing-black-and-gray-jacket-in-front-of-bookshelf-Nwkh-n6l25w?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [matthew Feeney](https://unsplash.com/@matt__feeney?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/person-wearing-black-and-gray-jacket-in-front-of-bookshelf-Nwkh-n6l25w?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: In the realm of Natural Language Processing (NLP), cutting-edge Large Language
    Models (LLMs) offer remarkable few-shot learning and reasoning capabilities. However,
    the computational demands and latency associated with these models can sometimes
    render them impractical for certain applications. If your goal, for instance,
    is to develop a translation service, you probably don’t require your back-end
    LLM to possess the ability to crack jokes or explain quantum physics to a kindergartner.
    This highlights the demand for specialized, smaller-scale models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域，前沿的大型语言模型（LLMs）提供了出色的少样本学习和推理能力。然而，这些模型的计算需求和延迟有时会使它们在某些应用中变得不切实际。例如，如果您的目标是开发一项翻译服务，您可能并不需要后端的LLM具备开玩笑或向幼儿解释量子物理的能力。这突显了对专门化、小规模模型的需求。
- en: 'A viable solution to this challenge is to construct tailored LLMs that cater
    precisely to your specific use case. This involves annotating significant volumes
    of data and then fine-tuning a more compact model like Tiny-llama to suit your
    requirements. Such an approach not only ensures that the model aligns closely
    with your needs but also mitigates the computational and deployment expenses associated
    with larger LLMs. However, one must acknowledge the downside of this method: the
    process of data annotation is often laborious and time-consuming.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这一挑战的可行解决方案是构建量身定制的LLM，精确满足您的特定用例。这涉及到对大量数据进行标注，然后对像Tiny-llama这样的小型模型进行微调，以适应您的需求。这种方法不仅确保了模型与您的需求紧密契合，还减少了与大型LLM相关的计算和部署成本。然而，必须承认这种方法的缺点：数据标注过程通常是繁琐且耗时的。
