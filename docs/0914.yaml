- en: 'ORPO: Preference Optimization without the Supervised Fine-tuning (SFT) Step'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ORPO：无监督微调（SFT）步骤的偏好优化
- en: 原文：[https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10](https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10](https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10)
- en: A much cheaper alignment method performing as well as DPO
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种比 DPO 更便宜的对齐方法，性能相当
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    ·7 min read·Apr 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    ·阅读时长 7 分钟·2024 年 4 月 10 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4d246d66e25e62577f87b7c194f20d7b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d246d66e25e62577f87b7c194f20d7b.png)'
- en: Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALL-E 生成
- en: There are now many methods to align large language models (LLMs) with human
    preferences. Reinforcement learning with human feedback (RLHF) was one of the
    first and brought us ChatGPT, but RLHF is very costly. DPO, IPO, and KTO are notably
    cheaper than RLHF as they don’t need a reward model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有许多方法可以将大型语言模型（LLM）与人类偏好对齐。带有人工反馈的强化学习（RLHF）是最早的一种方法，并且催生了 ChatGPT，但 RLHF
    成本非常高。DPO、IPO 和 KTO 比 RLHF 显著便宜，因为它们不需要奖励模型。
- en: While DPO and IPO are cheaper, they still require to train two different models.
    One model for the supervised fine-tuning (SFT) step, i.e., training the model
    to answer instructions, and then the model to align with human preferences using
    the SFT model for initialization and as a reference.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DPO 和 IPO 更便宜，但它们仍然需要训练两个不同的模型。一个模型用于监督微调（SFT）步骤，即训练模型回答指令，然后使用该 SFT 模型进行初始化和作为参考，来对齐人类偏好。
- en: ORPO is yet another new method for LLM alignment but this one doesn’t even need
    the SFT model. With ORPO, the LLM jointly learns to answer instructions and human
    preferences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ORPO 是另一种新的大型语言模型（LLM）对齐方法，但这一方法甚至不需要 SFT 模型。使用 ORPO，LLM 可以共同学习如何回答指令和人类偏好。
- en: In this article, I explain ORPO and review its performance. I show how to use
    it to turn Mistral 7B into a chat model using consumer hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将解释 ORPO 并回顾其性能。我展示了如何使用它将 Mistral 7B 转换为一个聊天模型，使用普通消费者硬件即可实现。
- en: Joint SFT and Preference Optimization
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联合 SFT 和偏好优化
- en: 'ORPO is presented in this paper:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 ORPO：
- en: '[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[ORPO：无参考模型的单体偏好优化](https://arxiv.org/abs/2403.07691)'
