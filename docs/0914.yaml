- en: 'ORPO: Preference Optimization without the Supervised Fine-tuning (SFT) Step'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10](https://towardsdatascience.com/orpo-preference-optimization-without-the-supervised-fine-tuning-sft-step-60632ad0f450?source=collection_archive---------4-----------------------#2024-04-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A much cheaper alignment method performing as well as DPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--60632ad0f450--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60632ad0f450--------------------------------)
    ·7 min read·Apr 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d246d66e25e62577f87b7c194f20d7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: There are now many methods to align large language models (LLMs) with human
    preferences. Reinforcement learning with human feedback (RLHF) was one of the
    first and brought us ChatGPT, but RLHF is very costly. DPO, IPO, and KTO are notably
    cheaper than RLHF as they don’t need a reward model.
  prefs: []
  type: TYPE_NORMAL
- en: While DPO and IPO are cheaper, they still require to train two different models.
    One model for the supervised fine-tuning (SFT) step, i.e., training the model
    to answer instructions, and then the model to align with human preferences using
    the SFT model for initialization and as a reference.
  prefs: []
  type: TYPE_NORMAL
- en: ORPO is yet another new method for LLM alignment but this one doesn’t even need
    the SFT model. With ORPO, the LLM jointly learns to answer instructions and human
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain ORPO and review its performance. I show how to use
    it to turn Mistral 7B into a chat model using consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Joint SFT and Preference Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ORPO is presented in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)'
  prefs: []
  type: TYPE_NORMAL
