<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introducing Univariate Exemplar Recommenders: how to profile Customer Behavior in a single vector</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introducing Univariate Exemplar Recommenders: how to profile Customer Behavior in a single vector</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-univariate-exemplar-recommenders-how-to-profile-customer-behavior-in-a-single-vector-c90c9943fe7d?source=collection_archive---------3-----------------------#2024-12-04">https://towardsdatascience.com/introducing-univariate-exemplar-recommenders-how-to-profile-customer-behavior-in-a-single-vector-c90c9943fe7d?source=collection_archive---------3-----------------------#2024-12-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="58e9" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Customer Profiling</h2><div/><div><h2 id="9863" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Surveying and improving the current methodologies for customer profiling</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ardito.bryan?source=post_page---byline--c90c9943fe7d--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Michelangiolo Mazzeschi" class="l ep by dd de cx" src="../Images/9211748ac638d2ed07679ac73ea17296.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c90c9943fe7d--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@ardito.bryan?source=post_page---byline--c90c9943fe7d--------------------------------" rel="noopener follow">Michelangiolo Mazzeschi</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c90c9943fe7d--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mo mp mq"><p id="7e59" class="mr ms mt mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">***To understand this article, knowledge of <strong class="mu ga">embeddings, clustering, and recommendation systems </strong>is required. The implementation of this algorithm has been released on <a class="af no" href="https://github.com/atlantis-nova/univariate-sequential-recommender" rel="noopener ugc nofollow" target="_blank">GitHub</a> and is fully open-source. <strong class="mu ga">I am open to criticism</strong> and <strong class="mu ga">welcome any feedback.</strong></p></blockquote><p id="845b" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Most platforms, nowadays, understand that tailoring individual choices for each customer leads to increased user engagement. Because of this,<strong class="mu ga"> the recommender systems' domain has been constantly evolving</strong>, witnessing the birth of new algorithms every year.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw nx"><img src="../Images/d918db3f6c1275e082b084893edeb392.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*8YjeQJ0IkEZiWDfyXjjFIA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">hierarchical clustering, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ef42" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Unfortunately, <strong class="mu ga">no existing taxonomy is keeping track</strong> of all algorithms in this domain. While most recommendation algorithms, such as matrix factorization, employ a neural network to make recommendations based on a list of choices, in this article, I will focus on the ones that <strong class="mu ga">employ a vector-based architecture to keep track of user preferences.</strong></p><h1 id="91a2" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Exemplar Recommenders</h1><p id="1da1" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Thanks to the simplicity of embeddings, each sample that can be recommended (ex. products, content…) is converted into a vector using a pre-trained neural network (for example a matrix factorization): we can then use knn to make recommendations of similar products/customers. The algorithms following this paradigm are known as <strong class="mu ga">vector-based recommender systems. </strong>However, when these models take into consideration the previous user choices,<strong class="mu ga"> they add a sequential layer</strong> to their base architecture and become technically known as <strong class="mu ga">vector-based</strong> <strong class="mu ga">sequential recommenders</strong>. Because these architectures are becoming increasingly difficult (to both remember and pronounce), I am calling them <strong class="mu ga">exemplar recommenders</strong>: they extract a set of representative vectors from an initial set of choices to represent a <strong class="mu ga">user vector</strong>.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div class="nv nw pq"><img src="../Images/2791e63bf766ddfe1cf2da8d8b469e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*BRbcUi6LmEnjpDKyrKQDOQ.png"/></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">subdivision of recommender systems, <strong class="bf op">image by Author</strong></figcaption></figure><p id="af1a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">One of the first systems built on top of this architecture is <strong class="mu ga">Pinterest</strong>, which is running on top of <a class="af no" href="https://medium.com/pinterest-engineering/pinnersage-multi-modal-user-embedding-framework-for-recommendations-at-pinterest-bfd116b49475" rel="noopener">its Pinnersage Recommendation engine</a>: this scaled engine capable of managing over 2 Billion pins runs its own specific architecture and <strong class="mu ga">performs clustering on the choices</strong> of each individual user. As we can imagine, this represents a computational challenge when scaled. Especially after discovering <strong class="mu ga">covariate encoding</strong>, I would like to introduce four complementary architectures (two in particular, with the article's name) that can <strong class="mu ga">relieve the stress of clustering algorithms</strong> when trying to profile each customer. You can refer to the following diagram to differentiate between them.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw pr"><img src="../Images/736be38c19281ee285c158b9b656dd4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*YNqke1bkyfjBl4AXixuPlQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">summary of exemplar recommenders, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cc3c" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Note that all the above approaches are classified as content-based filtering, and <strong class="mu ga">not collaborative filtering</strong>. In regards to the exemplar architecture, we can identify <strong class="mu ga">two main defining parameters</strong>: <strong class="mu ga">in-stack clustering implementation</strong> (we either perform clustering on the sample embedding or directly on the user embedding), and<strong class="mu ga"> the number of vectors</strong> used to store user preferences over time.</p><h1 id="ecd2" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">In-Stack Clustering implementation</h1><p id="c043" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Using once again Pinnersage as an example, we can see how it performs <strong class="mu ga">a novel clustering iter for each user</strong>. However advantageous from an accuracy perspective, this is computationally very heavy.</p><h2 id="9b6a" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">Post-Clustering</h2><p id="23a5" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">When clustering is used on top of the user embeddings, we can refer to this approach (in this specific stack) as <strong class="mu ga">post-clustering</strong>. However inefficient this may look, applying a non-parametric clustering algorithm on billions of samples is borderline impossible, and probably not the best option.</p><h2 id="9661" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">Pre-Clustering</h2><p id="8d95" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">There might be some use cases when applying clustering on top of the sample data could be advantageous: we can refer to this approach (in this specific stack) as <strong class="mu ga">pre-clustering.</strong> For example, a retail store may need to track the history of millions of users, requiring the same computational resources of the Pinnersage architecture.</p><p id="0347" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, the number of samples of a retail store, compared to the Pinterest platform, <strong class="mu ga">should not exceed 10.000</strong>, against the staggering <strong class="mu ga">2 Billion</strong> in comparison. With such a small number of samples, performing clustering on the sample embedding <strong class="mu ga">is very efficient</strong>, and will relieve the need to use it on the user embedding, if utilized properly.</p><h1 id="20c8" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Introducing the Univariate Architecture</h1><p id="ca83" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">As mentioned, the biggest challenge when creating these architectures is scalability. Each user amounts to <strong class="mu ga">hundreds of past choices held in record </strong>that need to be computed for <strong class="mu ga">exemplar extraction</strong>.</p><h2 id="7f2f" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">Multivariate architecture</h2><p id="da8d" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">The most common way of building a vector-based recommender is to pin every user choice to an existing pre-computed vector. However, even if we resort to decay functions to minimize the number of vectors to take into account for our calculation, we still need to <strong class="mu ga">fill the cache with all the vectors at the time of our computation</strong>. In addition, at the time of retrieval, the vectors cannot be stored on the machine that performs the calculation, but need to be queried from a database: this sets an additional challenge for scalability.</p><p id="9dd0" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The flow of this approach is the limited variance in recommendations. The recommended samples will be spatially very close to each other (the sample variance is minimized) and will only belong to the same category (unless there is in place a more complex logic defining this interaction).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qi"><img src="../Images/309a8b6ba10a2d190222402b49ef26d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DiN7aDMNIXfEyq6jkHj4qw.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">multivariate exemplar recommendation, <strong class="bf op">image by Author</strong></figcaption></figure><p id="3b7d" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">WHEN TO USE: This approach (I am only taking into account the behavior of the model, not its computational needs) is suited for applications where <strong class="mu ga">we can recommend a batch of samples all from the same category. </strong>Art or social media applications are one example.</p><h2 id="1304" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">Univariate architecture</h2><p id="3252" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">With this novel approach, we can store each user choice using a single vector that keeps updating over time. This should prove to be a remarkable improvement in scalability, minimizing the computational stress derived from both <strong class="mu ga">knn </strong>and <strong class="mu ga">retrieval</strong>.</p><p id="cd92" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">To make it even more complicated, there are two indexes where we can perform clustering. We can either cluster the <strong class="mu ga">items </strong>or the <strong class="mu ga">categories </strong>(both labeled using tags). There is no superior approach, we have to choose one depending on our use case.</p><h2 id="5378" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">&gt; category-based</h2><p id="ef63" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">This article is entirely based on the construction of a category-based model. After tagging our data we can perform <strong class="mu ga">a clustering to group our data into a hierarchy of categories </strong>(in case our data is already organized into categories, there is no need to apply hierarchical clustering).</p><p id="0e52" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The main advantage of this approach is that the exemplar indicating the user preferences will be linked to similar categories (increasing product variance).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qj"><img src="../Images/f0a97569cbae95f0f87036ceadefcc2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01jyXjugizOaesFAk6oymw.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">univariate category-based exemplar recommendation, <strong class="bf op">image by Author</strong></figcaption></figure><p id="8a8a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">WHEN TO USE: Sometimes, we want to focus on recommending an entire category to our customers, rather than individual products. For example, if our user enjoys buying shirts (and by chance the exemplar is located in the latent region of <strong class="mu ga">red shirts</strong>), we would benefit more from recommending him the entire clothing category, rather than <strong class="mu ga">only red shirts</strong>. This approach is best suited for retail and fashion companies.</p><h2 id="2674" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">&gt; item-based</h2><p id="c59f" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">With an item-based approach, we are performing clustering on top of our samples. This will allow us to capture more granular information on the data, rather than focusing on separated categories: we want to expand beyond the limitations of the product categorization and recommend items across existing categories.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qk"><img src="../Images/b0111b8cd5ace445e774e16f707bdb73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5DPmgzLEJFYK5OOr9i6Dw.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">univariate item-based exemplar recommendation, <strong class="bf op">image by Author</strong></figcaption></figure><p id="3d1e" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">WHEN TO USE: The best companies that can make the best use for this approach are human resources and retailers with cross-categorical products (ex. videogames).</p><h1 id="d6d1" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Univariate Exemplar Recommenders</h1><p id="5287" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Finally, we can explain in depth the architecture behind the category-based approach. This algorithm will perform exemplar extraction <strong class="mu ga">by only storing a single vector</strong> over time: the only technology capable of managing it is <strong class="mu ga">covariate encoding</strong>, hence <strong class="mu ga">we will use tags </strong>on top of the data. Because it uses <strong class="mu ga">pre-clustering</strong>, it is ideal for use cases with a manageable number of samples, but an unlimited number of users.</p><p id="884a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">For this example, I will be using the open-source collection of the <strong class="mu ga">Steam game library</strong> (<a class="af no" href="https://www.kaggle.com/datasets/fronkongames/steam-games-dataset" rel="noopener ugc nofollow" target="_blank">downloadable from Kaggle</a> — <a class="af no" href="https://www.mit.edu/~amini/LICENSE.md" rel="noopener ugc nofollow" target="_blank">MIT License</a>), which is a perfect use case for this recommender at scale: Steam uses no more than 450 tags, and the number can occasionally increase over time; yet,<strong class="mu ga"> it is manageable</strong>. This set of tags can be clustered very easily, and <strong class="mu ga">can even allow for manual intervention</strong> if we question the cluster assignment. Last, it serves millions of users, proving to be <strong class="mu ga">a realistic use case</strong> for our recommender.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw ql"><img src="../Images/65304e7769ea6e3d06f49ac9958745e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*nSkaO6QeduiosqTVD60uiQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">Sample of the Steam game dataset, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5fd8" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Its architecture can be articulated into the following phases:<br/>***Note that when creating the sample code of this architecture I am using LLMs to make the entire process <strong class="mu ga">free from any human supervision</strong>. However, <strong class="mu ga">LLMs remain optional</strong>, and while they may improve the level of this recommender system, they are not an essential part of it.</p><ol class=""><li id="1dc5" class="mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn qm qn qo bk">Sample Labeling<br/>We need to make sure to assign tags to each of our samples. Because of semantic tag filtering, we do not need to resort to zero-shots, but we can let a LLM manage this process without any supervision.</li><li id="2cdd" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Pre-Clustering<br/>We are going to divide the tag embedding into different clusters. For a higher level of accuracy, we are going to use <strong class="mu ga">hierarchical clustering </strong>with a depth of 3.</li><li id="f3e4" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Cluster labeling<br/>Once we have defined our cluster tree, we need to label each generated supercluster. We can still use LLM for this purpose. If you decide to avoid using LLMs, not that clusters can remain in a numerical form (this may only alter the user perception of the recommender).</li><li id="caba" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Balance non-uniform tag frequency<br/>The first challenge in picking from a list of tags is that the tags that appear the most (and are assigned to one cluster), heavily skew the recommender <strong class="mu ga">to propose that very cluster</strong>. We need to make sure that each cluster has the same probability of being recommended. We can achieve this by adding a custom multiplier that uniforms the probability of each cluster being recommended.</li><li id="7682" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Univariate sequential encoding<br/>Now that our encoding weights have been defined, we can encode the user history in a vector, but with the possibility of updating it over time (using a decay function to get rid of old user preferences).</li><li id="b15b" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Account for scalability: pruning mechanism<br/>Because the dimensions of our vector are equivalent to the number of tags, we need to find a way to limit the size of the vector over time. PCA is a valid option, but because of the sum operations on the vector, feature pruning has proved to be more efficient.</li><li id="c727" class="mr ms fq mu b gt qp mw mx gw qq mz na nb qr nd ne nf qs nh ni nj qt nl nm nn qm qn qo bk">Exemplar estimation<br/>This is where the innovation lies. We can encode the user profile <strong class="mu ga">as a single exemplar</strong> and <strong class="mu ga">still obtain separate cluster recommendations </strong>without any information loss that would arise IF we were to average multiple exemplars. This means that each of the previous multivariate methods would be incompatible with this architecture.</li></ol><p id="63d3" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Let us begin with the full explanation behind the Univariate Exemplar Recommender:</p><h2 id="0683" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">1. Sample Labeling</h2><p id="e0fa" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">In our reference dataset all samples have already been labeled using tags. If by any chance we are working with labeled data, we can easily do that using a LLM, <strong class="mu ga">prompting a request for a list of tags</strong> for each sample. As explained in my article on semantic tag filtering, we do not need to use zero-shots to guide the choice of labels, and the process can be completely unsupervised.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qu"><img src="../Images/23bfbf713c6524eaa34c3b854c8b5df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*QdxQImGZSTuBbBQvwob32Q.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">Screenshot of our sample data, each sample labeled with tags, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="fcf3" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">2. Pre-Clustering</h2><p id="8126" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">As mentioned, the idea behind this recommender is to first organize the data into clusters, and then identify the most common clusters (exemplars) that define the preferences of every single user. Because the data is ideally very small (thousands of tags against billions of samples), clustering is no longer a burden and can be done on the tag embedding, <strong class="mu ga">rather than on the millions of user embeddings</strong>.</p><p id="b426" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The more the number of tags increases, the more it makes sense to use a hierarchical structure to manage its complexity. Ideally, I would want not only to keep track of the main interests of each user but also <strong class="mu ga">their sub-interests</strong> and make recommendations accordingly. By using a dendrogram, we can define the different levels of clusters by <strong class="mu ga">using a threshold level</strong>.</p><p id="42e5" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The first superclusters (level 1) will be the result of using a threshold of 11.4, resulting in the first 81 clusters. We can also see how their distribution is non-uniform (some clusters are bigger than others), but all considered, is not excessively skewed.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qv"><img src="../Images/1fb4156cb9e3ba2388e560fa57b2e64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*JpeWinvkTEYlYiou0Bh5Uw.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">hierarchical clustering, level 1, threshold=11.4, <strong class="bf op">image by Author</strong></figcaption></figure><figure class="lg np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qw"><img src="../Images/09e6f67f767fc4b0b96b752b9da264f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ayjQpPU1e-AHAKd6l_SkqA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">all the cluster sizes of level 1 clustering, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6202" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The next clustering level will be defined by a smaller threshold (9), which organizes the data in 181 clusters. Equivalently for the first level of clustering, the size distribution is uneven, but there are only two big clusters, so it should not be this big of an issue.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qx"><img src="../Images/27ec7f451249f727f4b7667d98a6556a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*I4cWVMnaA2qsagObJ5jMIA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">hierarchical clustering, level 2, threshold=9, <strong class="bf op">image by Author</strong></figcaption></figure><figure class="lg np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qy"><img src="../Images/ebd50f4535567bb1b45eb9d06eeb0f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FLVp7NvZoQ3POo43rZvUcQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">all the cluster sizes of level 2 clustering, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8ab4" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">These thresholds have been arbitrarily chosen. Although <strong class="mu ga">there are non-parametric clustering algorithms</strong> that can perform the clustering process without any human input, they are quite challenging to manage, especially at scale, and show side effects such as the <strong class="mu ga">non-uniform distribution of cluster sizes</strong>. If among our clusters there are some that are too big (ex. one single cluster may even account for 20% of the overall data), then they may incorporate most recommendations without much sense.</p><p id="7f1a" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Our priority when executing clustering is to <strong class="mu ga">obtain the most uniform distribution while maximizing the number of clusters</strong> so that the data can be split and differently represented as much as possible.</p><h2 id="1477" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">3. Cluster labeling</h2><p id="5b39" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Because we have chosen to perform clustering on two levels of depths on top of our existing data, we have reached a total of 3 layers. The last layer is made by individual labels and is the only labeled layer. The other two, instead, only hold the cluster number without proper naming.</p><p id="7248" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">To solve this problem (note that this supercluster labeling step is not mandatory, but can improve how the user interacts with our recommender) we can use LLM on top of the superclusters. <br/>Let us try to automatically label all our clusters by feeding the tags inside of each group:</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw qz"><img src="../Images/5cf71c6a898b38ca7d779b7921154232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnLcSZB7y1hMjWhS7CkiNA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">labeling for clusters at different depths, <strong class="bf op">image by Author</strong></figcaption></figure><p id="0a62" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Now that also our clusters have been labeled correctly, we can start building the foundation of our sequential recommender.</p><h2 id="ff61" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">4. Balance non-uniform tag frequency</h2><p id="0455" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">So far, we have completed the easy part. Now that we have all our elements ready to create a recommender, we still need to adjust the imbalances. It would be much more intuitive to showcase this step after the recommender is done, but, unfortunately, it is part of its base structure, you will need to bear this with me.</p><h2 id="2833" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">4.1 What if we skip balancing?</h2><p id="cb51" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Let us, for a moment, skip ahead of time, and show the capabilities of our finished recommender by simply <strong class="mu ga">skipping this essential step</strong>. By assigning a score of 1 to each tag, there will be some tags that are so common that they will heavily skew the recommendation scores.</p><p id="b4da" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The following is a Monte Carlo simulation <strong class="mu ga">of 5000 random tag choices from the dataset</strong>. What we are looking at is the distribution of clusters that end up being chosen randomly after summing the scores. As we can see, the distribution is highly skewed and it will certainly break the recommender in favor of the clusters with the highest score.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw ra"><img src="../Images/18af5b555ed587667fe24d0fe67e92aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ZVOCiUt7DJ0-E00F8RVFbg.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">recommended cluster frequency over 10k simulations, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="436f" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">For example, the cluster <strong class="mu ga">“Dark Norse Realms”</strong> contains the tag <strong class="mu ga">Indie</strong>, which appears in 64% of all Samples (basically is almost impossible not to pick repetitively).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div class="nv nw rb"><img src="../Images/d9c04470b7d75d0810db6ee85642ea54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*2C5bR6FlekKhLcw1duwOcQ.png"/></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">example of recommended clusters, <strong class="bf op">image by Author</strong></figcaption></figure><p id="7df0" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">To be even more precise, let us directly simulate 100 different random sessions, each one picking <strong class="mu ga">the top 3 clusters from the session</strong> (the main user preference we keep track of), let us simulate entire user sessions so that the data is more complete. It is normal, especially when using a decay function, for the distribution to be non-uniform, and keep shifting over time.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rc"><img src="../Images/1194c03cc13c07c59ea1bb2c4d06832a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2Cm3QIWjqsNZ-pcsyHFlQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">recommended cluster frequency over 10k simulations, <strong class="bf op">image by Author</strong></figcaption></figure><p id="7929" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, if the skewness is excessive, the result is that the majority of users will be recommended <strong class="mu ga">the top 5% of the clusters 95% of the time</strong> (it is not precise numbers, just to prove my point).</p><h2 id="1a78" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">4.2 Balancing probability distribution</h2><p id="1ef3" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk"><strong class="mu ga">Instead</strong>, let us use a proper formula for frequency adjustment. Because the probability for each cluster is different, we want to assign a score that, when used to balance the weights of our user vector, <strong class="mu ga">will balance cluster retrieval:</strong></p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rd"><img src="../Images/3e1caf0f15a64ec7ed217ae81ea38b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GIpR-ApIjbn1Y7-SwzM9LA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">scoring function to balance probability non-uniformity, <strong class="bf op">image by Author</strong></figcaption></figure><p id="9bed" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Let us look at the score assigned to each tag for <strong class="mu ga">4 different random clusters</strong>:</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div class="nv nw re"><img src="../Images/0135220f5fb5a238c79b7cfb8a1ef387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*6X3iztcztVnm-n-izQfxfw.png"/></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">example of recommended clusters, <strong class="bf op">image by Author</strong></figcaption></figure><p id="0ceb" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">If we apply the score to the random pick (5000 picks, counting the frequency adjusted by the aforementioned <strong class="mu ga">weight</strong>), we can see how the tag distribution is now balanced (the outline ~ “Adrenaline Rush” is caused by a duplicate name):</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rf"><img src="../Images/177ebdc4f83d3186500aa718391e901b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*b-YsRFcXwm-NF0QoSSlLmQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">cluster probability over 10k simulations, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2fc8" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In fact, by looking at the normal distribution of the fluctuations, we see that the standard deviation for picking any cluster is approx. 0.1, <strong class="mu ga">which is extremely low</strong> (especially compared to before).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div class="nv nw rg"><img src="../Images/96ba2f4a02e2a0f3f14b9119710dd98e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*ynw5jL56fpWHuRqMA5nb3Q.png"/></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">fluctuation distribution over 10k simulations, <strong class="bf op">image by Author</strong></figcaption></figure><p id="b778" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">By replicating 100 sessions, we see how, even with a pseudo-uniform probability distribution, the clusters amass over time following the Pareto principle.</p></div></div><div class="np"><div class="ab cb"><div class="lr nq ls nr lt ns cf nt cg nu ci bh"><figure class="ny nz oa ob oc np od oe paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rh"><img src="../Images/c26f6ebf20506d50f86549f157ceb8dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Eog3gh-3btnrdQClfKotYQ.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">recommended cluster frequency over 10k simulations, <strong class="bf op">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b95c" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk"><strong class="al">5. Univariate sequential encoding</strong></h2><p id="a6c8" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">It is time to build the sequential mechanism to keep track of user choices over time. The mechanism I idealized <strong class="mu ga">works on two separate vectors</strong> (that after the process end up being one, hence univariate), a <strong class="mu ga">historical vector</strong> and a <strong class="mu ga">caching vector</strong>.</p><p id="dd16" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The <strong class="mu ga">historical vector</strong> is the one that is used to perform knn on the existing clusters. Once a session is concluded, we update the historical vector with the new user choices. At the same time, we adjust existing values with a decay function that diminishes the existing weights over time. By doing so, we make sure to keep up with the customer trends and <strong class="mu ga">give more weight to new choices, rather than older ones</strong>.</p><p id="d465" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Rather than updating the vector at each user makes a choice (which is not computationally efficient, in addition, we risk letting older choices decay too quickly, as every user interaction will trigger the decay mechanism), <strong class="mu ga">we can store a temporary vector </strong>that is only valid for the current session. Each user interaction, converted into a vector <strong class="mu ga">using the tag frequency as one hot weight</strong>, will be summed to the existing cached vector.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw ri"><img src="../Images/c2b1254238d7ad8f05e0f56050143d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64mX4qp-fpoMgHSaDBa37A.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">vector sum workflow, <strong class="bf op">image by Author</strong></figcaption></figure><p id="1b38" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Once the session is closed, we will retrieve the historical vector from the database, merge it with the cached vector, and <strong class="mu ga">apply the adjustment mechanisms</strong>, such as the decay function and pruning, as we will see later). After the historical vector has been updated, it will be stored in the database replacing the old one.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rj"><img src="../Images/03b301f6696962b86a1f8eb98a594437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yEvcBQAoJ-uQzDOAo0fQqA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">session recommender workflow, <strong class="bf op">image by Author</strong></figcaption></figure><p id="ef55" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The two reasons to follow this approach are to minimize the weight difference between older and newer interactions and to make the entire process scalable and computationally efficient.</p><h2 id="5321" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">6. Pruning Mechanism</h2><p id="8ee3" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">The system has been completed. However, there is an additional problem: covariate encoding has one flaw: its base vector <strong class="mu ga">is scaled proportionally to the number of encoded tags.</strong> For example, if our database were to reach 100k tags, the vector would have an equivalent number of dimensions.</p><p id="7714" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">The original covariate encoding architecture already takes this problem into account, proposing a PCA compression mechanism as a solution. However, applied to our recommender, PCA causes issues when iteratively summing vectors, resulting in information loss. Because every user choice will cause a summation of existing vectors with a new one, this solution is not advisable.</p><p id="97da" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, If we cannot compress the vector we can prune the dimensions with the lowest scores. The system will execute a knn based on the most relevant scores of the vector; this direct method of feature engineering won’t affect negatively (better yet, not excessively) the results of the final recommendation.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rk"><img src="../Images/46c1392526fd1dff0a09f0e42bbeef5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBjNYHo7CRrfnA4cUjI1-w.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">pruning mechanism, <strong class="bf op">image by Author</strong></figcaption></figure><p id="e612" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">By pruning our vector, we can arbitrarily set a maximum number of dimensions to our vectors. Without altering the tag indexes, we can start operating on sparse vectors, rather than a dense one, a data structure that only saves the active indexes of our vectors, being able to scale indefinitely. We can compare the recommendations obtained from a full vector (dense vector) against a sparse vector (pruned vector).</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rl"><img src="../Images/b204cc738fbcbbf25a164ce6503df5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7gzaC-49NBE9dlI5GJKwA.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">recommendation of the same user vector using a <strong class="bf op">dense </strong>vs. <strong class="bf op">sparse </strong>vector, <strong class="bf op">image by Author</strong></figcaption></figure><p id="4e10" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">As we can see, we can spot minor differences, but the overall integrity of the vector has been maintained <strong class="mu ga">in exchange for scalability</strong>. A very intuitive alternative to this process is by performing clustering at the tag level, maintaining the vector size fixed. In this case, a tag will need to be assigned to the closest tag semantically, and will not occupy its dedicated dimension.</p><h2 id="8cf2" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">7. Exemplar estimation</h2><p id="ce67" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Now that you have fully grasped the theory behind this new approach, we can compare them more clearly. In a multivariate approach, the first step was to identify the top user preferences using clustering. As we can see, this process required us to store as many vectors as found exemplars.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rm"><img src="../Images/55e9fe07679c0340a30f4bd9c6e2f76c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-6uWsBXJ0B4V0GAVBX3jg.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">Examplar extraction, <strong class="bf op">image by Author</strong></figcaption></figure><p id="3b78" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">However, in a univariate approach, <strong class="mu ga">because covariate encoding works on a transposed version of the encoded data</strong>, we can <strong class="mu ga">use sections of our historical vector</strong> to store user preferences, hence only using a single vector for the entire process. Using <strong class="mu ga">the historical vector as a query </strong>to search through encoded tags: its <strong class="mu ga">top-k results from a knn search</strong> will be equivalent to the top-k preferential clusters.</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div role="button" tabindex="0" class="of og ed oh bh oi"><div class="nv nw rn"><img src="../Images/5cf17d3e31b20326a241e4f55ab28ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WymC-puyCXNAzUgv13I2Hg.png"/></div></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">difference between multivariate and univariate sets of vectors, <strong class="bf op">image by Author</strong></figcaption></figure><h2 id="3ef4" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">8. Recommendation approaches</h2><p id="3162" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Now that we have captured more than one preference, how do we plan to recommend items? This is the major difference between the two systems. The traditional multivariate recommender will use the exemplar to <strong class="mu ga">recommend k items</strong> to a user. However, our system has assigned our customer one supercluster and the top subclusters under it (depending on our level of tag segmentation, we can increase the number of levels). We will not recommend the top k items, <strong class="mu ga">but the top k subclusters</strong>.</p><h2 id="ca47" class="ps or fq bf op pt pu pv ou pw px py ox nb pz qa qb nf qc qd qe nj qf qg qh fw bk">Using groupby instead of vector search</h2><p id="a8ba" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">So far, we have been using a vector to store data, but that <strong class="mu ga">does not mean we need to rely on vector search</strong> to perform recommendations, because it will be much slower than a SQL operation. Note that obtaining the same exact results using vector search on the user array is indeed possible.</p><p id="32cf" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">If you are wondering why you would be switching from a vector-based system to a count-based system, it is a legitimate question. The simple answer to <strong class="mu ga">that is that this is the most loyal replica of the multivariate system</strong> (as portrayed in the reference images), but much more scalable (it can reach up to 3000 recommendations/s on 16 CPU cores using pandas). Originally, the univariate recommender was designed to employ vector search, but, as showcased, there are simpler and better search algorithms.</p><h1 id="df8e" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Simulation</h1><p id="24ff" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">Let us run a full test that we can monitor. We can use the code from the sample notebook: for our simple example, the user selects at least one game <strong class="mu ga">labeled with corresponding tags</strong>.</p><pre class="ny nz oa ob oc ro rp rq bp rr bb bk"><span id="d4d4" class="rs or fq rp b bg rt ru l rv rw"># if no vector exists, the first choices are the historical vector<br/>historical_vector = user_choices(5, tag_lists=[['Shooter', 'Fantasy']], tag_frequency=tag_frequency, display_tags=False)<br/><br/># day1<br/>cached_vector = user_choices(3, tag_lists=[['Puzzle-Platformer'], ['Dark Fantasy'], ['Fantasy']], tag_frequency=tag_frequency, display_tags=False)<br/>historical_vector = update_vector(historical_vector, cached_vector, 1, 0.8)<br/><br/># day2<br/>cached_vector = user_choices(3, tag_lists=[['Puzzle'], ['Puzzle-Platformer']], tag_frequency=tag_frequency, display_tags=False)<br/>historical_vector = update_vector(historical_vector, cached_vector, 1, 0.8)<br/><br/># day3<br/>cached_vector = user_choices(3, tag_lists=[['Adventure'], ['2D', 'Turn-Based']], tag_frequency=tag_frequency, display_tags=False)<br/>historical_vector = update_vector(historical_vector, cached_vector, 1, 0.8)<br/><br/>compute_recommendation(historical_vector, label_1_max=3)</span></pre><p id="9339" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">At the end of 3 sessions, these are the top 3 exemplars (label_1) <strong class="mu ga">extracted from our recommender</strong>:</p><figure class="ny nz oa ob oc np nv nw paragraph-image"><div class="nv nw rx"><img src="../Images/9ba1cb6df92b3ec947667c1b4c82be9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*408DO3nyW9Jd-tZ8HAfiFA.png"/></div><figcaption class="ok ol om nv nw on oo bf b bg z dx">recommendation after 3 sessions, <strong class="bf op">image by Author</strong></figcaption></figure><p id="69ed" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In the notebook, you will find the option to perform Monte Carlo simulations, but there would be no easy way to validate them (mostly because team games are not tagged with the highest accuracy, and I noticed that most small games list too many unrelated or common tags).</p><h1 id="8ac1" class="oq or fq bf op os ot gv ou ov ow gy ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Conclusion</h1><p id="1e93" class="pw-post-body-paragraph mr ms fq mu b gt pl mw mx gw pm mz na nb pn nd ne nf po nh ni nj pp nl nm nn fj bk">The architectures of the most popular recommender systems still do not take into account session history, but with the development of new algorithms and the increase in computing power, it is now possible to tackle a higher level of complexity.</p><p id="0a20" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This new approach should offer a comprehensive alternative to the <strong class="mu ga">sequential recommender systems</strong> available on the market, but I am convinced that there is always room for improvement. To further enhance this architecture it would be possible to switch from a <strong class="mu ga">clustering-based</strong> to a <strong class="mu ga">network-based</strong> approach.</p><p id="0a08" class="pw-post-body-paragraph mr ms fq mu b gt mv mw mx gw my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">It is important to note that this recommender system can only excel when applied to a limited number of domains but has the potential to shine in conditions of scarce computational resources or extremely high demand.</p></div></div></div></div>    
</body>
</html>