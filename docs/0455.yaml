- en: Quantized Mistral 7B vs TinyLlama for Resource-Constrained Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/quantized-mistral-7b-vs-tinyllama-for-resource-constrained-systems-a6ce4ab95b03?source=collection_archive---------4-----------------------#2024-02-17](https://towardsdatascience.com/quantized-mistral-7b-vs-tinyllama-for-resource-constrained-systems-a6ce4ab95b03?source=collection_archive---------4-----------------------#2024-02-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Performance comparison between these models for accuracy and response time in
    a RAG question-answering setup.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@heelara?source=post_page---byline--a6ce4ab95b03--------------------------------)[![Kennedy
    Selvadurai, PhD](../Images/8c9175b153ef6a6fdc1793473c506718.png)](https://medium.com/@heelara?source=post_page---byline--a6ce4ab95b03--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a6ce4ab95b03--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a6ce4ab95b03--------------------------------)
    [Kennedy Selvadurai, PhD](https://medium.com/@heelara?source=post_page---byline--a6ce4ab95b03--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a6ce4ab95b03--------------------------------)
    ·8 min read·Feb 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d8c76deed39329ca0bb4fe416a44d64.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated using Canva as prompted by author
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of the open-source language model Mistral 7B by a French
    startup, Mistral, the breathtaking performance demonstrated by proprietary models
    like ChatGPT and claude.ai became available for the open-source community as well.
    To explore the feasibility of using this model on resource-constrained systems,
    its *quantized* versions have been shown to maintain great performance.
  prefs: []
  type: TYPE_NORMAL
- en: Even though 2-bit quantized Mistral 7B model passed accuracy test with flying
    colors in our [earlier study](https://medium.com/@heelara/querying-internal-documents-using-mistral-7b-with-context-of-ensemble-retrievers-f40aadaf8a6e),
    it was taking around 2 minutes on average to respond to questions on a Mac. Enter
    TinyLlama [1], a compact 1.1B language model pretrained on 3 trillion tokens with
    the same architecture and tokenizer as Llama 2\. It is aimed for more resource-constrained
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will compare the accuracy and response time performance
    of question answering capabilities of quantized Mistral 7B against quantized TinyLlama
    1.1B in an ensemble Retrieval-Augmented Generation (RAG) setup.
  prefs: []
  type: TYPE_NORMAL
- en: '**Contents**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Enabling Technologies](#d434)'
  prefs: []
  type: TYPE_NORMAL
- en: '[System Architecture](#9cab)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Environment Setup](#fd36)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Implementation](#2e2c)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Results and](#f541)…'
  prefs: []
  type: TYPE_NORMAL
