- en: Sensitivity Analysis for Unobserved Confounding
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=collection_archive---------10-----------------------#2024-02-13](https://towardsdatascience.com/sensitivity-analysis-for-unobserved-confounding-465970a969e0?source=collection_archive---------10-----------------------#2024-02-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to know the unknowable in observational studies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@uguryi?source=post_page---byline--465970a969e0--------------------------------)[![Ugur
    Yildirim](../Images/33db36531a170c9621504f466d61334b.png)](https://medium.com/@uguryi?source=post_page---byline--465970a969e0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--465970a969e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--465970a969e0--------------------------------)
    [Ugur Yildirim](https://medium.com/@uguryi?source=post_page---byline--465970a969e0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--465970a969e0--------------------------------)
    Â·10 min readÂ·Feb 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Outline
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#48c3)'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Problem Setup](#c5b8)'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2.1\. [Causal Graph](#3d15)
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.2\. [Model With and Without *Z*](#b3b3)2.3\. [Strength of *Z* as a Confounder](#4b5a)
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Sensitivity Analysis](#e05a)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1\. [Goal](#3e88)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2\. [Robustness Value](#dd07)
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PySensemakr](#5e81)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Conclusion](#8395)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Acknowledgements](#d671)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[References](#b5bc)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Introduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The specter of unobserved confounding (aka omitted variable bias) is a notorious
    problem in observational studies. In most observational studies, unless we can
    reasonably assume that treatment assignment is *as-if* random as in a natural
    experiment, we can never be truly certain that we controlled for all possible
    confounders in our model. As a result, our model estimates can be severely biased
    if we fail to control for an important confounderâ€“and we wouldnâ€™t even know it
    since the unobserved confounder is, well, unobserved!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this problem, it is important to assess how sensitive our estimates are
    to possible sources of unobserved confounding. In other words, it is a helpful
    exercise to ask ourselves: how much unobserved confounding would there have to
    be for our estimates to drastically change (e.g., treatment effect no longer statistically
    significant)? Sensitivity analysis for unobserved confounding is an active area
    of research, and there are several approaches to tackling this problem. In this
    post, I will cover a simple linear method [[1]](https://academic.oup.com/jrsssb/article/82/1/39/7056023)
    based on the concept of partial *RÂ²* that is widely applicable to a large spectrum
    of cases.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Problem Setup
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2.1\. Causal Graph
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us assume that we have four variables:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '*Y*: outcome'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*: treatment'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*X*: observed confounder(s)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Z*: unobserved confounder(s)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Z*: æœªè§‚å¯Ÿåˆ°çš„æ··æ‚å˜é‡'
- en: This is a common setting in many observational studies where the researcher
    is interested in knowing whether the treatment of interest has an effect on the
    outcome after controlling for possible treatment-outcome confounders.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è®¸å¤šè§‚å¯Ÿæ€§ç ”ç©¶ä¸­çš„å¸¸è§æƒ…å¢ƒï¼Œç ”ç©¶äººå‘˜å¸Œæœ›åœ¨æ§åˆ¶å¯èƒ½çš„æ²»ç–—â€”ç»“æœæ··æ‚å˜é‡åï¼Œäº†è§£æ‰€å…³æ³¨çš„æ²»ç–—æ˜¯å¦å¯¹ç»“æœäº§ç”Ÿå½±å“ã€‚
- en: In our hypothetical setting, the relationship between these variables are such
    that *X* and *Z* both affect *D* and *Y*, but *D* has no effect on *Y*. In other
    words, we are describing a scenario where the true treatment effect is null. As
    will become clear in the next section, the purpose of sensitivity analysis is
    being able to reason about this treatment effect when we have no access to *Z*,
    as we normally wonâ€™t since itâ€™s unobserved. Figure 1 visualizes our setup.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å‡è®¾æƒ…å¢ƒä¸­ï¼Œè¿™äº›å˜é‡ä¹‹é—´çš„å…³ç³»æ˜¯*X*å’Œ*Z*éƒ½å½±å“*D*å’Œ*Y*ï¼Œä½†*D*å¯¹*Y*æ²¡æœ‰å½±å“ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æè¿°çš„æ˜¯ä¸€ä¸ªçœŸå®æ²»ç–—æ•ˆåº”ä¸ºé›¶çš„æƒ…æ™¯ã€‚æ­£å¦‚ä¸‹ä¸€éƒ¨åˆ†å°†æ˜ç¡®æŒ‡å‡ºçš„ï¼Œæ•æ„Ÿæ€§åˆ†æçš„ç›®çš„æ˜¯èƒ½å¤Ÿæ¨ç†å‡ºè¿™ä¸ªæ²»ç–—æ•ˆåº”ï¼Œå°½ç®¡æˆ‘ä»¬é€šå¸¸æ— æ³•è®¿é—®*Z*ï¼Œå› ä¸ºå®ƒæ˜¯æœªè§‚å¯Ÿåˆ°çš„ã€‚å›¾1å±•ç¤ºäº†æˆ‘ä»¬çš„è®¾å®šã€‚
- en: '**Figure 1: Problem Setup**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1ï¼šé—®é¢˜è®¾å®š**'
- en: '![](../Images/bfd5d56edce6e7152043c61de6fe28f6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfd5d56edce6e7152043c61de6fe28f6.png)'
- en: 2.2\. Model With and Without Z
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. å¸¦*Z*å’Œä¸å¸¦*Z*çš„æ¨¡å‹
- en: To demonstrate the problem that our unobserved *Z* can cause, I simulated some
    data in line with the problem setup described above. You can refer to [this notebook](https://github.com/uguryi/unobserved_confounding/blob/main/unobserved_confounding.ipynb)
    for the details of the simulation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å±•ç¤ºæœªè§‚å¯Ÿåˆ°çš„*Z*å¯èƒ½é€ æˆçš„é—®é¢˜ï¼Œæˆ‘æ ¹æ®ä¸Šè¿°é—®é¢˜è®¾å®šæ¨¡æ‹Ÿäº†ä¸€äº›æ•°æ®ã€‚ä½ å¯ä»¥å‚è€ƒ[è¿™ä¸ªç¬”è®°æœ¬](https://github.com/uguryi/unobserved_confounding/blob/main/unobserved_confounding.ipynb)æŸ¥çœ‹æ¨¡æ‹Ÿçš„è¯¦ç»†ä¿¡æ¯ã€‚
- en: Since *Z* would be unobserved in real life, the only model we can normally fit
    to data is *Y~D+X*. Let us see what results we get if we run that regression.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº*Z*åœ¨ç°å®ç”Ÿæ´»ä¸­æ— æ³•è§‚å¯Ÿåˆ°ï¼Œæˆ‘ä»¬é€šå¸¸èƒ½æ‹Ÿåˆåˆ°æ•°æ®çš„å”¯ä¸€æ¨¡å‹æ˜¯*Y~D+X*ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬è¿è¡Œè¿™ä¸ªå›å½’ï¼Œç»“æœä¼šæ˜¯ä»€ä¹ˆã€‚
- en: Based on these results, it seems like *D* has a statistically significant effect
    of 0.2686 (*p*<0.001) per one unit change on *Y*, which we know isnâ€™t true based
    on how we generated the data (no *D* effect).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è¿™äº›ç»“æœï¼Œä¼¼ä¹*D*å¯¹*Y*çš„æ¯å•ä½å˜åŒ–æœ‰ç»Ÿè®¡æ˜¾è‘—æ•ˆåº”0.2686ï¼ˆ*p*<0.001ï¼‰ï¼Œä½†æˆ‘ä»¬çŸ¥é“è¿™ä¸ç¬¦åˆäº‹å®ï¼Œå› ä¸ºæˆ‘ä»¬ç”Ÿæˆæ•°æ®çš„æ–¹å¼æ˜¯æ²¡æœ‰*D*æ•ˆåº”çš„ã€‚
- en: Now, letâ€™s see what happens to our *D* estimate when we control for *Z* as well.
    (In real life, we of course wonâ€™t be able to run this additional regression since
    *Z* is unobserved but our simulation setting allows us to peek behind the curtain
    into the true data generation process.)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬åŒæ—¶æ§åˆ¶*Z*æ—¶ï¼Œ*D*ä¼°è®¡å€¼ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ã€‚ï¼ˆåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œå½“ç„¶æˆ‘ä»¬æ— æ³•è¿›è¡Œè¿™ä¸ªé¢å¤–çš„å›å½’ï¼Œå› ä¸º*Z*æ˜¯æœªè§‚å¯Ÿåˆ°çš„ï¼Œä½†æˆ‘ä»¬çš„æ¨¡æ‹Ÿè®¾ç½®å…è®¸æˆ‘ä»¬çª¥æ¢çœŸæ­£çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ã€‚ï¼‰
- en: As expected, controlling for *Z* correctly removes the *D* effect by shrinking
    the estimate towards zero and giving us a *p*-value that is no longer statistically
    significant at the ğ›¼=0.05 threshold (*p*=0.059).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ§åˆ¶*Z*åï¼Œ*D*æ•ˆåº”è¢«æ­£ç¡®ç§»é™¤ï¼Œä¼°è®¡å€¼æ”¶ç¼©è‡³é›¶ï¼Œå¹¶ä¸”æˆ‘ä»¬å¾—åˆ°çš„*p*å€¼åœ¨ğ›¼=0.05çš„æ˜¾è‘—æ€§æ°´å¹³ä¸‹ä¸å†æ˜¾è‘—ï¼ˆ*p*=0.059ï¼‰ã€‚
- en: 2.3\. Strength of Z as a Confounder
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. *Z*ä½œä¸ºæ··æ‚å˜é‡çš„å¼ºåº¦
- en: At this point, we have established that *Z* is strong enough of a confounder
    to eliminate the spurious *D* effect since the statistically significant *D* effect
    disappears when we control for *Z*. What we havenâ€™t discussed yet is exactly how
    strong *Z* is as a confounder. For this, we will utilize a useful statistical
    concept called partial *RÂ²*, which quantifies the proportion of variation that
    a given variable of interest can explain that canâ€™t already be explained by the
    existing variables in a model. In other words, partial *RÂ²* tells us the *added*
    explanatory power of that variable of interest, above and beyond the other variables
    that are already in the model. Formally, it can be defined as follows
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ç¡®è®¤*Z*è¶³å¤Ÿå¼ºå¤§ï¼Œèƒ½å¤Ÿæ¶ˆé™¤è™šå‡çš„*D*æ•ˆåº”ï¼Œå› ä¸ºåœ¨æˆ‘ä»¬æ§åˆ¶*Z*ä¹‹åï¼Œç»Ÿè®¡ä¸Šæ˜¾è‘—çš„*D*æ•ˆåº”æ¶ˆå¤±äº†ã€‚æˆ‘ä»¬å°šæœªè®¨è®ºçš„æ˜¯*Z*ä½œä¸ºæ··æ‚å˜é‡çš„å…·ä½“å¼ºåº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ä¸€ä¸ªæœ‰ç”¨çš„ç»Ÿè®¡å­¦æ¦‚å¿µï¼Œå«åšéƒ¨åˆ†*RÂ²*ï¼Œå®ƒé‡åŒ–äº†ä¸€ä¸ªç»™å®šçš„å…´è¶£å˜é‡èƒ½å¤Ÿè§£é‡Šçš„å˜å¼‚é‡çš„æ¯”ä¾‹ï¼Œè¿™äº›å˜å¼‚é‡æ˜¯æ¨¡å‹ä¸­ç°æœ‰å˜é‡æ— æ³•è§£é‡Šçš„ã€‚æ¢å¥è¯è¯´ï¼Œéƒ¨åˆ†*RÂ²*å‘Šè¯‰æˆ‘ä»¬è¯¥å˜é‡çš„*é¢å¤–*è§£é‡ŠåŠ›ï¼Œè¶…å‡ºäº†æ¨¡å‹ä¸­å·²åŒ…å«çš„å…¶ä»–å˜é‡ã€‚å½¢å¼ä¸Šï¼Œå®ƒå¯ä»¥å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![](../Images/841f01ffb074c5bbcd67165995585666.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/841f01ffb074c5bbcd67165995585666.png)'
- en: where *RSS_reduced* is the residual sum of squares from the model that doesnâ€™t
    include the variable(s) of interest and *RSS_full* is the residual sum of squares
    from the model that includes the variable(s) of interest.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: In our case, the variable of interest is *Z*, and we would like to know what
    proportion of the variation in *Y* and *D* that *Z* can explain that canâ€™t already
    be explained by the existing variables. More precisely, we are interested in the
    following two partial *RÂ²* values
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac6d3bca9e64679028bcb56422a4b6fc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: where (1) quantifies the proportion of variance in *Y* that can be explained
    by *Z* that canâ€™t already be explained by *D* and *X* (so the reduced model is
    Y~D+X and the full model is Y~D+X+Z), and (2) quantifies the proportion of variance
    in *D* that can be explained by *Z* that canâ€™t already be explained by *X* (so
    the reduced model is D~X and the full model is D~X+Z).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us see how strongly associated *Z* is with *D* and *Y* in our data
    in terms of partial *RÂ²*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out that *Z* explains 16% of the variation in *Y* that canâ€™t already
    be explained by *D* and *X* (this is partial *RÂ²* equation #1 above), and 20%
    of the variation in *D* that canâ€™t already be explained by *X* (this is partial
    *RÂ²* equation #2 above).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Sensitivity Analysis
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3.1\. Goal
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in the previous section, unobserved confounding poses a problem
    in real research settings precisely because, unlike in our simulation setting,
    *Z* cannot be observed. In other words, we are stuck with the model *Y~D+X*, having
    no way to know what our results would have been if we could run the model *Y~D+X+Z*
    instead. So, what can we do?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, a reasonable sensitivity analysis approach should be able to tell
    us that if a *Z* such as the one we have in our data *were to exist*, it would
    nullify our results. Remember that our *Z* explains 16% of the variation in *Y*
    and 20% of the variation in *D* that canâ€™t be explained by observed variables.
    Therefore, we expect sensitivity analysis to tell us that a hypothetical *Z*-like
    confounder of similar strength would be enough to eliminate the statistically
    significant *D* effect.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: But how can we calculate that the unobserved confounderâ€™s strength should be
    in this 16â€“20% range in the partial *RÂ²* scale *without ever having access to
    it*? Enter robustness value.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Robustness Value
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Robustness value (RV) formalizes the idea we mentioned above of determining
    the necessary strength of a hypothetical unobserved confounder that could nullify
    our results. The usefulness of RV emanates from the fact that we only need our
    observable model *Y~D+X* and not the unobservable model *Y~D+X+Z* to be able to
    calculate it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we can write down as follows the RV that quantifies how strong unobserved
    confounding needs to be to change our observed statistical significance of the
    treatment effect (if the notation is too much to follow, just remember the key
    idea that the RV is a measure of the strength of confounding needed to change
    our results)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a792c163556de1c4ad40364b6a2578b2.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Image by author, equations based on [[1]](https://academic.oup.com/jrsssb/article/82/1/39/7056023),
    see pages 49â€“52
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: ğ›¼ is our chosen significance level (generally set to 0.05 or 5%),
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*q* determines the percent reduction *q**100% in significance that we care
    about (generally set to 1, since we usually care about confounding that would
    reduce statistical significance by 1*100%=100% hence rendering it not statistically
    significant),'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t_betahat_treat* is the observed *t*-value of our treatment from the model
    *Y~D+X* (which is 8.389 in this case as can be seen from the regression results
    above),'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*df* is our degrees of freedom (which is 1000â€“3=997 in this case since we simulated
    1000 samples and are estimating 3 parameters including the intercept), and'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*_alpha,df-1* is the *t*-value threshold associated with a given ğ›¼ and *df-1*
    (1.96 if ğ›¼ is set to 0.05).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now ready to calculate the RV in our own data using only the observed
    model *Y~D+X* (*res_ydx*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: It is by no struck of luck that our RV (18%) falls right in the range of the
    partial *RÂ²* values we calculated for *Y~Z|D,X* (16%) and *D~Z|X* (20%) above.
    What the RV is telling us here is that, *even without any explicit knowledge of
    Z*, we can still reason that any unobserved confounder needs, on average, at least
    18% strength in the partial *RÂ²* scale vis-Ã -vis both the treatment and the outcome
    to be able to nullify our statistically significant result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why the RV isnâ€™t 16% or 20% but falls somewhere in between (18%)
    is that it is designed to be a single number that *summarizes* the necessary strength
    of the confounder with both the outcome and the treatment, so 18% makes perfect
    sense given what we know about the data. You can think about it like this: since
    the method doesnâ€™t have access to the actual numbers 16% and 20% when calculating
    the RV, it is doing its best to quantify the strength of the confounder by assigning
    18% to both partial *RÂ²* values (*Y~Z|D,X* and *D~Z|X*), which isnâ€™t too far off
    from the truth at all and actually does a great job summarizing the strength of
    the confounder.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Of course, in real life we wonâ€™t have the *Z* variable to double check that
    our RV is correct, but seeing how the two results align here should at least give
    you some confidence in the method. Finally, once we calculate the RV, we should
    think about whether an unobserved confounder of that strength is plausible. In
    our case, the answer is â€˜yesâ€™ because we have access to the data generation process,
    but for your specific real-life application, the existence of such a strong confounder
    might be an unreasonable assumption. This would be good news for you since no
    realistic unobserved confounder could drastically change your results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 4\. PySensemakr
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sensitivity analysis technique described above has already been implemented
    with all of its bells and whistles as a Python package under the name [PySensemakr](https://github.com/nlapier2/PySensemakr)
    (R, Stata, and Shiny App versions exist as well). For example, to get the exact
    same result that we manually calculated in the previous section, we can simply
    run the following code chunk.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Note that â€œRobustness Value, q = 1 alpha = 0.05â€ is 0.184, which is exactly
    what we calculated above. In addition to the RV for statistical significance,
    the package also provides the RV that is needed for the coefficient estimate itself
    to shrink to 0\. Not surprisingly, unobserved confounding needs to be even larger
    for this to happen (0.233 vs 0.184).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The package also provides contour plots for the two partial *RÂ²* values, which
    allows for an intuitive visual display of sensitivity to possible levels of confounding
    with the treatment and the outcome (in this case, it shouldnâ€™t be surprising to
    see that the x/y-axis value pairs that meet the red dotted line include 0.18/0.18
    as well as 0.20/0.16).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: One can even add benchmark values to the contour plot as proxies for possible
    amounts of confounding. In our case, since we only have one observed covariate
    *X*, we can set our benchmarks to be 0.25x, 0.5x and 1x as strong as that observed
    covariate. The resulting plot tells us that a confounder that is half as strong
    as *X* should be enough to nullify our statistically significant result (since
    the â€œ0.5x Xâ€ value falls right on the red dotted line).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I would like to note that while the simulated data in this example
    used a continuous treatment variable, in practice the method works for any kind
    of treatment variable including binary treatments. On the other hand, the outcome
    variable technically needs to be a continuous one since we are operating in the
    OLS framework. However, the method can still be used even with a binary outcome
    if we model it using OLS (this is called a LPM [[2]](https://murraylax.org/rtutorials/linearprob.html)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The possibility that our effect estimate may be biased due to unobserved confounding
    is a common danger in observational studies. Despite this potential danger, observational
    studies are a vital tool in data science because randomization simply isnâ€™t feasible
    in many cases. Therefore, it is important to know how we can address the issue
    of unobserved confounding by running sensitivity analyses to see how robust our
    estimates are to potential such confounding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The robustness value method by Cinelli and Hazlett discussed in this post is
    a simple and intuitive approach to sensitivity analysis formulated in a familiar
    linear model framework. If you are interested in learning more about the method,
    I highly recommend taking a look at the original paper and the [package documentation](https://pysensemakr.readthedocs.io/en/latest/index.html)
    where you can learn about many more interesting applications of the method such
    as â€˜extreme scenarioâ€™ analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other approaches to sensitivity analysis for unobserved
    confounding, and I would like briefly mention some of them here for readers who
    would like to continue learning more on this topic. One versatile technique is
    the E-value developed by VanderWeele and Ding that formulates the problem in terms
    of risk ratios [[3]](https://hrr.w.uib.no/files/2019/01/VanderWeeleDing_2017_e_-value.pdf)
    (implemented in R [here](https://cran.r-project.org/web/packages/EValue/index.html)).
    Another technique is the Austen plot developed by Veitch and Zaveri based on the
    concepts of partial *RÂ²* and propensity score [[4]](https://proceedings.neurips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf)
    (implemented in Python [here](https://github.com/anishazaveri/austen_plots)),
    and yet another recent approach is by Chernozhukov et al [[5]](https://www.nber.org/system/files/working_papers/w30302/w30302.pdf)
    (implemented in Python [here](https://docs.doubleml.org/stable/examples/py_double_ml_sensitivity.html)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Acknowledgements
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I would like to thank Chad Hazlett for answering my question related to using
    the method with binary outcomes and Xinyi Zhang for providing a lot of valuable
    feedback on the post. Unless otherwise noted, all images are by the author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 7\. References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] C. Cinelli and C. Hazlett, [Making Sense of Sensitivity: Extending Omitted
    Variable Bias](https://academic.oup.com/jrsssb/article/82/1/39/7056023) (2019),
    Journal of the Royal Statistical Society'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[2] J. Murray, [Linear Probability Model](https://murraylax.org/rtutorials/linearprob.html),
    Murrayâ€™s personal website'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[3] T. VanderWeele and P. Ding, [Sensitivity Analysis in Observational Research:
    Introducing the E-Value](https://hrr.w.uib.no/files/2019/01/VanderWeeleDing_2017_e_-value.pdf)
    (2017), Annals of Internal Medicine'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[4] V. Veitch and A. Zaveri, [Sense and Sensitivity Analysis: Simple Post-Hoc
    Analysis of Bias Due to Unobserved Confounding](https://proceedings.neurips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf)
    (2020), NeurIPS'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] V. Veitch å’Œ A. Zaveriï¼Œ [æ•æ„Ÿæ€§ä¸çµæ•åº¦åˆ†æï¼šå¯¹æœªè§‚å¯Ÿåˆ°çš„æ··æ‚å› ç´ å¼•èµ·çš„åå·®çš„ç®€å•äº‹ååˆ†æ](https://proceedings.neurips.cc/paper_files/paper/2020/file/7d265aa7147bd3913fb84c7963a209d1-Paper.pdf)
    ï¼ˆ2020ï¼‰ï¼ŒNeurIPS'
- en: '[5] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, and V. Syrgkanis, [Long
    Story Short: Omitted Variable Bias in Causal Machine Learning](https://www.nber.org/system/files/working_papers/w30302/w30302.pdf)
    (2022), NBER'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] V. Chernozhukov, C. Cinelli, W. Newey, A. Sharma, å’Œ V. Syrgkanisï¼Œ [ç®€è€Œè¨€ä¹‹ï¼šå› æœæœºå™¨å­¦ä¹ ä¸­çš„é—æ¼å˜é‡åå·®](https://www.nber.org/system/files/working_papers/w30302/w30302.pdf)
    ï¼ˆ2022ï¼‰ï¼ŒNBER'
