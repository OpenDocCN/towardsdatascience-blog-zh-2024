<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Build a Generative AI Tool for Information Extraction from Receipts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Build a Generative AI Tool for Information Extraction from Receipts</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10">https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/486a5de0d5063d0a6db0c23a138ff29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZqch57DB0_7oudI3luF_Q.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">DALLE-2’s interpretation of “A futuristic industrial document scanning facility”</figcaption></figure><div/><div><h2 id="a2e3" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Use LangChain and OpenAI tools to extract structured information from images of receipts stored in Google Drive</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Robert Martin-Short" class="l ep by dd de cx" src="../Images/e3910071b72a914255b185b850579a5a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-jfunT2ldyjlLaMX0t8PDA.jpeg"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------" rel="noopener follow">Robert Martin-Short</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">4</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="nc nd ne"><p id="5ed2" class="nf ng nh ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni gl">This article details how we can use open source Python packages such as LangChain, pytesseract and PyPDF, along with gpt-4-vision and gpt-3.5-turbo, to identify and extract key information from images of receipts. The resulting dataset could be used for a “chat to receipts” application. Check out the full code </strong><a class="af oc" href="https://github.com/rmartinshort/receiptchat/tree/main" rel="noopener ugc nofollow" target="_blank"><strong class="ni gl">here</strong></a><strong class="ni gl">.</strong></p></blockquote><p id="cee0" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Paper receipts come in all sorts of styles and formats and represent an interesting target for automated information extraction. They also provide a wealth of itemized costs that, if aggregated into a database, could be very useful for anyone interested in tracking their spend at more detailed level than offered by bank statements.</p><p id="6f1c" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Wouldn’t it be cool if you could take a photo of a receipt, upload it some application, then have its information extracted and appended to your personal database of expenses, which you could then query in natural language? You could then ask questions of the data like “what did I buy when I last visited IKEA?” or “what items do I spend most money on at Safeway”. Such a system might also naturally extend to corporate finance and expense tracking. In this article, we’ll build a simple application that deals with the first part of this process — namely extracting information from receipts ready to be stored in a database. Our system will monitor a Google Drive folder for new receipts, process them and append the results to a .csv file.</p><h1 id="1f91" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk"><strong class="al">1. Background and motivation</strong></h1><p id="49d3" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Technically, we’ll be doing a type of automated information extraction called template filling. We have a pre-defined schema of fields that we want to extract from our receipts and the task will be to fill these out, or leave them blank where appropriate. One major issue here is that the information contained in images or scans of receipts is unstructured, and although Optical Character Recognition (OCR) or PDF text extraction libraries might do a decent job at finding the text, they are not good preserving the relative positions of words in a document, which can make it difficult to match an item’s price to its cost for example.</p><p id="e0bc" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Traditionally, this issue is solved by template matching, where a pre-defined geometric template of the document is created and then extraction is only run in the areas known to contain important information. A great description of this can be found <a class="af oc" href="https://aicha-fatrah.medium.com/how-to-extract-information-from-documents-template-matching-e0540ae79599" rel="noopener">here</a>. However, this system is inflexible. What if a new format of receipt is added?</p><p id="aa76" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To get around this, more advanced services like <a class="af oc" href="https://docs.aws.amazon.com/textract/latest/dg/invoices-receipts.html" rel="noopener ugc nofollow" target="_blank">AWS Textract</a> and <a class="af oc" href="https://aws.amazon.com/rekognition/image-features/?nc=sn&amp;loc=3&amp;dn=3&amp;refid=f5358c45-1ee6-4a07-b08d-0f669e6cd865" rel="noopener ugc nofollow" target="_blank">AWS Rekognition </a>use a combination of pre-trained deep learning models for object detection, bounding box generation and named entity recognition (NER). I haven’t actually tried out these services on the problem at hand, but it would be really interesting to do so in order to compare the results against what we build with OpenAI’s LLMs.</p><p id="b130" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Large Language Models (LLM) such as gpt-3.5-turbo are also great at information extraction and template filling from unstructured text, especially after being given a few examples in their prompt. This makes them much more flexible than template matching or fine-tuning, since adding a few examples of a new receipt format is much faster and cheaper than re-training the model or building a new geometric template.</p><p id="6fbc" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If we are to use gpt-3.5-turbo on text extracted from a receipts, the question then becomes how can we build the examples from which it can learn? We could of course do this manually, but that wouldn’t scale well. Here we will explore the option of using gpt-4-vision for this. This version of gpt-4 can handle conversations that include images, and <a class="af oc" href="https://medium.com/@nageshmashette32/gpt4-vision-and-its-alternatives-6ed9d39508cd" rel="noopener">appears particularly good at describing the content of images</a>. Given an image of a receipt and a description of the key information we want to extract, gpt-4-vision should therefore be able to do the job in one shot, providing that the image is sufficiently clear.</p><p id="55dc" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Why wouldn’t we just use gpt-4-vision alone for this task and abandon gpt-3.5-turbo or other smaller LLMs? Technically we could, and the result might even be more accurate. But gpt-4-vision is very expensive and API calls are limited, so this system also won’t scale. Perhaps in the not-to-distant future though, vision LLMs will become a standard tool in this field of information extraction from documents.</p><p id="add3" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Another motivation for this article is about exploring how we can build this system using Langchain, a popular open source LLM orchestration library. In order to force an LLM to return structured output, prompt engineering is required and Langchain has some excellent tools for this. We will also try to ensure that our system is built in a way that is extensible, because this is just the first part of what could become a larger “chat to receipts” project.</p><p id="380d" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">With a brief background out of the way, lets get started with the code! I will be using Python3.9 and Langchain 0.1.14 here, and full details can be found in the <a class="af oc" href="https://github.com/rmartinshort/receiptchat/tree/main" rel="noopener ugc nofollow" target="_blank">repo</a>.</p><h1 id="bec5" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk"><strong class="al">2. Connect to Google Drive</strong></h1><p id="79b4" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">We need a convenient place to store our raw receipt data. Google Drive is one choice, and it provides a Python API that is relatively easy to use. To capture the receipts I use the <a class="af oc" href="https://thegrizzlylabs.com/genius-scan/" rel="noopener ugc nofollow" target="_blank">GeniusScan</a> app, which can upload .pdf, .jpeg or other file types from the phone directly to a Google Drive folder. The app also does some useful pre-processing such as automatic document cropping, which helps with the extraction process.</p><p id="20d6" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To set up API access to Google Drive, you’ll need to create service account credentials which can be generated by following the instructions <a class="af oc" href="https://developers.google.com/drive/api/quickstart/python" rel="noopener ugc nofollow" target="_blank">here</a>. For reference, I created a folder in my drive called “receiptchat” and set up a key pair that enables reading of data from that folder.</p><p id="c5b7" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The following code can be used to set up a drive service object, which gives you access to various methods to query Google Drive</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="d556" class="pn oe gk pk b bg po pp l pq pr">import os<br/>from googleapiclient.discovery import build<br/>from oauth2client.service_account import ServiceAccountCredentials<br/><br/>class GoogleDriveService:<br/><br/>    SCOPES = ["https://www.googleapis.com/auth/drive"]<br/><br/>    def __init__(self):<br/>        # the directory where your credentials are stored<br/>        base_path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))<br/>        <br/>        # The name of the file containing your credentials<br/>        credential_path = os.path.join(base_path, "gdrive_credential.json")<br/>        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credential_path<br/><br/>    def build(self):<br/>        <br/>        # Get credentials into the desired format<br/>        creds = ServiceAccountCredentials.from_json_keyfile_name(<br/>            os.getenv("GOOGLE_APPLICATION_CREDENTIALS"), self.SCOPES<br/>        )  <br/>        <br/>        # Set up the Gdrive service object<br/>        service = build("drive", "v3", credentials=creds, cache_discovery=False)<br/><br/>        return service</span></pre><p id="4e79" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In our simple application, we only really need to do two things: List all the files in the drive folder and download some list of them. The following class handles this:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="fd6e" class="pn oe gk pk b bg po pp l pq pr">import io<br/>from googleapiclient.errors import HttpError<br/>from googleapiclient.http import MediaIoBaseDownload<br/>import googleapiclient.discovery<br/>from typing import List<br/><br/><br/>class GoogleDriveLoader:<br/><br/>    # These are the types of files we want to download<br/>    VALID_EXTENSIONS = [".pdf", ".jpeg"]<br/><br/>    def __init__(self, service: googleapiclient.discovery.Resource):<br/><br/>        self.service = service<br/><br/>    def search_for_files(self) -&gt; List:<br/>        """<br/>        See https://developers.google.com/drive/api/guides/search-files#python<br/>        """<br/>      <br/>        # This query searches for objects that are not folders and <br/>        # contain the valid extensions<br/>        query = "mimeType != 'application/vnd.google-apps.folder' and ("<br/>        for i, ext in enumerate(self.VALID_EXTENSIONS):<br/>            if i == 0:<br/>                query += "name contains '{}' ".format(ext)<br/>            else:<br/>                query += "or name contains '{}' ".format(ext)<br/>        query = query.rstrip()<br/>        query += ")"<br/><br/>        # create drive api client<br/>        files = []<br/>        page_token = None<br/>        try:<br/>            while True:<br/>                response = (<br/>                    self.service.files()<br/>                    .list(<br/>                        q=query,<br/>                        spaces="drive",<br/>                        fields="nextPageToken, files(id, name)",<br/>                        pageToken=page_token,<br/>                    )<br/>                    .execute()<br/>                )<br/>                for file in response.get("files"):<br/>                    # Process change<br/>                    print(f'Found file: {file.get("name")}, {file.get("id")}')<br/><br/>                    file_id = file.get("id")<br/>                    file_name = file.get("name")<br/><br/>                    files.append(<br/>                        {<br/>                            "id": file_id,<br/>                            "name": file_name,<br/>                        }<br/>                    )<br/><br/>                page_token = response.get("nextPageToken", None)<br/>                if page_token is None:<br/>                    break<br/><br/>        except HttpError as error:<br/>            print(f"An error occurred: {error}")<br/>            files = None<br/><br/>        return files<br/><br/>    def download_file(self, real_file_id: str) -&gt; bytes:<br/>        """<br/>        Downloads a single file<br/>        """<br/><br/>        try:<br/>            file_id = real_file_id<br/>            request = self.service.files().get_media(fileId=file_id)<br/>            file = io.BytesIO()<br/>            downloader = MediaIoBaseDownload(file, request)<br/>            done = False<br/>            while done is False:<br/>                status, done = downloader.next_chunk()<br/>                print(f"Download {int(status.progress() * 100)}.")<br/><br/>        except HttpError as error:<br/>            print(f"An error occurred: {error}")<br/>            file = None<br/><br/>        return file.getvalue()</span></pre><p id="77eb" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Running this gives the following:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="a51e" class="pn oe gk pk b bg po pp l pq pr">service = GoogleDriveService().build()<br/>loader = GoogleDriveLoader(service)<br/>all_files loader.search_for_files() #returns a list of unqiue file ids and names <br/>pdf_bytes = loader.download_file({some_id}) #returns bytes for that file</span></pre><p id="ef8c" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Great! So now we can connect to Google Drive and bring image or pdf data onto our local machine. Next, we must process it and extract text.</p><h1 id="f49d" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk">3. Extract raw text from .pdfs and images</h1><p id="b9ae" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Multiple well-documented open source libraries exist to extract raw text from pdfs and images. For pdfs we will use <code class="cx ps pt pu pk b">PyPDF</code> here, although for a more comprehensive view of similar packages I recommend this <a class="af oc" rel="noopener" target="_blank" href="/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517">article</a>. For images in jpeg format, we will make use of <code class="cx ps pt pu pk b">pytesseract</code> , which is a wrapper for the <code class="cx ps pt pu pk b">tesseract</code> OCR engine. Installation instructions for that can be found <a class="af oc" href="https://tesseract-ocr.github.io/tessdoc/Installation.html" rel="noopener ugc nofollow" target="_blank">here</a>. Finally, we also want to be able to convert pdfs into jpeg format. This can be done with the <code class="cx ps pt pu pk b">pdf2image</code> package.</p><p id="ffb1" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Both <code class="cx ps pt pu pk b">PyPDF</code> and <code class="cx ps pt pu pk b">pytesseract</code> provide high level methods for extraction of text from documents. They both also have options for tuning this. <code class="cx ps pt pu pk b">pytesseract</code> , for example, can extract both text and boundary boxes (see <a class="af oc" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank">here</a>), which may be of useful in future if we want to feed the LLM more information about the format of the receipt whose text its processing. <code class="cx ps pt pu pk b">pdf2image</code> provides a method to convert pdf bytes to jpeg image, which is exactly what we want to do here. To convert jpeg bytes to an image that can be visualized, we’ll use the <code class="cx ps pt pu pk b">PIL</code> package.</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="778b" class="pn oe gk pk b bg po pp l pq pr">from abc import ABC, abstractmethod<br/>from pdf2image import convert_from_bytes<br/>import numpy as np<br/>from PyPDF2 import PdfReader<br/>from PIL import Image<br/>import pytesseract<br/>import io<br/><br/>DEFAULT_DPI = 50<br/><br/>class FileBytesToImage(ABC):<br/><br/>    @staticmethod<br/>    @abstractmethod<br/>    def convert_bytes_to_jpeg(file_bytes):<br/>        raise NotImplementedError<br/><br/>    @staticmethod<br/>    @abstractmethod<br/>    def convert_bytes_to_text(file_bytes):<br/>        raise NotImplementedError<br/><br/><br/>class PDFBytesToImage(FileBytesToImage):<br/><br/>    @staticmethod<br/>    def convert_bytes_to_jpeg(file_bytes, dpi=DEFAULT_DPI, return_array=False):<br/>        jpeg_data = convert_from_bytes(file_bytes, fmt="jpeg", dpi=dpi)[0]<br/>        if return_array:<br/>            jpeg_data = np.asarray(jpeg_data)<br/>        return jpeg_data<br/><br/>    @staticmethod<br/>    def convert_bytes_to_text(file_bytes):<br/>        pdf_data = PdfReader(<br/>            stream=io.BytesIO(initial_bytes=file_bytes) <br/>        )<br/>        # receipt data should only have one page<br/>        page = pdf_data.pages[0]<br/>        return page.extract_text()<br/><br/><br/>class JpegBytesToImage(FileBytesToImage):<br/><br/>    @staticmethod<br/>    def convert_bytes_to_jpeg(file_bytes, dpi=DEFAULT_DPI, return_array=False):<br/>        jpeg_data = Image.open(io.BytesIO(file_bytes))<br/>        if return_array:<br/>            jpeg_data = np.array(jpeg_data)<br/>        return jpeg_data<br/><br/>    @staticmethod<br/>    def convert_bytes_to_text(file_bytes):<br/>        jpeg_data = Image.open(io.BytesIO(file_bytes))<br/>        text_data = pytesseract.image_to_string(image=jpeg_data, nice=1)<br/>        return text_data</span></pre><p id="7db4" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The code above uses the concept of abstract base classes to improve extensibility. Lets say we want to add support for another file type in future. If we write the associated class and inherit from <code class="cx ps pt pu pk b">FileBytesToImage</code> , we are forced to write <code class="cx ps pt pu pk b">convert_bytes_to_image</code> and <code class="cx ps pt pu pk b">convert_bytes_to_text</code> methods in that. This makes it less likely that our classes will introduce errors downstream in a large application.</p><p id="b3bc" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The code can be used as follows:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="c556" class="pn oe gk pk b bg po pp l pq pr">bytes_to_image = PDFBytesToImage()<br/>image = PDFBytesToImage.convert_bytes_to_jpeg(pdf_bytes)<br/>text = PDFBytesToImage.convert_bytes_to_jpeg(pdf_bytes)</span></pre><figure class="pe pf pg ph pi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pv"><img src="../Images/9eeed45beddda0c8af8e2d5deaf3917c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OtewJS7P8bhTsOT-zHBXPA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Example of text extracted from a pdf document using the code above. Since receipts contain PII, here we are just demonstrating with a random document uploaded to the Google Drive. Image generated by the author.</figcaption></figure><h1 id="9858" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk">4. Information extraction with gpt-4-vision</h1><p id="9f74" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Now let’s use Langchain to prompt gpt-4-vision to extract some information from our receipts. We can start by using Langchain’s support for Pydantic to create a model for the output.</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="e85f" class="pn oe gk pk b bg po pp l pq pr">from langchain_core.pydantic_v1 import BaseModel, Field<br/>from typing import List<br/><br/><br/>class ReceiptItem(BaseModel):<br/>    """Information about a single item on a reciept"""<br/><br/>    item_name: str = Field("The name of the purchased item")<br/>    item_cost: str = Field("The cost of the item")<br/><br/><br/>class ReceiptInformation(BaseModel):<br/>    """Information extracted from a receipt"""<br/><br/>    vendor_name: str = Field(<br/>        description="The name of the company who issued the reciept"<br/>    )<br/>    vendor_address: str = Field(<br/>        description="The street address of the company who issued the reciept"<br/>    )<br/>    datetime: str = Field(<br/>        description="The date and time that the receipt was printed in MM/DD/YY HH:MM format"<br/>    )<br/>    items_purchased: List[ReceiptItem] = Field(description="List of purchased items")<br/>    subtotal: str = Field(description="The total cost before tax was applied")<br/>    tax_rate: str = Field(description="The tax rate applied")<br/>    total_after_tax: str = Field(description="The total cost after tax")</span></pre><p id="4e16" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is very powerful because Langchain can use this Pydantic model to construct format instructions for the LLM, which can be included in the prompt to force it to produce a json output with the specified fields. Adding new fields is as straightforward as just updating the model class.</p><p id="6d27" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Next, let’s build the prompt, which will just be static:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="ae72" class="pn oe gk pk b bg po pp l pq pr">from dataclasses import dataclass<br/><br/>@dataclass<br/>class VisionReceiptExtractionPrompt:<br/>    template: str = """<br/>       You are an expert at information extraction from images of receipts.<br/><br/>       Given this of a receipt, extract the following information:<br/>       - The name and address of the vendor<br/>       - The names and costs of each of the items that were purchased<br/>       - The date and time that the receipt was issued. This must be formatted like 'MM/DD/YY HH:MM'<br/>       - The subtotal (i.e. the total cost before tax)<br/>       - The tax rate<br/>       - The total cost after tax<br/><br/>       Do not guess. If some information is missing just return "N/A" in the relevant field.<br/>       If you determine that the image is not of a receipt, just set all the fields in the formatting instructions to "N/A". <br/>       <br/>       You must obey the output format under all circumstances. Please follow the formatting instructions exactly.<br/>       Do not return any additional comments or explanation. <br/>       """</span></pre><p id="9ae4" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now, we need to build a class that will take in an image and send it to the LLM along with the prompt and format instructions.</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="3687" class="pn oe gk pk b bg po pp l pq pr">from langchain.chains import TransformChain<br/>from langchain_core.messages import HumanMessage<br/>from langchain_core.runnables import chain<br/>from langchain_core.output_parsers import JsonOutputParser<br/>import base64<br/>from langchain.callbacks import get_openai_callback<br/><br/>class VisionReceiptExtractionChain:<br/><br/>    def __init__(self, llm):<br/>        self.llm = llm<br/>        self.chain = self.set_up_chain()<br/><br/>    @staticmethod<br/>    def load_image(path: dict) -&gt; dict:<br/>        """Load image and encode it as base64."""<br/><br/>        def encode_image(path):<br/>            with open(path, "rb") as image_file:<br/>                return base64.b64encode(image_file.read()).decode("utf-8")<br/><br/>        image_base64 = encode_image(path["image_path"])<br/>        return {"image": image_base64}<br/><br/>    def set_up_chain(self):<br/>        extraction_model = self.llm<br/>        prompt = VisionReceiptExtractionPrompt()<br/>        parser = JsonOutputParser(pydantic_object=ReceiptInformation)<br/><br/>        load_image_chain = TransformChain(<br/>            input_variables=["image_path"],<br/>            output_variables=["image"],<br/>            transform=self.load_image,<br/>        )<br/><br/>        # build custom chain that includes an image<br/>        @chain<br/>        def receipt_model_chain(inputs: dict) -&gt; dict:<br/>            """Invoke model"""<br/>            msg = extraction_model.invoke(<br/>                [<br/>                    HumanMessage(<br/>                        content=[<br/>                            {"type": "text", "text": prompt.template},<br/>                            {"type": "text", "text": parser.get_format_instructions()},<br/>                            {<br/>                                "type": "image_url",<br/>                                "image_url": {<br/>                                    "url": f"data:image/jpeg;base64,{inputs['image']}"<br/>                                },<br/>                            },<br/>                        ]<br/>                    )<br/>                ]<br/>            )<br/>            return msg.content<br/><br/>        return load_image_chain | receipt_model_chain | JsonOutputParser()<br/><br/>    def run_and_count_tokens(self, input_dict: dict):<br/>        with get_openai_callback() as cb:<br/>            result = self.chain.invoke(input_dict)<br/><br/>        return result, cb</span></pre><p id="a205" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The main method to understand here is <code class="cx ps pt pu pk b">set_up_chain</code> , which we will walk through step by step. These steps were inspired by this <a class="af oc" href="https://medium.com/@bpothier/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215" rel="noopener">blog post</a>.</p><ul class=""><li id="dd35" class="nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pw px py bk">Initialize the prompt, which in this case is just a block of text with some general instructions</li><li id="484e" class="nf ng gk ni b hi pz nk nl hl qa nn no np qb nr ns nt qc nv nw nx qd nz oa ob pw px py bk">Create a <code class="cx ps pt pu pk b">JsonOutputParser</code> from the Pydantic model we made above. This converts the model into a set of formatting instructions that can be added to the prompt</li><li id="5e55" class="nf ng gk ni b hi pz nk nl hl qa nn no np qb nr ns nt qc nv nw nx qd nz oa ob pw px py bk">Make a <code class="cx ps pt pu pk b">TransformChain</code> that allows us to incorporate custom functions — in this case the <code class="cx ps pt pu pk b">load_image</code> function — into the overall chain. Note that the chain will take in a variable called <code class="cx ps pt pu pk b">image_path</code> and output a variable called <code class="cx ps pt pu pk b">image</code> , which is a base64-encoded string representing the image. This is one of the formats accepted by gpt-4-vision.</li><li id="1976" class="nf ng gk ni b hi pz nk nl hl qa nn no np qb nr ns nt qc nv nw nx qd nz oa ob pw px py bk">To the best of my knowledge, <code class="cx ps pt pu pk b">ChatOpenAI</code> doesn’t yet natively support sending both text and images. Therefore, we need to make a custom chain that invokes the instance of <code class="cx ps pt pu pk b">ChatOpenAI</code> we made with the encoded image, prompt and formatting instructions.</li></ul><p id="df71" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Note that we’re also making use of openai callbacks to count the tokens and spend associated with each call.</p><p id="1234" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">To run this, we can do the following:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="b7ba" class="pn oe gk pk b bg po pp l pq pr">from langchain_openai import ChatOpenAI<br/>from tempfile import NamedTemporaryFile<br/><br/>model = ChatOpenAI(<br/>  api_key={your open_ai api key},<br/>  temperature=0, model="gpt-4-vision-preview", <br/>  max_tokens=1024<br/>)<br/><br/>extractor = VisionReceiptExtractionChain(model)<br/><br/># image from PDFBytesToImage.convert_bytes_to_jpeg()<br/>prepared_data = {<br/>    "image": image<br/>}<br/><br/>with NamedTemporaryFile(suffix=".jpeg") as temp_file:<br/>    prepared_data["image"].save(temp_file.name)<br/>    res, cb = extractor.run_and_count_tokens(<br/>        {"image_path": temp_file.name}<br/>    )</span></pre><p id="442a" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Given our random document above, the result looks like this:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="c78c" class="pn oe gk pk b bg po pp l pq pr">{'vendor_name': 'N/A',<br/> 'vendor_address': 'N/A',<br/> 'datetime': 'N/A',<br/> 'items_purchased': [],<br/> 'subtotal': 'N/A',<br/> 'tax_rate': 'N/A',<br/> 'total_after_tax': 'N/A'}</span></pre><p id="1687" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Not too exciting, but at least its structured in the correct way! When a valid receipt is provided, these fields are filled out and my assessment from running a few tests on different receipts it that its very accurate.</p><p id="227c" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Our callbacks look like this:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="9136" class="pn oe gk pk b bg po pp l pq pr">Tokens Used: 1170<br/>Prompt Tokens: 1104<br/>Completion Tokens: 66<br/>Successful Requests: 1<br/>Total Cost (USD): $0.01302</span></pre><p id="f978" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">This is essential for tracking costs, which can quickly grow during testing of a model like gpt-4.</p><h1 id="b35e" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk">5. Information extraction with gpt-3.5-turbo</h1><p id="6c67" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Let’s assume that we’ve used the steps in part 4 to generate some examples and saved them as a json file. Each example consists of some extracted text and corresponding key information as defined by our <code class="cx ps pt pu pk b">ReceiptInformation</code> Pydantic model. Now, we want to inject these examples into a call to gpt-3.5-turbo, in the hope that it can generalize what it learns from them to a new receipt. Few-shot learning is a powerful tool in prompt engineering and, if it works, would be great for this use case because whenever a new format of receipt is detected we can generate one example using gpt-4-vision and append it to the list of examples used to prompt gpt-3.5-turbo. Then when a similarly formatted receipt comes along, gpt-3.5-turbo can be used to extract its content. In a way this is like template matching, but without the need to manually define the template.</p><p id="2dc2" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">There are many ways to encourage text based LLMs to extract structured information from a block of text. One of the newest and most powerful that I’ve found is <a class="af oc" href="https://python.langchain.com/docs/use_cases/extraction/how_to/examples/" rel="noopener ugc nofollow" target="_blank">here</a> in the Langchain documentation. The idea is to create a prompt that contains a placeholder for some examples, then inject the examples into the prompt as if they were being returned by some function that the LLM had called. This is done with the <code class="cx ps pt pu pk b">model.with_structured_output()</code> functionality, which you can read about <a class="af oc" href="https://python.langchain.com/docs/modules/model_io/chat/structured_output/" rel="noopener ugc nofollow" target="_blank">here</a>. Note that this is currently in beta and so might change!</p><p id="c144" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let’s look at the code to see how this is achieved. We’ll first write the prompt.</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="cd0e" class="pn oe gk pk b bg po pp l pq pr">from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder<br/><br/>@dataclass<br/>class TextReceiptExtractionPrompt:<br/>    system: str = """<br/>       You are an expert at information extraction from images of receipts.<br/><br/>       Given this of a receipt, extract the following information:<br/>       - The name and address of the vendor<br/>       - The names and costs of each of the items that were purchased<br/>       - The date and time that the receipt was issued. This must be formatted like 'MM/DD/YY HH:MM'<br/>       - The subtotal (i.e. the total cost before tax)<br/>       - The tax rate<br/>       - The total cost after tax<br/><br/>       Do not guess. If some information is missing just return "N/A" in the relevant field.<br/>       If you determine that the image is not of a receipt, just set all the fields in the formatting instructions to "N/A". <br/>       <br/>       You must obey the output format under all circumstances. Please follow the formatting instructions exactly.<br/>       Do not return any additional comments or explanation.<br/>       """<br/><br/>    prompt: ChatPromptTemplate = ChatPromptTemplate.from_messages(<br/>        [<br/>            (<br/>                "system",<br/>                system,<br/>            ),<br/>            MessagesPlaceholder("examples"),<br/>            ("human", "{input}"),<br/>        ]<br/>    )</span></pre><p id="b09e" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The prompt text is exactly the same as it was in section 4, only we now have a <code class="cx ps pt pu pk b">MessagesPlaceholder</code> to hold the examples that we’re going to insert.</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="2927" class="pn oe gk pk b bg po pp l pq pr">class Example(TypedDict):<br/>    """A representation of an example consisting of text input and expected tool calls.<br/><br/>    For extraction, the tool calls are represented as instances of pydantic model.<br/>    """<br/><br/>    input: str<br/>    tool_calls: List[BaseModel]<br/><br/><br/>class TextReceiptExtractionChain:<br/><br/>    def __init__(self, llm, examples: List):<br/><br/>        self.llm = llm<br/>        self.raw_examples = examples<br/>        self.prompt = TextReceiptExtractionPrompt()<br/>        self.chain, self.examples = self.set_up_chain()<br/><br/>    @staticmethod<br/>    def tool_example_to_messages(example: Example) -&gt; List[BaseMessage]:<br/>        """Convert an example into a list of messages that can be fed into an LLM.<br/><br/>        This code is an adapter that converts our example to a list of messages<br/>        that can be fed into a chat model.<br/><br/>        The list of messages per example corresponds to:<br/><br/>        1) HumanMessage: contains the content from which content should be extracted.<br/>        2) AIMessage: contains the extracted information from the model<br/>        3) ToolMessage: contains confirmation to the model that the model requested a tool correctly.<br/><br/>        The ToolMessage is required because some of the chat models are hyper-optimized for agents<br/>        rather than for an extraction use case.<br/>        """<br/>        messages: List[BaseMessage] = [HumanMessage(content=example["input"])]<br/>        openai_tool_calls = []<br/>        for tool_call in example["tool_calls"]:<br/>            openai_tool_calls.append(<br/>                {<br/>                    "id": str(uuid.uuid4()),<br/>                    "type": "function",<br/>                    "function": {<br/>                        # The name of the function right now corresponds<br/>                        # to the name of the pydantic model<br/>                        # This is implicit in the API right now,<br/>                        # and will be improved over time.<br/>                        "name": tool_call.__class__.__name__,<br/>                        "arguments": tool_call.json(),<br/>                    },<br/>                }<br/>            )<br/>        messages.append(<br/>            AIMessage(content="", additional_kwargs={"tool_calls": openai_tool_calls})<br/>        )<br/>        tool_outputs = example.get("tool_outputs") or [<br/>            "You have correctly called this tool."<br/>        ] * len(openai_tool_calls)<br/>        for output, tool_call in zip(tool_outputs, openai_tool_calls):<br/>            messages.append(ToolMessage(content=output, tool_call_id=tool_call["id"]))<br/>        return messages<br/><br/>    def set_up_examples(self):<br/><br/>        examples = [<br/>            (<br/>                example["input"],<br/>                ReceiptInformation(<br/>                    vendor_name=example["output"]["vendor_name"],<br/>                    vendor_address=example["output"]["vendor_address"],<br/>                    datetime=example["output"]["datetime"],<br/>                    items_purchased=[<br/>                        ReceiptItem(<br/>                            item_name=example["output"]["items_purchased"][i][<br/>                                "item_name"<br/>                            ],<br/>                            item_cost=example["output"]["items_purchased"][i][<br/>                                "item_cost"<br/>                            ],<br/>                        )<br/>                        for i in range(len(example["output"]["items_purchased"]))<br/>                    ],<br/>                    subtotal=example["output"]["subtotal"],<br/>                    tax_rate=example["output"]["tax_rate"],<br/>                    total_after_tax=example["output"]["total_after_tax"],<br/>                ),<br/>            )<br/>            for example in self.raw_examples<br/>        ]<br/><br/>        messages = []<br/><br/>        for text, tool_call in examples:<br/>            messages.extend(<br/>                self.tool_example_to_messages(<br/>                    {"input": text, "tool_calls": [tool_call]}<br/>                )<br/>            )<br/><br/>        return messages<br/><br/>    def set_up_chain(self):<br/><br/>        extraction_model = self.llm<br/>        prompt = self.prompt.prompt<br/>        examples = self.set_up_examples()<br/>        runnable = prompt | extraction_model.with_structured_output(<br/>            schema=ReceiptInformation,<br/>            method="function_calling",<br/>            include_raw=False,<br/>        )<br/><br/>        return runnable, examples<br/><br/>    def run_and_count_tokens(self, input_dict: dict):<br/><br/>        # inject the examples here<br/>        input_dict["examples"] = self.examples<br/>        with get_openai_callback() as cb:<br/>            result = self.chain.invoke(input_dict)<br/><br/>        return result, cb</span></pre><p id="f043" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><code class="cx ps pt pu pk b">TextReceiptExtractionChain</code> is going to take in a list of examples, each of which has <code class="cx ps pt pu pk b">input</code> and <code class="cx ps pt pu pk b">output</code> keys (note how these are used in the <code class="cx ps pt pu pk b">set_up_examples</code> method). For each example, we will make a <code class="cx ps pt pu pk b">ReceiptInformation</code> object. Then we format the result into a list of messages that can be passed into the prompt. All the work in <code class="cx ps pt pu pk b">tool_examples_to_messages</code> is there just to convert between different Langchain formats.</p><p id="9b30" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Running this looks very similar to what we did with the vision model:</p><pre class="pe pf pg ph pi pj pk pl bp pm bb bk"><span id="f5ad" class="pn oe gk pk b bg po pp l pq pr"># Load the examples <br/>EXAMPLES_PATH = "receiptchat/datasets/example_extractions.json"<br/>with open(EXAMPLES_PATH) as f:<br/>    loaded_examples = json.load(f)<br/><br/>loaded_examples = [<br/>    {"input": x["file_details"]["extracted_text"], "output": x}<br/>    for x in loaded_examples<br/>]<br/><br/># Set up the LLM caller<br/>llm = ChatOpenAI(<br/>  api_key=secrets["OPENAI_API_KEY"], <br/>  temperature=0, <br/>  model="gpt-3.5-turbo"<br/>)<br/>extractor = TextReceiptExtractionChain(llm, loaded_examples)<br/><br/># convert a PDF file form Google Drive into text<br/>text = PDFBytesToImage.convert_bytes_to_text(downloaded_data)<br/><br/>extracted_information, cb = extractor.run_and_count_tokens(<br/>            {"input": text}<br/>)</span></pre><p id="f7b2" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Even with 10 examples, this call is less than half the cost of the gpt-4-vision and also alot faster to return. As more examples get added, you may need to use gpt-3.5-turbo-16k to avoid exceeding the context window.</p><h1 id="24d1" class="od oe gk bf of og oh hk oi oj ok hn ol om on oo op oq or os ot ou ov ow ox oy bk">The output dataset</h1><p id="a728" class="pw-post-body-paragraph nf ng gk ni b hi oz nk nl hl pa nn no np pb nr ns nt pc nv nw nx pd nz oa ob fj bk">Having collected some receipts, you can run the extraction methods described in sections 4 and 5 and collect the result in a dataframe. This then gets stored and can be appended to whenever a new receipt appears in the Google Drive.</p><figure class="pe pf pg ph pi fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qe"><img src="../Images/07908e8c252f1d406db696da159899c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0I6sPqQ-o9Yue2umRXuhMg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Sample of the output dataset, showing fields extracted from multiple receipts. Image generated by the author</figcaption></figure><p id="b388" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once my database of extracted receipt information grows a bit larger, I plan to explore LLM-based question answering on top of it, so look out for that article soon! I’m also curious about exploring a more formal evaluation method for this project and comparing the results to what can be obtained via AWS Textract or similar products.</p><p id="a8df" class="pw-post-body-paragraph nf ng gk ni b hi nj nk nl hl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Thanks for making it to the end! Please feel free to explore the full codebase here <a class="af oc" href="https://github.com/rmartinshort/receiptchat" rel="noopener ugc nofollow" target="_blank">https://github.com/rmartinshort/receiptchat</a>. Any suggestions for improvement or extensions to the functionality would be much appreciated!</p></div></div></div></div>    
</body>
</html>