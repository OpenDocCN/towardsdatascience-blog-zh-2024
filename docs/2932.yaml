- en: 'LLMs for Coding in 2024: Price, Performance, and the Battle for the Best'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2024 年用于编程的 LLM：价格、性能与最佳之争
- en: 原文：[https://towardsdatascience.com/llms-for-coding-in-2024-performance-pricing-and-the-battle-for-the-best-fba9a38597b6?source=collection_archive---------5-----------------------#2024-12-04](https://towardsdatascience.com/llms-for-coding-in-2024-performance-pricing-and-the-battle-for-the-best-fba9a38597b6?source=collection_archive---------5-----------------------#2024-12-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llms-for-coding-in-2024-performance-pricing-and-the-battle-for-the-best-fba9a38597b6?source=collection_archive---------5-----------------------#2024-12-04](https://towardsdatascience.com/llms-for-coding-in-2024-performance-pricing-and-the-battle-for-the-best-fba9a38597b6?source=collection_archive---------5-----------------------#2024-12-04)
- en: Evaluating the current LLM landscape based both benchmarks and real-world insights
    to help you make informed choices.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于基准测试和实际世界的洞察，评估当前 LLM 的格局，帮助你做出明智的选择。
- en: '[](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)[![Ruben
    Broekx](../Images/68f9dd486a291bf619e68b9e2edbadf1.png)](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)
    [Ruben Broekx](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)[![Ruben
    Broekx](../Images/68f9dd486a291bf619e68b9e2edbadf1.png)](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)
    [Ruben Broekx](https://medium.com/@broekxruben?source=post_page---byline--fba9a38597b6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)
    ·11 min read·Dec 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fba9a38597b6--------------------------------)
    ·11 分钟阅读·2024年12月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f7ff9f49d899a8ab06a59413877a8c5d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ff9f49d899a8ab06a59413877a8c5d.png)'
- en: Image generated by Flux.1 - Schnell
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Flux.1 - Schnell 生成
- en: The landscape of Large Language Models (LLMs) for coding has never been more
    competitive. With major players like Alibaba, Anthropic, Google, Meta, Mistral,
    OpenAI, and xAI all offering their own models, developers have more options than
    ever before.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编程的大型语言模型（LLM）的竞争格局从未如此激烈。像阿里巴巴、Anthropic、谷歌、Meta、Mistral、OpenAI 和 xAI 等主要厂商都在提供自己的模型，开发者比以往任何时候都有更多的选择。
- en: '**But how can you choose the best LLM for your coding use case?**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**但你如何选择最适合你编程用例的 LLM 呢？**'
- en: In this post, I provide an in-depth analysis of the top LLMs available through
    public APIs. I focus on their performance in coding tasks as measured by benchmarks
    like HumanEval, and their observed real-world performance as reflected by their
    respective Elo scores.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将深入分析通过公共 API 提供的顶级 LLM。我将重点关注它们在编码任务中的表现，评估基准如 HumanEval，以及它们在实际应用中的表现，反映在各自的
    Elo 分数上。
- en: Whether you’re working on a personal project or integrating AI into your development
    workflow, understanding the strengths and weaknesses of these models will help
    you make a more informed decision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是在做个人项目，还是将人工智能集成到你的开发工作流程中，了解这些模型的优缺点将帮助你做出更明智的决策。
- en: 'Disclaimer: challenges when comparing LLMs'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 免责声明：比较 LLM 时的挑战
- en: Comparing LLMs is hard. Models frequently receive updates that have a significant
    influence on their performance — say for example OpenAI’s updates from GPT-4 to
    GPT-4-turbo to GPT-4o to the o1 models. However, even minor updates have an effect
    — GPT-4o, for example, received already 3 updates after its release on May 13th!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 比较大型语言模型（LLMs）是很困难的。模型经常接收到更新，这些更新对它们的表现有着显著影响——例如，OpenAI 从 GPT-4 到 GPT-4-turbo
    再到 GPT-4o，再到 o1 模型的更新。然而，即使是小的更新也会产生影响——例如，GPT-4o 自 5 月 13 日发布以来，已经收到了 3 次更新！
- en: Additionally, the stochastic nature of these models means their performance
    can vary across different runs, leading to inconsistent results in studies. Finally,
    some companies may tailor benchmarks and configurations — such as specific Chain-of-Thought
    techniques — to showcase their models in the best light, which skew comparisons
    and mislead conclusions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些模型的随机性意味着它们的表现可能会在不同的运行中有所不同，从而导致研究中的结果不一致。最后，一些公司可能会定制基准和配置——例如特定的 Chain-of-Thought
    技术——以展示其模型的最佳表现，这会扭曲比较结果并误导结论。
- en: '**Conclusion: comparing LLM performance is hard.**'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：比较 LLM 的表现是困难的。**'
- en: This post represents a best-effort comparison of various models for coding tasks
    based on the information available. I welcome any feedback to improve the accuracy
    of this analysis!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文代表了基于现有信息对各种编码任务模型的最佳努力比较。我欢迎任何反馈，以提高此分析的准确性！
- en: 'Evaluating LLMs: HumanEval and Elo scores'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 LLM：HumanEval 和 Elo 分数
- en: As hinted at in the disclaimer above, to properly understand how LLMs perform
    in coding tasks, it’s advisable to evaluate them from multiple perspectives.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如上文免责声明所示，为了正确理解 LLM 在编码任务中的表现，建议从多个角度进行评估。
- en: Benchmarking through HumanEval
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 HumanEval 进行基准测试
- en: 'Initially, I tried to aggregate results from several benchmarks to see which
    model comes out on top. However, this approach had as core problem: different
    models use different benchmarks and configurations. Only one benchmark seemed
    to be the default for evaluating coding performance: *HumanEval*. This is a benchmark
    dataset consisting of human-written coding problems, evaluating a model’s ability
    to generate correct and functional code based on specified requirements. By assessing
    code completion and problem-solving skills, HumanEval serves as a standard measure
    for coding proficiency in LLMs.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我尝试从多个基准中汇总结果，看看哪个模型表现最好。然而，这种方法的核心问题是：不同的模型使用不同的基准和配置。似乎只有一个基准是评估编码表现的默认标准：*HumanEval*。这是一个由人类编写的编码问题数据集，评估模型根据特定要求生成正确和功能性代码的能力。通过评估代码补全和解决问题的能力，HumanEval
    成为评估 LLM 编码能力的标准。
- en: The voice of the people through Elo scores
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Elo 分数展现人们的声音
- en: While benchmarks give a good view of a model’s performance, they should also
    be taken with a grain of salt. Given the vast amounts of data LLMs are trained
    on, some of a benchmark’s content (or highly similar content) might be part of
    that training. That’s why it’s beneficial to also evaluate models based on how
    well they perform as judged by humans. Elo ratings, such as those from *Chatbot
    Arena (coding only)*, do just that. These are scores derived from head-to-head
    comparisons of LLMs in coding tasks, evaluated by human judges. Models are pitted
    against each other, and their Elo scores are adjusted based on wins and losses
    in these pairwise matches. An Elo score shows a model’s relative performance compared
    to others in the pool, with higher scores indicating better performance. For example,
    a difference of 100 Elo points suggests that the higher-rated model is expected
    to win about 64% of the time against the lower-rated model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基准测试能很好地反映模型的表现，但也应谨慎对待。考虑到 LLM 是在大量数据上训练的，某些基准内容（或非常相似的内容）可能已经包含在这些训练数据中。这就是为什么基于人类评判的模型表现也值得评估。像
    *Chatbot Arena（仅限编码）* 这样的 Elo 排名正是这样做的。这些分数来自 LLM 在编码任务中的对抗性比较，由人类评审员进行评估。模型彼此对抗，根据胜负调整
    Elo 分数。Elo 分数显示了模型相对于池中其他模型的表现，较高的分数意味着更好的表现。例如，100 Elo 分数的差距意味着排名较高的模型在与排名较低的模型对抗时，预计会有约
    64% 的获胜概率。
- en: Current state of model performance
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当前模型表现状态
- en: Now, let’s examine how these models perform when we compare their HumanEval
    scores with their Elo ratings. The following image illustrates the current coding
    landscape for LLMs, where the models are clustered by the companies that created
    them. Each company’s best performing model is annotated.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查一下当我们将这些模型的 HumanEval 分数与它们的 Elo 排名进行比较时，它们的表现如何。以下图像展示了当前 LLM 的编码领域，其中模型按创建它们的公司进行了聚类。每个公司的最佳表现模型都有注释。
- en: '![](../Images/f9f2105027c2e5e83d5b176afc8474ee.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9f2105027c2e5e83d5b176afc8474ee.png)'
- en: 'Figure 1: Elo score by HumanEval — colored by company. X- and y-axis ticks
    show all models released by each company, with the best performing model shown
    in bold.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：按 HumanEval 的 Elo 分数 — 按公司着色。X 轴和 Y 轴的刻度显示了每个公司发布的所有模型，最佳表现的模型用粗体标出。
- en: OpenAI’s models are at the top of both metrics, demonstrating their superior
    capability in solving coding tasks. The top OpenAI model outperforms the best
    non-OpenAI model — Anthropic’s *Claude Sonnet 3.5* — by 46 Elo points , with an
    expected win rate of 56.6% in head-to-head coding tasks , and a 3.9% difference
    in HumanEval. While this difference isn’t overwhelming, it shows that OpenAI still
    has the edge. Interestingly, the best model is *o1-mini*, which scores higher
    than the larger *o1* by 10 Elo points and 2.5% in HumanEval.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的模型在两个指标上都位于最前端，展示了它们在解决编程任务中的优越能力。OpenAI 排名最高的模型在对比非 OpenAI 的最佳模型——Anthropic
    的 *Claude Sonnet 3.5* 时，超出了 46 Elo 分，预计在对决中的胜率为 56.6%，在 HumanEval 中的差距为 3.9%。虽然这个差距并不惊人，但它显示了
    OpenAI 仍然占有优势。有趣的是，表现最好的模型是 *o1-mini*，它比更大的 *o1* 高出 10 Elo 分和 2.5% 的 HumanEval
    得分。
- en: '**Conclusion: OpenAI continues to dominate, positioning themselves at the top
    in benchmark performance and real-world usage. Remarkably, o1-mini is the best
    performing model, outperforming its larger counterpart o1.**'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：OpenAI 继续占据主导地位，在基准测试表现和现实世界应用中位居榜首。值得注意的是，o1-mini 是表现最好的模型，超越了更大的对手 o1。**'
- en: Other companies follow closely behind and seem to exist within the same “performance
    ballpark”. To provide a clearer sense of the difference in model performance,
    the following figure shows the win probabilities of each company’s best model
    — as indicated by their Elo rating.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其他公司紧随其后，似乎都处于相同的“表现区间”内。为了更清晰地了解模型表现的差异，以下图表显示了每家公司最佳模型的胜率——由它们的 Elo 评分指示。
- en: '![](../Images/b2c8383cd3b78fc056360c79317e9527.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2c8383cd3b78fc056360c79317e9527.png)'
- en: 'Figure 2: Win probability of each company’s best (coding) model — as illustrated
    by the Elo ratings’ head-to-head battle win probabilities.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每家公司最佳（编码）模型的获胜概率 — 如 Elo 评分的对决胜率所示。
- en: Mismatch between benchmark results and real-world performance
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试结果与现实世界表现的不匹配
- en: From Figure 1, one thing that stands out is the misalignment between HumanEval
    (benchmark) and the Elo scores (real-world performance). Some models — like Mistral’s
    Mistral Large — have significantly better HumanEval scores relative to their Elo
    rating. Other models — like Google’s Gemini 1.5 Pro — have significantly better
    Elo ratings relative to the HumanEval score they obtain.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 1 可以看出，一个显著的特点是 HumanEval（基准测试）与 Elo 评分（现实世界表现）之间的错配。一些模型——如 Mistral 的 Mistral
    Large——在 HumanEval 得分上显著优于它们的 Elo 评分。其他模型——如 Google 的 Gemini 1.5 Pro——在 Elo 评分上显著优于它们获得的
    HumanEval 得分。
- en: It’s hard to know when to trust benchmarks, as the benchmark data might as well
    be included in the model’s training dataset. This can lead to (overfitted) models
    that memorize and repeat the answer to a coding question, rather than understand
    and actually solve the problem.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 很难知道何时信任基准测试，因为基准数据可能已经包含在模型的训练数据集中。这可能导致（过拟合的）模型记住并重复回答编程问题，而不是理解并真正解决问题。
- en: Similarly, It’s also problematic to take the Elo ratings as a ground truth,
    given that these are scores obtained by a crowdsourcing effort. By doing so, you
    add a human bias to the scoring, favoring models that output in a specific style,
    take a specific approach, … over others, which does not always align with a factually
    better model.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，考虑到 Elo 评分是通过众包努力获得的，因此将其作为事实依据也是有问题的。这样做会在评分中引入人为偏见，偏向那些以特定风格输出、采用特定方法的模型……而忽视其他模型，这并不总是与一个客观更好的模型相符。
- en: '**Conclusion: better benchmark results don’t always reflect better real-world
    performance. It’s advised to look at both independently.**'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：更好的基准测试结果并不总能反映更好的现实世界表现。建议分别查看两者。**'
- en: The following image shows the disagreement between HumanEval and Elo scores.
    All models are *sorted* based on their respective scores, ignoring “how much better”
    one model is compared to another for simplicity. It shows visually which models
    perform better on benchmarks than in real life and vice-versa.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了 HumanEval 和 Elo 评分之间的不一致。所有模型都根据各自的得分进行*排序*，为了简化，忽略了“一个模型相较另一个模型有多好”的因素。它直观地展示了哪些模型在基准测试中表现更好，但在现实中表现较差，反之亦然。
- en: '![](../Images/41d86c99ec635acbd5ffc82012ac298a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41d86c99ec635acbd5ffc82012ac298a.png)'
- en: 'Figure 3: Misalignment in HumanEval and Elo scores — colored by company. Scores
    are transformed to ranks for simplicity, going from worst (left) to best (right)
    on each metric respectively.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：HumanEval 和 Elo 评分的错配 — 按公司颜色区分。为了简化，得分被转换为排名，从最差（左）到最好（右），分别显示在每个指标上。
- en: 'Figure 4 further highlights the difference between benchmarking and real-world
    performance by simplifying the comparison even further. Here, the figure shows
    the relative difference in rank, indicating when a model is likely overfitting
    the benchmark or performs better than reported. Some interesting conclusions can
    be drawn here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4通过进一步简化比较，进一步突出了基准测试和现实世界表现之间的差异。这里，图中显示了排名的相对差异，表明模型何时可能出现过拟合基准，或表现优于报告的结果。可以得出一些有趣的结论：
- en: '**Overfitting on benchmark:** Alibaba and Mistral both stick out for systematically
    creating models that perform better on benchmarks than in real life. Their most
    recent models, Alibaba’s *Qwen 2.5 Coder* (*-20.0%*) and Mistral’s *Mistral Large*
    (*-11.5%*) follow this pattern, too.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基准过拟合：** 阿里巴巴和Mistral两家公司都在不断推出那些在基准测试中表现更好，但在现实中表现较差的模型。它们最近的模型，包括阿里巴巴的*Qwen
    2.5 Coder*（*–20.0%*）和Mistral的*Mistral Large*（*–11.5%*），也遵循了这一模式。'
- en: '**Better than reported:** Google stands out for producing models that perform
    significantly better than reported, with its newest *Gemini 1.5 Pro* model on
    top with a difference of *+31.5%*. Their focus on “honest training and evaluation”
    is evident in their model reporting and the dicision to develop their own Natural2Code
    benchmark instead of using HumanEval. *“Natural2Code is a code generation benchmark
    across Python, Java, C++, JS, Go . Held out dataset HumanEval-like, not leaked
    on the web”* ~ Google in the Gimini 1.5 release.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优于报告结果：** Google凭借其生产的模型，在实际表现上显著优于报告结果，尤其是其最新的*Gemini 1.5 Pro*模型，表现出*+31.5%*的差异。Google对“诚实训练与评估”的关注在其模型报告中得到了体现，并且他们选择开发自己的Natural2Code基准，而非使用HumanEval。*“Natural2Code是一个涵盖Python、Java、C++、JS、Go等语言的代码生成基准。保留的数据集类似于HumanEval，但未泄露在网络上。”*
    ~ Google在Gimmini 1.5发布中。'
- en: '**Well balanced:** It’s very interesting and particular how well and consistently
    Meta nails the balance between benchmark a real-world performance. Of course,
    given that the figure displays rank over score, this stability also depends on
    the performance of other models.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平衡良好：** Meta在平衡基准与现实世界表现方面做得非常有趣且特别好。由于图中显示的是排名而非得分，这种稳定性也取决于其他模型的表现。'
- en: '![](../Images/aa4c2767d606e3c66e364f5de745a537.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa4c2767d606e3c66e364f5de745a537.png)'
- en: 'Figure 4: Performance difference going from HumanEval to Elo scores — colored
    by company. Negative scores indicate better HumanEval than Elo (overfitting on
    benchmark) where positive scores indicate better Elo than HumanEval (better performing
    than reported).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：从HumanEval到Elo分数的表现差异——按公司着色。负分表示HumanEval优于Elo（基准过拟合），而正分表示Elo优于HumanEval（表现优于报告）。
- en: '**Conclusion: Alibaba and Mistral tend to create models that overfit on the
    benchmark data.**'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：阿里巴巴和Mistral倾向于创建在基准数据上过拟合的模型。**'
- en: ''
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conclusion: Google’s models are underrated in benchmark results, due to their
    focus on fair training and evaluation.**'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：Google的模型在基准测试结果中被低估，原因在于其专注于公平的训练和评估。**'
- en: 'Balancing performance and price: the models that provide the best bang for
    buck'
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡性能和价格：提供最佳性价比的模型
- en: When choosing an LLM as your coding companion, performance isn’t the only factor
    to consider. Another important dimension to consider is price. This section re-evaluates
    the different LLMs and compares how well they fare when evaluated on performance
    — as indicated by their Elo rating — and price.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择LLM作为编程伴侣时，性能并不是唯一需要考虑的因素。另一个重要的维度是价格。本节重新评估了不同的LLM，并比较了它们在性能（由其Elo评分表示）和价格方面的表现。
- en: 'Before starting the comparison, it’s worth noting of the odd one out: Meta.
    Meta’s Llama models are open-source and not hosted by Meta themselves. However,
    given their popularity, I include them. The price attached to these models is
    the best pay-as-you-go price offered by the big three cloud vendors (Google, Microsoft,
    Amazon) — which usually comes down to AWS’s price.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始比较之前，值得注意的是一个例外：Meta。Meta的Llama模型是开源的，并非由Meta自己托管。然而，鉴于它们的受欢迎程度，我仍然将其纳入其中。这些模型的定价是大三云服务商（Google、Microsoft、Amazon）提供的最优按需定价——通常是AWS的价格。
- en: Figure 5 compares the different models and shows the Pareto front. Elo ratings
    are used to represent model performance, this seemed the best choice given it’s
    evaluated by humans and doesn’t include an overfitting bias. Next, the pay-as-you-go
    API price is used with the displayed price being the average of input- and output-token
    cost for a total of one million generated tokens.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5对不同的模型进行了比较，并展示了帕累托前沿。使用Elo评分来代表模型表现，这被认为是最好的选择，因为Elo评分是由人类评估的，并且不包括过拟合偏差。接下来，使用按需付费的API价格，所显示的价格为生成一百万个token的输入和输出token费用的平均值。
- en: '![](../Images/06681166d54cbbbcce4e1bfaa2c6590c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06681166d54cbbbcce4e1bfaa2c6590c.png)'
- en: 'Figure 5: Model coding performance (Elo rating) by API price — colored by company.
    The models that make up the Pareto front are annotated.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：按API价格分类的模型编码表现（Elo评分）——按公司着色。组成帕累托前沿的模型已标注。
- en: 'The Pareto front is made up of models coming from only two companies: OpenAI
    and Google. As mentioned in the previous sections, OpenAI’s models dominate in
    performance, and they appear to be fairly priced too. Meanwhile, Google seems
    to focus on lighter weight — thus cheaper — models that still perform well. This
    makes sense, given their focus on on-device LLM use-cases which hold great strategic
    value for their mobile operating system (Android).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托前沿仅由两家公司提供的模型构成：OpenAI和谷歌。正如前面所提到的，OpenAI的模型在性能上占据主导地位，并且它们的定价也相对合理。与此同时，谷歌似乎专注于较轻量——因此更便宜——但仍然表现良好的模型。这是有道理的，因为他们专注于适用于移动操作系统（Android）的设备端LLM应用场景，这对其战略具有重要价值。
- en: '**Conclusion: the Pareto front is made up of models coming from either OpenAI
    (high performance) or Google (good value for money).**'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：帕累托前沿由来自OpenAI（高性能）或谷歌（高性价比）的模型组成。**'
- en: 'The next figure shows a similar trend when using HumanEval instead of Elo scores
    to represent coding performance. Some observations:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图表展示了在使用HumanEval而不是Elo评分来表示编码表现时的相似趋势。一些观察结果：
- en: Anthropic’s *Claude 3.5 Haiku* is the only notable addition, as this model does
    not yet have an Elo rating. Could it be a potential contender for middle-priced,
    high-performance models?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic的*Claude 3.5 Haiku*是唯一的显著新增项，因为该模型目前还没有Elo评分。它有可能成为中端高性能模型的竞争者吗？
- en: The differences for Google’s *Gemini 1.5 Pro* and Mistral’s *Mistral Large*
    are explained in the previous section that compared HumanEval scores with Elo
    ratings.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的*Gemini 1.5 Pro*和Mistral的*Mistral Large*之间的差异在前一节中已经解释过，该节将HumanEval评分与Elo评分进行了对比。
- en: Given that Google’s *Gemini 1.5 Flash 8B* does not have a HumanEval score, it
    is excluded from this figure.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于谷歌的*Gemini 1.5 Flash 8B*没有HumanEval评分，因此它被排除在这个图表之外。
- en: '![](../Images/19e5baf158fc9df2476b763f433d0cde.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19e5baf158fc9df2476b763f433d0cde.png)'
- en: 'Figure 6: Model coding performance (HumanEval score) by API price — colored
    by company. The models that make up the Pareto front are annotated.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：按API价格分类的模型编码表现（HumanEval评分）——按公司着色。组成帕累托前沿的模型已标注。
- en: 'Shifting through the data: additional insights and trends'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析：额外的见解和趋势
- en: 'To conclude, I will discuss some extra insights worth noting in the current
    LLM (coding) landscape. This section explores three key observations: the steady
    improvement of models over time, the continued dominance of proprietary models,
    and the significant impact even minor model updates can have. All the observations
    stem from the Elo rating by price comparison shown in Figure 5.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我将讨论一些在当前LLM（编码）领域值得注意的额外见解。本节探讨了三个关键观察结果：模型随时间的稳定改进、专有模型的持续主导地位以及即便是微小的模型更新也能产生显著影响。所有这些观察结果都来源于图5所示的基于价格对比的Elo评分。
- en: Models are getting better and cheaper
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型越来越好且越来越便宜
- en: The following figure illustrates how new models continue to achieve higher accuracy
    while simultaneously driving down costs. It’s remarkable to see how three time
    segments — 2023 and before, H1 of 2024, and H2 of 2024 — each define their own
    Pareto front and occupy almost completely distinct segments. Curious to see how
    this will continue to progress in 2025!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了新模型如何不断提高准确度，同时降低成本。值得注意的是，三个时间段——2023年及之前、2024年上半年和2024年下半年——每个都定义了自己独特的帕累托前沿，并几乎占据了完全不同的区段。期待看到这一趋势在2025年如何继续发展！
- en: '![](../Images/e402dd3128c2ae5ddb402b2368a3d29b.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e402dd3128c2ae5ddb402b2368a3d29b.png)'
- en: 'Figure 7: Evolution of time as indicated by three different time segments —
    2023 and before, H1 of 2024, and H2 of 2024.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：由三个不同时间段所指示的时间演变——2023年及之前、2024年上半年和2024年下半年。
- en: '**Conclusion: models get systematically better and cheaper, a trend observed
    with almost every new model release.**'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：模型变得越来越好且越来越便宜，这是几乎所有新模型发布时都能观察到的趋势。**'
- en: Proprietary models remain in power
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专有模型依然占据主导地位
- en: The following image shows which of the analyzed models are proprietary and which
    are open-source. We see that proprietary models continue to dominate the LLM coding
    landscape. The Pareto front is still driven by these “closed-source” models, both
    on the high-performing and low-cost ends.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了哪些分析过的模型是专有的，哪些是开源的。我们看到，专有模型仍然主导着 LLM 编程领域。帕累托前沿仍然由这些“闭源”模型主导，无论是在高性能端还是低成本端。
- en: However, open-source models are closing the gap. It’s interesting to see, though,
    that for each open-source model, there is a proprietary model with the same predictive
    performance that is significantly cheaper. This suggests that the proprietary
    are either more lighterweight or better optimized, thus requiring less computational
    power — though this is just a personal hunch.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，开源模型正在缩小差距。有趣的是，对于每个开源模型，都有一个具有相同预测性能且显著更便宜的专有模型。这表明专有模型要么更加轻量化，要么经过了更好的优化，因此需要更少的计算能力——尽管这仅仅是个人的推测。
- en: '![](../Images/88a3f9134ef566b783e8c444111953d2.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88a3f9134ef566b783e8c444111953d2.png)'
- en: 'Figure 8: Proprietary versus open-source models.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：专有模型与开源模型的对比。
- en: '**Conclusion: proprietary models continue to hold the performance-cost Pareto
    front.**'
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：专有模型继续保持性能-成本帕累托前沿。**'
- en: '**Even minor model updates have an effect**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**即使是小的模型更新也会产生影响**'
- en: The following and final image illustrates how even minor updates to the same
    models can have an impact. Most often, these updates bring a performance boost,
    improving the models gradually over time without a major release. Occasionally
    though, a model’s performance might drop for coding tasks following a minor update,
    but this is almost always accompanied by a reduction in price. This is likely
    because the models were optimized in some way, such as through quantization or
    pruning parts of their network.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下这张图片展示了即使是相同模型的小更新也可能带来影响。通常，这些更新会带来性能提升，逐步改进模型，而无需进行重大发布。然而，偶尔某个模型在小更新后可能会在编程任务上表现下降，但这通常伴随着价格的降低。这可能是因为模型以某种方式进行了优化，比如通过量化或剪枝其网络的一部分。
- en: '![](../Images/48c128f76b057bdb0e24e317d58d839b.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48c128f76b057bdb0e24e317d58d839b.png)'
- en: 'Figure 9: Evolution of model performance and price for minor model updates.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：小模型更新的性能和价格演变。
- en: '**Conclusion: minor model updates almost always improve performance or push
    down cost.**'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论：小的模型更新几乎总是能提高性能或降低成本。**'
- en: 'Conclusion: key takeaways of LLMs for coding'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：LLMs 在编程中的关键要点
- en: The LLM landscape for coding is rapidly evolving, with newer models regularly
    pushing the Pareto front toward better-performing and/or cheaper options. Developers
    must stay informed about the latest models to identify those that offer the best
    capabilities within their budget. Recognizing the misalignment between real-world
    results and benchmarks is essential to making informed decisions. By carefully
    weighing performance against cost, developers can choose the tools that best meet
    their needs and stay ahead in this dynamic field.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在编程领域的格局正在迅速发展，新模型不断推动帕累托前沿向更高性能和/或更便宜的选项迈进。开发者必须保持对最新模型的关注，以识别那些在预算内提供最佳功能的模型。认识到现实结果与基准测试之间的不一致性对于做出明智的决策至关重要。通过仔细衡量性能与成本，开发者可以选择最符合自己需求的工具，并在这个动态变化的领域中保持领先。
- en: 'Here’s a quick overview of all the conclusions made in this post:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本文所做的所有结论的快速概览：
- en: Comparing LLM performance is hard.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较 LLM 性能是很困难的。
- en: OpenAI continues to dominate, positioning themselves at the top in benchmark
    performance and real-world usage. Remarkably, o1-mini is the best performing model,
    outperforming its larger counterpart o1.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 继续主导市场，在基准性能和实际应用中位居前列。值得注意的是，o1-mini 是表现最好的模型，超越了更大的 o1 模型。
- en: Better benchmark results don’t always reflect better real-world performance.
    It’s advised to look at both independently.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的基准结果并不总是意味着更好的现实世界表现。建议分别独立查看两者。
- en: Alibaba and Mistral tend to create models that overfit on the benchmark data.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿里巴巴和 Mistral 倾向于创建在基准数据上过拟合的模型。
- en: Google’s models are underrated in benchmark results, due to their focus on fair
    training and evaluation.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的模型在基准测试结果中被低估，因为它们更注重公平训练和评估。
- en: The Pareto front is made up of models coming from either OpenAI (high performance)
    or Google (good value for money).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕累托前沿由来自OpenAI（高性能）或Google（物有所值）的模型组成。
- en: Models get systematically better and cheaper, a trend observed with almost every
    new model release.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在不断系统性地提高性能并降低成本，这一趋势几乎出现在每一次新模型发布中。
- en: Proprietary models continue to hold the performance-cost Pareto front.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专有模型仍然占据着性能-成本的帕累托前沿。
- en: Minor model updates almost always improve performance or push down cost.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小幅的模型更新几乎总是能提高性能或降低成本。
- en: Found this useful? Feel free to follow me on [**LinkedIn**](https://www.linkedin.com/in/rubenbroekx/)
    to see my next explorations!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 觉得有用吗？欢迎在[**LinkedIn**](https://www.linkedin.com/in/rubenbroekx/)上关注我，查看我接下来的探索！
- en: The images shown in this article were created by myself, the author, unless
    specified otherwise.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中展示的图像由我本人创作，除非另有说明。
