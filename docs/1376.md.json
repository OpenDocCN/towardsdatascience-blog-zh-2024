["```py\nexplain codegen\nselect\n    id,\n    (id > 1 and id > 2) and (id < 1000 or (id + id) = 12) as test  \nfrom\n    range(0, 10000, 1, 32)\n```", "```py\n|== Physical Plan ==\n* Project (2)\n+- * Range (1)\n\n(1) Range [codegen id : 1]\nOutput [1]: [id#36167L]\nArguments: Range (0, 10000, step=1, splits=Some(32))\n\n(2) Project [codegen id : 1]\nOutput [2]: [id#36167L, (((id#36167L > 1) AND (id#36167L > 2)) AND ((id#36167L < 1000) OR ((id#36167L + id#36167L) = 12))) AS test#36161]\nInput [1]: [id#36167L]\n```", "```py\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean range_initRange_0;\n/* 010 */   private long range_nextIndex_0;\n/* 011 */   private TaskContext range_taskContext_0;\n/* 012 */   private InputMetrics range_inputMetrics_0;\n/* 013 */   private long range_batchEnd_0;\n/* 014 */   private long range_numElementsTodo_0;\n/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] range_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];\n/* 016 */\n/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n/* 018 */     this.references = references;\n/* 019 */   }\n/* 020 */\n/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 022 */     partitionIndex = index;\n/* 023 */     this.inputs = inputs;\n/* 024 */\n/* 025 */     range_taskContext_0 = TaskContext.get();\n/* 026 */     range_inputMetrics_0 = range_taskContext_0.taskMetrics().inputMetrics();\n/* 027 */     range_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n/* 028 */     range_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 0);\n/* 029 */     range_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);\n/* 030 */\n/* 031 */   }\n/* 032 */\n/* 033 */   private void project_doConsume_0(long project_expr_0_0) throws java.io.IOException {\n/* 034 */     // common sub-expressions\n/* 035 */\n/* 036 */     boolean project_value_4 = false;\n/* 037 */     project_value_4 = project_expr_0_0 > 1L;\n/* 038 */     boolean project_value_3 = false;\n/* 039 */\n/* 040 */     if (project_value_4) {\n/* 041 */       boolean project_value_7 = false;\n/* 042 */       project_value_7 = project_expr_0_0 > 2L;\n/* 043 */       project_value_3 = project_value_7;\n/* 044 */     }\n/* 045 */     boolean project_value_2 = false;\n/* 046 */\n/* 047 */     if (project_value_3) {\n/* 048 */       boolean project_value_11 = false;\n/* 049 */       project_value_11 = project_expr_0_0 < 1000L;\n/* 050 */       boolean project_value_10 = true;\n/* 051 */\n/* 052 */       if (!project_value_11) {\n/* 053 */         long project_value_15 = -1L;\n/* 054 */\n/* 055 */         project_value_15 = project_expr_0_0 + project_expr_0_0;\n/* 056 */\n/* 057 */         boolean project_value_14 = false;\n/* 058 */         project_value_14 = project_value_15 == 12L;\n/* 059 */         project_value_10 = project_value_14;\n/* 060 */       }\n/* 061 */       project_value_2 = project_value_10;\n/* 062 */     }\n/* 063 */     range_mutableStateArray_0[2].reset();\n/* 064 */\n/* 065 */     range_mutableStateArray_0[2].write(0, project_expr_0_0);\n/* 066 */\n/* 067 */     range_mutableStateArray_0[2].write(1, project_value_2);\n/* 068 */     append((range_mutableStateArray_0[2].getRow()));\n/* 069 */\n/* 070 */   }\n/* 071 */\n/* 072 */   private void initRange(int idx) {\n/* 073 */     java.math.BigInteger index = java.math.BigInteger.valueOf(idx);\n/* 074 */     java.math.BigInteger numSlice = java.math.BigInteger.valueOf(32L);\n/* 075 */     java.math.BigInteger numElement = java.math.BigInteger.valueOf(10000L);\n/* 076 */     java.math.BigInteger step = java.math.BigInteger.valueOf(1L);\n/* 077 */     java.math.BigInteger start = java.math.BigInteger.valueOf(0L);\n/* 078 */     long partitionEnd;\n/* 079 */\n/* 080 */     java.math.BigInteger st = index.multiply(numElement).divide(numSlice).multiply(step).add(start);\n/* 081 */     if (st.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n/* 082 */       range_nextIndex_0 = Long.MAX_VALUE;\n/* 083 */     } else if (st.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n/* 084 */       range_nextIndex_0 = Long.MIN_VALUE;\n/* 085 */     } else {\n/* 086 */       range_nextIndex_0 = st.longValue();\n/* 087 */     }\n/* 088 */     range_batchEnd_0 = range_nextIndex_0;\n/* 089 */\n/* 090 */     java.math.BigInteger end = index.add(java.math.BigInteger.ONE).multiply(numElement).divide(numSlice)\n/* 091 */     .multiply(step).add(start);\n/* 092 */     if (end.compareTo(java.math.BigInteger.valueOf(Long.MAX_VALUE)) > 0) {\n/* 093 */       partitionEnd = Long.MAX_VALUE;\n/* 094 */     } else if (end.compareTo(java.math.BigInteger.valueOf(Long.MIN_VALUE)) < 0) {\n/* 095 */       partitionEnd = Long.MIN_VALUE;\n/* 096 */     } else {\n/* 097 */       partitionEnd = end.longValue();\n/* 098 */     }\n/* 099 */\n/* 100 */     java.math.BigInteger startToEnd = java.math.BigInteger.valueOf(partitionEnd).subtract(\n/* 101 */       java.math.BigInteger.valueOf(range_nextIndex_0));\n/* 102 */     range_numElementsTodo_0  = startToEnd.divide(step).longValue();\n/* 103 */     if (range_numElementsTodo_0 < 0) {\n/* 104 */       range_numElementsTodo_0 = 0;\n/* 105 */     } else if (startToEnd.remainder(step).compareTo(java.math.BigInteger.valueOf(0L)) != 0) {\n/* 106 */       range_numElementsTodo_0++;\n/* 107 */     }\n/* 108 */   }\n/* 109 */\n/* 110 */   protected void processNext() throws java.io.IOException {\n/* 111 */     // initialize Range\n/* 112 */     if (!range_initRange_0) {\n/* 113 */       range_initRange_0 = true;\n/* 114 */       initRange(partitionIndex);\n/* 115 */     }\n/* 116 */\n/* 117 */     while (true) {\n/* 118 */       if (range_nextIndex_0 == range_batchEnd_0) {\n/* 119 */         long range_nextBatchTodo_0;\n/* 120 */         if (range_numElementsTodo_0 > 1000L) {\n/* 121 */           range_nextBatchTodo_0 = 1000L;\n/* 122 */           range_numElementsTodo_0 -= 1000L;\n/* 123 */         } else {\n/* 124 */           range_nextBatchTodo_0 = range_numElementsTodo_0;\n/* 125 */           range_numElementsTodo_0 = 0;\n/* 126 */           if (range_nextBatchTodo_0 == 0) break;\n/* 127 */         }\n/* 128 */         range_batchEnd_0 += range_nextBatchTodo_0 * 1L;\n/* 129 */       }\n/* 130 */\n/* 131 */       int range_localEnd_0 = (int)((range_batchEnd_0 - range_nextIndex_0) / 1L);\n/* 132 */       for (int range_localIdx_0 = 0; range_localIdx_0 < range_localEnd_0; range_localIdx_0++) {\n/* 133 */         long range_value_0 = ((long)range_localIdx_0 * 1L) + range_nextIndex_0;\n/* 134 */\n/* 135 */         project_doConsume_0(range_value_0);\n/* 136 */\n/* 137 */         if (shouldStop()) {\n/* 138 */           range_nextIndex_0 = range_value_0 + 1L;\n/* 139 */           ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localIdx_0 + 1);\n/* 140 */           range_inputMetrics_0.incRecordsRead(range_localIdx_0 + 1);\n/* 141 */           return;\n/* 142 */         }\n/* 143 */\n/* 144 */       }\n/* 145 */       range_nextIndex_0 = range_batchEnd_0;\n/* 146 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(range_localEnd_0);\n/* 147 */       range_inputMetrics_0.incRecordsRead(range_localEnd_0);\n/* 148 */       range_taskContext_0.killTaskIfInterrupted();\n/* 149 */     }\n/* 150 */   }\n/* 151 */\n/* 152 */ }\n```", "```py\n select /* #3 */\n    Imagepath,\n    CommandLine,\n    PID,\n    map_keys(map_filter(results_map, (k,v) -> v = TRUE)) as matching_rules\nfrom (\n    select /* #2 */\n        *,\n        map('rule1', rule1, 'rule2', rule2) as results_map\n    from (\n        select /* #1 */\n            *,\n            (lower_Imagepath like '%schtasks.exe') as rule1,\n            (lower_Imagepath like 'd:%') as rule2\n        from (\n            select \n                lower(PID) as lower_PID,\n                lower(CommandLine) as lower_CommandLine,\n                lower(Imagepath) as lower_Imagepath,\n                *\n            from (\n                select\n                    uuid() as PID,\n                    uuid() as CommandLine,\n                    uuid() as Imagepath,\n                    id \n                from\n                    range(0, 10000, 1, 32)\n            )\n        )\n    )\n)\n```", "```py\n == Physical Plan ==\nProject (4)\n+- * Project (3)\n   +- * Project (2)\n      +- * Range (1)\n\n...\n\n(4) Project\nOutput [4]: [Imagepath#2, CommandLine#1, PID#0, map_keys(map_filter(map(rule1, EndsWith(lower_Imagepath#5, schtasks.exe), rule2, StartsWith(lower_Imagepath#5, d:)), lambdafunction(lambda v#12, lambda k#11, lambda v#12, false))) AS matching_rules#9]\nInput [4]: [lower_Imagepath#5, PID#0, CommandLine#1, Imagepath#2]\n```", "```py\n+--------------------+--------------------+--------------------+--------------+\n|           Imagepath|         CommandLine|                 PID|  matched_rule|\n+--------------------+--------------------+--------------------+--------------+\n|09401675-dc09-4d0...|6b8759ee-b55a-486...|44dbd1ec-b4e0-488...|         rule1|\n|e2b4a0fd-7b88-417...|46dd084d-f5b0-4d7...|60111cf8-069e-4b8...|         rule1|\n|1843ee7a-a908-400...|d1105cec-05ef-4ee...|6046509a-191d-432...|         rule2|\n+--------------------+--------------------+--------------------+--------------+\n```", "```py\n select\n    Imagepath,\n    CommandLine,\n    PID,\n    matched_rule\nfrom (\n    select\n        *,\n        explode(matching_rules) as matched_rule\n    from (\n        /* original statement */\n    )\n)\n```", "```py\n== Physical Plan ==\n* Project (7)\n+- * Generate (6)\n   +- Project (5)\n      +- * Project (4)\n         +- Filter (3)\n            +- * Project (2)\n               +- * Range (1)\n\n...\n\n(3) Filter\nInput [3]: [PID#34, CommandLine#35, Imagepath#36]\nCondition : (size(map_keys(map_filter(map(rule1, EndsWith(lower(Imagepath#36),\n schtasks.exe), rule2, StartsWith(lower(Imagepath#36), d:)), \nlambdafunction(lambda v#47, lambda k#46, lambda v#47, false))), true) > 0)\n...\n\n(6) Generate [codegen id : 3]\nInput [4]: [PID#34, CommandLine#35, Imagepath#36, matching_rules#43]\nArguments: explode(matching_rules#43), [PID#34, CommandLine#35, Imagepath#36], false, [matched_rule#48]\n\n(7) Project [codegen id : 3]\nOutput [4]: [Imagepath#36, CommandLine#35, PID#34, matched_rule#48]\nInput [4]: [PID#34, CommandLine#35, Imagepath#36, matched_rule#48]\n```", "```py\n select\n    Imagepath,\n    CommandLine,\n    PID,\n    matched_rule\nfrom (\n    select\n        *\n    from (\n        select\n            *,\n            explode(results_map) as (matched_rule, matched_result)\n        from (\n            /* original statement */\n        )\n    )\n    where\n        matched_result = TRUE\n)\n```", "```py\n == Physical Plan ==\n* Project (8)\n+- * Filter (7)\n   +- * Generate (6)\n      +- * Project (5)\n         +- * Project (4)\n            +- * Filter (3)\n               +- * Project (2)\n                  +- * Range (1)\n```", "```py\nCaused by: org.codehaus.commons.compiler.InternalCompilerException: Code grows beyond 64 KB\n```", "```py\nspark.sql(\"SET spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.InferFiltersFromGenerate\")\n```", "```py\n == Physical Plan ==\n* Project (6)\n+- * Generate (5)\n   +- Project (4)\n      +- * Project (3)\n         +- * Project (2)\n            +- * Range (1)\n\n== Physical Plan ==\n* Project (7)\n+- * Filter (6)\n   +- * Generate (5)\n      +- * Project (4)\n         +- * Project (3)\n            +- * Project (2)\n               +- * Range (1)\n```"]