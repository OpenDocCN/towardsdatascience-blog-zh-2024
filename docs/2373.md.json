["```py\n# Shift-by-1 so that tokens < n predict n\nshift_logits = logits[..., :-1, :].contiguous()\nshift_labels = labels[..., 1:].contiguous()\n\n# Flatten the tensors\nshift_logits = shift_logits.view(-1, self.config.vocab_size)\nshift_labels = shift_labels.view(-1)\n\n# Enable model parallelism\nshift_labels = shift_labels.to(shift_logits.device)\n\n# Compute loss\nloss_fct = CrossEntropyLoss()\nloss = loss_fct(shift_logits, shift_labels)\n```", "```py\nloss_fct = CrossEntropyLoss(reduction=\"none\")\n```", "```py\nclass PLWTrainer(Trainer):\n    def __init__(self, *args, prompt_loss_weight=1.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.plw = prompt_loss_weight\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # get outputs without computing loss (by not passing in labels)\n        outputs = model(input_ids=inputs[\"input_ids\"], \n                        attention_mask=inputs[\"attention_mask\"])\n        logits = outputs.get(\"logits\")\n        labels = inputs.pop(\"labels\")\n\n        # compute per-token weights\n        weights = self.plw * inputs[\"prompt_mask\"] + inputs[\"completion_mask\"]\n\n        # Shift-by-1 so that tokens < n predict n\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        shift_weights = weights[..., 1:].contiguous()\n\n        # Enable model parallelism\n        shift_labels = shift_labels.to(shift_logits.device)\n        shift_weights = shift_weights.to(shift_logits.device)\n\n        # Compute per-token losses\n        loss_fct = CrossEntropyLoss(reduction=\"none\")\n        token_losses = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), \n                                shift_labels.view(-1))\n\n        # Compute weighted average of losses\n        loss = token_losses @ shift_weights.view(-1) / shift_weights.sum()\n        return (loss, outputs) if return_outputs else loss\n```", "```py\n# from preprocess_logits_for_metrics\npredictions = logits.argmax(-1)[..., :-1]\n```", "```py\n# uses PyTorch tensors (on GPU)\ndef preprocess_logits_for_metrics(logits, labels):\n    # get predictions\n    token_preds = logits.argmax(-1)[..., :-1]\n\n    # compute per-token losses\n    loss_fct = CrossEntropyLoss(reduction=\"none\")\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = labels[..., 1:].contiguous()\n    token_losses = loss_fct(shift_logits.transpose(1, 2), shift_labels)\n\n    # pass predictions and losses to compute_metrics()\n    predictions = (token_preds, token_losses)\n    return predictions\n```", "```py\n# uses numpy arrays (on CPU)\ndef compute_metrics(data):\n    # data.predictions contains the tuple (token_preds, token_losses)\n    # from preprocess_logits_for_metrics()\n    token_preds, token_losses = data.predictions\n\n    # shift labels and masks\n    labels = data.label_ids[..., 1:]\n    shift_prompt_mask = prompt_mask[..., 1:]\n    shift_comp_mask = completion_mask[..., 1:]\n\n    # average both losses (prompt and completion) over their respective tokens\n    prompt_loss = token_losses.reshape(-1) @ shift_prompt_mask.reshape(-1) / shift_prompt_mask.sum()\n    completion_loss = token_losses.reshape(-1) @ shift_comp_mask.reshape(-1) / shift_comp_mask.sum()\n\n    # compute response token accuracy\n    nz = np.nonzero(shift_comp_mask)\n    idx = np.where(np.isin(labels[nz], ABCD_token_ids))\n    accuracy = np.mean(preds[nz][idx] == labels[nz][idx])\n\n    return {\n        'comp_loss': completion_loss,\n        'prompt_loss': prompt_loss,\n        'acc': accuracy,\n    }\n```"]