<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Scaling AI Models Like You Mean It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Scaling AI Models Like You Mean It</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10">https://towardsdatascience.com/scaling-ai-models-like-you-mean-it-3afa56c1e14b?source=collection_archive---------5-----------------------#2024-04-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e485" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Strategies for Overcoming the Challenges of Scaling Open-Source AI Models in Production</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sean Sheng" class="l ep by dd de cx" src="../Images/ae58cf760ce5c482e7a6614995b8b8e1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ZfrE9gS9kN3hpaTuZRbBCA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ssheng?source=post_page---byline--3afa56c1e14b--------------------------------" rel="noopener follow">Sean Sheng</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3afa56c1e14b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="4b07" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you’re reading this article, you probably need no introduction to the advantages of deploying open-source models. Over the past couple of years, we have seen incredible growth in the both the quantity and quality of open source models.</p><ul class=""><li id="66a5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Platforms such as Hugging Face have democratized access to a wide array of models, including Large Language Models (LLMs) and diffusion models, empowering developers to innovate freely and efficiently.</li><li id="c441" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">Developers enjoy greater autonomy, as they can fine-tune and combine different models at will, leading to innovative approaches like Retrieval-Augmented Generation (RAG) and the creation of advanced agents.</li><li id="68f5" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">From an economic perspective, open-source models provide substantial cost savings, enabling the use of smaller, specialized models that are more budget-friendly compared to general-purpose models like GPT-4.</li></ul><p id="a4e4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Open-source models present an attractive solution, but what’s the next hurdle? Unlike using a model endpoint like OpenAI, where the model is a scalable black box behind the API, deploying your own open-source models introduces scaling challenges. It’s crucial to ensure that your model scales effectively with production traffic and maintains a seamless experience during traffic spikes. Additionally, it’s important to manage costs efficiently, so you only pay for what you use and avoid any financial surprises at the end of the month.</p><h1 id="6fcf" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">True north: Serverless functions for GPUs</h1><p id="e3e1" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Interestingly, this sounds like a challenge that modern serverless architectures, like AWS Lambda, have already solved — a solution that have existed for almost a decade. However, when it comes to AI model deployment, this isn’t quite the case.</p><p id="44e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The limitations of serverless functions for AI deployments are multifaceted.</p><ul class=""><li id="02d4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">No GPU support</strong>. Platforms like AWS Lambda don’t support GPU. This isn’t merely a technical oversight; it’s rooted in architectural and practical considerations.</li><li id="77c6" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">GPUs cannot be easily shared.</strong> GPUs, while highly parallelizable as devices, is not as flexible in handling multiple inference tasks on different models simultaneously.</li><li id="6ae2" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">GPUs are expensive</strong>. They’re exceptional for model inferencetasks but costly to maintain, especially if not utilized continuously.</li></ul><p id="68bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, let’s take a look at our scaling journey and the important lessons we have learned along the way.</p><h1 id="b2ab" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">The cold start problem</h1><p id="e5a4" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Before we could even begin to work on scaling, we have the notorious “cold start” problem. This issue presents itself in three different stages:</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op oq"><img src="../Images/a91758d3ec08a5076eb6b8d716db9fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OL5eErDqMaNVHvqz.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Breakdown of the cold start problem. Image by the author.</figcaption></figure><ol class=""><li id="0f37" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ph ng nh bk"><strong class="ml fr">Cloud provisioning</strong>: This phase involves the time it takes for a cloud provider to allocate an instance and integrate it into our cluster. This process varies widely, ranging from as quick as 30 seconds to several minutes, and in some cases, even hours, especially for high-demand instances like the Nvidia A100 and H100 GPUs.</li><li id="fe0e" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne ph ng nh bk"><strong class="ml fr">Container image pulling</strong>: Unlike simple Python job images, AI model serving images are very complex, due to the dependencies and custom libraries they require. Although cloud providers boast multi-gigabit network bandwidth, our experience often saw download speeds far below them, with image pulling time about 3 minutes.</li><li id="c51a" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne ph ng nh bk"><strong class="ml fr">Model loading</strong>. The time required here is largely dependent on the model’s size, with larger models like LLMs and diffusion models taking significantly longer time due to their billions of parameters. For example, loading a 5GB model like Stable Diffusion 2 might take approximately 1.3 minutes with 1Gbps network bandwidth, while larger models like Llama 13B and Mixtral 8x7B could require 3.5 minutes and 12.5 minutes respectively.</li></ol><p id="8fd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each phase of the cold start issue demands specific strategies to minimize delays. In the following sections, we’ll explore each of them in more detail, sharing our strategies and solutions.</p><h2 id="c6fc" class="pi no fq bf np pj pk pl ns pm pn po nv ms pp pq pr mw ps pt pu na pv pw px py bk">Cloud provisioning</h2><p id="7a1d" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">In contrast to the homogeneous environment of serverless CPUs, managing a diverse range of compute instance types is crucial when dealing with GPUs, each tailored for specific use cases. For instance, IO-bound LLMs require high GPU memory bandwidth and capacity, while generative models need more powerful GPU compute.</p><p id="9112" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Ensuring availability during peak traffic by maintaining all GPU instance types could lead to prohibitively high costs. To avoid the financial strain of idle instances, we implemented a “standby instances” mechanism. Rather than preparing for the maximum potential load, we maintained a calculated number of standby instances that match the incremental scaling step sizes. For example, if we scale by two GPUs at a time, we need to have two standby instances ready. This allows us to quickly add resources to our serving fleet as demand surges, significantly reducing wait time, while keeping cost manageable.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op pz"><img src="../Images/f06c07c4b29d12ce233597218a55b698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5CfN5iEju2-rn6tF.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image by the author.</figcaption></figure><p id="007d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a multi-tenant environment, where multiple teams or, in our case, multiple organizations, share a common resource pool, we can achieve more efficient utilization rates. This shared environment allows us to balance varying resource demands, contributing to improved cost efficiency. However, managing multi-tenancy introduces challenges, such as enforcing quotas and ensuring network isolation, which can add complexity to the cluster.</p><h2 id="923b" class="pi no fq bf np pj pk pl ns pm pn po nv ms pp pq pr mw ps pt pu na pv pw px py bk">Container image pulling</h2><p id="b6a7" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Serverless CPU workloads often use lightweight images, like the Python slim image (around 154 MB). In stark contrast, a container image built for serving an LLM can be much larger (6.7 GB); the bulk of this size comes from the various dependencies required to run the AI model.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qa"><img src="../Images/46e47cc296d52e7e9f4085be53b44b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*799lkwOqT-BYZiKP.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image by the author.</figcaption></figure><p id="f7db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Despite high-bandwidth networks advertised by cloud providers, the reality often falls short, with actual download speeds being a fraction of the promised rates.</p><p id="b951" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Practically, a significant portion of the files were never used. One way is to optimize the container image itself, but that quickly proved to be unmanageable. Instead, we shifted our focus to an on-demand file pulling approach. Specifically, we first downloaded only the image metadata, with the actual remote files being fetched later as needed. In addition, we leveraged peer-to-peer networking within the cluster to dramatically increase pulling efficiency.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qb"><img src="../Images/ae31e765f9e62607e7e19c9024344b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*We_KIJXR5gdsBOe-.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Container image metadata can be pull in seconds. Image by the author.</figcaption></figure><p id="417e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With these optimizations, we reduced the image pulling time from several minutes to mere seconds. However, we all know this measurement is “cheating” since the actual files are not pulled at this stage. The real file pulling occurs when the service runs. Therefore, it’s crucial to have a service framework that allows you to define behaviors at various lifecycle stages, such as initialization and serving. By doing all of the bootstrapping during initialization, we can ensure that all file dependencies are pulled. This way, when it comes to serving time, there are no delays caused by file pulling.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qc"><img src="../Images/afd20effc8387a63201471c4453e6382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x6hJUxbPegtJqt_H.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Service framework that enables service initialization and API definitions. Image by the author.</figcaption></figure><p id="6a40" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the above example, model loading is done during the initialization lifecycle within <code class="cx qd qe qf qg b">__init__</code> and serving happens within the <code class="cx qd qe qf qg b">@bentoml.api</code> named <code class="cx qd qe qf qg b">txt2img</code>.</p><h2 id="a986" class="pi no fq bf np pj pk pl ns pm pn po nv ms pp pq pr mw ps pt pu na pv pw px py bk">Model loading</h2><p id="4d96" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Initially, the most straightforward method for model loading was to fetch it directly from a remote store like Hugging Face. Using Content Delivery Networks (CDNs), NVMe SSDs, and shared memory, we could remove some of the bottlenecks. While this worked, it was far from optimal.</p><p id="3279" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To improve this process, we considered using in-region network bandwidth. We seeded models in our distributed file systems and broke them into smaller chunks, allowing for parallel downloads. This drastically improved performance, but we still encountered cloud provider’s network bandwidth bottlenecks.</p><p id="b760" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In response, we further optimized to leverage in-cluster network bandwidth by using peer-to-peer sharing and tapping into local caches. While the improvements were substantial, they added a layer of complexity to the process, which we need to abstract away from the developers.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div class="oo op qh"><img src="../Images/2ed77644411ae062984ed93fc5af4986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/0*OnXqZdd22lnaV5tR.png"/></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image by the author.</figcaption></figure><p id="7d86" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even with the above practices, we still suffer from a sequential bottleneck: the need to wait for each step to complete before proceeding with the next. Models had to be downloaded to persistent drive entirely before loading into CPU memory, and then into the GPU.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qi"><img src="../Images/d24c7294db86b83eabfdd2ec71787907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gjzKBXni-LJPMt2x.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image by the author.</figcaption></figure><p id="76c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We turned to a stream-based method for loading model weights, using the distributed file cache system we had in place. This system allows programs to operate as if all files were logically available on disk. In reality, the required data is fetched on-demand from remote storage therefore bypassed disk writing. By leveraging a format like <a class="af qj" href="https://github.com/huggingface/safetensors" rel="noopener ugc nofollow" target="_blank">Safetensors</a>, we can efficiently load the model weights into the main memory through memory mapping (mmap) before loading to the GPU memory in a streaming fashion.</p><p id="4456" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Moreover, we adopted asynchronous writing to disk. By doing so, we created a faster-access cache layer on the local disk. Thus, new deployments with only code changes could bypass the slower remote storage fetch phase, reading the model weights from local cache directly.</p><p id="f64e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To summarize, we managed to optimize the cold start time and we were happy with the results:</p><ul class=""><li id="c8b4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">No cloud provision delay</strong> with standby instances.</li><li id="d761" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Faster container image pulling</strong> with on-demand and peer-to-peer streaming.</li><li id="cacb" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Accelerated model loading</strong> time with distributed file systems, peer-to-peer caching, and streamed loading to GPU memory.</li><li id="6eb1" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Parallelized</strong> image pulling and model loading enabled by service framework.</li></ul><h1 id="626d" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Scaling metrics</h1><p id="51eb" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Next, we need to identify the most indicative signal for scaling AI model deployments on GPUs.</p><h2 id="0c2f" class="pi no fq bf np pj pk pl ns pm pn po nv ms pp pq pr mw ps pt pu na pv pw px py bk">Resource utilization metrics</h2><p id="08e4" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Initially, we considered CPU utilization. It’s straightforward and has an intuitive default threshold, such as 80%. However, the obvious drawback is that CPU metrics don’t capture GPU utilization. Additionally, the Global Interpreter Lock (GIL) in Python limits parallelism, preventing high CPU utilization on multi-core instances, making CPU utilization a less feasible metric.</p><p id="462d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also explored GPU utilization as a more direct measure of our models’ workloads. However, we encountered an issue: the GPU utilization reported by tools like <code class="cx qd qe qf qg b">nvml</code> didn't accurately represent the actual utilization of the GPU. This metric samples kernel usage over a period of time, and a GPU is considered utilized if at least one kernel is executing. This aligns with our observation that better performance can often be achieved through improved batching, even though the GPU device was already reported as having high utilization.</p><blockquote class="qk ql qm"><p id="1066" class="mj mk qn ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="fq">Note</em></strong><em class="fq">: According to </em><a class="af qj" href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries" rel="noopener ugc nofollow" target="_blank"><em class="fq">the NVIDIA documentation</em></a><em class="fq">, utilization.gpu means “Percent of time over the past sample period during which one or more kernels was executing on the GPU. The sample period may be between 1 second and 1/6 second depending on the product”.</em></p></blockquote><p id="48b5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Resource-based metrics are inherently retrospective as they only reflect usage after the resources have been consumed. They’re also capped at 100%, which presents a problem: when scaling based on these metrics, the maximum ratio for adjustment is typically the current utilization over the desired threshold (see scaling formula below). This results in a conservative scale-up behavior that doesn’t necessarily match the actual demand of production traffic.</p><pre class="or os ot ou ov qo qg qp bp qq bb bk"><span id="ab0a" class="qr no fq qg b bg qs qt l qu qv">desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]</span></pre><h2 id="a8df" class="pi no fq bf np pj pk pl ns pm pn po nv ms pp pq pr mw ps pt pu na pv pw px py bk">Request-based metrics</h2><p id="2211" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">We turned to request-based metrics for more proactive signaling that are also not capped at a 100%.</p><p id="09f8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">QPS is a widely recognized metric for its simplicity. However, its application in generative AI, such as with LLMs, is still a question. QPS is not easy to configure and due to the variable cost per request, which depends on the number of tokens processed and generated, using QPS as a scaling metric can lead to inaccuracies.</p><p id="a3c9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Concurrency, on the other hand, has proven to be an ideal metric for reflecting the actual load on the system. It represents the number of active requests either queued or being processed. This metric:</p><ul class=""><li id="e7e5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Precisely reflects the load on the system. <a class="af qj" href="https://en.wikipedia.org/wiki/Little%27s_law" rel="noopener ugc nofollow" target="_blank">Little’s Law</a>, which states that QPS multiplied by average latency equals concurrency, provides an elegant way to understand the relationship between QPS and concurrency. In practice, the average latency per request is rather unknown in model serving. However, by measuring concurrency, we don’t need to calculate average latency.</li><li id="1ab4" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">Accurately calculate the desired replicas using the scaling formula. Allowing the deployment to directly scale to the ideal size without intermediate steps.</li><li id="ca64" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">Easy to configure based on batch size. For non-batchable models, it’s simply the number of GPUs, since each can only handle one generation task at a time. For models that support batching, the batch size determines the concurrency level.</li></ul><p id="7b93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For concurrency to work, we need the support from the service framework to automatically instrument concurrency as a metric and serve it as a scaling signal for the deployment platform. We must also establish right scaling policies to help against overzealous scale-up during a traffic spike or premature scale-down when traffic is sparse.</p><h1 id="e7af" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Request queue</h1><p id="91dd" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">A another important mechanism we integrated with concurrency is the request queue. It acts as a buffer and an orchestrator, ensuring that incoming requests are handled efficiently and without overloading any single server replica.</p><p id="9917" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a scenario without a request queue, all incoming requests are dispatched directly to the server (6 requests in the image below). If multiple requests arrive simultaneously, and there’s only one active server replica, it becomes a bottleneck. The server tries to process each request in a first-come-first-serve manner, often leading to timeouts and a bad client experience.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qw"><img src="../Images/fbca33197fc380dd90ab8b0fc7f2b698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fttyHjlZWOzp0fTv.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image by the author.</figcaption></figure><p id="6e9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Conversely, with a request queue in place, the server consumes requests at an optimal rate, processing at a rate based on the <em class="qn">concurrency</em> defined for the service. When additional server replicas scale up, they too begin to pull from the queue. This mechanism prevents any single server from becoming overwhelmed and allows for a smoother, more manageable distribution of requests across the available infrastructure.</p><h1 id="1440" class="nn no fq bf np nq nr gq ns nt nu gt nv nw nx ny nz oa ob oc od oe of og oh oi bk">Conclusions</h1><p id="964c" class="pw-post-body-paragraph mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne fj bk">Our journey in exploring AI model scaling solutions has been an adventure, which has led us to ultimately create the scaling experience on BentoCloud — a platform that encapsulates all our learnings.</p><p id="3581" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To avoid the impression of a promotion, we’ll illustrate our point with a picture that’s worth a thousand words. The monitoring dashboard below demonstrates the correlation between incoming requests and the scaling up of server instances.</p><p id="6684" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Equally important to scaling up is the ability to scale down. As the requests waned to zero, the deployment reduced the number of active instances accordingly. This ability ensures that no unnecessary costs are incurred for unused resources, aligning expenditure with actual usage.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op qx"><img src="../Images/7aedb48ff2a14a63c2d7a79d5738bd22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E5cOlyCuLe2mY4tW.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">BentoCloud monitoring dashboard. Image by the author.</figcaption></figure><p id="6ab4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We hope the takeaway is that scaling for model deployments should be considered an important aspect of production applications. Unlike scaling CPU workloads, scaling model deployments on GPUs presents unique challenges, including cold start times, configuring scaling metrics, and orchestrating requests. When evaluating deployment platforms, their solutions to these challenges should be thoroughly assessed.</p></div></div></div></div>    
</body>
</html>