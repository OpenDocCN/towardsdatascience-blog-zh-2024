<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Lexicon-Based Sentiment Analysis Using R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Lexicon-Based Sentiment Analysis Using R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13">https://towardsdatascience.com/lexicon-based-sentiment-analysis-using-r-5c1db85984a1?source=collection_archive---------13-----------------------#2024-02-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8984" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An empirical analysis of sentiments conveyed through media briefings during the COVID-19 pandemic</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Okan Bulut" class="l ep by dd de cx" src="../Images/555a4b1818ac0b5d0766f3ad7ab71a6f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Kfv4ax3PFwc2w1SSh8vi4A.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://drokanbulut.medium.com/?source=post_page---byline--5c1db85984a1--------------------------------" rel="noopener follow">Okan Bulut</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5c1db85984a1--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/5eb08923969d0e60e8b3c64f98ab28eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Of1fqWi7LHMUnqSlCXuASQ.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by <a class="af nb" href="https://pixabay.com/users/absolutvision-6158753/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2979107" rel="noopener ugc nofollow" target="_blank">Gino Crescoli</a> from <a class="af nb" href="https://pixabay.com//?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2979107" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="cd33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During the COVID-19 pandemic, I decided to learn a new statistical technique to keep my mind occupied rather than constantly immersing myself in pandemic-related news. After evaluating several options, I found the concepts related to natural language processing (NLP) particularly captivating. So, I opted to delve deeper into this field and explore one specific technique: sentiment analysis, also known as “opinion mining” in academic literature. This analytical method empowers researchers to extract and interpret the emotions conveyed toward a specific subject within the written text. Through sentiment analysis, one can discern the polarity (positive or negative), nature, and intensity of sentiments expressed across various textual formats such as documents, customer reviews, and social media posts.</p><p id="017e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Amidst the pandemic, I observed a significant trend among researchers who turned to sentiment analysis as a tool to measure public responses to news and developments surrounding the virus. This involved analyzing user-generated content on popular social media platforms such as Twitter, YouTube, and Instagram. Intrigued by this methodology, my colleagues and I endeavored to contribute to the existing body of research by scrutinizing the daily briefings provided by public health authorities. In Alberta, Dr. Deena Hinshaw, who used to be the province’s chief medical officer of health, regularly delivered <a class="af nb" href="https://www.youtube.com/watch?v=fvw_USRfXgY" rel="noopener ugc nofollow" target="_blank">updates on the region’s response</a> to the ongoing pandemic. Through our analysis of these public health announcements, we aimed to assess Alberta’s effectiveness in implementing communication strategies during this intricate public health crisis. Our investigation, conducted through the lenses of sentiment analysis, sought to shed light on the efficacy of communication strategies employed during this challenging period in public health [1, 2].</p><p id="71d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post, I aim to walk you through the process of performing sentiment analysis using R. Specifically, I’ll focus on “lexicon-based sentiment analysis,” which I’ll discuss in more detail in the next section. I’ll provide examples of lexicon-based sentiment analysis that we’ve integrated into the publications referenced earlier. Additionally, in future posts, I’ll delve into more advanced forms of sentiment analysis, making use of state-of-the-art pre-trained models accessible on <a class="af nb" href="https://huggingface.co/docs/transformers/en/index" rel="noopener ugc nofollow" target="_blank">Hugging Face</a>.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7617" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Lexicon-Based Sentiment Analysis</h1><p id="ada6" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">As I learned more about sentiment analysis, I discovered that the predominant method for extracting sentiments is lexicon-based sentiment analysis. This approach entails utilizing a specific lexicon, essentially the vocabulary of a language or subject, to discern the direction and intensity of sentiments conveyed within a given text. Some lexicons, like the Bing lexicon [3], classify words as either positive or negative. Conversely, other lexicons provide more detailed sentiment labels, such as the NRC Emotion Lexicon [4], which categorizes words based on both positive and negative sentiments, as well as Plutchik’s [5] psych evolutionary theory of basic emotions (e.g., anger, fear, anticipation, trust, surprise, sadness, joy, and disgust).</p><p id="2ea8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Lexicon-based sentiment analysis operates by aligning words within a given text with those found in widely used lexicons such as NRC and Bing. Each word receives an assigned sentiment, typically categorized as positive or negative. The text’s collective sentiment score is subsequently derived by summing the individual sentiment scores of its constituent words. For instance, in a scenario where a text incorporates 50 positive and 30 negative words according to the Bing lexicon, the resulting sentiment score would be 20. This value indicates a predominance of positive sentiments within the text. Conversely, a negative total would imply a prevalence of negative sentiments.</p><p id="4ebd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Performing lexicon-based sentiment analysis using R can be both fun and tricky at the same time. While analyzing public health announcements in terms of sentiments, I found Julia Silge and David Robinson’s book, <a class="af nb" href="https://www.tidytextmining.com/" rel="noopener ugc nofollow" target="_blank"><em class="ph">Text Mining with R</em></a>, to be very helpful. The book has <a class="af nb" href="https://www.tidytextmining.com/sentiment" rel="noopener ugc nofollow" target="_blank">a chapter dedicated to sentiment analysis</a>, where the authors demonstrate how to conduct sentiment analysis using general-purpose lexicons like Bing and NRC. However, Julia and David also highlight a major limitation of lexicon-based sentiment analysis. The analysis considers only single words (i.e., unigrams) and does not consider qualifiers before a word. For instance, negation words like “not” in “not true” are ignored, and sentiment analysis processes them as two separate words, “not” and “true”. Furthermore, if a particular word (either positive or negative) is repeatedly used throughout the text, this may skew the results depending on the polarity (positive or negative) of this word. Therefore, the results of lexicon-based sentiment analysis should be interpreted carefully.</p><p id="a195" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s move to our example, where we will conduct lexicon-based sentiment analysis using Dr. Deena Hinshaw’s media briefings during the COVID-19 pandemic. My goal is to showcase two R packages capable of running sentiment analysis 📉.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1f0d" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Example</h1><p id="6f90" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">For the sake of simplicity, we will focus on the first wave of the pandemic (March 2020 — June 2020). The transcripts of all media briefings were publicly available on the government of Alberta’s COVID-19 pandemic website (<a class="af nb" href="https://www.alberta.ca/covid" rel="noopener ugc nofollow" target="_blank">https://www.alberta.ca/covid</a>). This dataset comes with an <a class="af nb" href="https://open.alberta.ca/licence" rel="noopener ugc nofollow" target="_blank">open data license</a> that allows the public to access and use the information, including for commercial purposes. After importing these transcripts into R, I turned all the text into lowercase and then applied word tokenization using the <strong class="ne fr">tidytext</strong> and <strong class="ne fr">tokenizers</strong> packages. Word tokenization split the sentences in the media briefings into individual words for each entry (i.e., day of media briefings). Next, I applied lemmatization to the tokens to resolve each word into its canonical form using the <strong class="ne fr">textstem</strong> package. Finally, I removed common stopwords, such as “my,” “for,” “that,” “with,” and “for, using the stopwords package. The final dataset is available <a class="af nb" href="https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta.RData" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">here</strong></a>. Now, let’s import the data into R and then review its content.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="a9e9" class="pm oh fq pj b bg pn po l pp pq">load("wave1_alberta.RData")<br/><br/>head(wave1_alberta, 10)</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/d1603ea9ba4436bfd2d51e226a839b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZOFO96TCK8LddPBGvUpAQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A preview of the dataset (Image by author)</figcaption></figure><p id="9511" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The dataset has three columns:</p><ul class=""><li id="0f6b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ps pt pu bk">month (the month of the media briefing)</li><li id="8ce1" class="nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx ps pt pu bk">date (the exact date of the media briefing), and</li><li id="22d2" class="nc nd fq ne b go pv ng nh gr pw nj nk nl px nn no np py nr ns nt pz nv nw nx ps pt pu bk">word (words or tokens used in media briefing)</li></ul><h2 id="950c" class="qa oh fq bf oi qb qc qd ol qe qf qg oo nl qh qi qj np qk ql qm nt qn qo qp qq bk">Descriptive Analysis</h2><p id="b625" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Now, we can calculate some descriptive statistics to better understand the content of our dataset. We will begin by finding the top 5 words (based on their frequency) for each month.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="0e49" class="pm oh fq pj b bg pn po l pp pq">library("dplyr")<br/><br/>wave1_alberta %&gt;%<br/>  group_by(month) %&gt;%<br/>  count(word, sort = TRUE) %&gt;%<br/>  slice_head(n = 5) %&gt;%<br/>  as.data.frame()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qr"><img src="../Images/759fd7f9b972d9892bc76ad34db081be.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*M4m1PYI6doDJqLY8MIdQ0A.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Top 5 words by months (Image by author)</figcaption></figure><p id="6b6f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output shows that words such as health, continue, and test were commonly used in the media briefings across this 4-month period. We can also expand our list to the most common 10 words and view the results visually:</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="f21d" class="pm oh fq pj b bg pn po l pp pq">library("tidytext")<br/>library("ggplot2")<br/><br/>wave1_alberta %&gt;%<br/>  # Group by month<br/>  group_by(month) %&gt;%<br/>  count(word, sort = TRUE) %&gt;%<br/>  # Find the top 10 words<br/>  slice_head(n = 10) %&gt;%<br/>  ungroup() %&gt;%<br/>  # Order the words by their frequency within each month<br/>  mutate(word = reorder_within(word, n, month)) %&gt;%<br/>  # Create a bar graph<br/>  ggplot(aes(x = n, y = word, fill = month)) +<br/>  geom_col() +<br/>  scale_y_reordered() +<br/>  facet_wrap(~ month, scales = "free_y") +<br/>  labs(x = "Frequency", y = NULL) +<br/>  theme(legend.position = "none",<br/>        axis.text.x = element_text(size = 11),<br/>        axis.text.y = element_text(size = 11),<br/>        strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 13))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qs"><img src="../Images/2b8308d5610aef09dbe4729f628a162d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2e4A0jG0ZDJ3XaWckUnqg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Most common words based on frequency (Image by author)</figcaption></figure><p id="1b9c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since some words are common across all four months, the plot above may not necessarily show us the important words that are unique to each month. To find such important words, we can use Term Frequency — Inverse Document Frequency (TF-IDF)–a widely used technique in NLP for measuring how important a term is within a document relative to a collection of documents (for more detailed information about TF-IDF, check out <a class="af nb" href="https://okan.cloud/posts/2022-01-16-text-vectorization-using-python-tf-idf/#tf-idf" rel="noopener ugc nofollow" target="_blank">my previous blog post</a>). In our example, we will treat media briefings for each month as a document and calculate TF-IDF for the tokens (i.e., words) within each document. The first part of the R codes below creates a new dataset, <em class="ph">wave1_tf_idf</em>, by calculating TF-IDF for all tokens and selecting the tokens with the highest TF-IDF values within each month. Next, we use this dataset to create a bar plot with the TF-IDF values to view the common words unique to each month.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="4237" class="pm oh fq pj b bg pn po l pp pq"># Calculate TF-IDF for the words for each month<br/>wave1_tf_idf &lt;- wave1_alberta %&gt;%<br/>  count(month, word, sort = TRUE) %&gt;%<br/>  bind_tf_idf(word, month, n) %&gt;%<br/>  arrange(month, -tf_idf) %&gt;%<br/>  group_by(month) %&gt;%<br/>  top_n(10) %&gt;%<br/>  ungroup<br/><br/># Visualize the results<br/>wave1_tf_idf %&gt;%<br/>  mutate(word = reorder_within(word, tf_idf, month)) %&gt;%<br/>  ggplot(aes(word, tf_idf, fill = month)) +<br/>  geom_col(show.legend = FALSE) + <br/>  facet_wrap(~ month, scales = "free", ncol = 2) +<br/>  scale_x_reordered() +<br/>  coord_flip() +<br/>  theme(strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 13),<br/>        axis.text.x = element_text(size = 11),<br/>        axis.text.y = element_text(size = 11)) +<br/>  labs(x = NULL, y = "TF-IDF")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/d6e77561244b9eb493176747ea3cac6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_XRHl83dr334nYPSdLeLRw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Most common words based on TIF-IDF (Image by author)</figcaption></figure><p id="e9b6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These results are more informative because the tokens shown in the figure reflect unique topics discussed each month. For example, in March 2020, the media briefings were mostly about limiting travel, returning from crowded conferences, and COVID-19 cases on cruise ships. In June 2020, the focus of the media briefings shifted towards mask requirements, people protesting pandemic-related restrictions, and so on.</p><p id="3927" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before we switch back to the sentiment analysis, let’s take a look at another descriptive variable: the length of each media briefing. This will show us whether the media briefings became longer or shorter over time.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="c3e4" class="pm oh fq pj b bg pn po l pp pq">wave1_alberta %&gt;%<br/>  # Save "day" as a separate variable<br/>  mutate(day = substr(date, 9, 10)) %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  # Count the number of words<br/>  summarize(n = n()) %&gt;%<br/>  ggplot(aes(day, n, color = month, shape = month, group = month)) +<br/>  geom_point(size = 2) + <br/>  geom_line() + <br/>  labs(x = "Days", y = "Number of Words") +<br/>  theme(legend.position = "none", <br/>        axis.text.x = element_text(angle = 90, size = 11),<br/>        strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 11),<br/>        axis.text.y = element_text(size = 11)) +<br/>  ylim(0, 800) +<br/>  facet_wrap(~ month, scales = "free_x")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/8c46ac6ce65bdb88d89a63b92a501b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsxkR-_BLiaBrwJdO8_yLA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Number of words in the media briefings by day (Image by author)</figcaption></figure><p id="7364" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The figure above shows that the length of media briefings varied quite substantially over time. Especially in March and May, there are larger fluctuations (i.e., very long or short briefings), whereas, in June, the daily media briefings are quite similar in terms of length.</p><h2 id="20ab" class="qa oh fq bf oi qb qc qd ol qe qf qg oo nl qh qi qj np qk ql qm nt qn qo qp qq bk">Sentiment Analysis with tidytext</h2><p id="251c" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">After analyzing the dataset descriptively, we are ready to begin with the sentiment analysis. In the first part, we will use the <strong class="ne fr">tidytext</strong> package for performing sentiment analysis and computing sentiment scores. We will first import the lexicons into R and then merge them with our dataset. Using the Bing lexicon, we need to find the difference between the number of positive and negative words to produce a sentiment score (i.e., sentiment = the number of positive words — the number of negative words).</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="7822" class="pm oh fq pj b bg pn po l pp pq"># From the three lexicons, Bing is already available in the tidytext page<br/># for AFINN and NRC, install the textdata package by uncommenting the next line<br/># install.packages("textdata")<br/>get_sentiments("bing")<br/>get_sentiments("afinn") <br/>get_sentiments("nrc")<br/><br/># We will need the spread function from tidyr<br/>library("tidyr")<br/><br/># Sentiment scores with bing (based on frequency)<br/>wave1_alberta %&gt;%<br/>  mutate(day = substr(date, 9, 10)) %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  inner_join(get_sentiments("bing")) %&gt;%<br/>  count(month, day, sentiment) %&gt;%<br/>  spread(sentiment, n) %&gt;%<br/>  mutate(sentiment = positive - negative) %&gt;%<br/>  ggplot(aes(day, sentiment, fill = month)) +<br/>  geom_col(show.legend = FALSE) +<br/>  labs(x = "Days", y = "Sentiment Score") +<br/>  ylim(-50, 50) + <br/>  theme(legend.position = "none", axis.text.x = element_text(angle = 90)) +<br/>  facet_wrap(~ month, ncol = 2, scales = "free_x") +<br/>  theme(strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 11),<br/>        axis.text.x = element_text(size = 11),<br/>        axis.text.y = element_text(size = 11))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/52c36e29a5fde07cefdd40594b2fc537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8rQ5oPxmhLm9zHSeooHQA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sentiment scores based on the Bing lexicon (Image by author)</figcaption></figure><p id="4ac5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The figure above shows that the sentiments delivered in the media briefings were generally negative, which is not necessarily surprising since the media briefings were all about how many people passed away, hospitalization rates, potential outbreaks, etc. On certain days (e.g., March 24, 2020 and May 4, 2020), the media briefings were particularly more negative in terms of sentiments.</p><p id="7d8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we will use the AFINN lexicon. Unlike Bing that labels words as positive or negative, AFINN assigns a numerical weight to each word. The sign of the weight indicates the polarity of sentiments (i.e., positive or negative), while the value indicates the intensity of sentiments. Now, let’s see if these weighted values produce different sentiment scores.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="da5b" class="pm oh fq pj b bg pn po l pp pq">wave1_alberta %&gt;%<br/>  mutate(day = substr(date, 9, 10)) %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  inner_join(get_sentiments("afinn")) %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  summarize(sentiment = sum(value),<br/>            type = ifelse(sentiment &gt;= 0, "positive", "negative")) %&gt;%<br/>  ggplot(aes(day, sentiment, fill = type)) +<br/>  geom_col(show.legend = FALSE) +<br/>  labs(x = "Days", y = "Sentiment Score") +<br/>  ylim(-100, 100) + <br/>  facet_wrap(~ month, ncol = 2, scales = "free_x") +<br/>  theme(legend.position = "none", <br/>        strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 11),<br/>        axis.text.x = element_text(size = 11, angle = 90),<br/>        axis.text.y = element_text(size = 11))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/fd3bcd5f5955b4a3ae40a0156952837f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CSZF-ZM3D-Rtewpmt-sZFg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sentiment scores based on the AFINN lexicon (Image by author)</figcaption></figure><p id="872d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results based on the AFINN lexicon seem to be quite different! Once we take the “weight” of the tokens into account, most media briefings turn out to be positive (see the green bars), although there are still some days with negative sentiments (see the red bars). The two analyses we have done so far have yielded very different for two reasons. First, as I mentioned above, the Bing lexicon focuses on the polarity of the words but ignores the intensity of the words (dislike and hate are considered negative words with equal intensity). Unlike the Bing lexicon, the AFINN lexicon takes the intensity into account, which impacts the calculation of the sentiment scores. Second, the Bing lexicon (6786 words) is fairly larger than the AFINN lexicon (2477 words). Therefore, it is likely that some tokens in the media briefings are included in the Bing lexicon, but not in the AFINN lexicon. Disregarding those tokens might have impacted the results.</p><p id="1282" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The final lexicon we are going to try using the <strong class="ne fr">tidytext</strong> package is NRC. As I mentioned earlier, this lexicon uses Plutchik’s psych-evolutionary theory to label the tokens based on basic emotions such as anger, fear, and anticipation. We are going to count the number of words or tokens associated with each emotion and then visualize the results.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="f9fc" class="pm oh fq pj b bg pn po l pp pq">wave1_alberta %&gt;%<br/>  mutate(day = substr(date, 9, 10)) %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  inner_join(get_sentiments("nrc")) %&gt;%<br/>  count(month, day, sentiment) %&gt;%<br/>  group_by(month, sentiment) %&gt;%<br/>  summarize(n_total = sum(n)) %&gt;%<br/>  ggplot(aes(n_total, sentiment, fill = sentiment)) +<br/>  geom_col(show.legend = FALSE) +<br/>  labs(x = "Frequency", y = "") +<br/>  xlim(0, 2000) + <br/>  facet_wrap(~ month, ncol = 2, scales = "free_x") +<br/>  theme(strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 11),<br/>        axis.text.x = element_text(size = 11),<br/>        axis.text.y = element_text(size = 11))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/e8024c832941740e0f097412e6fe3ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJKIcqOh1au2WI4QHulJYQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sentiment scores based on the NRC lexicon (Image by author)</figcaption></figure><p id="7b96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The figure shows that the media briefings are mostly positive each month. Dr. Hinshaw used words associated with “trust”, “anticipation”, and “fear”. Overall, the pattern of these emotions seems to remain very similar over time, indicating the consistency of the media briefings in terms of the type and intensity of the emotions delivered.</p><h1 id="1992" class="og oh fq bf oi oj qu gq ol om qv gt oo op qw or os ot qx ov ow ox qy oz pa pb bk">Sentiment Analysis with sentimentr</h1><p id="a304" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">Another package for lexicon-based sentiment analysis is <strong class="ne fr">sentimentr</strong> (<a class="af nb" href="https://okan.cloud/posts/2024-02-09-lexicon-based-sentiment-analysis-using-r/#ref-R-sentiment" rel="noopener ugc nofollow" target="_blank">Rinker, 2021</a>). Unlike the <strong class="ne fr">tidytext</strong> package, this package takes valence shifters (e.g., negation) into account, which can easily flip the polarity of a sentence with one word. For example, the sentence “I am not unhappy” is actually positive, but if we analyze it word by word, the sentence may seem to have a negative sentiment due to the words “not” and “unhappy”. Similarly, “I hardly like this book” is a negative sentence but the analysis of individual words, “hardly” and “like”, may yield a positive sentiment score. The <strong class="ne fr">sentimentr</strong> package addresses the limitations around sentiment detection with valence shifters (see the package author Tyler Rinker’s Github page for further details on <strong class="ne fr">sentimentr</strong>: <a class="af nb" href="https://github.com/trinker/sentimentr" rel="noopener ugc nofollow" target="_blank">https://github.com/trinker/sentimentr</a>).</p><p id="b84b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To benefit from the <strong class="ne fr">sentimentr</strong> package, we need the actual sentences in the media briefings rather than the individual tokens. Therefore, I had to create an untokenized version of the dataset, which is available <a class="af nb" href="https://github.com/okanbulut/blog/raw/master/data_and_codes/wave1_alberta_sentence.RData" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">here</strong></a>. We will first import this dataset into R, get individual sentences for each media briefing using the <code class="cx qz ra rb pj b">get_sentences()</code> function, and then calculate sentiment scores by day and month via <code class="cx qz ra rb pj b">sentiment_by()</code>.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="e3ae" class="pm oh fq pj b bg pn po l pp pq">library("sentimentr")<br/>library("magrittr")<br/><br/>load("wave1_alberta_sentence.RData")<br/><br/># Calculate sentiment scores by day and month<br/>wave1_sentimentr &lt;- wave1_alberta_sentence %&gt;%<br/>  mutate(day = substr(date, 9, 10)) %&gt;%<br/>  get_sentences() %$%<br/>  sentiment_by(text, list(month, day))<br/><br/># View the dataset<br/>head(wave1_sentimentr, 10)</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rc"><img src="../Images/1ac1b34b41d6208e8255af9454d93fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XFsdcUxpCu5px-wCxt8Z-A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A preview of the dataset (Image by author)</figcaption></figure><p id="df91" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the dataset we created, “ave_sentiment” is the average sentiment score for each day in March, April, May, and June (i.e., days where a media briefing was made). Using this dataset, we can visualize the sentiment scores.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="a3c1" class="pm oh fq pj b bg pn po l pp pq">wave1_sentimentr %&gt;%<br/>  group_by(month, day) %&gt;%<br/>  ggplot(aes(day, ave_sentiment, fill = ave_sentiment)) +<br/>  scale_fill_gradient(low="red", high="blue") + <br/>  geom_col(show.legend = FALSE) +<br/>  labs(x = "Days", y = "Sentiment Score") +<br/>  ylim(-0.1, 0.3) +<br/>  facet_wrap(~ month, ncol = 2, scales = "free_x") +<br/>  theme(legend.position = "none", <br/>        strip.background = element_blank(),<br/>        strip.text = element_text(colour = "black", face = "bold", size = 11),<br/>        axis.text.x = element_text(size = 11, angle = 90),<br/>        axis.text.y = element_text(size = 11))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/b571e9a77534d4efa810382622d45a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYObg-R4Yb4VMtjo92yhpg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sentiment scores based on sentiment (Image by author)</figcaption></figure><p id="507e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the figure above, the blue bars represent highly positive sentiment scores, while the red bars depict comparatively lower sentiment scores. The patterns observed in the sentiment scores generated by <strong class="ne fr">sentimentr</strong> closely resemble those derived from the AFINN lexicon. Notably, this analysis is based on the original media briefings rather than solely tokens, with consideration given to valence shifters in the computation of sentiment scores. The convergence between the sentiment patterns identified by <strong class="ne fr">sentimentr</strong> and those from AFINN is not entirely unexpected. Both approaches incorporate similar weighting systems and mechanisms that account for word intensity. This alignment reinforces our confidence in the initial findings obtained through AFINN, validating the consistency and reliability of our analyses with <strong class="ne fr">sentiment</strong>.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="46ae" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Concluding Remarks</h1><p id="3a03" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">In conclusion, lexicon-based sentiment analysis in R offers a powerful tool for uncovering the emotional nuances within textual data. Throughout this post, we have explored the fundamental concepts of lexicon-based sentiment analysis and provided a practical demonstration of its implementation using R. By leveraging packages such as <strong class="ne fr">sentimentr</strong> and <strong class="ne fr">tidytext</strong>, we have illustrated how sentiment analysis can be seamlessly integrated into your data analysis workflow. As you embark on your journey into sentiment analysis, remember that the insights gained from this technique extend far beyond the surface of the text. They provide valuable perspectives on public opinion, consumer sentiment, and beyond. I encourage you to delve deeper into lexicon-based sentiment analysis, experiment with the examples presented here, and unlock the rich insights waiting to be discovered within your own data. Happy analyzing!</p><h1 id="f85e" class="og oh fq bf oi oj qu gq ol om qv gt oo op qw or os ot qx ov ow ox qy oz pa pb bk">References</h1><p id="14ba" class="pw-post-body-paragraph nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx fj bk">[1] Bulut, O., &amp; Poth, C. N. (2022). Rapid assessment of communication consistency: Sentiment analysis of public health briefings during the COVID-19 pandemic. <em class="ph">AIMS Public Health</em>, <em class="ph">9</em>(2), 293–306. <a class="af nb" href="https://doi.org/10.3934/publichealth.2022020" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3934/publichealth.2022020</a></p><p id="a57f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] Poth, C. N., Bulut, O., Aquilina, A. M., &amp; Otto, S. J. G. (2021). Using data mining for rapid complex case study descriptions: Example of public health briefings during the onset of the COVID-19 pandemic. <em class="ph">Journal of Mixed Methods Research</em>, <em class="ph">15</em>(3), 348–373. <a class="af nb" href="https://doi.org/10.1177/15586898211013925" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1177/15586898211013925</a></p><p id="21d6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Hu, M., &amp; Liu, B. (2004). Mining and summarizing customer reviews. <em class="ph">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 168–177.</p><p id="b021" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] Mohammad, S. M., &amp; Turney, P. D. (2013). Crowdsourcing a word–emotion association lexicon. <em class="ph">Computational Intelligence</em>, <em class="ph">29</em>(3), 436–465.</p><p id="60fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5] Plutchik, R. (1980). A general psychoevolutionary theory of emotion. In <em class="ph">Theories of emotion</em> (pp. 3–33). Elsevier.</p></div></div></div></div>    
</body>
</html>