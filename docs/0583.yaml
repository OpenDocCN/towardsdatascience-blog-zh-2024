- en: Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System
    with Ragas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557?source=collection_archive---------0-----------------------#2024-03-03](https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557?source=collection_archive---------0-----------------------#2024-03-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation
    Questions and their relationships to source documents with Ragas, OpenAI, Langchain
    and ChromaDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@markus.stoll?source=post_page---byline--fc2486308557--------------------------------)[![Markus
    Stoll](../Images/236ce5901f817a72c6cceb40e0ca2fc5.png)](https://medium.com/@markus.stoll?source=post_page---byline--fc2486308557--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc2486308557--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc2486308557--------------------------------)
    [Markus Stoll](https://medium.com/@markus.stoll?source=post_page---byline--fc2486308557--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc2486308557--------------------------------)
    ·13 min read·Mar 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation (RAG) adds a retrieval step to the workflow of
    an LLM, enabling it to query relevant data from additional sources like private
    documents when responding to questions and queries [1]. This workflow does not
    require costly training or fine-tuning of LLMs on the additional documents. The
    documents are split into snippets, which are then indexed, often using a compact
    ML-generated vector representation (embedding). Snippets with similar content
    will be in proximity to each other in this embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG application projects the user-provided questions into the embedding
    space to retrieve relevant document snippets based on their distance to the question.
    The LLM can use the retrieved information to answer the query and to substantiate
    its conclusion by presenting the snippets as references.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab15fb445c5c48c93aa871fc42c61295.png)'
  prefs: []
  type: TYPE_IMG
- en: Animation of the iterations of a UMAP [3] dimensionality reduction for Wikipedia
    Formula One articles in the embedding space with manually labeled clusters — created
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation of a RAG application is challenging [2]. Different approaches
    exist: on one hand, there are methods where the answer as ground truth must be
    provided by the developer; on the other hand, the answer (and the question) can
    also be generated by another LLM. One of the largest open-source systems for LLM-supported
    answering is Ragas [4](Retrieval-Augmented Generation Assessment), which provides'
  prefs: []
  type: TYPE_NORMAL
- en: Methods for generating test data based on the documents and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluations based on different metrics for evaluating retrieval and generation
    steps one-by-one and end-to-end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, you will learn
  prefs: []
  type: TYPE_NORMAL
- en: How to briefly build a RAG system for Formula One (see the previous article
    [Visualize your RAG Data — EDA for Retrieval-Augmented Generation](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)
    for detailed descriptions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate questions and answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the RAG system with [Ragas](https://github.com/explodinggradients/ragas)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most importantly how to visualize the results with [Renumics Spotlight](https://github.com/Renumics/spotlight)
    and interpret the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The* [*code is available at Github*](https://github.com/Renumics/renumics-rag/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get your environment ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a notebook and install the required python packages
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This tutorial uses the following python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Langchain**](https://github.com/langchain-ai/langchain): A framework to
    integrate language models and RAG components, making the setup process smoother.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Renumics-Spotlight**](https://github.com/Renumics/spotlight): A visualization
    tool to interactively explore unstructured ML datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Ragas**](https://github.com/explodinggradients/ragas): a framework that
    helps you evaluate your RAG pipelines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Disclaimer: The author of this article is also one of the developers of Spotlight.*'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare documents and embeddings for the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use your own RAG Application, skip to the next part to learn how to
    evaluate, extract and visualize.
  prefs: []
  type: TYPE_NORMAL
- en: Or you can use the RAG application from the [last article](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f)
    with [our prepared dataset of all Formula One articles of Wikipedia](https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip).
    There you can also insert your own Documents into a ‘docs/’ subfolder.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset is based on articles from [Wikipedia](https://www.wikipedia.org/)
    and is licensed under the Creative Commons Attribution-ShareAlike License. The
    original articles and a list of authors can be found on the respective Wikipedia
    pages.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now you can use Langchain’s `DirectoryLoader` to load all files from the docs
    subdirectory and split the documents in snippets using the `RecursiveCharacterTextSpliter`.
    With `OpenAIEmbeddings` you can create embeddings and store them in a `ChromaDB`
    as vector store. For the Chain itself you can use LangChains `ChatOpenAI` and
    a `ChatPromptTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: The [linked code](https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb)
    for this article contains all necessary steps and you can find a detailed description
    of all steps above in [the last article](https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f).
  prefs: []
  type: TYPE_NORMAL
- en: One important point is, that you should use a hash function to create ids for
    snippets in `ChromaDB`. This allows to find the embeddings in the db if you only
    have the document with its content and metadata. This makes it possible to skip
    documents that already exist in the database.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Evaluation Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a common topic like Formula One, one can also use ChatGPT directly to generate
    general questions. In this article, four methods of question generation are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT4**: 30 questions were generated using ChatGPT 4 with the following prompt
    “Write 30 question about Formula one”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '– Random Example: “Which Formula 1 team is known for its prancing horse logo?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**GPT3.5:** Another 199 question were generated with ChatGPT 3.5 with the following
    prompt “Write 100 question about Formula one” and repeating “Thanks, write another
    100 please”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '– Example: “”Which driver won the inaugural Formula One World Championship
    in 1950?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Ragas_GPT4**: 113 questions were generated using Ragas. Ragas utilizes the
    documents again and its own embedding model to construct a vector database, which
    is then used to generate questions with GPT4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '– Example: “Can you tell me more about the performance of the Jordan 198 Formula
    One car in the 1998 World Championship?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Rags_GPT3.5**: 226 additional questions were generated with Ragas — here
    we use GPT3.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '– Example: “What incident occurred at the 2014 Belgian Grand Prix that led
    to Hamilton’s retirement from the race?”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The questions and answers were not reviewed or modified in any way. All questions
    are combined in a single dataframe with the columns `id`, `question`, `ground_truth`,
    `question_by` and `answer`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e3c079da63fa016b279acfea51bef1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, the questions will be posed to the RAG system. For over 500 questions,
    this can take some time and incur costs. If you ask the questions row-by-row,
    you can pause and continue the process or recover from a crash without losing
    the results so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Not only is the answer stored but also the source IDs of the retrieved document
    snippets, and their text content as context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5fc1f88bf931c7f85b2f884635d5ce03.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, the embeddings for all questions are generated and stored in the
    dataframe as well. This allows for visualizing them alongside the documents.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation with Ragas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Ragas](https://github.com/explodinggradients/ragas) provides metrics for evaluating
    each component of your RAG pipeline in isolation and end-to-end metrics for overall
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Context Precision:** Uses the `question` and retrieved `contexts` to measure
    the signal-to-noise ratio.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Context Relevancy:** Measures the relevance of the retrieved context to the
    question, calculated using the `question` and `contexts`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Context Recall:** Based on the `ground truth` and `contexts` to check if
    all relevant information for the answer is retrieved.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Faithfulness:** Utilizes the `contexts` and `answer` to measure how factually
    accurate the generated answer is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer Relevance:** Computed using the `question` and `answer` to assess
    the relevance of the generated answer to the question (does not consider factuality).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer Semantic Similarity:** Evaluated using the `ground truth` and `answer`
    to assess the semantic resemblance between the generated and the correct answer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer Correctness:** Relies on the `ground truth` and `answer` to measure
    the accuracy and alignment of the generated answer with the correct one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Aspect Critique:** Involves analyzing the `answer` to evaluate submissions
    based on predefined or custom aspects such as correctness or harmfulness.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For now, we focus on the end-to-end metric of answer correctness. The column
    names and content in the dataframe are copied and adapted to meet the naming and
    formatting requirements according to the Ragas API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This again can take some time and even more money than just querying your RAG
    system. Let’s apply the evaluation row-by-row to be able to recover from a crash
    without losing the results so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterwards, you can store the results in the `df_questions_answer` dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Prepare visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To include the document snippets in the visualization, we add references from
    documents to questions that used the document as a source. Additionally, the count
    of questions referencing a document is stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now concatenate the dataframe of questions with the dataframe of the documents
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, let’s prepare some different UMAP [3] mappings. You could do much
    the same in the Spotlight GUI later, but doing it upfront can save time.
  prefs: []
  type: TYPE_NORMAL
- en: 'umap_all: UMAP with fit and transform applied on all document and question
    embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'umap_questions: UMAP with fit applied on questions embeddings only and transform
    applied on both'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'umap_docs: UMAP with fit applied on document embeddings only and transform
    applied on both'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We prepare each of the UMAP transformations like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Another interesting metric for each of the document snippets is the distance
    between its embeddings and the embeddings of the nearest question
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This metric can be helpful to find documents that are not referenced by questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ec8f60de7d7a00c65c3c4a825bec1e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualize results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you skipped the previous steps, you can download the dataframe and load
    it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'and start [Renumics Spotlight](https://github.com/Renumics/spotlight) to visualize
    it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'It will open a new browser window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e6da163c2994fe3ee68adcf015fed02.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula One documents and evaluation questions statistics and similarity maps
    — created by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)
  prefs: []
  type: TYPE_NORMAL
- en: 'On the top left side, you can see a **table of all questions and all document**
    snippets. You can use the “visible columns” button to control which columns of
    the dataframe are shown in the table. It is useful to create a filter directly
    that selects only the questions to be able to turn the questions on and off in
    the visualizations: Select all questions and and then create a filter using the
    “Create filter from selected row” button.'
  prefs: []
  type: TYPE_NORMAL
- en: To the right of the table, the `answer correctness` **is displayed as a metric**
    across all questions. Below there are two **histograms**; the left one shows the
    distribution of `answer correctness` divided into the different methods of question
    generation. The right one shows the distribution of methods of question generation.
    Here, it is advisable to create a filter for the questions using the filter button
    to display only the selected rows (the questions) if needed.
  prefs: []
  type: TYPE_NORMAL
- en: On the right side, there are **two similarity maps.** The first one uses the
    `umap_questions` column and shows the questions and documents based on the transformation
    applied only to the questions. It is helpful for viewing the distribution of questions
    independently from the associated documents because this approach allows analysts
    to identify patterns or clusters within the questions themselves.
  prefs: []
  type: TYPE_NORMAL
- en: The second similarity map shows the questions and documents based on the transformation
    applied only to the documents (`umap_docs`). It is useful for viewing the questions
    in the context of their associated documents. A similarity map that simultaneously
    transforms questions and documents has proven to be less helpful with a larger
    number of questions, as more or fewer questions get clustered together and tend
    to be separated from the documents. Therefore, this representation is omitted
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15014bcc7e0dc0ea4f7b99ca395d4993.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula One evaluation questions statistics and similarity maps — created by
    the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)
  prefs: []
  type: TYPE_NORMAL
- en: 'Document Embedding Similarity Map: Observations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the similarity map `umap_docs`, you can identify areas in the embedding space
    of the documents that have no neighboring questions. It is even better recognized
    when selecting `nearest_question_dist` for coloring.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d498a101193db93856b2220584aa1c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity map of Formula One documents and questions (highlighted) — created
    by the author with [Renumics Spotlight](https://github.com/Renumics/spotlight)
  prefs: []
  type: TYPE_NORMAL
- en: Some clusters can be identified, including snippets that contain only headings
    or tabular data containing only numbers page by page, whose meaning is lost during
    splitting. Additionally, many Wikipedia-specific text additions that contain no
    relevant information, such as links to other languages or editing notes, form
    clusters with no neighboring questions.
  prefs: []
  type: TYPE_NORMAL
- en: Removing the noise in form of Wikipedia-related text is very simple when using
    the Wikipedia API. It is probably not particularly necessary, as it mainly costs
    some space — it is not expected that the RAG result will be particularly worsened
    by it. However, data contained in large tables are hardly captured by the RAG
    system and it could ne benifical to extract these using advanced pre-processing
    methods for Table Extraction and to connect them to the RAG system.
  prefs: []
  type: TYPE_NORMAL
- en: Another point that you can observe in the `umap_docs` similarity map is how
    the questions from different sources are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16c4d18dec8db36dc504497edf4a09a4.png)![](../Images/e6efde6c3a40a1bdf15ae12e744d07f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Questions generated from ChatGPT (GPT-3.5 and GPT-4), Right: Questions
    generated with ragas using GPT-3.5 and GPT-4 — created by the author with [Renumics
    Spotlight](https://github.com/Renumics/spotlight)'
  prefs: []
  type: TYPE_NORMAL
- en: The questions that were directly generated by ChatGPT (GPT-3.5, GPT-4) are located
    in a more confined area in the center, whereas the questions generated with ragas
    based on the documents cover a larger area.
  prefs: []
  type: TYPE_NORMAL
- en: Answer correctness histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The histogram can be used as a starting point to get an initial impression of
    the global statistics of the data. Overall, across all questions, the `answer
    correctness` is 0.45\. For the questions created without ragas, it is 0.36, and
    for questions with ragas, it is 0.52\. It was expected that the system would perform
    better for questions generated by ragas, as these questions are based on the available
    data, whereas the questions directly generated by ChatGPT could come from all
    the data with which ChatGPT was trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0823d03c0ade2f06c1123e122c0311a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of the answer correctness colored by the source of the question -
    created by the author
  prefs: []
  type: TYPE_NORMAL
- en: A quick, random manual review of some of the questions/answers and ground truth
    shows that in the interval of`answer correctness`0.3–0.4, most questions were
    still correctly answered according to the ground truth. In the interval 0.2–0.3,
    many incorrect answers are present. In the interval 0.1–0.2, most answers are
    incorrect. Notably, almost all questions in this range came from GPT-3.5\. The
    two questions in this interval generated with GPT-4 were answered correctly even
    though they received an `answer correctness` of below 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Questions Embedding Similarity Map: Observations'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Questions Embedding Similarity Map can be helpful to dig deeper into `answer
    correctness` by examining clusters of similar questions that may cause similar
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster “Term for driver/process/cars”:** average `answer correctness` 0.23:
    Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or
    brake tuning vs. brake bias adjustment. It is questionable whether these types
    of questions are suitable for evaluating the system, as it seems very difficult
    to judge the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster “Terms for fuel strategy:”** average `answer correctness`0.44, similar
    to the global`answer correctness`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster “Names of tracks”:** average `answer correctness` 0.49, similar to
    the global `answer correctnes`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster “Who holds the record for…”**: average `answer correctness` 0.44,
    similar to the global `answer correctness`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster “Win championship with…”**: average `answer correctnes` 0.26 — looks
    challenging. Questions with many conditions, e.g., “Who is the only driver to
    win the Formula One World Championship with a British racing license, driving
    for an Italian team with an American engine.” Extended RAG methods like Multi
    Query might help improve here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster “Who is the only driver to win… with a car bearing the number <number>”**:
    average `answer correctness` 0.23 — looks like GPT-3.5 was lazy here, repeating
    the same question with different numbers, even though most ground truth entries
    are wrong!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fe6a9f7fe12e115d68dc9cf1906b1461.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity map of Formula One questions (highlighted) and documents — created
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, utilizing UMAP-based visualizations offers a interesting approach
    to dig deeper than just analyzing global metrics. The document embedding similarity
    map gives a good overview, illustrating the clustering of similar documents and
    their relation to evaluation questions. The question similarity map reveals patterns
    that allow the differentiation and analysis of questions in conjunction with quality
    metrics to enable insight generation. Follow the Visualize results section to
    apply the visualization on your evaluation strategy — what insights will you uncover?
  prefs: []
  type: TYPE_NORMAL
- en: '*I am a professional with expertise in creating advanced software solutions
    for the interactive exploration of unstructured data. I write about unstructured
    data and use powerful visualization tools to analyze and make informed decisions.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi
    Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: [Retrieval-Augmented Generation
    for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997) (2024),
    arxiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Yixuan Tang, Yi Yang: [MultiHop-RAG: Benchmarking Retrieval-Augmented Generation
    for Multi-Hop Queries](https://arxiv.org/abs/2401.15391) (2021), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Leland McInnes, John Healy, James Melville: [UMAP: Uniform Manifold Approximation
    and Projection for Dimension Reduction](https://arxiv.org/abs/1802.03426) (2018),
    arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert: [RAGAS:
    Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
    (2023), arXiv'
  prefs: []
  type: TYPE_NORMAL
