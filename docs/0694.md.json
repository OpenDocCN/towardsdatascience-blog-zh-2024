["```py\nmodel_name = \"bigscience/bloom-560m\"\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name, trust_remote_code=True, padding_side=\"right\"\n)  # padding side should be right for CausalLM models\n# overfit to 5 made up examples\nstr1 = '\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant: perro'\nstr2 = '\\n\\n### Human: How do you say \"water\" in Spanish?\\n\\n### Assistant: agua'\nstr3 = '\\n\\n### Human: How do you say \"hello\" in Spanish?\\n\\n### Assistant: hola'\nstr4 = '\\n\\n### Human: How do you say \"tree\" in Spanish?\\n\\n### Assistant: árbol'\nstr5 = '\\n\\n### Human: How do you say \"mother\" in Spanish?\\n\\n### Assistant: madre'\ntrain_data = {\n    \"text\": [str1, str2, str3, str4, str5],\n}\ndataset_text = Dataset.from_dict(train_data)\n\n# to test if we learn how to generate an unknown word. \nholdout_str = (\n    '\\n\\n### Human: How do you say \"day\" in Spanish?\\n\\n### Assistant:<s>'  # día\n)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nholdout_input = tokenizer(holdout_str, return_tensors=\"pt\").to(device)\n```", "```py\nINSTRUCTION_TEMPLATE_BASE = \"\\n\\n### Human:\"\nRESPONSE_TEMPLATE_BASE = \"\\n\\n### Assistant:\"\ndef add_special_tokens(\n    example: Dict,\n    tokenizer: PreTrainedTokenizerBase,\n) -> Dict:\n    # add eos_token before human text and bos_token before assistant text\n    example[\"text\"] = (\n        example[\"text\"]\n        .replace(\n            INSTRUCTION_TEMPLATE_BASE, tokenizer.eos_token + INSTRUCTION_TEMPLATE_BASE\n        )\n        .replace(RESPONSE_TEMPLATE_BASE, RESPONSE_TEMPLATE_BASE + tokenizer.bos_token)\n    )\n    if not example[\"text\"].endswith(tokenizer.eos_token):\n        example[\"text\"] += tokenizer.eos_token\n    # Remove leading EOS tokens\n    while example[\"text\"].startswith(tokenizer.eos_token):\n        example[\"text\"] = example[\"text\"][len(tokenizer.eos_token) :]\n    return example\n\ndataset_text = dataset_text.map(lambda x: add_special_tokens(x, tokenizer))\nprint(f\"{dataset_text=}\")\nprint(f\"{dataset_text[0]=}\")\n>>> dataset_text=Dataset({\n    features: ['text'],\n    num_rows: 5\n})\n>>> dataset_text[0]={'text': '\\n\\n### Human: How do you say \"dog\" in Spanish?\\n\\n### Assistant:<s> perro</s>'}\n```", "```py\n# tokenize the text\ndataset = dataset_text.map(\n    lambda example: tokenizer(example[\"text\"]), batched=True, remove_columns=[\"text\"]\n)\n# copy the input_ids to labels\ndataset = dataset.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\nprint(f\"{dataset=}\")\nprint(f\"{dataset[0]['input_ids']=}\")\nprint(f\"{dataset[0]['labels']=}\")\n>>> dataset=Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 5\n})\n>>> dataset[0]['input_ids']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n>>> dataset[0]['labels']=[603, 105311, 22256, 29, 7535, 727, 1152, 5894, 20587, 744, 5, 361, 49063, 7076, 105311, 143005, 29, 1, 82208, 2]\n```", "```py\n# training code inspired by\n#https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html\nmodel = load_model(model_name)\noutput_dir = \"./results\"\n# How many times to iterate over the entire dataset\nnum_train_epochs = 15\n# We're not aligning the sequence length (ie padding or truncating)\n# so batch training won't work for our toy example.\nper_device_train_batch_size = 1\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    seed=1,\n)\ntrainer = Trainer(\n    model=model,\n    train_dataset=dataset,\n    args=training_arguments,\n)\ntraining1 = trainer.train()\n\n# Sample generate prediction on holdout set\n“\\n\\n### Human: How do you say \"good\" in Spanish?\\n\\n### Assistant:”\n# the correct output is “bueno</s>”\n\nsample_generate(model, tokenizer, holdout_inputs, max_new_tokens=5)\n>>> ‘</s>’\n```", "```py\ntrainer.train()\nsample_generate(model, tokenizer, holdout_input, max_new_tokens=5)\n>>> bueno </s>\n```", "```py\nprint_iterative_generate(model, tokenizer, inputs)\n>>>\n#\n: How do you say \"how morning in Spanish?\n\n### Assistant: gu buenopu\n```", "```py\ndef create_special_mask(example: Dict) -> Dict:\n    \"\"\"Mask human text and keep assistant text as it is.\n\n    Args:\n        example (Dict): Result of tokenizing some text\n\n    Returns:\n        Dict: The dict with the label masked\n    \"\"\"\n    # setting a token to -100 is how we \"mask\" a token\n    # and tell the model to ignore it when calculating the loss\n    mask_token_id = -100\n    # assume we always start with a human text\n    human_text = True\n    for idx, tok_id in enumerate(example[\"labels\"]):\n        if human_text:\n            # mask all human text up until and including the bos token\n            example[\"labels\"][idx] = mask_token_id\n            if tok_id == tokenizer.bos_token_id:\n                human_text = False\n        elif not human_text and tok_id == tokenizer.eos_token_id:\n            # don’t mask the eos token, but the next token will be human text to mask\n            human_text = True\n        elif not human_text:\n            # leave example['labels'] text as it is when assistant text\n            continue\n    return example\n\ndataset_masked = dataset.map(create_special_mask)\n# convert dataset from lists to torch tensors\ndataset_masked.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\nprint(f\"{dataset_masked[0][\"labels\"]=}\")\n\n>>> dataset[0][\"labels\"]=tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 82208,     2])\n```", "```py\nmodel = load_model(model_name)\ntrainer = Trainer(\n    model=model,\n    train_dataset=dataset_masked,\n    args=training_arguments,\n)\n\ntraining2 = trainer.train()\n\nprint(f\"{training2.metrics['train_runtime']=}\")\nprint(f\"{training1.metrics['train_runtime'] =}\")\nprint(\n    f\"{100*round((training1.metrics['train_runtime']  - training2.metrics['train_runtime']) / training1.metrics['train_runtime'] , 2)}%\"\n)\n\n>>> training2.metrics['train_runtime']=61.7164\n>>> training1.metrics['train_runtime'] =70.8013\n>>> 13.0%\n```", "```py\nsample_generate(model, tokenizer, holdout_input, max_new_tokens=5)\n>>> bueno </s>\n```", "```py\nprint_iterative_generate(model, tokenizer, inputs)\n>>>#include\n code\n to I get \"we\" in English?\nA: Spanish: How bueno\n```", "```py\nmodel = load_model(model_name)\n\n# a hugging face function to do the copying of labels for you.\n# using the instruction and response templates will mask everything between the instruction template and the start of the response_template\ncollator = DataCollatorForCompletionOnlyLM(\n    instruction_template=tokenizer.eos_token,\n    response_template=tokenizer.bos_token,\n    tokenizer=tokenizer,\n)\n\ntrainersft = SFTTrainer(\n    model,\n    train_dataset=dataset_text,\n    dataset_text_field=\"text\",\n    data_collator=collator,\n    args=training_arguments,\n    tokenizer=tokenizer,\n)\nsftrain = trainersft.train()\n\nsample_generate(model, tokenizer, holdout_input, max_new_tokens=5)\n>>> ' bueno</s>'\n```"]