- en: Depth Anything â€”A Foundation Model for Monocular Depth Estimation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Depth Anything â€” å•ç›®æ·±åº¦ä¼°è®¡çš„åŸºç¡€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20](https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20](https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20)
- en: '[ğŸš€Saschaâ€™s Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[ğŸš€Saschaçš„è®ºæ–‡ä¿±ä¹éƒ¨](https://towardsdatascience.com/tagged/saschas-paper-club)'
- en: 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data by L. Yang
    et. al.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Depth Anythingï¼šé‡Šæ”¾å¤§è§„æ¨¡æœªæ ‡è®°æ•°æ®çš„åŠ›é‡ï¼ŒL. Yangç­‰äºº
- en: '[](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    Â·11 min readÂ·Mar 20, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š11åˆ†é’ŸÂ·2024å¹´3æœˆ20æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/072b25a65b15d2c83fdfe3a30967c559.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/072b25a65b15d2c83fdfe3a30967c559.png)'
- en: Image created from [publication](https://arxiv.org/abs/2401.10891) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥è‡ª[Sascha Kirch](https://medium.com/@SaschaKirch)çš„[å‡ºç‰ˆç‰©](https://arxiv.org/abs/2401.10891)
- en: Monocular depth estimation, the prediction of distance in 3D space from a 2D
    image. The â€œill posed and inherently ambiguous problemâ€, as stated in literally
    every paper on depth estimation, is a fundamental problem in computer vision and
    robotics. At the same time foundation models dominate the scene in deep learning
    based NLP and computer vision. Wouldnâ€™t it be awesome if we could leverage their
    success for depth estimation too?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å•ç›®æ·±åº¦ä¼°è®¡ï¼Œå³ä»äºŒç»´å›¾åƒé¢„æµ‹ä¸‰ç»´ç©ºé—´ä¸­çš„è·ç¦»ã€‚æ­£å¦‚å‡ ä¹æ‰€æœ‰å…³äºæ·±åº¦ä¼°è®¡çš„è®ºæ–‡æ‰€æŒ‡å‡ºçš„é‚£æ ·ï¼Œè¿™ä¸ªâ€œç—…æ€ä¸”å›ºæœ‰æ¨¡ç³Šçš„é—®é¢˜â€æ˜¯è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººå­¦ä¸­çš„ä¸€ä¸ªåŸºç¡€æ€§é—®é¢˜ã€‚åŒæ—¶ï¼ŒåŸºç¡€æ¨¡å‹ä¸»å¯¼äº†åŸºäºæ·±åº¦å­¦ä¹ çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚è‹¥æˆ‘ä»¬èƒ½å¤Ÿå°†å®ƒä»¬çš„æˆåŠŸåº”ç”¨äºæ·±åº¦ä¼°è®¡ï¼Œå²‚ä¸æ˜¯å¤ªæ£’äº†ï¼Ÿ
- en: In todayâ€™s paper walkthrough weâ€™ll dive into Depth Anything, a foundation model
    for monocular depth estimation. We will discover its architecture, the tricks
    used to train it and how it is used for metric depth estimation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»Šå¤©çš„è®ºæ–‡è®²è§£ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨Depth Anythingï¼Œè¿™æ˜¯ä¸€ç§ç”¨äºå•ç›®æ·±åº¦ä¼°è®¡çš„åŸºç¡€æ¨¡å‹ã€‚æˆ‘ä»¬å°†äº†è§£å®ƒçš„æ¶æ„ã€è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æŠ€å·§ä»¥åŠå®ƒå¦‚ä½•ç”¨äºåº¦é‡æ·±åº¦ä¼°è®¡ã€‚
- en: '**Paper:** [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891),
    Lihe Yang et.al., 19 Jan. 2024'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è®ºæ–‡ï¼š** [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891)ï¼ŒLihe
    Yangç­‰ï¼Œ2024å¹´1æœˆ19æ—¥'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/LiheYoung/Depth-Anything) â€” [Project
    Page](https://depth-anything.github.io/) â€” [Demo](https://huggingface.co/spaces/LiheYoung/Depth-Anything)
    â€” [Checkpoints](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**èµ„æºï¼š** [GitHub](https://github.com/LiheYoung/Depth-Anything) â€” [é¡¹ç›®é¡µé¢](https://depth-anything.github.io/)
    â€” [æ¼”ç¤º](https://huggingface.co/spaces/LiheYoung/Depth-Anything) â€” [æ£€æŸ¥ç‚¹](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main)'
- en: ''
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conference:** CVPR2024'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ä¼šè®®ï¼š** CVPR2024'
- en: ''
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation models, monocular depth estimation'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«ï¼š** åŸºç¡€æ¨¡å‹ï¼Œå•ç›®æ·±åº¦ä¼°è®¡'
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[**å…¶ä»–è®²è§£**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    â€” [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    â€” [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    â€” [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    â€” [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
