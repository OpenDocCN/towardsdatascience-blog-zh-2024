- en: Depth Anything ‚ÄîA Foundation Model for Monocular Depth Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20](https://towardsdatascience.com/depth-anything-a-foundation-model-for-monocular-depth-estimation-8a7920b5c9cc?source=collection_archive---------3-----------------------#2024-03-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[üöÄSascha‚Äôs Paper Club](https://towardsdatascience.com/tagged/saschas-paper-club)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data by L. Yang
    et. al.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Sascha
    Kirch](../Images/a0d45da9dc9c602075b2810786c660c9.png)](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    [Sascha Kirch](https://medium.com/@SaschaKirch?source=post_page---byline--8a7920b5c9cc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a7920b5c9cc--------------------------------)
    ¬∑11 min read¬∑Mar 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/072b25a65b15d2c83fdfe3a30967c559.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created from [publication](https://arxiv.org/abs/2401.10891) by [Sascha
    Kirch](https://medium.com/@SaschaKirch)
  prefs: []
  type: TYPE_NORMAL
- en: Monocular depth estimation, the prediction of distance in 3D space from a 2D
    image. The ‚Äúill posed and inherently ambiguous problem‚Äù, as stated in literally
    every paper on depth estimation, is a fundamental problem in computer vision and
    robotics. At the same time foundation models dominate the scene in deep learning
    based NLP and computer vision. Wouldn‚Äôt it be awesome if we could leverage their
    success for depth estimation too?
  prefs: []
  type: TYPE_NORMAL
- en: In today‚Äôs paper walkthrough we‚Äôll dive into Depth Anything, a foundation model
    for monocular depth estimation. We will discover its architecture, the tricks
    used to train it and how it is used for metric depth estimation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Paper:** [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891),
    Lihe Yang et.al., 19 Jan. 2024'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Resources:** [GitHub](https://github.com/LiheYoung/Depth-Anything) ‚Äî [Project
    Page](https://depth-anything.github.io/) ‚Äî [Demo](https://huggingface.co/spaces/LiheYoung/Depth-Anything)
    ‚Äî [Checkpoints](https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Conference:** CVPR2024'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Category:** foundation models, monocular depth estimation'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[**Other Walkthroughs**](https://medium.com/@SaschaKirch/list/paper-walkthroughs-by-sascha-kirch-89c7847da8e2)**:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[[BYOL](/byol-the-alternative-to-contrastive-self-supervised-learning-5d0a26983d7c?sk=fc5a3b3a556088181d8726226862252c)]
    ‚Äî [[CLIP](/the-clip-foundation-model-7770858b487d?sk=a7b10ba1d0c3a20ecd4adb8200a48500)]
    ‚Äî [[GLIP](/glip-introducing-language-image-pre-training-to-object-detection-5ddb601873aa?sk=4f0acb404a38d342b7669f861c013a05)]
    ‚Äî [[Segment Anything](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DINO](/segment-anything-promptable-segmentation-of-arbitrary-objects-f28958c5612d?sk=bd1311a6d8b1e0e6d3369d536dba0700)]
    ‚Äî [[DDPM](/the-rise-of-diffusion-models-a-new-era-of-generative-deep-learning-3ef4779f6e1b?sk=8c178422a977c6f49ec24b13502be4fd)]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
