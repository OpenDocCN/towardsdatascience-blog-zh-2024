<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Heckman Selection Bias Modeling in Causal Studies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Heckman Selection Bias Modeling in Causal Studies</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/heckman-selection-bias-modeling-in-causal-studies-30e207987025?source=collection_archive---------9-----------------------#2024-08-14">https://towardsdatascience.com/heckman-selection-bias-modeling-in-causal-studies-30e207987025?source=collection_archive---------9-----------------------#2024-08-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a0df" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How selection bias is related to the identification assumptions of OLS, and what steps should be taken to address it</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pollak.daniel?source=post_page---byline--30e207987025--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Pollak" class="l ep by dd de cx" src="../Images/a48f0aa944aeb4189e75cfc99949b4a7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*dn7WpqyK53Mbg_hxTfgFWg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--30e207987025--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@pollak.daniel?source=post_page---byline--30e207987025--------------------------------" rel="noopener follow">Daniel Pollak</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--30e207987025--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/8ca5a683283586a11d43d54ab03986aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*dlK3XLj7UWE1dFSr"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Photo by <a class="af ni" href="https://unsplash.com/@dimitry_b?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dimitry B</a> on <a class="af ni" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a042" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk of"><span class="l og oh oi bo oj ok ol om on ed">T</span>hroughout my applied studies, I struggled to grasp the complexities of selection and sample bias problems. These issues manifest in various forms, can arise from different factors, and can affect both external and internal validity in causal models. Additionally, they are often the source of semantic confusion.</p><p id="024b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One of the foundational concepts to understand when addressing bias and inconsistencies in a linear causal model is the <strong class="nl fr">omitted variable problem</strong>. This occurs when a typically unobserved random variable is correlated with both the independent variable and the model error. Failing to account for this variable when estimating a linear model leads to biased estimators. Consequently, this problem hinders the isolation of variance in the dependent variable in response to changes in the independent variable, thus obscuring the true causal relationship between the two.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp oo"><img src="../Images/e2ffe873e11bc0eac0f1fb74cc3ca058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*0fWpKYAkik_TTRPzQXV_Aw.png"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Confounder variable in causal DAG</figcaption></figure><p id="e4fa" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Are these concepts connected? Can selection bias be considered a form of the omitted variable problem? Let’s dive in and explore!</p></div></div></div><div class="ab cb op oq or os" role="separator"><span class="ot by bm ou ov ow"/><span class="ot by bm ou ov ow"/><span class="ot by bm ou ov"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3360" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Background</h1><p id="1cc2" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">I’d like to lay out the foundational elements needed to fully grasp how selection bias affects our linear model estimation process. We have a dependent random variable, Y, which we assume has a linear relationship (subject to some error terms) with another variable, X, known as the independent variable.</p><h2 id="bb96" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk">Identification Assumptions</h2><p id="69b6" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">Given a subsample Y’, X’ of the population variables Y, X -</p><ol class=""><li id="7ca7" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qp qq qr bk">The error terms (<strong class="nl fr">of the original model !!!</strong>) and X’ are not correlated.</li><li id="531b" class="nj nk fq nl b go qs nn no gr qt nq nr ns qu nu nv nw qv ny nz oa qw oc od oe qp qq qr bk">The mean of the error terms is zero.</li><li id="e3cd" class="nj nk fq nl b go qs nn no gr qt nq nr ns qu nu nv nw qv ny nz oa qw oc od oe qp qq qr bk">Y and X are really related in a linear way —</li></ol><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp qx"><img src="../Images/9a4c58ead8f2e2f9a2e5d8b01f5e4722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*cXZsoi0HiG5ULLxuiCSmgw.png"/></div></figure><p id="ae81" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s important to note that in empirical research, we observe X and Y (or a subsample of them), but <strong class="nl fr">we don’t observe the error terms</strong>, making assumption (1) impossible to test or validate directly. At this point, we usually rely on a theoretical explanation to justify this assumption. A common justification is through randomized controlled trials (RCTs), where the subsample, X, is collected entirely at random, ensuring that it is uncorrelated with any other variable, particularly with the error terms.</p><h2 id="dc6d" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk">Conditional Expectation</h2><p id="c68f" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">Given the assumptions mentioned earlier, we can precisely determine the form of the conditional expectation of Y given X —</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qy"><img src="../Images/675b7d031b44f15b10d8266152485914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oERYRFWT_5W9l1wzrYwm-A.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Conditional expectation in linear models</figcaption></figure><p id="3f01" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The last transition follows from the identification assumptions. It’s important to note that this is a function of x, meaning it represents the average of all observed values of y given that x is equal to a specific value (Or the local average of y’s given a small range of values of x’s — more information can be found <a class="af ni" href="https://theeffectbook.net/ch-DescribingRelationships.html#conditional-means" rel="noopener ugc nofollow" target="_blank">here</a>)</p><h2 id="a354" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk">OLS</h2><p id="c34d" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">Given a sample of X that meets the identification assumptions, it’s well-established that the ordinary least squares (OLS) method provides a closed-form solution for consistent and unbiased estimators of the linear model parameters, alpha and beta, and thus for the conditional expectation function of Y given X.</p><p id="2fd3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">At its core, OLS is a technique for <strong class="nl fr">fitting a linear line</strong> (or linear hyperplane in the case of a multivariate sample) to a set of (y_i, x_i) pairs. What’s particularly interesting about OLS is that —</p><ol class=""><li id="3ea7" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qp qq qr bk">If Y and X have a linear relationship (accounting for classic error terms), we’ve seen that the conditional expectation of Y given X is perfectly linear. In this scenario, OLS effectively uncovers this function with strong statistical accuracy.</li><li id="479c" class="nj nk fq nl b go qs nn no gr qt nq nr ns qu nu nv nw qv ny nz oa qw oc od oe qp qq qr bk">OLS achieves this even with <strong class="nl fr">any subsample</strong> of X that meets the identification assumptions previously discussed — with large enough sample.</li></ol><h1 id="8677" class="ox oy fq bf oz pa qz gq pc pd ra gt pf pg rb pi pj pk rc pm pn po rd pq pr ps bk">Motivation</h1><p id="485f" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">Let’s begin with a straightforward example using simulated data. We’ll simulate the linear model from above.</p><p id="e505" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">A significant advantage of working with simulated data is that it allows us to better understand relationships between variables that are not observable in real-world scenarios, such as the error terms in the model.</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="fa33" class="ri oy fq rf b bg rj rk l rl rm">import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import statsmodels.api as sm<br/><br/>N = 5000<br/>BETA = 0.7<br/>INTERCEPT = -2<br/>SIGMA = 5<br/><br/>df = pd.DataFrame({<br/>    "x": np.random.uniform(0, 50, N),<br/>    "e": np.random.normal(0, SIGMA, N)<br/>})<br/>df["y"] = INTERCEPT + BETA * df["x"] + df["e"]</span></pre><p id="2664" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and running OLS for the full sample -</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="2f55" class="ri oy fq rf b bg rj rk l rl rm">r1 = sm.OLS(df["y"], sm.add_constant(df["x"])).fit()<br/>plt.scatter(df["x"], df["y"], label="population")<br/>plt.plot(df["x"], r1.fittedvalues, label="OLS Population", color="r")</span></pre><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rn"><img src="../Images/d8e5548a623c71f3021dd30046846b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-kyDnjvQKMu78rfohfHPw.png"/></div></div></figure><p id="c57b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, let’s generate a random subsample of our population, X, and apply OLS to this subsample. I’ll randomly select 100 x’s from the 500 samples I previously generated, and then run OLS on this subset.</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="7319" class="ri oy fq rf b bg rj rk l rl rm">sample1 = df.sample(100)<br/>r2 = sm.OLS(sample1["y"], sm.add_constant(sample1["x"])).fit()<br/><br/>plt.scatter(sample1["x"], sample1["y"], label="sample")<br/>plt.plot(sample1["x"], r2.fittedvalues, label="OLS Random sample")</span></pre><p id="e28f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and plot -</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ro"><img src="../Images/9ea78ac3e7ba238e593e06adf5b3ebc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESShUQAN2H6SqTd74eO6zw.png"/></div></div></figure><p id="3016" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It appears we obtain consistent estimators for the random subsample, as both OLS results produce quite similar conditional expectation lines. Additionally, you can observe the correlation between X and the error terms —</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="8d01" class="ri oy fq rf b bg rj rk l rl rm">print(f"corr {np.corrcoef(df['x'], df['e'])}")<br/>print(f"E(e|x) {np.mean(df['e'])}")<br/><br/># corr [[ 1.         -0.02164744]<br/>#  [-0.02164744  1.        ]]<br/># E(e|x) 0.004016713100777963</span></pre><p id="97b1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This suggests that the identification assumptions are being met. In practice, however, we cannot directly calculate these since the errors are not observable. Now, let’s create a new subsample — I’ll select all (y, x) pairs where y ≤ 10:</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="2f4b" class="ri oy fq rf b bg rj rk l rl rm">sample2 = df[df["y"] &lt;= 10]<br/>r3 = sm.OLS(sample2["y"], sm.add_constant(sample2["x"])).fit()<br/><br/>plt.scatter(sample2["x"], sample2["y"], label="Selected sample")<br/>plt.plot(sample["x"], r3.fittedvalues, label="OLS Selected Sample")</span></pre><p id="68fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and we get -</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rp"><img src="../Images/92fb9d05ee88537e6a2028ba330184bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjIuQr35toVNVEngqn_h1A.png"/></div></div></figure><p id="a5a9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, OLS has provided us with a completely different line. Let’s check the correlation between the subsample X’s and the errors.</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="f86d" class="ri oy fq rf b bg rj rk l rl rm">print(f"corr {np.corrcoef(df['x'], df['e'])}")<br/>print(f"E(e|x) {np.mean(df['e'])}")<br/><br/># corr [[ 1.         -0.48634973]<br/>#  [-0.48634973  1.        ]]<br/># E(e|x) -2.0289245650303616</span></pre><p id="e1f4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Seems like the identification assumptions are violated. Let’s also plot the sub-sample error terms, as a function of X -</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rq"><img src="../Images/83fcaf6e5c9f1ff6b6ac3f022ee00d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nT2ZVGCHUhoNY7v44DRTcw.png"/></div></div></figure><p id="4466" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As you can see, as X increases, there are fewer large errors, indicating a clear correlation that results in biased and inconsistent OLS estimators.</p><p id="8357" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s explore this further.</p><h1 id="61b5" class="ox oy fq bf oz pa qz gq pc pd ra gt pf pg rb pi pj pk rc pm pn po rd pq pr ps bk">Modeling</h1><p id="724b" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">So, what’s going on here?</p><p id="28ea" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I’ll reference the model introduced by James Heckman, who, along with Daniel McFadden, received the Nobel Memorial Prize in Economic Sciences in 2000. Heckman is renowned for his pioneering work in econometrics and microeconomics, particularly for his contributions to addressing selection bias and self-selection in quantitative analysis. His well-known Heckman correction will be discussed later in this context.</p><p id="6ad6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In his paper from 1979, “Sample Selection Bias as a Specification Error,” Heckman illustrates how selection bias arises from censoring the dependent variable — a specific case of selection that can be extended to more non-random sample selection processes.</p><p id="e8c6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Censoring the dependent variable is exactly what we did when creating the last subsample in the previous section. Let’s examine Heckman’s framework.</p><p id="7437" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We start with a full sample (or population) of (y_i, x_i) pairs. In this scenario, given x_i, ε_i can vary — it can be positive, negative, small, or large, depending solely on the error distribution. We refer to this complete sample of the dependent variable as y*. We then define y as the censored dependent variable, which includes only the values we actually observe.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp rr"><img src="../Images/4d2158cfcf3d7fc86a1652cc5430bee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*ezwW4xzyNJJmRdI4hXQ7uw.png"/></div></figure><p id="bfa1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, let’s calculate the conditional expectation of the censored variable, y:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rs"><img src="../Images/b1b77e1aa939cf4b1452c3d4aeb796fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7z01atp8qVsZGPCU-sBHw.png"/></div></div></figure><p id="16c3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As you can see, this function resembles the one we saw earlier, but it includes an additional term that differs from before. This last term cannot be ignored, which means the conditional expectation function is <strong class="nl fr">not purely linear</strong> in terms of x (with some noise). Consequently, running OLS on the uncensored values will produce biased estimators for alpha and beta.</p><p id="59fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Moreover, this equation illustrates how the selection bias problem can be viewed as an <strong class="nl fr">omitted variable</strong> problem. Since the last term depends on X, it shares a significant amount of variance with the dependent variable.</p><h1 id="2bc9" class="ox oy fq bf oz pa qz gq pc pd ra gt pf pg rb pi pj pk rc pm pn po rd pq pr ps bk">Heckman’s Correction</h1><h2 id="d8d6" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk"><strong class="al">Inverse Mills ratio</strong></h2><p id="d962" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">Heckman’s correction method is based on the following principle: Given a random variable Z that follows a normal distribution with mean μ and standard deviation σ, the following equations apply:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rt"><img src="../Images/bee5544cf6b47d5414ca07ac6945e0b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yTrmQ29chX_cFgAv7E6zOA.png"/></div></div></figure><p id="4037" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Given any constant α, Φ (capital phi) represents the standard normal distribution’s CDF, and ɸ denotes the standard normal distribution’s PDF. These values are known as the <strong class="nl fr">inverse Mills ratio</strong>.</p><p id="05cd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, how does this help us? Let’s revisit the last term of the previous conditional expectation equation —</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp ru"><img src="../Images/6a0c28909cde6017e96ff61941315479.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*V0IB2kvsT2hcvPFeRFi5aQ.png"/></div></figure><p id="3e03" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Combined with the fact that our error terms follow a normal distribution, we can use the inverse Mills ratio to characterize their behavior.</p><h2 id="c00c" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk">Back to our model</h2><p id="2b16" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">The advantage of the inverse Mills ratio is that it transforms the previous conditional expectation function into the following form —</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rv"><img src="../Images/34182c74fe884cfa79f029dbb26060db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-vwZ42ReXerkY-IKgDznQ.png"/></div></div></figure><p id="ebb8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This results in a linear function with an additional covariate — the inverse Mills ratio. Therefore, to estimate the model parameters, we can apply OLS to this revised formula.</p><p id="1950" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s first calculate the inverse Mills ratio -</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp rw"><img src="../Images/3080cc31de48d31065b1b2686b2c64b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqqdfI6BHHCF_HbWZmG2-w.png"/></div></div></figure><p id="723f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and in code:</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="a71c" class="ri oy fq rf b bg rj rk l rl rm">from scipy.stats import norm<br/><br/>sample["z"] = (CENSOR-INTERCEPT-BETA*sample["x"])/SIGMA<br/>sample["mills"] = -SIGMA*(norm.pdf(sample["z"])/(norm.cdf(sample["z"])))</span></pre><p id="63fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and run OLS —</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="3979" class="ri oy fq rf b bg rj rk l rl rm">correcred_ols = sm.OLS(sample["y"], sm.add_constant(sample[["x", "mills"]])).fit(cov_type="HC1")<br/>print(correcred_ols.summary())</span></pre><p id="4720" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">And the output —</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="d52a" class="ri oy fq rf b bg rj rk l rl rm">                            OLS Regression Results                            <br/>==============================================================================<br/>Dep. Variable:                      y   R-squared:                       0.313<br/>Model:                            OLS   Adj. R-squared:                  0.313<br/>Method:                 Least Squares   F-statistic:                     443.7<br/>Date:                Mon, 12 Aug 2024   Prob (F-statistic):          3.49e-156<br/>Time:                        16:47:01   Log-Likelihood:                -4840.1<br/>No. Observations:                1727   AIC:                             9686.<br/>Df Residuals:                    1724   BIC:                             9702.<br/>Df Model:                           2                                         <br/>Covariance Type:                  HC1                                         <br/>==============================================================================<br/>                 coef    std err          z      P&gt;|z|      [0.025      0.975]<br/>------------------------------------------------------------------------------<br/>const         -1.8966      0.268     -7.088      0.000      -2.421      -1.372<br/>x              0.7113      0.047     14.982      0.000       0.618       0.804<br/>mills          1.0679      0.130      8.185      0.000       0.812       1.324<br/>==============================================================================<br/>Omnibus:                       96.991   Durbin-Watson:                   1.993<br/>Prob(Omnibus):                  0.000   Jarque-Bera (JB):              115.676<br/>Skew:                          -0.571   Prob(JB):                     7.61e-26<br/>Kurtosis:                       3.550   Cond. No.                         34.7<br/>==============================================================================</span></pre><h2 id="322d" class="py oy fq bf oz pz qa qb pc qc qd qe pf ns qf qg qh nw qi qj qk oa ql qm qn qo bk">In Reality</h2><p id="cc7e" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">α and β are the unobserved parameters of the model that we aim to estimate, so in practice, we cannot directly calculate the inverse Mills ratio as we did previously. Heckman introduces a preliminary step in his correction method to assist in estimating the inverse Mills ratio. This is why the Heckman’s correction is known as a <strong class="nl fr">two stage estimator</strong>.</p><p id="aeaf" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To recap, our issue is that we don’t observe all the values of the dependent variable. For instance, if we’re examining how education (Z) influences wage (Y), but only observe wages above a certain threshold, we need to develop a theoretical explanation for the education levels of individuals with wages below this threshold. Once we have that, we can estimate a <a class="af ni" href="https://en.wikipedia.org/wiki/Probit" rel="noopener ugc nofollow" target="_blank">probit</a> model of the following form:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp rx"><img src="../Images/8d7d7775e31a6d62647800c4a63ec600.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*u-wVcPbDllu1waOaHnCI4g.png"/></div></figure><p id="1c4c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and use the estimated parameters of this probit model to calculate an estimator for the inverse Mills ratio. In our case (notice I don’t use α and β) —</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="96fa" class="ri oy fq rf b bg rj rk l rl rm">from statsmodels.discrete.discrete_model import Probit<br/><br/>pbit = Probit(df["y"] &lt;= CENSOR, sm.add_constant(df["x"])).fit()<br/>sample["z_pbit"] = sample["z"] = (pbit.params.const + sample["x"]*pbit.params.x)<br/>sample["mills_pbit"] = -SIGMA*(norm.pdf(sample["z_pbit"])/(norm.cdf(sample["z_pbit"])))<br/>correcred_ols = sm.OLS(sample["y"], sm.add_constant(sample[["x", "mills_pbit"]])).fit(cov_type="HC1")</span></pre><p id="b02d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">and again, OLS for the second stage gives us consistent estimators —</p><pre class="mr ms mt mu mv re rf rg bp rh bb bk"><span id="f4ad" class="ri oy fq rf b bg rj rk l rl rm">                            OLS Regression Results                            <br/>...<br/>==============================================================================<br/>                 coef    std err          z      P&gt;|z|      [0.025      0.975]<br/>------------------------------------------------------------------------------<br/>const         -1.8854      0.267     -7.068      0.000      -2.408      -1.363<br/>x              0.7230      0.049     14.767      0.000       0.627       0.819<br/>mills_pbit     1.1005      0.135      8.165      0.000       0.836       1.365<br/>==============================================================================</span></pre><h1 id="5431" class="ox oy fq bf oz pa qz gq pc pd ra gt pf pg rb pi pj pk rc pm pn po rd pq pr ps bk">Wrap up</h1><p id="d951" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">We used simulated data to demonstrate a sample selection bias problem resulting from censoring dependent variable values. We explored how this issue relates to OLS causal identification assumptions by examining the simulated errors of our model and the biased subsample. Finally, we introduced Heckman’s method for correcting the bias, allowing us to obtain consistent and unbiased estimators even when working with a biased sample.</p></div></div></div><div class="ab cb op oq or os" role="separator"><span class="ot by bm ou ov ow"/><span class="ot by bm ou ov ow"/><span class="ot by bm ou ov"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1140" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you enjoyed this story, I’d greatly appreciate your support —<a class="af ni" href="https://ko-fi.com/dapollak" rel="noopener ugc nofollow" target="_blank"> buying me a coffee would mean a lot!</a></p><h1 id="40e7" class="ox oy fq bf oz pa qz gq pc pd ra gt pf pg rb pi pj pk rc pm pn po rd pq pr ps bk">References</h1><p id="621f" class="pw-post-body-paragraph nj nk fq nl b go pt nn no gr pu nq nr ns pv nu nv nw pw ny nz oa px oc od oe fj bk">[1] James J. Heckman, <a class="af ni" href="https://www.jstor.org/stable/1912352" rel="noopener ugc nofollow" target="_blank">Sample Selection Bias as a Specification Error</a> (1979), Econometrica</p><p id="e7da" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">[2] Nick Huntington-Klein, <a class="af ni" href="https://theeffectbook.net/index.html" rel="noopener ugc nofollow" target="_blank">The Effect</a> Book (2022)</p><p id="be37" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">[3] Christopher Winship, <a class="af ni" href="https://typeset.io/pdf/models-for-sample-selection-bias-4qnrg4cc1f.pdf" rel="noopener ugc nofollow" target="_blank">Models for sample bias</a> (1992)</p><p id="652e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><em class="ry">Unless otherwise noted, all images are by the author</em></p></div></div></div></div>    
</body>
</html>