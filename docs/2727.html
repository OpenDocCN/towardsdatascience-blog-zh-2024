<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Preference Alignment for Everyone!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Preference Alignment for Everyone!</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08">https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c070" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Frugal RLHF with multi-adapter PPO on Amazon SageMaker</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Aris Tsakpinis" class="l ep by dd de cx" src="../Images/2cc1101aed68e1f71a0026bfdec28f58.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*HiuDPBJ3_XhJLRi40lBb7Q.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------" rel="noopener follow">Aris Tsakpinis</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">26 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/77e9a1dd091dae519aca481072d8bae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbLNScjpzOf80TRVme0VZA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by StableDiffusionXL on Amazon Web Services</figcaption></figure><p id="0b70" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note: All images, unless otherwise noted, are by the author.</p><h1 id="a521" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">What is this about and why is it important?</h1><p id="2fac" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Over the last 2 years, research and practice have delivered plenty of proof that preference alignment (PA) is a game changer for boosting Large Language Models (LLMs) performance, especially (but not exclusively) for models directly exposed to humans. PA uses (human) feedback to align model behavior to what is preferred in the environment a model is actually living in, instead of relying solely on proxy datasets like other fine-tuning approaches do (as I explain in detailed in <a class="af oz" rel="noopener" target="_blank" href="/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224">this blog post</a> on fine-tuning variations). This improvement in model performance, as perceived by human users, has been a key factor in making LLMs and other Foundation Models (FMs) more accessible and popular, contributing significantly to the current excitement around Generative AI.</p><p id="13d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Over time various approaches to PA have been proposed by research and quickly adapted by some practitioners. Amongst them, RLHF is (as of Autumn 2024) by far the most popular and proven approach.</p><p id="23ea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, due to challenges around implementation complexity, compute requirements or training orchestration, so far the adaptation of PA approaches like RLHF in practice is limited to mainly high-skill profile individuals and organizations like FM producers. Also, most practical examples and tutorials I found showcasing how to master an approach like RLHF are limited or incomplete.</p><p id="e654" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This blog post provides you with a comprehensive introduction into RLHF, discusses challenges around the implementation, and suggests RLHF with multi-adapter PPO, a light-weight implementation approach tackling some key ones of these challenges.</p><p id="b925" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we present an end-to-end (E2E) implementation of this approach in a Jupyter notebook, covering data collection, preparation, model training, and deployment. We leverage HuggingFace frameworks and Amazon SageMaker to provide a user-friendly interface for implementation, orchestration, and compute resources. The blog post then guides you through the key sections of this notebook, explaining implementation details and the rationale behind each step. This hands-on approach allows readers to understand the practical aspects of the process and easily replicate the results.</p><h1 id="5043" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">The principles of RLHF</h1><p id="db03" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Reinforcement learning from human feedback was one of the major hidden technical backbones of the early Generative AI hype, giving the breakthrough achieved with great large decoder models like Anthropic Claude or OpenAI’s GPT models an additional boost into the direction of user alignment.</p><p id="76bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The great success of PA for FMs perfectly aligns with the concept of user-centric product development, a core and well-established principle of agile product development. Iteratively incorporating feedback from actual target users has proven highly effective in developing outstanding products. This approach allows developers to continually refine and improve their offerings based on real-world user preferences and needs, ultimately leading to more successful and user-friendly products.</p><p id="df12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Other fine-tuning approaches like continued pre-training (CPT) or supervised fine-tuning (SFT) don’t cover this aspect since:</p><ul class=""><li id="f88b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">the datasets used for these approaches are (labelled or unlabelled) proxies for what we think our users like or need (i.e. knowledge or information, language style, acronyms or task-specific behaviour like instruction-following, chattiness or others), crafted by a few in charge of model training or fine-tuning data.</li><li id="f794" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk">the algorithm(s), training objective(s) and loss function(s) used for these approaches (i.e. causal language modeling) are using next-token prediction as proxy for higher level metrics (e.g. accuracy, perplexity, …).</li></ul><p id="3bcc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore, PA is undoubtedly a technique we should employ when aiming to create an exceptional experience for our users. This approach can significantly enhance the quality, safety and relevance of AI-generated responses, leading to more satisfying interactions and improved overall user satisfaction.</p><h2 id="6534" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">How does RLHF work?</h2><p id="b5f1" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk"><em class="pz">Note: This section is an adapted version of the RLHF section in my </em><a class="af oz" rel="noopener" target="_blank" href="/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224"><em class="pz">blog post about different fine-tuning variations</em></a><em class="pz">. For a comprehensive overview about fine-tuning you might want to check it out as well.</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/0c206faae0d0b57cb16f4bcbf0ba5631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gbdQlZYSHqhZI0uH.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: Reward model training for RLHF (Source: Lambert et al, 2022)</figcaption></figure><p id="74b6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">RLHF works in a two-step process and is illustrated in Figures 13 and 14:</p><p id="f4c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Step 1 (Figure 1): First, a reward model needs to be trained for later usage in the actual RL-powered training approach. Therefore, a prompt dataset aligned with the objective (e.g. chat/instruct model or domain-specific task objective) to optimize is being fed to the model to be fine-tuned, while requesting not only one but two or more inference results. These results will be presented to human labelers for scoring (1st, 2nd, 3rd, …) based on the optimization objective. There are also a few open-sourced preference ranking datasets, among them <a class="af oz" href="https://huggingface.co/datasets/Anthropic/hh-rlhf" rel="noopener ugc nofollow" target="_blank">“Anthropic/hh-rlhf”</a> (we will use this dataset in the practical part of this blog) which is tailored towards red-teaming and the objectives of honesty and harmlessness. After normalizing and converting the scores into reward values, a reward model is trained using individual sample-reward pairs, where each sample is a single model response. The reward model architecture is usually similar to the model to be fine-tuned, adapted with a small head eventually projecting the latent space into a reward value instead of a probability distribution over tokens. However, the ideal sizing of this model in parameters is still subject to research, and different approaches have been chosen by model providers in the past. In the practical part of this blog, for the reward model we will use the same model architecture compared to the model to be fine-tuned.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/762043a6b8c3fb52e183b1886888c937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5f2O_dl8EHliYO8O.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 2: Reinforcement learning based model tuning with PPO for RLHF (Source: Lambert et al, 2022)</figcaption></figure><p id="9fec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Step 2 (Figure 2): Our new reward model is now used for training the actual model. Therefore, another set of prompts is fed through the model to be tuned (grey box in illustration), resulting in one response each. Subsequently, these responses are fed into the reward model for retrieval of the individual reward. Then, Proximal Policy Optimization (PPO), a policy-based RL algorithm, is used to gradually adjust the model’s weights in order to maximize the reward allocated to the model’s answers. As opposed to Causal Language Modeling (CLM — you can find a detailed explanation <a class="af oz" href="https://medium.com/towards-data-science/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224" rel="noopener">here</a>), instead of gradient descent, this approach leverages gradient ascent (or gradient descent over <em class="pz">1 — reward</em>) since we are now trying to maximize an objective (reward). For increased algorithmic stability to prevent too heavy drifts in model behavior during training, which can be caused by RL-based approaches like PPO, a prediction shift penalty is being added to the reward term, penalizing answers diverging too much from the initial language model’s predicted probability distribution on the same input prompt.</p><h2 id="397e" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Challenges with RLHF</h2><p id="473d" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The way how RLHF is working poses some core challenges to implementing and running it at scale, amongst them the following:</p><p id="2860" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">- <strong class="ne fr">Cost of training the reward model:</strong> Picking the right model architecture and size for the reward model is still current state of research. These models are usually transformer models similar to the model to be fine-tuned, equipped with a modified head delivering reward scores instead of a vocabular probability distribution. This means, that independent from the actual choice, most reward models are in the billions of parameters. Full parameter training of such a reward model is data and compute expensive.</p><p id="c12e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">- <strong class="ne fr">Cost of training cluster:</strong> With the reward model (for the reward values), the base model (for the KL prediction shift penalty) and the model actually being fine-tuned three models need to be hosted in parallel in the training cluster. This leads to massive compute requirements usually only being satisfied by a multi-node cluster of multi-GPU instances (in the cloud), leading to hardware and operational cost.</p><p id="00e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">- <strong class="ne fr">Orchestration of training cluster:</strong> The RLHF algorithm requires a combination of inference- and training-related operations in every training loop. This needs to be orchestrated in a multi-node multi-GPU cluster while keeping communication overhead minimal for optimal training throughput.</p><p id="c8aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">- <strong class="ne fr">Training/inference cost in highly specialized setups:</strong> PA shines through aligning model performance towards a user group or target domain. Since most professional use cases are characterized by specialized domains with heterogenous user groups, this leads to an interesting tradeoff: Optimizing for performance will lead in training and hosting many specialized models excelling in performance. However, optimizing for resource consumption (i.e. cost) will lead to overgeneralization of models and decreasing performance.</p><h2 id="d676" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">RLHF with multi-adapter PPO</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/43d376a2044c3fc5511ddd397cca4912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65-tSmGhWl4imgj_5CqOrw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 3: Minimizing GPU footprint of PPO through dynamic multi-adapter loading</figcaption></figure><p id="dd98" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Multi-adapter PPO is a particularly GPU-frugal approach to the second step of the RLHF training process. Instead of using full-parameter fine-tuning, it leverages parameter-efficient fine-tuning (PEFT) techniques to reduce the infrastructure and orchestration footprint drastically. Instead of hosting three distinct models (model being fine-tuned, reward model, reference model for KL prediction shift penalty) in parallel in the training cluster this approach leverages Low Rank Adaptation (LoRA) adapters during the fine-tuning which are dynamically loaded and unloaded into the accelerators of the training cluster.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/3fd297299c11b108133ded22488394b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPmUozk1F74LrbuauE8Y9g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 4: E2E RLHF with multi-adapter PPO for a harmless Q&amp;A bot</figcaption></figure><p id="c1fe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While this approach’s goal is ultimately a resource and orchestration frugal approach to the second step of RLHF, it has implications on the first step:</p><ul class=""><li id="dad4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk"><strong class="ne fr">Reward model choice:</strong> A reward model with the same model architecture as the model to be fine-tuned is picked and equipped with a reward classification head.</li><li id="0163" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx pa pb pc bk"><strong class="ne fr">Reward model training approach:</strong> As illustrated in figure 4(2), instead of full-parameter reward model training, a reward model LoRA adapter is being trained, leading to a much leaner training footprint.</li></ul><p id="5ba5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similarly to the this, the RLHF fine-tuning of the model being performed in the second step is not done in a full-parameter fine-tuning manner. Instead, a LoRA adapter is trained. As depicted in figure 4, during a training iteration, first the RLHF model adapter is being loaded to generate model responses to the prompts of the current training batch (4a). Then, the reward model adapter is loaded to calculate the corresponding raw reward values (4b). To complete the reward term, the input prompt is fed through the base model for calculation of the KL prediction shift penalty. Therefor, all adapters need to be unloaded (4c, 4d). Finally, the RLHF model adapter is loaded again to perform the weight updates for this iteration step (4e).</p><p id="4ee6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This approach to RLHF reduces the memory footprint as well as orchestration complexity significantly.</p><h1 id="964a" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Running RLHF with multi-adapter PPO with HuggingFace and Amazon SageMaker</h1><p id="2cfa" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In what follows we will go through a notebook showcasing RLHF with multi-adapter PPO in an E2E fashion. Thereby we use HuggingFace and Amazon SageMaker for an especially user-friendly interface towards the implementation, orchestration and compute layers. The entire notebook can be found <a class="af oz" href="https://github.com/aws-samples/build-language-models-on-aws/tree/main/align-models-with-amazon-sagemaker/rlhf-with-multi-adapter-ppo" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 id="05d3" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Scenario</h1><p id="52a6" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The pace model producers nowadays are releasing new models is impressive. Hence, I want to keep the scenario we are looking into as generic as possible.</p><p id="47a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While most of the models published these days have already gone through multiple fine-tuning steps like SFT or even PA, since these models are general purpose ones they where certainly not performed tailored to your target users or target domain. This means that even though we are using a pre-aligned model (e.g. an instruction fine-tuned model), for optimising model performance in your domain further alignment steps are required.</p><p id="3dcb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For this blog we will assume the model should be optimised towards maximising the helpfulness while carrying out user-facing single- and multi-turn conversations in a Q&amp;A style in the scientific domain. Thus, we will start from a general-purpose instruct / Q&amp;A pre-trained FM.</p><h1 id="403c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Model</h1><p id="d713" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Despite of being generic we need to choose a model for our endeavour. For this blog we will be working with Meta Llama3.1–8b-instruct. This model is the smallest fashion of a new collection of multilingual pre-trained and instruction-tuned decoder models Meta released in Summer 2024. More details can be found in the <a class="af oz" href="https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1#llama-3.1-instruct" rel="noopener ugc nofollow" target="_blank">documentation</a> in the Meta homepage and in the <a class="af oz" href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct" rel="noopener ugc nofollow" target="_blank">model card</a> provided by HuggingFace.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/92f35b3b8bd00507db12d2a4a26d3815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLaufsuJsEdNVV1apuBOow.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 5: Llama-3.1–8b-instruct model card on HuggingFace hub</figcaption></figure><h1 id="0672" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Prerequisites</h1><p id="4c62" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We start our notebook walkthrough with some prerequisite preparation steps.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/448c07c74376c62d180935b6d48a716b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDOSdTbr2qbqMBraS0RyNQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 6: Accepting Meta’s licensing agreement through HuggingFace hub</figcaption></figure><p id="a55d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will be retrieving the model’s weights from the HuggingFace model hub. To be able to do so we need to accept Meta‘s licensing agreement and provide some information. This can be submitted directly through the HuggingFace model hub.</p><p id="aa57" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further, for storage of the adapter weights of both the reward model as well as the preference-aligned model we will be using private model repositories on the HuggingFace model hub. This requires a HuggingFace account. Once logged into the HuggingFace platform we need to create two model repositories. For this click on the account icon on the top right of the HuggingFace landing page and pick “+ New Model” in the menu.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/913dabd67e327e505f481f44bc411501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sU8Ycy3tQvvIDpq4NfeKiA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 7: Creating model repositories on HuggingFace model hub</figcaption></figure><p id="651d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can then create two private model repositories. Feel free to stick to my naming convention or pick a name of choice. If you name your repositories differently make sure to also adjust the code in the notebook.</p><p id="01e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once created, we can see the model repositories in our HuggingFace profile.</p><p id="1df8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To authenticate against the HuggingFace model hub when pulling or pushing models we need to create an access token, which we will use later in the notebook. For this click on the account icon on the top right of the HuggingFace landing page and pick „Settings“ in the menu.</p><p id="fee3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the settings we select the menu item “Access Tokens” and then “+ Create new token.”</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/6444c90a1d12546dd6a9699087935638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0drz8jVzPyzUTMV58uXc8Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 8: Creating access tokens on HuggingFace hub</figcaption></figure><p id="dd47" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">According to the principle of least privileges we want to create a token with fine-grained permission configurability. For our purpose read and write access to repositories is sufficient — this is why we check all three boxes in this section. Then we scroll down and create the token.</p><p id="9431" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once created the access token appears in plain text. Since the token will only be displayed once it makes sense to store it in encrypted format for example in a password manager.</p><h1 id="077b" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Datasets</h1><p id="5adf" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Now that we are finished with the prerequisites we can move on to the datasets we will be using for our endeavor.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/91ccb2b30be64e7b3eb90d0bb193d5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qYJUHWjfu5qHYnKGGF8yg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 9: Anthropic hh-rlhf dataset on HuggingFace hub</figcaption></figure><p id="c23e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For training our reward model we will be using the Anthropic/hh-rlhf dataset, which is distributed under <a class="af oz" href="https://opensource.org/license/mit" rel="noopener ugc nofollow" target="_blank">MIT license</a>. This is a handcrafted preference dataset Anthropic has open-sourced. It consists of chosen and rejected model completions to one and the same prompt input. Further, it comes in different fashions, targeting alignment areas like harmlessness, helpfulness and more. For our demonstration we will use the ”helpful” subset to preference align our Llama model towards helpful answers.</p><p id="7335" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the actual PA step with PPO and the previously trained reward model we need an additional dataset representing the target domain of our model. Since we are fine-tuning an instruct model towards helpfulness we need a set of instruction-style prompts. <a class="af oz" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">The Stanford Question&amp;Answering dataset (SQuAD)</a>, distributed under the <a class="af oz" href="https://creativecommons.org/licenses/by-sa/4.0/deed.en" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0 license</a><strong class="ne fr">, </strong>provides us with question — context — answer pairs across a broad range of different areas of expertise. For our experiment we will aim for single-turn open Question&amp;Answering. Hence we will use only the “question” feature of the dataset.</p><h1 id="7420" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Code repository</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/8558c4acfd289147c8bac0697f5e8d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4v5krfYmJX_igEciT6lTQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 10: Code repository</figcaption></figure><p id="d691" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After having looked into the datasets we will use let‘s take a look into the directory structure and the files we will use in this demonstration. The directory consists of 3 files: config.yaml, a configuration file for running SageMaker jobs through the remote decorator and requirements.txt for extending the dependencies installed in the training container. Finally, there is the rlhf-multi-adapter-ppo.ipynb notebook containing the code for our E2E PA.</p><p id="87e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The previously mentioned config.yaml file holds important configurations for the training jobs triggered by the remote decorator, e.g. training instance type or training image.</p><h1 id="0dd0" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Notebook</h1><p id="52be" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Now, let’s open the rlhf-multi-adapter-ppo.ipynb notebook. First, we install and import the required dependencies.</p><h2 id="7b88" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Data preprocessing reward model training dataset</h2><p id="3b86" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">As previously discussed, we will be using the Anthropic/hh-rlhf dataset for training our reward model. Therefore, we need to convert the raw dataset into the above specified structure, where “input_ids” and “attention_mask” are the outputs of input tokenization. This format is specified as interface definition by the HuggingFace trl RewardTrainer class and makes the accepted and rejected answers easily accessible during reward model training.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="b452" class="qo nz fq ql b bg qp qq l qr qs">DatasetDict({<br/>    train: Dataset({<br/>        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],<br/>        num_rows: ...<br/>    })<br/>    test: Dataset({<br/>        features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],<br/>        num_rows: ...<br/>    })<br/>})</span></pre><p id="2531" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We login to the HuggingFace hub. Then, we retrieve the “helpful-base” of the „Anthropic/hh-rlhf“ dataset. The raw dataset structure looks as follows, we also take a look into an example dataset item.</p><p id="fb1c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we parse the conversations into an array seperated by conversation turn and role.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="fdbc" class="qo nz fq ql b bg qp qq l qr qs">def extract_dialogue(input_text):<br/>    # Split the input by lines and initialize variables<br/>    lines = input_text.strip().split("\n\n")<br/>    dialogue_list = []<br/><br/>    # Iterate through each line and extract the dialogue<br/>    for line in lines:<br/>        # Check if the line starts with "Human" or "Assistant" and split accordingly<br/>        if line.startswith("Human:"):<br/>            role = "user"<br/>            content = line.replace("Human: ", "").strip()<br/>        elif line.startswith("Assistant:"):<br/>            role = "assistant"<br/>            content = line.replace("Assistant: ", "").strip()<br/>        else:<br/>            # If the line doesn't start with "Human" or "Assistant", it's part of the previous message's content<br/>            # Append it to the last message's content<br/>            dialogue_list[-1]["content"] += "\n\n" + line.strip()<br/>            continue<br/><br/>        # Append the extracted dialogue piece to the list<br/>        dialogue_list.append({"role": role, "content": content})<br/><br/>    return dialogue_list<br/><br/>def process(row):<br/>        row["chosen"] = extract_dialogue(row["chosen"])<br/>        row["rejected"] = extract_dialogue(row["rejected"])<br/>        row["prompt"] = row["chosen"][0]["content"]<br/>        return row<br/><br/>ds_processed = ds.map(<br/>        process,<br/>        load_from_cache_file=False,<br/>    )</span></pre><p id="3875" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on it’s pre-training process, every model has a specific set of syntax and special tokens prompts should be optimized towards — this is the essence of prompt engineering and needs to be considered when fine-tuning. For the Meta Llama models this can be found in the llama-recipes<a class="af oz" href="https://github.com/meta-llama/llama-recipes" rel="noopener ugc nofollow" target="_blank"> GitHub repository</a>. To follow these prompting guidelines for an ideal result we are encoding our dataset accordingly.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="4a34" class="qo nz fq ql b bg qp qq l qr qs"># Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes<br/>system_prompt = "Please answer the user's question to the best of your knowledge. If you don't know the answer respond that you don't know."<br/><br/>def encode_dialogue_turn(message):<br/>    return f'&lt;|start_header_id|&gt;{message.get("role")}&lt;|end_header_id|&gt;{message.get("content")}&lt;|eot_id|&gt;'<br/><br/>def encode_dialogue(dialogue):<br/>    if system_prompt:<br/>        return f'&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;{system_prompt}&lt;|eot_id|&gt;{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, "")}'<br/>    else:<br/>        return f'&lt;|begin_of_text|&gt;{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, "")}'<br/><br/><br/>def encode_row(item):<br/>    return {"chosen": encode_dialogue(item["chosen"]), "rejected": encode_dialogue(item["rejected"]), "prompt": item["prompt"]}<br/>                                      <br/>def encode_dataset(dataset):<br/>    return list(map(encode_row, dataset))<br/><br/>encoded_dataset = ds_processed.map(encode_row)</span></pre><p id="f9fb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we are tokenizing the “chosen” and “rejected” columns. Subsequently we remove the plain text columns as we don’t need them any more. The dataset is now in the format we were aiming for.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="74a7" class="qo nz fq ql b bg qp qq l qr qs"># Tokenize and stack into target format<br/>def preprocess_function(examples):<br/>    new_examples = {<br/>        "input_ids_chosen": [],<br/>        "attention_mask_chosen": [],<br/>        "input_ids_rejected": [],<br/>        "attention_mask_rejected": [],<br/>    }<br/>    for chosen, rejected in zip(examples["chosen"], examples["rejected"]):<br/>        tokenized_chosen = tokenizer(chosen)<br/>        tokenized_rejected = tokenizer(rejected)<br/><br/>        new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])<br/>        new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])<br/>        new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])<br/>        new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])<br/><br/>    return new_examples<br/><br/>tokenized_dataset_hhrlhf = encoded_dataset.map(<br/>        preprocess_function,<br/>        batched=True,<br/>    ).remove_columns(["chosen", "rejected", "prompt"])</span></pre><p id="2dd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, we are uploading the dataset to Amazon S3. Please adjust the bucket path to a path pointing to a bucket in your account.</p><h2 id="eaba" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Data preprocessing PPO dataset</h2><p id="85ce" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">As previously discussed, we will be using the Stanford Question&amp;Answering Dataset (SQuAD) for the actual PA step with PPO. Therefore we need to convert the raw dataset into a pre-define structure, where “input_ids“ is the vectorized format of the “query“” a padded version of a question.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="30c9" class="qo nz fq ql b bg qp qq l qr qs">DatasetDict({<br/>    train: Dataset({<br/>        features: ['input_ids', 'query'],<br/>        num_rows: ...<br/>    })<br/>    test: Dataset({<br/>        features: ['input_ids', 'query'],<br/>        num_rows: ...<br/>    })<br/>})</span></pre><p id="7738" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This time we are not pulling the datasets from the HuggingFace hub — instead we are cloning them from a GitHub repository.</p><p id="bbb4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we parse the conversations into an array separated by conversation turn and role. Then we are encoding our dataset according to the Meta Llama prompting guidelines for an ideal result.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="4e76" class="qo nz fq ql b bg qp qq l qr qs">def extract_questions(dataset):<br/>    ret_questions = []<br/>    for topic in dataset:<br/>        paragraphs = topic['paragraphs']<br/>        for paragraph in paragraphs:<br/>            qas = paragraph['qas']<br/>            for qa in qas:<br/>                ret_questions.append([{<br/>            "role": "system", "content": f'Instruction: Please answer the user\'s question to the best of your knowledge. If you don\'t know the answer respond that you don\'t know.',<br/>        }, {<br/>            "role": "user", "content": qa['question'],<br/>        }])<br/>    return ret_questions<br/><br/># Adjusting to llama prompt template format: https://github.com/meta-llama/llama-recipes<br/>def encode_dialogue_turn(message):<br/>    message = message<br/>    return f'&lt;|start_header_id|&gt;{message.get("role")}&lt;|end_header_id|&gt;{message.get("content")}&lt;|eot_id|&gt;'<br/><br/>def encode_dialogue(dialogue):<br/>    return {'input': f'&lt;|begin_of_text|&gt;{functools.reduce(lambda a, b: a + encode_dialogue_turn(b), dialogue, "")}'}<br/><br/>                                      <br/>def encode_dataset(dataset):<br/>    #print(dataset)<br/>    return list(map(encode_dialogue, dataset))<br/><br/>encoded_train = encode_dataset(extract_questions(d_train['data']))<br/>encoded_test = encode_dataset(extract_questions(d_test['data']))</span></pre><p id="f13c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are padding our training examples to a maximum of 2048 tokens to reduce our training memory footprint. This can be adjusted to up to a model’s maximum context window. The threshold should be a good compromise between adhering to prompt length required by a specific use case or domain and keeping the training memory footprint small. Note, that larger input token sizes might require scaling out your compute infrastructure.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="9375" class="qo nz fq ql b bg qp qq l qr qs"># Restrict training context size (due to memory limitations, can be adjusted)<br/>input_min_text_length = 1<br/>input_max_text_length = 2048<br/><br/>def create_and_prepare_dataset(tokenizer, dataset):<br/>    <br/>    input_size = LengthSampler(input_min_text_length, input_max_text_length)<br/><br/>    def tokenize(example):<br/>        text_size = input_size()<br/>        example["input_ids"] = tokenizer.encode(example["input"])[:text_size]<br/>        example["query"] = tokenizer.decode(example["input_ids"])<br/>        return example<br/><br/>    dataset = dataset.map(tokenize, batched=False)<br/>        <br/>    dataset.set_format("torch")<br/>    return dataset<br/><br/><br/>tokenized_dataset_squad = create_and_prepare_dataset(tokenizer, dataset_dict).remove_columns(["input"])</span></pre><p id="f285" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, we are uploading the dataset to s3. Please adjust the bucket path to a path pointing to a bucket in your account.</p><h2 id="69bd" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Reward model training</h2><p id="916b" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">For the training of the reward model we are defining two helper functions: One function counting the trainable parameters of a model to showcase how LoRA impacts the trainable parameters and another function to identify all linear modules in a model since they will be targeted by LoRA.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="d864" class="qo nz fq ql b bg qp qq l qr qs">def print_trainable_parameters(model):<br/>    """<br/>    Prints the number of trainable parameters in the model.<br/>    """<br/>    trainable_params = 0<br/>    all_param = 0<br/>    for _, param in model.named_parameters():<br/>        all_param += param.numel()<br/>        if param.requires_grad:<br/>            trainable_params += param.numel()<br/>    print(<br/>        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"<br/>    )<br/>    <br/>def find_all_linear_names(hf_model):<br/>    lora_module_names = set()<br/>    for name, module in hf_model.named_modules():<br/>        if isinstance(module, bnb.nn.Linear4bit):<br/>            names = name.split(".")<br/>            lora_module_names.add(names[0] if len(names) == 1 else names[-1])<br/><br/>    if "lm_head" in lora_module_names:  # needed for 16-bit<br/>        lora_module_names.remove("lm_head")<br/>    return list(lora_module_names)</span></pre><p id="48c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The training fuction “train_fn“ is decorated with the remote decorator. This allows us to execute it as SageMaker training job. In the decorator we define a couple of parameters alongside the ones specified in the config.yaml. These parameters can be overwritten by the actual function call when triggering the training job.</p><p id="d8ff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the training function we first set a seed for determinism. Then we initialize an Accelerator object for handling distributed training. This object will orchestrate our distributed training in a data parallel manner across 4 ranks (note <em class="pz">nproc_per_node=4</em> in decorator parameters) on a ml.g5.12xlarge instance (note <em class="pz">InstanceType: ml.g5.12xlarge</em> in <em class="pz">config.yaml</em>).</p><p id="8eab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We then log into the HuggingFace hub and load and configure the tokenizer.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="afe9" class="qo nz fq ql b bg qp qq l qr qs"># Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. <br/>@remote(keep_alive_period_in_seconds=0, volume_size=100, job_name_prefix=f"train-{model_id.split('/')[-1].replace('.', '-')}-reward", use_torchrun=True, nproc_per_node=4)<br/>def train_fn(<br/>        model_name,<br/>        train_ds,<br/>        test_ds=None,<br/>        lora_r=8,<br/>        lora_alpha=32,<br/>        lora_dropout=0.1,<br/>        per_device_train_batch_size=8,<br/>        per_device_eval_batch_size=8,<br/>        gradient_accumulation_steps=1,<br/>        learning_rate=2e-4,<br/>        num_train_epochs=1,<br/>        fsdp="",<br/>        fsdp_config=None,<br/>        chunk_size=10000,<br/>        gradient_checkpointing=False,<br/>        merge_weights=False,<br/>        seed=42,<br/>        token=None,<br/>        model_hub_repo_id=None,<br/>        range_train=None,<br/>        range_eval=None<br/>):<br/><br/>    set_seed(seed)<br/><br/>    # Initialize Accelerator object handling distributed training<br/>    accelerator = Accelerator()<br/><br/>    # Login to HuggingFace<br/>    if token is not None:<br/>        login(token=token)<br/><br/>    # Load tokenizer. Padding side is "left" because focus needs to be on completion<br/>    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = "left")<br/><br/>    # Set tokenizer's pad Token<br/>    tokenizer.pad_token = tokenizer.eos_token <br/>    tokenizer.pad_token_id = tokenizer.eos_token_id </span></pre><p id="45a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the next step we are loading the training data from S3 and load them into a HuggingFace DatasetDict object. Since this is a demonstration we want to be able training with only a subset of the data to save time and resources. For this we can configure the range of dataset items to be used.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="f591" class="qo nz fq ql b bg qp qq l qr qs">    # Load data from S3<br/>    s3 = s3fs.S3FileSystem()<br/>    dataset = load_from_disk(train_ds)  <br/>    <br/>    <br/>    # Allow for partial dataset training<br/>    if range_train:<br/>        train_dataset = dataset["train"].select(range(range_train))<br/>    else: <br/>        train_dataset = dataset["train"]<br/>  <br/>    if range_eval:<br/>        eval_dataset = dataset["test"].select(range(range_eval))<br/>    else:<br/>        eval_dataset = dataset["test"]</span></pre><p id="4820" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are using the HuggingFace bitsandbytes library for quantization. In this configuration, bitsandbytes will replace all linear layers of the model with NF4 layers and the computation as well as storage data type to bfloat16. Then, the model is being loaded from HuggingFace hub in this quantization configuration using the flash attention 2 attention implementation for the attention heads for further improved memory usage and computational efficiency. We also print out all trainable parameters of the model in this state. Then, the model is prepared for quantized training.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="6b98" class="qo nz fq ql b bg qp qq l qr qs">    # Specify quantization config<br/>    bnb_config = BitsAndBytesConfig(<br/>        load_in_4bit=True,<br/>        bnb_4bit_use_double_quant=True,<br/>        bnb_4bit_quant_type="nf4",<br/>        bnb_4bit_compute_dtype=torch.bfloat16,<br/>        quant_storage_dtype=torch.bfloat16<br/>    )<br/>    <br/>    # Load model with classification head for reward<br/>    model = AutoModelForSequenceClassification.from_pretrained(<br/>        model_name,<br/>        #num_labels=1,<br/>        trust_remote_code=True,<br/>        quantization_config=bnb_config,<br/>        attn_implementation="flash_attention_2",<br/>        use_cache=False if gradient_checkpointing else True,<br/>        cache_dir="/tmp/.cache"<br/>    )<br/>    <br/>    # Pre-LoRA trainable paremeters<br/>    print_trainable_parameters(model)     <br/>    <br/>    # Set model pad token id<br/>    model.config.pad_token_id = tokenizer.pad_token_id<br/>    <br/>    # Prepare model for quantized training<br/>    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=gradient_checkpointing)</span></pre><p id="3bc6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we discover all linear layers of the model to pass them into a LoraConfig which specifies some LoRA hyperparameters. Please note, that unlike for traditional LLM training the task_type is not “CAUSAL_LM” but ”SEQ_CLS” since we are training a reward model and not a text completion model. The configuration is applied to the model and the training parameters are printed out again. Please note the difference in trainable and total parameters.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="c362" class="qo nz fq ql b bg qp qq l qr qs">    # Get lora target modules<br/>    modules = find_all_linear_names(model)<br/>    print(f"Found {len(modules)} modules to quantize: {modules}")<br/>    <br/>    # Specify LoRA config<br/>    config = LoraConfig(<br/>        r=lora_r,<br/>        lora_alpha=lora_alpha,<br/>        target_modules=modules,<br/>        lora_dropout=lora_dropout,<br/>        bias="none",<br/>        task_type="SEQ_CLS"<br/>    )<br/>    <br/>    # Make sure to not train for CLM<br/>    if config.task_type != "SEQ_CLS":<br/>        warnings.warn(<br/>            "You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs"<br/>            " Make sure to pass --lora_task_type SEQ_CLS when using this script."<br/>        )<br/>    <br/>    # Create PeftModel<br/>    model = get_peft_model(model, config)<br/>    <br/>    # Post-LoRA trainable paremeters<br/>    print_trainable_parameters(model)     </span></pre><p id="3382" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We define the RewardConfig holding important training hyperparameters like training batch size, training epochs, learning rate and more. We also define a <em class="pz">max_length=512. </em>This<em class="pz"> </em>will be the maximum length of prompt+response pairs being used for reward adapter training and will be enforced through left-side padding to preserve the last conversation turn which marks the key difference between chosen and rejected sample. Again, this can be adjusted to up to a model’s maximum context window while finding a good compromise between adhering to prompt length required by a specific use case or domain and keeping the training memory footprint small.</p><p id="0c39" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Further, we initialize the RewardTraining object orchestrating the training with this configuration and further training inputs like model, tokenizer and datasets. Then we kick off the training. Once the training has finished we push the reward model adapter weights to the reward model model repository we have created in the beginning.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="a950" class="qo nz fq ql b bg qp qq l qr qs">    # Specify training config<br/>    reward_config = RewardConfig(<br/>                        per_device_train_batch_size=per_device_train_batch_size,<br/>                        per_device_eval_batch_size=per_device_eval_batch_size,<br/>                        gradient_accumulation_steps=gradient_accumulation_steps,<br/>                        gradient_checkpointing=gradient_checkpointing,<br/>                        logging_strategy="steps",<br/>                        logging_steps=100,<br/>                        log_on_each_node=False,<br/>                        num_train_epochs=num_train_epochs,<br/>                        learning_rate=learning_rate,<br/>                        bf16=True,<br/>                        ddp_find_unused_parameters=False,<br/>                        fsdp=fsdp,<br/>                        fsdp_config=fsdp_config,<br/>                        save_strategy="no",<br/>                        output_dir="outputs",<br/>                        max_length=512, <br/>                        remove_unused_columns=False,<br/>                        gradient_checkpointing_kwargs = {"use_reentrant": False}<br/>                        )<br/>    <br/>    # Initialize RewardTrainer object handling training<br/>    trainer = RewardTrainer(<br/>        model=model,<br/>        tokenizer=tokenizer,<br/>        args=reward_config,<br/>        train_dataset=train_dataset,<br/>        eval_dataset=eval_dataset,<br/>    )<br/><br/>    trainer.train()<br/><br/>    <br/>    trainer.model.save_pretrained("/opt/ml/model", safe_serialization=True)<br/>    <br/>    if model_hub_repo_id is not None:<br/>        trainer.model.push_to_hub(repo_id=model_hub_repo_id)<br/><br/>    with accelerator.main_process_first():<br/>        tokenizer.save_pretrained("/opt/ml/model")</span></pre><p id="3544" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can now kickoff the training itself. Therefor we call the training function which kicks off an ephemeral training job in Amazon SageMaker. For this we need to pass some parameters to the training function, e.g. the model id, training dataset path and some hyperparameters. Note that the hyperparameters used for this demonstration can be adjusted as per requirement. For this demonstration we work with 100 training and 10 evaluation examples to keep the resource and time footprint low. For a real-world use case a full dataset training should be considered. Once the training has started the training logs are streamed to the notebook.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="4f7c" class="qo nz fq ql b bg qp qq l qr qs"># Start training job<br/>train_fn(<br/>    model_id,<br/>    train_ds=dataset_path_hhrlhf,<br/>    per_device_train_batch_size=8,<br/>    per_device_eval_batch_size=8,<br/>    gradient_accumulation_steps=2,<br/>    gradient_checkpointing=True,<br/>    num_train_epochs=1,<br/>    token=hf_token,<br/>    model_hub_repo_id=model_hub_repo_id,<br/>    range_train=100,<br/>    range_eval=10<br/>)</span></pre><h2 id="edc6" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Multi-adapter PPO</h2><p id="2484" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">For the actual PA step with PPO we are reusing function counting the trainable parameters of a model to showcase how LoRA impacts the trainable parameters. Sililarily to the reward model training step, the training fuction “train_fn“ is decorated with the remote decorator allowing us to execute it as SageMaker training job.</p><p id="aafa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the training function we first set a seed for determinism. Then we initialize an Accelerator object for handling distributed training. As with the reward adapter training, this object will handle our distributed training in a data parallel manner across 4 ranks on a ml.g5.12xlarge instance.</p><p id="349e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We then log into the HuggingFace hub and load and configure the tokenizer. In the next step we are loading the training data from S3 and load them into a HuggingFace DatasetDict object. Since this is a demonstration we want to be able training with only a subset of the data to save time and resources. For this we can configure the range of dataset items to be used.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="2148" class="qo nz fq ql b bg qp qq l qr qs"># Start training with remote decorator (https://docs.aws.amazon.com/sagemaker/latest/dg/train-remote-decorator.html). Additional job config is being pulled in from config.yaml. <br/>@remote(keep_alive_period_in_seconds=0, volume_size=100, job_name_prefix=f"train-{model_id.split('/')[-1].replace('.', '-')}-multi-adapter-ppo", use_torchrun=True, nproc_per_node=4)<br/>def train_fn(<br/>        model_name,<br/>        train_ds,<br/>        rm_adapter,<br/>        log_with=None,<br/>        use_safetensors=None,<br/>        use_score_scaling=False,<br/>        use_score_norm=False,<br/>        score_clip=None,<br/>        seed=42,<br/>        token=None,<br/>        model_hub_repo_id=None,<br/>        per_device_train_batch_size=8,<br/>        per_device_eval_batch_size=8,<br/>        gradient_accumulation_steps=2,<br/>        gradient_checkpointing=True,<br/>        num_train_epochs=1,<br/>        merge_weights=True,<br/>        range_train=None,<br/>        ):<br/><br/>    set_seed(seed)<br/><br/>    # Initialize Accelerator object handling distributed training<br/>    accelerator = Accelerator()<br/>    <br/>    # Login to HuggingFace <br/>    if token is not None:<br/>        login(token=token)<br/>        <br/>    # Load tokenizer. Padding side is "left" because focus needs to be on completion<br/>    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = "left")<br/><br/>    # Set tokenizer's pad Token<br/>    tokenizer.pad_token = tokenizer.eos_token <br/>    tokenizer.pad_token_id = tokenizer.eos_token_id  <br/>    <br/>    <br/>    # Load data from S3<br/>    s3 = s3fs.S3FileSystem()<br/>    dataset = load_from_disk(train_ds)  <br/>    <br/>    <br/>    # Allow for partial dataset training<br/>    if range_train:<br/>        train_dataset = dataset["train"].select(range(range_train))<br/>    else: <br/>        train_dataset = dataset["train"]</span></pre><p id="ec93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we define a LoraConfig which specifies the LoRA hyperparameters. Please note, that this time the task_type is “CAUSAL_LM” since we are aiming to fine-tune a text completion model.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="7ced" class="qo nz fq ql b bg qp qq l qr qs">    # Specify LoRA config<br/>    lora_config = LoraConfig(<br/>        r=16,<br/>        lora_alpha=32,<br/>        lora_dropout=0.05,<br/>        bias="none",<br/>        task_type="CAUSAL_LM",<br/>    )</span></pre><p id="4f35" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are using the HuggingFace bitsandbytes library for quantization. In this configuration, bitsandbytes will replace all linear layers of the model with NF4 layers and the computation to bfloat16.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="8652" class="qo nz fq ql b bg qp qq l qr qs">    # Specify quantization config<br/>    bnb_config = BitsAndBytesConfig(<br/>        load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16<br/>    )</span></pre><p id="49d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, the model is being loaded from HuggingFace hub in this quantization using both the specified LoraConfig and BitsAndBytesConfig. Note that this model is not wrapped into a simple AutoModelForCausalLM class, instead we are using a AutoModelForCausalLMWithValueHead class taking our reward model adapter as input. This is a model class purposely built for multi-adapter PPO, orchestrating adapter loading and plugins during the actual training loop we will discuss subsequently.For the sake of completeness we also print out all trainable parameters of the model in this state.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="b47b" class="qo nz fq ql b bg qp qq l qr qs">    # Load model<br/>    model = AutoModelForCausalLMWithValueHead.from_pretrained(<br/>        model_name,<br/>        #device_map='auto',<br/>        peft_config=lora_config,<br/>        quantization_config=bnb_config,<br/>        reward_adapter=rm_adapter,<br/>        use_safetensors=use_safetensors,<br/>        #attn_implementation="flash_attention_2",<br/>    )<br/>    <br/>    # Set model pad token id<br/>    model.config.pad_token_id = tokenizer.pad_token_id<br/><br/>    if gradient_checkpointing:<br/>        model.gradient_checkpointing_enable()<br/>        <br/>    # Trainable paremeters<br/>    print_trainable_parameters(model)    </span></pre><p id="cfb7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We define the PPOConfig holding important training hyperparameters like training batch size, learning rate and more. Further, we initialize the PPOTrainer object orchestrating the training with this configuration and further training inputs like model, tokenizer and datasets. Note, that the ref_model for the computation of the KL divergence is not specified. As previously discussed, in this configuration the PPOTrainer uses a reference model with the same architecture as the model to be optimized with shared layers. Further, the inference parameters for inference to retrieve the text completion based on the query from the training dataset are defined.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="cceb" class="qo nz fq ql b bg qp qq l qr qs">    # Specify PPO training config<br/>    config = PPOConfig(<br/>        model_name,<br/>        log_with=None,<br/>        learning_rate=1e-5,<br/>        batch_size=per_device_train_batch_size,<br/>        mini_batch_size=1,<br/>        gradient_accumulation_steps=gradient_accumulation_steps,<br/>        optimize_cuda_cache=True,<br/>        seed=42,<br/>        use_score_scaling=False,<br/>        use_score_norm=False,<br/>        score_clip=None,<br/>    )<br/><br/>    # Initialize PPOTrainer object handling training<br/>    ppo_trainer = PPOTrainer(<br/>        config,<br/>        model,<br/>        ref_model=None,<br/>        tokenizer=tokenizer,<br/>        dataset=train_dataset,<br/>        data_collator=collator,<br/>    )<br/><br/>    # Specifying inference params<br/>    generation_kwargs = {<br/>        "top_k": 0.0,<br/>        "top_p": 0.9,<br/>        "do_sample": True,<br/>        "pad_token_id": tokenizer.pad_token_id,<br/>        "max_new_tokens": 32,<br/>    }</span></pre><p id="a7c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we execute the actual multi-adapter PPO training loop as follows on a batch of training data: First, the LoRA adapters we are RLHF fine-tuning are applied for inference to retrieve a text completion based on the query from the training dataset. The response is decoded into plain text and combined with the query. Then, the reward adapters are applied to compute the reward of the the query — completion pair in tokenized form. Subsequently, the reward value is used alongside the question and response tensors for the optimization step. Note, that in the background the Kullback–Leibler-divergence (KL-divergence) between the inference logits of the fine-tuned model and base model (prediction shift penalty) is computed and included as additional reward signal integrated term used during the optimization step. Since this is based on the same input prompt, the KL-divergence acts as a measure of how these two probability distributions and hence the models themselves differ from each other over training time. This divergence is subtracted from the reward term, penalizing divergence from the base model to assure algorithmic stability and linguistic consistency. Finally, the adapters we are RLHF fine-tuning are applied again for the back propagation.</p><p id="2213" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we kick off the training. Once the training has finished we push the preference-alignged model adapter weights to the rlhf model model repository we have created in the beginning.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="2af2" class="qo nz fq ql b bg qp qq l qr qs">step = 0<br/><br/>    for _epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):<br/>        <br/>        question_tensors = batch["input_ids"]<br/>        <br/>        # Inference through model being fine-tuned<br/>        response_tensors = ppo_trainer.generate(<br/>            question_tensors,<br/>            return_prompt=False,<br/>            **generation_kwargs,<br/>        )<br/>        <br/>        # Decode response<br/>        batch["response"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)<br/>        <br/>        # Concat query and response<br/>        texts = [q + r for q, r in zip(batch["query"], batch["response"])]<br/>        <br/>        # Tokenize query - response pair<br/>        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(ppo_trainer.accelerator.device)<br/>        <br/>        # Compute reward score<br/>        raw_rewards = ppo_trainer.accelerator.unwrap_model(ppo_trainer.model).compute_reward_score(**inputs)<br/>        rewards = [raw_rewards[i, -1, 1] for i in range(len(raw_rewards))]  # take last token<br/><br/>        # Run PPO step<br/>        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)<br/>        ppo_trainer.log_stats(stats, batch, rewards)<br/>        <br/>        step = step + 1      <br/>    <br/>    if accelerator.is_main_process:<br/><br/>        ppo_trainer.save_pretrained("/opt/ml/model", safe_serialization=True)<br/><br/>        if model_hub_repo_id is not None:<br/>            ppo_trainer.push_to_hub(repo_id=model_hub_repo_id)<br/>            tokenizer.push_to_hub(repo_id=model_hub_repo_id)<br/><br/>    with accelerator.main_process_first():<br/>        tokenizer.save_pretrained("/opt/ml/model")</span></pre><p id="7a67" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can now kickoff the training itself. Therefore we call the training function which kicks off an ephemeral training job in Amazon SageMaker. For this we need to pass some parameters to the training function, e.g. the model id, training dataset path, reward model path and some hyperparameters. Note that the hyperparameters used for this demonstration can be adjusted as per requirement. For this demonstration we work with 100 training examples to keep the resource and time footprint low. For a real-world use case a full dataset training should be considered. Once the training has started the training logs are streamed to the notebook.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="dbb7" class="qo nz fq ql b bg qp qq l qr qs">train_fn(<br/>    model_id,<br/>    train_ds=dataset_path_squad,<br/>    rm_adapter=rm_adapter,<br/>    per_device_train_batch_size=4,<br/>    per_device_eval_batch_size=4,<br/>    gradient_accumulation_steps=4,<br/>    gradient_checkpointing=True,<br/>    num_train_epochs=1,<br/>    token=hf_token,<br/>    model_hub_repo_id=model_hub_repo_id,<br/>    range_train=100<br/>)</span></pre><h2 id="537e" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Deployment</h2><p id="4c91" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Finally, we want to test the tuned model. Therefore we will deploy it to a SageMaker endpoint. We start with importing required dependencies as well as setting up the SageMaker session and IAM.</p><p id="b0c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the deployment we are using the <a class="af oz" href="https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/" rel="noopener ugc nofollow" target="_blank">SageMaker — Huggingface integration</a> with the <a class="af oz" href="https://huggingface.co/docs/text-generation-inference/en/index" rel="noopener ugc nofollow" target="_blank">TGI containers</a>. We define the instance type, image as well as model-related parameters like the base model, LoRA adapter, quantization and others.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="f214" class="qo nz fq ql b bg qp qq l qr qs"># sagemaker config<br/>instance_type = "ml.g5.4xlarge"<br/>number_of_gpu = 1<br/>health_check_timeout = 300<br/><br/># TGI config<br/>config = {<br/>'HF_MODEL_ID': "meta-llama/Meta-Llama-3.1-8B-Instruct",<br/>'LORA_ADAPTERS': "**HF_REPO_ID**",<br/>'SM_NUM_GPUS': json.dumps(1), # Number of GPU used per replica<br/>'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text<br/>'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text),<br/>'QUANTIZE': "bitsandbytes", # comment in to quantize<br/>'HUGGING_FACE_HUB_TOKEN': hf_token<br/>}<br/><br/>image_uri = get_huggingface_llm_image_uri(<br/>    "huggingface",<br/>    version="2.0"<br/>)<br/><br/># create HuggingFaceModel<br/>llm_model = HuggingFaceModel(<br/>    role=role,<br/>    image_uri=image_uri,<br/>    env=config<br/>)</span></pre><p id="f08f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we deploy the model. Once the model has been deployed we can test the model inference with a prompt of our choice. Note that we are using the encode_dialogue function defined during data preprocessing to optimize the prompt for the Llama model.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="8bad" class="qo nz fq ql b bg qp qq l qr qs"># Deploy model to an endpoint<br/># https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy<br/>llm = llm_model.deploy(<br/>    endpoint_name=f'llama-31-8b-instruct-rlhf-{datetime.now().strftime("%Y%m%d%H%M%S")}', # alternatively "llama-2-13b-hf-nyc-finetuned"<br/>    initial_instance_count=1,<br/>    instance_type=instance_type,<br/>    container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model<br/>)<br/><br/>parameters = {<br/>        "top_p": 0.8,<br/>        "temperature": 0.1,<br/>        "return_full_text": True,<br/>        "stop": [],<br/>    }<br/><br/>encoded_message = encode_dialogue([{'content': 'Who won the FIFA World cup 2014 in Brazil?', 'role': 'user'}])<br/>                   <br/>response = llm.predict({"inputs": encoded_message['input'], **parameters})</span></pre><h2 id="5e7c" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Cleanup</h2><p id="5b56" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Finally, we cleanup the deployed endpoint and model entity to be responsible in resource usage.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="c03d" class="qo nz fq ql b bg qp qq l qr qs"># Delete model and endpoint<br/>llm.delete_model()<br/>llm.delete_endpoint()</span></pre><h2 id="67fb" class="pi nz fq bf oa pj pk pl od pm pn po og nl pp pq pr np ps pt pu nt pv pw px py bk">Cost</h2><p id="1dfe" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Both reward model adapter training and multi-adapter PPO training were executed on an <em class="pz">ml.g5.12xlarge</em> instance using a dataset of 100 randomly sampled rows from the respective training datasets. The average training time was approximately 400 seconds for each step. As of November 2024, this instance type is priced at <strong class="ne fr">$7.09/hour</strong> in the us-east-1 region.</p><p id="47d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Consequently, the end-to-end training cost for this RLHF implementation with multi-adapter PPO amounts to less than <em class="pz">($7.09 * 400s)/(3600s * 100)</em> <strong class="ne fr">~ $0.0079 per individual training sample</strong> for each of the two training steps. This translates to <strong class="ne fr">less than $0.015 per 1000 training tokens for the reward model training</strong> and <strong class="ne fr">less than $0.0039 per 1000 training tokens</strong> for the multi-adapter PPO step.</p><p id="441e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For inference, the model is hosted on an <em class="pz">ml.g5.4xlarge</em> instance. As of November 2024, this instance type is priced at <strong class="ne fr">$2.03/hour</strong> in the us-east-1 region.</p><h1 id="f729" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><p id="cf8b" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In this blog post, we explored RLHF with multi-adapter PPO, a frugal approach to preference alignment for large language models. We covered the following key points:</p><ol class=""><li id="912f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qt pb pc bk">The importance of preference alignment in boosting LLM performance and its role in the democratization of AI.</li><li id="9adc" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qt pb pc bk">The principles of RLHF and its two-step process involving reward model training and PPO-based fine-tuning.</li><li id="a8a2" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qt pb pc bk">Challenges associated with implementing RLHF, including computational resources and orchestration complexity.</li><li id="e75c" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qt pb pc bk">The multi-adapter PPO approach as a solution to reduce infrastructure and orchestration footprint.</li><li id="90d8" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qt pb pc bk">A detailed, end-to-end implementation using HuggingFace frameworks and Amazon SageMaker, covering data preprocessing, reward model training, multi-adapter PPO training, and model deployment.</li></ol><p id="5f0e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This frugal approach to RLHF makes preference alignment more accessible to a broader range of practitioners, potentially accelerating the development and deployment of aligned AI systems.</p><p id="f695" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By reducing computational requirements and simplifying the implementation process, multi-adapter PPO opens up new possibilities for fine-tuning language models to specific domains or user preferences.</p><p id="8d32" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the field of AI continues to evolve, techniques like this will play a crucial role in creating more efficient, effective, and aligned language models. I’d like to encourage readers to experiment with this approach, adapt it to their specific use cases, and share their success stories in building responsible and user-centric LLMs.</p><p id="738f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr"><em class="pz">If you’re interested in learning more about LLM pre-training and alignment, I recommend checking out the </em></strong><a class="af oz" href="https://explore.skillbuilder.aws/learn/course/external/view/elearning/17556/building-language-models-on-aws" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr"><em class="pz">AWS SkillBuilder course</em></strong></a><strong class="ne fr"><em class="pz"> I recently published with my esteemed colleagues </em></strong><a class="af oz" href="https://anastasia-tzeveleka.medium.com/" rel="noopener"><strong class="ne fr"><em class="pz">Anastasia</em></strong></a><strong class="ne fr"><em class="pz"> and </em></strong><a class="af oz" href="https://medium.com/@gilinachum" rel="noopener"><strong class="ne fr"><em class="pz">Gili</em></strong></a><strong class="ne fr"><em class="pz">.</em></strong></p></div></div></div></div>    
</body>
</html>