# AI 解释性的重要性

> 原文：[`towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04`](https://towardsdatascience.com/the-meaning-of-explainability-for-ai-d8ae809c97fa?source=collection_archive---------3-----------------------#2024-06-04)

## 我们现在还关心我们的机器学习是如何做出决策的吗？

[](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)![Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------) [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--d8ae809c97fa--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d8ae809c97fa--------------------------------) ·8 分钟阅读·2024 年 6 月 4 日

--

今天，我想谈谈一点哲学，讨论一下机器学习中的解释性和风险如何交汇。

![](img/dea845c74d9370ff46f3baf0d29920fd.png)

图片由 [Kenny Eliason](https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral) 提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# 我们所说的解释性是什么意思？

简而言之，机器学习中的[解释性](https://www.researchgate.net/profile/Kai-Heinrich-3/publication/344357897_White_Grey_Black_Effects_of_XAI_Augmentation_on_the_Confidence_in_AI-based_Decision_Support_Systems/links/5f6ba89392851c14bc922907/White-Grey-Black-Effects-of-XAI-Augmentation-on-the-Confidence-in-AI-based-Decision-Support-Systems.pdf)是指你可以向一个人类用户（不一定是技术专家）解释模型是如何做出决策的。例如，决策树就是一个容易解释的（有时被称为“白盒”）模型，你可以指出“模型将数据分为房屋面积大于 1 的和面积小于或等于 1 的”等等。其他类型的更复杂的模型可以是“灰盒”或“黑盒”——这些模型对于人类用户来说，理解起来会越来越困难，甚至是不可能的。

# 老学校

我在机器学习教育中的一个基础性教训是，我们与模型（通常是增强型树模型）之间的关系，最多应该是“信任，但要验证”。当你训练一个模型时，不要仅仅接受初步预测的表面结果，而是要花费一些时间去彻底检验。测试模型在非常异常的离群值上的表现，即便这些情况在现实中不太可能发生。如果树足够浅，可以绘制出树的结构。使用特征重要性、Shapley 值和 [LIME](https://arxiv.org/abs/1602.04938) 等技术来验证模型是否使用了与你对主题和逻辑的理解相符的特征。给定树中的特征分割是否与你对该领域的了解一致？在建模物理现象时，你还可以将模型的行为与我们对事物如何运作的科学知识进行比较。不要仅仅相信你的模型在正确地处理问题，而是要进行验证。

> 不要仅仅相信你的模型在正确地处理问题，而是要进行验证。

# 引入神经网络

随着神经网络的重要性爆炸式增长，我们不得不考虑的最大权衡之一就是，这种可解释性变得极为困难，并且因为架构的工作方式而发生显著变化。

神经网络模型在每一层中都对输入数据应用函数，将数据以多种方式变化，然后最终将数据传递到最终层的目标值。这种作用的结果是，与基于树的模型的分支不同，输入和输出之间的中间层通常无法合理地被人类理解。你也许可以找到某个中间层中的特定节点，并查看其值如何影响输出，但将其与人类能够理解的实际、具体输入联系起来通常会失败，因为即使是一个简单的神经网络，其层次的抽象程度也非常高。

这一点可以通过“哈士奇与狼”问题来清楚地说明。一个卷积神经网络被训练来区分哈士奇和狼的照片，但经过调查发现，模型是根据背景的颜色做出判断的。哈士奇的训练照片比狼的训练照片更不容易出现在雪地中，因此每当模型接收到一张雪景背景的图像时，它就预测图像中会出现狼。模型使用了人类没有考虑到的信息，并基于错误的特征发展出了其内部逻辑。

这意味着传统的“这个模型是否‘思考’问题的方式与物理或直观现实相符？”的测试变得过时。我们不能像以前那样告诉模型是如何做出选择的，而是我们更多地依赖于试错法。为此，存在系统性的实验策略，本质上是将模型与许多反事实进行测试，以确定输入的哪些种类和程度的变化会引起输出的变化，但这必然是艰巨且计算密集型的。

> 我们不能像以前那样告诉模型是如何做出选择的，而是我们更多地依赖于试错法。

我并不是要辩称理解神经网络如何执行其任务的努力完全没有希望。许多学者对[可解释 AI，文献中称为 XAI](https://arxiv.org/pdf/2404.09554)非常感兴趣。今天可用的各种模型意味着我们可以并且应该追求许多方法。注意力机制是一项技术进步，帮助我们理解模型在处理输入时最关注的部分/受到驱动的部分，这非常有帮助。[Anthropic 最近发布了一份非常有趣的报告，深入探讨了 Claude 的可解释性，试图了解哪些单词、短语或图像根据提示使用稀疏自编码器能引发最大的激活。](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html) 我在上面描述的工具，[包括 Shapley](https://skirene.medium.com/demystifying-neural-nets-with-shapley-values-cca29c836089) [和 LIME](https://github.com/marcotcr/lime/blob/master/doc/notebooks/Tutorial%20-%20images%20-%20Pytorch.ipynb)，也可以应用于一些种类的神经网络，比如 CNNs，尽管结果可能很难解释。但随着我们增加复杂性，按定义，人类观众或用户将更难理解和解释模型是如何工作的。

# 考虑随机性

这里一个重要的额外因素是要认识到许多神经网络包含随机性，因此你不能总是依赖模型在看到相同输入时返回相同的输出。特别是，生成性 AI 模型有意地可能会从相同输入生成不同的输出，这样它们看起来更“人性化”或富有创造性——我们可以通过[tuning the “temperature”](https://medium.com/@harshit158/softmax-temperature-5492e4007f71#:~:text=Temperature%20is%20a%20hyperparameter%20of%20LSTMs%20(and%20neural%20networks%20generally,the%20logits%20before%20applying%20softmax.)来增加或减少这种变化的极端性。这意味着有时我们的模型会选择返回一个不是最具概率性期望的输出，而是一些“令人惊讶”的内容，这增强了结果的创造性。

在这种情况下，我们仍然可以通过试错的方法来尽量发展对模型行为及其原因的理解，但这变得复杂得多。方程式中的唯一变化不再是不同的输入，而是输入的变化加上由于随机性引起的未知变动。你的输入变化是否改变了响应，还是那是随机性的结果？我们常常无法真正知道。

> 你的输入变化是否改变了响应，还是那是随机性的结果？

# 现实世界的影响

那么，这把我们带到哪里呢？我们为什么要知道模型是如何进行推理的？作为机器学习开发者和模型使用者，这对我们来说有什么意义？

如果我们构建的机器学习模型将帮助我们做出决策并塑造人们的行为，那么结果的责任就应该由我们承担。有时，模型预测会经过人工调解之后再应用于我们的世界，但我们越来越多地看到模型被放开，生产中的推理结果被直接使用而没有进一步审查。公众现在比以往任何时候都更容易接触到这些极其复杂的机器学习模型。

因此，对我来说，理解模型如何以及为什么做出它所做的决定，就像测试一个制造的玩具是否没有铅漆，或者一台机器是否在正常使用下不会断裂并伤害到人的手一样，是一种尽职调查。这要比测试更难，但确保我不会将一个使生活更糟的产品推向市场，是我坚定的道德立场。如果你正在构建一个机器学习模型，你就有责任对该模型所做的事以及它对人们和世界的影响负责。因此，要真正有信心你的模型是安全的，你需要一定程度的理解，了解它是如何以及为什么返回它的输出的。

> 如果你正在构建一个机器学习模型，你就有责任对该模型所做的事以及它对人们和世界的影响负责。

顺便提一下，读者可能还记得我之前关于[欧盟人工智能法案](https://medium.com/towards-data-science/uncovering-the-eu-ai-act-22b10f946174)的文章，其中提到模型的预测需要经过人工监督，并且不得根据受保护的特征做出具有歧视效应的决策。所以，即使你不被道德论点所打动，对于我们中的许多人来说，法律的动机也是存在的。

即使我们使用神经网络，我们仍然可以使用工具更好地理解我们的模型是如何做出决策的——我们只需要花时间并努力去做到这一点。

# 但是，进展呢？

从哲学角度来看，我们可以（而且人们确实这么做）辩称，机器学习技术达到一定复杂度后，需要放弃完全理解它的愿望。这可能是对的！但我们不应忽视由此带来的权衡和我们所接受的风险。最好的情况是，你的生成性 AI 模型主要按照预期运行（也许如果你控制了温度，并且你的模型非常缺乏创意），不会做出太多意外的事情；而最坏的情况是，你释放了一个灾难，因为模型以你完全没有预料到的方式做出反应。这可能意味着你看起来很傻，或者可能意味着你的生意结束，或者可能意味着对人们造成真正的身体伤害。当你接受模型的可解释性是无法实现的时，这就是你自己肩负的风险。你不能说“哦，模型就是这样”——当你建造了这个东西并做出了释放它或使用其预测的有意识决定时。

各大大小小的科技公司都已接受生成性 AI 有时会产生不正确、危险、歧视性以及其他有害的结果，并认为为了获得其感知的好处，这些风险是值得的——我们知道这一点，因为那些经常表现出不良行为的生成性 AI 模型已经被公开发布。我个人很困扰的是，科技行业在没有任何明确考虑或讨论的情况下，选择将公众置于这种风险之中，但魔 genie 已经放出来了。

# 现在怎么办？

对我来说，追求 XAI 并试图让它跟上生成性 AI 的进步是一个崇高的目标，但我认为我们不会看到大多数人能够轻松理解这些模型是如何运作的，因为它们的架构非常复杂且具有挑战性。因此，我认为我们还需要实施风险缓解措施，确保那些负责日益复杂的模型的人，对这些影响我们日常生活的产品及其安全负责。由于结果往往是不可预测的，我们需要框架来保护我们的社区免受最坏情况的影响。

我们不应把所有的风险都视为无法承受，但我们需要清醒地认识到风险的存在，并且由于 AI 前沿的可解释性挑战，机器学习的风险比以往任何时候都更难以衡量和预见。唯一负责任的选择是将这种风险与这些模型所带来的实际利益进行平衡（而不是将某些未来版本的预期或承诺利益视为理所当然），并据此做出深思熟虑的决策。

阅读更多我的作品，请访问 [www.stephaniekirmer.com](http://www.stephaniekirmer.com)。

# 进一步阅读

+   [扩展单一语义性：从 Claude 3 Sonnet 中提取可解释特征](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)（2024 年 5 月 21 日，Anthropic 团队）

+   [可解释的生成型人工智能：调查、概念化与研究议程](https://arxiv.org/pdf/2404.09554)（2024 年 4 月 15 日；Johannes Schneider）——这篇文章内容非常易读，推荐大家阅读。

+   [卷积神经网络可解释性方法分析](https://www.sciencedirect.com/science/article/pii/S0952197622005966)（2023 年 1 月，Von der Haar 等人）

+   [可解释的卷积神经网络：分类法、回顾与未来方向](https://dl.acm.org/doi/full/10.1145/3563691)（2023 年 2 月 2 日；Ibrahim 等人）

+   [谷歌的人工智能告诉用户往比萨饼上加胶水、吃石头并制造氯气](https://www.livescience.com/technology/artificial-intelligence/googles-ai-tells-users-to-add-glue-to-their-pizza-eat-rocks-and-make-chlorine-gas)（2024 年 5 月 23 日）
