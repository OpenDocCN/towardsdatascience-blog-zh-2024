- en: 'User Action Sequence Modeling: From Attention to Transformers and Beyond'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/user-action-sequence-modeling-from-attention-to-transformers-and-beyond-5f280268b399?source=collection_archive---------5-----------------------#2024-07-15](https://towardsdatascience.com/user-action-sequence-modeling-from-attention-to-transformers-and-beyond-5f280268b399?source=collection_archive---------5-----------------------#2024-07-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The quest to LLM-ify recommender systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page---byline--5f280268b399--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page---byline--5f280268b399--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f280268b399--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f280268b399--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page---byline--5f280268b399--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f280268b399--------------------------------)
    ·11 min read·Jul 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ea5d8b3f09223e21df594f5dad9876c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated using ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: 'User action sequences are among the most powerful inputs in recommender systems:
    your next click, read, watch, play, or purchase is likely at least somewhat related
    to what you’ve clicked on, read, watched, played, or purchased minutes, hours,
    days, months, or even years ago.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Historically, the status quo for modeling such user engagement sequences has
    been pooling: for example, a classic 2016 YouTube [paper](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45530.pdf)
    describes a system that takes the latest 50 watched videos, collects their embeddings
    from an embedding table, and pools these into a single feature vector with sum
    pooling. To save memory, the embedding table for these sequence videos is shared
    with the embedding table for candidate videos themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99b5651ad79480206ae75e94ce5f0c74.png)'
  prefs: []
  type: TYPE_IMG
- en: YouTube’s recommender system sum-pools the sequence of watched videos for a
    user. [Covinton et al 2016](https://storage.googleapis.com/gweb-research2023-media/pubtools/pdf/45530.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'This simplistic approach corresponds roughly to a bag-of-words approach in
    the NLP domain: it works, but it’s far from ideal. Pooling does not take into
    account the sequential nature of inputs, nor the relevance of the item in the
    user history with respect to the candidate item we need to rank, nor any of the
    temporal information: an…'
  prefs: []
  type: TYPE_NORMAL
