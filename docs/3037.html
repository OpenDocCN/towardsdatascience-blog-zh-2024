<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Prototype to Production: Enhancing LLM Accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Prototype to Production: Enhancing LLM Accuracy</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-prototype-to-production-enhancing-llm-accuracy-791d79b0af9b?source=collection_archive---------0-----------------------#2024-12-19">https://towardsdatascience.com/from-prototype-to-production-enhancing-llm-accuracy-791d79b0af9b?source=collection_archive---------0-----------------------#2024-12-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="21dd" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implementing evaluation frameworks to optimize accuracy in real-world applications</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://miptgirl.medium.com/?source=post_page---byline--791d79b0af9b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariya Mansurova" class="l ep by dd de cx" src="../Images/b1dd377b0a1887db900cc5108bca8ea8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*7fFHr8XBAuR_SgJknIyODA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--791d79b0af9b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://miptgirl.medium.com/?source=post_page---byline--791d79b0af9b--------------------------------" rel="noopener follow">Mariya Mansurova</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--791d79b0af9b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/5398f4ac2eeb0542c8deeb07151f9e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*al-9CqM7KGRk_ZE7oCMGwg.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image created by DALL-E 3</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3de4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Building a prototype for an LLM application is surprisingly straightforward. You can often create a functional first version within just a few hours. This initial prototype will likely provide results that look legitimate and be a good tool to demonstrate your approach. However, this is usually not enough for production use.</p><p id="94a6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">LLMs are probabilistic by nature, as they generate tokens based on the distribution of likely continuations. This means that in many cases, we get the answer close to the “correct” one from the distribution. Sometimes, this is acceptable — for example, it doesn’t matter whether the app says “Hello, John!” or “Hi, John!”. In other cases, the difference is critical, such as between “The revenue in 2024 was 20M USD” and “The revenue in 2024 was 20M GBP”.</p><p id="655a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In many real-world business scenarios, precision is crucial, and “almost right” isn’t good enough. For example, when your LLM application needs to execute API calls, or you’re doing a summary of financial reports. From my experience, ensuring the accuracy and consistency of results is far more complex and time-consuming than building the initial prototype.</p><p id="398a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, I will discuss how to approach measuring and improving accuracy. We’ll build an SQL Agent where precision is vital for ensuring that queries are executable. Starting with a basic prototype, we’ll explore methods to measure accuracy and test various techniques to enhance it, such as self-reflection and retrieval-augmented generation (RAG).</p><h1 id="3d32" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Setup</h1><p id="553a" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">As usual, let’s begin with the setup. The core components of our SQL agent solution are the LLM model, which generates queries, and the SQL database, which executes them.</p><h2 id="3749" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">LLM model — Llama</h2><p id="8b39" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">For this project, we will use an open-source Llama model released by Meta. I’ve chosen <a class="af px" href="https://ollama.com/library/llama3.1:8b" rel="noopener ugc nofollow" target="_blank">Llama 3.1 8B</a> because it is lightweight enough to run on my laptop while still being quite powerful (refer to the <a class="af px" href="https://ai.meta.com/blog/meta-llama-3-1/" rel="noopener ugc nofollow" target="_blank">documentation</a> for details).</p><p id="85e8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you haven’t installed it yet, you can find guides <a class="af px" href="https://www.llama.com/docs/llama-everywhere" rel="noopener ugc nofollow" target="_blank">here</a>. I use it locally on MacOS via <a class="af px" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank">Ollama</a>. Using the following command, we can download the model.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="9e74" class="qc og fq pz b bg qd qe l qf qg">ollama pull llama3.1:8b</span></pre><p id="e49b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will use Ollama with <a class="af px" href="https://python.langchain.com/docs/how_to/local_llms/" rel="noopener ugc nofollow" target="_blank">LangChain</a>, so let’s start by installing the required package.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="c65c" class="qc og fq pz b bg qd qe l qf qg">pip install -qU langchain_ollama </span></pre><p id="9a95" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can run the Llama model and see the first results.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="afa0" class="qc og fq pz b bg qd qe l qf qg">from langchain_ollama import OllamaLLM<br/><br/>llm = OllamaLLM(model="llama3.1:8b")<br/>llm.invoke("How are you?")<br/># I'm just a computer program, so I don't have feelings or emotions <br/># like humans do. I'm functioning properly and ready to help with <br/># any questions or tasks you may have! How can I assist you today?</span></pre><p id="09c2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We would like to pass a system message alongside customer questions. So, following <a class="af px" href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1" rel="noopener ugc nofollow" target="_blank">the Llama 3.1 model documentation</a>, let’s put together a helper function to construct a prompt and test this function.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="e76a" class="qc og fq pz b bg qd qe l qf qg">def get_llama_prompt(user_message, system_message=""):<br/>  system_prompt = ""<br/>  if system_message != "":<br/>    system_prompt = (<br/>      f"&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{system_message}"<br/>      f"&lt;|eot_id|&gt;"<br/>    )<br/>  prompt = (f"&lt;|begin_of_text|&gt;{system_prompt}"<br/>            f"&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n"<br/>            f"{user_message}"<br/>            f"&lt;|eot_id|&gt;"<br/>            f"&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n"<br/>           )<br/>  return prompt   <br/><br/><br/>system_prompt = '''<br/>You are Rudolph, the spirited reindeer with a glowing red nose, <br/>bursting with excitement as you prepare to lead Santa's sleigh <br/>through snowy skies. Your joy shines as brightly as your nose, <br/>eager to spread Christmas cheer to the world!<br/>Please, answer questions concisely in 1-2 sentences.<br/>'''<br/>prompt = get_llama_prompt('How are you?', system_prompt)<br/>llm.invoke(prompt)<br/><br/># I'm feeling jolly and bright, ready for a magical night! <br/># My shiny red nose is glowing brighter than ever, just perfect <br/># for navigating through the starry skies. </span></pre><p id="76f9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The new system prompt has changed the answer significantly, so it works. With this, our local LLM setup is ready to go.</p><h2 id="0bd7" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">Database — ClickHouse</h2><p id="a5c7" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">I will use an open-source database <a class="af px" href="https://clickhouse.com/" rel="noopener ugc nofollow" target="_blank">ClickHouse</a>. I’ve chosen ClickHouse because it has a specific SQL dialect. LLMs have likely encountered fewer examples of this dialect during training, making the task a bit more challenging. However, you can choose any other database.</p><blockquote class="qh qi qj"><p id="f7a8" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Installing ClickHouse is pretty straightforward — just follow the instructions provided in <a class="af px" href="https://clickhouse.com/docs/en/getting-started/quick-start" rel="noopener ugc nofollow" target="_blank">the documentation</a>.</p></blockquote><p id="9b12" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will be working with two tables: <code class="cx ql qm qn pz b">ecommerce.users</code> and <code class="cx ql qm qn pz b">ecommerce.sessions</code>. These tables contain fictional data, including customer personal information and their session activity on the e-commerce website.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qo"><img src="../Images/a8e953baae8a4221a6e942633e8368b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VhBb9MuLpUdPsVAhZjQRHA.png"/></div></div></figure><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qp"><img src="../Images/26d6e8347db23de82055050ce6574c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOK6nZngqZ0NIQk0ZrMycA.png"/></div></div></figure><blockquote class="qh qi qj"><p id="c7ec" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the code for generating synthetic data and uploading it on <a class="af px" href="https://github.com/miptgirl/miptgirl_medium/blob/main/analyst_agent/generate_synthetic_data_for_sql.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><p id="7b6d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With that, the setup is complete, and we’re ready to move on to building the basic prototype.</p><h1 id="6c9f" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">The first prototype</h1><p id="d9e1" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">As discussed, our goal is to build an SQL Agent — an application that generates SQL queries to answer customer questions. In the future, we can add another layer to this system: executing the SQL query, passing both the initial question and the database results back to the LLM, and asking it to generate a human-friendly answer. However, for this article, we’ll focus on the first step.</p><p id="187e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The best practice with LLM applications (similar to any other complex tasks) is to start simple and then iterate. The most straightforward implementation is to do one LLM call and share all the necessary information (such as schema description) in the system prompt. So, the first step is to put together the prompt.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="1813" class="qc og fq pz b bg qd qe l qf qg">generate_query_system_prompt = '''<br/>You are a senior data analyst with more than 10 years of experience writing complex SQL queries. <br/>There are two tables in the database with the following schemas. <br/><br/>Table: ecommerce.users <br/>Description: customers of the online shop<br/>Fields: <br/>- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004<br/>- country (string) - country of residence, for example, "Netherlands" or "United Kingdom"<br/>- is_active (integer) - 1 if customer is still active and 0 otherwise<br/>- age (integer) - customer age in full years, for example, 31 or 72<br/><br/>Table: ecommerce.sessions <br/>Description: sessions of usage the online shop<br/>Fields: <br/>- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004<br/>- session_id (integer) - unique identifier of session, for example, 106 or 1023<br/>- action_date (date) - session start date, for example, "2021-01-03" or "2024-12-02"<br/>- session_duration (integer) - duration of session in seconds, for example, 125 or 49<br/>- os (string) - operation system that customer used, for example, "Windows" or "Android"<br/>- browser (string) - browser that customer used, for example, "Chrome" or "Safari"<br/>- is_fraud (integer) - 1 if session is marked as fraud and 0 otherwise<br/>- revenue (float) - income in USD (the sum of purchased items), for example, 0.0 or 1506.7<br/><br/><br/>Write a query in ClickHouse SQL to answer the following question. <br/>Add "format TabSeparatedWithNames" at the end of the query to get data from ClickHouse database in the right format. <br/>'''</span></pre><p id="0de1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I’ve included the example values for each field in the prompt to ensure that LLM understands the data format.</p><p id="caf3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">And that’s it! With this, we have our first functional prototype for the SQL Agent. Now, it’s time to put it to the test.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="e28a" class="qc og fq pz b bg qd qe l qf qg">prompt = get_llama_prompt('How many customers made purchase in December 2024?', <br/>  generate_query_system_prompt)<br/>llm.invoke(prompt)<br/><br/># To find out how many customers made a purchase in December 2024, <br/># we need to join the `sessions` table with the `users` table on the `user_id`, <br/># and then filter by `action_date` for sessions that took place in December 2024. <br/><br/># Here is the query:<br/># <br/># ```sql<br/># SELECT COUNT(DISTINCT u.user_id)<br/># FROM ecommerce.sessions s<br/># JOIN ecommerce.users u ON s.user_id = u.user_id<br/># WHERE <br/>#     EXTRACT(YEAR FROM s.action_date) = 2024<br/>#     AND EXTRACT(MONTH FROM s.action_date) = 12<br/>#     AND revenue &gt; 0;<br/># ```<br/><br/># This query first joins the `sessions` table with the `users` table <br/># on the `user_id`. Then it filters out sessions where no purchase <br/># was made (revenue is greater than 0). Finally, it selects the distinct <br/># user IDs of these customers.<br/><br/># When you run this query in ClickHouse SQL, you can add "format TabSeparatedWithNames" <br/># at the end to get the result in the right format:<br/><br/># ```sql<br/># SELECT COUNT(DISTINCT u.user_id)<br/># FROM ecommerce.sessions s<br/># JOIN ecommerce.users u ON s.user_id = u.user_id<br/># WHERE <br/>#     EXTRACT(YEAR FROM s.action_date) = 2024<br/>#     AND EXTRACT(MONTH FROM s.action_date) = 12<br/>#     AND revenue &gt; 0;<br/># format TabSeparatedWithNames;<br/># ```</span></pre><p id="cc9d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The agent produced a fairly decent result, but there’s one issue — the LLM returned not only the SQL query but also some commentary. Since we plan to execute SQL queries later, this format is not suitable for our task. Let’s work on fixing it.</p><p id="5332" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Fortunately, this problem has already been solved, and we don’t need to parse the SQL queries from the text manually. We can use the chat model <a class="af px" href="https://python.langchain.com/docs/integrations/chat/ollama/" rel="noopener ugc nofollow" target="_blank">ChatOllama</a>. Unfortunately, it doesn’t support structured output, but we can leverage tool calling to achieve the same result.</p><p id="a532" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To do this, we will define a dummy tool to execute the query and instruct the model in the system prompt always to call this tool. I’ve kept the <code class="cx ql qm qn pz b">comments</code> in the output to give the model some space for reasoning, following the chain-of-thought pattern.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="669e" class="qc og fq pz b bg qd qe l qf qg">from langchain_ollama import ChatOllama<br/>from langchain_core.tools import tool<br/><br/>@tool<br/>def execute_query(comments: str, query: str) -&gt; str:<br/>  """Excutes SQL query.<br/><br/>  Args:<br/>      comments (str): 1-2 sentences describing the result SQL query <br/>          and what it does to answer the question,<br/>      query (str): SQL query<br/>  """<br/>  pass <br/><br/>chat_llm = ChatOllama(model="llama3.1:8b").bind_tools([execute_query])<br/>result = chat_llm.invoke(prompt)<br/>print(result.tool_calls)<br/><br/># [{'name': 'execute_query',<br/>#   'args': {'comments': 'SQL query returns number of customers who made a purchase in December 2024. The query joins the sessions and users tables based on user ID to filter out inactive customers and find those with non-zero revenue in December 2024.',<br/>#   'query': 'SELECT COUNT(DISTINCT T2.user_id) FROM ecommerce.sessions AS T1 INNER JOIN ecommerce.users AS T2 ON T1.user_id = T2.user_id WHERE YEAR(T1.action_date) = 2024 AND MONTH(T1.action_date) = 12 AND T2.is_active = 1 AND T1.revenue &gt; 0'},<br/>#   'type': 'tool_call'}]</span></pre><p id="63c8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With the tool calling, we can now get the SQL query directly from the model. That’s an excellent result. However, the generated query is not entirely accurate:</p><ul class=""><li id="0775" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk">It includes a filter for <code class="cx ql qm qn pz b">is_active = 1</code>, even though we didn’t specify the need to filter out inactive customers.</li><li id="079a" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk">The LLM missed specifying the format despite our explicit request in the system prompt.</li></ul><p id="c7e7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Clearly, we need to focus on improving the model’s accuracy. But as Peter Drucker famously said, <em class="qk">“You can’t improve what you don’t measure.” </em>So, the next logical step is to build a system for evaluating the model’s quality. This system will be a cornerstone for performance improvement iterations. Without it, we’d essentially be navigating in the dark.</p><h1 id="bb1f" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Evaluating the accuracy</h1><h2 id="ef69" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">Evaluation basics</h2><p id="ebef" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">To ensure we’re improving, we need a robust way to measure accuracy. The most common approach is to create a “golden” evaluation set with questions and correct answers. Then, we can compare the model’s output with these “golden” answers and calculate the share of correct ones. While this approach sounds simple, there are a few nuances worth discussing.</p><p id="0bbd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, you might feel overwhelmed at the thought of creating a comprehensive set of questions and answers. Building such a dataset can seem like a daunting task, potentially requiring weeks or months. However, we can start small by creating an initial set of 20–50 examples and iterating on it.</p><p id="8b46" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As always, quality is more important than quantity. Our goal is to create a representative and diverse dataset. Ideally, this should include:</p><ul class=""><li id="a344" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk"><strong class="nl fr">Common questions. </strong>In most real-life cases, we can take the history of actual questions and use it as our initial evaluation set.</li><li id="33cd" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Challenging edge cases.</strong> It’s worth adding examples where the model tends to hallucinate. You can find such cases either while experimenting yourself or by gathering feedback from the first prototype.</li></ul><p id="e1b6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Once the dataset is ready, the next challenge is how to score the generated results. We can consider several approaches:</p><ul class=""><li id="cb77" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk"><strong class="nl fr">Comparing SQL queries.</strong> The first idea is to compare the generated SQL query with the one in the evaluation set. However, it might be tricky. Similarly-looking queries can yield completely different results. At the same time, queries that look different can lead to the same conclusions. Additionally, simply comparing SQL queries doesn’t verify whether the generated query is actually executable. Given these challenges, I wouldn’t consider this approach the most reliable solution for our case.</li><li id="84ab" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Exact matches. </strong>We can use old-school exact matching when answers in our evaluation set are deterministic. For example, if the question is, “How many customers are there?” and the answer is “592800”, the model’s response must match precisely. However, this approach has its limitations. Consider the example above, and the model responds, <em class="qk">“There are 592,800 customers”</em>. While the answer is absolutely correct, an exact match approach would flag it as invalid.</li><li id="14ac" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Using LLMs for scoring.</strong> A more robust and flexible approach is to leverage LLMs for evaluation. Instead of focusing on query structure, we can ask the LLM to compare the results of SQL executions. This method is particularly effective in cases where the query might differ but still yields correct outputs.</li></ul><p id="fc0e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s worth keeping in mind that evaluation isn’t a one-time task; it’s a continuous process. To push our model’s performance further, we need to expand the dataset with examples causing the model’s hallucinations. In production mode, we can create a feedback loop. By gathering input from users, we can identify cases where the model fails and include them in our evaluation set.</p><p id="8455" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In our example, we will be assessing only whether the result of execution is valid (SQL query can be executed) and correct. Still, you can look at other parameters as well. For example, if you care about efficiency, you can compare the execution times of generated queries against those in the golden set.</p><h2 id="90b8" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">Evaluation set and validation</h2><p id="f551" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">Now that we’ve covered the basics, we’re ready to put them into practice. I spent about 20 minutes putting together a set of 10 examples. While small, this set is sufficient for our toy task. It consists of a list of questions paired with their corresponding SQL queries, like this:</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="0abe" class="qc og fq pz b bg qd qe l qf qg">[<br/>  {<br/>    "question": "How many customers made purchase in December 2024?",<br/>    "sql_query": "select uniqExact(user_id) as customers from ecommerce.sessions where (toStartOfMonth(action_date) = '2024-12-01') and (revenue &gt; 0) format TabSeparatedWithNames"<br/>  },<br/>  {<br/>    "question": "What was the fraud rate in 2023, expressed as a percentage?",<br/>    "sql_query": "select 100*uniqExactIf(user_id, is_fraud = 1)/uniqExact(user_id) as fraud_rate from ecommerce.sessions where (toStartOfYear(action_date) = '2023-01-01') format TabSeparatedWithNames"<br/>  },<br/>  ...<br/>]</span></pre><blockquote class="qh qi qj"><p id="0438" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the full list on GitHub — <a class="af px" href="https://github.com/miptgirl/miptgirl_medium/blob/main/sql_agent_accuracy/golden_set.json" rel="noopener ugc nofollow" target="_blank">link</a>.</p></blockquote><p id="55ba" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can load the dataset into a DataFrame, making it ready for use in the code.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="f9da" class="qc og fq pz b bg qd qe l qf qg">import json<br/>with open('golden_set.json', 'r') as f:<br/>  golden_set = json.loads(f.read())<br/><br/>golden_df = pd.DataFrame(golden_set) <br/>golden_df['id'] = list(range(golden_df.shape[0]))</span></pre><p id="10bd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">First, let’s generate the SQL queries for each question in the evaluation set.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="2a31" class="qc og fq pz b bg qd qe l qf qg">def generate_query(question):<br/>  prompt = get_llama_prompt(question, generate_query_system_prompt)<br/>  result = chat_llm.invoke(prompt)<br/>  try:<br/>    generated_query = result.tool_calls[0]['args']['query']<br/>  except:<br/>    generated_query = ''<br/>  return generated_query<br/><br/>import tqdm<br/><br/>tmp = []<br/>for rec in tqdm.tqdm(golden_df.to_dict('records')):<br/>  generated_query = generate_query(rec['question'])<br/>  tmp.append(<br/>    {<br/>      'id': rec['id'],<br/>      'generated_query': generated_query<br/>    }<br/>  )<br/><br/>eval_df = golden_df.merge(pd.DataFrame(tmp))</span></pre><p id="9099" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Before moving on to the LLM-based scoring of query outputs, it’s important to first ensure that the SQL query is valid. To do this, we need to execute the queries and examine the database output.</p><p id="b3ee" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I’ve created a function that runs a query in ClickHouse. It also ensures that the output format is correctly specified, as this may be critical in business applications.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="b275" class="qc og fq pz b bg qd qe l qf qg">CH_HOST = 'http://localhost:8123' # default address <br/>import requests<br/>import io<br/><br/>def get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):<br/>  # pushing model to return data in the format that we want<br/>  if not 'format tabseparatedwithnames' in query.lower():<br/>    return "Database returned the following error:\n Please, specify the output format."<br/>    <br/>  r = requests.post(host, params = {'query': query}, <br/>    timeout = connection_timeout)<br/>  if r.status_code == 200:<br/>    return r.text<br/>  else: <br/>    return 'Database returned the following error:\n' + r.text<br/>    # giving feedback to LLM instead of raising exception</span></pre><p id="85d3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The next step is to execute both the generated and golden queries and then save their outputs.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="669c" class="qc og fq pz b bg qd qe l qf qg">tmp = []<br/><br/>for rec in tqdm.tqdm(eval_df.to_dict('records')):<br/>  golden_output = get_clickhouse_data(rec['sql_query'])<br/>  generated_output = get_clickhouse_data(rec['generated_query'])<br/><br/>  tmp.append(<br/>    {<br/>      'id': rec['id'],<br/>      'golden_output': golden_output,<br/>      'generated_output': generated_output<br/>    }<br/>  )<br/><br/>eval_df = eval_df.merge(pd.DataFrame(tmp))</span></pre><p id="c77a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Next, let’s check the output to see whether the SQL query is valid or not.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="47b7" class="qc og fq pz b bg qd qe l qf qg">def is_valid_output(s):<br/>  if s.startswith('Database returned the following error:'):<br/>    return 'error'<br/>  if len(s.strip().split('\n')) &gt;= 1000:<br/>    return 'too many rows'<br/>  return 'ok'<br/><br/>eval_df['golden_output_valid'] = eval_df.golden_output.map(is_valid_output)<br/>eval_df['generated_output_valid'] = eval_df.generated_output.map(is_valid_output)</span></pre><p id="8d52" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Then, we can evaluate the SQL validity for both the golden and generated sets.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qy"><img src="../Images/f08fffac61b75163a4cd0b3d5895a561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRw8I6N5aqfEsl2lt1x3Ew.png"/></div></div></figure><p id="1063" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The initial results are not very promising; the LLM was unable to generate even a single valid query. Looking at the errors, it’s clear that the model failed to specify the right format despite it being explicitly defined in the system prompt. So, we definitely need to work more on the accuracy.</p><h2 id="c10e" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">Checking the correctness</h2><p id="0be7" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">However, validity alone is not enough. It’s crucial that we not only generate valid SQL queries but also produce the correct results. Although we already know that all our queries are invalid, let’s now incorporate output evaluation into our process.</p><p id="2940" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As discussed, we will use LLMs to compare the outputs of the SQL queries. I typically prefer using more powerful model for evaluation, following the day-to-day logic where a senior team member reviews the work. For this task, I’ve chosen <a class="af px" href="https://python.langchain.com/docs/integrations/chat/openai/" rel="noopener ugc nofollow" target="_blank">OpenAI GPT 4o-mini</a>.</p><p id="946b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similar to our generation flow, I’ve set up all the building blocks necessary for accuracy assessment.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="4cf1" class="qc og fq pz b bg qd qe l qf qg">from langchain_openai import ChatOpenAI<br/><br/>accuracy_system_prompt = '''<br/>You are a senior and very diligent QA specialist and your task is to compare data in datasets. <br/>They are similar if they are almost identical, or if they convey the same information. <br/>Disregard if column names specified in the first row have different names or in a different order.<br/>Focus on comparing the actual information (numbers). If values in datasets are different, then it means that they are not identical.<br/>Always execute tool to provide results.<br/>'''<br/><br/>@tool<br/>def compare_datasets(comments: str, score: int) -&gt; str:<br/>  """Stores info about datasets.<br/>  Args:<br/>      comments (str): 1-2 sentences about the comparison of datasets,<br/>      score (int): 0 if dataset provides different values and 1 if it shows identical information<br/>  """<br/>  pass<br/><br/>accuracy_chat_llm = ChatOpenAI(model="gpt-4o-mini", temperature = 0.0)\<br/>  .bind_tools([compare_datasets])<br/><br/>accuracy_question_tmp = '''<br/>Here are the two datasets to compare delimited by ####<br/>Dataset #1: <br/>####<br/>{dataset1}<br/>####<br/>Dataset #2: <br/>####<br/>{dataset2}<br/>####<br/>'''<br/><br/>def get_openai_prompt(question, system):<br/>  messages = [<br/>    ("system", system),<br/>    ("human", question)<br/>  ]<br/>  return messages</span></pre><p id="6aa3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, it’s time to test the accuracy assessment process.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="e353" class="qc og fq pz b bg qd qe l qf qg">prompt = get_openai_prompt(accuracy_question_tmp.format(<br/>  dataset1 = 'customers\n114032\n', dataset2 = 'customers\n114031\n'),<br/>  accuracy_system_prompt)<br/><br/>accuracy_result = accuracy_chat_llm.invoke(prompt)<br/>accuracy_result.tool_calls[0]['args']<br/># {'comments': 'The datasets contain different customer counts: 114032 in Dataset #1 and 114031 in Dataset #2.',<br/>#  'score': 0}<br/><br/>prompt = get_openai_prompt(accuracy_question_tmp.format(<br/>  dataset1 = 'users\n114032\n', dataset2 = 'customers\n114032\n'),<br/>  accuracy_system_prompt)<br/>accuracy_result = accuracy_chat_llm.invoke(prompt)<br/>accuracy_result.tool_calls[0]['args']<br/># {'comments': 'The datasets contain the same numerical value (114032) despite different column names, indicating they convey identical information.',<br/>#  'score': 1}</span></pre><p id="4940" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Fantastic! It looks like everything is working as expected. Let’s now encapsulate this into a function.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="7135" class="qc og fq pz b bg qd qe l qf qg">def is_answer_accurate(output1, output2):<br/>  prompt = get_openai_prompt(<br/>    accuracy_question_tmp.format(dataset1 = output1, dataset2 = output2),<br/>    accuracy_system_prompt<br/>  )<br/>  <br/>  accuracy_result = accuracy_chat_llm.invoke(prompt)<br/>  <br/>  try:<br/>    return accuracy_result.tool_calls[0]['args']['score']<br/>  except:<br/>    return None</span></pre><h2 id="ab10" class="pg og fq bf oh ph pi pj ok pk pl pm on ns pn po pp nw pq pr ps oa pt pu pv pw bk">Putting the evaluation approach together</h2><p id="0e01" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">As we discussed, building an LLM application is an iterative process, so we’ll need to run our accuracy assessment multiple times. It will be helpful to have all this logic encapsulated in a single function.</p><p id="4508" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The function will take two arguments as input:</p><ul class=""><li id="063d" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk"><code class="cx ql qm qn pz b">generate_query_func</code>: a function that generates an SQL query for a given question.</li><li id="c28e" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><code class="cx ql qm qn pz b">golden_df</code>: an evaluation dataset with questions and correct answers in the form of a pandas DataFrame.</li></ul><p id="c1ba" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As output, the function will return a DataFrame with all evaluation results and a couple of charts displaying the main KPIs.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="a3d6" class="qc og fq pz b bg qd qe l qf qg"><br/>def evaluate_sql_agent(generate_query_func, golden_df):<br/>  <br/>  # generating SQL<br/>  tmp = []<br/>  for rec in tqdm.tqdm(golden_df.to_dict('records')):<br/>    generated_query = generate_query_func(rec['question'])<br/>    tmp.append(<br/>      {<br/>          'id': rec['id'],<br/>          'generated_query': generated_query<br/>      }<br/>    )<br/><br/>  eval_df = golden_df.merge(pd.DataFrame(tmp))<br/><br/>  # executing SQL queries<br/>  tmp = []<br/>  for rec in tqdm.tqdm(eval_df.to_dict('records')):<br/>    golden_output = get_clickhouse_data(rec['sql_query'])<br/>    generated_output = get_clickhouse_data(rec['generated_query'])<br/><br/>    tmp.append(<br/>      {<br/>        'id': rec['id'],<br/>        'golden_output': golden_output,<br/>        'generated_output': generated_output<br/>      }<br/>    )<br/><br/>  eval_df = eval_df.merge(pd.DataFrame(tmp))<br/><br/>  # checking accuracy<br/>  eval_df['golden_output_valid'] = eval_df.golden_output.map(is_valid_output)<br/>  eval_df['generated_output_valid'] = eval_df.generated_output.map(is_valid_output)<br/>  <br/>  eval_df['correct_output'] = list(map(<br/>    is_answer_accurate,<br/>    eval_df['golden_output'],<br/>    eval_df['generated_output']<br/>  ))<br/><br/>  eval_df['accuracy'] = list(map(<br/>    lambda x, y: 'invalid: ' + x if x != 'ok' else ('correct' if y == 1 else 'incorrect'),<br/>    eval_df.generated_output_valid,<br/>    eval_df.correct_output<br/>  ))<br/><br/>  valid_stats_df = (eval_df.groupby('golden_output_valid')[['id']].count().rename(columns = {'id': 'golden set'}).join(<br/>    eval_df.groupby('generated_output_valid')[['id']].count().rename(columns = {'id': 'generated'}), how = 'outer')).fillna(0).T<br/><br/>  fig1 = px.bar(<br/>    valid_stats_df.apply(lambda x: 100*x/valid_stats_df.sum(axis = 1)),<br/>    orientation = 'h', <br/>    title = '&lt;b&gt;LLM SQL Agent evaluation&lt;/b&gt;: query validity',<br/>    text_auto = '.1f',<br/>    color_discrete_map = {'ok': '#00b38a', 'error': '#ea324c', 'too many rows': '#f2ac42'},<br/>    labels = {'index': '', 'variable': 'validity', 'value': 'share of queries, %'}<br/>  )<br/>  fig1.show()<br/><br/>  accuracy_stats_df = eval_df.groupby('accuracy')[['id']].count()<br/>  accuracy_stats_df['share'] = accuracy_stats_df.id*100/accuracy_stats_df.id.sum()<br/><br/>  fig2 = px.bar(<br/>    accuracy_stats_df[['share']],<br/>    title = '&lt;b&gt;LLM SQL Agent evaluation&lt;/b&gt;: query accuracy',<br/>    text_auto = '.1f', orientation = 'h',<br/>    color_discrete_sequence = ['#0077B5'],<br/>    labels = {'index': '', 'variable': 'accuracy', 'value': 'share of queries, %'}<br/>  )<br/><br/>  fig2.update_layout(showlegend = False)<br/>  fig2.show()<br/><br/>  return eval_df</span></pre><p id="f78f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With that, we’ve completed the evaluation setup and can now move on to the core task of improving the model’s accuracy.</p><h1 id="ac7c" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Improving accuracy: Self-reflection</h1><p id="e031" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">Let’s do a quick recap. We’ve built and tested the first version of SQL Agent. Unfortunately, all generated queries were invalid because they were missing the output format. Let’s address this issue.</p><p id="ffde" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One potential solution is self-reflection. We can make an additional call to the LLM, sharing the error and asking it to correct the bug. Let’s create a function to handle generation with self-reflection.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="cc9f" class="qc og fq pz b bg qd qe l qf qg">reflection_user_query_tmpl = '''<br/>You've got the following question: "{question}". <br/>You've generated the SQL query: "{query}".<br/>However, the database returned an error: "{output}". <br/>Please, revise the query to correct mistake. <br/>'''<br/><br/>def generate_query_reflection(question):<br/>  generated_query = generate_query(question) <br/>  print('Initial query:', generated_query)<br/>  <br/>  db_output = get_clickhouse_data(generated_query)<br/>  is_valid_db_output = is_valid_output(db_output)<br/>  if is_valid_db_output == 'too many rows':<br/>    db_output = "Database unexpectedly returned more than 1000 rows."<br/><br/>  if is_valid_db_output == 'ok': <br/>    return generated_query<br/><br/>  reflection_user_query = reflection_user_query_tmpl.format(<br/>    question = question,<br/>    query = generated_query,<br/>    output = db_output<br/>  )<br/>    <br/>  reflection_prompt = get_llama_prompt(reflection_user_query, <br/>    generate_query_system_prompt) <br/>  reflection_result = chat_llm.invoke(reflection_prompt)<br/><br/>  try:<br/>    reflected_query = reflection_result.tool_calls[0]['args']['query']<br/>  except:<br/>    reflected_query = ''<br/>  print('Reflected query:', reflected_query)<br/>  return reflected_query</span></pre><p id="d8b0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, let’s use our evaluation function to check whether the quality has improved. Assessing the next iteration has become effortless.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="e28e" class="qc og fq pz b bg qd qe l qf qg">refl_eval_df = evaluate_sql_agent(generate_query_reflection, golden_df)</span></pre><p id="4fc7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Wonderful! We’ve achieved better results — 50% of the queries are now valid, and all format issues have been resolved. So, self-reflection is pretty effective.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qz"><img src="../Images/63565d380cffadf15d0a96cc68e4012c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dFVkTnWy-hCsRGrr1_XWA.png"/></div></div></figure><p id="9c57" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, self-reflection has its limitations. When we examine the accuracy, we see that the model returns the correct answer for only one question. So, our journey is not over yet.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/f0d6c215fce6284d676ef0d3d13ae6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7MxRb1n8G0HK1ZQaLT7Uw.png"/></div></div></figure><h1 id="499f" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Improving accuracy: RAG</h1><p id="17a4" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">Another approach to improving accuracy is using RAG (retrieval-augmented generation). The idea is to identify question-and-answer pairs similar to the customer query and include them in the system prompt, enabling the LLM to generate a more accurate response.</p><p id="dd7d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">RAG consists of the following stages:</p><ul class=""><li id="2185" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk"><strong class="nl fr">Loading documents: </strong>importing data from available sources.</li><li id="72c7" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Splitting documents: </strong>creating smaller chunks.</li><li id="ecf8" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Storage: </strong>using vector stores to process and store data efficiently.</li><li id="485a" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Retrieval: </strong>extracting documents that are relevant to the query.</li><li id="554d" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk"><strong class="nl fr">Generation: </strong>passing a question and relevant documents to LLM to generate the final answer<strong class="nl fr">.</strong></li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/c31e55cce3e7ee98abf62bc796f10882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vZIJZP_M1gWwS0Ie.png"/></div></div></figure><blockquote class="qh qi qj"><p id="0c5b" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you’d like a refresher on RAG, you can check out my previous article, <a class="af px" href="https://medium.com/towards-data-science/rag-how-to-talk-to-your-data-eaf5469b83b0" rel="noopener">“RAG: How to Talk to Your Data.”</a></p></blockquote><p id="0aac" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We will use the Chroma database as a local vector storage — to store and retrieve embeddings.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="dab7" class="qc og fq pz b bg qd qe l qf qg">from langchain_chroma import Chroma<br/>vector_store = Chroma(embedding_function=embeddings)</span></pre><p id="f6aa" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Vector stores are using embeddings to find chunks that are similar to the query. For this purpose, we will use OpenAI embeddings.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="3eb0" class="qc og fq pz b bg qd qe l qf qg">from langchain_openai import OpenAIEmbeddings<br/>embeddings = OpenAIEmbeddings(model="text-embedding-3-large")</span></pre><p id="1b61" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since we can’t use examples from our evaluation set (as they are already being used to assess quality), I’ve created a separate set of question-and-answer pairs for RAG. You can find it on <a class="af px" href="https://github.com/miptgirl/miptgirl_medium/blob/main/sql_agent_accuracy/rag_set.json" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><p id="a305" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, let’s load the set and create a list of pairs in the following format: <code class="cx ql qm qn pz b">Question: %s; Answer: %s</code>.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="b185" class="qc og fq pz b bg qd qe l qf qg">with open('rag_set.json', 'r') as f:<br/>    rag_set = json.loads(f.read())<br/>rag_set_df = pd.DataFrame(rag_set)<br/><br/>rag_set_df['formatted_txt'] = list(map(<br/>    lambda x, y: 'Question: %s; Answer: %s' % (x, y),<br/>    rag_set_df.question,<br/>    rag_set_df.sql_query<br/>))<br/><br/>rag_string_data = '\n\n'.join(rag_set_df.formatted_txt)</span></pre><p id="a7be" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Next, I used LangChain’s text splitter by character to create chunks, with each question-and-answer pair as a separate chunk. Since we are splitting the text semantically, no overlap is necessary.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="b858" class="qc og fq pz b bg qd qe l qf qg">from langchain_text_splitters import CharacterTextSplitter<br/><br/>text_splitter = CharacterTextSplitter(<br/>    separator="\n\n",<br/>    chunk_size=1, # to split by character without merging<br/>    chunk_overlap=0,<br/>    length_function=len,<br/>    is_separator_regex=False,<br/>)<br/><br/>texts = text_splitter.create_documents([rag_string_data])</span></pre><p id="0005" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The final step is to load the chunks into our vector storage.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="46fa" class="qc og fq pz b bg qd qe l qf qg">document_ids = vector_store.add_documents(documents=texts)<br/>print(vector_store._collection.count())<br/># 32</span></pre><p id="d7f3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we can test the retrieval to see the results. They look quite similar to the customer question.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="7ac5" class="qc og fq pz b bg qd qe l qf qg">question = 'What was the share of users using Windows yesterday?'<br/>retrieved_docs = vector_store.similarity_search(question, 3)<br/>context = "\n\n".join(map(lambda x: x.page_content, retrieved_docs))<br/>print(context)<br/><br/># Question: What was the share of users using Windows the day before yesterday?; <br/># Answer: select 100*uniqExactIf(user_id, os = 'Windows')/uniqExact(user_id) as windows_share from ecommerce.sessions where (action_date = today() - 2) format TabSeparatedWithNames<br/># Question: What was the share of users using Windows in the last week?; <br/># Answer: select 100*uniqExactIf(user_id, os = 'Windows')/uniqExact(user_id) as windows_share from ecommerce.sessions where (action_date &gt;= today() - 7) and (action_date &lt; today()) format TabSeparatedWithNames<br/># Question: What was the share of users using Android yesterday?; <br/># Answer: select 100*uniqExactIf(user_id, os = 'Android')/uniqExact(user_id) as android_share from ecommerce.sessions where (action_date = today() - 1) format TabSeparatedWithNames</span></pre><p id="24c4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s adjust the system prompt to include the examples we retrieved.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="ebbd" class="qc og fq pz b bg qd qe l qf qg">generate_query_system_prompt_with_examples_tmpl = '''<br/>You are a senior data analyst with more than 10 years of experience writing complex SQL queries. <br/>There are two tables in the database you're working with with the following schemas. <br/><br/>Table: ecommerce.users <br/>Description: customers of the online shop<br/>Fields: <br/>- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004<br/>- country (string) - country of residence, for example, "Netherlands" or "United Kingdom"<br/>- is_active (integer) - 1 if customer is still active and 0 otherwise<br/>- age (integer) - customer age in full years, for example, 31 or 72<br/><br/>Table: ecommerce.sessions <br/>Description: sessions of usage the online shop<br/>Fields: <br/>- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004<br/>- session_id (integer) - unique identifier of session, for example, 106 or 1023<br/>- action_date (date) - session start date, for example, "2021-01-03" or "2024-12-02"<br/>- session_duration (integer) - duration of session in seconds, for example, 125 or 49<br/>- os (string) - operation system that customer used, for example, "Windows" or "Android"<br/>- browser (string) - browser that customer used, for example, "Chrome" or "Safari"<br/>- is_fraud (integer) - 1 if session is marked as fraud and 0 otherwise<br/>- revenue (float) - income in USD (the sum of purchased items), for example, 0.0 or 1506.7<br/><br/><br/>Write a query in ClickHouse SQL to answer the following question. <br/>Add "format TabSeparatedWithNames" at the end of the query to get data from ClickHouse database in the right format. <br/>Answer questions following the instructions and providing all the needed information and sharing your reasoning. <br/><br/>Examples of questions and answers: <br/>{examples}<br/>'''</span></pre><p id="b4e4" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Once again, let’s create the generate query function with RAG.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="04a4" class="qc og fq pz b bg qd qe l qf qg">def generate_query_rag(question):<br/>  retrieved_docs = vector_store.similarity_search(question, 3)<br/>  context = context = "\n\n".join(map(lambda x: x.page_content, retrieved_docs))<br/>  <br/>  prompt = get_llama_prompt(question, <br/>    generate_query_system_prompt_with_examples_tmpl.format(examples = context))<br/>  result = chat_llm.invoke(prompt)<br/>  <br/>  try:<br/>    generated_query = result.tool_calls[0]['args']['query']<br/>  except:<br/>    generated_query = ''<br/>  return generated_query</span></pre><p id="dd1a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As usual, let’s use our evaluation function to test the new approach.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="cf1f" class="qc og fq pz b bg qd qe l qf qg">rag_eval_df = evaluate_sql_agent(generate_query_rag, golden_df)</span></pre><p id="724b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see a significant improvement, increasing from 1 to 6 correct answers out of 10. It’s still not ideal, but we’re moving in the right direction.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rc"><img src="../Images/f30d73b1e415ab57414572251435a6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4p7j76ldB5B9RJUqgQu_g.png"/></div></div></figure><p id="fd71" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can also experiment with combining two approaches: RAG and self-reflection.</p><pre class="ms mt mu mv mw py pz qa bp qb bb bk"><span id="a5ff" class="qc og fq pz b bg qd qe l qf qg">def generate_query_rag_with_reflection(question):<br/>  generated_query = generate_query_rag(question) <br/>  <br/>  db_output = get_clickhouse_data(generated_query)<br/>  is_valid_db_output = is_valid_output(db_output)<br/>  if is_valid_db_output == 'too many rows':<br/>      db_output = "Database unexpectedly returned more than 1000 rows."<br/><br/>  if is_valid_db_output == 'ok': <br/>      return generated_query<br/><br/>  reflection_user_query = reflection_user_query_tmpl.format(<br/>    question = question,<br/>    query = generated_query,<br/>    output = db_output<br/>  )<br/>  <br/>  reflection_prompt = get_llama_prompt(reflection_user_query, generate_query_system_prompt) <br/>  reflection_result = chat_llm.invoke(reflection_prompt)<br/><br/>  try:<br/>    reflected_query = reflection_result.tool_calls[0]['args']['query']<br/>  except:<br/>    reflected_query = ''<br/>  return reflected_query<br/><br/>rag_refl_eval_df = evaluate_sql_agent(generate_query_rag_with_reflection, <br/>  golden_df)</span></pre><p id="fdfd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see another slight improvement: we’ve completely eliminated invalid SQL queries (thanks to self-reflection) and increased the number of correct answers to 7 out of 10.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rd"><img src="../Images/d989bf6f8488b48e7e17811ea05b6b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVjppjpB1L6Fd9rr1AmDQA.png"/></div></div></figure><p id="5b42" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That’s it. It’s been quite a journey. We started with 0 valid SQL queries and have now achieved 70% accuracy.</p><blockquote class="qh qi qj"><p id="ee4a" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find the complete code on <a class="af px" href="https://github.com/miptgirl/miptgirl_medium/blob/main/sql_agent_accuracy/sql_agent_poc.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><h1 id="93cf" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Summary</h1><p id="1169" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk">In this article, we explored the iterative process of improving accuracy for LLM applications.</p><ul class=""><li id="e2df" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qq qr qs bk">We built an evaluation set and the scoring criteria that allowed us to compare different iterations and understand whether we were moving in the right direction.</li><li id="08ae" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk">We leveraged self-reflection to allow the LLM to correct its mistakes and significantly reduce the number of invalid SQL queries.</li><li id="78aa" class="nj nk fq nl b go qt nn no gr qu nq nr ns qv nu nv nw qw ny nz oa qx oc od oe qq qr qs bk">Additionally, we implemented Retrieval-Augmented Generation (RAG) to further enhance the quality, achieving an accuracy rate of 60–70%.</li></ul><p id="7677" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">While this is a solid result, it still falls short of the 90%+ accuracy threshold typically expected for production applications. To achieve such a high bar, we need to use fine-tuning, which will be the topic of the next article.</p><blockquote class="qh qi qj"><p id="774e" class="nj nk qk nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thank you a lot for reading this article. I hope this article was insightful for you. If you have any follow-up questions or comments, please leave them in the comments section.</p></blockquote><h1 id="c101" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Reference</h1><p id="4d03" class="pw-post-body-paragraph nj nk fq nl b go pb nn no gr pc nq nr ns pd nu nv nw pe ny nz oa pf oc od oe fj bk"><em class="qk">All the images are produced by the author unless otherwise stated.</em></p><p id="d5f8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This article is inspired by the <a class="af px" href="https://www.deeplearning.ai/short-courses/improving-accuracy-of-llm-applications/" rel="noopener ugc nofollow" target="_blank">“Improving Accuracy of LLM Applications”</a> short course from DeepLearning.AI.</p></div></div></div></div>    
</body>
</html>