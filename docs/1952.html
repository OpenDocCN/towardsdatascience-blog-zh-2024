<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Accelerating AI/ML Model Training with Custom Operators</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Accelerating AI/ML Model Training with Custom Operators</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12?source=collection_archive---------1-----------------------#2024-08-11">https://towardsdatascience.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12?source=collection_archive---------1-----------------------#2024-08-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c995" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">On the potential benefits of creating model-specific GPU kernels and their application to optimizing the use of dynamically shaped tensors</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--163ef2a04b12--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--163ef2a04b12--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--163ef2a04b12--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--163ef2a04b12--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/85652d4b9d9dc933840a03993bb7ae6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*duOakYB68UGVJnh7"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@dgeneva68?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">David Marioni</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="024e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This post continues a long <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">series of posts</a> on the topic of analyzing and optimizing the runtime performance of training AI/ML models. The post could easily have been titled “PyTorch Model Performance Analysis and Optimization — Part 7”, but due to the weight of the topic at hand, we decided that a dedicated post (or series of posts) was warranted. In our previous posts, we have spoken at length about the importance of analyzing and optimizing your AI/ML workloads and the potentially significant impact it can have on the speed and costs of AI/ML model development. We have advocated for having multiple tools and techniques for profiling and optimizing training performance and have demonstrated many of these in practice. In this post we will discuss one of the more advanced optimization techniques — one that sets apart the true rock stars from the simple amateurs — creating a custom PyTorch operator in C++ and CUDA.</p><p id="841f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Popular ML frameworks, such as PyTorch, TensorFlow, and JAX are typically built using SW components that are optimized for the underlying hardware that the AI/ML workload is run on, be it a CPU, a GPU, or an AI-specific ASIC such as a Google TPU. However, inevitably, you may find the performance of certain computation blocks that comprise your model to be unsatisfactory or in-optimal. Oftentimes, tuning the low-level code blocks — often referred to as <em class="nz">kernels — </em>to the specific needs of the AI/ML model, can result in significant speed-ups to the runtime performance of model training and inference. Such speed-ups can be accomplished by implementing functionalities that were previously unsupported (e.g., an advanced attention block), fusing together individual operations (e.g., as in <a class="af nc" href="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial" rel="noopener ugc nofollow" target="_blank">PyTorch’s tutorial on multiply-add fusion</a>), and/or optimizing existing kernels based on the specific properties of the model at hand. Importantly, the ability to perform such customization depends on the support of both the AI HW and the ML framework. Although our focus on this post will be on NVIDIA GPUs and the PyTorch framework, it should be noted that other AI ASICs and ML frameworks enable similar capabilities for custom kernel customization. NVIDIA enables the development of custom kernels for its GPUs through its <a class="af nc" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank">CUDA toolkit</a>. And PyTorch includes dedicated <a class="af nc" href="https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.CUDAExtension" rel="noopener ugc nofollow" target="_blank">APIs</a> and <a class="af nc" href="https://pytorch.org/tutorials/advanced/cpp_custom_ops.html#cpp-custom-ops-tutorial" rel="noopener ugc nofollow" target="_blank">tutorials</a> for exposing this functionality and integrating it into the design of your model.</p><p id="4e48" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our intention in this post is to draw attention to the power and potential of kernel customization and demonstrate its application to the unique challenge of training models with dynamically shaped tensors. Our intention is not — by any means — to replace the official documentation on developing custom operations. Furthermore, the examples we will share were chosen for demonstrative purposes only. We have made no effort to optimize these or verify their robustness, durability, or accuracy. If, based on this post, you choose to invest in AI/ML optimization via custom CUDA kernel development, you should be sure to undergo the appropriate training.</p><h1 id="69d3" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Toy Model — The Challenge of Dynamically Shaped Tensors</h1><p id="af64" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The prevalence of tensors with dynamic shapes in AI models can pose unique and exciting challenges with regards to performance optimization. We have already seen one example of this in a <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-part-3-1c5876d78fe2">previous post</a> in which we demonstrated how the use of boolean masks can trigger an undesired CPU-GPU sync event and advocated against their use. Generally speaking, AI accelerators tend to prefer tensors with fixed shapes over ones with dynamic shapes. Not only does it simplify the management of memory resources, but it also enables greater opportunity for performance optimization (e.g., using <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a>). The toy example that follows demonstrates this challenge.</p><p id="d500" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Suppose we are tasked with creating a face detection model for a next-generation digital camera. To train this model we are provided with a dataset of one million <em class="nz">256</em>x<em class="nz">256</em> grayscale images and associated ground-truth bounding boxes for each image. Naturally, the number of faces in each image can vary greatly, with the vast majority of images containing five or fewer faces, and just a few containing dozens or even hundreds. The requirement from our model is to support all variations. Specifically, our model needs to support the detection of up to <em class="nz">256</em> faces in an image.</p><p id="622a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To address this challenge, we define the following naïve model that generates bounding boxes and an accompanying loss function. In particular, we naïvely truncate the model outputs based on the number of target boxes rather than perform some form of <a class="af nc" href="https://en.wikipedia.org/wiki/Hungarian_algorithm" rel="noopener ugc nofollow" target="_blank">assignment algorithm</a> for matching between the bounding box predictions and ground truth targets. We (somewhat arbitrarily) choose the <a class="af nc" href="https://giou.stanford.edu/" rel="noopener ugc nofollow" target="_blank">Generalized Intersection Over Union (GIOU)</a> loss. A real-world solution would likely be far more sophisticated (e.g., it would include a loss component that includes a penalty for false positives).</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="ce39" class="pf ob fq pc b bg pg ph l pi pj">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/><br/>class Net(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        conv_layers = []<br/>        for i in range(4):<br/>            conv_layers.append(nn.Conv2d(4 ** i, 4 ** (i + 1), 3,<br/>                                         padding='same'))<br/>            conv_layers.append(nn.MaxPool2d(2, 2))<br/>            conv_layers.append(nn.ReLU())<br/>        self.conv_layers = nn.Sequential(*conv_layers)<br/><br/>        self.lin1 = nn.Linear(256 * 256, 256 * 64)<br/>        self.lin2 = nn.Linear(256 * 64, 256 * 4)<br/><br/>    def forward(self, x):<br/>        x = self.conv_layers(x.float())<br/>        x = self.lin2(F.relu(self.lin1(x.view((-1, 256 * 256)))))<br/>        return x.view((-1, 256, 4))<br/><br/><br/>def generalized_box_iou(boxes1, boxes2):<br/>    # loosly based on torchvision generalized_box_iou_loss code<br/>    epsilon = 1e-5<br/><br/>    area1 = (boxes1[..., 2]-boxes1[..., 0])*(boxes1[..., 3]-boxes1[..., 1])<br/>    area2 = (boxes2[..., 2]-boxes2[..., 0])*(boxes2[..., 3]-boxes2[..., 1])<br/><br/>    lt = torch.max(boxes1[..., :2], boxes2[..., :2])<br/>    rb = torch.min(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    wh = (rb - lt).clamp(min=0)<br/>    inter = wh[..., 0] * wh[..., 1]<br/><br/>    union = area1 + area2 - inter<br/>    iou = inter / union.clamp(epsilon)<br/><br/>    lti = torch.min(boxes1[..., :2], boxes2[..., :2])<br/>    rbi = torch.max(boxes1[..., 2:], boxes2[..., 2:])<br/><br/>    whi = (rbi - lti).clamp(min=0)<br/>    areai = (whi[..., 0] * whi[..., 1]).clamp(epsilon)<br/><br/>    return iou - (areai - union) / areai<br/><br/><br/>def loss_fn(pred, targets_list):<br/>    batch_size = len(targets_list)<br/>    total_boxes = 0<br/>    loss_sum = 0.<br/>    for i in range(batch_size):<br/>        targets = targets_list[i]<br/>        num_targets = targets.shape[0]<br/>        if num_targets &gt; 0:<br/>            sample_preds = pred[i, :num_targets]<br/>            total_boxes += num_targets<br/>            loss_sum += generalized_box_iou(sample_preds, targets).sum()<br/>    return loss_sum / max(total_boxes, 1)</span></pre><p id="91c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Due to the varying number of faces per image, the loss is calculated separately for each individual sample rather than a single time (for the entire batch). In particular, the CPU will launch each of the GPU kernels associated with the loss function <em class="nz">B</em> times, where <em class="nz">B </em>is the chosen batch size. Depending on the size of the batch, this could entail a significant overhead, as we will see below.</p><p id="9df9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following block we define a dataset that generates random images and associated bounding boxes. Since the number of faces varies per image, we require a <a class="af nc" href="https://pytorch.org/docs/stable/data.html#working-with-collate-fn" rel="noopener ugc nofollow" target="_blank">custom collate function</a> for grouping samples into batches:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4736" class="pf ob fq pc b bg pg ph l pi pj">from torch.utils.data import Dataset, DataLoader<br/>import numpy as np<br/><br/># A dataset with random images and gt boxes<br/>class FakeDataset(Dataset):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.size = 256<br/>        self.img_size = [256, 256]<br/><br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randint(low=0, high=256, <br/>                                   size=[1]+self.img_size,<br/>                                   dtype=torch.uint8)<br/>        <br/>        # set the distribution over the number of boxes to reflect the fact<br/>        # that the vast majority of images have fewer than 10 faces<br/>        n_boxes = np.clip(np.floor(np.abs(np.random.normal(0, 3)))<br/>                                      .astype(np.int32), 0, 255)<br/>        <br/>        box_sizes = torch.randint(low=1, high=self.size, size=(n_boxes,2))<br/>        top_left = torch.randint(low=0, high=self.size-1, size=(n_boxes, 2))<br/>        bottom_right = torch.clamp(top_left + box_sizes, 0, self.size -1)<br/>        rand_boxes = torch.concat((top_left,bottom_right), dim = 1)<br/>        return rand_image, rand_boxes.to(torch.uint8)<br/><br/>def collate_fn(batch):<br/>    images = torch.stack([b[0] for b in batch],dim=0)<br/>    boxes = [b[1] for b in batch]<br/>    return images, boxes<br/><br/>train_loader = DataLoader(<br/>    dataset = FakeDataset(),<br/>    batch_size=1024,<br/>    pin_memory=True,<br/>    num_workers=16,<br/>    collate_fn=collate_fn<br/>)</span></pre><p id="eb62" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Typically, each training step starts with copying the training batch from the host (CPU) to the device (GPU). When our data samples are of fixed size, they are copied in batches. However, one of the implications of the varying number of faces per image is that the bounding box targets of each sample is copied separately requiring many more individual copy operations.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="864b" class="pf ob fq pc b bg pg ph l pi pj">def data_to_device(data, device):<br/>    if isinstance(data, (list, tuple)):<br/>        return type(data)(<br/>            data_to_device(val, device) for val in data<br/>        )<br/>    elif isinstance(data, torch.Tensor):<br/>        return data.to(device=device, non_blocking=True)</span></pre><p id="72c4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, we define our training/evaluation loop. For the purposes of our discussion, we have chosen to focus just on the forward pass of our training loop. Note the inclusion of a <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html" rel="noopener ugc nofollow" target="_blank">PyTorch profiler</a> object and our use of explicit <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.cuda.synchronize.html" rel="noopener ugc nofollow" target="_blank">synchronization events</a> (to facilitate performance evaluation of different portions of the forward pass).</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4087" class="pf ob fq pc b bg pg ph l pi pj">device = torch.device("cuda:0")<br/>model = torch.compile(Net()).to(device).train()<br/><br/># forward portion of training loop wrapped with profiler object<br/>with torch.profiler.profile(<br/>   schedule=torch.profiler.schedule(wait=5, warmup=5, active=10, repeat=1),<br/>   on_trace_ready=torch.profiler.tensorboard_trace_handler('/tmp/perf/'),<br/>   profile_memory=True<br/>) as prof:<br/>    for step, data in enumerate(train_loader):<br/><br/>        with torch.profiler.record_function('copy data'):<br/>            images, boxes = data_to_device(data, device)<br/>            torch.cuda.synchronize(device)<br/><br/>        with torch.profiler.record_function('forward'):<br/>            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):<br/>                outputs = model(images)<br/>            torch.cuda.synchronize(device)<br/><br/>        with torch.profiler.record_function('calc loss'):<br/>            loss = loss_fn(outputs, boxes)<br/>            torch.cuda.synchronize(device)<br/>        prof.step()<br/>        if step &gt; 30:<br/>            break<br/><br/>    # filter and print profiler results<br/>    event_list = prof.key_averages()<br/>    for i in range(len(event_list) - 1, -1, -1):<br/>        if event_list[i].key not in ['forward', 'calc loss', 'copy data']:<br/>            del event_list[i]<br/>    print(event_list.table())</span></pre><h2 id="38ef" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Performance Analysis</h2><p id="c27f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Running our script on a Google Cloud <a class="af nc" href="https://cloud.google.com/compute/docs/gpus#l4-gpus" rel="noopener ugc nofollow" target="_blank">g2-standard-16</a> VM (with a single L4 GPU), a dedicated <a class="af nc" href="https://cloud.google.com/deep-learning-vm/docs/release-notes" rel="noopener ugc nofollow" target="_blank">deep learning VM image</a>, and PyTorch 2.4.0, generates the output below (which we trimmed for readability).</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="b906" class="pf ob fq pc b bg pg ph l pi pj">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data     288.164ms      28.816ms<br/>      forward        1.192s     119.221ms<br/>    calc loss        9.381s     938.067ms<br/>-------------  ------------  ------------<br/>Self CPU time total: 4.018s<br/>Self CUDA time total: 10.107s</span></pre><p id="31af" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Despite the fact that the loss function contains far fewer operations, it completely dominates the overall step time. The overhead of the repeated invocations of the underlying GPU kernels (for each sample in the batch) is clearly evident in the <em class="nz">Trace </em>view in <a class="af nc" href="https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#use-tensorboard-to-view-results-and-analyze-model-performance" rel="noopener ugc nofollow" target="_blank">TensorBoard</a>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/adbbc45d08a2f4f6b3d425b40a66e026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkzapLhvsFrmXTRXUdss-w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Impact of Individual Invocations of the Loss Function Per Batch Sample as Seen in TensorBoard (by Author)</figcaption></figure><h2 id="0aa7" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Optimization Through Concatenation</h2><p id="e616" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">One way to reduce the number of calls to the <em class="nz">loss</em> function is to combine together all of the valid boxes each batch using <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.concatenate.html" rel="noopener ugc nofollow" target="_blank">concatenation</a>, as shown in the following block.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4973" class="pf ob fq pc b bg pg ph l pi pj">def loss_with_concat(pred, targets_list):<br/>    bs = len(targets_list)<br/>    all_targets = torch.concat(targets_list, dim = 0)<br/>    num_boxes = [targets_list[i].shape[0] for i in range(bs)]<br/>    all_preds = torch.concat([pred[i,: num_boxes[i]] for i in range(bs)],<br/>                              dim=0)<br/>    total_boxes = sum(num_boxes)<br/>    loss_sum = generalized_box_iou(all_targets, all_preds).sum()<br/>    return loss_sum/max(total_boxes, 1)</span></pre><p id="5b8e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results of this optimization are captured below.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="a80c" class="pf ob fq pc b bg pg ph l pi pj">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data     522.326ms      52.233ms<br/>      forward        1.187s     118.715ms<br/>    calc loss     254.047ms      25.405ms<br/>-------------  ------------  ------------<br/>Self CPU time total: 396.674ms<br/>Self CUDA time total: 1.871s</span></pre><p id="b6ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The concatenation optimization resulted in a 37X (!!) speed-up of the loss function. Note, however, that it did not address the overhead of the individual host-to-device copies of the sample ground-truth data. This overhead is captured in the screenshot below from TensorBoard’s <em class="nz">Trace</em> view:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/f360f4261cdc01a8d47bd17bf7273872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZQix6GCGZT4b5LVU8vdyQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The Impact of Individual Host to Device Copies of the Batch Samples as Seen in TensorBoard (by Author)</figcaption></figure><h2 id="a7b0" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Optimization Through Padding</h2><p id="737f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">A common approach to avoiding the use of dynamically shaped tensors is padding. In the following code block, we modify the collate function to pad (with zeros) the ground-truth bounding-boxes of each data sample to the maximum number of supported boxes, 256. (Note that the padding could also have been performed in the Dataset class.)</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="bfe4" class="pf ob fq pc b bg pg ph l pi pj">def collate_with_padding(batch):<br/>    images = torch.stack([b[0] for b in batch],dim=0)<br/>    padded_boxes = []<br/>    for b in batch:<br/>        p = torch.nn.functional.pad(<br/>                       b[1], (0, 0, 0, 256 - b[1].shape[0]), value = 0)<br/>        padded_boxes.append(p)<br/>    boxes = torch.stack(padded_boxes,dim=0)<br/>    return images, boxes</span></pre><p id="ae61" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Padding the samples to fixed sized tensors enables us to copy the ground truth of the batch with a single call. It also allows us to compute the loss with a single invocation of the loss function. Note, that this method requires masking the resultant loss, as shown below, so that only the valid boxes are taken into consideration.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4eca" class="pf ob fq pc b bg pg ph l pi pj">def loss_with_padding(pred, targets):<br/>    mask = (targets[...,3] &gt; 0).to(pred.dtype)<br/>    total_boxes = mask.sum()<br/>    loss = generalized_box_iou(targets, pred)<br/>    masked_loss = loss*mask<br/>    loss_sum = masked_loss.sum()<br/>    return loss_sum/torch.clamp(total_boxes, 1)</span></pre><p id="6ef4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant runtime performance is captured below:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="b356" class="pf ob fq pc b bg pg ph l pi pj">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      57.125ms       5.713ms<br/>      forward        1.315s     131.503ms<br/>    calc loss      18.438ms       1.844ms<br/>-------------  ------------  ------------<br/>Self CPU time total: 11.723ms<br/>Self CUDA time total: 1.378s</span></pre><p id="2dc9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note the nearly 10X boost in the data copy and the additional 14X boost in the loss function performance. Keep in mind that padding may increase the use of the GPU memory. In our case, this increase is less than 1%.</p><p id="3f27" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While the runtime of our loss function has improved dramatically, we note that the vast majority of the calculations that are performed in the loss functions are immediately masked away. We can’t help but wonder whether there is a way to further improve the performance by avoiding these redundant operations. In the next section, we will explore the opportunities provided by using custom CUDA kernels.</p><h1 id="ee85" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Creating a Custom CUDA Kernel</h1><p id="8141" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Many tutorials will highlight the difficulty of creating CUDA kernels and the high entrance barrier. While mastering CUDA development and tuning kernels to maximize the utilization of the GPU could indeed require years of experience as well as an intimate understanding of the GPU architecture, we strongly believe that even a novice (but ambitious) CUDA enthusiast/ML developer can succeed at — and greatly benefit from — building custom CUDA kernels. In this section we will take PyTorch’s (relatively simple) example of a C++/CUDA extension for PyTorch and enhance it with a GIOU kernel. We will do this in two stages: First we will naïvely carry over all of the GIOU logic to C++/CUDA to assess the performance impact of <em class="nz">kernel fusion. </em>Then, we will take advantage of our new-found low-level control to add conditional logic and reduce unneeded arithmetic operations.</p><p id="9229" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Developing CUDA kernels allows you to determine the core logic that is performed in each of the GPU threads and how these are distributed onto the underlying GPU <em class="nz">streaming multiprocessors</em> (SMs). Doing this in the most optimal manner requires an expert understanding of the GPU architecture including the different levels of GPU memory, memory bandwidth, the on-chip acceleration engines (e.g., TensorCores), the supported number of concurrent threads per SM and how they are scheduled, and much much more. What makes things even more complicated is that these properties can vary between GPU generations and flavors. See <a class="af nc" href="https://developer.nvidia.com/blog/even-easier-introduction-cuda" rel="noopener ugc nofollow" target="_blank">this blog</a> for a very basic, but very easy, introduction to CUDA.</p><h2 id="ac63" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Step 1 — Kernel Fusion</h2><p id="ea2c" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Looking back at the Trace view of our last experiment, you may notice that the forward pass of our loss calculation includes roughly thirty independent arithmetic operations which translate to launching and running an independent CUDA kernel (as can be seen by simply counting the number of <em class="nz">cudaLaunchKernel</em> events). This can negatively impact performance in a number of ways. For example:</p><ol class=""><li id="4cbd" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qd qe qf bk">Each kernel launch requires dedicated communication between the CPU and GPU — something we always try to minimize.</li><li id="7a54" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">Each kernel needs to wait for the previous kernel to be completed before running. Sometimes, this can’t be avoided, but in some cases, such as ours — where most of the operations are performed “per-pixel”, it can.</li><li id="1f4d" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">The use of many independent kernels can have implications on how the GPU memory is used.</li></ol><p id="fc4d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Optimization through <em class="nz">kernel fusion</em> attempts to reduce this overhead by combining these operations into a lower number of kernels so as to reduce the overhead of multiple kernels.</p><p id="e38d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the code block below, we define a kernel that performs our GIOU on a single bounding-box prediction-target pair. We use a 1-D grid to allocate thread blocks of size <em class="nz">256</em> each where each block corresponds to one sample in the training batch and each thread corresponds to one bounding box in the sample. Thus, each thread — uniquely identified by a combination of the <em class="nz">block </em>and <em class="nz">thread </em>IDs — receives the predictions (<em class="nz">boxes1</em>) and targets (<em class="nz">boxes2</em>) and performs the GIOU calculation on the single bounding box determined by the IDs. As before, the “validity” of the bounding box is controlled by the value of the target boxes. In particular, the GIOU is explicitly zeroed wherever the corresponding box is invalid.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4428" class="pf ob fq pc b bg pg ph l pi pj">#include &lt;torch/extension.h&gt;<br/><br/>#include &lt;cuda.h&gt;<br/>#include &lt;cuda_runtime.h&gt;<br/><br/>namespace extension_cpp {<br/><br/>__global__ void giou_kernel(const float* boxes1,<br/>                            const float* boxes2, <br/>                            float* giou, <br/>                            bool* mask) {<br/>  int idx = blockIdx.x * blockDim.x + threadIdx.x;<br/>  bool valid = boxes2[4*idx+3] != 0;<br/>  mask[idx] = valid;<br/><br/>  const float epsilon = 1e-5;<br/><br/>  const float* box1 = &amp;boxes1[idx * 4];<br/>  const float* box2 = &amp;boxes2[idx * 4];<br/><br/>  // Compute area of each box<br/>  float area1 = (box1[2] - box1[0]) * (box1[3] - box1[1]);<br/>  float area2 = (box2[2] - box2[0]) * (box2[3] - box2[1]);<br/><br/>  // Compute the intersection<br/>  float left = max(box1[0], box2[0]);<br/>  float top = max(box1[1], box2[1]);<br/>  float right = min(box1[2], box2[2]);<br/>  float bottom = min(box1[3], box2[3]);<br/><br/>  float inter_w = max(right - left, 0);<br/>  float inter_h = max(bottom - top, 0);<br/>  float inter_area = inter_w * inter_h;<br/><br/>  // Compute the union area<br/>  float union_area = area1 + area2 - inter_area;<br/><br/>  // IoU<br/>  float iou_val = inter_area / max(union_area, epsilon);<br/><br/>  // Compute the smallest enclosing box<br/>  float enclose_left = min(box1[0], box2[0]);<br/>  float enclose_top = min(box1[1], box2[1]);<br/>  float enclose_right = max(box1[2], box2[2]);<br/>  float enclose_bottom = max(box1[3], box2[3]);<br/><br/>  float enclose_w = max(enclose_right - enclose_left, 0);<br/>  float enclose_h = max(enclose_bottom - enclose_top, 0);<br/>  float enclose_area = enclose_w * enclose_h;<br/><br/>  float result = iou_val - (enclose_area-union_area)/max(enclose_area, epsilon);<br/>  // Generalized IoU<br/>  giou[idx] = result * valid;<br/>}<br/><br/>at::Tensor giou_loss_cuda(const at::Tensor&amp; a, const at::Tensor&amp; b) {<br/>  TORCH_CHECK(a.sizes() == b.sizes());<br/>  TORCH_CHECK(a.dtype() == at::kFloat);<br/>  TORCH_CHECK(b.dtype() == at::kFloat);<br/>  TORCH_INTERNAL_ASSERT(a.device().type() == at::DeviceType::CUDA);<br/>  TORCH_INTERNAL_ASSERT(b.device().type() == at::DeviceType::CUDA);<br/>  int bs = a.sizes()[0];<br/>  at::Tensor a_contig = a.contiguous();<br/>  at::Tensor b_contig = b.contiguous();<br/>  at::Tensor giou = torch::empty({a_contig.sizes()[0], a_contig.sizes()[1]},<br/>                                  a_contig.options());<br/>  at::Tensor mask = torch::empty({a_contig.sizes()[0], a_contig.sizes()[1]},<br/>                                  a_contig.options().dtype(at::kBool));<br/>  const float* a_ptr = a_contig.data_ptr&lt;float&gt;();<br/>  const float* b_ptr = b_contig.data_ptr&lt;float&gt;();<br/>  float* giou_ptr = giou.data_ptr&lt;float&gt;();<br/>  bool* mask_ptr = mask.data_ptr&lt;bool&gt;();<br/><br/>  // Launch the kernel<br/>  // The number of blocks is set according to the batch size.<br/>  // Each block has 256 threads corresponding to the number of boxes per sample<br/>  giou_kernel&lt;&lt;&lt;bs, 256&gt;&gt;&gt;(a_ptr, b_ptr, giou_ptr, mask_ptr);<br/> <br/>  at::Tensor total_boxes = torch::clamp(mask.sum(), 1);<br/>  torch::Tensor loss_sum = giou.sum();<br/>  return loss_sum/total_boxes;<br/>}<br/><br/><br/>// Registers CUDA implementations for giou_loss<br/>TORCH_LIBRARY_IMPL(extension_cpp, CUDA, m) {<br/>  m.impl("giou_loss", &amp;giou_loss_cuda);<br/>}<br/><br/>}</span></pre><p id="59a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To complete the kernel creation, we need to add the appropriate C++ and Python operator definitions (see <a class="af nc" href="https://github.com/pytorch/extension-cpp/blob/master/extension_cpp/csrc/muladd.cpp#L68" rel="noopener ugc nofollow" target="_blank">muladd.cpp</a> and <a class="af nc" href="https://github.com/pytorch/extension-cpp/blob/master/extension_cpp/ops.py#L7" rel="noopener ugc nofollow" target="_blank">ops.py</a>)</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="9481" class="pf ob fq pc b bg pg ph l pi pj">// Add the C++ definition<br/>m.def(“giou_loss(Tensor a, Tensor b) -&gt; Tensor”);</span></pre><pre class="ql pb pc pd bp pe bb bk"><span id="572f" class="pf ob fq pc b bg pg ph l pi pj"># define the Python operator<br/>def giou_loss(a: Tensor, b: Tensor) -&gt; Tensor:<br/>    return torch.ops.extension_cpp.giou_loss.default(a, b)</span></pre><p id="280f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To compile our kernel, run the installation script (<code class="cx qm qn qo pc b">pip install .</code>) from the base directory.</p><p id="a9d7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following block uses our newly defined GIOU CUDA kernel:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="5281" class="pf ob fq pc b bg pg ph l pi pj">def loss_with_kernel(pred, targets):<br/>    pred = pred.to(torch.float32)<br/>    targets = targets.to(torch.float32)<br/>    import extension_cpp<br/>    return extension_cpp.ops.giou_loss(pred, targets)</span></pre><p id="3c1d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note the explicit casting to <em class="nz">torch.float32</em>. This is a rather expensive operation that could be easily avoided by enhancing our CUDA kernel support. We leave this as an exercise to the reader :).</p><p id="7ca8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results of running our script with our custom kernel are displayed below.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="83d1" class="pf ob fq pc b bg pg ph l pi pj">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg   <br/>-------------  ------------  ------------<br/>    copy data      56.901ms       5.690ms    <br/>      forward        1.327s     132.704ms      <br/>    calc loss       6.287ms     628.743us     <br/>-------------  ------------  ------------<br/>Self CPU time total: 6.907ms<br/>Self CUDA time total: 1.380s</span></pre><p id="02c1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Despite the naïveté of our kernel (and our inexperience at CUDA), we have boosted the loss function performance by an additional ~3X over our previous experiment (628 microseconds compare to 1.8 milliseconds). As noted above, this can be improved even further without much effort.</p><h2 id="107f" class="pk ob fq bf oc pl pm pn of po pp pq oi nm pr ps pt nq pu pv pw nu px py pz qa bk">Step 2 — Conditional Execution</h2><p id="0261" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The thread-level control that CUDA provides us allows us to add a conditional statement that avoids computation on the invalid bounding boxes:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="f30c" class="pf ob fq pc b bg pg ph l pi pj">__global__ void giou_kernel(const float* boxes1,<br/>                            const float* boxes2,<br/>                            float* giou,<br/>                            bool* mask) {<br/>  int idx = blockIdx.x * blockDim.x + threadIdx.x;<br/>  bool valid = boxes2[4*idx+3] != 0;<br/>  mask[idx] = valid;<br/>  if (valid)<br/>  {<br/>    const float* box1 = &amp;boxes1[idx * 4];<br/>    const float* box2 = &amp;boxes2[idx * 4];<br/>    giou[idx] = compute_giou(box1, box2);<br/>  }<br/>  else<br/>  {<br/>    giou[idx] = 0;<br/>  }<br/>}</span></pre><p id="ec4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the case of our kernel, the impact on runtime performance is negligible. The reason for this (presumably) is that our kernel is relatively small to the point that its runtime is negligible compared to the time required to load and instantiate it. The impact of our conditional execution might become apparent for larger kernels only. (The impact, as a function of the kernel size can be assessed by making our GIOU output dependent on a <em class="nz">for </em>loop that we run for a varying number of fixed steps. This, too, we leave as an exercise :).) It is also important to take into consideration how a conditional execution flow behaves on CUDA’s <a class="af nc" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" rel="noopener ugc nofollow" target="_blank">SIMT architecture</a>, particularly, the potential performance penalty when threads belonging to the same <em class="nz">warp </em>diverge.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="c118" class="pf ob fq pc b bg pg ph l pi pj">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      57.008ms       5.701ms<br/>      forward        1.318s     131.850ms<br/>    calc loss       6.234ms     623.426us<br/>-------------  ------------  ------------<br/>Self CPU time total: 7.139ms<br/>Self CUDA time total: 1.371s</span></pre><h1 id="fd93" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results and Next Steps</h1><p id="5637" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We summarize the results of our experiments in the table below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/705347a88876ca779d62ab6e9af3704e.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*fR77ZQK_GId8-IRN9hSvoQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Summary of Average of Loss Runtimes (by Author)</figcaption></figure><p id="7814" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, our work is not done. Admittedly, we have taken some shortcuts in the example we have shared:</p><ol class=""><li id="9f54" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qd qe qf bk">In order to use our custom kernel for training, we would need to implement the backward pass. Typically, this can be a bit more complicated than the forward pass.</li><li id="426a" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">We have fixed both the tensor types (to float32) and tensor shapes (to 256 boxes per sample). Ideally, a more robust solution is desired that supports varying input types and shapes.</li><li id="fe3c" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">We limited our experiments to a single GPU type. In reality, we would want our implementation to support (and be tested on) multiple GPUs.</li><li id="cbb0" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">We have completely neglected opportunities for kernel optimization — some of which may require greater CUDA expertise than we have demonstrated here.</li></ol><h1 id="9006" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Summary</h1><p id="d879" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">In this post we demonstrated the potential of the use of a custom CUDA kernel on the runtime performance of AI/ML applications. We attempted, in particular, to utilize the low-level control enabled by CUDA to introduce a conditional flow to limit the number of redundant arithmetic operations in the case of dynamically shaped inputs. While the performance boost resulting from the fusion of multiple kernel operations was significant, we found the size of our kernel to be too small to benefit from the conditional execution flow.</p><p id="4de1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Throughout many of our posts we have emphasized the importance of having multiple tools and techniques for optimizing ML and reducing its costs. Custom kernel development is one of the most powerful techniques at our disposal. However, for many AI/ML engineers, it is also one of the most intimidating techniques. We hope that we have succeeded in convincing you that this opportunity is within reach of any ML developer and that it does not require major specialization in CUDA.</p><p id="7d5e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In recent years, new frameworks have been introduced with the goal of making custom kernel development and optimization more accessible to AI/ML developers. One of the most popular of these frameworks is <a class="af nc" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton</a>. In our <a class="af nc" href="https://chaimrand.medium.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e" rel="noopener">next post</a> we will continue our exploration of the topic of custom kernel development by assessing the capabilities and potential impact of developing Triton kernels.</p></div></div></div></div>    
</body>
</html>