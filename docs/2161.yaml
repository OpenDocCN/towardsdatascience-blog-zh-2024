- en: Approximating Stochastic Functions with Multivariate Outputs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿‘ä¼¼å…·æœ‰å¤šå…ƒè¾“å‡ºçš„éšæœºå‡½æ•°
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04](https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04](https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04)
- en: A novel method for training generative machine learning models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–°é¢–çš„ç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ–¹æ³•
- en: '[](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[![Nicolas
    Arroyo Duran](../Images/a755f8b85873b94c1714e113d4ceaa18.png)](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    [Nicolas Arroyo Duran](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[![Nicolas
    Arroyo Duran](../Images/a755f8b85873b94c1714e113d4ceaa18.png)](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    [Nicolas Arroyo Duran](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    Â·21 min readÂ·Sep 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    Â·é˜…è¯»æ—¶é•¿21åˆ†é’ŸÂ·2024å¹´9æœˆ4æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/522cfa51317ab1b574f2ce704fff8216.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522cfa51317ab1b574f2ce704fff8216.png)'
- en: Pin Movement Training â€” Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pin Movement Training â€” ä½œè€…å›¾åƒ
- en: You can reproduce the experiments in this article by cloning [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥é€šè¿‡å…‹éš† [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt) æ¥é‡ç°æœ¬æ–‡ä¸­çš„å®éªŒã€‚
- en: The previous article in this series named [*Approximating stochastic functions*](https://medium.com/p/be7d6ccf4f6)
    introduced a novel method to train generative machine learning models capable
    of approximating any stochastic function with a single output variable. From this
    point on I will refer to this method as ***Pin Movement Training*** or ***PMT***
    for short. This because of the analogy of placing pins on fabric and moving them
    that is used to illustrate it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç³»åˆ—ä¹‹å‰çš„æ–‡ç« åä¸º [*è¿‘ä¼¼éšæœºå‡½æ•°*](https://medium.com/p/be7d6ccf4f6)ï¼Œä»‹ç»äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•æ¥è®­ç»ƒç”Ÿæˆå¼æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿè¿‘ä¼¼ä»»ä½•å…·æœ‰å•ä¸€è¾“å‡ºå˜é‡çš„éšæœºå‡½æ•°ã€‚ä»ç°åœ¨å¼€å§‹ï¼Œæˆ‘å°†ç®€ç§°è¿™ç§æ–¹æ³•ä¸º***Pin
    Movement Training***ï¼Œæˆ–ç®€ç§°ä¸º***PMT***ã€‚è¿™æ˜¯å› ä¸ºå®ƒé€šè¿‡å°†å¤§å¤´é’ˆæ’å…¥å¸ƒæ–™å¹¶ç§»åŠ¨å®ƒä»¬çš„ç±»æ¯”æ¥åŠ ä»¥è¯´æ˜ã€‚
- en: The method was described for functions with any number of inputs ***X*** but
    with only a single output ***Y***. The present article will generalize ***PMT***
    for functions with any number of outputs. A summary of the method will be provided
    and should be enough to understand how it works, but if you would like a more
    in depth description you can read the previous article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•æ˜¯é’ˆå¯¹å…·æœ‰ä»»æ„è¾“å…¥æ•°é‡çš„***X***å‡½æ•°ï¼Œä½†ä»…æœ‰å•ä¸€è¾“å‡º***Y***è€Œæè¿°çš„ã€‚æœ¬æ–‡å°†å¯¹å…·æœ‰ä»»æ„è¾“å‡ºæ•°é‡çš„å‡½æ•°å¯¹***PMT***è¿›è¡Œæ¨å¹¿ã€‚å°†æä¾›è¯¥æ–¹æ³•çš„æ€»ç»“ï¼Œå¹¶ä¸”åº”è¯¥è¶³ä»¥ç†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä½†å¦‚æœæ‚¨æƒ³è¦æ›´æ·±å…¥çš„æè¿°ï¼Œå¯ä»¥é˜…è¯»ä¹‹å‰çš„æ–‡ç« ã€‚
- en: The generalized method, for reasons you will learn below, utilizes an architecture
    similar to that of autoencoders. Because of this and because the uniform sampling
    distribution may be more convenient for many applications, I believe this method
    is a valid alternative to Variational Autoencoders.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨å¹¿æ–¹æ³•ç”±äºä¸‹æ–‡æ‰€è¿°çš„åŸå› ï¼Œåˆ©ç”¨äº†ç±»ä¼¼äºè‡ªç¼–ç å™¨çš„æ¶æ„ã€‚ç”±äºè¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”ç”±äºå‡åŒ€é‡‡æ ·åˆ†å¸ƒå¯èƒ½å¯¹è®¸å¤šåº”ç”¨æ›´åŠ æ–¹ä¾¿ï¼Œæˆ‘è®¤ä¸ºæ­¤æ–¹æ³•æ˜¯å˜åˆ†è‡ªç¼–ç å™¨çš„æœ‰æ•ˆæ›¿ä»£æ–¹æ¡ˆã€‚
- en: Refresher of the original method
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸå§‹æ–¹æ³•å›é¡¾
- en: Letâ€™s say that we want to use the a neural network to approximate a stochastic
    function defined as ğ‘“(ğ‘¥) â†’ ğ‘Œ where ***x*** is an input of any number of dimensions
    in ***X*** and ***Y*** is a one dimensional random variable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼å®šä¹‰ä¸º ğ‘“(ğ‘¥) â†’ ğ‘Œ çš„éšæœºå‡½æ•°ï¼Œå…¶ä¸­***x***æ˜¯***X***ä¸­ä»»æ„ç»´åº¦çš„è¾“å…¥ï¼Œè€Œ***Y***æ˜¯ä¸€ä¸ªä¸€ç»´éšæœºå˜é‡ã€‚
- en: The first thing we will do is to introduce a secondary input ***Z*** that we
    define as a uniformly distributed random variable in a range *[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]*. This
    is necessary in order to introduce randomness to an otherwise deterministic system.
    This gives us a neural network defined by ğ‘“ğœƒ(ğ‘¥,ğ‘§âˆ¼ğ‘) â†’ ğ‘Œ where ğœƒ represents the
    network trained weights.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å¼•å…¥ä¸€ä¸ªæ¬¡çº§è¾“å…¥***Z***ï¼Œå®ƒå®šä¹‰ä¸ºä¸€ä¸ªå‡åŒ€åˆ†å¸ƒçš„éšæœºå˜é‡ï¼ŒèŒƒå›´ä¸º*[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]*ã€‚è¿™æ˜¯ä¸ºäº†ç»™åŸæœ¬ç¡®å®šçš„ç³»ç»Ÿå¼•å…¥éšæœºæ€§ã€‚è¿™å°†ç»™æˆ‘ä»¬å¸¦æ¥ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå®šä¹‰ä¸º
    ğ‘“ğœƒ(ğ‘¥,ğ‘§âˆ¼ğ‘) â†’ ğ‘Œï¼Œå…¶ä¸­ğœƒä»£è¡¨ç½‘ç»œè®­ç»ƒåçš„æƒé‡ã€‚
- en: Now letâ€™s visualize any given point ğ‘¥â€², ğ‘ .ğ‘¡. *x*â€² *âˆˆ X*. For this ***xâ€™*** we
    want to map the whole range *[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]* to *Yâ‚“*â€². That is *f(xâ€², Zâ‚˜áµ¢â‚™)* should
    be as similar as possible to *min(Yâ‚“â€²)* and *f(xâ€², Zâ‚˜â‚â‚“)* should be as similar
    as possible to *max(Yâ‚“â€²)*. Additionally the mid-points *f(xâ€², mid(Z))* and *mid(Yâ‚“â€²)*
    should be as similar as possible and of course the same goes for every other point
    in the range (see **Fig. 1).**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å¯è§†åŒ–ä»»æ„ç»™å®šç‚¹ ğ‘¥â€², ğ‘ .ğ‘¡. *x*â€² *âˆˆ X*ã€‚å¯¹äºè¿™ä¸ª***x'***ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æ•´ä¸ªèŒƒå›´*[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]*æ˜ å°„åˆ°*Yâ‚“*â€²ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ*f(xâ€²,
    Zâ‚˜áµ¢â‚™)* åº”å°½å¯èƒ½æ¥è¿‘ *min(Yâ‚“â€²)*ï¼Œè€Œ *f(xâ€², Zâ‚˜áµâ‚â‚“)* åº”å°½å¯èƒ½æ¥è¿‘ *max(Yâ‚“â€²)*ã€‚æ­¤å¤–ï¼Œä¸­ç‚¹ *f(xâ€², mid(Z))*
    å’Œ *mid(Yâ‚“â€²)* åº”å°½å¯èƒ½ç›¸ä¼¼ï¼Œå½“ç„¶ï¼ŒèŒƒå›´å†…çš„å…¶ä»–æ¯ä¸ªç‚¹ä¹Ÿåº”å¦‚æ­¤ï¼ˆå‚è§**å›¾1**ï¼‰ã€‚
- en: '![](../Images/c6a0137b571f10fe4c1d034114540bfb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a0137b571f10fe4c1d034114540bfb.png)'
- en: '**Fig. 1** Mapping ***Z*** to ***Y*** â€” Image by author'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾1** å°†***Z***æ˜ å°„åˆ°***Y*** â€” å›¾ç‰‡æ¥æºï¼šä½œè€…'
- en: In order to achieve this letâ€™s think of model ğ‘“ğœƒ as a stretchable and transparent
    fabric on which ***X*** is represented horizontally and ***Z*** is represented
    vertically. Also letâ€™s imagine a board with all the data points in the dataset
    plotted in it, in this board ***X*** is represented horizontally and ***Y*** is
    represented vertically. We then proceed to place the fabric on top of the board.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹ ğ‘“ğœƒ æƒ³è±¡ä¸ºä¸€å—å¯æ‹‰ä¼¸çš„é€æ˜å¸ƒæ–™ï¼Œå…¶ä¸­ ***X*** æ°´å¹³è¡¨ç¤ºï¼Œ***Z*** å‚ç›´è¡¨ç¤ºã€‚æˆ‘ä»¬è¿˜å¯ä»¥è®¾æƒ³ä¸€ä¸ªæ¿å­ï¼Œæ¿å­ä¸Šç»˜åˆ¶äº†æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ•°æ®ç‚¹ï¼Œåœ¨è¿™ä¸ªæ¿å­ä¸Šï¼Œ***X***
    æ°´å¹³è¡¨ç¤ºï¼Œ***Y*** å‚ç›´è¡¨ç¤ºã€‚ç„¶åæˆ‘ä»¬å°†å¸ƒæ–™æ”¾ç½®åœ¨æ¿å­ä¸Šæ–¹ã€‚
- en: For every data point we place a â€œpinâ€ on the fabric at the vertical midpoint
    of ***Z*** or *mid(Z)*. We then compare the positions of the pin and the data
    point. If the data point is higher than the pin then, without unpinning the pin
    on the fabric, we move the pin upwards a predefined distance so that it lands
    in a higher position on the board. The pin will stretch or shrink the fabric with
    this motion. If it is lower then we move the pin downwards a predefined distance.
    We add the distances moved upwards and downwards and call the sum total movement.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªæ•°æ®ç‚¹ï¼Œæˆ‘ä»¬åœ¨å¸ƒæ–™çš„å‚ç›´ä¸­ç‚¹ä½ç½®æ”¾ç½®ä¸€ä¸ªâ€œé’ˆâ€ï¼Œè¯¥ä½ç½®ä¸º***Z***æˆ–*mid(Z)*ã€‚ç„¶åæˆ‘ä»¬æ¯”è¾ƒé’ˆçš„ä½ç½®ä¸æ•°æ®ç‚¹çš„ä½ç½®ã€‚å¦‚æœæ•°æ®ç‚¹é«˜äºé’ˆï¼Œæˆ‘ä»¬åœ¨ä¸å–ä¸‹å¸ƒæ–™ä¸Šé’ˆçš„æƒ…å†µä¸‹ï¼Œå°†é’ˆå‘ä¸Šç§»åŠ¨é¢„å®šçš„è·ç¦»ï¼Œä½¿å…¶åˆ°è¾¾æ¿å­ä¸Šçš„æ›´é«˜ä½ç½®ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œé’ˆä¼šæ‹‰ä¼¸æˆ–å‹ç¼©å¸ƒæ–™ã€‚å¦‚æœæ•°æ®ç‚¹ä½äºé’ˆï¼Œåˆ™å°†é’ˆå‘ä¸‹ç§»åŠ¨é¢„å®šçš„è·ç¦»ã€‚æˆ‘ä»¬å°†å‘ä¸Šå’Œå‘ä¸‹çš„ç§»åŠ¨è·ç¦»ç›¸åŠ ï¼Œå¹¶ç§°ä¹‹ä¸ºæ€»ç§»åŠ¨è·ç¦»ã€‚
- en: After processing every data point, if the pin was not initially in the midpoint,
    the total movement will be greater in the direction of the actual midpoint. After
    repeating the process enough times the pin will reach a position close to the
    midpoint where the total movement upwards and downwards is equal, that is, the
    number of data points above it is the same as the number of data points below
    it. See **Fig. 2** for an animation of how this process stabilizes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†æ¯ä¸ªæ•°æ®ç‚¹åï¼Œå¦‚æœé’ˆæœ€åˆä¸åœ¨ä¸­ç‚¹ä½ç½®ï¼Œæ€»ç§»åŠ¨è·ç¦»ä¼šæœç€å®é™…ä¸­ç‚¹çš„æ–¹å‘æ›´å¤§ã€‚ç»è¿‡è¶³å¤Ÿå¤šæ¬¡é‡å¤è¿™ä¸ªè¿‡ç¨‹åï¼Œé’ˆä¼šè¾¾åˆ°ä¸€ä¸ªæ¥è¿‘ä¸­ç‚¹çš„ä½ç½®ï¼Œåœ¨è¿™ä¸ªä½ç½®ï¼Œå‘ä¸Šå’Œå‘ä¸‹çš„æ€»ç§»åŠ¨è·ç¦»ç›¸ç­‰ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œé’ˆä¸Šæ–¹çš„æ•°æ®ç‚¹æ•°é‡ä¸ä¸‹æ–¹çš„æ•°æ®ç‚¹æ•°é‡ç›¸åŒã€‚è¯·å‚è§**å›¾2**ï¼Œäº†è§£è¿™ä¸ªè¿‡ç¨‹å¦‚ä½•ç¨³å®šã€‚
- en: '![](../Images/93fa85f00020c63dca89927509511737.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93fa85f00020c63dca89927509511737.png)'
- en: '**Fig. 2** Moving pin towards observed points until it stabilizes in the middle
    position â€” Image by author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾2** å°†é’ˆç§»å‘è§‚å¯Ÿç‚¹ï¼Œç›´åˆ°å®ƒç¨³å®šåœ¨ä¸­é—´ä½ç½® â€” å›¾ç‰‡æ¥æºï¼šä½œè€…'
- en: Now if instead of putting the pin on the midpoint of ***Z*** we put it in a
    point ***1/3*** of the distance in range *[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]* from the lowest point
    *Zâ‚˜áµ¢â‚™.* And instead of moving it the same predetermined distance upwards and downwards,
    we move it *1.5* times the predetermined distance when going downwards and *0.75*
    times the predetermined distance when going upwards. Then this pin will reach
    a stability point (where the total movement upwards and total movement downwards
    are equal) at a place roughly above ***1/3*** of the data points.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬ä¸æ˜¯å°†pinæ”¾åœ¨***Z***çš„ä¸­ç‚¹ï¼Œè€Œæ˜¯å°†å…¶æ”¾åœ¨ä»æœ€å°ç‚¹***Zâ‚˜áµ¢â‚™***åˆ°èŒƒå›´*[Zâ‚˜áµ¢â‚™, Zâ‚˜áµâ‚“]*ä¸­1/3è·ç¦»çš„ä½ç½®ä¸Šã€‚å¹¶ä¸”ä¸æ˜¯å°†å…¶ä¸Šä¸‹ç§»åŠ¨ç›¸åŒçš„é¢„å®šè·ç¦»ï¼Œè€Œæ˜¯å°†å…¶å‘ä¸‹ç§»åŠ¨é¢„å®šè·ç¦»çš„*1.5*å€ï¼Œå‘ä¸Šç§»åŠ¨é¢„å®šè·ç¦»çš„*0.75*å€ã€‚é‚£ä¹ˆè¿™ä¸ªpinå°†åœ¨ä¸€ä¸ªç¨³å®šç‚¹ä¸Šåœç•™ï¼ˆä¸Šä¸‹ç§»åŠ¨çš„æ€»è·ç¦»ç›¸ç­‰ï¼‰ï¼Œè¯¥ç‚¹å¤§è‡´ä½äºæ•°æ®ç‚¹çš„***1/3***å¤„ã€‚
- en: This is because *distance upwards* * *higher data points* = *distance downwards*
    * *lower data points* or (0.75âˆ—2/3=1.5âˆ—1/3=0.5). See **Fig. 3** for an animation
    of how this process stabilizes for pins at *Zâ‚˜áµ¢â‚™ + 1/3* and *Zâ‚˜áµ¢â‚™ + 2/3*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸º*å‘ä¸Šç§»åŠ¨çš„è·ç¦»* * *è¾ƒé«˜çš„æ•°æ®ç‚¹* = *å‘ä¸‹ç§»åŠ¨çš„è·ç¦»* * *è¾ƒä½çš„æ•°æ®ç‚¹* æˆ–è€… (0.75âˆ—2/3=1.5âˆ—1/3=0.5)ã€‚å‚è§**å›¾
    3**ï¼Œå…¶ä¸­å±•ç¤ºäº†æ­¤è¿‡ç¨‹å¦‚ä½•ä½¿*Zâ‚˜áµ¢â‚™ + 1/3*å’Œ*Zâ‚˜áµ¢â‚™ + 2/3*çš„pinsç¨³å®šã€‚
- en: '![](../Images/a41298fa268d0806434ba71f0037e866.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a41298fa268d0806434ba71f0037e866.png)'
- en: '**Fig. 3** Moving 2 pins towards observed points until they stabilize â€” Image
    by author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 3** å°†2ä¸ªpinsç§»åŠ¨è‡³è§‚æµ‹ç‚¹ç›´åˆ°å®ƒä»¬ç¨³å®šâ€”â€”å›¾æºè‡ªä½œè€…'
- en: '***How do we achieve this movement using a neural network?*** In order to move
    the â€œpins on the fabricâ€ with a neural network, we select a value in ***Z*** (which
    we call a ***z-pin***) and do backpropagation with the target value being the
    ***z-pin*** plus/minus the predetermined distance, like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘ä»¬å¦‚ä½•é€šè¿‡ç¥ç»ç½‘ç»œå®ç°è¿™ç§ç§»åŠ¨ï¼Ÿ*** ä¸ºäº†ç”¨ç¥ç»ç½‘ç»œç§»åŠ¨â€œå¸ƒæ–™ä¸Šçš„pinsâ€ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªåœ¨***Z***ä¸­çš„å€¼ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º***z-pin***ï¼‰ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­ï¼Œå°†ç›®æ ‡å€¼è®¾å®šä¸º***z-pin***åŠ /å‡é¢„å®šçš„è·ç¦»ï¼Œæ“ä½œå¦‚ä¸‹ï¼š'
- en: '![](../Images/3c449a2686c392296c2282a93f98ec1f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c449a2686c392296c2282a93f98ec1f.png)'
- en: Leveraging this principle we can select points uniformly in ***Z*** and over
    a number of epochs we obtain the mapping we require. i.e. ğ‘“ğœƒ(ğ‘¥,ğ‘§âˆ¼ğ‘) â†’ ğ‘Œ.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨è¿™ä¸€åŸç†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨***Z***ä¸­å‡åŒ€é€‰æ‹©ç‚¹ï¼Œå¹¶é€šè¿‡è‹¥å¹²å‘¨æœŸè·å¾—æˆ‘ä»¬éœ€è¦çš„æ˜ å°„ã€‚å³ ğ‘“ğœƒ(ğ‘¥,ğ‘§âˆ¼ğ‘) â†’ ğ‘Œã€‚
- en: Notes from the original article
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¥è‡ªåŸæ–‡çš„æ³¨é‡Š
- en: In the original article the fabric stretching/shrinking analogy referred to
    *pins* that were used to reshape the model, however the model definitions and
    training method used the term ***z-samples*** to refer to the same concrete term.
    In the present article and in the future these will be referred to exclusively
    as ***z-pins***.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŸæ–‡ä¸­ï¼Œå¸ƒæ–™æ‹‰ä¼¸/æ”¶ç¼©çš„ç±»æ¯”æŒ‡çš„æ˜¯ç”¨äºé‡å¡‘æ¨¡å‹çš„*pins*ï¼Œç„¶è€Œæ¨¡å‹å®šä¹‰å’Œè®­ç»ƒæ–¹æ³•ä½¿ç”¨äº†***z-samples***è¿™ä¸€æœ¯è¯­æ¥æŒ‡ä»£åŒä¸€å…·ä½“æ¦‚å¿µã€‚åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ä»¥åŠæœªæ¥çš„è®¨è®ºä¸­ï¼Œè¿™äº›å°†è¢«ä¸“é—¨ç§°ä¸º***z-pins***ã€‚
- en: When selecting ***z-pins*** the original article always placed them evenly distributed
    in ***Z*** and also used the same positions on every data point for every epoch.
    This is not necessary though, the only requirement is that the ***z-pins*** are
    uniformly distributed in ***Z***.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©***z-pins***æ—¶ï¼ŒåŸæ–‡æ€»æ˜¯å°†å®ƒä»¬å‡åŒ€åˆ†å¸ƒåœ¨***Z***ä¸­ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå‘¨æœŸå¯¹æ¯ä¸ªæ•°æ®ç‚¹ä½¿ç”¨ç›¸åŒçš„ä½ç½®ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯å¿…è¦çš„ï¼Œå”¯ä¸€çš„è¦æ±‚æ˜¯***z-pins***åœ¨***Z***ä¸­å‡åŒ€åˆ†å¸ƒã€‚
- en: The original article would use multiple ***z-pins*** per data point. This is
    also not necessary and it is sufficient to select a single ***z-pin*** per data
    point. In the present article all the experiments will select a single ***z-pin***
    per data point per epoch.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸæ–‡æ–‡ç« ä¼šåœ¨æ¯ä¸ªæ•°æ®ç‚¹ä½¿ç”¨å¤šä¸ª***z-pins***ã€‚ä½†è¿™å¹¶éå¿…è¦ï¼Œæ¯ä¸ªæ•°æ®ç‚¹åªéœ€é€‰æ‹©ä¸€ä¸ª***z-pin***å³å¯ã€‚åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæ‰€æœ‰å®éªŒå°†åœ¨æ¯ä¸ªæ•°æ®ç‚¹æ¯ä¸ªå‘¨æœŸé€‰æ‹©ä¸€ä¸ªå•ä¸€çš„***z-pin***ã€‚
- en: Generalizing for multiple outputs
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é’ˆå¯¹å¤šä¸ªè¾“å‡ºçš„æ¨å¹¿
- en: Having revisited the original method for one output, lets move on to the changes
    necessary to work for multiple outputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‡æ–°å®¡è§†åŸå§‹æ–¹æ³•ä»¥åº”å¯¹å•ä¸€è¾“å‡ºåï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†è®¨è®ºé’ˆå¯¹å¤šä¸ªè¾“å‡ºæ‰€éœ€çš„å˜åŒ–ã€‚
- en: Redefining the Z-space
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‡æ–°å®šä¹‰Zç©ºé—´
- en: Letâ€™s define ***Z***, the sampling ground from where we select our ***z-pins***.
    In the original article ***Z*** was defined as a single dimensional range described
    simply by lower and upper bounds *[Zâ‚˜áµ¢â‚™, Zâ‚˜â‚â‚“]*. However in the generalized method
    and in order to be able to handle multidimensional outputs ***Y***, ***Z*** must
    be defined in multiple dimensions as well (note however that the number of dimensions
    in ***Z*** and ***Y*** need not be the same).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å®šä¹‰***Z***ï¼Œå³æˆ‘ä»¬ä»ä¸­é€‰æ‹©***z-pins***çš„é‡‡æ ·ç©ºé—´ã€‚åœ¨åŸæ–‡ä¸­ï¼Œ***Z***è¢«å®šä¹‰ä¸ºä¸€ä¸ªå•ä¸€ç»´åº¦çš„èŒƒå›´ï¼Œç®€å•åœ°é€šè¿‡ä¸‹é™å’Œä¸Šé™æè¿°ä¸º*[Zâ‚˜áµ¢â‚™,
    Zâ‚˜áµâ‚“]*ã€‚ç„¶è€Œï¼Œåœ¨å¹¿ä¹‰æ–¹æ³•ä¸­ï¼Œä¸ºäº†èƒ½å¤Ÿå¤„ç†å¤šç»´è¾“å‡º***Y***ï¼Œ***Z***å¿…é¡»ä¹Ÿåœ¨å¤šä¸ªç»´åº¦ä¸­è¿›è¡Œå®šä¹‰ï¼ˆä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ***Z***å’Œ***Y***çš„ç»´åº¦æ•°é‡ä¸å¿…ç›¸åŒï¼‰ã€‚
- en: In theory it could be any bounded n-dimensional space, but because it makes
    calculating the scalars easier as youâ€™ll see later, I chose to use a [*hyper-sphere*](https://en.wikipedia.org/wiki/N-sphere)
    that can be defined by an origin ***O***, a radius ***R*** and a dimensionality
    ***N*** (see **Fig. 4)**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºä¸Šï¼Œå®ƒå¯ä»¥æ˜¯ä»»ä½•æœ‰ç•Œçš„ n ç»´ç©ºé—´ï¼Œä½†å› ä¸ºåç»­è®¡ç®—æ ‡é‡æ›´ä¸ºç®€ä¾¿ï¼Œæ­£å¦‚ä½ å°†çœ‹åˆ°çš„é‚£æ ·ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ä¸€ä¸ªå¯ä»¥é€šè¿‡åŸç‚¹ ***O***ã€åŠå¾„ ***R***
    å’Œç»´åº¦ ***N*** å®šä¹‰çš„ [*è¶…çƒé¢*](https://en.wikipedia.org/wiki/N-sphere)ï¼ˆè§ **å›¾ 4**ï¼‰ã€‚
- en: '![](../Images/087f11175461fb67f3a8c3eb86c70e94.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/087f11175461fb67f3a8c3eb86c70e94.png)'
- en: '**Fig. 4** 3-dimensional hypersphere ***Z-space*** â€” Image by author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 4** ä¸‰ç»´è¶…çƒé¢ ***Z-ç©ºé—´*** â€” ä½œè€…å›¾ç‰‡'
- en: Now letâ€™s define a few concepts related to ***Z*** that will be needed to move
    ahead.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€äº›ä¸ ***Z*** ç›¸å…³çš„æ¦‚å¿µï¼Œè¿™äº›æ¦‚å¿µå°†åœ¨æ¥ä¸‹æ¥çš„è®¨è®ºä¸­æ´¾ä¸Šç”¨åœºã€‚
- en: '***z-pins***: These are uniformly sampled points in ***Z***. They can be defined
    as an ***N***-dimensional vector like this: *zâ‚šáµ¢â‚™ = (zâ‚€, zâ‚, â€¦, zâ‚™)* where *zâ‚€,
    zâ‚, â€¦* are coordinates in ***Z***.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-é’ˆ***ï¼šè¿™äº›æ˜¯***Z***ä¸­çš„å‡åŒ€é‡‡æ ·ç‚¹ã€‚å®ƒä»¬å¯ä»¥å®šä¹‰ä¸ºä¸€ä¸ª***N***ç»´å‘é‡ï¼Œå½¢å¼å¦‚ä¸‹ï¼š*zâ‚šáµ¢â‚™ = (zâ‚€, zâ‚, â€¦, zâ‚™)*ï¼Œå…¶ä¸­
    *zâ‚€, zâ‚, â€¦* æ˜¯***Z***ä¸­çš„åæ ‡ã€‚'
- en: '***z-dirs***: A ***z-dir*** is a direction on ***Z*** that can be defined as
    a unit vector based at origin ***O*** like this: *z-dir = O + (Å¾â‚€, Å¾â‚, â€¦, Å¾â‚™)*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-æ–¹å‘***ï¼šä¸€ä¸ª ***z-æ–¹å‘*** æ˜¯ä¸€ä¸ªä½äºåŸç‚¹ ***O*** çš„å•ä½å‘é‡ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š*z-dir = O + (Å¾â‚€, Å¾â‚, â€¦,
    Å¾â‚™)*'
- en: '***z-lines***: A ***z-line*** is a line in ***Z*** such that it runs between
    any two points in ***Z***. We will define it as a line with a ***z-pin*** origin
    and a ***z-dir*** including all points in it that are inside of ***Z*** like this:
    *zâ‚—áµ¢â‚™â‚‘ = zâ‚šáµ¢â‚™ + z-dir s.t.âˆ€z âˆˆ zâ‚—áµ¢â‚™â‚‘ : z âˆˆ Z*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-çº¿***ï¼šä¸€ä¸ª ***z-çº¿*** æ˜¯ä¸€ä¸ªä½äº ***Z*** ä¸­çš„ç›´çº¿ï¼Œè¿æ¥ ***Z*** ä¸­çš„ä»»æ„ä¸¤ä¸ªç‚¹ã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰ä¸ºä¸€ä¸ªä»¥ ***z-é’ˆ***
    ä¸ºèµ·ç‚¹ï¼Œå¹¶å…·æœ‰ ***z-æ–¹å‘*** çš„ç›´çº¿ï¼ŒåŒ…å«æ‰€æœ‰ä½äº ***Z*** ä¸­çš„ç‚¹ï¼Œå½¢å¼å¦‚ä¸‹ï¼š*zâ‚—áµ¢â‚™â‚‘ = zâ‚šáµ¢â‚™ + z-dir ä½¿å¾— âˆ€z âˆˆ zâ‚—áµ¢â‚™â‚‘
    : z âˆˆ Z*'
- en: The model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: Moving into a multidimensional ***Z*** introduces an important challenge. In
    the case of one-dimensional ***Z*** and ğ‘Œ spaces, it was very simple to tell whether
    the selected ***z-pin*** projection i.e. ğ‘“ğœƒ*(x, zâ‚šáµ¢â‚™)* was greater or smaller
    than the observed data point in order to decide which direction to move it to.
    In one dimension â€œgreater thanâ€ in ğ‘Œ could simply be translated to â€œgreater thanâ€
    in ***Z*** and the ***z-pin*** could simply be moved up. This because we were
    mapping a line to another line.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›å…¥å¤šç»´ ***Z*** ç©ºé—´å¼•å…¥äº†ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ã€‚åœ¨ä¸€ç»´çš„ ***Z*** å’Œ ğ‘Œ ç©ºé—´ä¸­ï¼Œåˆ¤æ–­æ‰€é€‰çš„ ***z-é’ˆ*** æŠ•å½±ï¼Œå³ ğ‘“ğœƒ*(x, zâ‚šáµ¢â‚™)*
    æ˜¯å¦å¤§äºæˆ–å°äºè§‚æµ‹åˆ°çš„æ•°æ®ç‚¹ï¼Œä»¥å†³å®šå°†å…¶ç§»åŠ¨åˆ°å“ªä¸ªæ–¹å‘ï¼Œå˜å¾—éå¸¸ç®€å•ã€‚åœ¨ä¸€ç»´ä¸­ï¼Œâ€œå¤§äºâ€å¯ä»¥ç®€å•åœ°è½¬æ¢ä¸º***Z***ä¸­çš„â€œå¤§äºâ€ï¼Œè€Œ ***z-é’ˆ***
    å¯ä»¥ç®€å•åœ°å‘ä¸Šç§»åŠ¨ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åªæ˜¯å°†ä¸€æ¡çº¿æ˜ å°„åˆ°å¦ä¸€æ¡çº¿ã€‚
- en: 'But with multidimensional ğ‘Œ and ***Z*** it is not possible to assume that the
    spaces will have the same shape or even the same number of dimensions which means
    that in order to decide the direction where to move a ***z-pin*** based on its
    relation to a data point, it is necessary to map that data point from ğ‘Œ to ***Z***.
    This means that in addition to train function ğ‘“ğœƒ to generate values in ğ‘Œ, weâ€™ll
    also need to train an inverse function ğ‘“ğœƒâ»Â¹ to map data points to ***Z***. This
    fact changes our model architecture to something like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯åœ¨å¤šç»´ ğ‘Œ å’Œ ***Z*** çš„æƒ…å†µä¸‹ï¼Œæ— æ³•å‡è®¾è¿™ä¸¤ä¸ªç©ºé—´å…·æœ‰ç›¸åŒçš„å½¢çŠ¶æˆ–ç›¸åŒçš„ç»´åº¦æ•°é‡ï¼Œè¿™æ„å‘³ç€ä¸ºäº†æ ¹æ®æ•°æ®ç‚¹ä¸***z-é’ˆ***çš„å…³ç³»å†³å®šç§»åŠ¨æ–¹å‘ï¼Œæœ‰å¿…è¦å°†æ•°æ®ç‚¹ä»
    ğ‘Œ æ˜ å°„åˆ° ***Z***ã€‚è¿™æ„å‘³ç€é™¤äº†è®­ç»ƒå‡½æ•° ğ‘“ğœƒ æ¥ç”Ÿæˆ ğ‘Œ ä¸­çš„å€¼å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®­ç»ƒä¸€ä¸ªé€†å‡½æ•° ğ‘“ğœƒâ»Â¹ æ¥å°†æ•°æ®ç‚¹æ˜ å°„åˆ° ***Z***ã€‚è¿™ä¸€äº‹å®ä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹æ¶æ„å‘ç”Ÿäº†å˜åŒ–ï¼Œå˜æˆäº†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/ceb7da9d5dcef27e3f5586ee548e47e1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceb7da9d5dcef27e3f5586ee548e47e1.png)'
- en: '**Fig. 5** Model architecture â€” Image by author'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5** æ¨¡å‹æ¶æ„ â€” ä½œè€…å›¾ç‰‡'
- en: The left side of the model allows us to map points in ğ‘Œ to ***Z***. The right
    side of the model allows us to generate random samples in ğ‘Œ by sampling points
    in ***Z***.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„å·¦ä¾§å…è®¸æˆ‘ä»¬å°† ğ‘Œ ä¸­çš„ç‚¹æ˜ å°„åˆ° ***Z***ã€‚æ¨¡å‹çš„å³ä¾§å…è®¸æˆ‘ä»¬é€šè¿‡åœ¨ ***Z*** ä¸­é‡‡æ ·ç‚¹æ¥ç”Ÿæˆ ğ‘Œ ä¸­çš„éšæœºæ ·æœ¬ã€‚
- en: You may have noticed that this architecture is similar to that of plain autoencoders
    and indeed it is. This has the added benefit of making the method useful for learning
    latent representations that are bounded and evenly distributed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½æ³¨æ„åˆ°ï¼Œè¿™ä¸ªæ¶æ„ä¸æ™®é€šè‡ªç¼–ç å™¨çš„æ¶æ„ç›¸ä¼¼ï¼Œçš„ç¡®å¦‚æ­¤ã€‚è¿™ä¸€ä¼˜åŠ¿åœ¨äºï¼Œå®ƒä½¿å¾—è¯¥æ–¹æ³•å¯¹äºå­¦ä¹ æœ‰ç•Œä¸”å‡åŒ€åˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤ºéå¸¸æœ‰ç”¨ã€‚
- en: The Method
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹æ³•
- en: Having defined all the concepts we need we can proceed to discuss how pin movement
    works in multiple dimensions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰äº†æˆ‘ä»¬éœ€è¦çš„æ‰€æœ‰æ¦‚å¿µåï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¨è®ºå¦‚ä½•åœ¨å¤šç»´ç©ºé—´ä¸­è¿›è¡Œé’ˆçš„ç§»åŠ¨ã€‚
- en: Mapping data points to Z
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°†æ•°æ®ç‚¹æ˜ å°„åˆ° Z
- en: The first step is to use the inverse function ğ‘“ğœƒâ»Â¹ (or encoder going with autoencoder
    terminology) and map all data points in the batch from ğ‘Œ space to ***Z***. We
    will call the original data points *y-data* and the mapped data points *z-data*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯ä½¿ç”¨é€†å‡½æ•°ğ‘“ğœƒâ»Â¹ï¼ˆæˆ–ä½¿ç”¨è‡ªç¼–ç å™¨æœ¯è¯­ä¸­çš„ç¼–ç å™¨ï¼‰å°†æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰æ•°æ®ç‚¹ä»ğ‘Œç©ºé—´æ˜ å°„åˆ°***Z***ç©ºé—´ã€‚æˆ‘ä»¬å°†åŸå§‹æ•°æ®ç‚¹ç§°ä¸º*yæ•°æ®*ï¼Œå°†æ˜ å°„åçš„æ•°æ®ç‚¹ç§°ä¸º*zæ•°æ®*ã€‚
- en: '![](../Images/3b754a579f9e979c5ebcdf8545e2a44b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b754a579f9e979c5ebcdf8545e2a44b.png)'
- en: '**Fig. 6** Mapping data points to a 2-D ***Z-space*** â€” Image by author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾6** å°†æ•°æ®ç‚¹æ˜ å°„åˆ°2ç»´***Zç©ºé—´*** â€” å›¾åƒæ¥æºäºä½œè€…'
- en: Selecting the z-pins
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€‰æ‹©z-é’ˆ
- en: Next we must select some ***z-pins***. In order to do so, we start by selecting
    uniformly sampled ***z-dirs***, one for every data point. The easiest way to do
    so is by choosing random points in a hypersphere surface with the same dimensionality
    as ***Z***. Then we use the selected ***z-dirs*** and translate them to have the
    data points mapped in the previous step *z-data* as origins. This gives us some
    ***z-lines*** as you can see in ***Fig. 7***.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¿…é¡»é€‰æ‹©ä¸€äº›***z-é’ˆ***ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆé€‰æ‹©å‡åŒ€é‡‡æ ·çš„***z-æ–¹å‘***ï¼Œæ¯ä¸ªæ•°æ®ç‚¹é€‰æ‹©ä¸€ä¸ªã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯é€‰æ‹©ä¸€ä¸ªè¶…çƒé¢ä¸Šçš„éšæœºç‚¹ï¼Œå…¶ç»´åº¦ä¸***Z***ç›¸åŒã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨é€‰å®šçš„***z-æ–¹å‘***ï¼Œå¹¶å°†å®ƒä»¬å¹³ç§»ï¼Œä½¿å¾—å‰ä¸€æ­¥ä¸­æ˜ å°„çš„*zæ•°æ®*ä½œä¸ºåŸç‚¹ã€‚è¿™å°±å¾—åˆ°äº†å¦‚***å›¾7***æ‰€ç¤ºçš„ä¸€äº›***zçº¿***ã€‚
- en: '![](../Images/c2ef2cd3fb679b035576766185fe9707.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ef2cd3fb679b035576766185fe9707.png)'
- en: '**Fig. 7** Selecting random ***z-lines*** *in a* 2-D ***Z-space***â€” Image by
    author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾7** åœ¨2ç»´***Zç©ºé—´***ä¸­é€‰æ‹©éšæœº***zçº¿*** â€” å›¾åƒæ¥æºäºä½œè€…'
- en: Once we have our ***z-lines*** we proceed to randomly select points in these
    lines, these will be our ***z-pins***. ***Fig. 8*** shows how this can look like.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„***zçº¿***ï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥åœ¨è¿™äº›çº¿ä¸­éšæœºé€‰æ‹©ç‚¹ï¼Œè¿™äº›ç‚¹å°±æ˜¯æˆ‘ä»¬çš„***z-é’ˆ***ã€‚***å›¾8***å±•ç¤ºäº†è¿™ç§æƒ…å†µçš„ç¤ºæ„å›¾ã€‚
- en: '![](../Images/b711e7ef5c92a990ef4997613d8c8b57.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b711e7ef5c92a990ef4997613d8c8b57.png)'
- en: '**Fig. 8** Selecting random ***z-pins*** in a 2-D ***Z-space***â€” Image by author'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾8** åœ¨2ç»´***Zç©ºé—´***ä¸­é€‰æ‹©éšæœº***z-é’ˆ*** â€” å›¾åƒæ¥æºäºä½œè€…'
- en: It is necessary for the method to work that for any given ***z-line*** in ***Z***,
    every mapped data point *z-data* in it has an equal probability of occurring,
    otherwise the equations on [Calculating the movement scalars](#calculating-the-movement-scalars)
    would not hold up.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ–¹æ³•æœ‰æ•ˆï¼Œå¯¹äºä»»ä½•ç»™å®šçš„***zçº¿***ï¼Œå®ƒåœ¨***Z***ä¸­çš„æ¯ä¸ªæ˜ å°„æ•°æ®ç‚¹*zæ•°æ®*å‡ºç°çš„æ¦‚ç‡åº”è¯¥æ˜¯ç›¸ç­‰çš„ï¼Œå¦åˆ™åœ¨[è®¡ç®—è¿åŠ¨æ ‡é‡](#calculating-the-movement-scalars)ä¸­çš„æ–¹ç¨‹å°†æ— æ³•æˆç«‹ã€‚
- en: Given a 2-dimensional ***Z*** and for any given ***z-line*** in it, lets picture
    it as as a line with a minimal width ğœ– in a way that it seems like a long rectangle,
    similar to the ***z-lines*** in ***Fig. 8***. The probability of any given ğ‘§ existing
    in it is of the area of this â€œthinâ€ ***z-line*** over the area of ***Z***.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ª2ç»´çš„***Z***ç©ºé—´ï¼Œå¹¶ä¸”å¯¹äºå…¶ä¸­çš„ä»»ä½•ä¸€ä¸ª***zçº¿***ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªæœ€å°å®½åº¦ä¸ºğœ–çš„çº¿æ®µï¼Œä½¿å…¶çœ‹èµ·æ¥åƒä¸€ä¸ªé•¿çŸ©å½¢ï¼Œç±»ä¼¼äº***å›¾8***ä¸­çš„***zçº¿***ã€‚ä»»æ„ç»™å®šçš„ğ‘§å‡ºç°åœ¨å…¶ä¸­çš„æ¦‚ç‡å°±æ˜¯è¿™ä¸ªâ€œè–„â€***zçº¿***çš„é¢ç§¯ä¸***Z***é¢ç§¯ä¹‹æ¯”ã€‚
- en: '![](../Images/cb28153696cf9806dab5a4b0cbe779e4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb28153696cf9806dab5a4b0cbe779e4.png)'
- en: Since this â€œthinâ€ ***z-line*** is rectangular, any given segment ğ‘  of minimal
    length ğ›¿ across itâ€™s length has an equal area and therefore any given ğ‘§ has an
    equal probability of being in the segment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™ä¸ªâ€œè–„â€***zçº¿***æ˜¯çŸ©å½¢çš„ï¼Œæ‰€ä»¥å…¶ä»»æ„ä¸€ä¸ªæœ€å°é•¿åº¦ä¸ºğ›¿çš„çº¿æ®µğ‘ åœ¨å…¶é•¿åº¦ä¸Šå…·æœ‰ç›¸åŒçš„é¢ç§¯ï¼Œå› æ­¤ä»»æ„ä¸€ä¸ªğ‘§å‡ºç°åœ¨è¯¥çº¿æ®µçš„æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ã€‚
- en: '![](../Images/77b4ea01279ddaf80a6286a6ada82f5c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b4ea01279ddaf80a6286a6ada82f5c.png)'
- en: Also the probability of any given ğ‘§â€‹ inside this â€œthinâ€ ***z-line*** of selecting
    the ***z-dir*** of this â€œthinâ€ ***z-line*** is constant given that the ***z-dirs***
    are selected using a uniform distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ ·ï¼Œä»»æ„ç»™å®šçš„ğ‘§â€‹å‡ºç°åœ¨è¿™ä¸ªâ€œè–„â€***zçº¿***ä¸­çš„æ¦‚ç‡ï¼Œé€‰æ‹©è¯¥â€œè–„â€***zçº¿***çš„***z-æ–¹å‘***çš„æ¦‚ç‡æ˜¯æ’å®šçš„ï¼Œå› ä¸º***z-æ–¹å‘***æ˜¯é€šè¿‡å‡åŒ€åˆ†å¸ƒé€‰æ‹©çš„ã€‚
- en: '![](../Images/9ccc900b136d25b13b06acbba1d3b74f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ccc900b136d25b13b06acbba1d3b74f.png)'
- en: Taking equations (2) and (3) we get that the probability of any ğ‘§ being on any
    segment of a given ***z-line*** and selecting the same ***z-dir***, and that is
    the same for every segment which satisfies the requirement above.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æ–¹ç¨‹ï¼ˆ2ï¼‰å’Œï¼ˆ3ï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°ä»»æ„ä¸€ä¸ªğ‘§å‡ºç°åœ¨ç»™å®šçš„***zçº¿***çš„ä»»æ„çº¿æ®µä¸Šçš„æ¦‚ç‡ï¼Œå¹¶é€‰æ‹©ç›¸åŒçš„***z-æ–¹å‘***ï¼Œè€Œä¸”å¯¹äºæ»¡è¶³ä¸Šè¿°è¦æ±‚çš„æ¯ä¸ªçº¿æ®µï¼Œè¿™ä¸ªæ¦‚ç‡éƒ½æ˜¯ç›¸åŒçš„ã€‚
- en: '![](../Images/5e48f3cc06d88db83495d324f4e048fe.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e48f3cc06d88db83495d324f4e048fe.png)'
- en: The probability is independent of the position of ğ‘§ in ***z-line*** so the distribution
    in any ***z-line*** is uniform.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¦‚ç‡ä¸ğ‘§åœ¨***zçº¿***ä¸­çš„ä½ç½®æ— å…³ï¼Œå› æ­¤åœ¨ä»»ä½•***zçº¿***ä¸­çš„åˆ†å¸ƒéƒ½æ˜¯å‡åŒ€çš„ã€‚
- en: Calculating the target values
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¡ç®—ç›®æ ‡å€¼
- en: After selecting the ***z-pins*** we can proceed to calculate the target values
    (or ***z-targets***) to use in our backpropagation. All we have to do for this
    is to add to every ***z-pin*** the movement constant ğ‘€ in the direction where
    the mapped data point ğ‘§-ğ‘‘ğ‘ğ‘¡ğ‘ is.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©äº†***z-pins***ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¡ç®—ç›®æ ‡å€¼ï¼ˆæˆ–***z-targets***ï¼‰ç”¨äºåå‘ä¼ æ’­ã€‚æˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯å°†è¿åŠ¨å¸¸æ•°ğ‘€åŠ åˆ°æ¯ä¸ª***z-pin***ä¸Šï¼Œæ–¹å‘æ˜¯æ˜ å°„æ•°æ®ç‚¹ğ‘§-ğ‘‘ğ‘ğ‘¡ğ‘æ‰€åœ¨çš„æ–¹å‘ã€‚
- en: '![](../Images/c0190b58e863341e169c02ff8d431e0a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0190b58e863341e169c02ff8d431e0a.png)'
- en: '***Fig. 9*** Shows how the ***z-targets*** are calculated.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾9** æ˜¾ç¤ºäº†å¦‚ä½•è®¡ç®—***z-targets***ã€‚'
- en: '![](../Images/2819cde11a256939e77fc07b132def79.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2819cde11a256939e77fc07b132def79.png)'
- en: '**Fig. 9** Calculating the ***z-targets*** *in a* 2-D ***Z-space*** â€” Image
    by author'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾9** è®¡ç®—***z-targets*** *åœ¨* 2-D ***Z-space*** ä¸­ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…'
- en: Calculating the movement scalars
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¡ç®—è¿åŠ¨æ ‡é‡
- en: The way that movement scalars are calculated is similar to the way it was done
    in the original one-dimensional method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿åŠ¨æ ‡é‡çš„è®¡ç®—æ–¹æ³•ç±»ä¼¼äºåŸå§‹ä¸€ç»´æ–¹æ³•ä¸­çš„è®¡ç®—æ–¹å¼ã€‚
- en: Letâ€™s start by picturing a ***z-line*** along with a ***z-pin*** and some mapped
    data points ğ‘§ğ‘‘ğ‘ğ‘¡ğ‘ like we see on ***Fig .10***.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆé€šè¿‡å›¾ç¤ºä¸€ä¸ª***z-line***å’Œä¸€ä¸ª***z-pin***ä»¥åŠä¸€äº›æ˜ å°„çš„æ•°æ®ç‚¹ğ‘§ğ‘‘ğ‘ğ‘¡ğ‘ï¼Œå¦‚åŒåœ¨**å›¾10**ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: '![](../Images/697ca5cd5b94d0f0d5f9480dca809fed.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/697ca5cd5b94d0f0d5f9480dca809fed.png)'
- en: '**Fig. 10** Calculating the scalars â€” Image by author'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10** è®¡ç®—æ ‡é‡ â€” å›¾ç‰‡æ¥æºï¼šä½œè€…'
- en: Letâ€™s call ***a*** the distance from the ***z-pin*** to one end of the ***z-line***
    and ***b*** the distance to the other end. And letâ€™s call the number of data points
    on the former side ***aâ€™*** and the number of data points on latter side ***bâ€™***.
    Our purpose is to make quantity ***aâ€™*** proportional to distance ***a*** and
    ***bâ€™*** proportional to ***b*** i.e. ğ‘:ğ‘::ğ‘â€²:ğ‘â€².
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾***a***ä¸º***z-pin***åˆ°***z-line***ä¸€ç«¯çš„è·ç¦»ï¼Œ***b***ä¸ºåˆ°å¦ä¸€ç«¯çš„è·ç¦»ã€‚å¹¶ä¸”è®¾å‰ä¾§çš„æ•°æ®ç‚¹æ•°é‡ä¸º***a'***ï¼Œåä¾§çš„æ•°æ®ç‚¹æ•°é‡ä¸º***b'***ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ•°é‡***a'***ä¸è·ç¦»***a***æˆæ­£æ¯”ï¼Œ***b'***ä¸***b***æˆæ­£æ¯”ï¼Œå³
    ğ‘:ğ‘::ğ‘â€²:ğ‘â€²ã€‚
- en: Next we will call ğ›¼ the scalar that we will use on the movement applied to the
    ***z-pin*** for all data points on the side of length ***a***. And we will call
    ğ›½ the scalar that we will use on the movement applied to the ***z-pin*** for all
    data points on the side of length ***b***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŠŠğ›¼ç§°ä¸ºåœ¨é•¿åº¦ä¸º***a***çš„ä¾§é¢ä¸Šåº”ç”¨äº***z-pin***çš„è¿åŠ¨æ ‡é‡ã€‚è€Œæˆ‘ä»¬å°†æŠŠğ›½ç§°ä¸ºåœ¨é•¿åº¦ä¸º***b***çš„ä¾§é¢ä¸Šåº”ç”¨äº***z-pin***çš„è¿åŠ¨æ ‡é‡ã€‚
- en: We will also call ***T*** the total movement, which is the sum of moving the
    ***z-pin*** a constant movement ***M*** towards the side of every data point multiplied
    by that sideâ€™s scalar.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å°†***T***ç§°ä¸ºæ€»è¿åŠ¨é‡ï¼Œå®ƒæ˜¯å°†***z-pin***æ²¿æ¯ä¸ªæ•°æ®ç‚¹çš„ä¾§é¢ç§»åŠ¨ä¸€ä¸ªå¸¸æ•°è¿åŠ¨é‡***M***å¹¶ä¹˜ä»¥è¯¥ä¾§çš„æ ‡é‡çš„æ€»å’Œã€‚
- en: '![](../Images/fce7b3d44a7027e1e2931b7dd97bb295.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fce7b3d44a7027e1e2931b7dd97bb295.png)'
- en: 'We want ***T*** to be 0 (i.e. stabilized) when ğ‘â€²/(ğ‘â€²+ğ‘â€²)â‰ˆğ‘/(ğ‘+ğ‘)âˆ§ğ‘â€²/(ğ‘â€²+ğ‘â€²)â‰ˆğ‘/(ğ‘+ğ‘),
    that is when the ***z-pin*** divides the intended proportion of data points to
    both sides. Substituting ***T*** with ***0*** on (5) gives us the equation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›å½“ ğ‘â€²/(ğ‘â€²+ğ‘â€²)â‰ˆğ‘/(ğ‘+ğ‘)âˆ§ğ‘â€²/(ğ‘â€²+ğ‘â€²)â‰ˆğ‘/(ğ‘+ğ‘) æ—¶ï¼Œ***T*** ä¸º 0ï¼ˆå³ç¨³å®šï¼‰ï¼Œå³å½“***z-pin***å°†æ•°æ®ç‚¹æŒ‰ç…§é¢„å®šæ¯”ä¾‹åˆ†é…åˆ°ä¸¤ä¾§æ—¶ã€‚å°†***T***æ›¿æ¢ä¸º***0***åï¼Œæ–¹ç¨‹(5)ç»™å‡ºäº†ä»¥ä¸‹æ–¹ç¨‹ï¼š
- en: '![](../Images/9f8cb33871004aa8f21e05cbc98517a0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f8cb33871004aa8f21e05cbc98517a0.png)'
- en: 'Now letâ€™s remember that not all ***z-lines*** will have the same length, since
    they are bounded by the hypersphere defined by ***Z*** the ones towards the center
    will be longer than the ones at the edges. Longer lines will represent larger
    spaces in ***Z*** (see equation (1)) so their influence in the movement should
    be proportional to their length. We want ***T*** to be linearly proportional to
    the length of the ***z-line*** which gives us the equation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è®°ä½ï¼Œå¹¶éæ‰€æœ‰çš„***z-lines***é•¿åº¦ç›¸åŒï¼Œå› ä¸ºå®ƒä»¬è¢«ç”±***Z***å®šä¹‰çš„è¶…çƒé¢æ‰€é™åˆ¶ï¼Œæœå‘ä¸­å¿ƒçš„***z-lines***å°†æ¯”è¾¹ç¼˜çš„***z-lines***é•¿ã€‚è¾ƒé•¿çš„***z-lines***å°†è¡¨ç¤º***Z***ä¸­æ›´å¤§çš„ç©ºé—´ï¼ˆå‚è§æ–¹ç¨‹(1)ï¼‰ï¼Œå› æ­¤å®ƒä»¬åœ¨è¿åŠ¨ä¸­çš„å½±å“åº”è¯¥ä¸å…¶é•¿åº¦æˆæ¯”ä¾‹ã€‚æˆ‘ä»¬å¸Œæœ›***T***ä¸***z-line***çš„é•¿åº¦æˆçº¿æ€§å…³ç³»ï¼Œè¿™ç»™æˆ‘ä»¬å¸¦æ¥äº†ä»¥ä¸‹æ–¹ç¨‹ï¼š
- en: '![](../Images/0fa148f5c1a05e18cdb3c55a57c2eda1.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa148f5c1a05e18cdb3c55a57c2eda1.png)'
- en: 'If we put together *(6)* and *(7)* we get that the scalars should have these
    values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†*(6)*å’Œ*(7)*åˆå¹¶ï¼Œæˆ‘ä»¬å¾—åˆ°æ ‡é‡åº”å…·æœ‰ä»¥ä¸‹å€¼ï¼š
- en: '![](../Images/429c0d919d626ee53fe6e76f48b57299.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/429c0d919d626ee53fe6e76f48b57299.png)'
- en: Which are a similar equations to the one on the original article.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ–¹ç¨‹ä¸åŸæ–‡ä¸­çš„æ–¹ç¨‹ç›¸ä¼¼ã€‚
- en: You may have noticed that this equations break towards the edges i.e. when either
    ***a*** or ***b*** tends to *0*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œå½“***a***æˆ–***b***è¶‹è¿‘äº*0*æ—¶ï¼Œè¿™äº›æ–¹ç¨‹ä¼šåœ¨è¾¹ç¼˜å‘ç”Ÿæ–­è£‚ã€‚
- en: In order to solve this problem a maximum scalar constant ***S*** is introduced
    to clamp the scalars. Of course when clamping the scalars we have to be careful
    to adjust the value for both sides, for example if ***a*** is very small (and
    therefore ğ›¼ is large) but the data point is on side ***b*** the scalar ğ›½ must
    be adjusted as well, otherwise equation (5) will not hold.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥äº†ä¸€ä¸ªæœ€å¤§æ ‡é‡å¸¸æ•° ***S*** æ¥é’³åˆ¶æ ‡é‡ã€‚å½“ç„¶ï¼Œåœ¨é’³åˆ¶æ ‡é‡æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒè°ƒæ•´ä¸¤ä¾§çš„å€¼ï¼Œä¾‹å¦‚ï¼Œå¦‚æœ ***a*** éå¸¸å°ï¼ˆå› æ­¤
    ğ›¼ å¾ˆå¤§ï¼‰ï¼Œä½†æ•°æ®ç‚¹ä½äº ***b*** ä¸€ä¾§ï¼Œåˆ™æ ‡é‡ ğ›½ ä¹Ÿå¿…é¡»è¿›è¡Œè°ƒæ•´ï¼Œå¦åˆ™æ–¹ç¨‹ (5) å°†æ— æ³•æˆç«‹ã€‚
- en: We start by selecting the largest of the 2 scalars ğ‘šğ‘ğ‘¥(ğ›¼,ğ›½). Then we calculate
    an adjustment value by dividing ***S*** by ğ‘šğ‘ğ‘¥(ğ›¼,ğ›½) and clamping it to 1.0 so
    that it is always a number in the range [0, 1]. We will use the adjustment value
    to prevent scalars from going over ***S***. Finally, if ***a*** is 0.0, then the
    values of ğ›¼ and ğ›½ are ***S*** and 0.0 respectively, and the other way around if
    b is 0.0\. This gives us the revised equation *(8b)*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆé€‰æ‹©ä¸¤ä¸ªæ ‡é‡ ğ‘šğ‘ğ‘¥(ğ›¼,ğ›½) ä¸­çš„æœ€å¤§å€¼ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡å°† ***S*** é™¤ä»¥ ğ‘šğ‘ğ‘¥(ğ›¼,ğ›½) æ¥è®¡ç®—ä¸€ä¸ªè°ƒæ•´å€¼ï¼Œå¹¶å°†å…¶é’³åˆ¶åˆ° 1.0ï¼Œä»¥ç¡®ä¿å…¶å§‹ç»ˆä½äº
    [0, 1] çš„èŒƒå›´å†…ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¯¥è°ƒæ•´å€¼æ¥é˜²æ­¢æ ‡é‡è¶…è¿‡ ***S***ã€‚æœ€åï¼Œå¦‚æœ ***a*** ä¸º 0.0ï¼Œåˆ™ ğ›¼ å’Œ ğ›½ çš„å€¼åˆ†åˆ«ä¸º ***S***
    å’Œ 0.0ï¼Œå¦‚æœ ***b*** ä¸º 0.0ï¼Œåˆ™åä¹‹äº¦ç„¶ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¿®æ­£åçš„æ–¹ç¨‹å¼ *(8b)*ã€‚
- en: '![](../Images/69809bf37204fef652c2bbe616131352.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69809bf37204fef652c2bbe616131352.png)'
- en: Below you can see how the plots for the scalars proportional to ***a*** or ***b***
    look like. Notice how they are clamped beyond the selected ***S***.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢ä½ å¯ä»¥çœ‹åˆ°ä¸ ***a*** æˆ– ***b*** æˆæ¯”ä¾‹çš„æ ‡é‡å›¾åƒã€‚æ³¨æ„å®ƒä»¬åœ¨è¶…è¿‡é€‰å®šçš„ ***S*** ä¹‹åæ˜¯å¦‚ä½•è¢«é’³åˆ¶çš„ã€‚
- en: '![](../Images/4b7ce280f37aacb65182e9591d4998ae.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7ce280f37aacb65182e9591d4998ae.png)'
- en: '**Fig. 11** Movement scalar clamping for ***S=5.0*** â€” Image by author'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 11** é’ˆè„šæ ‡é‡é’³åˆ¶ï¼Œ***S=5.0*** â€” å›¾ç‰‡ç”±ä½œè€…æä¾›'
- en: Having calculated both scalars we can choose the one to use by determining the
    side on which the data point resides.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—å‡ºä¸¤ä¸ªæ ‡é‡åï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç¡®å®šæ•°æ®ç‚¹æ‰€åœ¨çš„è¾¹æ¥é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ ‡é‡ã€‚
- en: '![](../Images/c0a282a46fed428c55bff5a0cb6cb9cd.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0a282a46fed428c55bff5a0cb6cb9cd.png)'
- en: Training the model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å‹
- en: Now that all the concepts involved are clear we can move on to describe the
    training algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‰€æœ‰ç›¸å…³æ¦‚å¿µéƒ½å·²æ˜ç¡®ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­æè¿°è®­ç»ƒç®—æ³•ã€‚
- en: 1\. Pretraining and selecting the Z hyperparameters
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. é¢„è®­ç»ƒå’Œé€‰æ‹© Z è¶…å‚æ•°
- en: The algorithm described makes the assumption that the models ğ‘“ğœƒâ»Â¹ and ğ‘“ğœƒ inversely
    match each other. This can lead to a slow start if we train these 2 models to
    match each other at the same time as we do pin movement. So it has been found
    beneficial to do a â€œpretrainâ€ stage on which we only train ğ‘“ğœƒâ»Â¹ and ğ‘“ğœƒ to match
    each other. This stage is essentially an ordinary autoencoder training. After
    the reconstruction error has reached a reasonably low value the algorithm can
    proceed to the main training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æè¿°çš„ç®—æ³•å‡è®¾æ¨¡å‹ ğ‘“ğœƒâ»Â¹ å’Œ ğ‘“ğœƒ å½¼æ­¤åå‘åŒ¹é…ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿›è¡Œé’ˆè„šè¿åŠ¨æ—¶åŒæ—¶è®­ç»ƒè¿™ä¸¤ä¸ªæ¨¡å‹æ¥ä½¿å®ƒä»¬ç›¸äº’åŒ¹é…ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¯åŠ¨ç¼“æ…¢ã€‚å› æ­¤ï¼Œå·²ç»å‘ç°ï¼Œè¿›è¡Œâ€œé¢„è®­ç»ƒâ€é˜¶æ®µï¼Œä½¿å¾—æˆ‘ä»¬ä»…è®­ç»ƒ
    ğ‘“ğœƒâ»Â¹ å’Œ ğ‘“ğœƒ ä»¥ä½¿å®ƒä»¬åŒ¹é…ï¼Œä¼šæ›´æœ‰åˆ©ã€‚è¿™ä¸ªé˜¶æ®µæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ™®é€šçš„è‡ªç¼–ç å™¨è®­ç»ƒã€‚åœ¨é‡æ„è¯¯å·®è¾¾åˆ°ä¸€ä¸ªåˆç†ä½çš„å€¼ä¹‹åï¼Œç®—æ³•å¯ä»¥è¿›å…¥ä¸»è®­ç»ƒé˜¶æ®µã€‚
- en: This pretrain stage has the added advantage that it makes it easier to define
    ***Z*** when it completes. In the section [Redefining the *Z-space*](#redefining-the-z-space)
    it was mentioned that ***Z*** is defined by an origin ***O*** and a radius ***R***.
    Having pretrained the model for some time, all we do is run a batch of data points
    through the inverse model to calculate a set *Z-data*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé¢„è®­ç»ƒé˜¶æ®µè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„ä¼˜åŠ¿ï¼Œå®ƒä½¿å¾—åœ¨å®Œæˆåæ›´å®¹æ˜“å®šä¹‰ ***Z***ã€‚åœ¨ç« èŠ‚ [é‡æ–°å®šä¹‰ *Z-space*](#redefining-the-z-space)
    ä¸­æåˆ°ï¼Œ***Z*** æ˜¯ç”±åŸç‚¹ ***O*** å’ŒåŠå¾„ ***R*** å®šä¹‰çš„ã€‚ç»è¿‡ä¸€æ®µæ—¶é—´çš„é¢„è®­ç»ƒåï¼Œæˆ‘ä»¬åªéœ€è¦é€šè¿‡é€†æ¨¡å‹è¿è¡Œä¸€æ‰¹æ•°æ®ç‚¹æ¥è®¡ç®—ä¸€ç»„ *Z-data*ã€‚
- en: '![](../Images/b5420483b93263466ca02e09f6693b59.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5420483b93263466ca02e09f6693b59.png)'
- en: Then we take the mean of this set and use it as the origin ***O***.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å–è¿™ä¸ªé›†åˆçš„å¹³å‡å€¼ï¼Œå¹¶å°†å…¶ç”¨ä½œåŸç‚¹***O***ã€‚
- en: '![](../Images/77a3e2ec645ba5aacbb39566fec5aeb9.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77a3e2ec645ba5aacbb39566fec5aeb9.png)'
- en: We can also use the mean distance in *Z-data* to ***O*** as ***R***, however
    it has been seen that experimenting with and tuning this value may give better
    results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ *Z-data* åˆ° ***O*** çš„å¹³å‡è·ç¦»ä½œä¸º ***R***ï¼Œä½†æ˜¯å·²è§‚å¯Ÿåˆ°ï¼Œè°ƒæ•´å’Œè°ƒè¯•è¯¥å€¼å¯èƒ½ä¼šè·å¾—æ›´å¥½çš„ç»“æœã€‚
- en: '![](../Images/41ba99d0f2451905d4efc25c5c5dc90a.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41ba99d0f2451905d4efc25c5c5dc90a.png)'
- en: This works because after the â€œpretrainâ€ stage, the model has found a region
    capable of representing the data, so defining ***Z*** in its vicinity will likely
    have a low reconstruction error.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æœ‰æ•ˆçš„ï¼Œå› ä¸ºåœ¨â€œé¢„è®­ç»ƒâ€é˜¶æ®µåï¼Œæ¨¡å‹å·²ç»æ‰¾åˆ°äº†ä¸€ä¸ªèƒ½å¤Ÿè¡¨ç¤ºæ•°æ®çš„åŒºåŸŸï¼Œå› æ­¤åœ¨å…¶é™„è¿‘å®šä¹‰ ***Z*** å¾ˆå¯èƒ½ä¼šäº§ç”Ÿè¾ƒä½çš„é‡æ„è¯¯å·®ã€‚
- en: 2\. Pin Movement
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. é’ˆè„šè¿åŠ¨
- en: To start pin movement we select a batch of data *y-data = {y-dataâ‚€, y-dataâ‚,
    â€¦, y-dataâ‚™}* from the training dataset and map it to z*-data = {z-dataâ‚€, z-dataâ‚,
    â€¦, z-dataâ‚™}* like it is explained in [Mapping data points to Z](#mapping-data-points-to-z).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹pinçš„ç§»åŠ¨ï¼Œæˆ‘ä»¬ä»è®­ç»ƒæ•°æ®é›†ä¸­é€‰æ‹©ä¸€æ‰¹æ•°æ®*y-data = {y-dataâ‚€, y-dataâ‚, â€¦, y-dataâ‚™}*å¹¶å°†å…¶æ˜ å°„åˆ°z*-data
    = {z-dataâ‚€, z-dataâ‚, â€¦, z-dataâ‚™}*ï¼Œæ­£å¦‚åœ¨[å°†æ•°æ®ç‚¹æ˜ å°„åˆ°Z](#mapping-data-points-to-z)ä¸­æ‰€è§£é‡Šçš„é‚£æ ·ã€‚
- en: The next step is to randomly select the ***z-pins*** set *{z-pinâ‚€, z-pinâ‚, â€¦,
    z-pinâ‚™}* (one for every data point in the batch) in the way described on section
    [Selecting the Z-pins](#selecting-the-z-pins).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯éšæœºé€‰æ‹©***z-pins***é›†åˆ*{z-pinâ‚€, z-pinâ‚, â€¦, z-pinâ‚™}*ï¼ˆæ¯ä¸ªæ•°æ®ç‚¹ä¸€ä¸ªï¼‰çš„æ–¹æ³•ï¼Œå¦‚[é€‰æ‹©Z-pins](#selecting-the-z-pins)éƒ¨åˆ†æ‰€è¿°ã€‚
- en: '*Note that it is possible to select multiple* **z-pins** *per data point. But
    it is not necessary and for simplicity we will use only one on the experiments.*'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼Œæ¯ä¸ªæ•°æ®ç‚¹å¯ä»¥é€‰æ‹©å¤šä¸ª* **z-pins** *ã€‚ä½†æ˜¯è¿™ä¸æ˜¯å¿…é¡»çš„ï¼Œä¸ºäº†ç®€ä¾¿èµ·è§ï¼Œæˆ‘ä»¬åœ¨å®éªŒä¸­åªä½¿ç”¨ä¸€ä¸ªã€‚*'
- en: Then we calculate the target values *z-targets = {z-targetâ‚€, z-targetâ‚, â€¦, z-targetâ‚™}*
    and scalars s = *{sâ‚€, sâ‚, â€¦, sâ‚™}* as explained on sections [Calculating the target
    values](#calculating-the-target-values) and [Calculating the movement scalars](#calculating-the-movement-scalars).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¡ç®—ç›®æ ‡å€¼*z-targets = {z-targetâ‚€, z-targetâ‚, â€¦, z-targetâ‚™}*å’Œæ ‡é‡s = *{sâ‚€, sâ‚,
    â€¦, sâ‚™}*ï¼Œå¦‚[è®¡ç®—ç›®æ ‡å€¼](#calculating-the-target-values)å’Œ[è®¡ç®—ç§»åŠ¨æ ‡é‡](#calculating-the-movement-scalars)éƒ¨åˆ†æ‰€è¿°ã€‚
- en: 'Having the ***z-targets***, we calculate the current model predictions by running
    them through ğ‘“ğœƒ, this gives us:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è·å¾—***z-targets***åï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶ä¼ é€’ç»™ğ‘“ğœƒè®¡ç®—å½“å‰æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œè¿™å°†ç»™æˆ‘ä»¬ï¼š
- en: '![](../Images/4020948aed7c0eb227fe50b4c8084a7c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4020948aed7c0eb227fe50b4c8084a7c.png)'
- en: 'Now we have everything for the first component of our loss function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¸ºæŸå¤±å‡½æ•°çš„ç¬¬ä¸€ä¸ªç»„æˆéƒ¨åˆ†å‡†å¤‡å¥½äº†æ‰€æœ‰å†…å®¹ï¼š
- en: '![](../Images/bdfb2b259c8dfae0893c785dbcd1e429.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdfb2b259c8dfae0893c785dbcd1e429.png)'
- en: Notice that we are using a Weighted Mean Absolute Error *(WMAE)* function instead
    of a Weighted Mean Squared Error *(WMSE)*. This is because the latter is designed
    to punish larger differences while we are moving all of our pins the same distance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯åŠ æƒå¹³å‡ç»å¯¹è¯¯å·®*(WMAE)*å‡½æ•°ï¼Œè€Œä¸æ˜¯åŠ æƒå¹³å‡å¹³æ–¹è¯¯å·®*(WMSE)*ã€‚è¿™æ˜¯å› ä¸ºåè€…æ—¨åœ¨æƒ©ç½šè¾ƒå¤§çš„å·®å¼‚ï¼Œè€Œæˆ‘ä»¬å°†æ‰€æœ‰çš„pinéƒ½ç§»åŠ¨ç›¸åŒçš„è·ç¦»ã€‚
- en: 3\. Reconstruction Loss
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. é‡å»ºæŸå¤±
- en: The next component to our loss function is the difference between our model
    ğ‘“ğœƒ and our inverse model ğ‘“ğœƒâ»Â¹. This is very similar to the *Reconstruction Loss*
    in both variational and ordinary autoencoders. It is necessary to pass the batch
    data points to ğ‘“ğœƒâ»Â¹, take the results and pass them on to ğ‘“ğœƒ and then run backpropagation
    using the results and the original data points.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°çš„ä¸‹ä¸€ä¸ªç»„æˆéƒ¨åˆ†æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ğ‘“ğœƒå’Œæˆ‘ä»¬çš„é€†æ¨¡å‹ğ‘“ğœƒâ»Â¹ä¹‹é—´çš„å·®å¼‚ã€‚è¿™ä¸å˜åˆ†è‡ªç¼–ç å™¨å’Œæ™®é€šè‡ªç¼–ç å™¨ä¸­çš„*é‡å»ºæŸå¤±*éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬éœ€è¦å°†æ‰¹é‡æ•°æ®ç‚¹ä¼ é€’ç»™ğ‘“ğœƒâ»Â¹ï¼Œè·å–ç»“æœåå†ä¼ é€’ç»™ğ‘“ğœƒï¼Œç„¶åä½¿ç”¨è¿™äº›ç»“æœå’ŒåŸå§‹æ•°æ®ç‚¹è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: '![](../Images/edcf87003c4b92b959bca6251690f13e.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edcf87003c4b92b959bca6251690f13e.png)'
- en: 4\. Inverse Reconstruction Loss
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. é€†é‡å»ºæŸå¤±
- en: Before we define the last component of the loss function, letâ€™s explain why
    it is necessary. Ideally at the end of the training both ğ‘“ğœƒ and ğ‘“ğœƒâ»Â¹ will be bijective,
    meaning that there will be an exactly one-to-one correspondence between the domain
    and codomain ***Z*** and ***Y***. However during training this is not guaranteed
    to be the case and it is possible that areas in ***Z*** will not be mapped to
    ***Y***.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰æŸå¤±å‡½æ•°çš„æœ€åä¸€ä¸ªç»„æˆéƒ¨åˆ†ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆè§£é‡Šä¸€ä¸‹å®ƒä¸ºä»€ä¹ˆæ˜¯å¿…è¦çš„ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œğ‘“ğœƒå’Œğ‘“ğœƒâ»Â¹éƒ½åº”è¯¥æ˜¯åŒå°„çš„ï¼Œè¿™æ„å‘³ç€***Z***å’Œ***Y***ä¹‹é—´ä¼šæœ‰ä¸¥æ ¼çš„ä¸€ä¸€å¯¹åº”å…³ç³»ã€‚ç„¶è€Œï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¹¶ä¸èƒ½ä¿è¯è¿™ä¸€ç‚¹ï¼Œå¯èƒ½ä¼šå‡ºç°***Z***ä¸­çš„æŸäº›åŒºåŸŸæœªèƒ½æ˜ å°„åˆ°***Y***ä¸­ã€‚
- en: '![](../Images/b90e7c2975af97a75288d1862d5dad76.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b90e7c2975af97a75288d1862d5dad76.png)'
- en: '**Fig. 12** Model and inverse models may not be bijective â€” Image by author'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾12** æ¨¡å‹å’Œé€†æ¨¡å‹å¯èƒ½ä¸æ˜¯åŒå°„çš„ â€” ä½œè€…æä¾›çš„å›¾ç‰‡'
- en: As you can see in ***Fig. 12*** as a result of training with component *loss-y*,
    ğ‘“ğœƒ and ğ‘“ğœƒâ»Â¹ agree with each other as far as ***Y*** goes. That is *âˆ€y âˆˆ Y,* ğ‘“ğœƒâ»Â¹(ğ‘“ğœƒ(ğ‘¦))
    â‰ˆ *y*. However not all of ***Z*** is used and some points in ***Y*** map outside
    of it. This is a problem because the assumption of moving ***z-pins*** to a position
    that will map to a point in ***Y*** that both ğ‘“ğœƒ and ğ‘“ğœƒâ»Â¹ will agree on is broken.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨***å›¾12***ä¸­çœ‹åˆ°çš„ï¼Œç»è¿‡*loss-y*ç»„æˆéƒ¨åˆ†çš„è®­ç»ƒåï¼Œğ‘“ğœƒå’Œğ‘“ğœƒâ»Â¹åœ¨***Y***ä¸Šæ˜¯ä¸€è‡´çš„ã€‚å³*âˆ€y âˆˆ Y,* ğ‘“ğœƒâ»Â¹(ğ‘“ğœƒ(ğ‘¦))
    â‰ˆ *y*ã€‚ç„¶è€Œï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„***Z***éƒ½è¢«ä½¿ç”¨ï¼Œä¸€äº›***Y***ä¸­çš„ç‚¹æ˜ å°„åˆ°äº†å®ƒä¹‹å¤–ã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºå‡è®¾å°†***z-pins***ç§»åŠ¨åˆ°ä¸€ä¸ªä½ç½®ï¼Œè¿™ä¸ªä½ç½®ä¼šæ˜ å°„åˆ°ä¸€ä¸ª***Y***ä¸­çš„ç‚¹ï¼Œè€Œğ‘“ğœƒå’Œğ‘“ğœƒâ»Â¹éƒ½èƒ½ä¸€è‡´ï¼Œè¿™ä¸ªå‡è®¾è¢«æ‰“ç ´äº†ã€‚
- en: '***Fig. 12*** shows 2 of the problems that may happen. A â€œfoldâ€ in the fabric
    happens when 2 or more points in ***Z*** map to the same point in ***Y***. An
    â€œout-of-boundsâ€ happens when a point in ***Z*** maps to a point outside of ***Y***.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '***å›¾12***å±•ç¤ºäº†å¯èƒ½å‘ç”Ÿçš„ä¸¤ä¸ªé—®é¢˜ã€‚å¸ƒæ–™ä¸­çš„â€œæŠ˜å â€å‘ç”Ÿåœ¨***Z***ä¸­çš„ä¸¤ä¸ªæˆ–æ›´å¤šç‚¹æ˜ å°„åˆ°***Y***ä¸­çš„åŒä¸€ç‚¹æ—¶ã€‚å‘ç”Ÿâ€œè¶Šç•Œâ€æ—¶ï¼Œ***Z***ä¸­çš„ä¸€ä¸ªç‚¹æ˜ å°„åˆ°***Y***ä¹‹å¤–çš„ç‚¹ã€‚'
- en: 'In order to solve this problem we add third component to the loss function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å‘æŸå¤±å‡½æ•°ä¸­æ·»åŠ äº†ç¬¬ä¸‰ä¸ªç»„ä»¶ï¼š
- en: '![](../Images/dea9a9bb44d6b75d7c6090d65e6b603c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dea9a9bb44d6b75d7c6090d65e6b603c.png)'
- en: What this does is to synchronize ğ‘“ğœƒ and ğ‘“ğœƒâ»Â¹â€‹ with regards to ***Z*** and it
    does so by selecting random points in ***Z*** instead of using the training set
    data points.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšçš„ç›®çš„æ˜¯ä½¿ğ‘“ğœƒå’Œğ‘“ğœƒâ»Â¹â€‹åœ¨***Z***æ–¹é¢ä¿æŒåŒæ­¥ï¼Œæ–¹æ³•æ˜¯é€‰æ‹©***Z***ä¸­çš„éšæœºç‚¹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨è®­ç»ƒé›†ä¸­çš„æ•°æ®ç‚¹ã€‚
- en: Notice that for both the reconstruction loss and the inverse reconstruction
    loss we simply use Mean Squared Error (MSE).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¯¹äºé‡å»ºæŸå¤±å’Œé€†é‡å»ºæŸå¤±ï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚
- en: 5\. Loss function
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. æŸå¤±å‡½æ•°
- en: 'Now that we have all the components to the loss function all thatâ€™s left is
    to define weights for each of them which we will name ğ›¾-ğ‘, ğ›¾-*y* and ğ›¾-ğ‘§. We can
    put together (10), (11) and (12) to define the loss function like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†æŸå¤±å‡½æ•°çš„æ‰€æœ‰ç»„ä»¶ï¼Œå‰©ä¸‹çš„å°±æ˜¯ä¸ºå®ƒä»¬å®šä¹‰æƒé‡ï¼Œæˆ‘ä»¬å°†è¿™äº›æƒé‡å‘½åä¸ºğ›¾-ğ‘ã€ğ›¾-*y*å’Œğ›¾-ğ‘§ã€‚æˆ‘ä»¬å¯ä»¥å°†(10)ã€(11)å’Œ(12)ç»“åˆèµ·æ¥ï¼Œåƒè¿™æ ·å®šä¹‰æŸå¤±å‡½æ•°ï¼š
- en: '![](../Images/7563dfc4116d138b5cdd5bb187b801b8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7563dfc4116d138b5cdd5bb187b801b8.png)'
- en: All thatâ€™s left after this is to run backpropagation on the loss.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„å°±æ˜¯å¯¹æŸå¤±è¿›è¡Œåå‘ä¼ æ’­ã€‚
- en: Testing the model
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ¨¡å‹
- en: In the original paper we used goal 1 and goal 2 testing which measured the density
    of data points between ***z-pins*** and compared it to the densities of the test
    dataset. However on a multi-dimensional space doing that approach is not practical
    since the spaces between ***z-pins*** scale rapidly in number.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ç›®æ ‡1å’Œç›®æ ‡2æµ‹è¯•ï¼Œæµ‹é‡äº†***z-pins***ä¹‹é—´çš„æ•°æ®ç‚¹å¯†åº¦ï¼Œå¹¶å°†å…¶ä¸æµ‹è¯•æ•°æ®é›†çš„å¯†åº¦è¿›è¡Œäº†æ¯”è¾ƒã€‚ç„¶è€Œï¼Œåœ¨å¤šç»´ç©ºé—´ä¸­ï¼Œè¿™ç§æ–¹æ³•å¹¶ä¸å®ç”¨ï¼Œå› ä¸º***z-pins***ä¹‹é—´çš„ç©ºé—´æ•°é‡ä¼šè¿…é€Ÿå¢å¤§ã€‚
- en: The original paper also used [*Earth Moverâ€™s Distance (EMD)*](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)
    as an indicator of the modelâ€™s performance. For multiple dimension *PMT* we will
    use *EMD* to measure the modelâ€™s accuracy. We will define the EMD error by comparing
    data points from the training dataset against data points generated by the *PMT*
    model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹è®ºæ–‡è¿˜ä½¿ç”¨äº†[*åœ°çƒæ¬è¿å·¥è·ç¦»ï¼ˆEMDï¼‰*](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)ä½œä¸ºæ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚å¯¹äºå¤šç»´*PMT*ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨*EMD*æ¥è¡¡é‡æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬å°†é€šè¿‡å°†è®­ç»ƒæ•°æ®é›†ä¸­çš„æ•°æ®ç‚¹ä¸*PMT*æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ç‚¹è¿›è¡Œæ¯”è¾ƒæ¥å®šä¹‰EMDè¯¯å·®ã€‚
- en: '![](../Images/6ee897945e3f09ca4893e61980b65bc5.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ee897945e3f09ca4893e61980b65bc5.png)'
- en: And in order to have an idea of what the lowest EMD error would be we will also
    calculate a base EMD by comparing data points from the training dataset against
    data points in the test dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼°è®¡æœ€ä½çš„EMDè¯¯å·®æ˜¯å¤šå°‘ï¼Œæˆ‘ä»¬è¿˜å°†é€šè¿‡å°†è®­ç»ƒæ•°æ®é›†ä¸­çš„æ•°æ®ç‚¹ä¸æµ‹è¯•æ•°æ®é›†ä¸­çš„æ•°æ®ç‚¹è¿›è¡Œæ¯”è¾ƒæ¥è®¡ç®—ä¸€ä¸ªåŸºå‡†EMDã€‚
- en: '![](../Images/6baf14ad2aee559ec18772d16672feaf.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6baf14ad2aee559ec18772d16672feaf.png)'
- en: This gives us a baseline that we can compare against E-emd to measure the accuracy
    of the models.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºå‡†ï¼Œå¯ä»¥ç”¨å®ƒæ¥ä¸E-emdè¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œè¡¡é‡æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚
- en: Comparison with Variational Autoencoders
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸å˜åˆ†è‡ªç¼–ç å™¨çš„æ¯”è¾ƒ
- en: The most similar generative model to *PMT* is Variational Autoencoders (*VAE*).
    It has an almost identical neural network architecture and acts both a generative
    model and a latent representation mapper. The biggest difference between the two
    is that the source distribution in *VAE* is unbounded (Gaussian) and the one in
    *PMT* is bounded (uniform distribution).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸*PMT*æœ€ç›¸ä¼¼çš„ç”Ÿæˆæ¨¡å‹æ˜¯å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆ*VAE*ï¼‰ã€‚å®ƒå…·æœ‰å‡ ä¹ç›¸åŒçš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå¹¶ä¸”æ—¢æ˜¯ç”Ÿæˆæ¨¡å‹åˆæ˜¯æ½œåœ¨è¡¨ç¤ºæ˜ å°„å™¨ã€‚ä¸¤è€…ä¹‹é—´çš„æœ€å¤§åŒºåˆ«åœ¨äºï¼Œ*VAE*ä¸­çš„æºåˆ†å¸ƒæ˜¯æ— ç•Œçš„ï¼ˆé«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œè€Œ*PMT*ä¸­çš„æºåˆ†å¸ƒæ˜¯æœ‰ç•Œçš„ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ã€‚
- en: The experiments show however, that for both bounded and unbounded target distributions
    *PMT* outperforms *VAE*. Furthermore, the reconstruction error in *PMT* is significantly
    lower than on *VAE*. The reason for this may be that the components of the loss
    function cooperate with each other on *PMT* as opposed to competing with each
    other in *VAE*. Also because of the fact that the target distribution is uniform,
    the spacing between data points in ***Z*** can be larger.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå®éªŒè¡¨æ˜ï¼Œæ— è®ºæ˜¯æœ‰ç•Œè¿˜æ˜¯æ— ç•Œç›®æ ‡åˆ†å¸ƒï¼Œ*PMT*éƒ½ä¼˜äº*VAE*ã€‚æ­¤å¤–ï¼Œ*PMT*ä¸­çš„é‡å»ºè¯¯å·®æ˜¾è‘—ä½äº*VAE*ã€‚å…¶åŸå› å¯èƒ½åœ¨äºï¼ŒæŸå¤±å‡½æ•°çš„å„ä¸ªç»„ä»¶åœ¨*PMT*ä¸­ç›¸äº’åä½œï¼Œè€Œåœ¨*VAE*ä¸­åˆ™æ˜¯ç›¸äº’ç«äº‰ã€‚è€Œä¸”ï¼Œç”±äºç›®æ ‡åˆ†å¸ƒæ˜¯å‡åŒ€çš„ï¼Œ***Z***ä¸­æ•°æ®ç‚¹ä¹‹é—´çš„é—´è·å¯ä»¥æ›´å¤§ã€‚
- en: Another difference is that *PMT* takes a larger number of hyperparameters, ğ‘†
    (maximum scalar), ğ›¾-ğ‘ (pin movement weight), ğ›¾-ğ‘¦ (reconstruction loss weight),
    ğ›¾-ğ‘§ (inverse reconstruction loss weight) and ğ‘€ (movement constant) compared to
    *VAE* hyperparameters which are just the *kld weight*. This may make *PMT* training
    more difficult.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåŒºåˆ«æ˜¯ï¼Œ*PMT*æœ‰æ›´å¤šçš„è¶…å‚æ•°ï¼ŒåŒ…æ‹¬ğ‘†ï¼ˆæœ€å¤§æ ‡é‡ï¼‰ã€ğ›¾-ğ‘ï¼ˆé’ˆè„šç§»åŠ¨æƒé‡ï¼‰ã€ğ›¾-ğ‘¦ï¼ˆé‡å»ºæŸå¤±æƒé‡ï¼‰ã€ğ›¾-ğ‘§ï¼ˆåå‘é‡å»ºæŸå¤±æƒé‡ï¼‰å’Œğ‘€ï¼ˆè¿åŠ¨å¸¸æ•°ï¼‰ï¼Œè€Œ*VAE*çš„è¶…å‚æ•°ä»…ä¸º*kld
    æƒé‡*ã€‚è¿™å¯èƒ½ä¼šä½¿å¾—*PMT*çš„è®­ç»ƒæ›´åŠ å›°éš¾ã€‚
- en: Finally *PMT* takes longer to train per epoch than *VAE*, this is because it
    is necessary to do a pass to calculate the ***z-targets***, and also because the
    loss function has an additional component (see equation (12)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œ*PMT*æ¯ä¸ªå‘¨æœŸçš„è®­ç»ƒæ—¶é—´æ¯”*VAE*é•¿ï¼Œè¿™å› ä¸ºéœ€è¦è¿›è¡Œä¸€æ¬¡ä¼ é€’æ¥è®¡ç®—***z-targets***ï¼Œè€Œä¸”æŸå¤±å‡½æ•°æœ‰ä¸€ä¸ªé™„åŠ ç»„ä»¶ï¼ˆè§å…¬å¼ï¼ˆ12ï¼‰ï¼‰ã€‚
- en: Experiments
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®éªŒ
- en: Now I will try out the model in several datasets. In order to make them easier
    to plot, the experiments presented will have no ***X*** inputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘å°†åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šå°è¯•è¯¥æ¨¡å‹ã€‚ä¸ºäº†æ–¹ä¾¿ç»˜åˆ¶ï¼Œä¸‹é¢çš„å®éªŒå°†ä¸åŒ…å«***X***è¾“å…¥ã€‚
- en: Due to the similarities with *VAE*, every experiment will be done using both
    *PMT* and *VAE* models for comparison. In every experiment both models will have
    identical neural network architectures.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä¸*VAE*çš„ç›¸ä¼¼æ€§ï¼Œæ¯ä¸ªå®éªŒå°†ä½¿ç”¨*PMT*å’Œ*VAE*æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚åœ¨æ¯ä¸ªå®éªŒä¸­ï¼Œä¸¤ä¸ªæ¨¡å‹å°†é‡‡ç”¨ç›¸åŒçš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚
- en: The source code and everything needed to reproduce the experiments below can
    be found in [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt)æ‰¾åˆ°æºä»£ç å’Œé‡ç°ä¸‹é¢å®éªŒæ‰€éœ€çš„ä¸€åˆ‡ã€‚
- en: Multiple blobs
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šä¸ªæ•°æ®å—
- en: The first dataset Iâ€™ll try is generated using [*make_blobs()*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    from the *sklearn* library. As it name suggests it generates a number of Gaussian
    blobs and it is a good dataset to test how *PMT* performs with unbounded datasets.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å°è¯•çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†æ˜¯ä½¿ç”¨[*make_blobs()*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)ä»*sklearn*åº“ç”Ÿæˆçš„ã€‚é¡¾åæ€ä¹‰ï¼Œå®ƒç”Ÿæˆè‹¥å¹²ä¸ªé«˜æ–¯æ•°æ®å—ï¼Œæ˜¯æµ‹è¯•*PMT*åœ¨æ— ç•Œæ•°æ®é›†ä¸Šè¡¨ç°çš„ä¸€ä¸ªè‰¯å¥½æ•°æ®é›†ã€‚
- en: '![](../Images/c08abf0004b1126002cbcb212c12133c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c08abf0004b1126002cbcb212c12133c.png)'
- en: '**Fig. 13a** Generated data â€” Image by author'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13a** ç”Ÿæˆçš„æ•°æ® â€” ä½œè€…æä¾›çš„å›¾åƒ'
- en: '![](../Images/c4ac3c2ebd1e7dbfefe33340cc6e1f75.png)![](../Images/de4f045995b16018008a7261ac9b37ea.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ac3c2ebd1e7dbfefe33340cc6e1f75.png)![](../Images/de4f045995b16018008a7261ac9b37ea.png)'
- en: '**Fig. 13b** PMT training animation /**Fig. 13c** VAE training animation â€”
    Image by author'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13b** PMTè®­ç»ƒåŠ¨ç”» /**å›¾ 13c** VAEè®­ç»ƒåŠ¨ç”» â€” ä½œè€…æä¾›çš„å›¾åƒ'
- en: '![](../Images/632a68d1f622dafcbf04a9ff90ae6266.png)![](../Images/ec6296d1dfad186d646998a0a06b1c99.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/632a68d1f622dafcbf04a9ff90ae6266.png)![](../Images/ec6296d1dfad186d646998a0a06b1c99.png)'
- en: '**Fig. 13d** EMD plot/**Fig. 13e** Reconstruction loss plot â€” Image by author'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13d** EMDè¯¯å·®å›¾/**å›¾ 13e** é‡å»ºæŸå¤±å›¾ â€” ä½œè€…æä¾›çš„å›¾åƒ'
- en: '**Fig. 13a** shows the test data generated by the *make_blobs()* function.
    **Fig. 13b** and **Fig. 13c** show animations of the *PMT* and *VAE* training
    methods respectively.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13a**å±•ç¤ºäº†ç”±*make_blobs()*å‡½æ•°ç”Ÿæˆçš„æµ‹è¯•æ•°æ®ã€‚**å›¾ 13b**å’Œ**å›¾ 13c**åˆ†åˆ«å±•ç¤ºäº†*PMT*å’Œ*VAE*è®­ç»ƒæ–¹æ³•çš„åŠ¨ç”»ã€‚'
- en: '**Fig. 13d** shows the plot of the *EMD* errors (ğ¸-ğ‘’ğ‘šğ‘‘) calculated for both
    *PMT*, *VAE* and the base value (ğµ-ğ‘’ğ‘šğ‘‘). As you can see from this *PMT*â€™s ğ¸-ğ‘’ğ‘šğ‘‘
    are closer to ğµ-ğ‘’ğ‘šğ‘‘ than those of *VAE* which means that its performance is better.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13d**å±•ç¤ºäº†è®¡ç®—çš„*EMD*è¯¯å·®ï¼ˆğ¸-ğ‘’ğ‘šğ‘‘ï¼‰å›¾ï¼Œåˆ†åˆ«ä¸º*PMT*ã€*VAE*å’ŒåŸºå‡†å€¼ï¼ˆğµ-ğ‘’ğ‘šğ‘‘ï¼‰ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œ*PMT*çš„ğ¸-ğ‘’ğ‘šğ‘‘æ¯”*VAE*çš„æ›´æ¥è¿‘ğµ-ğ‘’ğ‘šğ‘‘ï¼Œè¿™æ„å‘³ç€å…¶æ€§èƒ½æ›´å¥½ã€‚'
- en: '**Fig. 13e** shows the plot of the reconstruction error for both *PMT* and
    *VAE*. As you can see from this *PMT*â€™s reconstruction error is an order of magnitude
    lower than that of of *VAE*.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 13e**å±•ç¤ºäº†*PMT*å’Œ*VAE*çš„é‡å»ºè¯¯å·®å›¾ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œ*PMT*çš„é‡å»ºè¯¯å·®æ¯”*VAE*ä½ä¸€ä¸ªæ•°é‡çº§ã€‚'
- en: Square with another square
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¹å½¢ä¸å¦ä¸€ä¸ªæ–¹å½¢
- en: The second dataset is quite simple. We just have an outer square with a uniform
    distribution of data points inside it, with an inner square that is also filled
    with a uniform distribution but with a greater density. This will help us test
    for non-gaussian distributions with sharp details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ•°æ®é›†ç›¸å½“ç®€å•ã€‚æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå¤–éƒ¨çš„æ–¹å½¢åŒºåŸŸï¼Œé‡Œé¢å‡åŒ€åˆ†å¸ƒç€æ•°æ®ç‚¹ï¼Œå†åŠ ä¸Šä¸€ä¸ªå†…åµŒçš„æ–¹å½¢åŒºåŸŸï¼Œé‡Œé¢åŒæ ·æ˜¯å‡åŒ€åˆ†å¸ƒçš„æ•°æ®ï¼Œä½†å¯†åº¦æ›´å¤§ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬æµ‹è¯•å…·æœ‰å°–é”ç»†èŠ‚çš„éé«˜æ–¯åˆ†å¸ƒã€‚
- en: '![](../Images/1bb4f3ed920e1990972c46ee0858c720.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bb4f3ed920e1990972c46ee0858c720.png)'
- en: '**Fig. 14a** Generated data â€” Image by author'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 14a** ç”Ÿæˆçš„æ•°æ® â€” ä½œè€…æä¾›çš„å›¾åƒ'
- en: '![](../Images/a1be878f264605880859aba8217440fa.png)![](../Images/56a7800a3c07bfaaba7aef6d3d124da5.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1be878f264605880859aba8217440fa.png)![](../Images/56a7800a3c07bfaaba7aef6d3d124da5.png)'
- en: '**Fig. 14b** PMT training animation/**Fig. 14c** VAE training animation â€” Image
    by author'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 14b** PMT è®­ç»ƒåŠ¨ç”»/**å›¾ 14c** VAE è®­ç»ƒåŠ¨ç”» â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '![](../Images/77f6326015da719f593d33c7e138cabc.png)![](../Images/8ef90a759f521bbcc862d0f8b2540a54.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77f6326015da719f593d33c7e138cabc.png)![](../Images/8ef90a759f521bbcc862d0f8b2540a54.png)'
- en: '**Fig. 14d** EMD plot/**Fig. 14e** Reconstruction loss plot â€” Image by author'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 14d** EMD å›¾/**å›¾ 14e** é‡å»ºæŸå¤±å›¾ â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '**Fig. 14d** shows the *EMD* errors plot and you can see from this that *PMT*
    outperforms *VAE*.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 14d** æ˜¾ç¤ºäº† *EMD* é”™è¯¯å›¾ï¼Œä½ å¯ä»¥ä»ä¸­çœ‹åˆ° *PMT* è¶…è¶Šäº† *VAE*ã€‚'
- en: '**Fig. 14e** shows the reconstruction error values and you can see from this
    *PMT*â€™s reconstruction error is over 2 orders of magnitude lower than that of
    *VAE*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 14e** æ˜¾ç¤ºäº†é‡å»ºè¯¯å·®å€¼ï¼Œä½ å¯ä»¥çœ‹åˆ° *PMT* çš„é‡å»ºè¯¯å·®æ¯” *VAE* ä½ä¸¤ä¸ªæ•°é‡çº§ã€‚'
- en: Human behavior
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: äººç±»è¡Œä¸º
- en: The next dataset is made of human body motion sensor data acquired by performing
    several physical activities. It was extracted from [Mobile Health Human Behavior
    Analysis Dataset](https://www.kaggle.com/datasets/gaurav2022/mobile-health)Â¹.
    This one has 3 dimensions instead of 2 like the previous datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªæ•°æ®é›†ç”±äººä½“è¿åŠ¨ä¼ æ„Ÿå™¨æ•°æ®æ„æˆï¼Œè¿™äº›æ•°æ®æ˜¯é€šè¿‡è¿›è¡Œå‡ ç§ä½“è‚²æ´»åŠ¨è·å¾—çš„ã€‚å®ƒæ¥æºäº [ç§»åŠ¨å¥åº·äººä½“è¡Œä¸ºåˆ†ææ•°æ®é›†](https://www.kaggle.com/datasets/gaurav2022/mobile-health)Â¹ã€‚è¿™ä¸ªæ•°æ®é›†å…·æœ‰3ä¸ªç»´åº¦ï¼Œè€Œä¸åƒä¹‹å‰çš„æ•°æ®é›†åªæœ‰2ä¸ªç»´åº¦ã€‚
- en: '![](../Images/4834f93ea998bc644806503e7241e0ab.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4834f93ea998bc644806503e7241e0ab.png)'
- en: '**Fig. 15a** Test data â€” Image by author'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 15a** æµ‹è¯•æ•°æ® â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '![](../Images/c74d9ef38f096283e44f1335265f73af.png)![](../Images/b4b21e69eceed64faf940b9ec81f704d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c74d9ef38f096283e44f1335265f73af.png)![](../Images/b4b21e69eceed64faf940b9ec81f704d.png)'
- en: '**Fig. 15b** PMT training animation/**Fig. 15c** VAE training animation â€” Image
    by author'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 15b** PMT è®­ç»ƒåŠ¨ç”»/**å›¾ 15c** VAE è®­ç»ƒåŠ¨ç”» â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '![](../Images/57f7598cc9a77276144674295864052a.png)![](../Images/9839cdb2d118c7ae47804972cdf1c958.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f7598cc9a77276144674295864052a.png)![](../Images/9839cdb2d118c7ae47804972cdf1c958.png)'
- en: '**Fig. 15d** EMD plot/**Fig. 15e** Reconstruction loss plot â€” Image by author'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 15d** EMD å›¾/**å›¾ 15e** é‡å»ºæŸå¤±å›¾ â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '**Fig. 15d** shows the *EMD* errors plot and once again *PMT* outperforms *VAE*
    again.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 15d** æ˜¾ç¤ºäº† *EMD* é”™è¯¯å›¾ï¼Œå†æ¬¡è¯æ˜ *PMT* è¶…è¶Šäº† *VAE*ã€‚'
- en: '**Fig. 15e** shows that *PMT*â€™s reconstruction error is over an order of magnitude
    lower than that of *VAE*.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 15e** æ˜¾ç¤º *PMT* çš„é‡å»ºè¯¯å·®æ¯” *VAE* ä½ä¸€ä¸ªæ•°é‡çº§ã€‚'
- en: MNIST
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST
- en: Lastly we have the famous [MNIST Dataset](https://huggingface.co/datasets/ylecun/mnist)Â².
    As you know it contains bitmaps of numbers written by humans and the task here
    is to try to generated new data points that look like real hand drawn numbers.
    This is an interesting dataset because it has a large number of output dimensions
    (784) and a latent space of 4 dimensions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åæ˜¯è‘—åçš„ [MNIST æ•°æ®é›†](https://huggingface.co/datasets/ylecun/mnist)Â²ã€‚æ­£å¦‚ä½ æ‰€çŸ¥ï¼Œå®ƒåŒ…å«äº†äººç±»ä¹¦å†™çš„æ•°å­—çš„ä½å›¾ï¼Œè€Œä»»åŠ¡æ˜¯ç”Ÿæˆçœ‹èµ·æ¥åƒæ‰‹å†™æ•°å­—çš„æ–°çš„æ•°æ®ç‚¹ã€‚è¿™ä¸ªæ•°æ®é›†å¾ˆæœ‰è¶£ï¼Œå› ä¸ºå®ƒå…·æœ‰å¤§é‡çš„è¾“å‡ºç»´åº¦ï¼ˆ784ï¼‰å’Œ4ç»´çš„æ½œåœ¨ç©ºé—´ã€‚
- en: '![](../Images/687c7882f1b5c102c2af06b17783767c.png)![](../Images/c819342f0f0639d2a25e8085a495a36c.png)![](../Images/cfb4cc8a2b64620e1015b78452b2c3c3.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/687c7882f1b5c102c2af06b17783767c.png)![](../Images/c819342f0f0639d2a25e8085a495a36c.png)![](../Images/cfb4cc8a2b64620e1015b78452b2c3c3.png)'
- en: '**Fig. 17a** PMT original data/**Fig. 17b** PMT reconstruction/**Fig. 17c**
    PMT generated samples â€” Image by author'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 17a** PMT åŸå§‹æ•°æ®/**å›¾ 17b** PMT é‡å»º/**å›¾ 17c** PMT ç”Ÿæˆçš„æ ·æœ¬ â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '![](../Images/4c31557abcf5abe663e67956a511168a.png)![](../Images/f02053feb01df2379b898c570723060d.png)![](../Images/574541dc3ffd8220ccf516c1d30951dd.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c31557abcf5abe663e67956a511168a.png)![](../Images/f02053feb01df2379b898c570723060d.png)![](../Images/574541dc3ffd8220ccf516c1d30951dd.png)'
- en: '**Fig. 16c** VAE original data/**Fig. 16e** VAE reconstruction/**Fig. 16e**
    VAE generated samples â€” Image by author'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 16c** VAE åŸå§‹æ•°æ®/**å›¾ 16e** VAE é‡å»º/**å›¾ 16e** VAE ç”Ÿæˆçš„æ ·æœ¬ â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: '![](../Images/6654897ba08399ce6612558a8d5f6720.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6654897ba08399ce6612558a8d5f6720.png)'
- en: '**Fig. 16g** Reconstruction loss plot â€” Image by author'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 16g** é‡å»ºæŸå¤±å›¾ â€” å›¾åƒæ¥æºï¼šä½œè€…'
- en: This dataset does not have an EMD error plot since it would be very difficult
    to calculate (and not indicative) given the large number of output dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè¾“å‡ºç»´åº¦è¿‡å¤šï¼Œè®¡ç®— EMD é”™è¯¯å›¾éå¸¸å›°éš¾ï¼ˆè€Œä¸”æ²¡æœ‰æŒ‡ç¤ºæ€§æ„ä¹‰ï¼‰ï¼Œå› æ­¤è¿™ä¸ªæ•°æ®é›†æ²¡æœ‰ EMD é”™è¯¯å›¾ã€‚
- en: '**Fig. 16b** plots the reconstruction errors, once again *PMT*â€™s is lower than
    *VAE*â€™s.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 16b** ç»˜åˆ¶äº†é‡å»ºè¯¯å·®å›¾ï¼Œå†æ¬¡è¯æ˜ *PMT* çš„è¯¯å·®ä½äº *VAE* çš„è¯¯å·®ã€‚'
- en: Conclusion and whatâ€™s next
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®ºä¸æœªæ¥çš„æ–¹å‘
- en: Approximating stochastic functions with a single output is very useful for forecasting
    single value distributions, like temperatures or market values. But the ability
    to produce multiple outputs make the method apt for a large variety of use cases
    like simulation and generative tasks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨å•ä¸€è¾“å‡ºé€¼è¿‘éšæœºå‡½æ•°å¯¹äºé¢„æµ‹å•å€¼åˆ†å¸ƒï¼ˆå¦‚æ¸©åº¦æˆ–å¸‚åœºå€¼ï¼‰éå¸¸æœ‰ç”¨ã€‚ä½†äº§ç”Ÿå¤šä¸ªè¾“å‡ºçš„èƒ½åŠ›ä½¿å¾—è¯¥æ–¹æ³•é€‚ç”¨äºå¤šç§åº”ç”¨åœºæ™¯ï¼Œå¦‚æ¨¡æ‹Ÿå’Œç”Ÿæˆä»»åŠ¡ã€‚
- en: The multiple output method described in this article has proved that in the
    experiment datasets, it is capable of outperforming VAEs in both probabilistic
    likeness and reconstruction. I believe they will produce better results in a variety
    of real world use cases too.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æè¿°çš„å¤šè¾“å‡ºæ–¹æ³•å·²ç»è¯æ˜ï¼Œåœ¨å®éªŒæ•°æ®é›†ä¸­ï¼Œå®ƒèƒ½å¤Ÿåœ¨æ¦‚ç‡ç›¸ä¼¼åº¦å’Œé‡å»ºæ–¹é¢ä¼˜äº VAEã€‚æˆ‘ç›¸ä¿¡å®ƒä»¬åœ¨å„ç§ç°å®ä¸–ç•Œåº”ç”¨ä¸­ä¹Ÿä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚
- en: In the future I would like to continue testing *PMT* on higher dimensional datasets
    for generative purposes, such as [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)
    and [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). For this purpose
    it will also be necessary to experiment with deep networks and Convolutional Neural
    Networks (CNN).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: æœªæ¥ï¼Œæˆ‘å¸Œæœ›åœ¨æ›´é«˜ç»´åº¦çš„æ•°æ®é›†ä¸Šç»§ç»­æµ‹è¯•*PMT*ï¼Œä»¥è¿›è¡Œç”Ÿæˆä»»åŠ¡ï¼Œæ¯”å¦‚[æ—¶å°š MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)å’Œ[CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)ã€‚ä¸ºæ­¤ï¼Œè¿˜éœ€è¦å°è¯•æ·±åº¦ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚
- en: Feel free to reach out to me with any questions or comments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿éšæ—¶è”ç³»æˆ‘ã€‚
- en: '[1]: Mobile Health Human Behavior Analysis'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: ç§»åŠ¨å¥åº·äººç±»è¡Œä¸ºåˆ†æ'
- en: https://www.kaggle.com/datasets/gaurav2022/mobile-health
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: https://www.kaggle.com/datasets/gaurav2022/mobile-health
- en: CC0 Public Domain [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: CC0 å…¬å…±é¢†åŸŸ [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)
- en: '[2] MNIST Handwritten Database'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] MNIST æ‰‹å†™æ•°å­—æ•°æ®åº“'
- en: '[## MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris
    Burges'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MNIST æ‰‹å†™æ•°å­—æ•°æ®åº“ï¼ŒYann LeCunï¼ŒCorinna Cortes å’Œ Chris Burges'
- en: of handwritten digits Yann LeCun, Courant Institute, NYU Corinna Cortes, Google
    Labs, New York Christopher J.C. Burgesâ€¦
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰‹å†™æ•°å­— Yann LeCunï¼Œçº½çº¦å¤§å­¦ Courant ç ”ç©¶æ‰€ Corinna Cortesï¼ŒGoogle å®éªŒå®¤ï¼Œçº½çº¦ Christopher J.C.
    Burgesâ€¦
- en: yann.lecun.com](http://yann.lecun.com/exdb/mnist?source=post_page-----ffefc7099a90--------------------------------)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: yann.lecun.com](http://yann.lecun.com/exdb/mnist?source=post_page-----ffefc7099a90--------------------------------)
- en: MIT [https://choosealicense.com/licenses/mit/](https://choosealicense.com/licenses/mit/)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: MIT [https://choosealicense.com/licenses/mit/](https://choosealicense.com/licenses/mit/)
