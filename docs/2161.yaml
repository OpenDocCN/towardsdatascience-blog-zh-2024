- en: Approximating Stochastic Functions with Multivariate Outputs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 近似具有多元输出的随机函数
- en: 原文：[https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04](https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04](https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04)
- en: A novel method for training generative machine learning models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种新颖的生成式机器学习模型训练方法
- en: '[](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[![Nicolas
    Arroyo Duran](../Images/a755f8b85873b94c1714e113d4ceaa18.png)](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    [Nicolas Arroyo Duran](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[![Nicolas
    Arroyo Duran](../Images/a755f8b85873b94c1714e113d4ceaa18.png)](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    [Nicolas Arroyo Duran](https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    ·21 min read·Sep 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------)
    ·阅读时长21分钟·2024年9月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/522cfa51317ab1b574f2ce704fff8216.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/522cfa51317ab1b574f2ce704fff8216.png)'
- en: Pin Movement Training — Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pin Movement Training — 作者图像
- en: You can reproduce the experiments in this article by cloning [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过克隆 [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt) 来重现本文中的实验。
- en: The previous article in this series named [*Approximating stochastic functions*](https://medium.com/p/be7d6ccf4f6)
    introduced a novel method to train generative machine learning models capable
    of approximating any stochastic function with a single output variable. From this
    point on I will refer to this method as ***Pin Movement Training*** or ***PMT***
    for short. This because of the analogy of placing pins on fabric and moving them
    that is used to illustrate it.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列之前的文章名为 [*近似随机函数*](https://medium.com/p/be7d6ccf4f6)，介绍了一种新颖的方法来训练生成式机器学习模型，能够近似任何具有单一输出变量的随机函数。从现在开始，我将简称这种方法为***Pin
    Movement Training***，或简称为***PMT***。这是因为它通过将大头针插入布料并移动它们的类比来加以说明。
- en: The method was described for functions with any number of inputs ***X*** but
    with only a single output ***Y***. The present article will generalize ***PMT***
    for functions with any number of outputs. A summary of the method will be provided
    and should be enough to understand how it works, but if you would like a more
    in depth description you can read the previous article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是针对具有任意输入数量的***X***函数，但仅有单一输出***Y***而描述的。本文将对具有任意输出数量的函数对***PMT***进行推广。将提供该方法的总结，并且应该足以理解它是如何工作的，但如果您想要更深入的描述，可以阅读之前的文章。
- en: The generalized method, for reasons you will learn below, utilizes an architecture
    similar to that of autoencoders. Because of this and because the uniform sampling
    distribution may be more convenient for many applications, I believe this method
    is a valid alternative to Variational Autoencoders.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该推广方法由于下文所述的原因，利用了类似于自编码器的架构。由于这一点，并且由于均匀采样分布可能对许多应用更加方便，我认为此方法是变分自编码器的有效替代方案。
- en: Refresher of the original method
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始方法回顾
- en: Let’s say that we want to use the a neural network to approximate a stochastic
    function defined as 𝑓(𝑥) → 𝑌 where ***x*** is an input of any number of dimensions
    in ***X*** and ***Y*** is a one dimensional random variable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用神经网络来近似定义为 𝑓(𝑥) → 𝑌 的随机函数，其中***x***是***X***中任意维度的输入，而***Y***是一个一维随机变量。
- en: The first thing we will do is to introduce a secondary input ***Z*** that we
    define as a uniformly distributed random variable in a range *[Zₘᵢₙ, Zₘₐₓ]*. This
    is necessary in order to introduce randomness to an otherwise deterministic system.
    This gives us a neural network defined by 𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌 where 𝜃 represents the
    network trained weights.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是引入一个次级输入***Z***，它定义为一个均匀分布的随机变量，范围为*[Zₘᵢₙ, Zₘₐₓ]*。这是为了给原本确定的系统引入随机性。这将给我们带来一个神经网络，定义为
    𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌，其中𝜃代表网络训练后的权重。
- en: Now let’s visualize any given point 𝑥′, 𝑠.𝑡. *x*′ *∈ X*. For this ***x’*** we
    want to map the whole range *[Zₘᵢₙ, Zₘₐₓ]* to *Yₓ*′. That is *f(x′, Zₘᵢₙ)* should
    be as similar as possible to *min(Yₓ′)* and *f(x′, Zₘₐₓ)* should be as similar
    as possible to *max(Yₓ′)*. Additionally the mid-points *f(x′, mid(Z))* and *mid(Yₓ′)*
    should be as similar as possible and of course the same goes for every other point
    in the range (see **Fig. 1).**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们可视化任意给定点 𝑥′, 𝑠.𝑡. *x*′ *∈ X*。对于这个***x'***，我们希望将整个范围*[Zₘᵢₙ, Zₘₐₓ]*映射到*Yₓ*′。也就是说，*f(x′,
    Zₘᵢₙ)* 应尽可能接近 *min(Yₓ′)*，而 *f(x′, Zₘᵐₐₓ)* 应尽可能接近 *max(Yₓ′)*。此外，中点 *f(x′, mid(Z))*
    和 *mid(Yₓ′)* 应尽可能相似，当然，范围内的其他每个点也应如此（参见**图1**）。
- en: '![](../Images/c6a0137b571f10fe4c1d034114540bfb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a0137b571f10fe4c1d034114540bfb.png)'
- en: '**Fig. 1** Mapping ***Z*** to ***Y*** — Image by author'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1** 将***Z***映射到***Y*** — 图片来源：作者'
- en: In order to achieve this let’s think of model 𝑓𝜃 as a stretchable and transparent
    fabric on which ***X*** is represented horizontally and ***Z*** is represented
    vertically. Also let’s imagine a board with all the data points in the dataset
    plotted in it, in this board ***X*** is represented horizontally and ***Y*** is
    represented vertically. We then proceed to place the fabric on top of the board.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们可以将模型 𝑓𝜃 想象为一块可拉伸的透明布料，其中 ***X*** 水平表示，***Z*** 垂直表示。我们还可以设想一个板子，板子上绘制了数据集中的所有数据点，在这个板子上，***X***
    水平表示，***Y*** 垂直表示。然后我们将布料放置在板子上方。
- en: For every data point we place a “pin” on the fabric at the vertical midpoint
    of ***Z*** or *mid(Z)*. We then compare the positions of the pin and the data
    point. If the data point is higher than the pin then, without unpinning the pin
    on the fabric, we move the pin upwards a predefined distance so that it lands
    in a higher position on the board. The pin will stretch or shrink the fabric with
    this motion. If it is lower then we move the pin downwards a predefined distance.
    We add the distances moved upwards and downwards and call the sum total movement.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据点，我们在布料的垂直中点位置放置一个“针”，该位置为***Z***或*mid(Z)*。然后我们比较针的位置与数据点的位置。如果数据点高于针，我们在不取下布料上针的情况下，将针向上移动预定的距离，使其到达板子上的更高位置。这个过程中，针会拉伸或压缩布料。如果数据点低于针，则将针向下移动预定的距离。我们将向上和向下的移动距离相加，并称之为总移动距离。
- en: After processing every data point, if the pin was not initially in the midpoint,
    the total movement will be greater in the direction of the actual midpoint. After
    repeating the process enough times the pin will reach a position close to the
    midpoint where the total movement upwards and downwards is equal, that is, the
    number of data points above it is the same as the number of data points below
    it. See **Fig. 2** for an animation of how this process stabilizes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理每个数据点后，如果针最初不在中点位置，总移动距离会朝着实际中点的方向更大。经过足够多次重复这个过程后，针会达到一个接近中点的位置，在这个位置，向上和向下的总移动距离相等，也就是说，针上方的数据点数量与下方的数据点数量相同。请参见**图2**，了解这个过程如何稳定。
- en: '![](../Images/93fa85f00020c63dca89927509511737.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93fa85f00020c63dca89927509511737.png)'
- en: '**Fig. 2** Moving pin towards observed points until it stabilizes in the middle
    position — Image by author'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2** 将针移向观察点，直到它稳定在中间位置 — 图片来源：作者'
- en: Now if instead of putting the pin on the midpoint of ***Z*** we put it in a
    point ***1/3*** of the distance in range *[Zₘᵢₙ, Zₘₐₓ]* from the lowest point
    *Zₘᵢₙ.* And instead of moving it the same predetermined distance upwards and downwards,
    we move it *1.5* times the predetermined distance when going downwards and *0.75*
    times the predetermined distance when going upwards. Then this pin will reach
    a stability point (where the total movement upwards and total movement downwards
    are equal) at a place roughly above ***1/3*** of the data points.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们不是将pin放在***Z***的中点，而是将其放在从最小点***Zₘᵢₙ***到范围*[Zₘᵢₙ, Zₘᵐₓ]*中1/3距离的位置上。并且不是将其上下移动相同的预定距离，而是将其向下移动预定距离的*1.5*倍，向上移动预定距离的*0.75*倍。那么这个pin将在一个稳定点上停留（上下移动的总距离相等），该点大致位于数据点的***1/3***处。
- en: This is because *distance upwards* * *higher data points* = *distance downwards*
    * *lower data points* or (0.75∗2/3=1.5∗1/3=0.5). See **Fig. 3** for an animation
    of how this process stabilizes for pins at *Zₘᵢₙ + 1/3* and *Zₘᵢₙ + 2/3*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为*向上移动的距离* * *较高的数据点* = *向下移动的距离* * *较低的数据点* 或者 (0.75∗2/3=1.5∗1/3=0.5)。参见**图
    3**，其中展示了此过程如何使*Zₘᵢₙ + 1/3*和*Zₘᵢₙ + 2/3*的pins稳定。
- en: '![](../Images/a41298fa268d0806434ba71f0037e866.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a41298fa268d0806434ba71f0037e866.png)'
- en: '**Fig. 3** Moving 2 pins towards observed points until they stabilize — Image
    by author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3** 将2个pins移动至观测点直到它们稳定——图源自作者'
- en: '***How do we achieve this movement using a neural network?*** In order to move
    the “pins on the fabric” with a neural network, we select a value in ***Z*** (which
    we call a ***z-pin***) and do backpropagation with the target value being the
    ***z-pin*** plus/minus the predetermined distance, like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们如何通过神经网络实现这种移动？*** 为了用神经网络移动“布料上的pins”，我们选择一个在***Z***中的值（我们称之为***z-pin***），并通过反向传播，将目标值设定为***z-pin***加/减预定的距离，操作如下：'
- en: '![](../Images/3c449a2686c392296c2282a93f98ec1f.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c449a2686c392296c2282a93f98ec1f.png)'
- en: Leveraging this principle we can select points uniformly in ***Z*** and over
    a number of epochs we obtain the mapping we require. i.e. 𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这一原理，我们可以在***Z***中均匀选择点，并通过若干周期获得我们需要的映射。即 𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌。
- en: Notes from the original article
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自原文的注释
- en: In the original article the fabric stretching/shrinking analogy referred to
    *pins* that were used to reshape the model, however the model definitions and
    training method used the term ***z-samples*** to refer to the same concrete term.
    In the present article and in the future these will be referred to exclusively
    as ***z-pins***.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原文中，布料拉伸/收缩的类比指的是用于重塑模型的*pins*，然而模型定义和训练方法使用了***z-samples***这一术语来指代同一具体概念。在本篇文章中以及未来的讨论中，这些将被专门称为***z-pins***。
- en: When selecting ***z-pins*** the original article always placed them evenly distributed
    in ***Z*** and also used the same positions on every data point for every epoch.
    This is not necessary though, the only requirement is that the ***z-pins*** are
    uniformly distributed in ***Z***.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择***z-pins***时，原文总是将它们均匀分布在***Z***中，并且在每个周期对每个数据点使用相同的位置。然而，这并不是必要的，唯一的要求是***z-pins***在***Z***中均匀分布。
- en: The original article would use multiple ***z-pins*** per data point. This is
    also not necessary and it is sufficient to select a single ***z-pin*** per data
    point. In the present article all the experiments will select a single ***z-pin***
    per data point per epoch.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原文文章会在每个数据点使用多个***z-pins***。但这并非必要，每个数据点只需选择一个***z-pin***即可。在本篇文章中，所有实验将在每个数据点每个周期选择一个单一的***z-pin***。
- en: Generalizing for multiple outputs
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对多个输出的推广
- en: Having revisited the original method for one output, lets move on to the changes
    necessary to work for multiple outputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新审视原始方法以应对单一输出后，接下来我们将讨论针对多个输出所需的变化。
- en: Redefining the Z-space
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新定义Z空间
- en: Let’s define ***Z***, the sampling ground from where we select our ***z-pins***.
    In the original article ***Z*** was defined as a single dimensional range described
    simply by lower and upper bounds *[Zₘᵢₙ, Zₘₐₓ]*. However in the generalized method
    and in order to be able to handle multidimensional outputs ***Y***, ***Z*** must
    be defined in multiple dimensions as well (note however that the number of dimensions
    in ***Z*** and ***Y*** need not be the same).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义***Z***，即我们从中选择***z-pins***的采样空间。在原文中，***Z***被定义为一个单一维度的范围，简单地通过下限和上限描述为*[Zₘᵢₙ,
    Zₘᵐₓ]*。然而，在广义方法中，为了能够处理多维输出***Y***，***Z***必须也在多个维度中进行定义（但需要注意的是，***Z***和***Y***的维度数量不必相同）。
- en: In theory it could be any bounded n-dimensional space, but because it makes
    calculating the scalars easier as you’ll see later, I chose to use a [*hyper-sphere*](https://en.wikipedia.org/wiki/N-sphere)
    that can be defined by an origin ***O***, a radius ***R*** and a dimensionality
    ***N*** (see **Fig. 4)**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，它可以是任何有界的 n 维空间，但因为后续计算标量更为简便，正如你将看到的那样，我选择使用一个可以通过原点 ***O***、半径 ***R***
    和维度 ***N*** 定义的 [*超球面*](https://en.wikipedia.org/wiki/N-sphere)（见 **图 4**）。
- en: '![](../Images/087f11175461fb67f3a8c3eb86c70e94.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/087f11175461fb67f3a8c3eb86c70e94.png)'
- en: '**Fig. 4** 3-dimensional hypersphere ***Z-space*** — Image by author'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4** 三维超球面 ***Z-空间*** — 作者图片'
- en: Now let’s define a few concepts related to ***Z*** that will be needed to move
    ahead.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一些与 ***Z*** 相关的概念，这些概念将在接下来的讨论中派上用场。
- en: '***z-pins***: These are uniformly sampled points in ***Z***. They can be defined
    as an ***N***-dimensional vector like this: *zₚᵢₙ = (z₀, z₁, …, zₙ)* where *z₀,
    z₁, …* are coordinates in ***Z***.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-针***：这些是***Z***中的均匀采样点。它们可以定义为一个***N***维向量，形式如下：*zₚᵢₙ = (z₀, z₁, …, zₙ)*，其中
    *z₀, z₁, …* 是***Z***中的坐标。'
- en: '***z-dirs***: A ***z-dir*** is a direction on ***Z*** that can be defined as
    a unit vector based at origin ***O*** like this: *z-dir = O + (ž₀, ž₁, …, žₙ)*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-方向***：一个 ***z-方向*** 是一个位于原点 ***O*** 的单位向量，定义如下：*z-dir = O + (ž₀, ž₁, …,
    žₙ)*'
- en: '***z-lines***: A ***z-line*** is a line in ***Z*** such that it runs between
    any two points in ***Z***. We will define it as a line with a ***z-pin*** origin
    and a ***z-dir*** including all points in it that are inside of ***Z*** like this:
    *zₗᵢₙₑ = zₚᵢₙ + z-dir s.t.∀z ∈ zₗᵢₙₑ : z ∈ Z*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***z-线***：一个 ***z-线*** 是一个位于 ***Z*** 中的直线，连接 ***Z*** 中的任意两个点。我们将其定义为一个以 ***z-针***
    为起点，并具有 ***z-方向*** 的直线，包含所有位于 ***Z*** 中的点，形式如下：*zₗᵢₙₑ = zₚᵢₙ + z-dir 使得 ∀z ∈ zₗᵢₙₑ
    : z ∈ Z*'
- en: The model
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Moving into a multidimensional ***Z*** introduces an important challenge. In
    the case of one-dimensional ***Z*** and 𝑌 spaces, it was very simple to tell whether
    the selected ***z-pin*** projection i.e. 𝑓𝜃*(x, zₚᵢₙ)* was greater or smaller
    than the observed data point in order to decide which direction to move it to.
    In one dimension “greater than” in 𝑌 could simply be translated to “greater than”
    in ***Z*** and the ***z-pin*** could simply be moved up. This because we were
    mapping a line to another line.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 进入多维 ***Z*** 空间引入了一个重要的挑战。在一维的 ***Z*** 和 𝑌 空间中，判断所选的 ***z-针*** 投影，即 𝑓𝜃*(x, zₚᵢₙ)*
    是否大于或小于观测到的数据点，以决定将其移动到哪个方向，变得非常简单。在一维中，“大于”可以简单地转换为***Z***中的“大于”，而 ***z-针***
    可以简单地向上移动。这是因为我们只是将一条线映射到另一条线。
- en: 'But with multidimensional 𝑌 and ***Z*** it is not possible to assume that the
    spaces will have the same shape or even the same number of dimensions which means
    that in order to decide the direction where to move a ***z-pin*** based on its
    relation to a data point, it is necessary to map that data point from 𝑌 to ***Z***.
    This means that in addition to train function 𝑓𝜃 to generate values in 𝑌, we’ll
    also need to train an inverse function 𝑓𝜃⁻¹ to map data points to ***Z***. This
    fact changes our model architecture to something like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在多维 𝑌 和 ***Z*** 的情况下，无法假设这两个空间具有相同的形状或相同的维度数量，这意味着为了根据数据点与***z-针***的关系决定移动方向，有必要将数据点从
    𝑌 映射到 ***Z***。这意味着除了训练函数 𝑓𝜃 来生成 𝑌 中的值外，我们还需要训练一个逆函数 𝑓𝜃⁻¹ 来将数据点映射到 ***Z***。这一事实使得我们的模型架构发生了变化，变成了如下所示：
- en: '![](../Images/ceb7da9d5dcef27e3f5586ee548e47e1.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ceb7da9d5dcef27e3f5586ee548e47e1.png)'
- en: '**Fig. 5** Model architecture — Image by author'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5** 模型架构 — 作者图片'
- en: The left side of the model allows us to map points in 𝑌 to ***Z***. The right
    side of the model allows us to generate random samples in 𝑌 by sampling points
    in ***Z***.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的左侧允许我们将 𝑌 中的点映射到 ***Z***。模型的右侧允许我们通过在 ***Z*** 中采样点来生成 𝑌 中的随机样本。
- en: You may have noticed that this architecture is similar to that of plain autoencoders
    and indeed it is. This has the added benefit of making the method useful for learning
    latent representations that are bounded and evenly distributed.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，这个架构与普通自编码器的架构相似，的确如此。这一优势在于，它使得该方法对于学习有界且均匀分布的潜在表示非常有用。
- en: The Method
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: Having defined all the concepts we need we can proceed to discuss how pin movement
    works in multiple dimensions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了我们需要的所有概念后，我们可以继续讨论如何在多维空间中进行针的移动。
- en: Mapping data points to Z
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据点映射到 Z
- en: The first step is to use the inverse function 𝑓𝜃⁻¹ (or encoder going with autoencoder
    terminology) and map all data points in the batch from 𝑌 space to ***Z***. We
    will call the original data points *y-data* and the mapped data points *z-data*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用逆函数𝑓𝜃⁻¹（或使用自编码器术语中的编码器）将批次中的所有数据点从𝑌空间映射到***Z***空间。我们将原始数据点称为*y数据*，将映射后的数据点称为*z数据*。
- en: '![](../Images/3b754a579f9e979c5ebcdf8545e2a44b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b754a579f9e979c5ebcdf8545e2a44b.png)'
- en: '**Fig. 6** Mapping data points to a 2-D ***Z-space*** — Image by author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**图6** 将数据点映射到2维***Z空间*** — 图像来源于作者'
- en: Selecting the z-pins
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择z-针
- en: Next we must select some ***z-pins***. In order to do so, we start by selecting
    uniformly sampled ***z-dirs***, one for every data point. The easiest way to do
    so is by choosing random points in a hypersphere surface with the same dimensionality
    as ***Z***. Then we use the selected ***z-dirs*** and translate them to have the
    data points mapped in the previous step *z-data* as origins. This gives us some
    ***z-lines*** as you can see in ***Fig. 7***.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须选择一些***z-针***。为了做到这一点，我们首先选择均匀采样的***z-方向***，每个数据点选择一个。最简单的方法是选择一个超球面上的随机点，其维度与***Z***相同。然后，我们使用选定的***z-方向***，并将它们平移，使得前一步中映射的*z数据*作为原点。这就得到了如***图7***所示的一些***z线***。
- en: '![](../Images/c2ef2cd3fb679b035576766185fe9707.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ef2cd3fb679b035576766185fe9707.png)'
- en: '**Fig. 7** Selecting random ***z-lines*** *in a* 2-D ***Z-space***— Image by
    author'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**图7** 在2维***Z空间***中选择随机***z线*** — 图像来源于作者'
- en: Once we have our ***z-lines*** we proceed to randomly select points in these
    lines, these will be our ***z-pins***. ***Fig. 8*** shows how this can look like.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了我们的***z线***，接下来就可以在这些线中随机选择点，这些点就是我们的***z-针***。***图8***展示了这种情况的示意图。
- en: '![](../Images/b711e7ef5c92a990ef4997613d8c8b57.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b711e7ef5c92a990ef4997613d8c8b57.png)'
- en: '**Fig. 8** Selecting random ***z-pins*** in a 2-D ***Z-space***— Image by author'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**图8** 在2维***Z空间***中选择随机***z-针*** — 图像来源于作者'
- en: It is necessary for the method to work that for any given ***z-line*** in ***Z***,
    every mapped data point *z-data* in it has an equal probability of occurring,
    otherwise the equations on [Calculating the movement scalars](#calculating-the-movement-scalars)
    would not hold up.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使方法有效，对于任何给定的***z线***，它在***Z***中的每个映射数据点*z数据*出现的概率应该是相等的，否则在[计算运动标量](#calculating-the-movement-scalars)中的方程将无法成立。
- en: Given a 2-dimensional ***Z*** and for any given ***z-line*** in it, lets picture
    it as as a line with a minimal width 𝜖 in a way that it seems like a long rectangle,
    similar to the ***z-lines*** in ***Fig. 8***. The probability of any given 𝑧 existing
    in it is of the area of this “thin” ***z-line*** over the area of ***Z***.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个2维的***Z***空间，并且对于其中的任何一个***z线***，可以将其视为一个最小宽度为𝜖的线段，使其看起来像一个长矩形，类似于***图8***中的***z线***。任意给定的𝑧出现在其中的概率就是这个“薄”***z线***的面积与***Z***面积之比。
- en: '![](../Images/cb28153696cf9806dab5a4b0cbe779e4.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb28153696cf9806dab5a4b0cbe779e4.png)'
- en: Since this “thin” ***z-line*** is rectangular, any given segment 𝑠 of minimal
    length 𝛿 across it’s length has an equal area and therefore any given 𝑧 has an
    equal probability of being in the segment.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个“薄”***z线***是矩形的，所以其任意一个最小长度为𝛿的线段𝑠在其长度上具有相同的面积，因此任意一个𝑧出现在该线段的概率是相等的。
- en: '![](../Images/77b4ea01279ddaf80a6286a6ada82f5c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77b4ea01279ddaf80a6286a6ada82f5c.png)'
- en: Also the probability of any given 𝑧​ inside this “thin” ***z-line*** of selecting
    the ***z-dir*** of this “thin” ***z-line*** is constant given that the ***z-dirs***
    are selected using a uniform distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，任意给定的𝑧​出现在这个“薄”***z线***中的概率，选择该“薄”***z线***的***z-方向***的概率是恒定的，因为***z-方向***是通过均匀分布选择的。
- en: '![](../Images/9ccc900b136d25b13b06acbba1d3b74f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ccc900b136d25b13b06acbba1d3b74f.png)'
- en: Taking equations (2) and (3) we get that the probability of any 𝑧 being on any
    segment of a given ***z-line*** and selecting the same ***z-dir***, and that is
    the same for every segment which satisfies the requirement above.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程（2）和（3），我们得到任意一个𝑧出现在给定的***z线***的任意线段上的概率，并选择相同的***z-方向***，而且对于满足上述要求的每个线段，这个概率都是相同的。
- en: '![](../Images/5e48f3cc06d88db83495d324f4e048fe.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e48f3cc06d88db83495d324f4e048fe.png)'
- en: The probability is independent of the position of 𝑧 in ***z-line*** so the distribution
    in any ***z-line*** is uniform.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率与𝑧在***z线***中的位置无关，因此在任何***z线***中的分布都是均匀的。
- en: Calculating the target values
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算目标值
- en: After selecting the ***z-pins*** we can proceed to calculate the target values
    (or ***z-targets***) to use in our backpropagation. All we have to do for this
    is to add to every ***z-pin*** the movement constant 𝑀 in the direction where
    the mapped data point 𝑧-𝑑𝑎𝑡𝑎 is.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了***z-pins***之后，我们可以继续计算目标值（或***z-targets***）用于反向传播。我们所需要做的就是将运动常数𝑀加到每个***z-pin***上，方向是映射数据点𝑧-𝑑𝑎𝑡𝑎所在的方向。
- en: '![](../Images/c0190b58e863341e169c02ff8d431e0a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0190b58e863341e169c02ff8d431e0a.png)'
- en: '***Fig. 9*** Shows how the ***z-targets*** are calculated.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9** 显示了如何计算***z-targets***。'
- en: '![](../Images/2819cde11a256939e77fc07b132def79.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2819cde11a256939e77fc07b132def79.png)'
- en: '**Fig. 9** Calculating the ***z-targets*** *in a* 2-D ***Z-space*** — Image
    by author'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**图9** 计算***z-targets*** *在* 2-D ***Z-space*** 中 — 图片来源：作者'
- en: Calculating the movement scalars
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算运动标量
- en: The way that movement scalars are calculated is similar to the way it was done
    in the original one-dimensional method.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运动标量的计算方法类似于原始一维方法中的计算方式。
- en: Let’s start by picturing a ***z-line*** along with a ***z-pin*** and some mapped
    data points 𝑧𝑑𝑎𝑡𝑎 like we see on ***Fig .10***.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过图示一个***z-line***和一个***z-pin***以及一些映射的数据点𝑧𝑑𝑎𝑡𝑎，如同在**图10**中看到的那样。
- en: '![](../Images/697ca5cd5b94d0f0d5f9480dca809fed.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/697ca5cd5b94d0f0d5f9480dca809fed.png)'
- en: '**Fig. 10** Calculating the scalars — Image by author'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**图10** 计算标量 — 图片来源：作者'
- en: Let’s call ***a*** the distance from the ***z-pin*** to one end of the ***z-line***
    and ***b*** the distance to the other end. And let’s call the number of data points
    on the former side ***a’*** and the number of data points on latter side ***b’***.
    Our purpose is to make quantity ***a’*** proportional to distance ***a*** and
    ***b’*** proportional to ***b*** i.e. 𝑎:𝑏::𝑎′:𝑏′.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 设***a***为***z-pin***到***z-line***一端的距离，***b***为到另一端的距离。并且设前侧的数据点数量为***a'***，后侧的数据点数量为***b'***。我们的目标是使数量***a'***与距离***a***成正比，***b'***与***b***成正比，即
    𝑎:𝑏::𝑎′:𝑏′。
- en: Next we will call 𝛼 the scalar that we will use on the movement applied to the
    ***z-pin*** for all data points on the side of length ***a***. And we will call
    𝛽 the scalar that we will use on the movement applied to the ***z-pin*** for all
    data points on the side of length ***b***.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把𝛼称为在长度为***a***的侧面上应用于***z-pin***的运动标量。而我们将把𝛽称为在长度为***b***的侧面上应用于***z-pin***的运动标量。
- en: We will also call ***T*** the total movement, which is the sum of moving the
    ***z-pin*** a constant movement ***M*** towards the side of every data point multiplied
    by that side’s scalar.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将***T***称为总运动量，它是将***z-pin***沿每个数据点的侧面移动一个常数运动量***M***并乘以该侧的标量的总和。
- en: '![](../Images/fce7b3d44a7027e1e2931b7dd97bb295.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fce7b3d44a7027e1e2931b7dd97bb295.png)'
- en: 'We want ***T*** to be 0 (i.e. stabilized) when 𝑎′/(𝑎′+𝑏′)≈𝑎/(𝑎+𝑏)∧𝑏′/(𝑎′+𝑏′)≈𝑏/(𝑎+𝑏),
    that is when the ***z-pin*** divides the intended proportion of data points to
    both sides. Substituting ***T*** with ***0*** on (5) gives us the equation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望当 𝑎′/(𝑎′+𝑏′)≈𝑎/(𝑎+𝑏)∧𝑏′/(𝑎′+𝑏′)≈𝑏/(𝑎+𝑏) 时，***T*** 为 0（即稳定），即当***z-pin***将数据点按照预定比例分配到两侧时。将***T***替换为***0***后，方程(5)给出了以下方程：
- en: '![](../Images/9f8cb33871004aa8f21e05cbc98517a0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f8cb33871004aa8f21e05cbc98517a0.png)'
- en: 'Now let’s remember that not all ***z-lines*** will have the same length, since
    they are bounded by the hypersphere defined by ***Z*** the ones towards the center
    will be longer than the ones at the edges. Longer lines will represent larger
    spaces in ***Z*** (see equation (1)) so their influence in the movement should
    be proportional to their length. We want ***T*** to be linearly proportional to
    the length of the ***z-line*** which gives us the equation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们记住，并非所有的***z-lines***长度相同，因为它们被由***Z***定义的超球面所限制，朝向中心的***z-lines***将比边缘的***z-lines***长。较长的***z-lines***将表示***Z***中更大的空间（参见方程(1)），因此它们在运动中的影响应该与其长度成比例。我们希望***T***与***z-line***的长度成线性关系，这给我们带来了以下方程：
- en: '![](../Images/0fa148f5c1a05e18cdb3c55a57c2eda1.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fa148f5c1a05e18cdb3c55a57c2eda1.png)'
- en: 'If we put together *(6)* and *(7)* we get that the scalars should have these
    values:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将*(6)*和*(7)*合并，我们得到标量应具有以下值：
- en: '![](../Images/429c0d919d626ee53fe6e76f48b57299.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/429c0d919d626ee53fe6e76f48b57299.png)'
- en: Which are a similar equations to the one on the original article.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程与原文中的方程相似。
- en: You may have noticed that this equations break towards the edges i.e. when either
    ***a*** or ***b*** tends to *0*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，当***a***或***b***趋近于*0*时，这些方程会在边缘发生断裂。
- en: In order to solve this problem a maximum scalar constant ***S*** is introduced
    to clamp the scalars. Of course when clamping the scalars we have to be careful
    to adjust the value for both sides, for example if ***a*** is very small (and
    therefore 𝛼 is large) but the data point is on side ***b*** the scalar 𝛽 must
    be adjusted as well, otherwise equation (5) will not hold.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，引入了一个最大标量常数 ***S*** 来钳制标量。当然，在钳制标量时，我们必须小心调整两侧的值，例如，如果 ***a*** 非常小（因此
    𝛼 很大），但数据点位于 ***b*** 一侧，则标量 𝛽 也必须进行调整，否则方程 (5) 将无法成立。
- en: We start by selecting the largest of the 2 scalars 𝑚𝑎𝑥(𝛼,𝛽). Then we calculate
    an adjustment value by dividing ***S*** by 𝑚𝑎𝑥(𝛼,𝛽) and clamping it to 1.0 so
    that it is always a number in the range [0, 1]. We will use the adjustment value
    to prevent scalars from going over ***S***. Finally, if ***a*** is 0.0, then the
    values of 𝛼 and 𝛽 are ***S*** and 0.0 respectively, and the other way around if
    b is 0.0\. This gives us the revised equation *(8b)*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先选择两个标量 𝑚𝑎𝑥(𝛼,𝛽) 中的最大值。然后，我们通过将 ***S*** 除以 𝑚𝑎𝑥(𝛼,𝛽) 来计算一个调整值，并将其钳制到 1.0，以确保其始终位于
    [0, 1] 的范围内。我们将使用该调整值来防止标量超过 ***S***。最后，如果 ***a*** 为 0.0，则 𝛼 和 𝛽 的值分别为 ***S***
    和 0.0，如果 ***b*** 为 0.0，则反之亦然。这样，我们就得到了修正后的方程式 *(8b)*。
- en: '![](../Images/69809bf37204fef652c2bbe616131352.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69809bf37204fef652c2bbe616131352.png)'
- en: Below you can see how the plots for the scalars proportional to ***a*** or ***b***
    look like. Notice how they are clamped beyond the selected ***S***.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到与 ***a*** 或 ***b*** 成比例的标量图像。注意它们在超过选定的 ***S*** 之后是如何被钳制的。
- en: '![](../Images/4b7ce280f37aacb65182e9591d4998ae.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7ce280f37aacb65182e9591d4998ae.png)'
- en: '**Fig. 11** Movement scalar clamping for ***S=5.0*** — Image by author'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11** 针脚标量钳制，***S=5.0*** — 图片由作者提供'
- en: Having calculated both scalars we can choose the one to use by determining the
    side on which the data point resides.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 计算出两个标量后，我们可以通过确定数据点所在的边来选择使用哪个标量。
- en: '![](../Images/c0a282a46fed428c55bff5a0cb6cb9cd.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0a282a46fed428c55bff5a0cb6cb9cd.png)'
- en: Training the model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now that all the concepts involved are clear we can move on to describe the
    training algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有相关概念都已明确，我们可以继续描述训练算法。
- en: 1\. Pretraining and selecting the Z hyperparameters
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 预训练和选择 Z 超参数
- en: The algorithm described makes the assumption that the models 𝑓𝜃⁻¹ and 𝑓𝜃 inversely
    match each other. This can lead to a slow start if we train these 2 models to
    match each other at the same time as we do pin movement. So it has been found
    beneficial to do a “pretrain” stage on which we only train 𝑓𝜃⁻¹ and 𝑓𝜃 to match
    each other. This stage is essentially an ordinary autoencoder training. After
    the reconstruction error has reached a reasonably low value the algorithm can
    proceed to the main training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所描述的算法假设模型 𝑓𝜃⁻¹ 和 𝑓𝜃 彼此反向匹配。如果我们在进行针脚运动时同时训练这两个模型来使它们相互匹配，可能会导致启动缓慢。因此，已经发现，进行“预训练”阶段，使得我们仅训练
    𝑓𝜃⁻¹ 和 𝑓𝜃 以使它们匹配，会更有利。这个阶段本质上是一个普通的自编码器训练。在重构误差达到一个合理低的值之后，算法可以进入主训练阶段。
- en: This pretrain stage has the added advantage that it makes it easier to define
    ***Z*** when it completes. In the section [Redefining the *Z-space*](#redefining-the-z-space)
    it was mentioned that ***Z*** is defined by an origin ***O*** and a radius ***R***.
    Having pretrained the model for some time, all we do is run a batch of data points
    through the inverse model to calculate a set *Z-data*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预训练阶段还有一个额外的优势，它使得在完成后更容易定义 ***Z***。在章节 [重新定义 *Z-space*](#redefining-the-z-space)
    中提到，***Z*** 是由原点 ***O*** 和半径 ***R*** 定义的。经过一段时间的预训练后，我们只需要通过逆模型运行一批数据点来计算一组 *Z-data*。
- en: '![](../Images/b5420483b93263466ca02e09f6693b59.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5420483b93263466ca02e09f6693b59.png)'
- en: Then we take the mean of this set and use it as the origin ***O***.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取这个集合的平均值，并将其用作原点***O***。
- en: '![](../Images/77a3e2ec645ba5aacbb39566fec5aeb9.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77a3e2ec645ba5aacbb39566fec5aeb9.png)'
- en: We can also use the mean distance in *Z-data* to ***O*** as ***R***, however
    it has been seen that experimenting with and tuning this value may give better
    results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 *Z-data* 到 ***O*** 的平均距离作为 ***R***，但是已观察到，调整和调试该值可能会获得更好的结果。
- en: '![](../Images/41ba99d0f2451905d4efc25c5c5dc90a.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41ba99d0f2451905d4efc25c5c5dc90a.png)'
- en: This works because after the “pretrain” stage, the model has found a region
    capable of representing the data, so defining ***Z*** in its vicinity will likely
    have a low reconstruction error.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有效的，因为在“预训练”阶段后，模型已经找到了一个能够表示数据的区域，因此在其附近定义 ***Z*** 很可能会产生较低的重构误差。
- en: 2\. Pin Movement
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 针脚运动
- en: To start pin movement we select a batch of data *y-data = {y-data₀, y-data₁,
    …, y-dataₙ}* from the training dataset and map it to z*-data = {z-data₀, z-data₁,
    …, z-dataₙ}* like it is explained in [Mapping data points to Z](#mapping-data-points-to-z).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始pin的移动，我们从训练数据集中选择一批数据*y-data = {y-data₀, y-data₁, …, y-dataₙ}*并将其映射到z*-data
    = {z-data₀, z-data₁, …, z-dataₙ}*，正如在[将数据点映射到Z](#mapping-data-points-to-z)中所解释的那样。
- en: The next step is to randomly select the ***z-pins*** set *{z-pin₀, z-pin₁, …,
    z-pinₙ}* (one for every data point in the batch) in the way described on section
    [Selecting the Z-pins](#selecting-the-z-pins).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是随机选择***z-pins***集合*{z-pin₀, z-pin₁, …, z-pinₙ}*（每个数据点一个）的方法，如[选择Z-pins](#selecting-the-z-pins)部分所述。
- en: '*Note that it is possible to select multiple* **z-pins** *per data point. But
    it is not necessary and for simplicity we will use only one on the experiments.*'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意，每个数据点可以选择多个* **z-pins** *。但是这不是必须的，为了简便起见，我们在实验中只使用一个。*'
- en: Then we calculate the target values *z-targets = {z-target₀, z-target₁, …, z-targetₙ}*
    and scalars s = *{s₀, s₁, …, sₙ}* as explained on sections [Calculating the target
    values](#calculating-the-target-values) and [Calculating the movement scalars](#calculating-the-movement-scalars).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算目标值*z-targets = {z-target₀, z-target₁, …, z-targetₙ}*和标量s = *{s₀, s₁,
    …, sₙ}*，如[计算目标值](#calculating-the-target-values)和[计算移动标量](#calculating-the-movement-scalars)部分所述。
- en: 'Having the ***z-targets***, we calculate the current model predictions by running
    them through 𝑓𝜃, this gives us:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 获得***z-targets***后，我们通过将其传递给𝑓𝜃计算当前模型的预测值，这将给我们：
- en: '![](../Images/4020948aed7c0eb227fe50b4c8084a7c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4020948aed7c0eb227fe50b4c8084a7c.png)'
- en: 'Now we have everything for the first component of our loss function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为损失函数的第一个组成部分准备好了所有内容：
- en: '![](../Images/bdfb2b259c8dfae0893c785dbcd1e429.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdfb2b259c8dfae0893c785dbcd1e429.png)'
- en: Notice that we are using a Weighted Mean Absolute Error *(WMAE)* function instead
    of a Weighted Mean Squared Error *(WMSE)*. This is because the latter is designed
    to punish larger differences while we are moving all of our pins the same distance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是加权平均绝对误差*(WMAE)*函数，而不是加权平均平方误差*(WMSE)*。这是因为后者旨在惩罚较大的差异，而我们将所有的pin都移动相同的距离。
- en: 3\. Reconstruction Loss
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 重建损失
- en: The next component to our loss function is the difference between our model
    𝑓𝜃 and our inverse model 𝑓𝜃⁻¹. This is very similar to the *Reconstruction Loss*
    in both variational and ordinary autoencoders. It is necessary to pass the batch
    data points to 𝑓𝜃⁻¹, take the results and pass them on to 𝑓𝜃 and then run backpropagation
    using the results and the original data points.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的下一个组成部分是我们的模型𝑓𝜃和我们的逆模型𝑓𝜃⁻¹之间的差异。这与变分自编码器和普通自编码器中的*重建损失*非常相似。我们需要将批量数据点传递给𝑓𝜃⁻¹，获取结果后再传递给𝑓𝜃，然后使用这些结果和原始数据点进行反向传播。
- en: '![](../Images/edcf87003c4b92b959bca6251690f13e.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edcf87003c4b92b959bca6251690f13e.png)'
- en: 4\. Inverse Reconstruction Loss
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 逆重建损失
- en: Before we define the last component of the loss function, let’s explain why
    it is necessary. Ideally at the end of the training both 𝑓𝜃 and 𝑓𝜃⁻¹ will be bijective,
    meaning that there will be an exactly one-to-one correspondence between the domain
    and codomain ***Z*** and ***Y***. However during training this is not guaranteed
    to be the case and it is possible that areas in ***Z*** will not be mapped to
    ***Y***.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义损失函数的最后一个组成部分之前，我们先解释一下它为什么是必要的。理想情况下，在训练结束时，𝑓𝜃和𝑓𝜃⁻¹都应该是双射的，这意味着***Z***和***Y***之间会有严格的一一对应关系。然而，在训练过程中并不能保证这一点，可能会出现***Z***中的某些区域未能映射到***Y***中。
- en: '![](../Images/b90e7c2975af97a75288d1862d5dad76.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b90e7c2975af97a75288d1862d5dad76.png)'
- en: '**Fig. 12** Model and inverse models may not be bijective — Image by author'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**图12** 模型和逆模型可能不是双射的 — 作者提供的图片'
- en: As you can see in ***Fig. 12*** as a result of training with component *loss-y*,
    𝑓𝜃 and 𝑓𝜃⁻¹ agree with each other as far as ***Y*** goes. That is *∀y ∈ Y,* 𝑓𝜃⁻¹(𝑓𝜃(𝑦))
    ≈ *y*. However not all of ***Z*** is used and some points in ***Y*** map outside
    of it. This is a problem because the assumption of moving ***z-pins*** to a position
    that will map to a point in ***Y*** that both 𝑓𝜃 and 𝑓𝜃⁻¹ will agree on is broken.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在***图12***中看到的，经过*loss-y*组成部分的训练后，𝑓𝜃和𝑓𝜃⁻¹在***Y***上是一致的。即*∀y ∈ Y,* 𝑓𝜃⁻¹(𝑓𝜃(𝑦))
    ≈ *y*。然而，并不是所有的***Z***都被使用，一些***Y***中的点映射到了它之外。这是一个问题，因为假设将***z-pins***移动到一个位置，这个位置会映射到一个***Y***中的点，而𝑓𝜃和𝑓𝜃⁻¹都能一致，这个假设被打破了。
- en: '***Fig. 12*** shows 2 of the problems that may happen. A “fold” in the fabric
    happens when 2 or more points in ***Z*** map to the same point in ***Y***. An
    “out-of-bounds” happens when a point in ***Z*** maps to a point outside of ***Y***.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '***图12***展示了可能发生的两个问题。布料中的“折叠”发生在***Z***中的两个或更多点映射到***Y***中的同一点时。发生“越界”时，***Z***中的一个点映射到***Y***之外的点。'
- en: 'In order to solve this problem we add third component to the loss function:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们向损失函数中添加了第三个组件：
- en: '![](../Images/dea9a9bb44d6b75d7c6090d65e6b603c.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dea9a9bb44d6b75d7c6090d65e6b603c.png)'
- en: What this does is to synchronize 𝑓𝜃 and 𝑓𝜃⁻¹​ with regards to ***Z*** and it
    does so by selecting random points in ***Z*** instead of using the training set
    data points.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的目的是使𝑓𝜃和𝑓𝜃⁻¹​在***Z***方面保持同步，方法是选择***Z***中的随机点，而不是使用训练集中的数据点。
- en: Notice that for both the reconstruction loss and the inverse reconstruction
    loss we simply use Mean Squared Error (MSE).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于重建损失和逆重建损失，我们简单地使用均方误差（MSE）。
- en: 5\. Loss function
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 损失函数
- en: 'Now that we have all the components to the loss function all that’s left is
    to define weights for each of them which we will name 𝛾-𝑝, 𝛾-*y* and 𝛾-𝑧. We can
    put together (10), (11) and (12) to define the loss function like this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了损失函数的所有组件，剩下的就是为它们定义权重，我们将这些权重命名为𝛾-𝑝、𝛾-*y*和𝛾-𝑧。我们可以将(10)、(11)和(12)结合起来，像这样定义损失函数：
- en: '![](../Images/7563dfc4116d138b5cdd5bb187b801b8.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7563dfc4116d138b5cdd5bb187b801b8.png)'
- en: All that’s left after this is to run backpropagation on the loss.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是对损失进行反向传播。
- en: Testing the model
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: In the original paper we used goal 1 and goal 2 testing which measured the density
    of data points between ***z-pins*** and compared it to the densities of the test
    dataset. However on a multi-dimensional space doing that approach is not practical
    since the spaces between ***z-pins*** scale rapidly in number.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文中，我们使用了目标1和目标2测试，测量了***z-pins***之间的数据点密度，并将其与测试数据集的密度进行了比较。然而，在多维空间中，这种方法并不实用，因为***z-pins***之间的空间数量会迅速增大。
- en: The original paper also used [*Earth Mover’s Distance (EMD)*](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)
    as an indicator of the model’s performance. For multiple dimension *PMT* we will
    use *EMD* to measure the model’s accuracy. We will define the EMD error by comparing
    data points from the training dataset against data points generated by the *PMT*
    model.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文还使用了[*地球搬运工距离（EMD）*](https://en.wikipedia.org/wiki/Earth_mover%27s_distance)作为模型性能的指标。对于多维*PMT*，我们将使用*EMD*来衡量模型的准确性。我们将通过将训练数据集中的数据点与*PMT*模型生成的数据点进行比较来定义EMD误差。
- en: '![](../Images/6ee897945e3f09ca4893e61980b65bc5.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ee897945e3f09ca4893e61980b65bc5.png)'
- en: And in order to have an idea of what the lowest EMD error would be we will also
    calculate a base EMD by comparing data points from the training dataset against
    data points in the test dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计最低的EMD误差是多少，我们还将通过将训练数据集中的数据点与测试数据集中的数据点进行比较来计算一个基准EMD。
- en: '![](../Images/6baf14ad2aee559ec18772d16672feaf.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6baf14ad2aee559ec18772d16672feaf.png)'
- en: This gives us a baseline that we can compare against E-emd to measure the accuracy
    of the models.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个基准，可以用它来与E-emd进行比较，从而衡量模型的准确性。
- en: Comparison with Variational Autoencoders
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与变分自编码器的比较
- en: The most similar generative model to *PMT* is Variational Autoencoders (*VAE*).
    It has an almost identical neural network architecture and acts both a generative
    model and a latent representation mapper. The biggest difference between the two
    is that the source distribution in *VAE* is unbounded (Gaussian) and the one in
    *PMT* is bounded (uniform distribution).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与*PMT*最相似的生成模型是变分自编码器（*VAE*）。它具有几乎相同的神经网络架构，并且既是生成模型又是潜在表示映射器。两者之间的最大区别在于，*VAE*中的源分布是无界的（高斯分布），而*PMT*中的源分布是有界的（均匀分布）。
- en: The experiments show however, that for both bounded and unbounded target distributions
    *PMT* outperforms *VAE*. Furthermore, the reconstruction error in *PMT* is significantly
    lower than on *VAE*. The reason for this may be that the components of the loss
    function cooperate with each other on *PMT* as opposed to competing with each
    other in *VAE*. Also because of the fact that the target distribution is uniform,
    the spacing between data points in ***Z*** can be larger.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实验表明，无论是有界还是无界目标分布，*PMT*都优于*VAE*。此外，*PMT*中的重建误差显著低于*VAE*。其原因可能在于，损失函数的各个组件在*PMT*中相互协作，而在*VAE*中则是相互竞争。而且，由于目标分布是均匀的，***Z***中数据点之间的间距可以更大。
- en: Another difference is that *PMT* takes a larger number of hyperparameters, 𝑆
    (maximum scalar), 𝛾-𝑝 (pin movement weight), 𝛾-𝑦 (reconstruction loss weight),
    𝛾-𝑧 (inverse reconstruction loss weight) and 𝑀 (movement constant) compared to
    *VAE* hyperparameters which are just the *kld weight*. This may make *PMT* training
    more difficult.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是，*PMT*有更多的超参数，包括𝑆（最大标量）、𝛾-𝑝（针脚移动权重）、𝛾-𝑦（重建损失权重）、𝛾-𝑧（反向重建损失权重）和𝑀（运动常数），而*VAE*的超参数仅为*kld
    权重*。这可能会使得*PMT*的训练更加困难。
- en: Finally *PMT* takes longer to train per epoch than *VAE*, this is because it
    is necessary to do a pass to calculate the ***z-targets***, and also because the
    loss function has an additional component (see equation (12)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*PMT*每个周期的训练时间比*VAE*长，这因为需要进行一次传递来计算***z-targets***，而且损失函数有一个附加组件（见公式（12））。
- en: Experiments
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: Now I will try out the model in several datasets. In order to make them easier
    to plot, the experiments presented will have no ***X*** inputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将在多个数据集上尝试该模型。为了方便绘制，下面的实验将不包含***X***输入。
- en: Due to the similarities with *VAE*, every experiment will be done using both
    *PMT* and *VAE* models for comparison. In every experiment both models will have
    identical neural network architectures.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于与*VAE*的相似性，每个实验将使用*PMT*和*VAE*模型进行比较。在每个实验中，两个模型将采用相同的神经网络架构。
- en: The source code and everything needed to reproduce the experiments below can
    be found in [https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/narroyo1/pmt](https://github.com/narroyo1/pmt)找到源代码和重现下面实验所需的一切。
- en: Multiple blobs
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个数据块
- en: The first dataset I’ll try is generated using [*make_blobs()*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    from the *sklearn* library. As it name suggests it generates a number of Gaussian
    blobs and it is a good dataset to test how *PMT* performs with unbounded datasets.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我将尝试的第一个数据集是使用[*make_blobs()*](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)从*sklearn*库生成的。顾名思义，它生成若干个高斯数据块，是测试*PMT*在无界数据集上表现的一个良好数据集。
- en: '![](../Images/c08abf0004b1126002cbcb212c12133c.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c08abf0004b1126002cbcb212c12133c.png)'
- en: '**Fig. 13a** Generated data — Image by author'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13a** 生成的数据 — 作者提供的图像'
- en: '![](../Images/c4ac3c2ebd1e7dbfefe33340cc6e1f75.png)![](../Images/de4f045995b16018008a7261ac9b37ea.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4ac3c2ebd1e7dbfefe33340cc6e1f75.png)![](../Images/de4f045995b16018008a7261ac9b37ea.png)'
- en: '**Fig. 13b** PMT training animation /**Fig. 13c** VAE training animation —
    Image by author'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13b** PMT训练动画 /**图 13c** VAE训练动画 — 作者提供的图像'
- en: '![](../Images/632a68d1f622dafcbf04a9ff90ae6266.png)![](../Images/ec6296d1dfad186d646998a0a06b1c99.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/632a68d1f622dafcbf04a9ff90ae6266.png)![](../Images/ec6296d1dfad186d646998a0a06b1c99.png)'
- en: '**Fig. 13d** EMD plot/**Fig. 13e** Reconstruction loss plot — Image by author'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13d** EMD误差图/**图 13e** 重建损失图 — 作者提供的图像'
- en: '**Fig. 13a** shows the test data generated by the *make_blobs()* function.
    **Fig. 13b** and **Fig. 13c** show animations of the *PMT* and *VAE* training
    methods respectively.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13a**展示了由*make_blobs()*函数生成的测试数据。**图 13b**和**图 13c**分别展示了*PMT*和*VAE*训练方法的动画。'
- en: '**Fig. 13d** shows the plot of the *EMD* errors (𝐸-𝑒𝑚𝑑) calculated for both
    *PMT*, *VAE* and the base value (𝐵-𝑒𝑚𝑑). As you can see from this *PMT*’s 𝐸-𝑒𝑚𝑑
    are closer to 𝐵-𝑒𝑚𝑑 than those of *VAE* which means that its performance is better.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13d**展示了计算的*EMD*误差（𝐸-𝑒𝑚𝑑）图，分别为*PMT*、*VAE*和基准值（𝐵-𝑒𝑚𝑑）。正如你所看到的，*PMT*的𝐸-𝑒𝑚𝑑比*VAE*的更接近𝐵-𝑒𝑚𝑑，这意味着其性能更好。'
- en: '**Fig. 13e** shows the plot of the reconstruction error for both *PMT* and
    *VAE*. As you can see from this *PMT*’s reconstruction error is an order of magnitude
    lower than that of of *VAE*.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13e**展示了*PMT*和*VAE*的重建误差图。正如你所看到的，*PMT*的重建误差比*VAE*低一个数量级。'
- en: Square with another square
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方形与另一个方形
- en: The second dataset is quite simple. We just have an outer square with a uniform
    distribution of data points inside it, with an inner square that is also filled
    with a uniform distribution but with a greater density. This will help us test
    for non-gaussian distributions with sharp details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数据集相当简单。我们只需要一个外部的方形区域，里面均匀分布着数据点，再加上一个内嵌的方形区域，里面同样是均匀分布的数据，但密度更大。这将帮助我们测试具有尖锐细节的非高斯分布。
- en: '![](../Images/1bb4f3ed920e1990972c46ee0858c720.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bb4f3ed920e1990972c46ee0858c720.png)'
- en: '**Fig. 14a** Generated data — Image by author'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14a** 生成的数据 — 作者提供的图像'
- en: '![](../Images/a1be878f264605880859aba8217440fa.png)![](../Images/56a7800a3c07bfaaba7aef6d3d124da5.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1be878f264605880859aba8217440fa.png)![](../Images/56a7800a3c07bfaaba7aef6d3d124da5.png)'
- en: '**Fig. 14b** PMT training animation/**Fig. 14c** VAE training animation — Image
    by author'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14b** PMT 训练动画/**图 14c** VAE 训练动画 — 图像来源：作者'
- en: '![](../Images/77f6326015da719f593d33c7e138cabc.png)![](../Images/8ef90a759f521bbcc862d0f8b2540a54.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77f6326015da719f593d33c7e138cabc.png)![](../Images/8ef90a759f521bbcc862d0f8b2540a54.png)'
- en: '**Fig. 14d** EMD plot/**Fig. 14e** Reconstruction loss plot — Image by author'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14d** EMD 图/**图 14e** 重建损失图 — 图像来源：作者'
- en: '**Fig. 14d** shows the *EMD* errors plot and you can see from this that *PMT*
    outperforms *VAE*.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14d** 显示了 *EMD* 错误图，你可以从中看到 *PMT* 超越了 *VAE*。'
- en: '**Fig. 14e** shows the reconstruction error values and you can see from this
    *PMT*’s reconstruction error is over 2 orders of magnitude lower than that of
    *VAE*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14e** 显示了重建误差值，你可以看到 *PMT* 的重建误差比 *VAE* 低两个数量级。'
- en: Human behavior
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类行为
- en: The next dataset is made of human body motion sensor data acquired by performing
    several physical activities. It was extracted from [Mobile Health Human Behavior
    Analysis Dataset](https://www.kaggle.com/datasets/gaurav2022/mobile-health)¹.
    This one has 3 dimensions instead of 2 like the previous datasets.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个数据集由人体运动传感器数据构成，这些数据是通过进行几种体育活动获得的。它来源于 [移动健康人体行为分析数据集](https://www.kaggle.com/datasets/gaurav2022/mobile-health)¹。这个数据集具有3个维度，而不像之前的数据集只有2个维度。
- en: '![](../Images/4834f93ea998bc644806503e7241e0ab.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4834f93ea998bc644806503e7241e0ab.png)'
- en: '**Fig. 15a** Test data — Image by author'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15a** 测试数据 — 图像来源：作者'
- en: '![](../Images/c74d9ef38f096283e44f1335265f73af.png)![](../Images/b4b21e69eceed64faf940b9ec81f704d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c74d9ef38f096283e44f1335265f73af.png)![](../Images/b4b21e69eceed64faf940b9ec81f704d.png)'
- en: '**Fig. 15b** PMT training animation/**Fig. 15c** VAE training animation — Image
    by author'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15b** PMT 训练动画/**图 15c** VAE 训练动画 — 图像来源：作者'
- en: '![](../Images/57f7598cc9a77276144674295864052a.png)![](../Images/9839cdb2d118c7ae47804972cdf1c958.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57f7598cc9a77276144674295864052a.png)![](../Images/9839cdb2d118c7ae47804972cdf1c958.png)'
- en: '**Fig. 15d** EMD plot/**Fig. 15e** Reconstruction loss plot — Image by author'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15d** EMD 图/**图 15e** 重建损失图 — 图像来源：作者'
- en: '**Fig. 15d** shows the *EMD* errors plot and once again *PMT* outperforms *VAE*
    again.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15d** 显示了 *EMD* 错误图，再次证明 *PMT* 超越了 *VAE*。'
- en: '**Fig. 15e** shows that *PMT*’s reconstruction error is over an order of magnitude
    lower than that of *VAE*.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15e** 显示 *PMT* 的重建误差比 *VAE* 低一个数量级。'
- en: MNIST
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST
- en: Lastly we have the famous [MNIST Dataset](https://huggingface.co/datasets/ylecun/mnist)².
    As you know it contains bitmaps of numbers written by humans and the task here
    is to try to generated new data points that look like real hand drawn numbers.
    This is an interesting dataset because it has a large number of output dimensions
    (784) and a latent space of 4 dimensions.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是著名的 [MNIST 数据集](https://huggingface.co/datasets/ylecun/mnist)²。正如你所知，它包含了人类书写的数字的位图，而任务是生成看起来像手写数字的新的数据点。这个数据集很有趣，因为它具有大量的输出维度（784）和4维的潜在空间。
- en: '![](../Images/687c7882f1b5c102c2af06b17783767c.png)![](../Images/c819342f0f0639d2a25e8085a495a36c.png)![](../Images/cfb4cc8a2b64620e1015b78452b2c3c3.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/687c7882f1b5c102c2af06b17783767c.png)![](../Images/c819342f0f0639d2a25e8085a495a36c.png)![](../Images/cfb4cc8a2b64620e1015b78452b2c3c3.png)'
- en: '**Fig. 17a** PMT original data/**Fig. 17b** PMT reconstruction/**Fig. 17c**
    PMT generated samples — Image by author'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 17a** PMT 原始数据/**图 17b** PMT 重建/**图 17c** PMT 生成的样本 — 图像来源：作者'
- en: '![](../Images/4c31557abcf5abe663e67956a511168a.png)![](../Images/f02053feb01df2379b898c570723060d.png)![](../Images/574541dc3ffd8220ccf516c1d30951dd.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c31557abcf5abe663e67956a511168a.png)![](../Images/f02053feb01df2379b898c570723060d.png)![](../Images/574541dc3ffd8220ccf516c1d30951dd.png)'
- en: '**Fig. 16c** VAE original data/**Fig. 16e** VAE reconstruction/**Fig. 16e**
    VAE generated samples — Image by author'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 16c** VAE 原始数据/**图 16e** VAE 重建/**图 16e** VAE 生成的样本 — 图像来源：作者'
- en: '![](../Images/6654897ba08399ce6612558a8d5f6720.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6654897ba08399ce6612558a8d5f6720.png)'
- en: '**Fig. 16g** Reconstruction loss plot — Image by author'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 16g** 重建损失图 — 图像来源：作者'
- en: This dataset does not have an EMD error plot since it would be very difficult
    to calculate (and not indicative) given the large number of output dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输出维度过多，计算 EMD 错误图非常困难（而且没有指示性意义），因此这个数据集没有 EMD 错误图。
- en: '**Fig. 16b** plots the reconstruction errors, once again *PMT*’s is lower than
    *VAE*’s.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 16b** 绘制了重建误差图，再次证明 *PMT* 的误差低于 *VAE* 的误差。'
- en: Conclusion and what’s next
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与未来的方向
- en: Approximating stochastic functions with a single output is very useful for forecasting
    single value distributions, like temperatures or market values. But the ability
    to produce multiple outputs make the method apt for a large variety of use cases
    like simulation and generative tasks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 用单一输出逼近随机函数对于预测单值分布（如温度或市场值）非常有用。但产生多个输出的能力使得该方法适用于多种应用场景，如模拟和生成任务。
- en: The multiple output method described in this article has proved that in the
    experiment datasets, it is capable of outperforming VAEs in both probabilistic
    likeness and reconstruction. I believe they will produce better results in a variety
    of real world use cases too.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本文描述的多输出方法已经证明，在实验数据集中，它能够在概率相似度和重建方面优于 VAE。我相信它们在各种现实世界应用中也会产生更好的结果。
- en: In the future I would like to continue testing *PMT* on higher dimensional datasets
    for generative purposes, such as [Fashion MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)
    and [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). For this purpose
    it will also be necessary to experiment with deep networks and Convolutional Neural
    Networks (CNN).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我希望在更高维度的数据集上继续测试*PMT*，以进行生成任务，比如[时尚 MNIST](https://www.kaggle.com/datasets/zalando-research/fashionmnist)和[CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)。为此，还需要尝试深度网络和卷积神经网络（CNN）。
- en: Feel free to reach out to me with any questions or comments.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何问题或建议，欢迎随时联系我。
- en: '[1]: Mobile Health Human Behavior Analysis'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]: 移动健康人类行为分析'
- en: https://www.kaggle.com/datasets/gaurav2022/mobile-health
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: https://www.kaggle.com/datasets/gaurav2022/mobile-health
- en: CC0 Public Domain [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: CC0 公共领域 [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)
- en: '[2] MNIST Handwritten Database'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] MNIST 手写数字数据库'
- en: '[## MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris
    Burges'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MNIST 手写数字数据库，Yann LeCun，Corinna Cortes 和 Chris Burges'
- en: of handwritten digits Yann LeCun, Courant Institute, NYU Corinna Cortes, Google
    Labs, New York Christopher J.C. Burges…
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手写数字 Yann LeCun，纽约大学 Courant 研究所 Corinna Cortes，Google 实验室，纽约 Christopher J.C.
    Burges…
- en: yann.lecun.com](http://yann.lecun.com/exdb/mnist?source=post_page-----ffefc7099a90--------------------------------)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: yann.lecun.com](http://yann.lecun.com/exdb/mnist?source=post_page-----ffefc7099a90--------------------------------)
- en: MIT [https://choosealicense.com/licenses/mit/](https://choosealicense.com/licenses/mit/)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: MIT [https://choosealicense.com/licenses/mit/](https://choosealicense.com/licenses/mit/)
