<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Interpretable Latent Spaces Using Space-Filling Vector Quantization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Interpretable Latent Spaces Using Space-Filling Vector Quantization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-latent-spaces-using-space-filling-vector-quantization-e4eb26691b14?source=collection_archive---------7-----------------------#2024-04-08">https://towardsdatascience.com/interpretable-latent-spaces-using-space-filling-vector-quantization-e4eb26691b14?source=collection_archive---------7-----------------------#2024-04-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c2b0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A new unsupervised method that combines two concepts of vector quantization and space-filling curves to interpret the latent space of DNNs.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mohammad.vali?source=post_page---byline--e4eb26691b14--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mohammad Hassan Vali" class="l ep by dd de cx" src="../Images/b057aa7bd9e1c629fc3743a7f69f013e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zgkovfqgOo7DTgjZy8xoSw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e4eb26691b14--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mohammad.vali?source=post_page---byline--e4eb26691b14--------------------------------" rel="noopener follow">Mohammad Hassan Vali</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e4eb26691b14--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mi mj mk"><p id="a2f4" class="ml mm mn mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This post is a short explanation of our novel unsupervised distribution modeling technique called space-filling vector quantization [1] published at Interspeech 2023 conference. For more details, please look at the paper under <a class="af ni" href="https://www.isca-archive.org/interspeech_2023/vali23_interspeech.pdf" rel="noopener ugc nofollow" target="_blank">this link</a>.</p></blockquote><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/6cc783b8a0d6df1f2d0381dab42c4cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*481OtKfJqJE7PgDyGstuvw.jpeg"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Image from <a class="af ni" href="https://stocksnap.io" rel="noopener ugc nofollow" target="_blank">StockSnap.io</a></figcaption></figure><p id="b236" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Deep generative models are well-known neural network-based architectures that learn a latent space whose samples can be mapped to sensible real-world data such as image, video, and speech. Such latent spaces act as a black-box and they are often difficult to interpret. In this post, we introduce our novel unsupervised distribution modeling technique that combines two concepts of space-filling curves and vector quantization (VQ) which is called <strong class="mo fr">Space-Filling Vector Quantization</strong> (SFVQ) [1]. SFVQ helps to make the latent space interpretable by capturing its underlying morphological structure. <strong class="mo fr">Important to note that SFVQ is a generic tool for modeling distributions and using it is not restricted to any specific neural network architecture nor any data type (e.g. image, video, speech and etc.)</strong>. In this post, we demonstrate the application of SFVQ to interpret the latent space of a voice conversion model. To understand this post you don’t need to know about speech signals technically, because we explain everything in general (not technical). Before everything, let me explain what is the SFVQ technique and how it works.</p><h2 id="4456" class="oc od fq bf oe of og oh oi oj ok ol om mv on oo op mz oq or os nd ot ou ov ow bk">Space-Filling Vector Quantization (SFVQ)</h2><p id="942f" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk"><a class="af ni" href="https://medium.com/towards-data-science/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce" rel="noopener"><strong class="mo fr">Vector quantization</strong></a><strong class="mo fr"> (VQ)</strong> [2] is a data compression technique similar to k-means algorithm which can model any data distribution. The figure below shows a VQ applied on a Gaussian distribution. VQ clusters this distribution (gray points) using 32 codebook vectors (blue points) or clusters. Each voronoi cell (green lines) contains one codebook vector such that this codebook vector is the closest codebook vector (in terms of Euclidean distance) to all data points located in that voronoi cell. In other words, each codebook vector is the representative vector of all data points located in its corresponding voronoi cell. Therefore, applying VQ on this Gaussian distribution means to map each data point to its closest codebook vector, i.e. represent each data point with its closest codebook vector. For more information about VQ and its other variants you can check out <a class="af ni" rel="noopener" target="_blank" href="/optimizing-vector-quantization-methods-by-machine-learning-algorithms-77c436d0749d">this post</a>.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pc"><img src="../Images/81eaa7098328be722e18aa9bddb60c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPTV7QNbtbvIjuVqY_oVQg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Vector Quantization applied on a Gaussian distribution using 32 codebook vectors. (image by author)</figcaption></figure><p id="2f15" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk"><strong class="mo fr">Space-filling curve</strong> is a piece-wise continuous line generated with a recursive rule and if the recursion iterations are repeated infinitely, the curve gets bent until it completely fills a multi-dimensional space. The following figure illustrates the Hilbert curve [3] which is a well-known type of space-filling curves in which the corner points are defined using a specific mathematical formulation at each recursion iteration.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pd"><img src="../Images/b51dc7c74f0380333dd333794763186e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gF4iQdaBAR_fAQkr_0K8Og.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Five first iterations of Hilbert curve to fill a 2D square distribution. (image by author)</figcaption></figure><p id="a902" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Getting intuition from space-filling curves, we can thus think of vector quantization (VQ) as mapping input data points on a space-filling curve (rather than only mapping data points exclusively on codebook vectors as what we do in normal VQ). Therefore, we incorporate vector quantization into space-filling curves, such that our proposed space-filling vector quantizer (SFVQ) [1] models a D-dimensional data distribution by continuous piece-wise linear curves whose corner points are vector quantization codebook vectors. The following figure illustrates VQ and SFVQ applied on a Gaussian distribution.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/b1c09ca46b6cb5e6418d27219253f3fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6lRaA640BswwQ-CtQ_PeA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Codebook vectors (blue points) of a vector quantizer, and a space-filling vector quantizer (curve in black) on a Gaussian distribution (gray points). Voronoi regions for VQ are shown in green. (image by author)</figcaption></figure><p id="e427" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For technical details on how to train SFVQ and how to map data points on SFVQ’s curve, please see section 2 in <a class="af ni" href="https://www.isca-archive.org/interspeech_2023/vali23_interspeech.pdf" rel="noopener ugc nofollow" target="_blank">our paper</a> [1].</p><p id="bd1d" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Note that when we train a normal VQ on a distribution, the adjacent codebook vectors that exists inside the learned codebook matrix can refer to totally different contents. For example, the first codebook element could refer to a vowel phone and the second one could refer to a silent part of speech signal. However, when we train SFVQ on a distribution, the learned codebook vectors will be located in an arranged form such that adjacent elements in the codebook matrix (i.e. adjacent codebook indices) will refer to similar contents in the distribution. We can use this property of SFVQ to interpret and explore the latent spaces in Deep Neural Networks (DNNs). As a typical example, in the following we will explain how we used our SFVQ method to interpret the latent space of a voice conversion model [4].</p><h2 id="08cb" class="oc od fq bf oe of og oh oi oj ok ol om mv on oo op mz oq or os nd ot ou ov ow bk">Voice Conversion</h2><p id="d5cc" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk">The following figure shows a voice conversion model [4] based on vector quantized variational autoencoder (VQ-VAE) [5] architecture. According to this model, encoder takes the speech signal of speaker A as the input and passes the output into vector quantization (VQ) block to extracts the phonetic information (phones) out of this speech signal. Then, these phonetic information together with the identity of speaker B goes into the decoder which outputs the converted speech signal. The converted speech would contain the phonetic information (context) of speaker A with the identity of speaker B.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pf"><img src="../Images/b9b1c1e72ab0b9cc21295fb4fa7a726d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFJl_WIHQCJYdK2FYHFJlQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Voice conversion model based on VQ-VAE architecture. (image by author)</figcaption></figure><p id="d05a" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this model, the VQ module acts as an information bottleneck that learns a discrete representation of speech that captures only phonetic content and discards the speaker-related information. In other words, VQ codebook vectors are expected to collect only the phone-related contents of the speech. Here, the representation of VQ output is considered the latent space of this model. Our objective is to replace the VQ module with our proposed SFVQ method to interpret the latent space. By interpretation we mean to figure out what phone each latent vector (codebook vector) corresponds to.</p><h2 id="8676" class="oc od fq bf oe of og oh oi oj ok ol om mv on oo op mz oq or os nd ot ou ov ow bk">Interpreting the Latent Space using SFVQ</h2><p id="3c95" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk">We evaluate the performance of our space-filling vector quantizer (SFVQ) on its ability to find the structure in the latent space (representing phonetic information) in the above voice conversion model. For our evaluations, we used the TIMIT dataset [6], since it contains phone-wise labeled data using the phone set from [7]. For our experiments, we use the following phonetic grouping:</p><ul class=""><li id="6927" class="ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh pg ph pi bk"><strong class="mo fr">Plosives (Stops):</strong> {p, b, t, d, k, g, jh, ch}</li><li id="1e60" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Fricatives:</strong> {f, v, th, dh, s, z, sh, zh, hh, hv}</li><li id="c412" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Nasals:</strong> {m, em, n, nx, ng, eng, en}</li><li id="42a6" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Vowels:</strong> {iy, ih, ix, eh, ae, aa, ao, ah, ax, ax-h, uh, uw, ux}</li><li id="2577" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Semi-vowels (Approximants):</strong> {l, el, r, er, axr, w, y}</li><li id="41c9" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Diphthongs:</strong> {ey, aw, ay, oy, ow}</li><li id="6cc7" class="ml mm fq mo b go pj mq mr gr pk mt mu mv pl mx my mz pm nb nc nd pn nf ng nh pg ph pi bk"><strong class="mo fr">Silence:</strong> {h#}.</li></ul><p id="bf1c" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To analyze the performance of our proposed SFVQ, we pass the labeled TIMIT speech files through the trained encoder and SFVQ modules, respectively, and extract the codebook vector indices corresponding to all existing phones in the speech. In other words, we pass a speech signal with labeled phones and then compute the index of the learned SFVQ’s codebook vector which those phones are getting mapped to them. As explained above, we expect our SFVQ to map similar phonetic contents next to each other (index-wise in the learned codebook matrix). To examine this expectation, in the following figure we visualize the <a class="af ni" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener ugc nofollow" target="_blank">spectrogram</a> of the sentence <em class="mn">“she had your dark suit”</em>, and its corresponding codebook vector indices for the ordinary vector quantizer (VQ) and our proposed SFVQ.</p></div></div><div class="nr"><div class="ab cb"><div class="ll po lm pp ln pq cf pr cg ps ci bh"><figure class="nm nn no np nq nr pu pv paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pt"><img src="../Images/090c06ffb8078ca531dbac910b0bea67.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ueUf5Zw6NIxMGUXEPHeTwQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx"><em class="pw">(Top) Codebook vector indices for the speech signal using our proposed SFVQ (in dark blue circles) and the ordinary VQ (in gray crosses). (Bottom) Spectrogram of the speech signal including codebook vector indices corresponding to speech frames. (image by author)</em></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="79e3" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We observe that the indices of the ordinary VQ does not have any particular structure. However, when using our proposed SFVQ, there is a clear structure for the codebook vector indices. The indices for the speech frames containing fricative phones of {sh, s} within the words {she, suit} are uniformly distributed next to each other throughout the frames. In addition, silence frames containing phone {h#} and some other low energy frames containing {kcl, tcl: k, t closures} within the words {dark, suit} are uniformly located next to each other in the range 0–20. Notice that the figure below remains sufficiently consistent for sentences with the same phonetic content, even across speakers with different genders, speech rhythms, and dialects.</p><p id="ad6b" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The figure below demonstrates the histogram of SFVQ’s codebook indices for each phonetic group (explained above) for the whole TIMIT speech files. At first glance, we observe that consonants:{silence, plosives, fricatives, nasals} and vowels:{vowels, diphthongs, approximants} can be separated around index 125 (apart from the peak near index 20). We also observe that the most prominent peaks of different groups are separated in different parts of the histogram.</p></div></div><div class="nr"><div class="ab cb"><div class="ll po lm pp ln pq cf pr cg ps ci bh"><figure class="nm nn no np nq nr pu pv paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk px"><img src="../Images/02c772787d2662cdd5edb31104c9ebd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*qNNHqkbI3gB2Ozd2-jbBuQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx"><em class="pw">Histogram of SFVQ’s codebook vector indices for different phonetic groups. (image by author)</em></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3e6b" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">By having this visualization, we have a better understanding of the latent space and we can now distinguish which part of the latent space refers to what phonetic group. We can even go further in details and explore the latent space in terms of phone level. As an example for distribution of phones within a phonetic group, the figure below illustrates the histograms of all phones in fricatives group.</p></div></div><div class="nr bh"><figure class="nm nn no np nq nr bh paragraph-image"><img src="../Images/54bdd8314ee51c54a2f70671b3d2f5ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*xcO5O0GARPzuzi902Cdqjg.png"/><figcaption class="nx ny nz nj nk oa ob bf b bg z dx"><em class="pw">Histogram of SFVQ’s codebook vector indices for fricative phones. (image by author)</em></figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="87af" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">By observing the most prominent peak for each phone, we find out the peaks of similar phones are located next to each other. To elaborate, we listed similar phones and their corresponding peak index here as {f:51, v:50}, {th:78, dh:80}, {s:71, z:67, sh:65, zh:67}, {hh:46, hv:50}. Except {hh, hv} phones, fricatives are mainly located in the range 50–85. Further structures can be readily identified from all provided figures by visual inspection.</p><p id="ebb0" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These experiments demonstrate that our proposed SFVQ achieves a coherently structured and easily interpretable representation for latent codebook vectors, which represent phonetic information of the input speech. Accordingly, there is an obvious distinction of various phonetic groupings such as {consonants vs. vowels}, {fricatives vs. nasals vs. diphthongs vs. …}, and we can simply tell apart which phone each codebook vector represents. In addition, similar phones within a specific phonetic group are encoded next to each other in the latent codebook space. This is the main interpretability that we aimed to obtain from a black-box called <em class="mn">latent space</em>.</p><p id="b38d" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">One advantage of SFVQ over other supervised approaches which tries to make the latent space interpretable is that SFVQ does not incur any human labeling and manual restrictions on the learned latent space. To make our method interpretable, it only requires the user to study the unsupervised learned latent space entirely once by observation. This observation needs much much less labeled data than what is necessary for supervised training of big models. <strong class="mo fr">Again we want to note that SFVQ is a generic tool for modeling distributions and using it is not restricted to any specific neural network architecture nor any data type (e.g. image, video, speech and etc.)</strong>.</p><h1 id="82f7" class="py od fq bf oe pz qa gq oi qb qc gt om qd qe qf qg qh qi qj qk ql qm qn qo qp bk">GitHub Repository</h1><p id="6116" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk">PyTorch implementation of our SFVQ technique is publicly available in GitHub using the following link:</p><div class="qq qr qs qt qu qv"><a href="https://github.com/MHVali/Space-Filling-Vector-Quantizer.git?source=post_page-----e4eb26691b14--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qw ab ig"><div class="qx ab co cb qy qz"><h2 class="bf fr hw z io ra iq ir rb it iv fp bk">GitHub - MHVali/Space-Filling-Vector-Quantizer</h2><div class="rc l"><h3 class="bf b hw z io ra iq ir rb it iv dx">Contribute to MHVali/Space-Filling-Vector-Quantizer development by creating an account on GitHub.</h3></div><div class="rd l"><p class="bf b dy z io ra iq ir rb it iv dx">github.com</p></div></div><div class="re l"><div class="rf l rg rh ri re rj lq qv"/></div></div></a></div></div></div></div><div class="ab cb rk rl rm rn" role="separator"><span class="ro by bm rp rq rr"/><span class="ro by bm rp rq rr"/><span class="ro by bm rp rq"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0504" class="oc od fq bf oe of og oh oi oj ok ol om mv on oo op mz oq or os nd ot ou ov ow bk">Acknowledgement</h2><p id="2400" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk">Special thanks to my doctoral program supervisor <a class="af ni" href="https://research.aalto.fi/en/persons/tom-bäckström" rel="noopener ugc nofollow" target="_blank">Prof. Tom Bäckström</a>, who supported me and was the other contributor for this work.</p><h1 id="b5c5" class="py od fq bf oe pz qa gq oi qb qc gt om qd qe qf qg qh qi qj qk ql qm qn qo qp bk">References</h1><p id="cc47" class="pw-post-body-paragraph ml mm fq mo b go ox mq mr gr oy mt mu mv oz mx my mz pa nb nc nd pb nf ng nh fj bk">[1] M.H. Vali, T. Bäckström, “Interpretable Latent Space Using Space-Filling Curves for Phonetic Analysis in Voice Conversion”, in <em class="mn">Proceedings of Interspeech</em>, 2023.</p><p id="95b9" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[2] M. H. Vali and T. Bäckström, “NSVQ: Noise Substitution in Vector Quantization for Machine Learning,” <em class="mn">IEEE Access</em>, 2022.</p><p id="42e5" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[3] H. Sagan, “Space-filling curves<em class="mn">”,</em> Springer Science &amp; Business Media, 2012.</p><p id="4adb" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[4] B. Van Niekerk, L. Nortje, and H. Kamper, “Vector-quantized neural networks for acoustic unit discovery in the Zerospeech 2020 challenge”, in <em class="mn">Proceedings of Interspeech</em>, 2020.</p><p id="cb2d" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[5] A. Van Den Oord, O. Vinyals, and K. Kavukcuoglu, “Neural Discrete Representation Learning,” in <em class="mn">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 2017.</p><p id="90dd" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[6] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren, “<em class="mn">The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM”,</em> Linguistic Data Consortium, 1993.</p><p id="06df" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">[7] C. Lopes and F. Perdigao, “Phoneme recognition on the TIMIT database”, in <em class="mn">Speech Technologies</em>. IntechOpen, 2011, ch. 14. [Online]. Available: <a class="af ni" href="https://doi.org/10.5772/17600" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.5772/17600</a></p></div></div></div></div>    
</body>
</html>