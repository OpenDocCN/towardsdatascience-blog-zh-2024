- en: 'Transformers: From NLP to Computer Vision'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05](https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Transformer architecture has been adapted to computer vision tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    ·7 min read·May 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/137ff68fa22ff9d7db4503dc87f98d3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [kyler trautner](https://unsplash.com/@kylertrautner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2017, the paper “Attention is all you need” [1] took the NLP research community
    by storm. Cited more than 100,000 times so far, its Transformer has become the
    cornerstone of most major NLP architectures nowadays. To learn about Transformers’
    notable works in NLP, you can take a look at my previous post here.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
    [## BERT vs GPT: Comparing the NLP Giants'
  prefs: []
  type: TYPE_NORMAL
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, as Computer Vision (CV) has long been dominated by CNN, Transformer
    applications in the field have remained limited until recently. In this article,
    we will discuss the challenges of applying Transformers to computer vision and
    how CV researchers have adapted them.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenizing a text sequence has been long researched, using various optimizations
    to generalize and adapt to unseen texts. All of these efforts, however, rely on
    the idea of considering characters and terms as units.
  prefs: []
  type: TYPE_NORMAL
