- en: 'Transformers: From NLP to Computer Vision'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers：从自然语言处理到计算机视觉
- en: 原文：[https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05](https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05](https://towardsdatascience.com/transformers-from-nlp-to-computer-vision-4f237386610c?source=collection_archive---------1-----------------------#2024-05-05)
- en: How Transformer architecture has been adapted to computer vision tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer架构是如何适应计算机视觉任务的
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page---byline--4f237386610c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    ·7 min read·May 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4f237386610c--------------------------------)
    ·阅读时间：7分钟·2024年5月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/137ff68fa22ff9d7db4503dc87f98d3a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/137ff68fa22ff9d7db4503dc87f98d3a.png)'
- en: Photo by [kyler trautner](https://unsplash.com/@kylertrautner?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[kyler trautner](https://unsplash.com/@kylertrautner?utm_source=medium&utm_medium=referral)拍摄，图片来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In 2017, the paper “Attention is all you need” [1] took the NLP research community
    by storm. Cited more than 100,000 times so far, its Transformer has become the
    cornerstone of most major NLP architectures nowadays. To learn about Transformers’
    notable works in NLP, you can take a look at my previous post here.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，论文《Attention is all you need》[1]引起了NLP研究界的轰动。至今已被引用超过10万次，它的Transformer成为了如今大多数主流NLP架构的基石。要了解Transformer在NLP领域的重要成果，可以查看我之前的文章。
- en: '[](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
    [## BERT vs GPT: Comparing the NLP Giants'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
    [## BERT与GPT：比较NLP巨头'
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它们的结构有何不同，这些差异如何影响模型的能力？
- en: towardsdatascience.com](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec?source=post_page-----4f237386610c--------------------------------)
- en: Meanwhile, as Computer Vision (CV) has long been dominated by CNN, Transformer
    applications in the field have remained limited until recently. In this article,
    we will discuss the challenges of applying Transformers to computer vision and
    how CV researchers have adapted them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，由于计算机视觉（CV）长期以来由CNN主导，Transformer在该领域的应用直到最近才有所突破。本文将讨论将Transformer应用于计算机视觉所面临的挑战，以及计算机视觉研究人员如何调整这些模型。
- en: Challenges
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: Tokenization
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenizing a text sequence has been long researched, using various optimizations
    to generalize and adapt to unseen texts. All of these efforts, however, rely on
    the idea of considering characters and terms as units.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 分词文本序列已经被长期研究，采用各种优化方法来概括和适应未见过的文本。然而，所有这些努力都依赖于将字符和术语作为单位来处理的思想。
