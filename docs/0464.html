<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Editing Text in Images with AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Editing Text in Images with AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18">https://towardsdatascience.com/editing-text-in-images-with-ai-03dee75d8b9c?source=collection_archive---------5-----------------------#2024-02-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4b16" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Research Review for Scene Text Editing: STEFANN, SRNet, TextDiffuser, AnyText and more.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Julia Turc" class="l ep by dd de cx" src="../Images/1ca27d7db36799dec53b8daf4099f5cb.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kY2xovOt_cQKjc9h9dYeRQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@turc.raluca?source=post_page---byline--03dee75d8b9c--------------------------------" rel="noopener follow">Julia Turc</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--03dee75d8b9c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="abeb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you ever tried to change the text in an image, you know it’s not trivial. Preserving the background, textures, and shadows takes a Photoshop license and hard-earned designer skills. In the video below, a Photoshop expert takes 13 minutes to fix a few misspelled characters in a poster that is not even stylistically complex. The good news is — in our relentless pursuit of AGI, humanity is also building AI models that are actually useful in real life. Like the ones that allow us to edit text in images with minimal effort.</p><figure class="ne nf ng nh ni nj"><div class="nk io l ed"><div class="nl nm l"/></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Photoshop expert manually editing an AI-generated image to correctly spell “The Midnight City”, taking them more than 13 minutes.</figcaption></figure><p id="9601" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The task of automatically updating the text in an image is formally known as <em class="nu">Scene Text Editing (STE)</em>. This article describes how STE model architectures have evolved over time and the capabilities they have unlocked. We will also talk about their limitations and the work that remains to be done. Prior familiarity with <a class="af nv" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">GANs</a> and <a class="af nv" href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">Diffusion models</a> will be helpful, but not strictly necessary.</p><p id="a999" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="nu">Disclaimer: I am the cofounder of </em><a class="af nv" href="https://storia.ai/?utm_source=article&amp;utm_medium=medium.com&amp;utm_campaign=editing-text-in-images-with-ai" rel="noopener ugc nofollow"><em class="nu">Storia AI</em></a><em class="nu">, building an AI copilot for visual editing. This literature review was done as part of developing </em><a class="af nv" href="https://storia.ai/textify?utm_source=article&amp;utm_medium=medium.com&amp;utm_campaign=editing-text-in-images-with-ai" rel="noopener ugc nofollow" target="_blank"><em class="nu">Textify</em></a><em class="nu">, a feature that allows users to seamlessly change text in images. While Textify is closed-source, we open-sourced a related library, </em><a class="af nv" href="https://github.com/iuliaturc/detextify" rel="noopener ugc nofollow" target="_blank"><em class="nu">Detextify</em></a><em class="nu">, which automatically removes text from a corpus of images.</em></p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr nw"><img src="../Images/5de7cdf4ca0f5f49118fc1da30bd6b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1faHKrDW96xnFpVUHrEfkQ.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Example of Scene Text Editing (STE). The original image (left) was generated via <a class="af nv" href="https://midjourney.com" rel="noopener ugc nofollow" target="_blank">Midjourney</a>. We used <a class="af nv" href="https://storia.ai/textify?utm_source=article&amp;utm_medium=medium.com&amp;utm_campaign=editing-text-in-images-with-ai" rel="noopener ugc nofollow" target="_blank">Textify</a> to annotate the image (center) and automatically fix the misspelling (right).</figcaption></figure><h1 id="0566" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">The Task of Scene Text Editing (STE)</h1><h2 id="542a" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">Definition</h2><p id="bf84" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">Scene Text Editing (STE) is the task of automatically modifying text in images that capture a visual scene (as opposed to images that mainly contain text, such as scanned documents). The goal is to change the text while preserving the original aesthetics (typography, calligraphy, background etc.) without the inevitably expensive human labor.</p><h2 id="a8e1" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">Use Cases</h2><p id="2164" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">Scene Text Editing might seem like a contrived task, but it actually has multiple practical uses cases:</p><p id="4a1d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">(1) Synthetic data generation for Scene Text Recognition (STR)</strong></p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr pu"><img src="../Images/a498baa12f1714aff00ab77313720739.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TPVZg7jQCfUQLH7dJU170w.jpeg"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Synthetic image (right) obtained by editing text in the original image (left, from <a class="af nv" href="https://unsplash.com/photos/no-trespassing-bn-rr-sign-i5O8-90L2P8" rel="noopener ugc nofollow" target="_blank">Unsplash</a>). This technique can be used to augment the training set of STR (Scene Text Recognition) models.</figcaption></figure><p id="a3e7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When I started researching this task, I was surprised to discover that <a class="af nv" href="https://www.alibaba.com/" rel="noopener ugc nofollow" target="_blank">Alibaba</a> (an e-commerce platform) and <a class="af nv" href="https://www.baidu.com/" rel="noopener ugc nofollow" target="_blank">Baidu</a> (a search engine) are consistently publishing research on STE.</p><p id="1e4b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At least in Alibaba’s case, it is likely their research is in support of <a class="af nv" href="https://www.alibabagroup.com/en-US/about-alibaba-businesses-1496655358913937408" rel="noopener ugc nofollow" target="_blank">AMAP</a>, their alternative to Google Maps [<a class="af nv" href="https://www.alibabacloud.com/blog/evolution-of-text-recognition-in-amap-data-production_596817" rel="noopener ugc nofollow" target="_blank">source</a>]. In order to map the world, you need a robust text recognition system that can read traffic and street signs in a variety of fonts, under various real-world conditions like occlusions or geometric distortions, potentially in multiple languages.</p><p id="ccd5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In order to build a training set for Scene Text Recognition, one could collect real-world data and have it annotated by humans. But this approach is bottlenecked by human labor, and might not guarantee enough data variety. Instead, synthetic data generation provides a virtually unlimited source of diverse data, with automatic labels.</p><p id="7697" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">(2) Control over AI-generated images</strong></p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr pv"><img src="../Images/c08c09ea6d644cc87485b3a9578120d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XR6mdDsvO_jgp8o9A57JCg.jpeg"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">AI-generated image via Midjourney (left) and corrected via Scene Text Editing.</figcaption></figure><p id="9033" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">AI image generators like <a class="af nv" href="http://midjourney.com" rel="noopener ugc nofollow" target="_blank">Midjourney</a>, <a class="af nv" href="http://stability.ai" rel="noopener ugc nofollow" target="_blank">Stability</a> and <a class="af nv" href="http://leonardo.ai" rel="noopener ugc nofollow" target="_blank">Leonardo</a> have democratized visual asset creation. Small business owners and social media marketers can now create images without the help of an artist or a designer by simply typing a text prompt. However, the text-to-image paradigm lacks the controllability needed for practical assets that go beyond concept art — event posters, advertisements, or social media posts.</p><p id="bc87" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Such assets often need to include textual information (a date and time, contact details, or the name of the company). Spelling correctly has been historically difficult for text-to-image models, though there has been recent process — <a class="af nv" href="https://github.com/deep-floyd/IF?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">DeepFloyd IF</a>, <a class="af nv" href="https://www.reddit.com/r/midjourney/comments/18p1jwp/midjourney_v6_can_now_do_text/" rel="noopener ugc nofollow" target="_blank">Midjourney v6</a>. But even when these models do eventually learn to spell perfectly, the UX constraints of the text-to-image interface remain. It is tedious to describe in words where and how to place a piece of text.</p><p id="d3fa" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">(3) Automatic localization of visual media</strong></p><p id="0201" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Movies and games are often localized for various geographies. Sometimes this might entail <a class="af nv" href="https://www.slashfilm.com/727914/why-inside-out-changed-so-many-scenes-for-its-overseas-release/" rel="noopener ugc nofollow" target="_blank">switching a broccoli for a green pepper</a>, but most times it requires translating the text that is visible on screen. With other aspects of the film and gaming industries getting automated (like <a class="af nv" href="https://flawlessai.com" rel="noopener ugc nofollow" target="_blank">dubbing and lip sync</a>), there is no reason for visual text editing to remain manual.</p><h1 id="d61d" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Timeline of Architectures: from GANs to Diffusion</h1><p id="b151" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">The training techniques and model architectures used for Scene Text Editing largely follow the trends of the larger task of image generation.</p><h2 id="ec5e" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">The GAN Era (2019–2021)</h2><p id="a435" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk"><a class="af nv" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">GANs</a> (Generative Adversarial Networks) dominated the mid-2010s for image generation tasks. GAN refers to a particular training framework (rather than prescribing a model architecture) that is adversarial in nature. A <em class="nu">generator </em>model is trained to capture the data distribution (and thus has the capability to <em class="nu">generate </em>new data), while a <em class="nu">discriminator </em>is trained to distinguish the output of the generator from real data. The training process is finalized when the discriminator’s guess is as good as a random coin toss. During inference, the discriminator is discarded.</p><p id="c5f1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">GANs are particularly suited for image generation because they can perform unsupervised learning — that is, learn the data distribution without requiring labeled data. Following the general trend of image generation, the initial Scene Text Editing models also leveraged GANs.</p><h2 id="903c" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">GAN Epoch #1: Character-Level Editing — STEFANN</h2><blockquote class="pw"><p id="e8a5" class="px py fq bf pz qa qb qc qd qe qf nd dx"><a class="af nv" href="https://arxiv.org/pdf/1903.01192.pdf" rel="noopener ugc nofollow" target="_blank">STEFANN</a>, recognized as the first work to modify text in scene images, operates at a character level. The character editing problem is broken into two: font adaptation and color adaptation.</p></blockquote><figure class="qh qi qj qk ql nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr qg"><img src="../Images/4f250c074811c2ea4cc9d4ce62a04875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06NUXZMag34teZKcJDJxYQ.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">The <a class="af nv" href="https://arxiv.org/pdf/1903.01192.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">STEFANN</strong></a> model architecture (<a class="af nv" href="https://prasunroy.github.io/stefann/" rel="noopener ugc nofollow" target="_blank">source</a>). The character editing task is broken into two: FANnet (Font Adaptation Network) generates a black-and-white target character in the desired shape, and Colornet fills in the appropriate color.</figcaption></figure><p id="dd5c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af nv" href="https://arxiv.org/pdf/1903.01192.pdf" rel="noopener ugc nofollow" target="_blank">STEFANN</a> is recognized as the first work to modify text in scene images. It builds on prior work in the space of <em class="nu">font synthesis </em>(the task of creating new fonts or text styles that closely resemble the ones observed in input data), and adds the constraint that the output needs to blend seamlessly back into the original image. Compared to previous work, STEFANN takes a pure machine learning approach (as opposed to e.g. explicit geometrical modeling) and does not depend on character recognition to label the source character.</p><p id="4afc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The STEFANN model architecture is based on <a class="af nv" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">CNN</a>s (Convolutional Neural Networks) and decomposes the problem into (1) <em class="nu">font adaptation</em> via FANnet — turning a binarized version of the source character into a binarized target character, (2) <em class="nu">color adaptation</em> via Colornet — colorizing the output of FANnet to match the rest of the text in the image, and (3) <em class="nu">character placement</em> — blending the target character back into the original image using previously-established techniques like <a class="af nv" href="https://www.olivier-augereau.com/docs/2004JGraphToolsTelea.pdf" rel="noopener ugc nofollow" target="_blank">inpainting</a> and <a class="af nv" href="https://perso.crans.org/frenoy/matlab2012/seamcarving.pdf" rel="noopener ugc nofollow" target="_blank">seam carving</a>. The first two modules are trained with a GAN objective.</p><figure class="ne nf ng nh ni nj"><div class="nk io l ed"><div class="nl nm l"/></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Official STEFANN demo made by its authors.</figcaption></figure><p id="67a1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While STEFANN paved the way for Scene Text Editing, it has multiple limitations that restrict its use in practice. It can only operate on one character at a time; changing an entire word requires multiple calls (one per letter) and constrains the target word to have the same length as the source word. Also, the character placement algorithm in step (3) assumes that the characters are non-overlapping.</p><h2 id="7d1b" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">GAN Epoch #2: Word-Level Editing — SRNet and 3-Module Networks</h2><blockquote class="pw"><p id="0fd4" class="px py fq bf pz qa qb qc qd qe qf nd dx"><a class="af nv" href="https://arxiv.org/abs/1908.03047" rel="noopener ugc nofollow" target="_blank"><strong class="al">SRNet</strong></a> was the first model to perform scene text editing <strong class="al">at the word level</strong>. SRNet decomposed the STE task into three (jointly-trained) modules: text conversion, background inpainting and fusion.</p></blockquote><figure class="qh qi qj qk ql nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr qm"><img src="../Images/1d9862f8ca14adb12030871024577166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgsPcaBrOQs1XQXHJXxSWQ.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">The <a class="af nv" href="https://arxiv.org/abs/1908.03047" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">SRNet</strong></a> model architecture. The three modules decompose the STE problem into smaller building blocks (text conversion, background inpainting and fusion), while being jointly trained. This architecture was largely adopted by follow-up work in the field.</figcaption></figure><p id="6194" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af nv" href="https://arxiv.org/abs/1908.03047" rel="noopener ugc nofollow" target="_blank">SRNet</a> was the first model to perform scene text editing at the word level. SRNet decomposed the STE task into three (jointly-trained) modules:</p><ol class=""><li id="8e07" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qn qo qp bk"><strong class="mk fr">The text conversion module </strong>(in blue) takes a programatic rendering of the target text (“barbarous” in the figure above) and aims to render it in the same typeface as the input word (“introduce”) on a plain background.</li><li id="2a5b" class="mi mj fq mk b go qq mm mn gr qr mp mq mr qs mt mu mv qt mx my mz qu nb nc nd qn qo qp bk"><strong class="mk fr">The background inpainting module </strong>(in green) erases the text from the input image and fills in the gaps to reconstruct the original background.</li><li id="b221" class="mi mj fq mk b go qq mm mn gr qr mp mq mr qs mt mu mv qt mx my mz qu nb nc nd qn qo qp bk"><strong class="mk fr">The fusion module</strong> (in orange) pastes the rendered target text onto the background.</li></ol><p id="7da2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">SRNet architecture. </strong>All three modules are flavors of Fully Convolutional Networks (FCNs), with the background inpainting module in particular resembling <a class="af nv" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">U-Net</a> (an FCN with the specific property that encoder layers are skip-connected to decoder layers of the same size).</p><p id="b974" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">SRNet training. </strong>Each module has its own loss, and the network is jointly trained on the sum of losses (<em class="nu">LT + LB + LF</em>), where the latter two are trained via GAN. While this modularization is conceptually elegant, it comes with the drawback of requiring paired training data, with supervision for each intermediate step. Realistically, this can only be achieved with artificial data. For each data point, one chooses a random image (from a dataset like <a class="af nv" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">COCO</a>), selects two arbitrary words from a dictionary, and renders them with an arbitrary typeface to simulate the “before” and “after” images. As a consequence, the training set doesn’t include any photorealistic examples (though it can somewhat generalize beyond rendered fonts).</p><p id="4efa" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Honorable mentions. </strong><a class="af nv" href="https://arxiv.org/abs/2003.08152" rel="noopener ugc nofollow" target="_blank">SwapText</a> followed the same GAN-based 3-module network approach to Scene Text Editing and proposed improvements to the text conversion module.</p><h2 id="d931" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">GAN Epoch #3: Self-supervised and Hybrid Networks</h2><p id="6fa6" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk"><strong class="mk fr">Leap to unsupervised learning. </strong>The next leap in STE research was to adopt a self-supervised training approach, where models are trained on unpaired data (i.e., a mere repository of images containing text). To achieve this, one had to remove the label-dependent intermediate losses LT and LB. And due to the design of GANs, the remaining final loss does not require a label either; the model is simply trained on the <em class="nu">discriminator</em>’s ability to distinguish between real images and the ones produced by the <em class="nu">generator</em>. <a class="af nv" href="https://arxiv.org/abs/2106.08385" rel="noopener ugc nofollow" target="_blank">TextStyleBrush</a> pioneered self-supervised training for STE, while <a class="af nv" href="https://arxiv.org/pdf/2107.11041.pdf" rel="noopener ugc nofollow" target="_blank">RewriteNet</a> and <a class="af nv" href="https://arxiv.org/abs/2212.01982" rel="noopener ugc nofollow" target="_blank">MOSTEL</a> made the best of both worlds by training in two stages: one supervised (advantage: abundance of synthetic labeled data) and one self-supervised (advantage: realism of natural unlabeled data).</p><p id="f936" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Disentangling text content &amp; style. </strong>To remove the intermediate losses, <a class="af nv" href="https://arxiv.org/abs/2106.08385" rel="noopener ugc nofollow" target="_blank">TextStyleBrush</a> and <a class="af nv" href="https://arxiv.org/pdf/2107.11041.pdf" rel="noopener ugc nofollow" target="_blank">RewriteNet</a> reframe the problem into disentangling <em class="nu">text content </em>from <em class="nu">text style. </em>To reiterate, the inputs to an STE system are (a) an image with original text, and (b) the desired text — more specifically, a programatic rendering of the desired text on a white or gray background, with a fixed font like Arial. The goal is to combine the <em class="nu">style </em>from (a) with the <em class="nu">content </em>from (b). In other words, we complementarily aim to discard the <em class="nu">content </em>from (a) and the <em class="nu">style </em>of (b). This is why it’s necessary to disentangle the text <em class="nu">content </em>from the <em class="nu">style </em>in a given image.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div class="nq nr qv"><img src="../Images/fd703fb961b78b3b5369e2ed6cb4e196.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*m1iXKBC1UjPGTEGu1iPQvg.png"/></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Inference architecture of <a class="af nv" href="https://arxiv.org/pdf/2107.11041.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">RewriteNet</strong></a>. The encoder E disentangles text style (circle) from text content (triangle). The style embedding from the original image and content embedding from the text rendering are fed into a generator, which fuses the two into an output image.</figcaption></figure><p id="29df" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">TextStyleBrush and why GANs went out of fashion</strong>. While the idea of disentangling text content from style is straightforward, achieving it in practice required complicated architectures. <a class="af nv" href="https://arxiv.org/abs/2106.08385" rel="noopener ugc nofollow" target="_blank">TextStyleBrush</a>, the most prominent paper in this category, used no less than <em class="nu">seven </em>jointly-trained subnetworks, a pre-trained typeface classifier, a pre-trained OCR model and multiple losses. Designing such a system must have been expensive, since all of these components require ablation studies to determine their effect. This, coupled with the fact that GANs are <a class="af nv" href="https://machinelearningmastery.com/how-to-train-stable-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">notoriously difficult to train</a> (in theory, the generator and discriminator need to reach Nash equilibrium), made STE researchers eager to switch to diffusion models once they proved so apt for image generation.</p><h2 id="a93c" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">The Diffusion Era (2022 — present)</h2><p id="055f" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">At the beginning of 2022, the image generation world shifted away from GANs towards <a class="af nv" href="https://arxiv.org/abs/2112.10752" rel="noopener ugc nofollow" target="_blank">Latent Diffusion Models</a> (LDM). A comprehensive explanation of LDMs is out of scope here, but you can refer to <a class="af nv" href="https://jalammar.github.io/illustrated-stable-diffusion/" rel="noopener ugc nofollow" target="_blank">The Illustrated Stable Diffusion</a> for an excellent tutorial. Here I will focus on the parts of the LDM architecture that are most relevant to the Scene Text Editing task.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr qw"><img src="../Images/0d1c9a8a512ff1ac45126a9d4427c3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ewvvvdJbCsMhTZHZ2xo9RQ.jpeg"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Diffusion-based Scene Text Editing. In addition to the text embedding passed to the actual diffusion module in a standard text-to-image-model, STE architectures also create embeddings that reflect desired properties of the target text (position, shape, style etc.). Illustration by the author.</figcaption></figure><p id="bc54" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As illustrated above, an LDM-based text-to-image model has three main components: (1) a text encoder — typically <a class="af nv" href="http://CLIP" rel="noopener ugc nofollow" target="_blank">CLIP</a>, (2) the actual diffusion module — which converts the text embedding into an image embedding in latent space, and (3) an image decoder — which upscales the latent image into a fully-sized image.</p><h2 id="0631" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">Scene Text Editing as a Diffusion Inpainting Task</h2><p id="a9e2" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">Text-to-image is not the only paradigm supported by diffusion models. After all, <a class="af nv" href="http://CLIP" rel="noopener ugc nofollow" target="_blank">CLIP</a> is equally a text <em class="nu">and </em>image encoder, so the embedding passed to the <em class="nu">image information creator </em>module can also encode an image. In fact, it can encode any modality, or a concatenation of multiple inputs.</p><p id="7c2a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is the principle behind <strong class="mk fr">inpainting</strong>, the task of modifying only a subregion of an input image based on given instructions, in a way that looks coherent with the rest of the image. The <em class="nu">image information creator</em> ingests an encoding that captures the input image, the mask of the region to be inpainted, and a textual instruction.</p><p id="0ead" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Scene Text Editing can be regarded as a specialized form of inpainting. Most of the STE research reduces to the following question: <em class="nu">How can we augment the text embedding with additional information about the task (i.e., the original image, the desired text and its positioning, etc.)? </em>Formally, this is known as <strong class="mk fr">conditional guidance.</strong></p><p id="0e53" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The research papers that fall into this bucket (<a class="af nv" href="https://arxiv.org/abs/2305.10855" rel="noopener ugc nofollow" target="_blank">TextDiffuser</a>, <a class="af nv" href="https://arxiv.org/abs/2311.16465" rel="noopener ugc nofollow" target="_blank">TextDiffuser 2</a>, <a class="af nv" href="https://arxiv.org/abs/2303.17870" rel="noopener ugc nofollow" target="_blank">GlyphDraw</a>, <a class="af nv" href="https://arxiv.org/abs/2311.03054" rel="noopener ugc nofollow" target="_blank">AnyText</a>, etc.) propose various forms of conditional guidance.</p><h2 id="c381" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">Positional guidance</h2><p id="b950" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">Evidently, there needs to be a way of specifying <em class="nu">where </em>to make changes to the original image. This can be a text instruction (e.g. “Change the title at the bottom”), a granular indication of the text line, or more fine-grained positional information for each target character.</p><p id="b514" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Positional guidance via image masks. </strong>One way of indicating the desired text position is via grayscale mask images, which can then be encoded into latent space via CLIP or an alternative image encoder. For instance, the <a class="af nv" href="https://arxiv.org/pdf/2305.10825.pdf" rel="noopener ugc nofollow" target="_blank">DiffUTE</a> model simply uses a black image with a white strip indicating the desired text location.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr qx"><img src="../Images/7213f885e9cb32a9d73ee11b9313bc37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lf6LljLqBowsDeXZkKHIbQ.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Input to the <a class="af nv" href="https://arxiv.org/pdf/2305.10825.pdf" rel="noopener ugc nofollow" target="_blank">DiffUTE</a> model. Positional guidance is achieved via the mask m and the masked input xm. These are deterministically rendered based on user input.</figcaption></figure><p id="f503" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af nv" href="https://arxiv.org/pdf/2305.10855.pdf" rel="noopener ugc nofollow" target="_blank">TextDiffuser</a> produces character-level segmentation masks: first, it roughly renders the desired text in the right position (black text in Arial font on a white image), then passes this rendering through a segmenter to obtain a grayscale image with individual bounding boxes for each character. The segmenter is a <a class="af nv" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">U-Net</a> model trained separately from the main network on 4M of synthetic instances.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div class="nq nr qy"><img src="../Images/3866a0aa4caa4da3fe8b5c9bc2cc6f84.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*OaBNf-6sn5nAZYtHLAxFog.png"/></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Character-level segmentation mask used by <a class="af nv" href="https://arxiv.org/pdf/2305.10855.pdf" rel="noopener ugc nofollow" target="_blank">TextDiffuser</a>. The target word (“WORK”) is rendered with a standard font on a white background, then passed through a segmenter (U-Net) to obtain the grayscale mask.</figcaption></figure><p id="d28d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Positional guidance via language modeling</strong>. In <a class="af nv" href="https://arxiv.org/abs/2206.07669" rel="noopener ugc nofollow" target="_blank">A Unified Sequence Inference for Vision Tasks</a>, the authors show that large language models (LLMs) can act as effective descriptors of object positions within an image by simply generating numerical tokens. Arguably, this was an unintuitive discovery. Since LLMs learn language based on statistical frequency (i.e., by observing how often tokens occur in the same context), it feels unrealistic to expect them to generate the right numerical tokens. But the massive scale of current LLMs often defies our expectations nowadays.</p><p id="fb02" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af nv" href="https://arxiv.org/pdf/2311.16465.pdf" rel="noopener ugc nofollow" target="_blank">TextDiffuser 2</a> leverage this discovery in an interesting way. They fine-tune an LLM on a synthetic corpus of &lt;text, OCR detection&gt; pairs, teaching it to generate the top-left and bottom-right coordinates of text bounding boxes, as show in the figure below. Notably, they decide to generate bounding boxes for text <em class="nu">lines </em>(as opposed to <em class="nu">characters</em>), giving the image generator more flexibility. They also run an interesting ablation study that uses a single point to encode text position (either top-left or center of the box), but observe poorer spelling performance — the model often hallucinates additional characters when not explicitly told where the text should end.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr qz"><img src="../Images/af4fe1ce054d35c906c275c657241da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQrlcxcwqN0Regll62IqFA.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">Architecture of <a class="af nv" href="https://arxiv.org/pdf/2311.16465.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">TextDiffuser 2</strong></a>. The language model M1 takes the target text from the user, then splits it into lines and predicts their positions as [x1] [y1] [x2] [y2] tokens. The language model M2 is a fine-tuned version of CLIP that encodes the modified prompt (which includes text lines and their positions) into latent space.</figcaption></figure><h2 id="ed3e" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">Glyph guidance</h2><p id="3c76" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">In addition to position, another piece of information that can be fed into the image generator is <em class="nu">the shape </em>of the characters. One could argue that shape information is redundant. After all, when we prompt a text-to-image model to generate a flamingo, we generally don’t need to pass any additional information about its long legs or the color of its feathers — the model has presumably learnt these details from the training data. However, in practice, the trainings sets (such as Stable Diffusion’s <a class="af nv" href="https://laion.ai/blog/laion-5b/" rel="noopener ugc nofollow" target="_blank">LAION-5B</a>) are dominated by natural pictures, in which text is underrepresented (and non-Latin scripts even more so).</p><p id="1dd2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Multiple studies (<a class="af nv" href="https://arxiv.org/pdf/2305.10825.pdf" rel="noopener ugc nofollow" target="_blank">DiffUTE</a>, <a class="af nv" href="https://arxiv.org/abs/2305.18259" rel="noopener ugc nofollow" target="_blank">GlyphControl</a>, <a class="af nv" href="http://GlyphDraw" rel="noopener ugc nofollow" target="_blank">GlyphDraw</a>, <a class="af nv" href="https://arxiv.org/pdf/2304.12519.pdf" rel="noopener ugc nofollow" target="_blank">GlyphDiffusion</a>, <a class="af nv" href="https://arxiv.org/pdf/2311.03054.pdf" rel="noopener ugc nofollow" target="_blank">AnyText</a> etc.) attempt to make up for this imbalance via explicit <strong class="mk fr">glyph guidance</strong> — effectively rendering the glyphs programmatically with a standard font, and then passing an encoding of the rendering to the image generator. Some simply place the glyphs in the center of the additional image, some close to the target positions (reminiscent of <a class="af nv" href="https://arxiv.org/abs/2302.05543" rel="noopener ugc nofollow" target="_blank">ControlNet</a>).</p><h2 id="bb23" class="oy od fq bf oe oz pa pb oh pc pd pe ok mr pf pg ph mv pi pj pk mz pl pm pn po bk">STE via Diffusion is (Still) Complicated</h2><p id="8d64" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">While the training process for diffusion models is more stable than GANs, the diffusion architectures for STE in particular are still quite complicated. The figure below shows the <a class="af nv" href="https://arxiv.org/pdf/2311.03054.pdf" rel="noopener ugc nofollow" target="_blank">AnyText</a> architecture, which includes (1) an auxiliary latent module (including the positional and glyph guidance discussed above), (2) a text embedding module that, among other components, requires a pre-trained OCR module, and (3) the standard diffusion pipeline for image generation. It is hard to argue this is conceptually much simpler than the GAN-based <a class="af nv" href="https://arxiv.org/abs/2106.08385" rel="noopener ugc nofollow" target="_blank">TextStyleBrush</a>.</p><figure class="ne nf ng nh ni nj nq nr paragraph-image"><div role="button" tabindex="0" class="nx ny ed nz bh oa"><div class="nq nr ra"><img src="../Images/f9e6d8990fefceee87e09e11b7c48702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lcstyzDupzuQQve-uw73Jg.png"/></div></div><figcaption class="nn no np nq nr ns nt bf b bg z dx">The (complex) architecture of <a class="af nv" href="https://arxiv.org/pdf/2311.03054.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">AnyText</strong></a><strong class="bf oe">.</strong></figcaption></figure><h1 id="230e" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">The Future of Scene Text Editing</h1><p id="026e" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">When the status quo is too complicated, we have a natural tendency to keep working on it until it converges to a clean solution. In a way, this is what happened to the natural language processing field: computational linguistics theories, grammars, dependency parsing — all collapsed under Transformers, which make a very simple statement: <em class="nu">the meaning of a token depends on all others around it.</em> Evidently, Scene Text Editing is miles away from this clarity. Architectures contain many jointly-trained subnetworks, pre-trained components, and require specific training data.</p><p id="1110" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Text-to-image models will inevitably become better at certain aspects of text generation (spelling, typeface diversity, and how crisp the characters look), with the right amount and quality of training data. But controllability will remain a problem for a much longer time. And even when models do eventually learn to follow your instructions to the t, the text-to-image paradigm might still be a subpar user experience — would you rather describe the position, look and feel of a piece of text in excruciating detail, or would you rather just draw an approximate box and choose an inspiration color from a color picker?</p><h1 id="e5cd" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Epilogue: Preventing Abuse</h1><p id="2c82" class="pw-post-body-paragraph mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd fj bk">Generative AI has brought to light many ethical questions, from authorship / copyright / licensing to authenticity and misinformation. While all these loom large in our common psyche and manifest in various abstract ways, the misuses of Scene Text Editing are down-to-earth and obvious — people faking documents.</p><p id="0ca0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While building <a class="af nv" href="https://storia.ai/textify?utm_source=article&amp;utm_medium=medium.com&amp;utm_campaign=editing-text-in-images-with-ai" rel="noopener ugc nofollow" target="_blank"><em class="nu">Textify</em></a>, we’ve seen it all. Some people bump up their follower count in Instagram screenshots. Others increase their running speed in Strava screenshots. And yes, some attempt to fake IDs, credit cards and diplomas. The temporary solution is to build classifiers for certain types of documents and simply refuse to edit them, but, long-term the generative AI community needs to invest in automated ways of determining document authenticity, be it a text snippet, an image or a video.</p></div></div></div></div>    
</body>
</html>