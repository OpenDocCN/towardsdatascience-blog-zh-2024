<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Optimizing the Data Processing Performance in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Optimizing the Data Processing Performance in PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-the-data-processing-performance-in-pyspark-4b895857c8aa?source=collection_archive---------3-----------------------#2024-11-07">https://towardsdatascience.com/optimizing-the-data-processing-performance-in-pyspark-4b895857c8aa?source=collection_archive---------3-----------------------#2024-11-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="94cb" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">PySpark techniques and strategies to tackle common performance challenges: A practical walkthrough</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@johnleungTJ?source=post_page---byline--4b895857c8aa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="John Leung" class="l ep by dd de cx" src="../Images/ef45063e759e3450fa7f3c32b2f292c3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-zL9bLkxy32p8chXW888zQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4b895857c8aa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@johnleungTJ?source=post_page---byline--4b895857c8aa--------------------------------" rel="noopener follow">John Leung</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4b895857c8aa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="2857" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">Apache Spark</a> has been one of the leading analytical engines in recent years due to its power in distributed data processing. PySpark, the Python API for Spark, is often used for personal and enterprise projects to address data challenges. For example, we can efficiently implement <a class="af nf" rel="noopener" target="_blank" href="/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287">feature engineering for time-series data</a> using PySpark, including ingestion, extraction, and visualization. However, despite its capacity to handle large datasets, performance bottlenecks can still arise under various scenarios such as extreme data distribution and complex data transformation workflow.</p><p id="dabf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This article will examine different common performance issues in data processing with PySpark on <a class="af nf" href="https://www.databricks.com/" rel="noopener ugc nofollow" target="_blank">Databricks</a>, and walk through various strategies for fine-tuning to achieve faster execution.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/7a51dfd6e0bb834e68f2dbd4ac63cace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bHGZL-UCrJKXBSmB"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Photo by <a class="af nf" href="https://unsplash.com/@veri_ivanova?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Veri Ivanova</a> on <a class="af nf" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="44ce" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine you open an online retail shop that offers a variety of products and is primarily targeted at U.S. customers. You plan to analyze buying habits from current transactions to satisfy more needs of current customers and serve more new ones. This motivates you to put much effort into processing the transaction records as a preparation step.</p><h2 id="1291" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#0 Mock data</h2><p id="9f52" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">We first simulate 1 million transaction records (surely expected to handle much larger datasets in real big data scenarios) in a CSV file. Each record includes a customer ID, product purchased, and transaction details such as payment methods and total amounts. One note worth mentioning is that a product agent with customer ID #100 has a significant customer base, and thus occupies a significant portion of purchases in your shop for drop-shipping.</p><p id="cd59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Below are the codes demonstrating this scenario:</p><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="4c44" class="pl oi fq pi b bg pm pn l po pp">import csv<br/>import datetime<br/>import numpy as np<br/>import random<br/><br/># Remove existing ‘retail_transactions.csv’ file, if any<br/>! rm -f /p/a/t/h retail_transactions.csv<br/><br/># Set the no of transactions and othet configs<br/>no_of_iterations = 1000000<br/>data = []<br/>csvFile = 'retail_transactions.csv'<br/><br/># Open a file in write mode<br/>with open(csvFile, 'w', newline='') as f:<br/> <br/>   fieldnames = ['orderID', 'customerID', 'productID', 'state', 'paymentMthd', 'totalAmt', 'invoiceTime']<br/>   writer = csv.DictWriter(f, fieldnames=fieldnames)<br/>   writer.writeheader()<br/> <br/>   for num in range(no_of_iterations):<br/>     # Create a transaction record with random values<br/>     new_txn = {<br/>     'orderID': num,<br/>     'customerID': random.choice([100, random.randint(1, 100000)]),<br/>     'productID': np.random.randint(10000, size=random.randint(1, 5)).tolist(),<br/>     'state': random.choice(['CA', 'TX', 'FL', 'NY', 'PA', 'OTHERS']),<br/>     'paymentMthd': random.choice(['Credit card', 'Debit card', 'Digital wallet', 'Cash on delivery', 'Cryptocurrency']),<br/>     'totalAmt': round(random.random() * 5000, 2),<br/>     'invoiceTime': datetime.datetime.now().isoformat()<br/>     }<br/> <br/>     data.append(new_txn)<br/><br/>  writer.writerows(data)</span></pre><p id="3e08" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After mocking the data, we load the CSV file into the PySpark DataFrame using Databrick’s Jupyter Notebook.</p><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="e3e4" class="pl oi fq pi b bg pm pn l po pp"># Set file location and type<br/>file_location = "/FileStore/tables/retail_transactions.csv"<br/>file_type = "csv"<br/><br/># Define CSV options<br/>schema = "orderID INTEGER, customerID INTEGER, productID INTEGER, state STRING, paymentMthd STRING, totalAmt DOUBLE, invoiceTime TIMESTAMP"<br/>first_row_is_header = "true"<br/>delimiter = ","<br/><br/># Read CSV files into DataFrame<br/>df = spark.read.format(file_type) \<br/>  .schema(schema) \<br/>  .option("header", first_row_is_header) \<br/>  .option("delimiter", delimiter) \<br/>  .load(file_location)<br/></span></pre><p id="8db2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We additionally create a reusable decorator utility to measure and compare the execution time of different approaches within each function.</p><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="b3e7" class="pl oi fq pi b bg pm pn l po pp">import time<br/><br/># Measure the excution time of a given function<br/>def time_decorator(func):<br/>  def wrapper(*args, **kwargs):<br/>    begin_time = time.time()<br/>    output = func(*args, **kwargs)<br/>    end_time = time.time()<br/>    print(f"Execution time of function {func.__name__}: {round(end_time - begin_time, 2)} seconds.")<br/>  return output<br/>return wrapper</span></pre><p id="4f40" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Okay, all the preparation is completed. Let’s explore different potential challenges of execution performance in the following sections.</p><h2 id="93f2" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#1 Storage</h2><p id="64ca" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Spark uses <a class="af nf" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank">Resilient Distributed Dataset (RDD)</a> as its core building blocks, with data typically kept in memory by default. Whether executing computations (like joins and aggregations) or storing data across the cluster, all operations contribute to memory usage in a unified region.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/f7f20c42fa3519fee671c569416280dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*exmFUSxKwBKh1bOkV8xLNA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">A unified region with execution memory and storage memory (Image by author)</figcaption></figure><p id="9790" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we design improperly, the available memory may become insufficient. This causes excess partitions to spill onto the disk, which results in performance degradation.</p><p id="8c47" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Caching and persisting intermediate results or frequently accessed datasets are common practices. While both cache and persist serve the same purposes, they may differ in their storage levels. The resources should be used optimally to ensure efficient read and write operations.</p><blockquote class="pr ps pt"><p id="2972" class="mj mk pu ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, if transformed data will be reused repeatedly for computations and algorithms across different subsequent stages, it is advisable to cache that data.</p></blockquote><p id="98b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Code example:</strong> Assume we want to investigate different subsets of transaction records using a digital wallet as the payment method.</p><ul class=""><li id="5ade" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Inefficient — Without caching</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="4067" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col<br/><br/>@time_decorator<br/>def without_cache(data):<br/>  # 1st filtering<br/>  df2 = data.where(col("paymentMthd") == "Digital wallet")<br/>  count = df2.count()<br/><br/>  # 2nd filtering<br/>  df3 = df2.where(col("totalAmt") &gt; 2000)<br/>  count = df3.count()<br/>    <br/>  return count<br/><br/>display(without_cache(df))</span></pre><ul class=""><li id="424e" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Efficient — Caching on a critical dataset</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="9552" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col<br/><br/>@time_decorator<br/>def after_cache(data):<br/>  # 1st filtering with cache<br/>  df2 = data.where(col("paymentMthd") == "Digital wallet").cache()<br/>  count = df2.count()<br/><br/>  # 2nd filtering<br/>  df3 = df2.where(col("totalAmt") &gt; 2000)<br/>  count = df3.count()<br/>    <br/>  return count<br/><br/>display(after_cache(df))</span></pre><p id="3b37" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After caching, even if we want to filter the transformed dataset with different transaction amount thresholds or other data dimensions, the execution times will still be more manageable.</p><h2 id="4a6a" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#2 Shuffle</h2><p id="36fd" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">When we perform operations like joining DataFrames or grouping by data fields, shuffling occurs. This is necessary to redistribute all records across the cluster and to ensure those with the same key are on the same node. This in turn facilitates simultaneous processing and combining of the results.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/ab84ce7db5a3682b102879ebcf4c8646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiWwglVyKvVo4M3GL_HydQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Shuffle join (Image by author)</figcaption></figure><p id="3e64" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, this shuffle operation is costly — high execution times and additional network overhead due to data movement between nodes.</p><p id="a39d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To reduce shuffling, there are several strategies:</p><p id="619a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(1) Use broadcast variables for the small dataset, to send a read-only copy to every worker node for local processing</p><blockquote class="pr ps pt"><p id="893f" class="mj mk pu ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While “small” dataset is often defined by a maximum memory threshold of 8GB per executor, the ideal size for broadcasting should be determined through experimentation on specific case.</p></blockquote><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh py"><img src="../Images/23bfddffdcd8abbc41eace540e8e045e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDR4lAD99vejLn0DAnSc0w.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Broadcast join (Image by author)</figcaption></figure><p id="f3c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(2) Early filtering, to minimize the amount of data processed as early as possible; and</p><p id="6628" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(3) Control the number of partitions to ensure optimal performance</p><p id="1e73" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Code examples:</strong> Assume we want to return the transaction records that match our list of states, along with their full names</p><ul class=""><li id="d608" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Inefficient — shuffle join between a large dataset and a small one</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="97b3" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col<br/><br/>@time_decorator<br/>def no_broadcast_var(data):<br/>  # Create small dataframe<br/>  small_data = [("CA", "California"), ("TX", "Texas"), ("FL", "Florida")]<br/>  small_df = spark.createDataFrame(small_data, ["state", "stateLF"])<br/><br/>  # Perform joining<br/>  result_no_broadcast = data.join(small_df, "state")<br/><br/>  return result_no_broadcast.count()<br/><br/>display(no_broadcast_var(df))</span></pre><ul class=""><li id="2e7f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Efficient — join the large dataset with the small one using a broadcast variable</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="b6ec" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col, broadcast<br/><br/>@time_decorator<br/>def have_broadcast_var(data):<br/>  small_data = [("CA", "California"), ("TX", "Texas"), ("FL", "Florida")]<br/>  small_df = spark.createDataFrame(small_data, ["state", "stateFullName"])<br/><br/>  # Create broadcast variable and perform joining<br/>  result_have_broadcast = data.join(broadcast(small_df), "state")<br/><br/>  return result_have_broadcast.count()<br/><br/>display(have_broadcast_var(df))</span></pre><h2 id="f8f1" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#3 Skewness</h2><p id="3898" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Data can sometimes be unevenly distributed, especially for data fields used as the key for processing. This leads to imbalanced partition sizes, in which some partitions are significantly larger or smaller than the average.</p><blockquote class="pr ps pt"><p id="471a" class="mj mk pu ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since the execution performance is limited by the longest-running tasks, it is necessary to address the over-burdened nodes.</p></blockquote><p id="3ca7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One common approach is salting. This works by adding randomized numbers to the skewed key so that there is a more uniform distribution across partitions. Let’s say when aggregating data based on the skewed key, we will aggregate using the salted key and then aggregate with the original key. Another method is re-partitioning, which increases the number of partitions to help distribute the data more evenly.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pz"><img src="../Images/119db606882ab6d12a63942cc007751e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09u2u0G1F1X8iSmyjlWDxA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Data distribution — Before and after salting (Image by author)</figcaption></figure><p id="d266" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Code examples:</strong> We want to aggregate an asymmetric dataset, mainly skewed by customer ID #100.</p><ul class=""><li id="df44" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Inefficient — directly use the skewed key</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="f84c" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col, desc<br/><br/>@time_decorator<br/>def no_salting(data):<br/>  # Perform aggregation<br/>  agg_data = data.groupBy("customerID").agg({"totalAmt": "sum"}).sort(desc("sum(totalAmt)"))<br/>  return agg_data<br/><br/>display(no_salting(df))</span></pre><ul class=""><li id="bed7" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Efficient — use the salting skewed key for aggregation</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="6318" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import col, lit, concat, rand, split, desc<br/><br/>@time_decorator<br/>def have_salting(data):<br/>  # Salt the customerID by adding the suffix<br/>  salted_data = data.withColumn("salt", (rand() * 8).cast("int")) \<br/>                .withColumn("saltedCustomerID", concat(col("customerID"), lit("_"), col("salt")))<br/><br/>  # Perform aggregation<br/> agg_data = salted_data.groupBy("saltedCustomerID").agg({"totalAmt": "sum"})<br/><br/> # Remove salt for further aggregation<br/> final_result = agg_data.withColumn("customerID", split(col("saltedCustomerID"), "_")[0]).groupBy("customerID").agg({"sum(totalAmt)": "sum"}).sort(desc("sum(sum(totalAmt))"))<br/><br/> return final_result<br/><br/>display(have_salting(df))</span></pre><p id="c636" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A random prefix or suffix to the skewed keys will both work. Generally, 5 to 10 random values are a good starting point to balance between spreading out the data and maintaining high complexity.</p><h2 id="4143" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#4 Serialization</h2><p id="129c" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">People often prefer using <a class="af nf" href="https://spark.apache.org/docs/3.5.2/sql-ref-functions-udf-scalar.html" rel="noopener ugc nofollow" target="_blank">user-defined functions (UDFs)</a> since it is flexible in customizing the data processing logic. However, UDFs operate on a row-by-row basis. The code shall be serialized by the Python interpreter, sent to the executor JVM, and then deserialized. This incurs high serialization costs and prevents Spark from optimizing and processing the code efficiently.</p><blockquote class="pr ps pt"><p id="e532" class="mj mk pu ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The simple and direct approach is to avoid using UDFs when possible.</p></blockquote><p id="b9a0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We should first consider using the <a class="af nf" href="https://spark.apache.org/docs/latest/api/sql/" rel="noopener ugc nofollow" target="_blank">built-in Spark functions</a>, which can handle tasks such as aggregation, arrays/maps operations, date/time stamps, and JSON data processing. If the built-in functions do not satisfy your desired tasks indeed, we can consider using <a class="af nf" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html" rel="noopener ugc nofollow" target="_blank">pandas <em class="pu">UDFs</em></a>. They are built on top of Apache Arrow for lower overhead costs and higher performance, compared to UDFs.</p><p id="b494" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Code examples: </strong>The transaction price is discounted based on the originating state.</p><ul class=""><li id="7efe" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Inefficient — using a UDF</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="6b08" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import udf<br/>from pyspark.sql.types import DoubleType<br/>from pyspark.sql import functions as F<br/>import numpy as np<br/><br/># UDF to calculate discounted amount<br/>def calculate_discount(state, amount):<br/>  if state == "CA":<br/>    return amount * 0.90  # 10% off<br/>  else:<br/>    return amount * 0.85  # 15% off<br/><br/>discount_udf = udf(calculate_discount, DoubleType())<br/><br/>@time_decorator<br/>def have_udf(data):<br/>  # Use the UDF<br/>  discounted_data = data.withColumn("discountedTotalAmt", discount_udf("state", "totalAmt"))<br/><br/>  # Show the results<br/>  return discounted_data.select("customerID", "totalAmt", "state", "discountedTotalAmt").show()<br/><br/>display(have_udf(df))</span></pre><ul class=""><li id="5379" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Efficient — using build-in PySpark functions</li></ul><pre class="nj nk nl nm nn ph pi pj bp pk bb bk"><span id="f7c1" class="pl oi fq pi b bg pm pn l po pp">from pyspark.sql.functions import when<br/><br/>@time_decorator<br/>def no_udf(data):<br/>  # Use when and otherwise to discount the amount based on conditions <br/>  discounted_data = data.withColumn(<br/>  "discountedTotalAmt",<br/>  when(data.state == "CA", data.totalAmt * 0.90)  # 10% off<br/>  .otherwise(data.totalAmt * 0.85))  # 15% off<br/><br/>  # Show the results<br/>  return discounted_data.select("customerID", "totalAmt", "state", "discountedTotalAmt").show()<br/><br/>display(no_udf(df))</span></pre><p id="7d84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this example, we use the built-in PySpark functions “when and otherwise” to effectively check multiple conditions in sequence. There are unlimited examples based on our familiarity with those functions. For instance, <code class="cx qa qb qc pi b">pyspark.sql.functions.transform</code>a function that aids in applying a transformation to each element in the input array has been introduced since PySpark version 3.1.0.</p><h2 id="ac36" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">#5 Spill</h2><p id="b37d" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">As discussed in the Storage section, a spill occurs by writing temporary data from memory to disk due to insufficient memory to hold all the required data. Many performance issues we have covered are related to spills. For example, operations that shuffle large amounts of data between partitions can easily lead to memory exhaustion and subsequent spill.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pq"><img src="../Images/8e0391d49e71ebd4918378b38ed64fe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-wVI0a7NnmDDjmw0e3XoQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Different scenarios of spill due to insufficient memory (Image by author)</figcaption></figure><p id="1b68" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is crucial to examine the performance metrics in Spark UI. If we discover the statistics for Spill(Memory) and Spill(Disk), the spill is probably the reason for long-running tasks. To remediate this, try to instantiate a cluster with more memory per worker, e.g. increase the executor process size, by tuning the configuration value <code class="cx qa qb qc pi b">spark.executor.memory</code>; Alternatively, we can configure <code class="cx qa qb qc pi b">spark.memory.fraction</code> to adjust how much memory is allocated for execution and storage.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1091" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Wrapping it Up</h2><p id="2ac8" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">We came across several common factors leading to performance degradation in PySpark, and the possible improvement methods:</p><ul class=""><li id="c9f4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">Storage: use cache and persist to store the frequently used intermediate results</li><li id="3646" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne pv pw px bk">Shuffle: use broadcast variables for a small dataset to facilitate Spark’s local processing</li><li id="0f52" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne pv pw px bk">Skewness: execute salting or repartitioning to distribute the skewed data more uniformly</li><li id="b501" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne pv pw px bk">Serialization: prefer to use built-in Spark functions to optimize the performance</li><li id="7197" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne pv pw px bk">Spill: adjust the configuration value to allocate memory wisely</li></ul><p id="d8ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Recently, <a class="af nf" href="https://docs.databricks.com/en/optimizations/aqe.html" rel="noopener ugc nofollow" target="_blank">Adaptive Query Execution (AQE)</a> has been newly addressed for dynamic planning and re-planning of queries based on runtime stats. This supports different features of query re-optimization that occur during query execution, which creates a great optimization technique. However, understanding data characteristics during the initial design is still essential, as it informs better strategies for writing effective codes and queries while using AQE for fine-tuning.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ea51" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Before you go</h2><p id="3cdd" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">If you enjoy this reading, I invite you to<strong class="ml fr"> </strong>follow my <a class="af nf" href="https://medium.com/@johnleungTJ" rel="noopener">Medium page</a> and <a class="af nf" href="https://www.linkedin.com/in/john-leung-639800115/" rel="noopener ugc nofollow" target="_blank">LinkedIn page</a>. By doing so, you can stay updated with exciting content related to data science side projects, Machine Learning Operations (MLOps) demonstrations, and project management methodologies.</p><div class="qi qj qk ql qm qn"><a rel="noopener follow" target="_blank" href="/simplifying-the-python-code-for-data-engineering-projects-95f0c41dc58a?source=post_page-----4b895857c8aa--------------------------------"><div class="qo ab ig"><div class="qp ab co cb qq qr"><h2 class="bf fr hw z io qs iq ir qt it iv fp bk">Simplifying the Python Code for Data Engineering Projects</h2><div class="qu l"><h3 class="bf b hw z io qs iq ir qt it iv dx">Python tricks and techniques for data ingestion, validation, processing, and testing: a practical walkthrough</h3></div><div class="qv l"><p class="bf b dy z io qs iq ir qt it iv dx">towardsdatascience.com</p></div></div><div class="qw l"><div class="qx l qy qz ra qw rb lr qn"/></div></div></a></div><div class="qi qj qk ql qm qn"><a rel="noopener follow" target="_blank" href="/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287?source=post_page-----4b895857c8aa--------------------------------"><div class="qo ab ig"><div class="qp ab co cb qq qr"><h2 class="bf fr hw z io qs iq ir qt it iv fp bk">Feature Engineering for Time-Series Using PySpark on Databricks</h2><div class="qu l"><h3 class="bf b hw z io qs iq ir qt it iv dx">Discover the potentials of PySpark for time-series data: Ingest, extract, and visualize data, accompanied by practical…</h3></div><div class="qv l"><p class="bf b dy z io qs iq ir qt it iv dx">towardsdatascience.com</p></div></div><div class="qw l"><div class="rc l qy qz ra qw rb lr qn"/></div></div></a></div></div></div></div></div>    
</body>
</html>