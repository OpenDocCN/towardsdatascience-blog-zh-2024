# 推动强化学习的边界：将基础模型（如LLMs和VLMs）整合到强化学习中

> 原文：[https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17](https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17)

## 深入探讨将基础模型（如LLMs和VLMs）整合到强化学习训练循环中的方法

[](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[![Elahe Aghapour](../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png)](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------) [Elahe Aghapour](https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------) ·15分钟阅读·2024年4月17日

--

**作者：** [Elahe Aghapour](https://medium.com/u/75214fb27311?source=post_page---user_mention--556cfb6d0632--------------------------------)，[Salar Rahili](https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--556cfb6d0632--------------------------------)

## 概述：

随着变压器架构的兴起和高吞吐量计算的应用，训练基础模型最近成为了一个热门话题。这引发了将基础模型整合或训练基础模型以增强强化学习（RL）算法能力的有前景的努力，标志着该领域一个激动人心的方向。在这里，我们讨论的是基础模型如何为强化学习提供重大提升。

在深入研究基础模型如何为强化学习提供重大提升之前，让我们进行一次头脑风暴。我们的目标是明确预训练的基础模型，特别是大型语言模型（LLMs）或视觉语言模型（VLMs），在哪些领域能够为我们提供帮助，或者我们如何从零开始训练一个基础模型。一种有用的方法是逐个检查强化学习训练循环中的每个要素，以识别可能存在的改进空间：

![](../Images/d6e65070c5e31ca5151feccf4c58c6b2.png)

图1：强化学习中的基础模型概览（图像来源：作者）

**1-** 环境：鉴于预训练的基础模型理解事件之间的因果关系，它们可以被用来预测当前动作引发的环境变化。尽管这一概念很有趣，但我们目前尚未了解有任何专门聚焦于此的研究。现在有两个主要原因使得我们暂时无法进一步探索这一想法。

+   强化学习的训练过程需要对下一步观察做出高度准确的预测，但预训练的LLM/VLM并没有在能够进行如此精确预测的数据集上进行直接训练，因此在这一方面存在不足。需要指出的是，正如我们在[上一篇文章](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)中强调的那样，一个高层次的规划者，特别是在终身学习场景中使用的规划者，可以有效地结合基础模型。

+   环境步骤中的延迟是一个关键因素，它可能会限制强化学习（RL）算法，尤其是在训练步骤预算固定的情况下。一个引入显著延迟的非常大模型的存在可能会非常具有限制性。需要注意的是，虽然这可能具有挑战性，但将其蒸馏为一个更小的网络可能是一个解决方案。

**2-** 状态（基于LLM/VLM的状态生成器）：虽然专家们经常将观察和状态这两个术语互换使用，但它们之间是有区别的。状态是环境的综合表示，而观察可能仅提供部分信息。在标准的RL框架中，我们通常不会讨论从观察、过去的动作以及任何环境的内部知识中提取和合并有用特征以生成“状态”，即策略输入的具体转化过程。通过使用LLM/VLM，这样的转化可以显著增强，它们使我们能够将更广泛的世界、物理和历史知识融入到“状态”中（参见图1，粉红色高亮部分）。

**3-** 策略（基础策略模型）：将基础模型融入RL中的核心决策组件——策略，可能会带来很大的益处。尽管采用此类模型生成高层次计划已经证明是成功的，但将状态转化为低层次动作仍然面临挑战，我们将在后面深入探讨。幸运的是，近期在这一领域已有一些 promising的研究成果。

**4-** 奖励（基于LLM/VLM的奖励生成器）：利用基础模型更准确地评估轨迹中选择的动作，已成为研究者们的主要关注点。考虑到奖励传统上作为人类与智能体之间的沟通渠道，设定目标并引导智能体朝着期望的方向发展，这一点并不令人惊讶。

+   预训练的基础模型具备深厚的世界知识，将这种理解注入我们的决策过程中，可以使这些决策更加符合人类的愿望，并更有可能成功。此外，使用基础模型评估代理的动作可以迅速缩小搜索空间，并为代理提供理解的起点，而不是从零开始。

+   预训练的基础模型已经在大规模的互联网数据上进行了训练，这些数据主要由人类生成，这使得它们能够像人类一样理解世界。这使得将基础模型作为成本效益高的标注器变得可能。它们可以在大规模上生成标签或评估轨迹或展开。

**1- 奖励中的基础模型**

使用基础模型生成低级控制动作是具有挑战性的，因为低级动作高度依赖于代理的设置，并且在基础模型的训练数据集中出现较少。因此，基础模型的应用通常集中在高层次的计划上，而不是低级动作。奖励弥补了高层规划者和低级动作之间的差距，这也是基础模型可以应用的地方。研究人员采用了多种方法将基础模型集成到奖励分配中。然而，核心原则是使用VLM/LLM来有效追踪朝向子目标或任务的进展。

**1.a 根据相似性分配奖励值**

将奖励值视为一种信号，表示代理的先前动作是否有助于朝着目标前进。一种合理的方法是评估先前的动作与当前目标的对齐程度。为了将这种方法付诸实践，如图2所示，至关重要的是：

- 生成这些动作的有意义的嵌入，可以通过图像、视频或对最新观察结果的文本描述来完成。

- 生成当前目标的有意义表示。

- 评估这些表示之间的相似性。

![](../Images/22b45a375b7df3fefaee9eb893077dc4.png)

图2. 基于相似性的奖励值（图源自作者）。

让我们探讨一下这一领域领先研究背后的具体机制。

稠密且良好形状的奖励函数能够提高RL代理的稳定性和训练速度。内在奖励通过奖励代理探索新状态来应对这一挑战。然而，在大规模环境中，大多数未见过的状态与下游任务无关，这种方法变得不太有效。[ELLM](https://arxiv.org/pdf/2302.06692.pdf)利用LLM的背景知识来塑造探索。它查询LLM生成一份可能的目标/子目标列表，给定代理可用的动作列表和由状态说明生成的代理当前观察的文本描述。然后，在每个时间步，奖励通过语义相似度，即LLM生成的目标与代理转移描述之间的余弦相似度来计算。

[LiFT](https://arxiv.org/pdf/2312.08958.pdf)有一个类似的框架，但还利用了[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)风格的VLM进行奖励分配。[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)通过对比学习预训练以对齐视频和相应的语言描述。在[LiFT](https://arxiv.org/pdf/2312.08958.pdf)中，代理根据任务指令和代理相应行为视频之间的对齐得分（余弦相似度）来获得奖励，二者都通过[CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf)进行编码。

[UAFM](https://arxiv.org/pdf/2307.09668.pdf)有一个类似的框架，主要关注机器人操作任务，例如堆叠一组物体。在奖励分配方面，它们通过测量代理状态图像与任务描述之间的相似度来分配奖励，二者都通过[CLIP](https://arxiv.org/pdf/2103.00020.pdf)进行嵌入。它们在模拟堆叠领域的少量数据上对[CLIP](https://arxiv.org/pdf/2103.00020.pdf)进行微调，以便在此用例中获得更好的对齐。

**1.b 通过推理辅助任务来分配奖励：**

在基础模型正确理解环境的情况下，直接将轨迹中的观察结果传递给模型LLM/VLM成为可行。这种评估可以通过基于观察结果的简单问答会话进行，或者通过验证模型仅通过观察轨迹预测目标的能力来进行。

![](../Images/43e8a34aa903d5be475522d623219faf.png)

图3. 通过推理分配奖励（图片来源：作者）。

[阅读与奖励](https://arxiv.org/pdf/2302.04449.pdf)通过两个关键组成部分将环境的说明手册融入奖励生成中，如图3所示：

1.  QA提取模块：它创建游戏目标和特征的摘要。这个基于LLM的模块，[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)-large，接受游戏手册和一个问题，并从文本中提取相应的答案。问题集中在游戏目标和代理-对象交互上，通过TF-IDF来识别其重要性。对于每个关键对象，都会增加一个类似于：“当玩家击中<对象>时，会发生什么？”的问题到问题集。然后，通过将所有非空的问答对连接起来，形成一个摘要。

1.  推理模块：在游戏过程中，基于规则的算法检测“击中”事件。在每个“击中”事件后，基于LLM的推理模块会查询环境的摘要和一个问题：“如果你想获胜，是否应该击中一个<互动对象>？”其中可能的回答仅限于{是，否}。回答“是”会增加正奖励，而回答“否”则会导致负奖励。

[EAGER](https://arxiv.org/pdf/2206.09674.pdf)引入了一种通过专门设计的辅助任务创建内在奖励的独特方法。这种方法提出了一个新概念，其中辅助任务涉及基于当前观察预测目标。如果模型预测准确，这表明与预期目标的高度一致，因此，根据预测的置信度水平，会给予更大的内在奖励。为实现这一目标，采用了两个模块：

+   问题生成（QG）：这个组件通过屏蔽用户提供的详细目标中的所有名词和形容词来工作。

+   问答（QA）：这是一个通过监督方式训练的模型，它接受观察、问题掩码和动作，并预测被掩盖的标记。

（P.S. 尽管这项工作没有使用基础模型，但由于它的有趣方法，我们将其包括在内，因为该方法可以很容易地适应任何预训练的LLM。）

**1.c 生成奖励函数代码**

到目前为止，我们已经讨论了直接为强化学习算法生成奖励值。然而，在每个RL循环步骤中运行一个大型模型可能会显著降低训练和推理的速度。为了绕过这一瓶颈，一种策略是利用我们的基础模型生成奖励函数的代码。这样可以在每一步直接生成奖励值，从而简化过程。

为了使代码生成方案有效工作，需要两个关键组件：

1- 一个代码生成器LLM，它接收一个包含所有必要信息的详细提示，以便生成代码。

2- 一个与代码生成器协作的细化过程，用于评估和增强代码。

让我们看看生成奖励代码的关键贡献：

[R2R2S](https://arxiv.org/pdf/2306.08647.pdf)通过两个主要组件生成奖励函数代码：

1.  基于LLM的动作描述符：该模块使用预定义模板来描述机器人动作，并利用大语言模型（LLMs）来理解这些动作。动作描述符填写模板，替换占位符，例如将“目标点坐标”替换为具体细节，以在预定义模板内描述所需的机器人动作。

1.  基于LLM的奖励编码器：该组件通过处理包含以下内容的提示来生成奖励函数：动作描述、LLM可以用来生成奖励函数代码的函数列表及其描述、响应应如何呈现的示例代码，以及奖励函数必须遵循的约束和规则。

[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf)开发了一种方法，用于在迭代细化过程中生成可执行代码的密集奖励函数。根据任务的子目标，它有两个关键组件：

1.  基于LLM的奖励编码器：生成奖励函数代码。它的提示包括：观察和可用动作的摘要，表示物体、机器人和可调用函数配置的简洁Python风格环境；奖励函数设计的背景知识（例如，“任务X的奖励函数通常包括物体x和y之间的距离项”），以及少量示例。他们假设可以访问一个指令和奖励函数对的池，并检索最相似的前k个指令作为少量示例。

1.  基于LLM的细化：一旦生成了奖励代码，代码会被执行以识别语法错误和运行时错误。这些反馈会被集成到后续的提示中，以生成更精细的奖励函数。此外，还会根据当前策略请求人类反馈，反馈内容来源于任务执行视频。

[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)与[Text2Reward](https://arxiv.org/pdf/2309.11489.pdf)具有相似的算法，用于生成奖励函数代码，见图4。主要区别在于细化阶段，它有两个模块，都是LLM：

1.  基于LLM的奖励评论员：它评估代码并提供反馈，判断代码是否自洽、是否没有语法和语义错误。

1.  基于LLM的轨迹分析器：它回顾训练过的智能体与环境之间的历史互动信息，并利用这些信息来指导奖励函数的修改。

![](../Images/9f84ed9097ad74b7b0d223a4572abf62.png)

图4\. [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)概览（图来自[Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf)论文）

[EUREKA](https://arxiv.org/pdf/2310.12931.pdf)生成奖励代码，无需特定任务提示、预定义奖励模板或预定义的少量示例。为实现这一目标，它有两个阶段：

1.  基于LLM的代码生成：原始环境代码、任务、通用奖励设计和格式化提示作为上下文输入到LLM，LLM返回可执行的奖励代码以及其组件列表。

1.  进化搜索和优化：在每次迭代中，[EUREKA](https://arxiv.org/pdf/2310.12931.pdf) 查询LLM生成多个独立同分布（i.i.d）奖励函数。使用可执行奖励函数训练代理可以提供关于代理表现的反馈。为了对奖励进行详细和专注的分析，反馈还包括奖励函数每个组件的标量值。LLM根据表现最好的奖励代码以及这些详细反馈，进行奖励代码的上下文突变。在每次后续迭代中，LLM使用最优奖励代码作为参考，生成K个新的i.i.d奖励代码。这个迭代优化过程会持续进行，直到达到指定的迭代次数。

在这两个步骤中，[EUREKA](https://arxiv.org/pdf/2310.12931.pdf)能够生成超越专家人工设计的奖励函数，而无需任何特定任务模板。

**1.d. 基于偏好的奖励模型训练（RLAIF）**

另一种方法是使用基础模型生成数据来训练奖励函数模型。最近，强化学习与人类反馈（[RLHF](https://arxiv.org/pdf/2203.02155.pdf)）的显著成功吸引了越来越多的关注，特别是如何在更大规模上使用训练好的奖励函数。这类算法的核心在于利用偏好数据集训练奖励模型，之后将其整合到强化学习算法中。由于通过人类反馈生成偏好数据（例如，行为A优于行为B）成本高昂，越来越多的研究开始关注通过AI代理（如VLM/LLM）获取反馈来构建该数据集。使用AI生成的数据训练奖励函数，并将其整合到强化学习算法中，这一过程被称为“AI反馈强化学习”（RLAIF）。

[MOTIF](https://arxiv.org/pdf/2310.00166.pdf)需要访问一个具有充分覆盖的被动观察数据集。最初，LLM通过提供环境中期望行为的总结和两个随机采样的观察描述来进行查询。然后，它生成偏好，在1、2或0之间进行选择（表示无偏好），如图5所示。这个过程构建了一个关于观察对之间偏好的数据集。随后，该数据集被用来训练一个奖励模型，采用[基于偏好的RL技术](https://jmlr.org/papers/volume18/16-634/16-634.pdf)。

![](../Images/80cc3f30ff6414b84db9a4b52ba2d4f2.png)

图 5. [MOTIF](https://arxiv.org/pdf/2310.00166.pdf)的三个阶段的示意图（图片来源于[MOTIF](https://arxiv.org/pdf/2310.00166.pdf)论文）

**2- 基础模型作为策略**

实现训练一个基础策略的能力，不仅能够在以前遇到的任务中表现出色，而且能够利用过去的学习对新任务进行推理和适应，这是RL社区的一个目标。这样的策略理想情况下能够从过去的经验中推广到新情况，并通过环境反馈，实现以前未见过的目标，展现出类人适应性。

然而，训练这种代理面临着一些挑战。其中一些挑战包括：

+   管理一个非常大的模型的必要性，这会给低级控制操作的决策过程带来显著的延迟。

+   收集大量跨多个任务的交互数据，以便进行有效学习的需求。

+   此外，从头开始使用RL训练一个非常大的网络还引入了额外的复杂性。这是因为与监督训练方法相比，反向传播效率在RL中本身就较弱。

到目前为止，主要是那些拥有大量资源和一流设备的团队在这一领域真正推动了技术的边界。

[AdA](https://arxiv.org/pdf/2301.07608.pdf)为在X.Land 2.0 3D环境中训练RL基础模型铺平了道路。该模型能够在没有进一步训练的情况下，在保留的测试任务上实现人类时间尺度的适应。该模型的成功建立在三个关键因素之上：

1.  [AdA](https://arxiv.org/pdf/2301.07608.pdf)学习机制的核心是一个23到2.65亿参数的Transformer-XL架构，与[ Muesli](https://arxiv.org/pdf/2104.06159.pdf) RL算法一起使用。Transformer-XL接受从时间t到T的一段观察、行动和奖励轨迹，并为每个时间步输出一系列隐藏状态。隐藏状态用于预测奖励、价值和行动分布π。长期记忆和短期记忆的结合对快速适应至关重要。长期记忆通过缓慢的梯度更新实现，而短期记忆则可以在变压器的上下文长度内捕获。这种独特的结合使得模型能够通过在试验之间保留记忆，从多个任务尝试中保留知识，即使环境在试验之间重置。

1.  该模型从在1⁰⁴⁰个不同的部分可观察马尔可夫决策过程（POMDP）任务上进行元强化学习训练中受益。由于[变压器是元学习者](https://arxiv.org/pdf/2206.06614.pdf)，因此不需要额外的元步骤。

1.  鉴于任务池的规模和多样性，许多任务将过于简单或过于困难，无法生成良好的训练信号。为了解决这个问题，他们使用了一个自动化课程来优先处理那些在其能力范围内的任务。

[RT-2](https://arxiv.org/pdf/2307.15818.pdf)提出了一种方法，通过在机器人轨迹数据和视觉语言任务上联合微调VLM，生成一种名为[RT-2](https://arxiv.org/pdf/2307.15818.pdf)的策略模型。为了使视觉语言模型能够生成低级动作，动作被离散化为256个桶，并表示为语言符号。

通过将动作表示为语言符号，[RT-2](https://arxiv.org/pdf/2307.15818.pdf)可以直接利用现有的视觉语言模型（VLM）架构，而无需进行大量修改。因此，VLM的输入包括机器人摄像头图像和格式化类似于视觉问答任务的文本任务描述，输出则是一系列语言符号，代表机器人低级动作；见图6。

![](../Images/94312d770f343bd4684840e4a373dbc6.png)

图6. [RT-2](https://arxiv.org/pdf/2307.15818.pdf) 概述（图像来自 [RT-2](https://arxiv.org/pdf/2307.15818.pdf) 论文）

他们注意到，使用原始网页数据与这两种类型数据进行联合微调，能够得到更具普适性的策略。联合微调过程使得[RT-2](https://arxiv.org/pdf/2307.15818.pdf)具备了理解和执行训练数据中未明确出现的指令的能力，展现出卓越的适应性。这种方法使得他们能够利用互联网规模的视觉语言模型预训练，通过语义推理泛化到新的任务。

**3- 基础模型作为状态表示**

在强化学习（RL）中，策略对环境的理解来自于它的“状态”，本质上就是它如何感知其周围的环境。通过观察强化学习的框架图，一个合理的模块来注入世界知识的是状态。如果我们能够通过有助于完成任务的一般知识来丰富观察数据，那么与从零开始学习的强化学习智能体相比，策略将能更快地掌握新任务。

[PR2L](https://arxiv.org/pdf/2402.02651.pdf)提出了一种新颖的方法，通过来自互联网规模数据的视觉语言模型（VLM）背景知识注入到强化学习（RL）中。[PR2L](https://arxiv.org/pdf/2402.02651.pdf)采用了生成型VLM，这些模型根据图像和文本输入生成语言。由于VLM擅长理解和响应视觉与文本输入，它们可以提供丰富的语义特征来源，这些特征可以与动作关联。

[PR2L](https://arxiv.org/pdf/2402.02651.pdf)查询VLM（视觉语言模型）并为每个收到的视觉观测提供与任务相关的提示，接收生成的文本响应和模型的中间表示。他们丢弃文本，并使用生成的视觉和文本输入的部分或全部模型中间表示以及VLM生成的文本响应作为“可提示表示”。由于这些表示的大小可变，[PR2L](https://arxiv.org/pdf/2402.02651.pdf)采用了一个编码-解码器Transformer层，将所有信息嵌入到一个固定大小的嵌入向量中。这个嵌入向量结合任何可用的非视觉观测数据，然后提供给策略网络，表示代理的状态。这种创新的整合方法使得RL（强化学习）代理能够利用VLM的丰富语义理解和背景知识，从而更快速和更有信息地学习任务。

> **另请阅读我们的上一篇文章：** [走向通用人工智能：大语言模型和基础模型在终身学习革命中的作用](/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66)

**参考文献：**

[1] [ELLM](https://arxiv.org/pdf/2302.06692.pdf): Du, Yuqing 等人。“通过大语言模型引导强化学习的预训练。”2023年。

[2] [Text2Reward](https://arxiv.org/pdf/2309.11489.pdf): Xie, Tianbao 等人。“Text2reward：强化学习的自动化稠密奖励函数生成。”2023年。

[3] [R2R2S](https://arxiv.org/pdf/2306.08647.pdf): Yu, Wenhao 等人。“从语言到奖励：机器人技能合成。”2023年。

[4] [EUREKA](https://arxiv.org/pdf/2310.12931.pdf): Ma, Yecheng Jason 等人。“Eureka：通过编码大语言模型进行人类水平的奖励设计。”2023年。

[5] [MOTIF](https://arxiv.org/pdf/2310.00166.pdf): Klissarov, Martin 等人。“Motif：来自人工智能反馈的内在动机。”2023年。

[6] [Read and Reward](https://arxiv.org/pdf/2302.04449.pdf): Wu, Yue 等人。“阅读并获得奖励：通过使用说明书学习玩Atari。”2024年。

[7] [Auto MC-Reward](https://arxiv.org/pdf/2312.09238.pdf): Li, Hao 等人。“Auto MC-reward：利用大语言模型为Minecraft自动设计稠密奖励。”2023年。

[8] [EAGER](https://arxiv.org/pdf/2206.09674.pdf): Carta, Thomas 等人。“Eager：提出和回答问题以进行语言引导的RL中的自动奖励塑造。”2022年。

[9] [LiFT](https://arxiv.org/pdf/2312.08958.pdf): Nam, Taewook 等人。“LiFT：使用基础模型作为教师的无监督强化学习。”2023年。

[10] [UAFM](https://arxiv.org/pdf/2307.09668.pdf): Di Palo, Norman 等人。“走向一个统一的代理与基础模型。”2023年。

[11] [RT-2](https://arxiv.org/pdf/2307.15818.pdf): Brohan, Anthony 等人。“Rt-2：视觉-语言-动作模型将网络知识转移到机器人控制。”2023年。

[12] [AdA](https://arxiv.org/pdf/2301.07608.pdf)：自适应代理团队等人。“在开放式任务空间中进行人类时间尺度的适应。”2023年。

[13] [PR2L](https://arxiv.org/pdf/2402.02651.pdf)：Chen, William 等人。“视觉-语言模型为强化学习提供可提示的表征。”2024年。

[14] [Clip4Clip](https://arxiv.org/pdf/2104.08860.pdf)：Luo, Huaishao 等人。“Clip4clip：关于Clip在端到端视频片段检索与字幕生成中的实证研究。”2022年。

[15] [Clip](https://arxiv.org/pdf/2103.00020.pdf)：Radford, Alec 等人。“从自然语言监督中学习可迁移的视觉模型。”2021年。

[16] [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)：Liu, Yinhan 等人。“Roberta：一种强健优化的BERT预训练方法。”2019年。

[17] [基于偏好的强化学习](https://jmlr.org/papers/volume18/16-634/16-634.pdf)：SWirth, Christian 等人。“基于偏好的强化学习方法综述。”2017年。

[18] [Muesli](https://arxiv.org/pdf/2104.06159.pdf)：Hessel, Matteo 等人。“Muesli：结合政策优化的改进。”2021年。

[19] Melo, Luckeciano C. “[变压器是元强化学习者](https://arxiv.org/pdf/2206.06614.pdf)。”2022年。

[20] [RLHF](https://arxiv.org/pdf/2203.02155.pdf)：Ouyang, Long 等人。“通过人类反馈训练语言模型遵循指令。”2022年。
