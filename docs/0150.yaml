- en: 'SentenceTransformer: A Model For Computing Sentence Embedding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16](https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Convert BERT to an efficient sentence transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    ·7 min read·Jan 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we look at SentenceTransformer [1] which was published in 2019\.
    SentenceTransformer has a bi-encoder architecture and adapts BERT to produce efficient
    sentence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT (Bidirectional Encoder Representation of Transformers) is built with the
    ideology that all NLP tasks rely on the meaning of tokens/words. BERT is trained
    in two phases: 1) pre-training phase where BERT learns the general meaning of
    the language, and 2) fine-tuning where BERT is trained on specific tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52937cabeba180d1e79c4be33abdf476.png)'
  prefs: []
  type: TYPE_IMG
- en: Image taken from [[3](https://arxiv.org/pdf/1810.04805.pdf)]
  prefs: []
  type: TYPE_NORMAL
- en: BERT is very good at learning the meaning of words/tokens. But It is not good
    at learning meaning of sentences. As a result it is not good at certain tasks
    such as sentence classification, sentence pair-wise similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Since BERT produces token embedding, one way to get sentence embedding out of
    BERT is to average the embedding of all tokens. The SentenceTransformer paper
    [1] showed this produces very low quality sentence embeddings almost as bad as
    getting GLOVE embeddings. These embeddings do not capture the meaning of sentences.
  prefs: []
  type: TYPE_NORMAL
