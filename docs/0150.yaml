- en: 'SentenceTransformer: A Model For Computing Sentence Embedding'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SentenceTransformer：一种计算句子嵌入的模型
- en: 原文：[https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16](https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16](https://towardsdatascience.com/sentencetransformer-a-model-for-computing-sentence-embedding-e8d31d9e6a8f?source=collection_archive---------6-----------------------#2024-01-16)
- en: Convert BERT to an efficient sentence transformer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将BERT转换为高效的句子变换器
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--e8d31d9e6a8f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    ·7 min read·Jan 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e8d31d9e6a8f--------------------------------)
    ·阅读时间7分钟·2024年1月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this post, we look at SentenceTransformer [1] which was published in 2019\.
    SentenceTransformer has a bi-encoder architecture and adapts BERT to produce efficient
    sentence embeddings.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨SentenceTransformer [1]，该模型于2019年发布。SentenceTransformer采用了双编码器架构，并将BERT模型适配以生成高效的句子嵌入。
- en: 'BERT (Bidirectional Encoder Representation of Transformers) is built with the
    ideology that all NLP tasks rely on the meaning of tokens/words. BERT is trained
    in two phases: 1) pre-training phase where BERT learns the general meaning of
    the language, and 2) fine-tuning where BERT is trained on specific tasks.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: BERT（双向编码器表示的变换器）是基于这样一种理念构建的：所有NLP任务都依赖于词元/单词的语义。BERT训练分为两个阶段：1）预训练阶段，在此阶段BERT学习语言的普遍语义；2）微调阶段，在此阶段BERT在特定任务上进行训练。
- en: '![](../Images/52937cabeba180d1e79c4be33abdf476.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52937cabeba180d1e79c4be33abdf476.png)'
- en: Image taken from [[3](https://arxiv.org/pdf/1810.04805.pdf)]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[[3](https://arxiv.org/pdf/1810.04805.pdf)]
- en: BERT is very good at learning the meaning of words/tokens. But It is not good
    at learning meaning of sentences. As a result it is not good at certain tasks
    such as sentence classification, sentence pair-wise similarity.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT非常擅长学习单词/词元的语义，但它不擅长学习句子的语义。因此，它在某些任务上表现不佳，例如句子分类、句子对之间的相似度计算。
- en: Since BERT produces token embedding, one way to get sentence embedding out of
    BERT is to average the embedding of all tokens. The SentenceTransformer paper
    [1] showed this produces very low quality sentence embeddings almost as bad as
    getting GLOVE embeddings. These embeddings do not capture the meaning of sentences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BERT生成的是词元嵌入，因此从BERT中获取句子嵌入的一种方式是对所有词元的嵌入取平均值。SentenceTransformer论文[1]表明，这种方式会产生非常低质量的句子嵌入，几乎和使用GLOVE嵌入一样差。这些嵌入不能捕捉句子的语义。
