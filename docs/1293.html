<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Language as a Universal Learning Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Language as a Universal Learning Machine</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23">https://towardsdatascience.com/language-as-a-universal-learning-machine-d2c67cb15e5f?source=collection_archive---------7-----------------------#2024-05-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3948" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">LANGUAGE PROCESSING IN HUMANS AND COMPUTERS: Part 4</h2><div/><div><h2 id="a1e4" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Saying is believing. Seeing is hallucinating.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Dusko Pavlovic" class="l ep by dd de cx" src="../Images/3d242896266291f7adbf6f131fe2e16d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*vob2tSJAXTCy2qHAmBArBg.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@dusko_p?source=post_page---byline--d2c67cb15e5f--------------------------------" rel="noopener follow">Dusko Pavlovic</a></p></div></div></div><div class="ie if l"><div class="ab ig"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ij ik ah ai aj ak al am an ao ap aq ar il im in" disabled="">Follow</button></p></div></div></span></div></div><div class="l io"><span class="bf b bg z dx"><div class="ab cn ip iq ir"><div class="is it ab"><div class="bf b bg z dx ab iu"><span class="iv l io">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d2c67cb15e5f--------------------------------" rel="noopener follow"><p class="bf b bg z iw ix iy iz ja jb jc jd bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">40 min read</span><div class="je jf l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv"><div class="h k w ea eb q"><div class="kl l"><div class="ab q km kn"><div class="pw-multi-vote-icon ed iv ko kp kq"><div class=""><div class="kr ks kt ku kv kw kx am ky kz la kq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lb lc ld le lf lg lh"><p class="bf b dy z dx"><span class="ks">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kr lk ll ab q ee lm ln" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count li lj">1</span></p></button></div></div></div><div class="ab q jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk"><div class="lo k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lp an ao ap il lq lr ls" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lt cn"><div class="l ae"><div class="ab cb"><div class="lu lv lw lx ly lz ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e636" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Machine-learned language models have transformed everyday life: they steer us when we study, drive, manage money. They have the potential to transform our civilization. But they hallucinate. Their realities are virtual. This 4th part of the series on language processing provides a high-level overview of low-level details of how the learning machines work. It turns out that, even after they become capable of recognizing hallucinations and dreaming safely, as humans tend to be, the learning machines will proceed to form broader systems of false beliefs and self-confirming theories, as humans tend to do.</p><p id="84c7" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">[I tried to make this text readable for all. Skipping the math underpinnings provided with some claims shouldn’t impact the later claims. Even just the pictures at the beginning and at the end are hoped to convey the main message. Suggestions for improvements are welcome :)]</p></div></div></div><div class="ab cb nn no np nq" role="separator"><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="04a1" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">Part 1 was:</h2><p id="85c3" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><a class="af ou" href="https://medium.com/towards-data-science/who-are-chatbots-and-what-are-they-to-you-5c77d9201d11" rel="noopener">Who are chatbots (and what are they to you)?</a> Afterthoughts: <a class="af ou" href="https://medium.com/towards-data-science/four-elephants-in-the-room-with-chatbots-82c48a823b94" rel="noopener">Four elephants in a room with chatbots</a></p><h2 id="5d23" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">Part 2 was:</h2><p id="95ac" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><a class="af ou" href="https://medium.com/towards-data-science/syntax-the-language-form-612257c4aa5f" rel="noopener">Syntax: The Language Form</a></p><h2 id="50ee" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">Part 3 was:</h2><p id="bc05" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41" rel="noopener">Semantics: The Meaning of Language</a></p><h2 id="7150" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">THIS IS Part 4:</h2><ol class=""><li id="082a" class="mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm ov ow ox bk"><a class="af ou" href="#3cda" rel="noopener ugc nofollow">Language models, celebrities, and steam engines</a></li><li id="25a0" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm ov ow ox bk"><a class="af ou" href="#63f9" rel="noopener ugc nofollow">Evolution of learning</a></li></ol><ul class=""><li id="7eb3" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><a class="af ou" href="#deb1" rel="noopener ugc nofollow">2.1. Learning causes and superstitions</a></li><li id="a115" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#e77a" rel="noopener ugc nofollow">2.2. General learning framework</a></li><li id="4ef7" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#0f24" rel="noopener ugc nofollow">2.3. Examples: From pigeons to perceptrons</a></li></ul><p id="bda8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">3. <a class="af ou" href="#12c0" rel="noopener ugc nofollow">Learning functions</a></p><ul class=""><li id="e13f" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><a class="af ou" href="#4211" rel="noopener ugc nofollow">3.1. Why learning is possible</a></li><li id="dc6f" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#ca7b" rel="noopener ugc nofollow">3.2. Decomposing continuous functions: Kolmogorov-Arnold</a><a class="af ou" href="#d246" rel="noopener ugc nofollow">⁶</a></li><li id="ecb6" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#e045" rel="noopener ugc nofollow">3.3. Wide learning</a></li><li id="2081" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#ece6" rel="noopener ugc nofollow">3.4. Approximating continuous functions: Cybenko et al</a></li><li id="467e" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#8395" rel="noopener ugc nofollow">3.5. Deep learning</a></li></ul><p id="093a" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">4. <a class="af ou" href="#0e5b" rel="noopener ugc nofollow">Learning channels and paying attention</a></p><ul class=""><li id="555f" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><a class="af ou" href="#4f8f" rel="noopener ugc nofollow">4.1 Channeling through concepts</a></li><li id="574a" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#b1bf" rel="noopener ugc nofollow">4.2 Static channel learning: RNN, LSTM…</a></li><li id="b40e" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#1d50" rel="noopener ugc nofollow">4.3 Dynamic channel learning: Attention, Transformer…</a></li></ul><p id="8b05" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">5. <a class="af ou" href="#50f8" rel="noopener ugc nofollow">Beyond hallucinations</a></p><ul class=""><li id="9a97" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><a class="af ou" href="#16d1" rel="noopener ugc nofollow">5.1. Parametric learning framework</a></li><li id="1b77" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#47ec" rel="noopener ugc nofollow">5.2. Self-learning</a></li><li id="12b1" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><a class="af ou" href="#72b2" rel="noopener ugc nofollow">5.3. Self-confirming beliefs</a></li></ul><p id="bf3b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><a class="af ou" href="#aa48" rel="noopener ugc nofollow">Attributions</a></p><p id="4edb" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><a class="af ou" href="http://f974" rel="noopener ugc nofollow" target="_blank">Notes</a></p></div></div></div><div class="ab cb nn no np nq" role="separator"><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3cda" class="pe nw fq bf nx pf pg gv ob ph pi gy of pj pk pl pm pn po pp pq pr ps pt pu pv bk">1. Language models, celebrities, and steam engines</h1><p id="27fd" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">Anyone can drive a car. Most people even know what the engine looks like. But when you need to fix it, you need to figure out how it works.</p><p id="19f6" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Anyone can chat with a chatbot. Most people know that there is a Large Language Model (LLM) under the hood. There are lots and lots and lots of articles describing what an LLM looks like. Lots of colorful pictures. Complicated meshes of small components, as if both mathematical abstraction and modular programming still wait to be invented. YouTube channels with fresh scoops on LLM celebrities. We get to know their parts and how they are connected, we know their performance, we even see how each of them changes a heat map of inputs to a heat map of outputs. One hotter than the other. But do we understand how they work? Experts say that they do, but they don’t seem to be able to explain it even to each other, as they continue to disagree about pretty much everything.</p><p id="374a" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Every child, of course, knows that it can be hard to explain what you just built. Our great civilization built lots of stuff that it couldn’t explain. Steam engines have been engineered for nearly 2000 years before scientists explained how they extract work from heat. There aren’t many steam engines around anymore, but there are lots of language engines and a whole industry of scientific explanations how they extract sense from references. The leading theory is that Santa Claus descended from the mountain and gave us the transformer architecture carved in a stone tablet.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px py"><img src="../Images/3bcce5a27ad24729bcbacb2d352afb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RR8CLPydX8d4KZegTCvxZg.jpeg"/></div></div></figure><p id="7857" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Transformers changed the world, spawned offspring and competitors. . . Just like steam engines. Which may be a good thing, since steam engines did not exterminate their creators just because the creators didn’t understand them.</p><p id="53e2" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">I wasn’t around in the times of steam engines, but I was around in the times of bulky computers, and when the web emerged and everything changed, and when the web giants emerged and changed the web. Throughout that time, AI research seemed like an effort towards the intelligent design of intelligence. It didn’t change anything, because intelligence, like life, is an evolutionary process<em class="qk">, </em>not a product of intelligent design<a class="af ou" href="#858d" rel="noopener ugc nofollow">¹</a>.<em class="qk"> </em>But now some friendly learning machines and chatbot AIs evolved and everything is changing again. Having survived and processed the paradigm shifts of the past, I am trying to figure out the present one. Hence this course and these writings. On one hand, I probably stand no chance to say anything that hasn’t been said before. Even after a lot of honest work, I remain a short-sighted non-expert. On the other hand, there are some powerful tools and ideas that evolved in the neighborhood of AI that AI experts don’t seem to be aware of. People clump into research communities, focus on the same things, and ignore the same things. Looking over the fences, neighbors sometimes understand neighbors better than they understand themselves. This sometimes leads to trouble. An ongoing temptation. Here is a view over the fence.</p><h1 id="63f9" class="pe nw fq bf nx pf ql gv ob ph qm gy of pj qn pl pm pn qo pp pq pr qp pt pu pv bk">2. Evolution of learning</h1><h2 id="deb1" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk"><a class="af ou" href="http://deb1" rel="noopener ugc nofollow" target="_blank">2.1. Learning causes and superstitions</a></h2><p id="0418" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">Spiders are primed to build spider webs. Their engineering skills to weave webs are programmed in their genes. They are pretrained builders and even their capability to choose and remember a good place for a web is automated.</p><p id="d39d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Dogs and pigeons are primed to seek food. Their capabilities to learn sources and actions that bring food are automated. In a famous experiment, physiologist Pavlov studied one of the simplest forms of learning, usually called <em class="qk">conditioning</em>.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qq"><img src="../Images/8ae6dd8b177d587867dbbe024c5abada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0v2-KJqHf5rFAHcygv3Mng.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">If the bell rings whenever the dog is fed, he learns to salivate whenever the bell rings.</figcaption></figure><p id="0f6b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Continuing in the same vein, psychologist Skinner showed that pigeons could even develop a form of superstition, also by trying to learn where the food comes from.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qw"><img src="../Images/7fca4475cde3c65f6bc8d6df78157582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0yBOM4NdLke5Iryax9NCQ.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">If food arrives while the pigeon is pecking, she learns that pecking conjures food</figcaption></figure><p id="f751" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Skinner fed pigeons at completely random times, with no correlation with their behaviors. About 70% of them developed beliefs that they could conjure food. If a pigeon happened to be pecking on the ground, or ruffling feathers just before the food arrived, this would make them engage in this action more frequently, which increased the chance that the food would arrive while they were performing that action. If one of the random associations, say of food and pecking, after a while prevails, then it gets promoted into a ritual dance for food. Each time, the food eventually arrives and confirms that the ritual works.</p><p id="75dc" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Humans are primed to seek causes and predict effects. Like pigeons, they associate coinciding events as correlated and develop superstitions, promoting coincidences into causal theories. While pigeons end up pecking empty surfaces to conjure grains, humans build monumental systems of false beliefs, attributing their fortunes and misfortunes, say, to the influence of stars millions of light years away, or to their neighbor’s evil eye, or to pretty much anything that can be seen, felt, or counted<a class="af ou" href="#a74e" rel="noopener ugc nofollow">²</a>.</p><p id="9997" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">But while our causal beliefs are shared with pigeons, our capabilities to build houses and span bridges are not shared with spiders. Unlike spiders, we are not primed to build but have to <em class="qk">learn </em>our engineering skills. <em class="qk">We are primed to learn.</em></p><h2 id="e77a" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">2.2. General learning framework</h2><p id="2056" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">A bird’s eye view of the scene of learning looks something like this:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qx"><img src="../Images/0c994833ec2d8ac882a394db11a8c84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JsqlHCO7n9UriQp2nUqlPw.jpeg"/></div></div></figure><p id="cef6" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The inputs come from the left. The main characters are:</p><ul class=""><li id="36fc" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk">a process <em class="qk">F</em>, the <em class="qk">supervisor</em> in supervised learning (<a class="af ou" href="https://medium.com/p/5c77d9201d11#e1d2" rel="noopener">Turing called it a “teacher”</a>) processing input data <em class="qk">x</em> of type <em class="qk">X </em>to produce output classes or parameters <em class="qk">y</em> of type <em class="qk">Y</em>;</li><li id="1d0a" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">an <strong class="mt ga">a</strong>-indexed family of functions 𝒰(−)<strong class="mt ga">a</strong>, where 𝒰 is a <em class="qk">learning machine </em>or <em class="qk">interpreter </em>(<a class="af ou" href="https://medium.com/p/5c77d9201d11#2e21" rel="noopener">Turing called it a “pupil”</a>) and the indices <strong class="mt ga">a</strong> are the <em class="qk">models</em>, usually expressed as <em class="qk">programs</em>; lastly, there is</li><li id="810a" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">a function ℒ, usually called the <em class="qk">loss</em>, comparing the outputs <em class="qk">y </em>= <em class="qk">F</em>(<em class="qk">x</em>) with the predictions <em class="qk">ỹ</em> = 𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong> and delivering a real number ℒ(<em class="qk">y</em>,<em class="qk">ỹ</em>) that measures their difference.</li></ul><p id="5949" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The learner overseeing the learning framework is given a finite set</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qy"><img src="../Images/68bfd63f4334bc5d85cfac7fe08f1098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WANvsYPR6RYO15hUKW02tA.jpeg"/></div></div></figure><p id="4591" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">where the <em class="qk">x</em>s are samples from a source <em class="qk">X </em>and the <em class="qk">y</em>s are the corresponding samples from the random variable <em class="qk">Y </em>= <em class="qk">F</em>(<em class="qk">X</em>). The learner’s task is to build a model <strong class="mt ga">a</strong> that minimizes the losses</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qz"><img src="../Images/f4652087b372746f1517dc1c3ce14083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dwZI2m5p5eUCXLYOeRCTQ.jpeg"/></div></div></figure><p id="8cd2" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">where <em class="qk">yi </em>= <em class="qk">F</em>(<em class="qk">xi</em>) and <em class="qk">ỹi </em>= 𝒰(<em class="qk">xi</em>)<strong class="mt ga">a</strong> for <em class="qk">i </em>= 1,2,…,<em class="qk">n</em>. Since some of the losses may increase when the others decrease, the learning algorithm is required to minimize the average <em class="qk">guessing risk</em></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ra"><img src="../Images/26df659663e5b994ef42eb2f950d4bd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UnGSxLbifbM-yZHwnCNLg.jpeg"/></div></div></figure><p id="3804" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">where [𝒰(<em class="qk">xi</em>)<strong class="mt ga">a</strong>] denotes the frequency with which the guesses 𝒰(<em class="qk">xi</em>)<strong class="mt ga">a </strong>are tried. Once a model <strong class="mt ga">a </strong>is found for which the risk is minimal, the function <em class="qk">F </em>is approximated by running the machine 𝒰 on a program implementing the model <strong class="mt ga">a</strong> and we write</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ra"><img src="../Images/15346bce3485caea133988d78905e635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kIgHnLcDZqK1gbkD6heeZw.jpeg"/></div></div></figure><p id="2107" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Potato, potahto, tomato, tomahto.</strong> What are the outcomes of learning? We just called the outcome <strong class="mt ga">a</strong> of a round of supervised learning a <em class="qk">model </em>of the supervisor <em class="qk">F</em>. Since <strong class="mt ga">a </strong>is an attempt to describe <em class="qk">F</em>, most logicians would call it a <em class="qk">theory </em>of <em class="qk">F</em>. If the interpretations 𝒰(<em class="qk">X</em>)<strong class="mt ga">a</strong> describe <em class="qk">F</em>(<em class="qk">X</em>) truthfully, the logicians would say that <em class="qk">F </em>is actually a model of the theory <strong class="mt ga">a</strong> under the semantical interpretation by 𝒰. So there is a terminological clash between the theory of learning, where <strong class="mt ga">a </strong>is a model of <em class="qk">F</em>, and logic, where <em class="qk">F </em>is a model of <strong class="mt ga">a. </strong>In a further contribution to the confusion, statisticians say that <strong class="mt ga">a </strong>is a <em class="qk">hypothesis </em>about <em class="qk">F</em>. If a hypothesis or a theory is believed to be true, then it is a part of the learner’s <em class="qk">belief state</em>. In the <a class="af ou" href="#50f8" rel="noopener ugc nofollow">final section</a>, we will arrive at a curious construction illustrating a need for studying the <em class="qk">belief logic of machine learning</em><a class="af ou" href="#0dbf" rel="noopener ugc nofollow">³</a>. We stick with calling the learning outcomes <strong class="mt ga">a </strong><em class="qk">models</em> since that seems to be the common usage. An important wrinkle, is, however, that a model <strong class="mt ga">a</strong> of <em class="qk">F </em>needs to be <em class="qk">executable </em>in order to allow computing the predictions 𝒰(<em class="qk">X</em>)<strong class="mt ga">a</strong> of the values <em class="qk">F</em>(<em class="qk">X</em>). But if you think about it, executable models are what we normally call <em class="qk">programs</em>. In summary, the outcome of a learning process is an executable model. The cumulative outcome of learning is the learner’s belief state. <strong class="mt ga"><em class="qk">The process of learning is the search for learnable programs.</em></strong></p><p id="45eb" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">All learning is language learning.</strong> In general, the process <em class="qk">F </em>to be learned is given as a channel, which means that the outputs are context-dependent. The story from <a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1" rel="noopener">Sec. 3.2 of the <em class="qk">Semantics </em>part</a> applies. The channel inputs <em class="qk">xj </em>depend on the earlier inputs <em class="qk">xi</em>, <em class="qk">i </em>&lt; <em class="qk">j</em>. When there is feedback, <em class="qk">xj </em>also depends on the earlier outputs <em class="qk">yi</em>, <em class="qk">i </em>&lt; <em class="qk">j</em>. To be able to learn <em class="qk">F</em>’s behavior, the learning machine 𝒰 must also be a channel. <em class="qk">Since capturing channel dependencies requires syntactic and semantic references, there is a language behind every learner</em>, whether it is apparent or not. The semiotic analyses of the languages of film, music, or images, etc., describe genuine syntactic and semantic structures. Different organisms learn in different ways, but for humans and their machines, all learning is language learning.</p><h2 id="0f24" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">2.3. Examples: From pigeons to perceptrons</h2><p id="7da0" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">Pigeon superstition. </strong>The function <em class="qk">F </em>that a pigeon learns to predict is a source of food. It can be viewed as a channel <em class="qk">[X </em>⊢ <em class="qk">Y]</em>, where the values <em class="qk">x</em>1, <em class="qk">x</em>2, . . . of type <em class="qk">X </em>are moments in time and <em class="qk">Y </em>= <em class="qk">F</em>(<em class="qk">X</em>) is a random variable, delivering seeds with a fixed probability. Suppose that <em class="qk">Y </em>= 1 means “food” and <em class="qk">Y </em>= 0 means “no food”. If we take the possible models (programs, beliefs) <strong class="mt ga">a </strong>to correspond to the elements of a set of actions available to the pigeon, then the pigeon is trying to learn for which actions <strong class="mt ga">a</strong> and at which moments <em class="qk">x </em>to output 𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong> = 1 and when to output 0. The loss ℒ(<em class="qk">y</em>, 𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong>) = |<em class="qk">y-</em>𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong>| is 0 if the food is delivered just when the pigeon takes the action <strong class="mt ga">a</strong>. After a sufficient amount of time, the random output <em class="qk">Y </em>= 1 will almost surely coincide with a prediction 𝒰(<em class="qk">X</em>)<strong class="mt ga">a</strong> = 1 for some <strong class="mt ga">a</strong>. The pigeon will then learn to do <strong class="mt ga">a</strong> more often and increase the chance of such coincidences. If one <strong class="mt ga">a</strong> prevails, the pigeon will learn that it causes food.</p><p id="297b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Statistical testing. </strong>Science is a family of methods designed to overcome superstition and prejudice. The idea is to prevent pigeon-style confirmations by systematically testing hypotheses and only accepting significant correlations. The mathematical foundations of statistical hypothesis testing were developed in the 1920s by Ronald Fisher, and have remained the bread and butter of scientific practices. The crucial assumption is that the interpretation 𝒰 for any hypothesis <strong class="mt ga">a</strong> is given together with its probability density <em class="qk">p</em><strong class="mt ga">a</strong>(<em class="qk">x</em>) = <em class="qk">d</em>𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong> . The loss ℒ is then estimated by the length of the description of this probability. If the value of <em class="qk">p</em><strong class="mt ga">a</strong>(<em class="qk">x</em>) is described by a string of digits, its description length is proportional to −log <em class="qk">p</em><strong class="mt ga">a</strong>(<em class="qk">x</em>). The guessing risk is thus ℛ(<strong class="mt ga">a</strong>) = ∫− log <em class="qk">p</em><strong class="mt ga">a</strong>(<em class="qk">x</em>)<em class="qk">d</em>𝒰(<em class="qk">x</em>)<strong class="mt ga">a</strong>. Values of this kind are studied in information theory as measures of uncertainty. Minimizing ℛ(<strong class="mt ga">a</strong>) thus boils down to choosing the hypothesis a that minimizes the uncertainty of sampling 𝒰 for <strong class="mt ga">a</strong>. Fisher recommended the learning algorithm that selects the hypothesis with a <em class="qk">maximal likelihood</em>.</p><p id="35a7" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The basic shortcoming of statistical testing is that the densities <em class="qk">p</em><strong class="mt ga">a</strong> must be known. They are presumed to arise from scientists’ minds, together with their hypotheses parametrized by <strong class="mt ga">a</strong>. Statistics thus provides a testing service, but the actual process of learning the hypotheses <strong class="mt ga">a</strong> is out of scope and left to the magic of insight and creativity. While Kolmogorov and his students were pondering this problem for decades and eventually solved it, a central part of the solution emerged inadvertently, and from an unexpected direction.</p><p id="43a9" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Perceptrons.</strong> In 1943, McCulloch and Pitts proposed a mathematical model of the neuron. It boiled down to a state machine, like Turing’s original 1936 computer, just simpler, since it didn’t have the external memory. In the late 1950s, Frank Rosenblatt was working on expanding the model of a neuron into a model of the brain. It was a very ambitious project.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rb"><img src="../Images/3e804fdec6259980b5176ec417cd51f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-A3xE1sVQwqbqjZnXLdl8w.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">Illustration from Rosenblatt’s 1958 project report to the Office of Naval Research. — Public domain</figcaption></figure><p id="a874" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Rosenblatt, however, arrived at a component simpler than the McCulloch-Pitts neuron. He called it <em class="qk">perceptron</em>, to emphasize the difference of his project from the “various engineering projects concerned with automatic pattern recognition and ‘artificial intelligence’ ”. Nevertheless, the project generated news reports with titles like “Frankenstein Monster Designed by Navy Robot That Thinks”, as Rosenblatt duly reports in his book<a class="af ou" href="#04e9" rel="noopener ugc nofollow">⁴</a>.</p><p id="81b0" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga"><em class="qk">Mathematical neurons</em></strong> were defined as pairs <strong class="mt ga">a</strong> = <em class="qk">(b, ⟨w |)</em>, where<a class="af ou" href="#7754" rel="noopener ugc nofollow">⁵</a></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rc"><img src="../Images/685ee22d76a5c57265d94dfb4e6f5f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Pf2AroeWndTyrmgaafjZA.jpeg"/></div></div></figure><p id="9c2c" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">and <em class="qk">b </em>is a scalar. It is meant to be a very simple program intepreted by the interpreter 𝒰. To evaluate <strong class="mt ga">a</strong> = <em class="qk">(b, ⟨w |) </em>on an input vector input vector | <em class="qk">x </em>⟩, the interpreter 𝒰 applies the projection ⟨<em class="qk">w </em>| on | <em class="qk">x </em>⟩ to get the inner product ⟨<em class="qk">w </em>| <em class="qk">x</em>⟩, which measures the length of the projection of either of the vectors on the other, and then it outputs the sign of the difference ⟨<em class="qk">w </em>| <em class="qk">x</em>⟩ − <em class="qk">b:</em></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rd"><img src="../Images/164dc55568f7482bc20ce6b309a98f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ix-TpVE8LCMVoP_3QsFK5A.jpeg"/></div></div></figure><p id="1c70" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">For a more succinct view, the pair <strong class="mt ga">a</strong> = <em class="qk">(b</em>, ⟨<em class="qk">w </em>|) and the input | <em class="qk">x </em>⟩ are often modified to</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px re"><img src="../Images/a574620827c0a2ab52b1486b235983d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7uvQrb--xYrvJhaT4cTvQ.jpeg"/></div></div></figure><p id="a959" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">so that the interpretation of a neuron boils down to</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rf"><img src="../Images/1bf3c497036c5ec001d527c51f3e5d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Vq8P-eZM6DG1BY3U24yMA.jpeg"/></div></div></figure><p id="20d1" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga"><em class="qk">Perceptrons</em></strong> are compositions of such neurons. If a neuron is presented as a single row vector, then a perceptron is an (<em class="qk">n </em>+ 1)-tuple of row vectors</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px re"><img src="../Images/b0de2483c9e7fd9cd0c13b6a8ae79518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qC2S1XEEefZ6lwZXwi49Jg.jpeg"/></div></div></figure><p id="f5a2" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">On an input | <em class="qk">x</em>⟩, the interpretation of a perceptron <strong class="mt ga">a </strong>computes</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px re"><img src="../Images/1f7918759774f69c8d77638d766f8aaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZUVEycUy0UtNB29uP5NHA.jpeg"/></div></div></figure><p id="b3dc" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">For a more succinct view, the <em class="qk">n</em>-tuple of vectors ⟨<em class="qk">w</em>1 |, . . . , ⟨<em class="qk">wn </em>| can be arranged into the matrix</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rg"><img src="../Images/7d1b7a7d209964290c21c98e9ff51128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g81i5oIo1R2yukPG4h8ghg.jpeg"/></div></div></figure><p id="209e" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">so that the perceptron <strong class="mt ga">a</strong> = (⟨<em class="qk">v</em>|, ⟨<em class="qk">w</em>1 |,…,⟨<em class="qk">wn </em>|) boils down to<strong class="mt ga"> a</strong> = (⟨<em class="qk">v</em>|, <em class="qk">W)</em> and its interpretation becomes</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rh"><img src="../Images/1916147fd646b3e9dd170034b5110296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13Y1Byr_pUMrFGjwyXKeig.jpeg"/></div></div></figure><p id="ad84" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To summarize in diagrams, here are the two presentations of a neuron on the left and the two presentations of a perceptron on the right.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ri"><img src="../Images/3e749bfb2dc23f8474b76caab7d7b485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTNHGS1WI-QgrC-BWR0QBw.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">Rosenblatt’s neuron and perceptron</figcaption></figure><p id="25b1" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The first row shows the neuron and the perceptron in the original form, with the thresholds <em class="qk">bj</em>. The second row shows the versions where each <em class="qk">bj </em>is absorbed as the 0-th component of the weight vector ⟨<em class="qk">wj </em>|.</p><p id="1fd8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Perceptrons were a breakthrough into machine learning and inductive inference as two sides of the same coin. </strong>Statistics provided the formal methods for hypothesis testing but left the task of learning and inferring hypotheses to informal methods and the magic of creativity. Perceptron training was the first formal method for inductive inference. Nowadays, this method looks obvious. The learner initiates the weights | <em class="qk">w </em>⟩ and the thresholds <em class="qk">b </em>to arbitrary values, runs the interpreter 𝒰<em class="qk"> </em>to generate predictions, compares them with the training data supplied by the supervisor <em class="qk">F</em>, and updates the weights proportionally to the losses ℒ. This didn’t seem like a big deal even to Frank Rosenblatt, who wrote that</p><blockquote class="rj rk rl"><p id="7aec" class="mr ms qk mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">the perceptron program [was] not primarily concerned with the invention of devices for “artificial intelligence”, but rather with investigating the physical structures and neurodynamic principles which underlie “natural intelligence”.</p></blockquote><p id="aedf" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Rosenblatt laid the stepping stone into machine learning while attempting to model the learning process in human brains. Even the very first learning machine was not purposefully designed but evolved spontaneously.</p><p id="2bf3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It is often said that airplanes were not built by studying how the birds fly and that intelligent machines will not be built by looking inside people’s heads. But there is more at hand. Perceptrons opened an alley into <strong class="mt ga">learning as a <em class="qk">universal computational process.</em></strong><em class="qk"> Machine learning and human learning are particular implementations of the universal process of learning</em>, which is a natural process that evolves and diversifies. Machine learning models offer insights into a common denominator of all avatars of learning. The pattern of perceptron computation will be repeated on each of the models presented in the rest of this note.</p><h1 id="12c0" class="pe nw fq bf nx pf ql gv ob ph qm gy of pj qn pl pm pn qo pp pq pr qp pt pu pv bk">3. Learning functions</h1><h2 id="4211" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">3.1. Why learning is possible</h2><p id="067a" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">To understand why learning is possible, we first consider the special case when channel <em class="qk">F </em>is memoryless and deterministic: an ordinary function.</p><p id="c46c" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Learnable functions are continuous. </strong>What can be learned about a function <em class="qk">F</em>:<em class="qk">X</em>⟶<em class="qk">Y </em>from a finite set of pairs (<em class="qk">x</em>1,<em class="qk">y</em>1), (<em class="qk">x</em>2,<em class="qk">y</em>2),…,(<em class="qk">xn</em>,<em class="qk">yn</em>), where <em class="qk">F</em>(<em class="qk">xi</em>) = <em class="qk">yi</em>? Generally nothing. Knowing <em class="qk">F</em>(<em class="qk">x</em>) does not tell anything about <em class="qk">F</em>(<em class="qk">x</em>′), unless <em class="qk">x </em>and <em class="qk">x</em>′ are related in some way, and <em class="qk">F </em>preserves their relation. To generalize the observed sample (<em class="qk">x</em>1, <em class="qk">y</em>1), . . . , (<em class="qk">xn</em>, <em class="qk">yn</em>) and predict a classification <em class="qk">F</em>(<em class="qk">x</em>′) = <em class="qk">y</em>′ for an unobserved data item <em class="qk">x</em>′, it is necessary that</p><ul class=""><li id="c711" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><em class="qk">x</em>′ is related to <em class="qk">x</em>1,…,<em class="qk">xn</em>,</li><li id="21a1" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><em class="qk">y</em>′ is related to <em class="qk">y</em>1,…,<em class="qk">yn</em>, where <em class="qk">yi </em>= <em class="qk">F</em>(<em class="qk">xi</em>) for <em class="qk">i </em>= 1,…,<em class="qk">n</em>, and</li><li id="6c00" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><em class="qk">F </em>preserves the relations.</li></ul><p id="6dd8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">If the sets of the <em class="qk">x</em>s and the <em class="qk">y</em>s in such relationships are viewed as <em class="qk">neighborhoods</em>, then the datatype <em class="qk">X </em>and the classifier type <em class="qk">Y </em>become <em class="qk">topological </em>spaces. The neighborhoods form topologies. Don’t worry if you don’t know the formal definition of a topology. It is just an abstract way to say that <em class="qk">x </em>and <em class="qk">x</em>′ live in the same neighborhood. A function <em class="qk">F</em>:<em class="qk">X</em>⟶<em class="qk">Y </em>is <em class="qk">continuous </em>when it maps neighbors to neighbors. And the neighborhoods don’t have to be physical proximities. Two words with similar meanings live in a semantical neighborhood. Any kind of relation can be expressed in terms of neighborhoods. So if <em class="qk">x</em>′ is related with <em class="qk">x</em>1 and <em class="qk">x</em>2, and <em class="qk">F </em>is continuous, then <em class="qk">y</em>′ = <em class="qk">F</em>(<em class="qk">x</em>′) is related with <em class="qk">y</em>1 = <em class="qk">F</em>(<em class="qk">x</em>1) and <em class="qk">y</em>2 = <em class="qk">F</em>(<em class="qk">x</em>2). That allows us to learn from a set of pairs (<em class="qk">x</em>1, <em class="qk">y</em>1), . . . , (<em class="qk">xn</em>, <em class="qk">yn</em>) where <em class="qk">F</em>(<em class="qk">xi</em>) = <em class="qk">yi </em>that <em class="qk">F</em>(<em class="qk">x</em>′) = <em class="qk">y</em>′ also holds. Then we can add the pair (<em class="qk">x</em>′, <em class="qk">y</em>′) to the list as a prediction. Without the neighborhoods and the continuity, we cannot make such predictions. To be learnable a function must be continuous.</p><p id="aaab" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">There are many ways in which this is used, and many details to work out. For the moment, just note that <em class="qk">learning is based on associations. </em>You associate a set of names <em class="qk">X </em>with a set of faces <em class="qk">Y </em>along a continuous function <em class="qk">F</em>:<em class="qk">X </em>⟶<em class="qk">Y. </em>You remember the face <em class="qk">F</em>(Allison) by searching through the pairs (<em class="qk">x</em>1, <em class="qk">y</em>1), . . . , (<em class="qk">xn</em>, <em class="qk">yn</em>) where the names <em class="qk">xi </em>are associated with Allison’s. Since <em class="qk">F </em>is continuous, the faces <em class="qk">yi </em>= <em class="qk">F</em>(<em class="qk">xi</em>) must be associated with Allison’s. Therefore, if you find a face of a neighbor of Allison’s name, then you can find Allison’s face in the neighborhood of the face of Allison’s neighbor. This is how <em class="qk">associative memory </em>works: as a family of continuous functions. The <em class="qk">key-value associations</em> in databases work similarly. Both in human memory and in databases, associative memory is implemented using referential neighborhoods. Functions are learnable when they preserve associations. They preserve associations when they are continuous.</p><p id="86e2" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Continuous functions can be partially evaluated and linearly approximated. </strong>The Fundamental Theorem of Calculus says, roughly, that the derivative and the integral, as operations on functions, are each other’s inverses. The integral approximates with arbitrary precision any differentiable function by linear combinations of step functions that approximate the derivative of the function. Any differentiable function is linearly approximable by piecewise linear functions.</p><p id="b094" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">A function that is just continuous (not differentiable) may not be approximable by piecewise linear functions. Yet it turns out that it can always be approximated by linear combinations of pieces of a continuous function (not linear or polynomial), usually called <em class="qk">actuation. </em>The approximating linear combinations of this nonlinear function are <em class="qk">learnable</em>. Hence machine learning.</p><p id="431d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">On the other hand, the approximability of continuous functions has remained one of the big secrets of calculus. The fact that <strong class="mt ga"><em class="qk">all continuous functions can be decomposed into sums of single-variable continuous functions</em></strong><em class="qk"> </em>defies most people’s intuitions. It says that, as far as computations are concerned, there are no genuine multi-dimensional phenomena among continuous functions. All those complicated multi-variable functions you may have seen in a vector calculus textbook, or encountered in practice if you are an engineer or a scientist — they can all be partially evaluated, each variable separately. Which is why they can be learned.</p><h2 id="ca7b" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">3.2 Decomposing continuous functions: Kolmogorov-Arnold<a class="af ou" href="#d246" rel="noopener ugc nofollow">⁶</a></h2><p id="339a" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">Hilbert’s 13th Problem.</strong> Back in the year 1900, the famous mathematician David Hilbert offered his famous list of 23 mathematical problems for the next century. Number 13 on the list was the question if all functions with 3 variables can be expressed by composing functions of 2 variables. Hilbert conjectured that a specific function, the formula for the solutions of the equation <em class="qk">x</em>⁷ + <em class="qk">ax³</em> + <em class="qk">bx</em>² + <em class="qk">cx </em>+ 1 = 0 expressed in terms of the coefficients <em class="qk">a</em>, <em class="qk">b</em>, and <em class="qk">c</em>, could not be decomposed into functions of pairs of the coefficients. More than half-way through the century, 19-year-old Vladimir Arnold proved that all continuous functions with 3 variables can be decomposed into continuous functions with 2 variables and disproved Hilbert’s conjecture. Next year, Arnold’s thesis advisor Andrey Kolmogorov proved a stunning generalization. The theorem has been strengthened and simplified ever since. Early simplifications were based on the following embedding of the <em class="qk">d</em>-dimensional cube into the (2<em class="qk">d</em>+1)-dimensional cube, constructed to allow separating the <em class="qk">d </em>variables in any continuous function.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rm"><img src="../Images/0fa93b3737e5e2706790a8e1dd455902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PuEVlQeDOYCIS5AlU38omw.jpeg"/></div></div></figure><p id="a678" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Together with a fixed (2<em class="qk">d</em>+1)-dimensional vector ⟨<em class="qk">v </em>| , the embedding<a class="af ou" href="#a35f" rel="noopener ugc nofollow">⁷</a> <em class="qk">W</em>, yields the claimed decomposition.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rn"><img src="../Images/9e71bdeaf3888d4c4fe110fee6cab9ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98k8rOeYUAGdEk-eG9CXHQ.jpeg"/></div></div></figure><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ro"><img src="../Images/fe99a93b37f4643fc3153b462fad4fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZjLTA9bKfEUm0jE7d-2wA.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">Kolmogorov-Arnold Decomposition</figcaption></figure><p id="69e3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Comments and explanations. </strong>Unfolding the decomposition yields</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rp"><img src="../Images/8c888e333ae7118389191320c5882db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I2jsd78OVdH2FJJ_re6wXg.jpeg"/></div></div></figure><p id="514b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Only 𝜑 depends on <em class="qk">f</em>, whereas <em class="qk">W </em>and <em class="qk">v </em>are given globally, for all functions of<em class="qk"> d </em>variables. They are not unique and <em class="qk">W </em>can be chosen so that ⟨<em class="qk">v</em>| is a vector of 1s, as it is assumed in this unfolded version. The constructions not only disproved Hilbert’s conjecture, but still defy most people’s geometric intuitions. The reason may be that we tend to think in terms of smooth functions, whereas the funcions 𝜓 and 𝜑 are heavily fractal. They are constructed using copies of the Devil Staircase or space-filling curves. The geometric interpretation of the embedding <em class="qk">W </em>is that the (2<em class="qk">d</em>+1)-tuple of 𝜓s draws a curve in the (2<em class="qk">d</em>+1)-dimensional cube and copies of that curve span a homeomorphic image of the <em class="qk">d</em>-dimensional cube:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rq"><img src="../Images/4183261dd51555a36478795ffce8233f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xxie_N9KV7ni7Ygnm-TfRQ.jpeg"/></div></div></figure><p id="87a8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This is the first component of <em class="qk">W </em>in the diagram above. The projections on ⟨<em class="qk">w</em>| of vectors from within the <em class="qk">d</em>-cube determine the linear combinations of copies of 𝜓 whose inverse images iteratively fill the <em class="qk">d</em>-cube. Kolmogorov’s original construction partitioned the mapping <em class="qk">f </em>along the edges of the<em class="qk"> d-</em>cube and combined<em class="qk"> d </em>different<em class="qk"> </em>functions 𝜑 to represent <em class="qk">f </em>. Sprecher and Lorentz later noticed that additional stretching allows capturing all parts of <em class="qk">f </em>by a single 𝜑. This is possible because the dependency of <em class="qk">f </em>on each of its<em class="qk"> d </em>variables can be approximated with arbitrary precision on a null-subset of its domain, and the null-subsets of <em class="qk">[0,1]</em> can be made disjoint. The upshot is that <em class="qk">the only genuinely multi-variable continuous function is the addition</em>. The multiple inputs for multi-variable continuous functions can always be preprocessed in such a way that each input is processed separately, by a single-variable function. The output of the original multi-variable function is then obtained by adding up the outputs of the single-variable components. <em class="qk">Continuous functions are thus partially evaluated, each input separately.</em></p><p id="ea4b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The price to be paid is that the single-variable continuous functions that perform the preprocessing and the processing are complicated, ineffective, and constructed through iterative approximations. For a long time, the iterative fugue of Kolmogorov’s proof was viewed as a glimpse from the darkness of a world of complexities beyond any our our imagination or utility. Then in the late 1980s, Hecht-Nielsen noticed that the Kolmogorov-Arnold decomposition seemed related to the perceptron architecture, as the diagrams above also suggest. What is going on?</p><h2 id="e045" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">3.3 Wide learning</h2><p id="5f08" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">So far we studied<strong class="mt ga"> </strong>a<strong class="mt ga"><em class="qk"> </em></strong><a class="af ou" href="#2f11" rel="noopener ugc nofollow"><strong class="mt ga"><em class="qk">theoretic</em> </strong>construction</a> providing</p><ul class=""><li id="5c26" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk">an <strong class="mt ga"><em class="qk">exact</em></strong><em class="qk"> representation</em> of<em class="qk"> f </em>using</li><li id="b5a4" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">a projection-embedding pair (⟨<em class="qk">v</em>|<em class="qk">,W)</em> <strong class="mt ga"><em class="qk">independent</em></strong> on<em class="qk"> f </em>and</li><li id="77a7" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">an <strong class="mt ga"><em class="qk">approximate</em></strong><em class="qk"> construction </em>𝜑 <strong class="mt ga"><em class="qk">dependent</em></strong> on<em class="qk"> f.</em></li></ul><p id="1686" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Now we turn to a <a class="af ou" href="#5646" rel="noopener ugc nofollow"><strong class="mt ga"><em class="qk">practical</em> </strong>construction</a> providing</p><ul class=""><li id="7c80" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk">an <strong class="mt ga"><em class="qk">approximate</em></strong><em class="qk"> representation </em>of<em class="qk"> f </em>using</li><li id="a9a5" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">a projection-embedding pair (⟨<em class="qk">v</em>|<em class="qk">,W)</em> <strong class="mt ga"><em class="qk">dependent</em></strong> on<em class="qk"> f </em>and</li><li id="d21a" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk">an <strong class="mt ga"><em class="qk">exact</em></strong><em class="qk"> construction σ </em><strong class="mt ga"><em class="qk">independent</em></strong> on<em class="qk"> f.</em></li></ul><p id="ec84" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The step from the Continuous Decomposition above to the Neural Approximation below is illustrated by comparing the <a class="af ou" href="#3071" rel="noopener ugc nofollow">diagram</a> of the KA representation above with the following diagrams of the CHSW representation:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rr"><img src="../Images/96d0dcc21aa912e2e6832016edf44773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51Bq_EDDxu4EfsWkFSm9FQ.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">Neuron with a <em class="rs">σ-activation</em></figcaption></figure><p id="0388" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Letting <em class="qk">W </em>vary with <em class="qk">f </em>allows omitting the deformations 𝜓. Letting ⟨<em class="qk">v</em>| vary with <em class="qk">f </em>allows replacing 𝜑 with a fixed <em class="qk">activation</em> function, independent on <em class="qk">f</em>.</p><h2 id="ece6" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">3.4. Approximating continuous functions: Cybenko et al</h2><p id="ff6d" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">Activation functions. </strong>The Neural Approximation theorem below states that any continuous function can be approximated by linear combinations of a fixed <em class="qk">activation function σ</em>. All we need from this function is that it restricts to a homeomorphism between two closed real intervals <em class="qk">not representable by a polynomial</em>. The construction can be set up to only use the part that establishes this continuous, monotonic bijection of intervals. That part can be conveniently renormalized to a <em class="qk">sigmoid</em>: a homeomorphism of the extended real line and the interval [0,1]. Early neural networks used the logistic sigmoid, which readily establishes that homeomorphism. The hyperbolic tangent and arcus tangent were also used, suitably renormalized. Nowadays the function max(0, <em class="qk">x</em>) is preferred. Its original designation as <em class="qk">“Rectified Linear Unit” </em>got mellowed down to <em class="qk">ReLU</em>, a nickname shared with small pets. The Neural Approximation construction fails if the activation function is representable by a polynomial. This obviously precludes all linear functions — <em class="qk">but</em> already a continuous combination of two linear functions works fine, as ReLU shows, combining the constant 0 below 0 and the identity above 0.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rn"><img src="../Images/fe33d2cf10d69af5ef92d875cddf60a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2t2kw1MU_tB1aCvBaJsaA.jpeg"/></div></div></figure><p id="f05f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Since both <em class="qk">f</em> and 𝒰(-)<strong class="mt ga">f</strong> are continuous, the approximation claim is equivalent to saying that for every <em class="qk">ε &gt; 0</em> there is <em class="qk">δ=δ(ε) &gt;0 </em>such that</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rt"><img src="../Images/2bf637223a5e666a3694d6b287a0d272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toLbOX2Hum1pP6X9ZpPu6g.jpeg"/></div></div></figure><p id="97f3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Neurons with activation functions are thus <em class="qk">universal approximators </em>of continuous functions, in the sense that for every continuous <em class="qk">f </em>on a cube there is a neuron <strong class="mt ga">f</strong>=<em class="qk">(W</em>,⟨<em class="qk">v</em>|) such that <em class="qk">f</em>|<em class="qk">x</em>⟩≈ 𝒰| <em class="qk">x </em>⟩<strong class="mt ga">f</strong>, with arbitrary precision.</p><p id="1afb" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The proofs of different versions of the Neural Approximation Theorem were published by Cybenko and by Harnik-Stinchcombe-White independently, both in 1989. In the meantime, the neural approximations have been widely used, and various other versions, views, and overviews have been provided. The overarching insight links the CHSW-approximation and the KA-decomposition in a computational framework that seems to have taken both beyond the original motivations.</p><p id="c83d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Continuous functions can be approximated because their variables can be separated.</strong> In computational terms, this means that continuous functions can be partially evaluated. That makes them learnable. Unfolding the CHSW-approximation in parallel with the KA-decomposition displays the common pattern:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ru"><img src="../Images/8363593ee2f3a1ceade56f979c144bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdttlY1l0PLV6FJkcpUz9A.jpeg"/></div></div></figure><p id="b90c" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The corresponding diagrams after the statements show the analogy yet again. But note the differences. The first difference is that <em class="qk">w</em> and <em class="qk">v </em>on the left depend on <em class="qk">f </em>, whereas on the right only 𝜑 depends on it. The second difference is that the number of separate variables that allow partial evaluation, for a fixed input <em class="qk">d</em>, is fixed at (2<em class="qk">d </em>+ 1) in the case of decomposition on the right, whereas in the case of approximation on the left, <em class="qk">n </em>= <em class="qk">n</em>(ε) depends on the approximation error ε. This is an important point.</p><p id="2ee0" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The dimension <em class="qk">n </em>of<em class="qk"> </em>the space on which a given function is approximable by a σ-neuron up to a required precision is the <strong class="mt ga"><em class="qk">width</em></strong><em class="qk"> of the neuron</em>. The Neural Approximation Theorem says that for any continuous function, there is a wide enough neuron that will approximate it up to any required precision. This is the essence of<strong class="mt ga"> wide learning</strong>. The idea of approximating continuous functions by a linear combination of copies of σ is similar to Lebesgue’s idea to approximate an integrable function by a linear combination of step functions. In both cases, closer approximations are achieved by larger numbers <em class="qk">n </em>of approximants.</p><p id="2196" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Wide neural networks. </strong>Everything stated for continuous real functions lifts without much ado to continuous <em class="qk">vector</em> functions. For finite dimensions, they are just tuples of continuous real functions. The approximations by σ-neurons lift to tuples of σ-neurons, a.k.a. the <em class="qk">single-layer neural networks</em>. The tupling step is the step from left to right in the following diagram.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rv"><img src="../Images/b97e862590919d34d3503b7ef4291b10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSBw03PHf_3a86JZLTIoZw.jpeg"/></div></div></figure><p id="08b1" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">A <em class="qk">q</em>-tuple of neurons (<em class="qk">W</em>, ⟨<em class="qk">v1</em>|), (<em class="qk">W</em>, ⟨<em class="qk">v2|),…,(W, ⟨vq</em>|), bundled together, gives a single-layer neural network a = (<em class="qk">W</em>,⟨<em class="qk">v1 |,⟨v2</em> |,…,⟨<em class="qk">vq </em>|), more succinctly written a = (<em class="qk">W</em>,<em class="qk">V</em>), where <em class="qk">V </em>is the matrix with the ⟨<em class="qk">vj</em>| vectors as rows, like before. The Neural Approximation Theorem implies that every continuous vector function can be approximated with arbitrary precision by a sufficiantly wide single-layer neural network. The term <em class="qk">wide neural network </em>usually refers to a single-layer network. The circuit view in the top row of the last figure is aligned with the more abstract view the middle row, with layers of variables enclosed in boxes. This will come handy when the networks become deep.</p><h2 id="8395" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">3.4. Deep learning</h2><p id="ff86" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">Scaling up. </strong>The trouble with wide learning is that there are simple functions where separating variables is hard, and the number <em class="qk">n</em> of variables that need to be separated increases exponentially with the dimension <em class="qk">d</em>. E.g., separating variables in function presenting a hemisphere in any dimension is hard. Although any continuous real function on a cube is approximable by a wide σ-neuron, and any continuous vector function by a single-layer network, the approximations are in the worst case intractable. The amounts of training data needed to extrapolate predictions also explode exponentially with the width<a class="af ou" href="#f923" rel="noopener ugc nofollow">⁸</a>.</p><p id="03bb" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Narrowing by deepening. </strong>The general idea of approximating a function <em class="qk">f </em>is to find an algorithm to transform the data</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rv"><img src="../Images/b7fafff38d31a32764e8d668ed255cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UNP0KG-5WzOF1DMga8B6XQ.jpeg"/></div></div></figure><p id="a1e7" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">and determine an approximator<em class="qk"> </em>with</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rv"><img src="../Images/e1a557e2c6252919270af0e31bbb4210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ztNG04CYvJfZjFu7oNIwg.jpeg"/></div></div></figure><p id="ab29" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">for a desired precision ε. The exponential growth of the width <em class="qk">n </em>of single-layer neural networks is thus tempered by descending through layers of deep neural networks, which look something like this:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rw"><img src="../Images/389c874787ee044fcb0f146c26f66016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdikawbWIlHXro7AJ3V_VQ.jpeg"/></div></div></figure><p id="5025" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">At each inner layer, composing the input transformation <em class="qk">W</em> with the output transformation <em class="qk">V</em> of the preceding layer gives a composite <em class="qk">H.</em></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rx"><img src="../Images/200df926bd76a73ab51b20a0df936c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qeJ20aPzjdYjbmeprXgPOw.jpeg"/></div></div></figure><p id="afdc" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As a composite of linear operators, <em class="qk">H</em> is linear itself, and can be trained directly, forgetting about the <em class="qk">W</em>s and the <em class="qk">V</em>s. Deep neural networks are thus programs in the form <strong class="mt ga">a</strong>=(<em class="qk">W</em>, <em class="qk">H</em>1, <em class="qk">H</em>2, . . . , <em class="qk">HL</em>, <em class="qk">V</em>).</p><p id="a8b5" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Neural networks are learnable programs.</strong> The <a class="af ou" href="#5da6" rel="noopener ugc nofollow">general learning process</a> can be viewed as the process of program development. To learn a function <em class="qk">F </em>means to converge to a program <strong class="mt ga">a </strong>whose executions 𝒰<em class="qk">(x)</em><strong class="mt ga">a</strong> approximate <em class="qk">F(x)</em>. Learners are programmers. It is true that the goal of programming is not just to approximate a function, but to precisely implement it. Ideally, a program <strong class="mt ga">a</strong> for a function <em class="qk">F</em> should satisfy 𝒰(<em class="qk">x</em>)<strong class="mt ga">a=</strong><em class="qk">F(x)</em>. In reality, a program implements a function only up to a correctness gauge ℒ(𝒰(<em class="qk">x</em>)<strong class="mt ga">a,</strong><em class="qk">F(x))</em>, realized through program testing or software assurance methodologies. Programming can be viewed as a special case of learning.</p><p id="7e4f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">We have already seen many of the main features of the syntax of neural networks as a programming language. A single neuron a = ⟨<em class="qk">w </em>| is an atomic program expression. A single-layer network a = (<em class="qk">W</em>,<em class="qk">V</em>) is a single-instruction program. A deep network a = (<em class="qk">W</em>, <em class="qk">H</em>1, <em class="qk">H</em>2, . . . , <em class="qk">HL</em>, <em class="qk">V</em>) is a general program. Its inner layers are the program instructions. For simplicity, the inner layers are often bundled under a common name, say <strong class="mt ga">h</strong> = (<em class="qk">H</em>1,<em class="qk">H</em>2,…,<em class="qk">HL</em>). A general neural program is in the form <strong class="mt ga">a</strong> = (<em class="qk">W</em>,<strong class="mt ga">h</strong>,<em class="qk">V</em>).</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px ry"><img src="../Images/fc59c0af992bb2b1ae0724cce727bb24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qmqAq98fASdRCwBr3Mx4A.jpeg"/></div></div></figure><h1 id="0e5b" class="pe nw fq bf nx pf ql gv ob ph qm gy of pj qn pl pm pn qo pp pq pr qp pt pu pv bk">4. Learning channels and paying attention</h1><p id="4f76" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">The trouble with applying function learning to language</strong> is that language is context-sensitive: the word “up”, for instance, means one thing in “shut up” and another thing in “cheer up”. We talked about this in the <a class="af ou" rel="noopener" target="_blank" href="/syntax-the-language-form-612257c4aa5f#8904"><em class="qk">Beyond sintax</em></a><em class="qk"> </em>section of the <a class="af ou" rel="noopener" target="_blank" href="/syntax-the-language-form-612257c4aa5f"><em class="qk">Syntax </em>part</a>. A function is required to assign to each input the same unique output in all contexts. Meaning is not a function but a communication channel, assigning to each context a probability distribution over the concepts <em class="qk">y</em>:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rz"><img src="../Images/80c8793cecf6d6778cf83c2d31d8b07c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xC9lDZeO6Jds7sudvc47wg.jpeg"/></div></div></figure><p id="c1da" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In the <a class="af ou" rel="noopener" target="_blank" href="/syntax-the-language-form-612257c4aa5f#4389"><em class="qk">Semantics </em>part</a>, we saw how concepts are modeled as vectors, usually linear combinations of words. Meaning is thus a random variable <em class="qk">Y</em>, sampled over the concept vectors <em class="qk">y</em>. There is an overview of the channel formalism in the <a class="af ou" rel="noopener" target="_blank" href="/syntax-the-language-form-612257c4aa5f#912e"><em class="qk">Dynamic semantics</em></a><em class="qk"> </em>section of the <em class="qk">Semantics </em>part. When there is no channel feedback, the context is the channel source</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rz"><img src="../Images/80787715e7edc3201d2471b68b420146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ivt2rghhSgcNANou3_mBXw.jpeg"/></div></div></figure><p id="9848" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">and the channel outputs are sampled according to the probabilities</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rz"><img src="../Images/0cfd8750c1bda6afbcc2c9598e24e08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8fKmuHhRND9N6FVtkedTA.jpeg"/></div></div></figure><p id="0ab9" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">You can think of the source <em class="qk">X</em> as a text and of the channel <em class="qk">F </em>as the process of translating the text to a text <em class="qk">Y </em>in another language:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sa"><img src="../Images/a6a7677c1a3737a1299f59276384b2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J3b9S6eDfFb-BZUafDVTLw.jpeg"/></div></div></figure><p id="79c2" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Similar interpretations subsume meaning, syntactic typing, classification, and generation under the channel model. The common denominator is the context dependency, be it syntactic or semantic, deterministic or stochastic. Semantic references can be remote. The meaning of a sentence in a novel may depend on a context from 800 pages earlier. The meaning that you assign to something that an old friend says may be based on a model of their personality from years ago. To make it more complicated, remote references and long established channel models may change from context to context, whenever new information becomes available.</p><h2 id="4f8f" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">4.1 Channeling through concepts</h2><p id="c184" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">In different languages, semantical references are mapped to syntactic references in different ways. Mapping a Mandarin phrase to a French phrase requires deviating from the <a class="af ou" rel="noopener" target="_blank" href="/syntax-the-language-form-612257c4aa5f#d37c">syntactic dependency mechanisms </a>of the two languages. Good translators first understand the phrase in one language and then express what they understood in the other language. It is a two-stage process:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sb"><img src="../Images/84951a402995cbfd179617dddad2ed5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bi-0TDmCc92jJgg8xmxtaA.jpeg"/></div></div></figure><p id="25d3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="qk">E </em>is a <em class="qk">concept encoding </em>map, whereas <em class="qk">D </em>is <em class="qk">concept decoding</em>. Similar pattern came up in the <a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aeca" rel="noopener"><em class="qk">Semantics </em>part</a>, as instances of concept mining through <em class="qk">Singular Value Decomposition (SVD).</em></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sc"><img src="../Images/a17b04e6bf6b57527b95680b6cc989b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVd2644xxC-NRYyORYHXOg.jpeg"/></div></div></figure><p id="05ad" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The concepts that are latent in a given data matrix M are mined as its singular values <em class="qk">λi</em>. Now compare the diagrams of <a class="af ou" href="#ef75" rel="noopener ugc nofollow">𝜎-neurons</a> and <a class="af ou" href="#e52e" rel="noopener ugc nofollow">single-layer networks</a> with the corresponding diagram of the SVD:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sd"><img src="../Images/c578eb08bf8e5d7e07cc1eb53e5b57d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ud5NFCfcZRAYkOO7rTYAhw.jpeg"/></div></div></figure><p id="fe6f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">A neural network approximates a continuous function by separating its variables and approximating the impact of each of them by a separate copy of the activation function 𝜎. The SVD algorithm decomposes a data matrix through a canonical basis eigenspaces, corresponding to the singular values of the matrix, viewed as the dominant concepts, spanning the concept space. The eigenspaces in the SVD are mutually orthogonal. The action of the data matrix boils down to multiplying each of them separately by the corresponding singular value. Both the neural network approximation and the SVD mine the latent concepts as the minimally correlated subspaces, preferrably orthogonal at each other. The diagrams display the same three-step pattern:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qx"><img src="../Images/b6338e436c878672cb2be82a18fc8ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZRpar2XJ5uVIThCcaEI_YQ.jpeg"/></div></div></figure><ul class=""><li id="a605" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><strong class="mt ga">encoding</strong> of inputs in terms of concepts,</li><li id="de44" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><strong class="mt ga">separate processing</strong> of each concept,</li><li id="c5f8" class="mr ms fq mt b gt oy mv mw gw oz my mz na pa nc nd ne pb ng nh ni pc nk nl nm pd ow ox bk"><strong class="mt ga">decoding</strong> of the concepts into the output terms.</li></ul><p id="fcb8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The three steps serve different purposes in different ways:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px se"><img src="../Images/ee3005c689e6e1335f5a54171b99090a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IvhBiN-vJJMRVTUA6W2QxQ.jpeg"/></div></div></figure><p id="7399" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">But difference (1) causes differences (2–3). When the function <em class="qk">F </em>happens to be linear and difference (1) disappears, the neural network converges to the SVD and differences (2–3) also disappear. Neural networks also mine latent concepts, like the SVD. They just learn them from arbitrary continuous functions.</p><h2 id="b1bf" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">4.2 Static channel learning</h2><p id="4a88" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">A network of neural networks is static if it processes its inputs by applying the same neural network <strong class="mt ga">h </strong>on all channel inputs.</p><p id="95ba" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga"><em class="qk">n</em>-grams of concepts. </strong>As a warmup, suppose that we want to make a static network of networks slightly context-sensitive by taking into account at the <em class="qk">j</em>-th step not only <em class="qk">Xj </em>but also <em class="qk">Xj</em>−1, for all <em class="qk">j </em>≥ 2.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qx"><img src="../Images/0d46519e956b5bf86ffadfafa3d67592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMrvhy5MTCz6f3wR4450tw.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">2-grams of concepts</figcaption></figure><p id="4624" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The weights <em class="qk">T </em>are updated in the same way as <em class="qk">W</em>, by minimizing the losses and propagating the updates back from layer to layer. They just add one training step per layer. This is not a big deal structurally, but it is a significant slowdown computationally. If the inner layers are viewed as latent concept spaces, then this architecture can be thought of as a lifting of the idea of 2-grams (capturing the dependencies of contests of length 2) from words to concepts. Generalizing to <em class="qk">n</em>-grams for larger <em class="qk">n</em>s causes further slowdown.</p><p id="4df3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Recurrent Neural Networks (RNNs). </strong>The RNNs also apply the same neural network on all input tokens and also pass to the <em class="qk">j</em>-th module not only <em class="qk">Xj </em>but also the information from <em class="qk">Xj</em>−1 — <em class="qk">b</em>ut they pass it <em class="qk">after </em>the previous network module was applied to <em class="qk">Xj</em>−1, not before.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qx"><img src="../Images/44981b49fbda774477e8199703430f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kM8GKPAt58Q6Tk7inUG7zQ.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">RNN idea</figcaption></figure><p id="84c3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Note that information from <em class="qk">Xj</em>−1 is this time forwarded by <em class="qk">S</em> not only to the <em class="qk">j</em>-th module, but with the output of the <em class="qk">j</em>-th module by the next copy of <em class="qk">S </em>also to the <em class="qk">(j+1)</em>-st module, and so on. The information propagation is thus in principle unbounded, and not truncated like in the <em class="qk">n</em>-gram model. The matrices <em class="qk">S </em>that propagate important information further are promoted in training. However, the weights assigned to all input entries are all packed in <em class="qk">S</em>. Propagating longer contexts requires exponentially wider network modules. So we are back to square one, the problem of width.</p><p id="07eb" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Long Short-Term Memory (LSTM). </strong>The LSTM networks address the problem of the cost of forwarding the context information between the iterations of the same neural network module by forwarding the information from the <em class="qk">(j−1)</em>-th input token to the <em class="qk">j</em>-th module both before it was processed by the <em class="qk">(j−1)</em>-th module, and after. The former makes passing the information from each input more efficient, the latter makes the propagation easier.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px qx"><img src="../Images/67ce3d32737e914c5b323b1b7c70ff64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbnlXCjxFhoZOTrmwz5Y_A.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">LSTM idea</figcaption></figure><p id="f617" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The idea of passing around the information at different stages of processing is simple enough, but optimizing the benefits is a conundrum, as already the “long short” name may be suggesting. The implementation details are many. Different activation functions are applied on different mixtures of the same inputs and remixed in different ways for the outputs. Expressing the concepts learned from the same data in multiple bases requires multiple matrices and provides more opportunities for training. Hence for improvements. But further steps require further ideas.</p><h2 id="1d50" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">4.3 Dynamic channel learning</h2><p id="9503" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">Just like the function learner, the channel learner seeks to learn how the inputs are transformed into the outputs. The difference is that the channel transformations are context-dependent. Not only are the outputs always dependent on the input contexts, but there may be feedforward dependencies of outputs on outputs, and feedback dependencies of inputs on outputs, as discussed in the <a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#d3b1" rel="noopener">channel section</a> of the <a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41" rel="noopener"><em class="qk">Semantics </em>part</a>. A dynamic network of neural networks learns a channel by adaptively updating the “key” subnetworks <strong class="mt ga">k</strong><em class="qk">, </em>processing the channel inputs, and the “value” subnetworks <strong class="mt ga">v,</strong> delivering the corresponding channel outputs.</p><p id="d0e9" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Encoder-Decoder Procedures. </strong>An important programming concept is the idea of a <em class="qk">procedure</em>. While the earliest programs were just sequences of program instructions, procedures enabled programmers to invoke within programs not just instructions but also entire programs, encapsulated in procedures as generalized instructions. Since procedures can be used inside most program control structures, this enabled <strong class="mt ga"><em class="qk">programming over programs</em></strong>, and gave rise to software engineering. The later programming paradigms, modular, object-oriented, component and connector-oriented, extend this basic idea.</p><p id="c8d5" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The encoder-decoder architecture is a <strong class="mt ga"><em class="qk">network of neural networks</em></strong>. If neural networks are thought of as programs, it is a program over programs. The encoder-decoder architecture <strong class="mt ga">A</strong> = (<strong class="mt ga">e</strong>, <strong class="mt ga">d</strong>) lifts the structure <strong class="mt ga">a</strong> = (<em class="qk">W</em>, <em class="qk">V</em>) of a wide neural network to a network of networks.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sf"><img src="../Images/f8c7a4f420ed913c2fe3f243a8df4626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*COltHbYB00sDvK1nceX6Jw.jpeg"/></div></div><figcaption class="qr qs qt pw px qu qv bf b bg z dx">Encoder-decoder</figcaption></figure><p id="d59d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The input remixing matrix <em class="qk">W </em>is replaced by an encoder network <strong class="mt ga">e</strong>, the output remixing matrix <em class="qk">V </em>by a decoder network <strong class="mt ga">d</strong>. Both the wide network <strong class="mt ga">a </strong>and its lifting <strong class="mt ga">A </strong>follow the architectural pattern of <a class="af ou" href="#4f8f" rel="noopener ugc nofollow">concept mining</a>. Just like procedural programming allowed lifting control structures from programs to software systems, the encoder-decoder architecture allows lifting the concept mining structures from neural networks to neural architectures. The problem with the basic form of the encoder-decoder architecture as a concept mining framework is that a concept space induced by a static dataset is static whereas channels are dynamic. To genuinely learn concepts from a channel, a neural network architecture needs dynamic components.</p><p id="d86c" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Idea of attention. </strong>A natural step towards enabling neural networks to predict the outputs of a channel</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sg"><img src="../Images/9d06990fcbde5f81cc2dfe1f5e034331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rdy9ExHVvCWqq-jIK6JuPw.jpeg"/></div></div></figure><p id="260a" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">is to generalize the basic 𝜎-neuron from the <a class="af ou" href="#5646" rel="noopener ugc nofollow">CHSW construction</a></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sg"><img src="../Images/9837ae9b75a6e5f0b870d1cf3ae0a1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSiRJqMVZoj5RDyMsZwP5g.jpeg"/></div></div></figure><p id="07ae" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In this format, a component of an Encoder-Decoder procedure output would be something like</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sg"><img src="../Images/b2cd3b620caf1f205ee938fc9cd86c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xE9fAGMQS39KBYsdXehNaw.jpeg"/></div></div></figure><p id="bb49" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">where</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sg"><img src="../Images/0214787a853c0c7be7df4df427a10cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUpzVA6wX5ip_DsS-z7AIQ.jpeg"/></div></div></figure><p id="10c6" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">are basic encoder and decoder matrices from a <a class="af ou" href="#4f8f" rel="noopener ugc nofollow">channel concept mining framework</a>. But now we need to take into account the concepts learned at inner layers of a deep network. The impacts on the (<em class="qk">n+1)-</em>st output value of the input vectors |<em class="qk">xj</em>⟩ are therefore weighed by their projections ⟨<em class="qk">ej </em>| <em class="qk">xj</em>⟩ on the input concepts ⟨<em class="qk">ej</em>| <em class="qk">and </em>the projections ⟨<em class="qk">yn </em>| <em class="qk">dj</em>⟩ of the row vector ⟨<em class="qk">yn</em>| of the previous outputs on the output concepts |<em class="qk">dj </em>⟩. The relationship between the corresponding concepts ⟨<em class="qk">ej</em>| and |<em class="qk">dj </em>⟩ are trained to align the channel inputs and the channel outputs. This is the basic idea of the <em class="qk">attention architecture</em>. It can be drawn as a common generalization of the <a class="af ou" href="#ef75" rel="noopener ugc nofollow">𝜎-neuron</a> and the <a class="af ou" href="#46fc" rel="noopener ugc nofollow">SVD-schema</a>, with dynamic singular values. (This is an instructive <strong class="mt ga">exercise</strong>.) For string outputs, the obvious extension is</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sh"><img src="../Images/a9e071205f4d39ec6785272b9402232b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qWZDn6YApIyCF65wEr9Mg.jpeg"/></div></div></figure><p id="6b88" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">But it is not obvious how to train the matrix <em class="qk">V </em>whose rows are the output mixtures ⟨<em class="qk">vi </em>|. The issue is solved by approaching the task from a slightly different direction.</p><p id="af42" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Dynamic concept decomposition.</strong> A set of vectors <em class="qk">|1⟩,|2⟩,…,|n⟩ </em>is said to span the vector space if every vector |<em class="qk">y</em>⟩ can be expressed in the form</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px si"><img src="../Images/bec39bcf8e4bfd5a0170e95f88c5b9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIAnCWozF1O9eAMFOvAYFw.jpeg"/></div></div></figure><p id="ce8f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The decomposition is unique if and only if the vectors <em class="qk">|1⟩,…,|n⟩</em> are orthogonal, in the sense that ⟨<em class="qk">i</em>|<em class="qk">j</em>⟩ = 0 as soon as <em class="qk">i≠j</em>. If the spanning vectors are not orthogonal, but there is an orthogonal set <em class="qk">|c1⟩,|c2⟩,…,|cn⟩</em>, then there is a unique decomposition</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px si"><img src="../Images/1745a488eda1099938cb5ae274429a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOWoxhaXz9hgy06bkw0qmg.jpeg"/></div></div></figure><p id="734e" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As discussed <a class="af ou" href="#4f8f" rel="noopener ugc nofollow">earlier</a>, concept analysis is the quest for concept bases with minimal interferences between the basic concepts. The basic concept vectors do not interfere at all when they are mutually orthogonal. If a channel<em class="qk"> </em>is implemented by a neural network, the above concept decomposition becomes</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px si"><img src="../Images/01935a41b836071a7c373f248f77b249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*METsGeUSW2Lbt_GkBuXYiA.jpeg"/></div></div></figure><p id="1be9" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The first difference is that the activation function σ allows approximating nonlinearities. The second is that the components are not projected on the original basis vectors ⟨<em class="qk">i</em>| anymore but on the output mixtures ⟨<em class="qk">vi</em>|. Lastly and most importantly, the concept decomposition was unique because the concept basis <em class="qk">|c1⟩,…,|cn⟩ </em>was orthogonal, whereas here the output is projected on the inputs <em class="qk">|x1⟩,…,|xn⟩</em>, which are not orthogonal. But if an orthogonal concept basis <em class="qk">|c1⟩,…,|cn⟩</em> exists, we can play the same trick again, and get a unique concept decomposition</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sc"><img src="../Images/50fd452382b15f00fcf7b2c5a0b7077f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13zMf5Tu96wHveUDtZm1PA.jpeg"/></div></div></figure><p id="c390" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">What does this abstract decomposition mean for a concrete channel? The projections on the concept basis vectors measure their weights in the inputs |<em class="qk">xj</em>⟩ and in the outputs |<em class="qk">yn</em>⟩. The sum of the products of the projections measures the impact of the input |<em class="qk">xj</em>⟩ on the output |<em class="qk">yn</em>⟩. This measurement, activated by σ, then impacts the <em class="qk">i</em>-th component of the channel output | <em class="qk">yn </em>⟩ according to the projection ⟨<em class="qk">vi </em>| <em class="qk">xj</em>⟩.</p><p id="86af" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The only problem is that the projection of |<em class="qk">yn</em>⟩ on the right is unknown since |<em class="qk">yn</em>⟩ is what we are trying to predict. What other value can be used to approximate the impact of a concept in the output |<em class="qk">yn</em>⟩? — Two answers have been proposed.</p><ul class=""><li id="6bf7" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><strong class="mt ga">Translator’s attention:</strong> If the channel <em class="qk">F </em>: <em class="qk">[X </em>⊢ <em class="qk">Y]</em> is a <a class="af ou" href="#d171" rel="noopener ugc nofollow">translation</a>, say from Mandarin to French through concepts <em class="qk">|c</em>⟩, then the summands ⟨<em class="qk">xj</em>|<em class="qk">c</em>⟩⟨<em class="qk">c</em>|<em class="qk">yn</em>⟩ can be thought of as distributing <em class="qk">translator’s attention </em>over the concepts <em class="qk">|c</em>⟩, latent in the Mandarin input tokens |<em class="qk">xj</em>⟩ <em class="qk">after </em>the French output token |<em class="qk">yn</em>⟩ is produced. That is the attention that effectively impacts the <em class="qk">(n+1)</em>-st output, and the above decomposition should be updated to</li></ul><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sj"><img src="../Images/72516dceb2c5967cdb28f109a60ebe17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gv0XjyidPAvWdukVUDg59Q.jpeg"/></div></div></figure><ul class=""><li id="ac4d" class="mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm pd ow ox bk"><strong class="mt ga">Speaker’s self-attention:</strong> If the channel <em class="qk">F</em>: <em class="qk">[X </em>⊢ <em class="qk">Y]</em> is not a translation to another language but a continuation <em class="qk">Y </em>in the same language, then it is not <a class="af ou" href="https://medium.com/@dusko_p/semantics-the-meaning-of-language-99b009ccef41#aba3" rel="noopener">feedback-free</a>, since<em class="qk"> Xn</em>+1 is not independent of <em class="qk">Yn</em>, but identical to it. To capture the feedback, the concept base splits into an encoding base and a decoding base as <a class="af ou" href="#5b17" rel="noopener ugc nofollow">above</a>, expressing the inputs as mixtures of concepts, and the concepts as mixtures of outputs. But since each output is now recast as the next input, the <em class="qk">encoder-decoder</em> view morphs into the <em class="qk">key-query-value </em>view of language production as an ongoing stochastic process of database retrieval, with the projections of the inputs to the queries modeling a rudimentary “attention span” over the preceding context:</li></ul><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sj"><img src="../Images/a5872daaf3b6d10313b4ffce8fb043fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqxkDQWjZwU-assycZDx3w.jpeg"/></div></div></figure><p id="20bc" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The self-attention modules in the form <strong class="mt ga">a</strong> = (<em class="qk">K</em>, <em class="qk">Q</em>, <em class="qk">V</em>), with</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sj"><img src="../Images/c8d1926aa0cee155b4691ff40706ba9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjABGEi5teVQKcZH6KqQrw.jpeg"/></div></div></figure><p id="9a96" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">are the central components of the <em class="qk">transformer </em>architecture, which is the “T” of the GPTs. They can be viewed as a step towards capturing the general language-production channels discussed in the <a class="af ou" href="https://medium.com/p/99b009ccef41#d3b1" rel="noopener">Semantics part</a>, with the query vectors capturing the basic feedback flows and the value vectors capturing the feedforward flows:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rh"><img src="../Images/cc442b45cc44e834929a4b9d97bfd248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nrsPAzBh0eC0zglK3v2AWw.jpeg"/></div></div></figure><p id="da6d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Reconciling the intuitions for attention as a mental process with the database terminology for key-query-value may feel awkward at first, yet the time may be ripe to expand our intuitions and recognize the same natural processes unfurling in our heads and in computers.</p><h1 id="50f8" class="pe nw fq bf nx pf ql gv ob ph qm gy of pj qn pl pm pn qo pp pq pr qp pt pu pv bk">5 Beyond hallucinations</h1><h2 id="16d1" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">5.1 Parametric learning framework</h2><p id="e981" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">Staring back at the <a class="af ou" href="#e77a" rel="noopener ugc nofollow">general learning framework</a>, after a while you realize that the transformer architecture uncovered a feature that was not visible in the <a class="af ou" href="#5da6" rel="noopener ugc nofollow">learning diagram</a> there. It uncovered that <em class="qk">models and their programs can be parametrized.</em></p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rh"><img src="../Images/4dbff571ec4acb9e2d41c0dffb262fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EynY7GHC74zGEkBNv3V4Q.jpeg"/></div></div></figure><p id="5c4c" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">A learner can train a model <strong class="mt ga">a</strong>(<em class="qk">X</em>) that captures the dependencies of a channel <em class="qk">F </em>on <em class="qk">n-</em>input contexts, for any <em class="qk">n</em>, and leave the <em class="qk">(n+1)</em>-st input <em class="qk">X </em>as a program parameter. When the <em class="qk">(n+1)</em>-st input is sampled to<em class="qk"> X=</em>|<em class="qk">x</em>⟩, the model is instantiated to <strong class="mt ga">a</strong>|<em class="qk">x</em>⟩<em class="qk">. </em>Interpreting this instance produces a prediction of the next channel output:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sk"><img src="../Images/6864d58788dac8258619861567c89542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T840SGdqvLSeZNDhDtNI1A.jpeg"/></div></div></figure><p id="c21b" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Transformers are parametric programs in the form a(<em class="qk">X</em>) = <em class="qk">(K</em>, <em class="qk">Q</em>(<em class="qk">X</em>),<em class="qk">V)</em>. Parametricity is an important feature of computation, arising from the partial interpretability of programs, which propagates to machine learning as the partial learnability of models. While the partial evaluation of programs grew from Gödel’s Substitution Lemma and Kleene’s Smn Theorem into a practical programming methodology, the partial learnability of models seems to have evolved in practice and, as far as I can tell, awaits a theory. In the rest of this note, I sketch some preliminary ideas<a class="af ou" href="http://ed33" rel="noopener ugc nofollow" target="_blank">⁹</a>.</p><h2 id="47ec" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">5.2 Self-learning</h2><p id="0a9b" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">The parametric learning framework in the <a class="af ou" href="#e981" rel="noopener ugc nofollow">above diagram</a> captures the learning scenarios where learners interact and learn to predict each other’s behaviors. This includes not only conversations between different learning machines, or betewen different instances of the same machine, but also a self-learning process where a learning machine learns to predict its own behaviors modulo a parameter. A framework for such self-learning can be obtained by instantiating the supervisor <em class="qk">F </em>in the <a class="af ou" href="#16d1" rel="noopener ugc nofollow">parametric learning framework</a> to</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sk"><img src="../Images/09e09095ad8f65befca4d21eb0961a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhTtXUzu_kk-L19Q8aBg9w.jpeg"/></div></div></figure><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sk"><img src="../Images/35dbb3bbaebdce7bcdeff653f4328efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v1YcAl6flezvKsYjNo68BA.jpeg"/></div></div></figure><p id="c12d" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The obtained model <strong class="mt ga">s </strong>of <em class="qk">self</em> allows the learner to predict its own future behaviors, as parametrized by future inputs:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sl"><img src="../Images/82ac4b6d98c393efbb04919680f7d220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5rQR4d9112vkGWStMtC4w.jpeg"/></div></div></figure><p id="a927" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This framework also captures the cases of unintended self-learning, where learning machines get trained on corpora saturated by their own outputs, due to overproduction and overuse, in a process familiar from other industries that tap into natural resources.</p><p id="bde4" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Predicting effects of predictions. </strong>Learning the consequences of what we do sometimes impacts what we do. <em class="qk">To make e</em>ff<em class="qk">ective predictions, the learner must take into account the e</em>ff<em class="qk">ects of their predictions. </em>Parametric learning provides a framework for that. The learner’s capability to predict the effects of their predictions allows them to steer the predictions in the desired direction. This is how intentionally self-fulfilling prophecies, self-validating, or self-invalidating theories come about. A particularly interesting and worrying case is presented by <em class="qk">adaptive</em> theories, designed to pass all tests by reinterpreting their predictions. Such logical phenomena are ubiquitous in history, culture, and religions<a class="af ou" href="#4f51" rel="noopener ugc nofollow">¹⁰</a>. The learning machines surely evolve such processes faster and more methodically. The method to produce them is based on the learner’s model <strong class="mt ga">s</strong><em class="qk"> </em>of self.</p><h2 id="72b2" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">5.3 Self-confirming beliefs</h2><p id="891e" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk"><strong class="mt ga">Learning is believing.</strong> A model <strong class="mt ga">a</strong> of a process <em class="qk">F </em>expresses a <em class="qk">belief </em>held by the learner 𝒰 about <em class="qk">F</em>. The learner updates the belief as they learn more. Learning is belief <em class="qk">updating</em>.</p><p id="a0be" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Beliefs impact their own truth values.</strong> Our beliefs have impacts on what we do, and what we do changes some aspects of reality: we change the world by moving things around. Since the reality determines whether our beliefs are true or false, and our beliefs, through our actions, change some aspects of reality, it follows that our beliefs may change their own truth values. Accusing an honest person of being a criminal may drive them into crime. Entrusting a poor but honest person with a lot of money may transform them into a rich and dishonest person.</p><p id="b9c8" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">Making self-confirming predictions.</strong> If <em class="qk">B</em>ob uses a learning machine 𝒰riel to decide what to do, then 𝒰riel can learn a model <strong class="mt ga">b</strong> that will always move <em class="qk">B</em>ob to behave as predicted by <strong class="mt ga">b</strong>. If <em class="qk">B</em>ob shares 𝒰riel’s beliefs, then those beliefs will be confirmed by <em class="qk">B</em>ob’s actions.</p><p id="10e1" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To spell out the learning process that 𝒰riel can use to construct the self-confirming belief <strong class="mt ga">b</strong>, suppose that <em class="qk">B</em>ob’s behavior is expressed through a channel <em class="qk">B. </em>The assumption that <em class="qk">B</em>ob uses 𝒰riel to decide what to do can be formalized by taking the outputs of the channel to be in the form</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sl"><img src="../Images/bd48e3aa95901ffed3e7df82c3e01bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8OlzMNAeaOgYBDeHt4eRQ.jpeg"/></div></div></figure><p id="bdea" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">meaning that <em class="qk">B</em>ob consults 𝒰riel and believes that the model <strong class="mt ga">a</strong>(X) explains the input <em class="qk">X</em>. The claim is that 𝒰riel can then find a model <strong class="mt ga">b</strong>(<em class="qk">X</em>) that will cause <em class="qk">B</em>ob to act as <strong class="mt ga">b</strong>(<em class="qk">X</em>) predicts:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sl"><img src="../Images/e7cd0a0ce2813d07f9ad96161ecbd171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rK1NcEp8ApquzQTgycpwxw.jpeg"/></div></div></figure><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sk"><img src="../Images/f35db90a77e7d7de1097cab618383916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f282lbZCYwsrPFLANUa3LA.jpeg"/></div></div></figure><p id="5cf3" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">To learn <strong class="mt ga">b</strong>(X), 𝒰riel first learns a model 𝛽 of <em class="qk">B</em> instantiated to 𝒰riel’s model of self:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sl"><img src="../Images/ad3792fc40ae36902ef4246d9e62a3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwzRwZCC-LY6KoxFtt8dxA.jpeg"/></div></div></figure><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sk"><img src="../Images/73689071181dce69471feb1a076f57f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdP5TkMYJy6RNVgMq40fjw.jpeg"/></div></div></figure><p id="8124" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Processing the 𝛽-explanation of the (2n+1)-th input as the (2n+2)-th input, the definition of 𝛽 yields</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px rp"><img src="../Images/94838ff55f41829384def431313e8864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1uVgEGECrrJAMEYJ1DG9g.jpeg"/></div></div></figure><p id="b26f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The claimed self-confirming model is now defined:</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sm"><img src="../Images/ae43a9b27cac920658fadc15cd938652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5h-joH6Dz5JeIo2Tc1VjZA.jpeg"/></div></div></figure><p id="e4da" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It satisfies the <a class="af ou" href="#3df3" rel="noopener ugc nofollow">claim</a> because</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sn"><img src="../Images/942cf993cef33dbe6ac1ce71918970d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQGLMV-AdJerJQxd8IxduQ.jpeg"/></div></div></figure><p id="727f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mt ga">From learnable programs to unfalsifiable theories and self-fulfilling prophecies.</strong> The insight that learning is like programming opens up a wide range of program fixpoint constructions. Applied on learning, such constructions produce models that steer their own truth, whether into self-confirmations or paradoxes, along the lines of logical completeness or incompleteness proofs. The above construction is one of the simplest examples from that range<a class="af ou" href="#ed33" rel="noopener ugc nofollow">⁹</a>. They prepare models and theories that absorb all future evidence, explain away counterexamples, and confirm predictions.</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px so"><img src="../Images/019ecfb85ebe1c40e697024c75c57cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSRffrv6rNsAFjM0zc9xlg.jpeg"/></div></div></figure></div></div></div><div class="ab cb nn no np nq" role="separator"><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt nu"/><span class="nr by bm ns nt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="aa48" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">Attributions</h2><p id="c9e5" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">The color tableaus were authored by DALL-E, prompted by Dusk-o. The hand-drawn diagrams and icons were authored by Dusk-o, prompted in some cases by DALL-E.</p><p id="cd29" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The results presented in this note originate from many publications which would normally be listed in a bibliography. But while the bibliographic formats were standardized in the pre-web era, the subject of this text is post-web. In the meantime, during the web era, we got used to finding all references on the web. The students who used this lecture note were asked to find the relevant references using the keywords in the text. They needed additional information in a handful of places. I added more keywords and notes in these places. If proper references turn out to be needed, or if the reference system gets updated for actual use, a bibliography will be added.</p><h2 id="f974" class="nv nw fq bf nx ny nz oa ob oc od oe of na og oh oi ne oj ok ol ni om on oo fw bk">Notes</h2><p id="858d" class="pw-post-body-paragraph mr ms fq mt b gt op mv mw gw oq my mz na or nc nd ne os ng nh ni ot nk nl nm fj bk">¹Alan Turing explained that machine intelligence could not be achieved through intelligent designs, because intelligence itself could not be completely specified, as it is its nature to always seek and find new paths. But Turing was also the first to realize that the process of computation was not bound by designs and specifications either, but could evolve and innovate. He anticipated that machine intelligence would evolve with computation. However, three years after Turing’s death, the concept of <em class="qk">machine intelligence</em>, which he thought and wrote about for the last 8 years of his life, got renamed to <em class="qk">artificial intelligence, </em>his writings sank into oblivion, and the logical systems designed to capture intelligence proliferated.</p><p id="a74e" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">²Skinner’s explorations of our intellectual kinship with pigeons have interesting interpretations in the context of arguments that the concept of causality as such is in essence unfounded. From different directions, such arguments have been developed by Hume, Russell, Bohr, and many other scientists and philosophers.</p><p id="0dbf" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">³Our <a class="af ou" href="https://arxiv.org/abs/2303.14338" rel="noopener ugc nofollow" target="_blank">paper on bots’ religions</a> also points in this direction.</p><p id="04e9" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁴Frank Rosenblatt. <em class="qk">Principles of neurodynamics; perceptrons and the theory of brain mechanisms</em>, volume 55. Spartan Books, Washington, D.C., 1962.</p><p id="7754" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁵Recall that ⟨<em class="qk">w</em>| is a convenient notation (attributed to Paul Dirac) for the row vector <em class="qk">(w</em>1 <em class="qk">w</em>2 ···<em class="qk">wd )</em>, whereas |<em class="qk">w</em>⟩ is the corresponding column vector. Viewed as a linear operator, the row vector ⟨<em class="qk">w </em>| denotes the projection on the column vector |<em class="qk">w</em>⟩.</p><p id="d246" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁶Since this note was written, a <a class="af ou" href="https://arxiv.org/abs/2404.19756" rel="noopener ugc nofollow" target="_blank">paper</a> proposing a new family of neural networks, called the <em class="qk">Kolmogorov-Arnold Networks (KAN) </em>appeared on arxiv. The idea is very natural, as even our derivation of <a class="af ou" href="#5646" rel="noopener ugc nofollow">neural approximation</a> from <a class="af ou" href="#2f11" rel="noopener ugc nofollow">continuous decomposition</a> confirms. Remarkably, though, the proposers of the KAN approach make no use of the substantial mathematical and computational simplifications and improvements of Kolmogorov’s 1957 construction, although they cite some papers with fairly complete reference lists. Since they seem to be actively updating the posted reports about their work, the missed opportunities for improvement will presumably be taken in the future versions.</p><p id="a35f" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁷In the old-style matrix notation the Lorentz-Sprecher embedding is</p><figure class="pz qa qb qc qd qe pw px paragraph-image"><div role="button" tabindex="0" class="qf qg ed qh bh qi"><div class="pw px sp"><img src="../Images/2b820513ddc9719dfad217e67d74d761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgSJ6q2gvK3LAA7hv4caew.jpeg"/></div></div></figure><p id="f923" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁸In their seminal, critical book <em class="qk">“Perceptrons”</em>, Minsky and Papert proved that the coefficients of a perceptron representing a boolean function are always invariant under the actions of a group under which the function is invariant itself. Since a perceptron therefore cannot tell apart the functions that are equivariant under the group actions, this was viewed as a no-go theorem. While the Minsky-Papert construction lifts from perceptrons and boolean functions to wide neural networks and continuous functions by standard methods, the resulting group invariances are nowadays viewed as proofs that the glass of neural approximations is half-full, not that it is half-empty.</p><p id="ed33" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">⁹The constructions and discussions presented in this section are based on the paper <a class="af ou" href="https://arxiv.org/abs/2303.14338" rel="noopener ugc nofollow" target="_blank">“<em class="qk">From Gödel’s Incompleteness Theorem to the completeness of bot beliefs</em>”</a>.</p><p id="4f51" class="pw-post-body-paragraph mr ms fq mt b gt mu mv mw gw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">¹⁰Shakespeare’s tragedy of Macbeth is built on a self-fulfilling prophecy. At the beginning, the witches predict that Macbeth will become King. To fulfill the inevitable, Macbeth kills the King. Even a completely rational Macbeth is forced to fulfill the prophecy, or risk that the King will hear of it and kill him to prevent it from being fulfilled. An example of a self-fulfilling prophecy from current life arises from the task of launching a social networking service. This service is only valuable to its users if their friends are also using it. To get its first users, the social network must convince them that it already has many users, enough to include their friends. Initially, this must be a lie. But if many people believe this lie, they will join the network, the network will get many users, and the lie will stop being a lie. Examples of adaptive theories include the religions that attribute any evidence contrary to their claims to demons or to faith testing and temptations.</p></div></div></div></div>    
</body>
</html>