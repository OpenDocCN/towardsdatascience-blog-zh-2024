- en: TinyLlama —The Promising Generation of Powerful Smaller Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20](https://towardsdatascience.com/tiny-llama-a-performance-review-and-discussion-a68d68bc2826?source=collection_archive---------5-----------------------#2024-04-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn about TinyLlama, a smaller language model capable of a variety of complex
    tasks with a small amount of compute
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--a68d68bc2826--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a68d68bc2826--------------------------------)
    ·10 min read·Apr 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: TinyLlama is an open-source project that trains a small language model of around
    1.1B parameters. The project aims to have a language model capable of performing
    tasks a full LLM like Llama 2 can achieve but with less memory usage. This article
    will discuss how TinyLlama can be implemented and run locally on your computer.
    Furthermore, it will also discuss TinyLlama's current performance, along with
    its strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/305119936432aa90313172421cf45caf.png)'
  prefs: []
  type: TYPE_IMG
- en: ChatGPT’s imagination of a TinyLlama model. OpenAI. (2024). *ChatGPT* (4) [Large
    language model]. [https://chat.openai.com](https://chat.openai.com)
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: · [Table of contents](#a4a9)
  prefs: []
  type: TYPE_NORMAL
- en: · [Motivation](#aa74)
  prefs: []
  type: TYPE_NORMAL
- en: · [Implementing the model locally](#051b)
  prefs: []
  type: TYPE_NORMAL
- en: · [Testing the model](#d239)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Fibonacci sequence](#dc58)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [RAG](#2509)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Generating dialog](#879b)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Coding with TinyLlama](#b65c)
  prefs: []
  type: TYPE_NORMAL
- en: · [My thoughts on the model](#da52)
  prefs: []
  type: TYPE_NORMAL
- en: · [Conclusion](#dc79)
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My motivation for writing this article is to keep up with the latest trends
    in machine learning. Though TinyLlama was released a few…
  prefs: []
  type: TYPE_NORMAL
