- en: Extensible and Customisable Vertex AI MLOps Platform
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展和可定制的 Vertex AI MLOps 平台
- en: 原文：[https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29](https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29](https://towardsdatascience.com/extensible-and-customisable-vertex-ai-mlops-platform-a2c146d13186?source=collection_archive---------6-----------------------#2024-02-29)
- en: MLOps Platform
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps 平台
- en: Building scalable Kubeflow ML pipelines on Vertex AI and ‘jailbreaking’ Google
    prebuilt containers
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Vertex AI 上构建可扩展的 Kubeflow ML 管道，并“越狱”Google 预构建容器
- en: '[](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)[![Kabeer
    Akande](../Images/5e1f083e75741690ae27b00d1e5f1dd3.png)](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)
    [Kabeer Akande](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)[![Kabeer
    Akande](../Images/5e1f083e75741690ae27b00d1e5f1dd3.png)](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)
    [Kabeer Akande](https://koakande.medium.com/?source=post_page---byline--a2c146d13186--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)
    ·18 min read·Feb 29, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a2c146d13186--------------------------------)
    ·18 分钟阅读·2024年2月29日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dc70146f0e90bd86c36b2bb56e9e0506.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc70146f0e90bd86c36b2bb56e9e0506.png)'
- en: Tools and corresponding operations supporting MLOps platform
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 支持 MLOps 平台的工具及相应操作
- en: When I decided to write an article on building scalable pipelines with Vertex
    AI last year, I contemplated the different formats it could take. I finally settled
    on building a fully functioning MLOps platform, as lean as possible due to time
    restriction, and open source the platform for the community to gradually develop.
    But time proved a limiting factor and I keep dillydallying. On some weekends,
    when I finally decided to put together the material, I found a litany of issues
    which I have now documented to serve as guide to others who might tread the same
    path.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我去年决定写一篇关于在 Vertex AI 上构建可扩展管道的文章时，我考虑了不同的格式。最终，我决定构建一个功能完善的 MLOps 平台，由于时间限制，尽量精简，并将平台开源，供社区逐步开发。但时间证明是一个制约因素，我一直在拖延。在一些周末，当我终于决定整理材料时，我发现了许多问题，现在我已将这些问题记录下来，作为指南，帮助其他可能走相同道路的人。
- en: 'This is what led to the development of [mlops-platform](https://github.com/kbakande/mlops-platform),
    an initiative designed to demonstrate a streamlined, end-to-end process of building
    scalable and operationalised machine learning models on VertexAI using Kubeflow
    pipelines. The major features of the platform can be broken down in fourfold:
    firstly, it encapsulates a modular and flexible pipeline architecture that accommodates
    various stages of the machine learning lifecycle, from data loading and preprocessing
    to model training, evaluation, deployment and inference. Secondly, it leverages
    Google Cloud’s Vertex AI services for seamless integration, ensuring optimal performance,
    scalability, and resource efficiency. Thirdly, it is scaffolded with a series
    of operations that are frequently used to automate ML workflows. Lastly, it documents
    common challenges experienced when building projects of this scale and their respective
    workarounds.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是促使 [mlops-platform](https://github.com/kbakande/mlops-platform) 项目的发展的原因，该项目旨在展示如何利用
    Kubeflow 管道在 VertexAI 上构建可扩展且具备操作能力的机器学习模型的简化、端到端流程。该平台的主要特点可以归纳为四个方面：首先，它封装了一个模块化且灵活的管道架构，能够支持机器学习生命周期的各个阶段，从数据加载和预处理到模型训练、评估、部署和推理。其次，它利用
    Google Cloud 的 Vertex AI 服务实现无缝集成，确保最佳的性能、可扩展性和资源效率。第三，它构建了一系列常用操作，用于自动化机器学习工作流。最后，它记录了在构建此类规模项目时常见的挑战及其相应的解决方案。
- en: 'I have built the *mlops platform* with two major purposes in mind:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我构建了这个*mlops平台*，有两个主要目的：
- en: To serve as an educational place where the community can learn about the fundamental
    components of MLOps platform including the various operations that enable such
    platform
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为一个教育平台，社区成员可以在这里了解MLOps平台的基本组成部分，包括使该平台得以运行的各种操作。
- en: To serve as building blocks for teams with little to no engineering support
    so they can self serve when developing data science and ML engineering projects
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为没有或几乎没有工程支持的团队的构建模块，使他们在开发数据科学和ML工程项目时能够自助服务
- en: I hope the platform will continue to grow from contributions from the community.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个平台能够通过社区的贡献继续成长。
- en: Though Google has a [GitHub repo](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main)
    containing numerous examples of using Vertex AI pipeline, the repo is daunting
    to navigate. Moreover, you often need a multiple of ops wrappers around your application
    for organisation purposes as you would have multiple teams using the platform.
    And more often, there are issues that crop up during development that do not get
    addressed enough, leaving developers frustrated. Google support might be insufficient
    especially when chasing production deadlines. On a personal experience, even though
    my company have enhanced support, I have an issue raised with Google Vertex engineering
    team which drags on for more than four months. In addition, due to the rapid pace
    at which technology is evolving, posting on forums might not yield desired solution
    since only few people might have experienced the issue being posted about. So
    having a working end to end platform to build upon with community support is invaluable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Google有一个包含大量使用Vertex AI流水线示例的[GitHub仓库](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main)，但这个仓库难以浏览。而且，通常你需要在应用程序周围加上多个操作封装器来进行组织，因为会有多个团队使用该平台。而且在开发过程中，常常会出现一些问题没有得到足够的解决，导致开发者感到沮丧。尤其是在追赶生产周期时，Google的支持可能不足。根据我的个人经验，即使我的公司有增强的支持，我也曾向Google
    Vertex工程团队提出一个问题，结果拖延了四个多月。此外，由于技术更新的速度非常快，论坛上的帖子可能无法得到期望的解决方案，因为只有少数人可能遇到过该问题。因此，拥有一个可用的端到端平台，并且能够获得社区支持是非常宝贵的。
- en: By the way, have you heard about pain driven development (PDD)? It is analogous
    to test or behaviour driven development. In PDD, the development is driven by
    pain points. This means changes are made to codebase when the team feels impacted
    and could justify the trade off. It follows the mantra of *if it ain’t broke,
    don’t fix*. Not to worry, this post will save some pains (emanating from frustration)
    when using Google Vertex AI, especially the prebuilt containers, for building
    scalable ML pipelines. But more appropriately, in line with the PDD principle,
    I have deliberately made it a working platform with some pain points. I have detailed
    those pain points hoping that interested parties from the community would join
    me in gradually integrating the fixes. With those house keeping out of the way,
    lets cut to the chase!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便问一下，你听说过“痛点驱动开发”（PDD）吗？它类似于测试驱动开发或行为驱动开发。在PDD中，开发是由痛点驱动的。这意味着当团队感到受到影响并且能够合理化折衷时，就会对代码库进行更改。它遵循这样的原则：*如果它没有坏，别修*。不用担心，这篇文章将帮助你解决一些在使用Google
    Vertex AI时（特别是预构建容器）遇到的痛点，尤其是在构建可扩展的ML流水线时带来的困惑。不过，更准确地说，遵循PDD原则，我有意将它做成一个包含一些痛点的工作平台。我已详细列出了这些痛点，希望有兴趣的社区成员能够加入我，共同逐步整合解决方案。废话不多说，接下来我们切入正题！
- en: Google Vertex AI pipelines provides a framework to run ML workflows using pipelines
    that are designed with Kubeflow or Tensorflow Extended frameworks. In this way,
    Vertex AI serves as an **orchestration platform** that allows composing a number
    of ML tasks and automating their executions on GCP infrastructure. This is an
    important distinction to make since we don’t write the pipelines with Vertex AI
    rather, it serves as the platform for orchestrating the pipelines. The underlying
    Kubeflow or Tensorflow Extended pipeline follows common framework used for orchestrating
    tasks in modern architecture. The framework separates logic from computing environment.
    The logic, in the case of ML workflow, is the ML code while the computing environment
    is a container. Both together are referred to as a **component**. When multiple
    components are grouped together, they are referred to as pipeline. There is modality
    in place, similar to other orchestration platforms, to pass data between the components.
    The best place to learn in depth about pipelines is from [Kubeflow](https://www.kubeflow.org/docs/components/pipelines/v2/)
    documentation and several other blog posts which I have linked in the references
    section.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI 流程提供了一个框架，通过使用 Kubeflow 或 Tensorflow Extended 框架设计的流程来运行 ML
    工作流。通过这种方式，Vertex AI 充当一个**编排平台**，允许将多个 ML 任务组合起来，并在 GCP 基础设施上自动执行它们。这是一个重要的区分点，因为我们并不是使用
    Vertex AI 编写流程，而是它作为编排流程的平台。底层的 Kubeflow 或 Tensorflow Extended 流程遵循用于现代架构中编排任务的常见框架。该框架将逻辑与计算环境分开。在
    ML 工作流的情况下，逻辑是 ML 代码，而计算环境是容器。两者合起来称为**组件**。当多个组件被组合在一起时，它们被称为流程。在这些编排平台中，类似的机制被用来在组件之间传递数据。关于流程的深入学习最好参考[Kubeflow](https://www.kubeflow.org/docs/components/pipelines/v2/)的文档以及我在参考文献部分链接的几篇博客文章。
- en: I mentioned the general architecture of orchestration platforms previously.
    Some other tools using similar architecture as Vertex AI where logic are separated
    from compute are Airflow (tasks and executors), GitHub actions (jobs and runners),
    CircleCI (jobs and executors) and so on. I have an article in the pipeline on
    how having a good grasp of the principle of separation of concerns integrated
    in this modern workflow architecture can significantly help in the day to day
    use of the tools and their troubleshooting. Though Vertex AI is synonymous for
    orchestrating ML pipelines, in theory any logic such as Python script, data pipeline
    or any containerised application could be run on the platform. Composer, which
    is a managed Apache Airflow environment, was the main orchestrating platform on
    GCP prior to Vertex AI. The two platforms have [pros and cons](https://datatonic.com/insights/kubeflow-pipelines-cloud-composer-data-orchestration/)
    that should be considered when making a decision to use either.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前提到过调度平台的整体架构。像 Vertex AI 这样将逻辑与计算分离的类似架构的其他工具有 Airflow（任务和执行器）、GitHub actions（工作和运行器）、CircleCI（工作和执行器）等。我有一篇文章在准备中，内容是关于如何深入理解这种现代工作流架构中集成的关注点分离原则，可以在日常使用这些工具和故障排除中带来显著帮助。尽管
    Vertex AI 是编排 ML 流程的代名词，但理论上，任何逻辑，如 Python 脚本、数据流程或任何容器化应用，都可以在该平台上运行。Composer，作为一个托管的
    Apache Airflow 环境，是在 Vertex AI 之前 GCP 上的主要编排平台。这两个平台各有[优缺点](https://datatonic.com/insights/kubeflow-pipelines-cloud-composer-data-orchestration/)，在决定使用其中一个时需要加以考虑。
- en: I am going to avoid spamming this post with code which are easily accessible
    from the platform [repository](https://github.com/kbakande/mlops-platform). However,
    I will run through the important parts of the mlops platform architecture. Please
    refer to the repo to follow along.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我将避免在这篇文章中大量展示代码，因为这些代码可以从平台的[代码库](https://github.com/kbakande/mlops-platform)轻松获取。不过，我会简要介绍
    mlops 平台架构中的重要部分。请参考代码库以便跟进。
- en: '![](../Images/c5bcc700a6db07cc2b0eb6ae80ffae6a.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5bcc700a6db07cc2b0eb6ae80ffae6a.png)'
- en: MLOps platform
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 平台
- en: '**Components**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**组件**'
- en: The architecture of the platform revolves around a set of well-defined components
    housed within the *components* directory. These components, such as data loading,
    preprocessing, model training, evaluation, and deployment, provide a modular structure,
    allowing for easy customisation and extension. Lets look through one of the components,
    the [preprocess_data.py](https://github.com/kbakande/mlops-platform/blob/main/components/preprocess_data.py),
    to understand the general structure of a component.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 平台的架构围绕一组定义良好的组件，这些组件位于 *components* 目录中。这些组件包括数据加载、预处理、模型训练、评估和部署，提供了一种模块化结构，便于定制和扩展。让我们看看其中一个组件，[preprocess_data.py](https://github.com/kbakande/mlops-platform/blob/main/components/preprocess_data.py)，以了解组件的一般结构。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: A closer look at the script above would show a familiar data science workflow.
    All the script does is read in some data, split them for model development and
    write the splits to some path where it can be readily accessed by downstream tasks.
    However, since this function would be run on Vertex AI, it is decorated by a Kubeflow
    pipeline *@dsl.component(base_image=base_image)* which marks the function as a
    Kubeflow pipeline component to be run within the `base_image` container. I will
    talk about the `base_image` later. This is all is required to run a function within
    a container on Vertex AI. Once we structured all our other functions in similar
    manner and decorate them as Kubeflow pipeline components, the `mlpipeline.py`
    function will import each components to structure the pipeline.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看上面的脚本，你会发现一个熟悉的数据科学工作流。这个脚本所做的只是读取一些数据，将它们拆分以进行模型开发，然后将拆分后的数据写入某个路径，以便下游任务可以轻松访问。然而，由于这个函数将在
    Vertex AI 上运行，它被一个 Kubeflow 流水线装饰器 *@dsl.component(base_image=base_image)*，这标记该函数为一个
    Kubeflow 流水线组件，在 `base_image` 容器中运行。稍后我会谈到 `base_image`。这就是在 Vertex AI 上的容器中运行函数所需的所有内容。一旦我们以类似的方式构建了其他所有函数，并将它们装饰为
    Kubeflow 流水线组件，`mlpipeline.py` 函数将导入每个组件以构建流水线。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`@pipeline` decorator enables the function `mlplatform_pipeline` to be run
    as a pipeline. The pipeline is then compiled to the specified pipeline filename.
    Here, I have specified `JSON` configuration extension for the compiled file but
    I think Google is moving to`YAML`. The compiled file is then picked up by `aiplatform`
    and submitted to Vertex AI platform for execution.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`@pipeline` 装饰器使得函数 `mlplatform_pipeline` 可以作为流水线运行。然后，流水线将被编译成指定的流水线文件名。在这里，我指定了
    `JSON` 配置扩展名用于编译后的文件，但我认为 Google 正在转向使用 `YAML`。编译后的文件随后会被 `aiplatform` 拿到，并提交给
    Vertex AI 平台执行。'
- en: The only other thing I found puzzling while starting out with the kubeflow pipelines
    are the [parameters and artifacts](https://www.kubeflow.org/docs/components/pipelines/v2/data-types/)
    set up so have a look to get up to speed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始使用 Kubeflow 流水线时，唯一让我感到困惑的就是[参数和工件](https://www.kubeflow.org/docs/components/pipelines/v2/data-types/)的设置，因此请查看一下，帮助你快速上手。
- en: '**Configuration**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**配置**'
- en: The configuration file in the *config* directory facilitates the adjustment
    of parameters and settings across different stages of the pipeline. Along with
    the config file, I have also included a `dot.env` file which has comments on the
    variables specifics and is meant to be a guide for the nature of the variables
    that are loaded into the `config` file.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*config* 目录中的配置文件便于在流水线的不同阶段调整参数和设置。除了配置文件，我还包含了一个 `dot.env` 文件，其中有关于变量的注释，旨在指导如何加载到
    `config` 文件中的变量。'
- en: '**Notebooks**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**笔记本**'
- en: I mostly start my workflow and exploration within notebooks as it enable easy
    interaction. As a result, I have included *notebooks* directory as a means of
    experimenting with the different components logics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常在笔记本中开始我的工作流和探索，因为它便于交互。因此，我包含了 *notebooks* 目录作为实验不同组件逻辑的一种方式。
- en: '**Testing**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**测试**'
- en: Testing plays a very important role in ensuring the robustness and reliability
    of machine learning workflows and pipelines. Comprehensive testing establishes
    a systematic approach to assess the functionality of each component and ensures
    that they behave as intended. This reduces the instances of errors and malfunctioning
    during the execution stage. I have included a `test_mlpipeline.py` script mostly
    as a guide for the testing process. It uses [pytest](https://pytest.org/en/7.4.x/getting-started.html)
    to illustrate testing concept and provides a framework to build upon.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 测试在确保机器学习工作流和管道的稳健性和可靠性方面起着非常重要的作用。全面的测试建立了一个系统化的方法来评估每个组件的功能，并确保它们按预期运行。这减少了在执行阶段出现错误和故障的情况。我已经包括了一个`test_mlpipeline.py`脚本，主要作为测试过程的指南。它使用[pytest](https://pytest.org/en/7.4.x/getting-started.html)来说明测试概念，并提供了一个构建框架。
- en: '**Project Dependencies**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**项目依赖**'
- en: Managing [dependencies](https://www.linkedin.com/posts/maria-vechtomova_python-softwaredevelopment-activity-7157288974921662464-q7kR?utm_source=share&utm_medium=member_desktop)
    can be a nightmare when developing enterprise scale applications. And given the
    myriads of packages required in a ML workflow, combined with the various software
    applications needed to operationalise it, it can become a Herculean task managing
    the dependencies in a sane manner. One package that is slowly gaining traction
    is [Poetry](https://python-poetry.org/docs/). It is a tool for dependency management
    and packaging in Python. The key files generated by Poetry are `pyproject.toml`
    and `poetry.lock`. `pyproject.toml`file is a configuration file for storing project
    metadata and dependencies while the `poetry.lock` file locks the exact versions
    of dependencies, ensuring consistent and reproducible builds across different
    environments. Together, these two files enhance dependency resolution. I have
    demonstrated how the two files replace the use of `requirement.txt` within a container
    by using them to generate the training container image for this project.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发企业级应用时，管理[依赖](https://www.linkedin.com/posts/maria-vechtomova_python-softwaredevelopment-activity-7157288974921662464-q7kR?utm_source=share&utm_medium=member_desktop)可能是一个噩梦。考虑到机器学习工作流中需要的各种包，以及为了使其能够运行所需的各种软件应用程序，管理这些依赖关系可能变成一项艰巨的任务。一个正在逐渐获得关注的包是[Poetry](https://python-poetry.org/docs/)。它是一个用于Python中的依赖管理和打包的工具。Poetry生成的关键文件是`pyproject.toml`和`poetry.lock`。`pyproject.toml`文件是一个配置文件，用于存储项目元数据和依赖关系，而`poetry.lock`文件则锁定依赖项的确切版本，确保在不同环境中的构建具有一致性和可重现性。这两个文件共同增强了依赖关系解析。我已经演示了如何使用这两个文件替代容器中的`requirement.txt`，并使用它们生成此项目的训练容器镜像。
- en: '**Makefile**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**Makefile**'
- en: A Makefile is a build automation tool that facilitates the compilation and execution
    of a project’s tasks through a set of predefined rules. Developers commonly use
    Makefiles to streamline workflows, automate repetitive tasks, and ensure consistent
    and reproducible builds. The Makefile within *mlops-platform* has predefined commands
    to seamlessly run the entire pipeline and ensure the reliability of the components.
    For example, the `all` target, specified as the default, efficiently orchestrates
    the execution of both the ML pipeline (`run_pipeline`) and tests (`run_tests`).
    Additionally, the Makefile provides a `clean` target for tidying up temporary
    files while the `help` target offers a quick reference to the available commands.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Makefile是一个构建自动化工具，通过一组预定义的规则促进项目任务的编译和执行。开发人员通常使用Makefile来简化工作流程，自动化重复任务，并确保一致和可重现的构建。*mlops-platform*中的Makefile具有预定义的命令，可以无缝运行整个管道并确保组件的可靠性。例如，指定为默认目标的`all`目标有效地编排了ML管道（`run_pipeline`）和测试（`run_tests`）的执行。此外，Makefile还提供了一个`clean`目标，用于清理临时文件，而`help`目标则提供了一个可用命令的快速参考。
- en: '**Documentation**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**文档**'
- en: The project is documented in the *README.md* file, which provides a comprehensive
    guide to the project. It includes detailed instructions on installation, usage,
    and setting up Google Cloud Platform services.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的文档记录在*README.md*文件中，提供了项目的全面指南。它包括有关安装、使用以及设置Google Cloud Platform服务的详细说明。
- en: '**Orchestration with CI/CD**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**CI/CD编排**'
- en: GitHub Actions workflow defined in *.github/workflows* directory is crucial
    for automating the process of testing, building, and deploying the machine learning
    pipeline to Vertex AI. This CI/CD approach ensures that changes made to the codebase
    are consistently validated and deployed, enhancing the project’s reliability and
    reducing the likelihood of errors. The workflow triggers on each push to the main
    branch or can be manually executed, providing a seamless and reliable integration
    process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 工作流定义在 *.github/workflows* 目录中，对于自动化测试、构建和将机器学习管道部署到 Vertex AI
    的过程至关重要。该 CI/CD 方法确保对代码库的每次更改都能被持续验证和部署，从而提高项目的可靠性并减少错误发生的可能性。工作流会在每次推送到主分支时触发，或者可以手动执行，提供无缝且可靠的集成过程。
- en: '**Inference Pipeline**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理管道**'
- en: 'There are multiple ways to implement inference or prediction pipeline. I have
    gone the good old way here by loading in both the prediction features and the
    uploaded model, getting predictions from the model and writing the predictions
    to a BigQuery table. It is worth noting that for all the talk about prediction
    containers, they are not really needed if all is required is batch prediction.
    We might as well use the training container for our batch prediction as demonstrated
    in the platform. However, the prediction container is required for online prediction.
    I have also included modality for local testing of the batch prediction pipeline
    which can be generalised to test any of the other components or any scripts for
    that matter. Local testing can be done by navigating to *batch_prediction/batch_prediction_test*
    directory, substituting for placeholder variables and running the following commands:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实现推理或预测管道的方法有多种。我在这里采用了传统的方式，通过加载预测特征和上传的模型，从模型中获取预测结果并将预测结果写入 BigQuery 表。值得注意的是，尽管有很多关于预测容器的讨论，但如果仅需要批量预测，实际上并不需要预测容器。我们完全可以使用训练容器来进行批量预测，正如平台中所演示的那样。然而，在线预测时需要使用预测容器。我还包括了本地测试批量预测管道的方式，且这一方法可以推广到测试其他组件或任何脚本。可以通过导航到
    *batch_prediction/batch_prediction_test* 目录，替换占位符变量并运行以下命令来进行本地测试：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The service account needs [proper access](https://stackoverflow.com/questions/46287267/how-can-i-get-the-file-service-account-json-for-google-translate-api)
    on GCP to execute the task above, it should have permission to read from the GCP
    bucket and write to the BigQuery table.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 服务账户需要在 GCP 上具有[适当的访问权限](https://stackoverflow.com/questions/46287267/how-can-i-get-the-file-service-account-json-for-google-translate-api)，才能执行上述任务，应该具有从
    GCP 存储桶读取和向 BigQuery 表写入的权限。
- en: 'Challenges and Solutions: `Jailbreaking’ Google Vertex AI prebuilt containers'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战与解决方案：`越狱` Google Vertex AI 预构建容器
- en: Some of the challenges encountered during the building of this project emanates
    from the use of container images and the associated package versions within the
    Google prebuilt containers. I presume the main goal of Google when creating prebuilt
    containers is to lift off major engineering tasks for the data scientists and
    enable them to focus mainly on ML logics. However, more work would be required
    to ensure this aim is achieved as the prebuilt containers have various versions
    mismatch requiring significant debugging effort to resolve. I have detailed some
    of the challenges and some possible fixes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建此项目过程中遇到的一些挑战来源于使用容器镜像以及 Google 预构建容器中的相关软件包版本。我推测 Google 创建预构建容器的主要目标是减轻数据科学家的主要工程任务，让他们能主要集中于机器学习逻辑。然而，为了确保实现这一目标，还需要做更多的工作，因为预构建容器存在版本不匹配的问题，这需要进行大量的调试工作。我已经详细列出了一些挑战以及可能的解决方案。
- en: '**Multi-Architectural image build**: While using macOS has its upsides, building
    container image on them to be deployed on cloud platforms might not be one of
    them. The main challenge is that most cloud platforms supports Linux running on
    *amd64* architecture while latest macOS systems run on *arm64* architecture. As
    a result, binaries compiled on macOS would ordinarily not be compatible with Linux.
    This means that built images that compile successfully on macOS might fail when
    run on most cloud platforms. And what is more, the log messages that result from
    this error is tacit and unhelpful, making it challenging to debug. **It should
    be noted that this is an issue with most modern cloud platforms and not peculiar
    to GCP**. As a result, there are multiple workarounds to overcome this challenge.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多架构镜像构建**: 虽然使用 macOS 有其优势，但在 macOS 上构建容器镜像并将其部署到云平台上可能并非其中之一。主要的挑战是，大多数云平台支持在
    *amd64* 架构上运行 Linux，而最新的 macOS 系统运行在 *arm64* 架构上。因此，在 macOS 上编译的二进制文件通常与 Linux
    不兼容。这意味着，在 macOS 上成功编译的镜像在大多数云平台上运行时可能会失败。而且，由于错误日志消息通常是含糊的且没有帮助，这使得调试变得非常困难。**需要注意的是，这个问题存在于大多数现代云平台中，并非
    GCP 独有**。因此，存在多种解决方法来克服这一挑战。'
- en: '**Use BuildX**: [Buildx](https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/)
    is a Docker CLI plugin that allows building a multi-architecture container image
    that can run on multiple platforms. Ensure Docker desktop is installed as it is
    required to build image locally. Alternatively, the image can be built from Google
    cloud shell. The following script would build a compatible container image on
    macOS and push it to GCP artifact registry.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 BuildX**: [Buildx](https://www.docker.com/blog/multi-arch-build-and-images-the-simple-way/)
    是一个 Docker CLI 插件，允许构建一个多架构容器镜像，可以在多个平台上运行。确保已安装 Docker 桌面版，因为它是本地构建镜像所必需的。或者，可以通过
    Google Cloud Shell 来构建镜像。以下脚本将在 macOS 上构建一个兼容的容器镜像，并将其推送到 GCP artifact registry。'
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The name of the container follows Google specific format for naming [containers](https://cloud.google.com/artifact-registry/docs/docker/names).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 容器的名称遵循 Google 特定的命名格式，详见 [容器](https://cloud.google.com/artifact-registry/docs/docker/names)。
- en: '**Set Docker environment** [**variable**](https://stackoverflow.com/questions/65612411/forcing-docker-to-use-linux-amd64-platform-by-default-on-macos):
    Set *DOCKER_DEFAULT_PLATFORM* permanently in the macOS system config file to ensure
    that Docker always build image compatible with Linux *amd64*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设置 Docker 环境** [**变量**](https://stackoverflow.com/questions/65612411/forcing-docker-to-use-linux-amd64-platform-by-default-on-macos):
    在 macOS 系统配置文件中永久设置 *DOCKER_DEFAULT_PLATFORM*，以确保 Docker 始终构建与 Linux *amd64* 兼容的镜像。'
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 2\. **Conflicting versions in prebuilt container images:** Google maintains
    a host of prebuilt images for [prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)
    and [training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)
    tasks. These container images are available for common ML frameworks in different
    versions. However, I found that the documented versions sometimes don’t match
    the actual version and this constitute a major point of failure when using these
    container images. Giving what the community has gone through in standardising
    versions and dependencies and the fact that container technology is developed
    to mainly address reliable execution of applications, I think Google should strive
    to address the conflicting versions in the prebuilt container images. Make no
    mistake, battling with version mismatch can be frustrating which is why I encourage
    ‘jailbreaking’ the prebuilt images prior to using them. When developing this tutorial,
    I decided to use`europe-docker.pkg.dev/vertex-ai/training/sklearn-gpu.1-0:latest`
    and `europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest`. From
    the naming conventions, both are supposed to be compatible and should have`sklearn==1.0`.
    In fact, this is confirmed on the site as shown in the screenshot below and also,
    on the container image artifact registry.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **预构建容器镜像中的版本冲突：** Google 为[预测](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers)和[训练](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)任务维护了一系列预构建镜像。这些容器镜像为常见的机器学习框架提供了不同版本。然而，我发现文档中列出的版本有时与实际版本不符，这在使用这些容器镜像时是一个主要的失败点。鉴于社区在标准化版本和依赖关系方面的努力，以及容器技术主要是为了解决应用程序可靠执行的问题，我认为
    Google 应该致力于解决预构建容器镜像中的版本冲突。不要误会，版本不匹配的斗争可能让人沮丧，这也是为什么我鼓励在使用这些镜像之前进行“越狱”。在编写这个教程时，我决定使用`europe-docker.pkg.dev/vertex-ai/training/sklearn-gpu.1-0:latest`和`europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest`。从命名约定来看，两个镜像应该是兼容的，并且都应该包含`sklearn==1.0`。事实上，网站上确认了这一点，如下图所示，容器镜像的
    artifact registry 也显示了这一点。
- en: '![](../Images/5012d1770574c50d6fd8c1f36cf158f5.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5012d1770574c50d6fd8c1f36cf158f5.png)'
- en: Screenshot from the training prebuilt image [page](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来自训练预构建镜像的截图，[页面](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers)
- en: However, the reality is different. I ran into version mismatch errors when deploying
    the built model to an endpoint. A section of the error message is shown below.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实却不同。当我将构建的模型部署到端点时，遇到了版本不匹配的错误。错误消息的一部分如下所示。
- en: '*Trying to unpickle estimator OneHotEncoder from version 1.0.2 when using version
    1.0*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*尝试从版本 1.0.2 中反序列化估算器 OneHotEncoder，而使用的是版本 1.0*'
- en: Suprise! Suprise! Suprise! Basically, what the log says is that you have pickled
    with version *1.0.2* but attempting to unpickle with version *1.0*. To make progress,
    I decided to do some ‘jailbreaking’ and looked under the hood of the prebuilt
    container images. It is a very basic procedure but opened many can of worms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 惊讶！惊讶！惊讶！基本上，日志所说的是你已经用版本*1.0.2*进行了序列化，但尝试用版本*1.0*进行反序列化。为了推进，我决定进行一些“越狱”，并查看预构建容器镜像的内部。这是一个非常基础的过程，但却引发了许多问题。
- en: From the terminal or Google [cloud shell](https://console.cloud.google.com/home/dashboard?cloudshell=true)
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从终端或 Google [Cloud Shell](https://console.cloud.google.com/home/dashboard?cloudshell=true)
- en: Pull the respective image from Google artifact registry
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Google Artifact Registry 拉取相应的镜像
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 3\. Run the image, overide its entrypoint command and drop onto its bash shell
    terminal
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 运行镜像，覆盖其入口命令，并进入其 bash shell 终端
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 4\. Check the *sklearn* version
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 检查 *sklearn* 版本
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output, as of the time of writing this post, is shown in the screenshot
    below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，输出如下截图所示：
- en: '![](../Images/6bc357ff054b1654dd689ff1c96a8097.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bc357ff054b1654dd689ff1c96a8097.png)'
- en: Conducting similar exercise for `europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest`
    , the sklearn version is `1.3.2` and `1.2.2` for the `1.2`version. What is even
    more baffling is that `pandas` is missing from both version `1–2`and `1-3` which
    begs the question of whether the prebuilt containers are being actively maintained.
    Of course, the issue is not the minor update but the fact that the corresponding
    prediction image did not have similar update which results in the mismatch error
    shown above.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `europe-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest` 执行类似的操作时，sklearn
    版本是 `1.3.2`，而 `1.2` 版本则是 `1.2.2`。更让人困惑的是，`pandas` 在版本 `1–2` 和 `1-3` 中都缺失了，这让人质疑预构建容器是否得到了积极维护。当然，问题不在于小更新，而在于相应的预测镜像没有类似的更新，这导致了上述的版本不匹配错误。
- en: When I contacted Google support to report the mismatch, the Vertex AI engineering
    team mentioned alternatives such as Custom prediction routines (CPR) and SklearnPredictor.
    And I was pointed to newer image versions with similar issues and missing `pandas`!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我联系 Google 支持报告不匹配问题时，Vertex AI 工程团队提到了替代方案，如自定义预测例程（Custom prediction routines,
    CPR）和 SklearnPredictor。并且我被指引查看了具有类似问题和缺失 `pandas` 的较新镜像版本！
- en: Moving on, if you are feeling like a Braveheart and want to explore further,
    you can access all the other files that Google runs when launching prebuilt containers
    by running `ls` command from within the container and looking through the files
    and folders.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果你感觉像个勇敢的心（Braveheart），并且想深入探索，可以通过在容器内运行 `ls` 命令，查看 Google 启动预构建容器时运行的所有其他文件，查看文件和文件夹。
- en: Build Base Image
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基础镜像
- en: So having discovered the issue, what can be done in order to still take advantage
    of prebuilt containers? What I did was to extract all the **relevant** packages
    from the container.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在发现问题后，如何才能仍然利用预构建容器呢？我所做的是从容器中提取所有**相关**的包。
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The commands above will extract all the installed packages and print them to
    the container terminal. The packages can then be copied and used in creating a
    custom container image, ensuring that the ML framework version in both the training
    and prediction container matches. If you prefer to copy the file content to your
    local directory then use the following command:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将提取所有已安装的包，并将其打印到容器终端。然后，可以复制这些包并在创建自定义容器镜像时使用，确保训练和预测容器中的 ML 框架版本匹配。如果你更喜欢将文件内容复制到本地目录，可以使用以下命令：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Some of the packages in the prebuilt containers would not be needed for individual
    project so it is better to select the ones that matches your workflow. The most
    important one to lock down is the ML framework version whether it is *sklearn*
    or *xgboost,* making sure both training and prediction versions match.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 预构建容器中的某些包对于单个项目可能不需要，因此最好选择与工作流程匹配的包。最重要的是锁定 ML 框架版本，无论是 *sklearn* 还是 *xgboost*，确保训练和预测的版本匹配。
- en: I have basically locked the sklearn version to match the version of the prebuilt
    prediction image. In this case, it is version `1.0` and I have left all the other
    packages as they are.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我基本上锁定了 sklearn 版本，以匹配预构建预测镜像的版本。在这种情况下，它是版本 `1.0`，其余的包保持不变。
- en: 'Then to build the custom training image, use the following commands:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，构建自定义训练镜像时，使用以下命令：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The above is saying is:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容的意思是：
- en: 'docker: hey Docker!'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'docker: 嘿，Docker！'
- en: 'build: build an image for me'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'build: 为我构建一个镜像'
- en: '-f: use the following file'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-f: 使用以下文件'
- en: '-t: tag (or name) it the following'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-t: 将其标记（或命名）为以下内容'
- en: '. : use files in this directory (current directory in this case) if needed'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '. : 如果需要，使用当前目录中的文件（此处为当前目录）'
- en: 'Then the built image can be pushed to the artifact registry as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以通过以下方式将构建的镜像推送到工件注册表：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Vision
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视野
- en: There are numerous extensions to be added to this project and I will invite
    willing contributors to actively pick on any of them. Some of my thoughts are
    detailed below but feel free to suggest any other improvements. Contributions
    are welcomed via PR. I hope the repo can be actively developed by those who wants
    to learn end to end MLOps as well as serve as a base on which small teams can
    build upon.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目需要添加许多扩展，我将邀请有意的贡献者积极参与。以下是我一些详细的想法，但也欢迎提出其他改进建议。通过 PR 欢迎贡献。我希望这个仓库能够得到那些想学习端到端
    MLOps 的人的积极开发，并且作为小团队构建基础的基石。
- en: '**Monitoring pipeline**: Observability is integral to MLOps platform. It enables
    team to proactively monitors the state and behaviour of their platform and take
    appropriate action in the event of an anomaly. The `mlops-platform` is missing
    a monitoring pipeline and it would be a good addition. I plan to write on custom
    implementation of monitoring pipeline but in the mean time, Vertex AI has [monitoring
    pipeline](https://cloud.google.com/vertex-ai/docs/model-monitoring/overview) that
    can be integrated.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控管道：** 可观察性是MLOps平台的核心功能。它使团队能够主动监控平台的状态和行为，并在出现异常时采取适当的行动。`mlops-platform`缺少一个监控管道，这将是一个不错的补充。我计划写一篇关于自定义监控管道实现的文章，但与此同时，Vertex
    AI有一个可以集成的[监控管道](https://cloud.google.com/vertex-ai/docs/model-monitoring/overview)。'
- en: '**Inference pipeline:** Vertex AI has [batch prediction](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)
    method that could be integrated. An argument can be put forward on whether the
    current custom batch prediction in the mlops platform would scale. The main issue
    is that the prediction features are loaded into the predicting environment which
    might run into memory issue with very large dataset. I havent experienced this
    issue previously but it can be envisaged. Prior to Google rebranding aiplatform
    to Vertex AI, I have always deployed models to the aiplatform to benefit from
    its model versioning but would run the batch prediction pipeline within Composer.
    I prefer this approach as it gives flexibility in terms of pre and post processing.
    Moreover, Google batch prediction method is fiddly and tricky to [debug](https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/)
    when things go wrong. Nevertheless, I think it will improve with time so would
    be a good addition to the platform.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理管道：** Vertex AI 有一个[批量预测](https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)方法，可以进行集成。可以提出一个论点，当前在mlops平台上的自定义批量预测是否具备可扩展性。主要问题是预测特征被加载到预测环境中，可能会在非常大的数据集上遇到内存问题。我之前没有遇到过这个问题，但可以预见到它的发生。在Google将aiplatform更名为Vertex
    AI之前，我一直将模型部署到aiplatform，以便利用其模型版本管理，但会在Composer中运行批量预测管道。我更喜欢这种方法，因为它在预处理和后处理方面提供了灵活性。此外，Google的批量预测方法在[调试](https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/)时比较繁琐和棘手。当出现问题时，调试过程比较困难。不过，我认为随着时间的推移，它会有所改进，因此会成为平台的一个不错的补充。'
- en: '**Refactoring:** While I have coupled together computing and logic code in
    the implementation on same file, I think it would be cleaner if they are separated.
    Decoupling both would improve the modularity of the code and enable reusability.
    In addition, there should be a pipeline directory for the different pipeline files
    with potential integration of monitoring pipeline.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重构：** 尽管我在实现中将计算和逻辑代码耦合在同一文件中，但我认为如果将它们分开会更清晰。解耦这两者将提高代码的模块化，并增强代码的可重用性。此外，应为不同的管道文件创建一个管道目录，并可能集成监控管道。'
- en: '**Full customisation:** Containers should be fully customised in order to have
    fine-grained control and flexibility. This means having both training and prediction
    containers custom built.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全自定义：** 容器应完全自定义，以便进行精细控制和灵活性。这意味着训练和预测容器都需要进行自定义构建。'
- en: '**Testing:** I have integrated a testing framework which runs successfully
    within the platform but it is not a functional test logic. It does provide a framework
    to build proper tests covering data quality, components and pipelines functional
    tests.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试：** 我已经集成了一个测试框架，它在平台内成功运行，但它不是一个功能性测试逻辑。它确实提供了一个框架，用于构建覆盖数据质量、组件和管道功能测试的适当测试。'
- en: '**Containerisation integration**: The creation of the container base image
    is done manually at the moment but should be integrated in both the makefile and
    GitHub action workflow.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器化集成：** 容器基础镜像的创建目前是手动进行的，但应该集成到makefile和GitHub action工作流中。'
- en: '**Documentation**: The documentation would need updating to reflect additional
    features being added and ensure people with different skill sets can easily navigate
    through the platform. Please update the READ.me file for now but this project
    should use [Sphinx](https://www.sphinx-doc.org/en/master/) in the long run.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档：** 文档需要更新，以反映新增的功能，并确保不同技能的人可以轻松浏览平台。目前请更新READ.me文件，但该项目长期应使用[Sphinx](https://www.sphinx-doc.org/en/master/)。'
- en: '**Pre-commit hooks**: This is an important automation tool that can be employed
    to good use. Pre-commit hooks are configuration scripts executed prior to actioning
    a commit to help enforce styles and policy. For example, the hooks in the platform
    enforced linting and prevent committing large files as well as committing to the
    main branch. However, my main thought was to use it for dynamically updating GitHub
    secrets from the values in `.env` file. The GitHub secrets are statically typed
    in the current implementation so when certain variables change, they don’t get
    automatically propagated to GitHub secrets. Similar thing would occur when new
    variables are added which then needs to be manually propagated to GitHub. Pre-commit
    can be used to address this problem by instructing it to automatically propagate
    changes in the local `.env`file to GitHub secrets.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预提交钩子**：这是一个可以很好利用的重要自动化工具。预提交钩子是配置脚本，在执行提交之前运行，帮助强制执行代码风格和策略。例如，平台中的钩子强制执行代码风格检查，防止提交大文件以及提交到主分支。然而，我的主要想法是使用它来动态更新来自`.env`文件的GitHub秘密。当前实现中，GitHub秘密是静态类型的，因此当某些变量发生变化时，它们不会自动传播到GitHub秘密。当添加新变量时，也需要手动将其传播到GitHub。可以使用预提交钩子来解决这个问题，指示其自动将本地`.env`文件中的更改传播到GitHub秘密。'
- en: I**nfrastructure provisioning:** Artifact registry, GCP bucket, BigQuery table
    and service account are all provisioned manually but their creation should be
    automated via [Terraform](https://cloud.google.com/docs/terraform).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施配置**：Artifact Registry、GCP Bucket、BigQuery表和服务账户目前都需要手动配置，但它们的创建应该通过[Terraform](https://cloud.google.com/docs/terraform)进行自动化。'
- en: '**Scheduler**: If this is a batch prediction or continuous training pipeline,
    we would want to schedule it to run at some specified time and frequency. Vertex
    AI gives a number of options to [configure](https://cloud.google.com/vertex-ai/docs/pipelines/schedule-pipeline-run#aiplatform_create_pipeline_schedule_sample-python_vertex_ai_sdk)
    schedules. Indeed, an orchestration platform would not be complete without this
    feature.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度程序**：如果这是一个批量预测或持续训练管道，我们希望将其安排在特定的时间和频率运行。Vertex AI提供了多种选项来[配置](https://cloud.google.com/vertex-ai/docs/pipelines/schedule-pipeline-run#aiplatform_create_pipeline_schedule_sample-python_vertex_ai_sdk)调度。事实上，没有这个功能，一个编排平台就不完整。'
- en: '**Additional models**: There are two models (Random forest and Decision trees)
    within the platfrom now but should be straightforward adding other frameworks,
    such as xgboost and light GBM, for modelling tabular data.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**附加模型**：目前平台内有两个模型（随机森林和决策树），但应该可以直接添加其他框架，例如xgboost和light GBM，用于建模表格数据。'
- en: '**Security:** The GitHub action uses service account for authentication to
    GCP services but should ideally be using [workflow identity federation](https://cloud.google.com/iam/docs/workload-identity-federation).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：GitHub操作使用服务账户来进行GCP服务的身份验证，但理想情况下应该使用[工作流身份联合](https://cloud.google.com/iam/docs/workload-identity-federation)。'
- en: '**Distribution**: The platform is suitable in the current state for educational
    purpose and perhaps individual projects. However, it would require adaptation
    for bigger team. Think about individuals that make up teams with different skill
    set and varying challenges. In this regard, the platform interface can be improved
    using [click](https://click.palletsprojects.com/en/8.1.x/) as detailed in [this](https://medium.com/@vmo2techteam/the-journey-to-streamlining-our-ml-platform-interface-using-our-cli-tool-fd4735474cd5)
    post. Afterwards, it can be packaged and [distributed](https://packaging.python.org/en/latest/tutorials/packaging-projects/)
    to ensure easy installation. Also, distribution enables us to make changes to
    the package and centralise its updates so that it propagates as needed. Poetry
    can be used for the packaging and distribution so using it for dependency management
    has laid a good foundation.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分发**：该平台在当前状态下适用于教育目的以及可能的个人项目。然而，对于更大的团队，它需要进行适配。考虑到由具有不同技能和面临不同挑战的个体组成的团队。在这方面，平台界面可以通过使用[click](https://click.palletsprojects.com/en/8.1.x/)进行改进，具体细节见[这篇](https://medium.com/@vmo2techteam/the-journey-to-streamlining-our-ml-platform-interface-using-our-cli-tool-fd4735474cd5)文章。之后，可以将其打包并[分发](https://packaging.python.org/en/latest/tutorials/packaging-projects/)以确保简便的安装。同时，分发使我们能够对包进行更改并集中更新，以便根据需要传播。可以使用Poetry进行打包和分发，因此，使用它进行依赖管理为我们奠定了良好的基础。'
- en: '**Summary**'
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**总结**'
- en: The MLOps platform provides a modular and scalable pipeline architecture for
    implementing different ML lifecycle stages. It includes various operations that
    enable such platform to work seamlessly. Most importantly, it provides a learning
    opportunity for would be contributors and should serve as a good base on which
    teams can build upon in their machine learning tasks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps平台提供了一个模块化和可扩展的管道架构，用于实现不同的机器学习生命周期阶段。它包含各种操作，使得该平台能够无缝运行。最重要的是，它为潜在的贡献者提供了学习机会，并应作为团队在其机器学习任务中构建的良好基础。
- en: Conclusion
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Well, that is it people! Congratulations and well done if you are able to make
    it here. I hope you have benefited from this post. Your comments and feedback
    are most welcome and please lets connect on [Linkedln](https://www.linkedin.com/in/koakande/).
    If you found this to be valuable, then don’t forget to like the post and give
    the [MLOps platform](https://github.com/kbakande/MLOPS-Platform) repository a
    star.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，就是这些！如果你能看到这里，恭喜你，做得很好。我希望你能从这篇文章中获益。欢迎留下评论和反馈，也请与我在[LinkedIn](https://www.linkedin.com/in/koakande/)上联系。如果你觉得这篇文章有价值，不要忘了点赞并为[MLOps平台](https://github.com/kbakande/MLOPS-Platform)仓库加个星。
- en: '**References**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'MLOps repo: [https://github.com/kbakande/mlops-platform](https://github.com/kbakande/mlops-platform)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'MLOps仓库: [https://github.com/kbakande/mlops-platform](https://github.com/kbakande/mlops-platform)'
- en: '[https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058](https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058](https://medium.com/google-cloud/machine-learning-pipeline-development-on-google-cloud-5cba36819058)'
- en: '[https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76](https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76](https://medium.com/@piyushpandey282/model-serving-at-scale-with-vertex-ai-custom-container-deployment-with-pre-and-post-processing-12ac62f4ce76)'
- en: '[https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290](https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290](https://medium.com/mlearning-ai/serverless-prediction-at-scale-part-2-custom-container-deployment-on-vertex-ai-103a43d0a290)'
- en: '[https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/](https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/](https://datatonic.com/insights/vertex-ai-improving-debugging-batch-prediction/)'
- en: '[https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html](https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html](https://econ-project-templates.readthedocs.io/en/v0.5.2/pre-commit.html)'
