<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Benchmarking LLM Inference Backends</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Benchmarking LLM Inference Backends</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17">https://towardsdatascience.com/benchmarking-llm-inference-backends-6c8ae46e72e4?source=collection_archive---------0-----------------------#2024-06-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ce71" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Comparing Llama 3 serving performance on vLLM, LMDeploy, MLC-LLM, TensorRT-LLM, and TGI</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sean Sheng" class="l ep by dd de cx" src="../Images/ae58cf760ce5c482e7a6614995b8b8e1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ZfrE9gS9kN3hpaTuZRbBCA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ssheng?source=post_page---byline--6c8ae46e72e4--------------------------------" rel="noopener follow">Sean Sheng</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c8ae46e72e4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c45e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Choosing the right inference backend for serving large language models (LLMs) is crucial. It not only ensures an optimal user experience with fast generation speed but also improves cost efficiency through a high token generation rate and resource utilization. Today, developers have a variety of choices for inference backends created by reputable research and industry teams. However, selecting the best backend for a specific use case can be challenging.</p><p id="9415" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To help developers make informed decisions, the <a class="af nf" href="https://bentoml.com/" rel="noopener ugc nofollow" target="_blank">BentoML</a> engineering team conducted a comprehensive benchmark study on the Llama 3 serving performance with <a class="af nf" href="https://github.com/vllm-project/vllm" rel="noopener ugc nofollow" target="_blank">vLLM</a>, <a class="af nf" href="https://github.com/InternLM/lmdeploy" rel="noopener ugc nofollow" target="_blank">LMDeploy</a>, <a class="af nf" href="https://github.com/mlc-ai/mlc-llm" rel="noopener ugc nofollow" target="_blank">MLC-LLM</a>, <a class="af nf" href="https://github.com/NVIDIA/TensorRT-LLM" rel="noopener ugc nofollow" target="_blank">TensorRT-LLM</a>, and <a class="af nf" href="https://github.com/huggingface/text-generation-inference" rel="noopener ugc nofollow" target="_blank">Hugging Face TGI</a> on <a class="af nf" href="https://cloud.bentoml.com/" rel="noopener ugc nofollow" target="_blank">BentoCloud</a>. These inference backends were evaluated using two key metrics:</p><ul class=""><li id="6958" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">Time to First Token (TTFT)</strong>: Measures the time from when a request is sent to when the first token is generated, recorded in milliseconds. TTFT is important for applications requiring immediate feedback, such as interactive chatbots. Lower latency improves perceived performance and user satisfaction.</li><li id="a00a" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">Token Generation Rate</strong>: Assesses how many tokens the model generates per second during decoding, measured in tokens per second. The token generation rate is an indicator of the model’s capacity to handle high loads. A higher rate suggests that the model can efficiently manage multiple requests and generate responses quickly, making it suitable for high-concurrency environments.</li></ul><h1 id="0bfb" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Key benchmark findings</h1><p id="e9fb" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">We conducted the benchmark study with the Llama 3 8B and 70B 4-bit quantization models on an A100 80GB GPU instance (<code class="cx op oq or os b">gpu.a100.1x80</code>) on BentoCloud across three levels of inference loads (10, 50, and 100 concurrent users). Here are some of our key findings:</p><h2 id="8253" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Llama 3 8B</h2><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl pm"><img src="../Images/52a24358b3da84dea6157b08563068d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TYgKrAVKRn05QsUF.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Llama 3 8B: Time to First Token (TTFT) of Different Backends</figcaption></figure><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl qd"><img src="../Images/7505a27c1f9720916db37986a19d329b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJ5fp3OsIR1PJkBi9jhUcA.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Llama 3 8B: Token Generation Rate of Different Backends</figcaption></figure><ul class=""><li id="13e4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy</strong>: Delivered the best decoding performance in terms of token generation rate, with up to 4000 tokens per second for 100 users. Achieved best-in-class TTFT with 10 users. Although TTFT gradually increases with more users, it remains low and consistently ranks among the best.</li><li id="5047" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">MLC-LLM:</strong> Delivered similar decoding performance to LMDeploy with 10 users. Achieved best-in-class TTFT with 10 and 50 users. However, it struggles to maintain that efficiency under very high loads. When concurrency increases to 100 users, the decoding speed and TFTT does not keep up with LMDeploy.</li><li id="f0ce" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM:</strong> Achieved best-in-class TTFT across all levels of concurrent users. But decoding performance is less optimal compared to LMDeploy and MLC-LLM, with 2300–2500 tokens per second similar to TGI and TRT-LLM.</li></ul><h2 id="cd5c" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Llama 3 70B with 4-bit quantization</h2><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl pm"><img src="../Images/efb3a288fc02e5f49173f29b47439a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vCTwvuGWRpNjAdVk.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Llama 3 70B Q4: Time to First Token (TTFT) of Different Backends</figcaption></figure><figure class="pn po pp pq pr ps pk pl paragraph-image"><div role="button" tabindex="0" class="pt pu ed pv bh pw"><div class="pk pl qd"><img src="../Images/7ec6d6cce672cf861f2a9c728f7d3604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dkuHDCjhJFx8xSGfkgDlw.png"/></div></div><figcaption class="py pz qa pk pl qb qc bf b bg z dx">Llama 3 70B Q4: Token Generate Rate for Different Backends</figcaption></figure><ul class=""><li id="390f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy:</strong> Delivered the best token generation rate with up to 700 tokens when serving 100 users while keeping the lowest TTFT across all levels of concurrent users.</li><li id="0c1b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TensorRT-LLM:</strong> Exhibited similar performance to LMDeploy in terms of token generation rate and maintained low TTFT at a low concurrent user count. However, TTFT increased significantly to over 6 seconds when concurrent users reach 100.</li><li id="854e" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM:</strong> Demonstrated consistently low TTFT across all levels of concurrent users, similar to what we observed with the 8B model. Exhibited a lower token generation rate compared to LMDeploy and TensorRT-LLM, likely due to a lack of inference optimization for quantized models.</li></ul><p id="0485" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We discovered that the token generation rate is strongly correlated with the GPU utilization achieved by an inference backend. Backends capable of maintaining a high token generation rate also exhibited GPU utilization rates approaching 100%. Conversely, backends with lower GPU utilization rates appeared to be bottlenecked by the Python process.</p><h1 id="3d0a" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Beyond performance</h1><p id="f0a0" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">When choosing an inference backend for serving LLMs, considerations beyond just performance also play an important role in the decision. The following list highlights key dimensions that we believe are important to consider when selecting the ideal inference backend.</p><h2 id="c6c8" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Quantization</h2><p id="9824" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Quantization trades off precision for performance by representing weights with lower-bit integers. This technique, combined with optimizations from inference backends, enables faster inference and a smaller memory footprint. As a result, we were able to load the weights of the 70B parameter Llama 3 model on a single A100 80GB GPU, whereas multiple GPUs would otherwise be necessary.</p><ul class=""><li id="3ef4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy</strong>: Supports 4-bit AWQ, 8-bit quantization, and 4-bit KV quantization.</li><li id="97d1" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM</strong>: Not fully supported as of now. Users need to quantize the model through AutoAWQ or find pre-quantized models on Hugging Face. Performance is under-optimized.</li><li id="987d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TensorRT-LLM</strong>: <a class="af nf" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/quantization/README.md#ptq-post-training-quantization" rel="noopener ugc nofollow" target="_blank">Supports quantization via modelopt</a>, and note that quantized data types are not implemented for all the models.</li><li id="9bbe" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TGI</strong>: Supports AWQ, GPTQ and bits-and-bytes quantization</li><li id="23a4" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">MLC-LLM</strong>: Supports 3-bit and 4-bit group quantization. AWQ quantization support is still experimental.</li></ul><h2 id="a3e6" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Model architectures</h2><p id="4b7b" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Being able to leverage the same inference backend for different model architectures offers agility for engineering teams. It allows them to switch between various large language models as new improvements emerge, without needing to migrate the underlying inference infrastructure.</p><ul class=""><li id="9c9d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy</strong>: <a class="af nf" href="https://github.com/InternLM/lmdeploy/blob/main/docs/en/supported_models/supported_models.md" rel="noopener ugc nofollow" target="_blank">About 20 models supported</a> by TurboMind engine. Models that require sliding window attention, e.g. Mistral, Qwen 1.5, are not fully supported as of now.</li><li id="1333" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM</strong>: <a class="af nf" href="https://docs.vllm.ai/en/latest/models/supported_models.html" rel="noopener ugc nofollow" target="_blank">30+ models supported</a></li><li id="b8b8" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TensorRT-LLM</strong>: <a class="af nf" href="https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html" rel="noopener ugc nofollow" target="_blank">30+ models supported</a></li><li id="8128" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TGI</strong>: <a class="af nf" href="https://huggingface.co/docs/text-generation-inference/en/supported_models" rel="noopener ugc nofollow" target="_blank">20+ models supported</a></li><li id="e1f2" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">MLC-LLM</strong>: <a class="af nf" href="https://github.com/mlc-ai/mlc-llm/tree/main/python/mlc_llm/model" rel="noopener ugc nofollow" target="_blank">20+ models supported</a></li></ul><h2 id="95f4" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Hardware limitations</h2><p id="06a9" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Having the ability to run on different hardware provides cost savings and the flexibility to select the appropriate hardware based on inference requirements. It also offers alternatives during the current GPU shortage, helping to navigate supply constraints effectively.</p><ul class=""><li id="f7bc" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy</strong>: Only optimized for Nvidia CUDA</li><li id="3835" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM</strong>: Nvidia CUDA, AMD ROCm, AWS Neuron, CPU</li><li id="375d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TensorRT-LLM</strong>: Only supports Nvidia CUDA</li><li id="1bfa" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TGI</strong>: Nvidia CUDA, AMD ROCm, Intel Gaudi, AWS Inferentia</li><li id="7230" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">MLC-LLM</strong>: Nvidia CUDA, AMD ROCm, Metal, Android, IOS, WebGPU</li></ul><h1 id="e3fe" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Developer experience</h1><p id="6d8c" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">An inference backend designed for production environments should provide stable releases and facilitate simple workflows for continuous deployment. Additionally, a developer-friendly backend should feature well-defined interfaces that support rapid development and high code maintainability, essential for building AI applications powered by LLMs.</p><ul class=""><li id="1e59" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">Stable releases</strong>: LMDeploy, TensorRT-LLM, vLLM, and TGI all offer stable releases. MLC-LLM does not currently have stable tagged releases, with only nightly builds; one possible solution is to build from source.</li><li id="4c3c" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">Model compilation</strong>: TensorRT-LLM and MLC-LLM require an explicit model compilation step before the inference backend is ready. This step could potentially introduce additional cold-start delays during deployment.</li><li id="6def" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">Documentation</strong>: LMDeploy, vLLM, and TGI were all easy to learn with their comprehensive documentation and examples. MLC-LLM presented a moderate learning curve, primarily due to the necessity of understanding the model compilation steps. TensorRT-LLM was the most challenging to set up in our benchmark test. Without enough quality examples, we had to read through the documentation of TensorRT-LLM, <em class="qe">tensorrtllm_backend</em> and Triton Inference Server, convert the checkpoints, build the TRT engine, and write a lot of configurations.</li></ul><h1 id="de26" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Concepts</h1><h2 id="19a9" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Llama 3</h2><p id="a304" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><a class="af nf" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">Llama 3</a> is the latest iteration in the Llama LLM series, available in various configurations. We used the following model sizes in our benchmark tests.</p><ul class=""><li id="d4c1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><strong class="ml fr">8B</strong>: This model has 8 billion parameters, making it powerful yet manageable in terms of computational resources. Using FP16, it requires about 16GB of RAM (excluding KV cache and other overheads), fitting on a single A100–80G GPU instance.</li><li id="9961" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">70B 4-bit Quantization</strong>: This 70 billion parameter model, when quantized to 4 bits, significantly reduces its memory footprint. Quantization compresses the model by reducing the bits per parameter, providing faster inference and lowering memory usage with minimal performance loss. With 4-bit AWQ quantization, it requires approximately 37GB of RAM for loading model weights, fitting on a single A100–80G instance. Serving quantized weights on a single GPU device typically achieves the best throughput of a model compared to serving on multiple devices.</li></ul><h2 id="6a70" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Inference platform</h2><p id="a8ca" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">We ensured that the inference backends served with <a class="af nf" href="https://github.com/bentoml/BentoML" rel="noopener ugc nofollow" target="_blank">BentoML</a> added only minimal performance overhead compared to serving natively in Python. The overhead is due to the provision of functionality for scaling, observability, and IO serialization. Using BentoML and <a class="af nf" href="https://bentoml.com/" rel="noopener ugc nofollow" target="_blank">BentoCloud</a> gave us a consistent RESTful API for the different inference backends, simplifying benchmark setup and operations.</p><h2 id="2145" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Inference backends</h2><p id="8284" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Different backends provide various ways to serve LLMs, each with unique features and optimization techniques. All of the inference backends we tested are under Apache 2.0 License.</p><ul class=""><li id="f889" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk"><a class="af nf" href="https://github.com/InternLM/lmdeploy" rel="noopener ugc nofollow" target="_blank">LMDeploy</a>: An inference backend focusing on delivering high decoding speed and efficient handling of concurrent requests. It supports various quantization techniques, making it suitable for deploying large models with reduced memory requirements.</li><li id="7823" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><a class="af nf" href="https://github.com/vllm-project/vllm" rel="noopener ugc nofollow" target="_blank">vLLM</a>: A high-performance inference engine optimized for serving LLMs. It is known for its efficient use of GPU resources and fast decoding capabilities.</li><li id="26e0" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><a class="af nf" href="https://github.com/NVIDIA/TensorRT-LLM" rel="noopener ugc nofollow" target="_blank">TensorRT-LLM</a>: An inference backend that leverages NVIDIA’s TensorRT, a high-performance deep learning inference library. It is optimized for running large models on NVIDIA GPUs, providing fast inference and support for advanced optimizations like quantization.</li><li id="f4c8" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><a class="af nf" href="https://github.com/huggingface/text-generation-inference" rel="noopener ugc nofollow" target="_blank">Hugging Face Text Generation Inference (TGI)</a>: A toolkit for deploying and serving LLMs. It is used in production at Hugging Face to power Hugging Chat, the Inference API and Inference Endpoint.</li><li id="bc4c" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><a class="af nf" href="https://github.com/mlc-ai/mlc-llm" rel="noopener ugc nofollow" target="_blank">MLC-LLM</a>: An ML compiler and high-performance deployment engine for LLMs. It is built on top of Apache TVM and requires compilation and weight conversion before serving models.</li></ul><p id="02d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Integrating BentoML with various inference backends to self-host LLMs is straightforward. The BentoML community provides the following example projects on GitHub to guide you through the process.</p><ul class=""><li id="3218" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk">vLLM: <a class="af nf" href="https://github.com/bentoml/BentoVLLM" rel="noopener ugc nofollow" target="_blank">https://github.com/bentoml/BentoVLLM</a></li><li id="9c0d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">MLC-LLM: <a class="af nf" href="https://github.com/bentoml/BentoMLCLLM" rel="noopener ugc nofollow" target="_blank">https://github.com/bentoml/BentoMLCLLM</a></li><li id="743b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">LMDeploy: <a class="af nf" href="https://github.com/bentoml/BentoLMDeploy" rel="noopener ugc nofollow" target="_blank">https://github.com/bentoml/BentoLMDeploy</a></li><li id="650e" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">TRT-LLM: <a class="af nf" href="https://github.com/bentoml/BentoTRTLLM" rel="noopener ugc nofollow" target="_blank">https://github.com/bentoml/BentoTRTLLM</a></li><li id="986f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">TGI: <a class="af nf" href="https://github.com/bentoml/BentoTGI" rel="noopener ugc nofollow" target="_blank">https://github.com/bentoml/BentoTGI</a></li></ul><h1 id="567e" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Benchmark setup</h1><h2 id="100f" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Models</h2><p id="a763" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">We tested both the Meta-Llama-3–8B-Instruct and Meta-Llama-3–70B-Instruct 4-bit quantization models. For the 70B model, we performed 4-bit quantization so that it could run on a single A100–80G GPU. If the inference backend supports native quantization, we used the inference backend-provided quantization method. For example, for MLC-LLM, we used the <code class="cx op oq or os b">q4f16_1</code> quantization scheme. Otherwise, we used the AWQ-quantized <code class="cx op oq or os b">casperhansen/llama-3-70b-instruct-awq</code> model from Hugging Face.</p><p id="0d4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that other than enabling common inference optimization techniques, such as continuous batching, flash attention, and prefix caching, we did not fine-tune the inference configurations (GPU memory utilization, max number of sequences, paged KV cache block size, etc.) for each individual backend. This is because this approach is not scalable as the number of LLMs we serve gets larger. Providing an optimal set of inference parameters is an implicit measure of performance and ease-of-use of the backends.</p><h2 id="0f98" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Benchmark client</h2><p id="1f89" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">To accurately assess the performance of different LLM backends, we created a custom benchmark script. This script simulates real-world scenarios by varying user loads and sending generation requests under different levels of concurrency.</p><p id="7157" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our benchmark client can spawn up to the target number of users within 20 seconds, after which it stress tests the LLM backend by sending concurrent generation requests with randomly selected prompts. We tested with 10, 50, and 100 concurrent users to evaluate the system under varying loads.</p><p id="284a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each stress test ran for 5 minutes, during which time we collected inference metrics every 5 seconds. This duration was sufficient to observe potential performance degradation, resource utilization bottlenecks, or other issues that might not be evident in shorter tests.</p><p id="6e58" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For more information, see <a class="af nf" href="https://github.com/bentoml/llm-bench" rel="noopener ugc nofollow" target="_blank">the source code of our benchmark client</a>.</p><h2 id="666a" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Prompt dataset</h2><p id="e9a1" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The prompts for our tests were derived from the <a class="af nf" href="https://github.com/bentoml/openllm-bench/blob/main/common.py#L294" rel="noopener ugc nofollow" target="_blank">databricks-dolly-15k dataset</a>. For each test session, we randomly selected prompts from this dataset. We also tested text generation with and without system prompts. Some backends might have additional optimizations regarding common system prompt scenarios by enabling prefix caching.</p><h2 id="849d" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Library versions</h2><ul class=""><li id="1f81" class="mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne ng nh ni bk"><strong class="ml fr">BentoML</strong>: 1.2.16</li><li id="9598" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">vLLM</strong>: 0.4.2</li><li id="bd5f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">MLC-LLM</strong>: mlc-llm-nightly-cu121 0.1.dev1251 (No stable release yet)</li><li id="52ca" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">LMDeploy</strong>: 0.4.0</li><li id="a6bd" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TensorRT-LLM</strong>: 0.9.0 (with Triton v24.04)</li><li id="8b8b" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk"><strong class="ml fr">TGI</strong>: 2.0.4</li></ul><h2 id="4e0c" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Recommendations</h2><p id="5784" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The field of LLM inference optimization is rapidly evolving and heavily researched. The best inference backend available today might quickly be surpassed by newcomers. Based on our benchmarks and usability studies conducted at the time of writing, we have the following recommendations for selecting the most suitable backend for Llama 3 models under various scenarios.</p><h2 id="c5fe" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Llama 3 8B</h2><p id="be29" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">For the Llama 3 8B model, <strong class="ml fr">LMDeploy</strong> consistently delivers low TTFT and the highest decoding speed across all user loads. Its ease of use is another significant advantage, as it can convert the model into TurboMind engine format on the fly, simplifying the deployment process. At the time of writing, LMDeploy offers limited support for models that utilize sliding window attention mechanisms, such as Mistral and Qwen 1.5.</p><p id="ba71" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">vLLM</strong> consistently maintains a low TTFT, even as user loads increase, making it suitable for scenarios where maintaining low latency is crucial. vLLM offers easy integration, extensive model support, and broad hardware compatibility, all backed by a robust open-source community.</p><p id="9569" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">MLC-LLM</strong> offers the lowest TTFT and maintains high decoding speeds at lower concurrent users. However, under very high user loads, MLC-LLM struggles to maintain top-tier decoding performance. Despite these challenges, MLC-LLM shows significant potential with its machine learning compilation technology. Addressing these performance issues and implementing a stable release could greatly enhance its effectiveness.</p><h2 id="077d" class="ot np fq bf nq ou ov ow nt ox oy oz nw ms pa pb pc mw pd pe pf na pg ph pi pj bk">Llama 3 70B 4-bit quantization</h2><p id="3e4b" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">For the Llama 3 70B Q4 model, <strong class="ml fr">LMDeploy</strong> demonstrates impressive performance with the lowest TTFT across all user loads. It also maintains a high decoding speed, making it ideal for applications where both low latency and high throughput are essential. LMDeploy also stands out for its ease of use, as it can quickly convert models without the need for extensive setup or compilation, making it ideal for rapid deployment scenarios.</p><p id="c58f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">TensorRT-LLM</strong> matches LMDeploy in throughput, yet it exhibits less optimal latency for TTFT under high user load scenarios. Backed by Nvidia, we anticipate these gaps will be quickly addressed. However, its inherent requirement for model compilation and reliance on Nvidia CUDA GPUs are intentional design choices that may pose limitations during deployment.</p><p id="7b5d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">vLLM</strong> manages to maintain a low TTFT even as user loads increase, and its ease of use can be a significant advantage for many users. However, at the time of writing, the backend’s lack of optimization for AWQ quantization leads to less than optimal decoding performance for quantized models.</p><h1 id="2c1b" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Acknowledgements</h1><p id="10d5" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">The article and accompanying benchmarks were collaboratively with my esteemed colleagues, Rick Zhou, Larme Zhao, and Bo Jiang. All images presented in this article were created by the authors.</p></div></div></div></div>    
</body>
</html>