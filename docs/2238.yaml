- en: 'Hands-On Imitation Learning: From Behavior Cloning to Multi-Modal Imitation
    Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µæ¨¡ä»¿å­¦ä¹ ï¼šä»è¡Œä¸ºå…‹éš†åˆ°å¤šæ¨¡æ€æ¨¡ä»¿å­¦ä¹ 
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12](https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12](https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12)
- en: '***An overview of the most prominent methods in imitation learning while testing
    on a grid environment***'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '***å¯¹æ¨¡ä»¿å­¦ä¹ ä¸­æœ€çªå‡ºçš„å‡ ç§æ–¹æ³•è¿›è¡Œæ¦‚è¿°ï¼Œå¹¶åœ¨ç½‘æ ¼ç¯å¢ƒä¸­è¿›è¡Œæµ‹è¯•***'
- en: '[](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Yasin
    Yousif](../Images/9c702c021e7e5285bddefe76a144a3e1.png)](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    [Yasin Yousif](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Yasin
    Yousif](../Images/9c702c021e7e5285bddefe76a144a3e1.png)](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    [Yasin Yousif](https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    Â·14 min readÂ·Sep 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------)
    Â·14åˆ†é’Ÿé˜…è¯»Â·2024å¹´9æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/120c495851c2f0539761925dddc91cbf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/120c495851c2f0539761925dddc91cbf.png)'
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Reinforcement learning is one branch of machine learning concerned with learning
    by guidance of scalar signals (rewards); in contrast to supervised learning, which
    needs full labels of the target variable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå…³æ³¨é€šè¿‡æ ‡é‡ä¿¡å·ï¼ˆå¥–åŠ±ï¼‰çš„æŒ‡å¯¼è¿›è¡Œå­¦ä¹ ï¼›ä¸éœ€è¦ç›®æ ‡å˜é‡å®Œæ•´æ ‡ç­¾çš„ç›‘ç£å­¦ä¹ ä¸åŒã€‚
- en: 'An intuitive example to explain reinforcement learning can be given in terms
    of a school with two classes having two types of tests repeated continuously.
    The first class solves the test and gets the full correct answers (supervised
    learning: SL). The second class solves the test and gets only the grades for each
    question (reinforcement learning: RL). In the first case, it seems easier for
    the students to learn the correct answers and memorize them. In the second class,
    the task is harder because they can learn only by trial and error. However, their
    learning will be more robust because they donâ€™t only know what is right but also
    all the wrong answers to avoid.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡ä¸€ä¸ªç›´è§‚çš„ä¾‹å­æ¥è§£é‡Šå¼ºåŒ–å­¦ä¹ ï¼Œä¾‹å­æè¿°çš„æ˜¯ä¸€æ‰€å­¦æ ¡çš„ä¸¤é—¨è¯¾ï¼Œæ¯é—¨è¯¾æœ‰ä¸¤ç§ç±»å‹çš„æµ‹è¯•ï¼Œä¸”æµ‹è¯•ä¸æ–­é‡å¤è¿›è¡Œã€‚ç¬¬ä¸€ç­è§£ç­”æµ‹è¯•å¹¶å¾—åˆ°äº†æ‰€æœ‰æ­£ç¡®ç­”æ¡ˆï¼ˆç›‘ç£å­¦ä¹ ï¼šSLï¼‰ã€‚ç¬¬äºŒç­è§£ç­”æµ‹è¯•å¹¶ä¸”æ¯ä¸ªé—®é¢˜åªå¾—åˆ°äº†åˆ†æ•°ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼šRLï¼‰ã€‚åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œå­¦ç”Ÿä»¬ä¼¼ä¹æ›´å®¹æ˜“å­¦ä¹ æ­£ç¡®ç­”æ¡ˆå¹¶è®°ä½å®ƒä»¬ã€‚åœ¨ç¬¬äºŒç­ä¸­ï¼Œä»»åŠ¡æ›´å›°éš¾ï¼Œå› ä¸ºä»–ä»¬åªèƒ½é€šè¿‡åå¤è¯•éªŒæ¥å­¦ä¹ ã€‚ç„¶è€Œï¼Œä»–ä»¬çš„å­¦ä¹ å°†æ›´åŠ ç¨³å¥ï¼Œå› ä¸ºä»–ä»¬ä¸ä»…çŸ¥é“ä»€ä¹ˆæ˜¯å¯¹çš„ï¼Œè¿˜çŸ¥é“æ‰€æœ‰é”™è¯¯çš„ç­”æ¡ˆï¼Œä»è€Œé¿å…å®ƒä»¬ã€‚
- en: In order to learn efficiently with RL, an accurate reward signal (the grades)
    should be designed, which is considered a difficult task, especially for real-world
    applications. For example, a human expert driver knows how to drive, but cannot
    set rewards for â€˜correct drivingâ€™ skill, same thing for cooking or painting. This
    created the need for imitation learning methods (IL). IL is a new branch of RL
    concerned with learning from mere expert trajectories, without knowing the rewards.
    Main application areas of IL are in robotics and autonomous driving fields.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­é«˜æ•ˆå­¦ä¹ ï¼Œåº”è¯¥è®¾è®¡ä¸€ä¸ªå‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼ˆè¯„åˆ†ï¼‰ï¼Œè¿™è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå›°éš¾çš„ä»»åŠ¡ï¼Œå°¤å…¶æ˜¯åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªäººç±»ä¸“å®¶é©¾é©¶å‘˜çŸ¥é“å¦‚ä½•å¼€è½¦ï¼Œä½†æ— æ³•ä¸ºâ€œæ­£ç¡®é©¾é©¶â€æŠ€èƒ½è®¾ç½®å¥–åŠ±ï¼Œåšé¥­æˆ–ç»˜ç”»ä¹Ÿæ˜¯å¦‚æ­¤ã€‚è¿™å°±äº§ç”Ÿäº†å¯¹æ¨¡ä»¿å­¦ä¹ æ–¹æ³•ï¼ˆILï¼‰çš„éœ€æ±‚ã€‚ILæ˜¯å¼ºåŒ–å­¦ä¹ çš„ä¸€ä¸ªæ–°åˆ†æ”¯ï¼Œä¸“æ³¨äºä»çº¯ç²¹çš„ä¸“å®¶è½¨è¿¹ä¸­å­¦ä¹ ï¼Œè€Œä¸éœ€è¦çŸ¥é“å¥–åŠ±ã€‚ILçš„ä¸»è¦åº”ç”¨é¢†åŸŸæ˜¯åœ¨æœºå™¨äººæŠ€æœ¯å’Œè‡ªåŠ¨é©¾é©¶é¢†åŸŸã€‚
- en: In the following, we will explore the most famous methods of IL in the literature,
    ordered by their proposal time from old to new, as shown in the timeline picture
    below.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ–‡çŒ®ä¸­æœ€è‘—åçš„ILæ–¹æ³•ï¼ŒæŒ‰æè®®æ—¶é—´ä»æ—§åˆ°æ–°æ’åºï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºçš„æ—¶é—´çº¿ã€‚
- en: '![](../Images/052ad6e337c649a08caa6e76fb3ecd9a.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/052ad6e337c649a08caa6e76fb3ecd9a.png)'
- en: Timeline of IL methods
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ILæ–¹æ³•çš„æ—¶é—´çº¿
- en: The mathematical formulations will be shown along with nomenclature of the symbols.
    However, the theoretical derivation is kept to a minimum here; if further depth
    is needed, the original references can be looked up as cited in the references
    section at the end. The full code for recreating all the experiments is provided
    in the accompanying [github repo](https://www.github.com/engyasin/ilsurvey).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°å­¦å…¬å¼å°†ä¸ç¬¦å·çš„å‘½åä¸€åŒå±•ç¤ºã€‚ç„¶è€Œï¼Œç†è®ºæ¨å¯¼åœ¨è¿™é‡Œä¿æŒåˆ°æœ€å°ï¼›å¦‚æœéœ€è¦è¿›ä¸€æ­¥çš„æ·±åº¦ï¼Œå¯ä»¥æŸ¥é˜…å¼•ç”¨éƒ¨åˆ†åˆ—å‡ºçš„åŸå§‹æ–‡çŒ®ã€‚é‡ç°æ‰€æœ‰å®éªŒçš„å®Œæ•´ä»£ç å·²æä¾›åœ¨éšé™„çš„[github
    ä»“åº“](https://www.github.com/engyasin/ilsurvey)ä¸­ã€‚
- en: So, buckle up! and letâ€™s dive through imitation learning, from behavior cloning
    (BC) to information maximization generative adversarial imitation learning (InfoGAIL).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œç³»å¥½å®‰å…¨å¸¦ï¼è®©æˆ‘ä»¬é€šè¿‡æ¨¡ä»¿å­¦ä¹ æ·±å…¥æ¢ç´¢ï¼Œä»è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰åˆ°ä¿¡æ¯æœ€å¤§åŒ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆInfoGAILï¼‰ã€‚
- en: Example Environment
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ç¯å¢ƒ
- en: 'The environment used in this post is represented as a 15x15 grid. The environment
    state is illustrated below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä½¿ç”¨çš„ç¯å¢ƒè¡¨ç¤ºä¸ºä¸€ä¸ª15x15çš„ç½‘æ ¼ã€‚ç¯å¢ƒçŠ¶æ€å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: 'Agent: red color'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†ï¼šçº¢è‰²
- en: 'Initial agent location: blue color'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆå§‹ä»£ç†ä½ç½®ï¼šè“è‰²
- en: 'Walls: green color'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¢™å£ï¼šç»¿è‰²
- en: '![](../Images/417a0103d83622ac19add6a8c7479bf8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/417a0103d83622ac19add6a8c7479bf8.png)'
- en: The goal of the agent is to reach the first row in the shortest possible way
    through any of the three windows and towards a symmetrical location to its initial
    position with respect to the vertical axis passing through the middle of the grid.
    The goal location will not be shown in the state grid.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç†çš„ç›®æ ‡æ˜¯é€šè¿‡ä»»ä½•ä¸‰ä¸ªçª—å£ï¼Œä»¥æœ€çŸ­çš„æ–¹å¼åˆ°è¾¾ç¬¬ä¸€è¡Œï¼Œå¹¶ä½¿å…¶ä½äºç›¸å¯¹äºé€šè¿‡ç½‘æ ¼ä¸­å¿ƒçš„å‚ç›´è½´çš„åˆå§‹ä½ç½®çš„å¯¹ç§°ä½ç½®ã€‚ç›®æ ‡ä½ç½®ä¸ä¼šåœ¨çŠ¶æ€ç½‘æ ¼ä¸­æ˜¾ç¤ºã€‚
- en: So the initial position has 15 possibilities only, and the goal location is
    changed based on that.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåˆå§‹ä½ç½®åªæœ‰15ç§å¯èƒ½æ€§ï¼Œç›®æ ‡ä½ç½®ä¼šåŸºäºæ­¤å‘ç”Ÿå˜åŒ–ã€‚
- en: Action Space
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´
- en: 'The action space *A* consists of a discrete number from 0 to 4 representing
    movements in four directions and the stopping action, as illustrated below:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨ä½œç©ºé—´ *A* åŒ…å«ä»0åˆ°4çš„ç¦»æ•£æ•°å€¼ï¼Œè¡¨ç¤ºå››ä¸ªæ–¹å‘çš„ç§»åŠ¨å’Œåœæ­¢åŠ¨ä½œï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/7a40b44e70067c461e30172c23fe1768.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a40b44e70067c461e30172c23fe1768.png)'
- en: Reward Function
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¥–åŠ±å‡½æ•°
- en: 'The ground truth reward here *R*(*s*,*a*) is a function of the current state
    and action, with a value equal to the displacement distance towards the goal:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„çœŸå®å¥–åŠ± *R*(*s*,*a*) æ˜¯å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œçš„å‡½æ•°ï¼Œå…¶å€¼ç­‰äºæœç›®æ ‡ç§»åŠ¨çš„ä½ç§»è·ç¦»ï¼š
- en: where ğ‘1â€‹ is the old position and *p*2â€‹ is the new position. The agent will
    always be initialized at the last row, but in a random position each time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œğ‘1â€‹æ˜¯æ—§ä½ç½®ï¼Œ*p*2â€‹æ˜¯æ–°ä½ç½®ã€‚ä»£ç†æ€»æ˜¯ä»æœ€åä¸€è¡Œåˆå§‹åŒ–ï¼Œä½†æ¯æ¬¡ä½ç½®éƒ½æ˜¯éšæœºçš„ã€‚
- en: Expert Policy
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸“å®¶ç­–ç•¥
- en: 'The expert policy used for all methods (except InfoGAIL) aims to reach the
    goal in the shortest possible path. This involves three steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ–¹æ³•ï¼ˆé™¤InfoGAILå¤–ï¼‰ä½¿ç”¨çš„ä¸“å®¶ç­–ç•¥æ—¨åœ¨ä»¥æœ€çŸ­çš„è·¯å¾„åˆ°è¾¾ç›®æ ‡ã€‚è¿™æ¶‰åŠä¸‰ä¸ªæ­¥éª¤ï¼š
- en: Moving towards the nearest window
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœæœ€è¿‘çš„çª—å£ç§»åŠ¨
- en: Moving directly towards the goal
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›´æ¥æœç›®æ ‡ç§»åŠ¨
- en: Stopping at the goal location
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœæ­¢åœ¨ç›®æ ‡ä½ç½®
- en: 'This behavior is illustrated by a GIF:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¡Œä¸ºç”±ä¸€ä¸ªGIFæ¼”ç¤ºï¼š
- en: '![](../Images/1629324006841fb80098705a78561bd6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1629324006841fb80098705a78561bd6.png)'
- en: The expert policy generates demonstration trajectories used by other IL methods.
    Each trajectory *Ï„* is represented as an ordered sequence of state-action tuples.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“å®¶ç­–ç•¥ç”Ÿæˆç”¨äºå…¶ä»–å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„æ¼”ç¤ºè½¨è¿¹ã€‚æ¯æ¡è½¨è¿¹*Ï„*è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æœ‰åºçš„çŠ¶æ€-åŠ¨ä½œå¯¹ã€‚
- en: where the expert demonstrations set is defined as D={*Ï„*0â€‹,â‹¯,*Ï„n*â€‹}
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“å®¶æ¼”ç¤ºé›†å®šä¹‰ä¸ºD={*Ï„*0â€‹,â‹¯,*Ï„n*â€‹}
- en: '*The expert episodic return was 16.33Â±6 on average for 30 episodes with a length
    of 32 steps each.*'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ä¸“å®¶çš„æ¯é›†å›æŠ¥å¹³å‡ä¸º16.33Â±6ï¼Œå…±30é›†ï¼Œæ¯é›†é•¿åº¦ä¸º32æ­¥ã€‚*'
- en: Forward Reinforcement Learning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‰å‘å¼ºåŒ–å­¦ä¹ 
- en: First, we will train a model using the ground truth reward to set some baselines
    and tune hyperparameters for later use with IL methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çœŸå®å¥–åŠ±è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œè®¾å®šä¸€äº›åŸºå‡†å¹¶è°ƒä¼˜è¶…å‚æ•°ï¼Œä»¥ä¾¿åç»­ä¸ILæ–¹æ³•ä¸€èµ·ä½¿ç”¨ã€‚
- en: The implementation of the Forward RL algorithm used in this post is based on
    Clean RL scripts [12], which provides a readable implementation of RL methods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­ä½¿ç”¨çš„å‰å‘å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„å®ç°åŸºäºClean RLè„šæœ¬[12]ï¼Œè¯¥è„šæœ¬æä¾›äº†å¯è¯»çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•å®ç°ã€‚
- en: Introduction
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: We will test both Proximal Policy Optimization (PPO) [2] and Deep Q-Network
    (DQN) [1], state-of-the-art on-policy and well-known off-policy RL methods, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åˆ†åˆ«æµ‹è¯•æœ€å…ˆè¿›çš„ç­–ç•¥å†…æ–¹æ³•â€”â€”è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆPPOï¼‰[2]å’Œè‘—åçš„ç­–ç•¥å¤–æ–¹æ³•â€”â€”æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰[1]ã€‚
- en: 'The following is a summary of the training steps for each method, along with
    their characteristics:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ¯ç§æ–¹æ³•çš„è®­ç»ƒæ­¥éª¤æ€»ç»“ä»¥åŠå®ƒä»¬çš„ç‰¹ç‚¹ï¼š
- en: On-Policy (PPO)
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥å†…ï¼ˆPPOï¼‰
- en: 'This method uses the current policy under training and updates its parameters
    after collecting rollouts for every episode. PPO has two main parts: critic and
    actor. The actor represents the policy, while the critic provides value estimations
    for each state with its own updated objective.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•ä½¿ç”¨å½“å‰è®­ç»ƒä¸­çš„ç­–ç•¥ï¼Œå¹¶åœ¨æ¯æ¬¡æ”¶é›†å›åˆåæ›´æ–°å…¶å‚æ•°ã€‚PPOåŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼šè¯„è®ºå‘˜å’Œæ¼”å‘˜ã€‚æ¼”å‘˜è¡¨ç¤ºç­–ç•¥ï¼Œè€Œè¯„è®ºå‘˜ä¸ºæ¯ä¸ªçŠ¶æ€æä¾›ä»·å€¼ä¼°è®¡ï¼Œå¹¶å…·æœ‰è‡ªå·±çš„æ›´æ–°ç›®æ ‡ã€‚
- en: Off-Policy (DQN)
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç­–ç•¥å¤–ï¼ˆDQNï¼‰
- en: DQN trains its policy offline by collecting rollouts in a replay buffer using
    epsilon-greedy exploration. This means that DQN does not take always the best
    action according to the current policy for every state but rather selects a random
    action. This enables the exploration of different solutions. An additional target
    network may be used with less frequently updated version of the policy to make
    the learning objective more stable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DQNé€šè¿‡æ”¶é›†å›æ”¾ç¼“å†²åŒºä¸­çš„å›åˆæ¥ç¦»çº¿è®­ç»ƒå…¶ç­–ç•¥ï¼Œé‡‡ç”¨Îµ-è´ªå©ªæ¢ç´¢ç­–ç•¥ã€‚è¿™æ„å‘³ç€DQNåœ¨æ¯ä¸ªçŠ¶æ€ä¸‹å¹¶ä¸æ€»æ˜¯é€‰æ‹©å½“å‰ç­–ç•¥ä¸‹çš„æœ€ä½³åŠ¨ä½œï¼Œè€Œæ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚è¿™æœ‰åŠ©äºæ¢ç´¢ä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚å¯èƒ½è¿˜ä¼šä½¿ç”¨ä¸€ä¸ªç›®æ ‡ç½‘ç»œï¼Œè¯¥ç½‘ç»œä»¥è¾ƒä½çš„é¢‘ç‡æ›´æ–°ç­–ç•¥ç‰ˆæœ¬ï¼Œä»¥ä½¿å­¦ä¹ ç›®æ ‡æ›´åŠ ç¨³å®šã€‚
- en: Results and Discussion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœä¸è®¨è®º
- en: The following figure shows the episodic return curves for both methods. DQN
    is in black, while PPO is shown as an orange line.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾æ˜¾ç¤ºäº†ä¸¤ç§æ–¹æ³•çš„æ¯é›†å›æŠ¥æ›²çº¿ã€‚DQNç”¨é»‘è‰²è¡¨ç¤ºï¼Œè€ŒPPOç”¨æ©™è‰²çº¿æ¡è¡¨ç¤ºã€‚
- en: '![](../Images/3a1db66541085517545021eef57c3ea4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a1db66541085517545021eef57c3ea4.png)'
- en: 'For this simple example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç®€å•çš„ä¾‹å­ï¼š
- en: Both PPO and DQN converge, but with a slight advantage for PPO. Neither method
    reaches the expert level of 16.6 (PPO comes close with 15.26).
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPOå’ŒDQNéƒ½èƒ½æ”¶æ•›ï¼Œä½†PPOç•¥æœ‰ä¼˜åŠ¿ã€‚ä¸¤ç§æ–¹æ³•éƒ½æœªè¾¾åˆ°ä¸“å®¶çº§åˆ«çš„16.6ï¼ˆPPOæ¥è¿‘ï¼Œè¾¾åˆ°15.26ï¼‰ã€‚
- en: DQN seems slower to converge in terms of interaction steps, known as sample
    inefficiency compared to PPO.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸PPOç›¸æ¯”ï¼ŒDQNåœ¨æ”¶æ•›é€Ÿåº¦ä¸Šä¼¼ä¹è¾ƒæ…¢ï¼Œè¡¨ç°ä¸ºä¸äº¤äº’æ­¥éª¤ï¼ˆä¹Ÿç§°ä¸ºæ ·æœ¬æ•ˆç‡ï¼‰ç›¸å…³çš„ä½æ•ˆç‡ã€‚
- en: PPO takes longer training time, possibly due to actor-critic training, updating
    two networks with different objectives.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPOè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå¯èƒ½æ˜¯ç”±äºæ¼”å‘˜-è¯„è®ºå‘˜è®­ç»ƒï¼Œéœ€è¦æ›´æ–°ä¸¤ä¸ªå…·æœ‰ä¸åŒç›®æ ‡çš„ç½‘ç»œã€‚
- en: The parameters for training both methods are mostly the same. For a closer look
    at how these curves were generated, check the scripts `ppo.py` and `dqn.py` in
    the accompanying repository.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿™ä¸¤ç§æ–¹æ³•çš„å‚æ•°å¤§è‡´ç›¸åŒã€‚æ¬²æ·±å…¥äº†è§£è¿™äº›æ›²çº¿æ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Œå¯ä»¥æŸ¥çœ‹éšé™„ä»“åº“ä¸­çš„`ppo.py`å’Œ`dqn.py`è„šæœ¬ã€‚
- en: Behavior Cloning (BC)
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰
- en: 'Behavior Cloning, first proposed in [4], is a direct IL method. It involves
    supervised learning to map each state to an action based on expert demonstrations
    D. The objective is defined as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼ŒBCï¼‰ï¼Œæœ€æ—©åœ¨[4]ä¸­æå‡ºï¼Œæ˜¯ä¸€ç§ç›´æ¥çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚å®ƒé€šè¿‡ç›‘ç£å­¦ä¹ å°†æ¯ä¸ªçŠ¶æ€æ˜ å°„åˆ°ä¸€ä¸ªåŠ¨ä½œï¼ŒåŸºäºä¸“å®¶æ¼”ç¤ºé›†Dã€‚ç›®æ ‡å®šä¹‰ä¸ºï¼š
- en: where *Ï€_bc*â€‹ is the trained policy, *Ï€_E*â€‹ is the expert policy, and *l*(*Ï€_bc*â€‹(*s*),*Ï€_E*â€‹(*s*))
    is the loss function between the expert and trained policy in response to the
    same state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*Ï€_bc*â€‹æ˜¯è®­ç»ƒåçš„ç­–ç•¥ï¼Œ*Ï€_E*â€‹æ˜¯ä¸“å®¶ç­–ç•¥ï¼Œ*l*(*Ï€_bc*â€‹(*s*),*Ï€_E*â€‹(*s*))æ˜¯å“åº”åŒä¸€çŠ¶æ€æ—¶ï¼Œä¸“å®¶ç­–ç•¥å’Œè®­ç»ƒç­–ç•¥ä¹‹é—´çš„æŸå¤±å‡½æ•°ã€‚
- en: The difference between BC and supervised learning lies in defining the problem
    as an interactive environment where actions are taken in response to dynamic states
    (e.g., a robot moving towards a goal). In contrast, supervised learning involves
    mapping input to output, like classifying images or predicting temperature. This
    distinction is explained in [8].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: BCå’Œç›‘ç£å­¦ä¹ çš„åŒºåˆ«åœ¨äºå°†é—®é¢˜å®šä¹‰ä¸ºä¸€ä¸ªäº¤äº’å¼ç¯å¢ƒï¼Œåœ¨è¯¥ç¯å¢ƒä¸­ï¼Œè¡Œä¸ºæ˜¯å¯¹åŠ¨æ€çŠ¶æ€çš„å“åº”ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªæœºå™¨äººæœç€ç›®æ ‡ç§»åŠ¨ï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç›‘ç£å­¦ä¹ æ¶‰åŠå°†è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºï¼Œæ¯”å¦‚å›¾åƒåˆ†ç±»æˆ–æ¸©åº¦é¢„æµ‹ã€‚è¿™ä¸ªåŒºåˆ«åœ¨[8]ä¸­æœ‰è§£é‡Šã€‚
- en: In this implementation, the full set of initial positions for the agent contains
    only 15 possibilities. Consequently, there are only 15 trajectories to learn from,
    which can be memorized by the BC network effectively. To make the problem harder,
    we clip the size of the training dataset D to half (only 240 state-action pairs
    out of 480) and repeat this for all IL methods that follow in this post.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤å®ç°ä¸­ï¼Œæ™ºèƒ½ä½“çš„åˆå§‹ä½ç½®å…¨é›†åªæœ‰15ç§å¯èƒ½æ€§ã€‚å› æ­¤ï¼Œåªæœ‰15æ¡è½¨è¿¹å¯ä»¥å­¦ä¹ ï¼Œè€Œè¿™äº›è½¨è¿¹å¯ä»¥è¢«BCç½‘ç»œæœ‰æ•ˆåœ°è®°ä½ã€‚ä¸ºäº†å¢åŠ é—®é¢˜çš„éš¾åº¦ï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®é›†Dçš„å¤§å°è£å‰ªä¸ºä¸€åŠï¼ˆä»…æœ‰480ä¸ªçŠ¶æ€-åŠ¨ä½œå¯¹ä¸­çš„240ä¸ªï¼‰ï¼Œå¹¶å°†æ­¤æ“ä½œåº”ç”¨äºåç»­æ‰€æœ‰çš„ILæ–¹æ³•ã€‚
- en: Results
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: After training the model (as shown in `bc.py` script), we get an average episodic
    return of 11.49 with a standard deviation of 5.24.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæ¨¡å‹åï¼ˆå¦‚`bc.py`è„šæœ¬æ‰€ç¤ºï¼‰ï¼Œæˆ‘ä»¬å¾—åˆ°çš„å¹³å‡å›æŠ¥æ˜¯11.49ï¼Œæ ‡å‡†å·®ä¸º5.24ã€‚
- en: This is much less than the forward RL methods before. The following GIF shows
    the trained BC model in action.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¯”ä¹‹å‰çš„å‰å‘å¼ºåŒ–å­¦ä¹ æ–¹æ³•è¦å°å¾—å¤šã€‚ä»¥ä¸‹GIFå±•ç¤ºäº†è®­ç»ƒå¥½çš„BCæ¨¡å‹çš„å®é™…è¡¨ç°ã€‚
- en: '![](../Images/4d844fec525ada99e1b12814cdde3f86.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d844fec525ada99e1b12814cdde3f86.png)'
- en: From the GIF, itâ€™s evident that almost two-thirds of the trajectories have learned
    to pass through the wall. However, the model gets stuck with the last third, as
    it cannot infer the true policy from previous examples, especially since it was
    given only half of the 15 expert trajectories to learn from.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»GIFä¸­å¯ä»¥çœ‹å‡ºï¼Œå‡ ä¹ä¸‰åˆ†ä¹‹äºŒçš„è½¨è¿¹å·²ç»å­¦ä¼šäº†ç©¿è¶Šå¢™å£ã€‚ç„¶è€Œï¼Œæ¨¡å‹åœ¨æœ€åä¸‰åˆ†ä¹‹ä¸€çš„è½¨è¿¹ä¸Šé‡åˆ°äº†å›°éš¾ï¼Œå› ä¸ºå®ƒæ— æ³•ä»ä¹‹å‰çš„ä¾‹å­ä¸­æ¨æ–­å‡ºçœŸå®çš„ç­–ç•¥ï¼Œç‰¹åˆ«æ˜¯ç”±äºå®ƒåªä»15æ¡ä¸“å®¶è½¨è¿¹ä¸­çš„ä¸€åŠè¿›è¡Œå­¦ä¹ ã€‚
- en: Maximum Entropy Inverse Reinforcement Learning (MaxENT)
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€å¤§ç†µé€†å¼ºåŒ–å­¦ä¹ ï¼ˆMaxENTï¼‰
- en: 'MaxEnt [3] is another method to train a reward model separately (not iteratively),
    beside Behavior Cloning (BC). Its main idea lies in maximizing the probability
    of taking expert trajectories based on the current reward function. This can be
    expressed as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: MaxEnt [3] æ˜¯ä¸€ç§ä¸è¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰å¹¶è¡Œçš„è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„å¦ä¸€ç§æ–¹æ³•ï¼ˆè€Œä¸æ˜¯è¿­ä»£æ–¹å¼ï¼‰ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯åŸºäºå½“å‰çš„å¥–åŠ±å‡½æ•°ï¼Œæœ€å¤§åŒ–é‡‡å–ä¸“å®¶è½¨è¿¹çš„æ¦‚ç‡ã€‚è¿™å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: Where *N* is the trajectory length, and *Z* is a normalizing constant of the
    sum of all possible trajectories returns under the given policy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *N* æ˜¯è½¨è¿¹é•¿åº¦ï¼Œ*Z* æ˜¯åœ¨ç»™å®šç­–ç•¥ä¸‹ï¼Œæ‰€æœ‰å¯èƒ½è½¨è¿¹å›æŠ¥ä¹‹å’Œçš„å½’ä¸€åŒ–å¸¸æ•°ã€‚
- en: 'From there, the method derives its main objective based on the maximum entropy
    theorem [3], which states that *the most representative policy fulfilling a given
    condition is the one with highest entropy H.* Therefore, MaxEnt requires an additional
    objective to maximize the entropy of the policy. This leads to the following formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œè¯¥æ–¹æ³•åŸºäºæœ€å¤§ç†µå®šç†[3]æ¨å¯¼å‡ºå…¶ä¸»è¦ç›®æ ‡ï¼Œè¯¥å®šç†æŒ‡å‡ºï¼š*æ»¡è¶³ç»™å®šæ¡ä»¶çš„æœ€å…·ä»£è¡¨æ€§çš„ç­–ç•¥æ˜¯å…·æœ‰æœ€å¤§ç†µHçš„ç­–ç•¥*ã€‚å› æ­¤ï¼ŒMaxEntéœ€è¦ä¸€ä¸ªé¢å¤–çš„ç›®æ ‡æ¥æœ€å¤§åŒ–ç­–ç•¥çš„ç†µã€‚è¿™å¯¼è‡´äº†ä»¥ä¸‹å…¬å¼ï¼š
- en: 'Which has the derivative:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶å¯¼æ•°ä¸ºï¼š
- en: Where *SVD* is the state visitation frequency, which can be calculated with
    a dynamic programming algorithm given the current policy.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *SVD* æ˜¯çŠ¶æ€è®¿é—®é¢‘ç‡ï¼Œå¯ä»¥é€šè¿‡åŠ¨æ€è§„åˆ’ç®—æ³•åœ¨ç»™å®šç­–ç•¥ä¸‹è®¡ç®—å¾—å‡ºã€‚
- en: In our implementation here of MaxEnt, we skip the training of a new reward,
    where the dynamic programming algorithm would be slow and lengthy. Instead, we
    opt to test the main idea of maximizing the entropy by re-training a BC model
    exactly as in the previous process, but with an added term of the negative entropy
    of the inferred action distribution to the loss. The entropy should be negative
    because we wish to maximize it by minimizing the loss.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬è¿™é‡Œå®ç°çš„æœ€å¤§ç†µï¼ˆMaxEntï¼‰ä¸­ï¼Œæˆ‘ä»¬è·³è¿‡äº†è®­ç»ƒä¸€ä¸ªæ–°çš„å¥–åŠ±æ¨¡å‹ï¼Œå› ä¸ºåŠ¨æ€è§„åˆ’ç®—æ³•åœ¨æ­¤è¿‡ç¨‹ä¸­ä¼šå˜å¾—ç¼“æ…¢ä¸”å†—é•¿ã€‚ç›¸åï¼Œæˆ‘ä»¬é€‰æ‹©é€šè¿‡é‡æ–°è®­ç»ƒä¸€ä¸ªè¡Œä¸ºå…‹éš†ï¼ˆBCï¼‰æ¨¡å‹æ¥æµ‹è¯•æœ€å¤§åŒ–ç†µçš„ä¸»è¦æ€æƒ³ï¼Œæ–¹æ³•ä¸ä¹‹å‰çš„è¿‡ç¨‹å®Œå…¨ç›¸åŒï¼Œåªæ˜¯å°†æ¨æ–­å‡ºçš„è¡ŒåŠ¨åˆ†å¸ƒçš„è´Ÿç†µé¡¹æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­ã€‚ç†µåº”è¯¥æ˜¯è´Ÿçš„ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›é€šè¿‡æœ€å°åŒ–æŸå¤±æ¥æœ€å¤§åŒ–å®ƒã€‚
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: After adding the negative entropy of the distributions of actions with a weight
    of 0.5 (choosing the right value is important; otherwise, it may lead to worse
    learning), we see a slight improvement over the performance of the previous BC
    model with an average episodic return of 11.56 now (+0.07). The small value of
    the improvement can be explained by the simple nature of the environment, which
    contains a limited number of states. If the state space gets bigger, the entropy
    is expected to have a bigger importance.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»™åŠ¨ä½œåˆ†å¸ƒæ·»åŠ äº†æƒé‡ä¸º0.5çš„è´Ÿç†µï¼ˆé€‰æ‹©æ­£ç¡®çš„å€¼å¾ˆé‡è¦ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´æ›´å·®çš„å­¦ä¹ ï¼‰åï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸ä¹‹å‰çš„BCæ¨¡å‹ç›¸æ¯”ï¼Œæ€§èƒ½ç•¥æœ‰æå‡ï¼Œç°åœ¨çš„å¹³å‡å›åˆå›æŠ¥ä¸º11.56ï¼ˆ+0.07ï¼‰ã€‚è¿™ç§å°å¹…æå‡å¯ä»¥é€šè¿‡ç¯å¢ƒçš„ç®€å•æ€§è´¨æ¥è§£é‡Šï¼Œè¯¥ç¯å¢ƒåŒ…å«æœ‰é™æ•°é‡çš„çŠ¶æ€ã€‚å¦‚æœçŠ¶æ€ç©ºé—´å˜å¤§ï¼Œç†µçš„ä½œç”¨é¢„è®¡ä¼šå˜å¾—æ›´åŠ é‡è¦ã€‚
- en: Generative Adversarial Imitation Learning (GAIL)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆGAILï¼‰
- en: The original work on GAIL [5] was inspired by the concept of Generative Adversarial
    Networks (GANs), which apply the idea of adversarial training to enhance the generative
    abilities of a main model. Similarly, in GAIL, the concept is applied to match
    state-action distributions between trained and expert policies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: GAILçš„åŸå§‹å·¥ä½œ[5]çµæ„Ÿæ¥æºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„æ¦‚å¿µï¼ŒGANså°†å¯¹æŠ—è®­ç»ƒçš„ç†å¿µåº”ç”¨äºå¢å¼ºä¸»æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚åŒæ ·ï¼Œåœ¨GAILä¸­ï¼Œè¿™ä¸€æ¦‚å¿µè¢«åº”ç”¨äºä½¿è®­ç»ƒç­–ç•¥ä¸ä¸“å®¶ç­–ç•¥ä¹‹é—´çš„çŠ¶æ€-åŠ¨ä½œåˆ†å¸ƒç›¸åŒ¹é…ã€‚
- en: 'This can be derived as Kullback-Leibler divergence, as shown in the main paper
    [5]. The paper finally derives the main objective for both models (called generator
    and discriminator models in GAIL) as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥æ¨å¯¼ä¸ºKullback-Leibleræ•£åº¦ï¼Œå¦‚ä¸»æ–‡çŒ®[5]æ‰€ç¤ºã€‚æ–‡çŒ®æœ€ç»ˆæ¨å¯¼å‡ºäº†ä¸¤ä¸ªæ¨¡å‹ï¼ˆåœ¨GAILä¸­ç§°ä¸ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨æ¨¡å‹ï¼‰çš„ä¸»è¦ç›®æ ‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/3d98990eb6f613a4842efbd3f5682a05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d98990eb6f613a4842efbd3f5682a05.png)'
- en: Where *Dt*â€‹ is the discriminator, *Ï€Î¸*â€‹ is the generator model (i.e., the policy
    under training), *Ï€E*â€‹ is the expert policy, and *H*(*Ï€Î¸*â€‹) is the entropy of
    the generator model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ*Dt*â€‹ æ˜¯åˆ¤åˆ«å™¨ï¼Œ*Ï€Î¸*â€‹ æ˜¯ç”Ÿæˆå™¨æ¨¡å‹ï¼ˆå³æ­£åœ¨è®­ç»ƒçš„ç­–ç•¥ï¼‰ï¼Œ*Ï€E*â€‹ æ˜¯ä¸“å®¶ç­–ç•¥ï¼Œ*H*(*Ï€Î¸*â€‹) æ˜¯ç”Ÿæˆå™¨æ¨¡å‹çš„ç†µã€‚
- en: The discriminator acts as a binary classifier, while the generator is the actual
    policy model being trained.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨å……å½“äºŒåˆ†ç±»å™¨ï¼Œè€Œç”Ÿæˆå™¨åˆ™æ˜¯å®é™…çš„ç­–ç•¥æ¨¡å‹ï¼Œæ­£åœ¨æ¥å—è®­ç»ƒã€‚
- en: The Main Benefit of GAIL
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAILçš„ä¸»è¦ä¼˜åŠ¿
- en: The main benefit of GAIL over previous methods (and the reason it performs better)
    lies in its interactive training process. The trained policy learns and explores
    different states guided by the discriminatorâ€™s reward signal.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GAILç›¸å¯¹äºä»¥å‰æ–¹æ³•çš„ä¸»è¦ä¼˜åŠ¿ï¼ˆä¹Ÿæ˜¯å…¶è¡¨ç°æ›´å¥½çš„åŸå› ï¼‰åœ¨äºå…¶äº¤äº’å¼è®­ç»ƒè¿‡ç¨‹ã€‚è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥åœ¨åˆ¤åˆ«å™¨å¥–åŠ±ä¿¡å·çš„æŒ‡å¯¼ä¸‹å­¦ä¹ å¹¶æ¢ç´¢ä¸åŒçš„çŠ¶æ€ã€‚
- en: Results
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: After training GAIL for 1.6 million steps, the model converged to a higher level
    than BC and MaxEnt models. If continued to be trained, even better results can
    be achieved.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒäº†1.6ç™¾ä¸‡æ­¥åï¼ŒGAILæ¨¡å‹æ”¶æ•›åˆ°äº†æ¯”BCå’ŒMaxEntæ¨¡å‹æ›´é«˜çš„æ°´å¹³ã€‚å¦‚æœç»§ç»­è®­ç»ƒï¼Œç”šè‡³å¯ä»¥å–å¾—æ›´å¥½çš„ç»“æœã€‚
- en: Specifically, we obtained an average episodic reward of 12.8, which is noteworthy
    considering that only 50% of demonstrations were provided without any real reward.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è·å¾—äº†12.8çš„å¹³å‡å›åˆå¥–åŠ±ï¼Œè¿™ä¸€ç‚¹å€¼å¾—æ³¨æ„ï¼Œå› ä¸ºåªæœ‰50%çš„ç¤ºèŒƒåœ¨æ²¡æœ‰ä»»ä½•çœŸå®å¥–åŠ±çš„æƒ…å†µä¸‹æä¾›ã€‚
- en: This figure shows the training curve for GAIL (with ground truth episodic rewards
    on the y-axis). Itâ€™s worth noting that the rewards coming from log(*D*(*s*,*a*))
    will be more chaotic than the ground truth due to GAILâ€™s adversarial training
    nature.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å›¾æ˜¾ç¤ºäº†GAILçš„è®­ç»ƒæ›²çº¿ï¼ˆyè½´ä¸ºçœŸå®çš„å›åˆå¥–åŠ±ï¼‰ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç”±äºGAILçš„å¯¹æŠ—æ€§è®­ç»ƒç‰¹æ€§ï¼Œæ¥è‡ªlog(*D*(*s*,*a*))çš„å¥–åŠ±ä¼šæ¯”çœŸå®å¥–åŠ±æ›´åŠ æ··ä¹±ã€‚
- en: '![](../Images/64b2b61d8406584fafe7665aed713417.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64b2b61d8406584fafe7665aed713417.png)'
- en: Adversarial Inverse Reinforcement Learning (AIRL)
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹æŠ—æ€§é€†å¼ºåŒ–å­¦ä¹ ï¼ˆAIRLï¼‰
- en: One remaining problem with GAIL is that the trained reward model (the discriminator)
    does not actually represent the ground truth reward. Instead, the discriminator
    is trained as a binary classifier between expert and generator state-action pairs,
    resulting in an average of its values of 0.5\. This means that the discriminator
    can only be considered a surrogate reward.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GAILçš„ä¸€ä¸ªå‰©ä½™é—®é¢˜æ˜¯ï¼Œè®­ç»ƒå¾—åˆ°çš„å¥–åŠ±æ¨¡å‹ï¼ˆåˆ¤åˆ«å™¨ï¼‰å¹¶ä¸çœŸæ­£ä»£è¡¨çœŸå®çš„å¥–åŠ±ã€‚ç›¸åï¼Œåˆ¤åˆ«å™¨è¢«è®­ç»ƒä¸ºä¸€ä¸ªä¸“å®¶ä¸ç”Ÿæˆå™¨çŠ¶æ€-åŠ¨ä½œå¯¹ä¹‹é—´çš„äºŒåˆ†ç±»å™¨ï¼Œå¯¼è‡´å…¶å€¼çš„å¹³å‡ä¸º0.5ã€‚è¿™æ„å‘³ç€åˆ¤åˆ«å™¨åªèƒ½ä½œä¸ºä¸€ä¸ªæ›¿ä»£å¥–åŠ±ã€‚
- en: 'To solve this problem, the paper in [6] reformulates the discriminator using
    the following formula:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ–‡çŒ®[6]é€šè¿‡ä»¥ä¸‹å…¬å¼é‡æ–°æ„é€ äº†åˆ¤åˆ«å™¨ï¼š
- en: where *fÏ‰*â€‹(*s*,*a*) should converge to the actual advantage function. In this
    example, this value represents how close the agent is to the invisible goal. The
    ground truth reward can be found by adding another term to include a shaped reward;
    however, for this experiment, we will restrict ourselves to the advantage function
    above.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ*fÏ‰*â€‹(*s*,*a*) åº”è¯¥æ”¶æ•›åˆ°å®é™…çš„ä¼˜åŠ¿å‡½æ•°ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œè¿™ä¸ªå€¼è¡¨ç¤ºä»£ç†ä¸ä¸å¯è§ç›®æ ‡çš„æ¥è¿‘ç¨‹åº¦ã€‚é€šè¿‡æ·»åŠ å¦ä¸€ä¸ªé¡¹ä»¥åŒ…å«å½¢çŠ¶å¥–åŠ±ï¼Œå¯ä»¥å¾—åˆ°åœ°é¢çœŸå®å¥–åŠ±ï¼›ç„¶è€Œï¼Œåœ¨æœ¬æ¬¡å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†é™åˆ¶ä½¿ç”¨ä¸Šé¢çš„ä¼˜åŠ¿å‡½æ•°ã€‚
- en: Results
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'After training the AIRL model with the same parameters as GAIL, we obtained
    the following training curve:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨ä¸ GAIL ç›¸åŒå‚æ•°è®­ç»ƒ AIRL æ¨¡å‹åï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹è®­ç»ƒæ›²çº¿ï¼š
- en: '![](../Images/fca3ec99819c63a12137e5791e9c6f12.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fca3ec99819c63a12137e5791e9c6f12.png)'
- en: It is noted that given the same training steps (1.6 Million steps), AIRL was
    slower to converge due to the added complexity of training the discriminator.
    However, now we have a meaningful advantage function, albeit with a performance
    of only 10.8 episodic reward, which is still good enough.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè€ƒè™‘åˆ°ç›¸åŒçš„è®­ç»ƒæ­¥æ•°ï¼ˆ160 ä¸‡æ­¥ï¼‰ï¼Œç”±äºè®­ç»ƒåˆ¤åˆ«å™¨çš„å¤æ‚æ€§å¢åŠ ï¼ŒAIRL æ”¶æ•›è¾ƒæ…¢ã€‚ç„¶è€Œï¼Œç°åœ¨æˆ‘ä»¬å·²ç»å¾—åˆ°äº†ä¸€ä¸ªæœ‰æ„ä¹‰çš„ä¼˜åŠ¿å‡½æ•°ï¼Œå°½ç®¡å…¶è¡¨ç°åªæœ‰
    10.8 çš„æ¯é›†å¥–åŠ±ï¼Œä½†ä»ç„¶è¶³å¤Ÿå¥½ã€‚
- en: 'Letâ€™s compare the values of this advantage function and the ground truth reward
    in response to expert demonstrations. To make these values more comparable, we
    also normalized the values of the learned advantage function *fÏ‰*â€‹. From this,
    we got the following plot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†è¿™ä¸ªä¼˜åŠ¿å‡½æ•°çš„å€¼ä¸å“åº”ä¸“å®¶æ¼”ç¤ºçš„åœ°é¢çœŸå®å¥–åŠ±è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†ä½¿è¿™äº›å€¼æ›´å…·å¯æ¯”æ€§ï¼Œæˆ‘ä»¬è¿˜å¯¹å­¦ä¹ åˆ°çš„ä¼˜åŠ¿å‡½æ•° *fÏ‰*â€‹ çš„å€¼è¿›è¡Œäº†å½’ä¸€åŒ–ã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](../Images/5a9cc6f81f11b38b39c4840e9be7e623.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a9cc6f81f11b38b39c4840e9be7e623.png)'
- en: In this figure, there are 15 pulses corresponding to the 15 initial states of
    the agent. We can see bigger errors in the trained model for the last half of
    the plot, which is due to the limited use of only half the expert demos in training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œæœ‰ 15 ä¸ªè„‰å†²å¯¹åº”ä»£ç†çš„ 15 ä¸ªåˆå§‹çŠ¶æ€ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨å›¾çš„ååŠéƒ¨åˆ†ï¼Œè®­ç»ƒæ¨¡å‹çš„è¯¯å·®è¾ƒå¤§ï¼Œè¿™æ˜¯ç”±äºåœ¨è®­ç»ƒä¸­ä»…æœ‰é™ä½¿ç”¨äº†ä¸“å®¶æ¼”ç¤ºçš„ä¸€åŠã€‚
- en: For the first half, we observe a low state when the agent stands still at the
    goal with zero reward, while it was evaluated as a high value in the trained model.
    In the second half, thereâ€™s a general shift towards lower values.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‰åŠéƒ¨åˆ†ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨ä»£ç†é™æ­¢åœ¨ç›®æ ‡ä½ç½®å¹¶è·å¾—é›¶å¥–åŠ±æ—¶ï¼ŒçŠ¶æ€è¾ƒä½ï¼Œè€Œåœ¨è®­ç»ƒæ¨¡å‹ä¸­è¯„ä¼°æ—¶è¯¥å€¼è¾ƒé«˜ã€‚ååŠéƒ¨åˆ†åˆ™æ™®éåå‘äºè¾ƒä½çš„å€¼ã€‚
- en: Roughly speaking, the learned function approximately follows the ground truth
    reward and has recovered useful information about it using AIRL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§è‡´æ¥è¯´ï¼Œå­¦ä¹ åˆ°çš„å‡½æ•°å¤§è‡´è·Ÿéšåœ°é¢çœŸå®å¥–åŠ±ï¼Œå¹¶é€šè¿‡ AIRL æ¢å¤äº†å…³äºå®ƒçš„æœ‰ç”¨ä¿¡æ¯ã€‚
- en: Information Maximization GAIL (InfoGAIL)
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¿¡æ¯æœ€å¤§åŒ–ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ï¼ˆInfoGAILï¼‰
- en: 'Despite the advancements made by previous methods, an important problem still
    persists in Imitation Learning (IL): **multi-modal learning**. To apply IL to
    practical problems, it is necessary to learn from multiple possible expert policies.
    For instance, when driving or playing football, there is no single â€œtrueâ€ way
    of doing things; experts vary in their methods, and the IL model should be able
    to learn these variations consistently.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰è¿°æ–¹æ³•å–å¾—äº†ä¸€å®šè¿›å±•ï¼Œä½†åœ¨æ¨¡ä»¿å­¦ä¹ ï¼ˆILï¼‰ä¸­ä»ç„¶å­˜åœ¨ä¸€ä¸ªé‡è¦é—®é¢˜ï¼š**å¤šæ¨¡æ€å­¦ä¹ **ã€‚ä¸ºäº†å°† IL åº”ç”¨äºå®é™…é—®é¢˜ï¼Œéœ€è¦ä»å¤šä¸ªå¯èƒ½çš„ä¸“å®¶ç­–ç•¥ä¸­å­¦ä¹ ã€‚ä¾‹å¦‚ï¼Œåœ¨å¼€è½¦æˆ–è¸¢è¶³çƒæ—¶ï¼Œå¹¶æ²¡æœ‰ä¸€ç§â€œæ­£ç¡®â€çš„åšäº‹æ–¹å¼ï¼›ä¸“å®¶åœ¨æ–¹æ³•ä¸Šæœ‰æ‰€ä¸åŒï¼ŒIL
    æ¨¡å‹åº”è¯¥èƒ½å¤Ÿä¸€è‡´åœ°å­¦ä¹ è¿™äº›å˜åŒ–ã€‚
- en: 'To address this issue, InfoGAIL was developed [7]. Inspired by InfoGAN [11],
    which conditions the style of outputs generated by GAN using an additional style
    vector, InfoGAIL builds on the GAIL objective and adds another criterion: maximizing
    the mutual information between state-action pairs and a new controlling input
    vector *z*. This objective can be derived as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼€å‘äº† InfoGAIL [7]ã€‚InfoGAIL å—åˆ° InfoGAN [11] çš„å¯å‘ï¼ŒInfoGAN ä½¿ç”¨é¢å¤–çš„é£æ ¼å‘é‡æ¥è°ƒèŠ‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”Ÿæˆè¾“å‡ºçš„é£æ ¼ï¼ŒInfoGAIL
    åœ¨ GAIL ç›®æ ‡çš„åŸºç¡€ä¸Šå¢åŠ äº†å¦ä¸€ä¸ªæ ‡å‡†ï¼šæœ€å¤§åŒ–çŠ¶æ€-åŠ¨ä½œå¯¹ä¸æ–°çš„æ§åˆ¶è¾“å…¥å‘é‡ *z* ä¹‹é—´çš„äº’ä¿¡æ¯ã€‚è¿™ä¸ªç›®æ ‡å¯ä»¥æ¨å¯¼ä¸ºï¼š
- en: Kullback-Leibler divergence,
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Kullback-Leibler æ•£åº¦ï¼Œ
- en: where estimating the posterior *p*(*z*âˆ£*s*,*a*) is approximated with a new model,
    *Q*, which takes (*s*,*a*) as input and outputs *z*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼ŒåéªŒæ¦‚ç‡ *p*(*z*âˆ£*s*,*a*) çš„ä¼°è®¡é€šè¿‡ä¸€ä¸ªæ–°æ¨¡å‹ *Q* è¿›è¡Œè¿‘ä¼¼ï¼Œ*Q* ä»¥ (*s*,*a*) ä¸ºè¾“å…¥ï¼Œè¾“å‡º *z*ã€‚
- en: 'The final objective for InfoGAIL can be written as:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAIL çš„æœ€ç»ˆç›®æ ‡å¯ä»¥å†™ä¸ºï¼š
- en: '![](../Images/b5bc5c1f07a8eea226011e43329d8b49.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5bc5c1f07a8eea226011e43329d8b49.png)'
- en: 'As a result, the policy has an additional input, namely *z*, as shown in the
    following figure:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ï¼Œç­–ç•¥æœ‰ä¸€ä¸ªé¢å¤–çš„è¾“å…¥ï¼Œå³ *z*ï¼Œå¦‚ä»¥ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](../Images/cdb73e4f39a5c4a376c9da575e4f02e8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdb73e4f39a5c4a376c9da575e4f02e8.png)'
- en: 'In our experiments, we generated new multi-modal expert demos where each expert
    could enter from one gap only (of the three gaps on the wall), regardless of their
    goal. The full demo set was used without labels indicating which expert was acting.
    The *z* variable is a one-hot encoding vector representing the expert class with
    three elements (e.g., `[1 0 0]` for the left door). The policy should:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†æ–°çš„å¤šæ¨¡æ€ä¸“å®¶æ¼”ç¤ºï¼Œå…¶ä¸­æ¯ä¸ªä¸“å®¶åªèƒ½ä»ä¸€ä¸ªé—´éš™è¿›å…¥ï¼ˆå¢™ä¸Šæœ‰ä¸‰ä¸ªé—´éš™ï¼‰ï¼Œä¸å…¶ç›®æ ‡æ— å…³ã€‚å®Œæ•´çš„æ¼”ç¤ºé›†åœ¨æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µä¸‹ä½¿ç”¨ï¼Œæ ‡ç­¾ä¸ä¼šæŒ‡æ˜æ˜¯å“ªä¸ªä¸“å®¶åœ¨è¡ŒåŠ¨ã€‚*z*
    å˜é‡æ˜¯ä¸€ä¸ªç‹¬çƒ­ç¼–ç å‘é‡ï¼Œè¡¨ç¤ºä¸“å®¶ç±»åˆ«ï¼ŒåŒ…å«ä¸‰ä¸ªå…ƒç´ ï¼ˆä¾‹å¦‚ï¼Œå·¦é—¨çš„è¡¨ç¤ºä¸º`[1 0 0]`ï¼‰ã€‚ç­–ç•¥åº”è¯¥ï¼š
- en: Learn to move towards the goal
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ æœå‘ç›®æ ‡ç§»åŠ¨
- en: Link randomly generated *z* values to different modes of experts (thus passing
    through different doors)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæœºç”Ÿæˆçš„ *z* å€¼ä¸ä¸åŒä¸“å®¶çš„æ¨¡å¼ç›¸è”ç³»ï¼ˆä»è€Œé€šè¿‡ä¸åŒçš„é—¨ï¼‰ã€‚
- en: The *Q* model should be able to detect which mode it is based on the direction
    of actions in every state
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q* æ¨¡å‹åº”è¯¥èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªçŠ¶æ€ä¸­åŠ¨ä½œçš„æ–¹å‘æ¥æ£€æµ‹å…¶æ‰€å¤„çš„æ¨¡å¼ã€‚'
- en: Note that the discriminator, Q-model, and policy model training graphs are chaotic
    due to adversarial training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œç”±äºå¯¹æŠ—æ€§è®­ç»ƒï¼Œåˆ¤åˆ«å™¨ã€Qæ¨¡å‹å’Œç­–ç•¥æ¨¡å‹çš„è®­ç»ƒå›¾è¡¨æ˜¯æ··ä¹±çš„ã€‚
- en: 'Fortunately, we were able to learn two modes clearly. However, the third mode
    was not recognized by either the policy or the Q-model. The following three GIFs
    show the learned expert modes from InfoGAIL when given different values of *z*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æˆåŠŸåœ°æ¸…æ™°åœ°å­¦ä¹ åˆ°äº†ä¸¤ç§æ¨¡å¼ã€‚ç„¶è€Œï¼Œç¬¬ä¸‰ç§æ¨¡å¼æ—¢æ²¡æœ‰è¢«ç­–ç•¥ä¹Ÿæ²¡æœ‰è¢« Q æ¨¡å‹è¯†åˆ«å‡ºæ¥ã€‚ä»¥ä¸‹ä¸‰ä¸ª GIF å±•ç¤ºäº† InfoGAIL åœ¨ç»™å®šä¸åŒ
    *z* å€¼æ—¶å­¦åˆ°çš„ä¸“å®¶æ¨¡å¼ï¼š
- en: '![](../Images/c12ec76451dee278e191b253b631f0ae.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c12ec76451dee278e191b253b631f0ae.png)'
- en: '**z = [1,0,0]**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [1,0,0]**'
- en: '![](../Images/ab355cdb776ac470e7b1e8b5aa1552a3.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab355cdb776ac470e7b1e8b5aa1552a3.png)'
- en: '**z = [0,1,0]**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [0,1,0]**'
- en: '![](../Images/0a66821dfd5078e24edbf1a7557fef59.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a66821dfd5078e24edbf1a7557fef59.png)'
- en: '**z = [0,0,1]**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**z = [0,0,1]**'
- en: Lastly, the policy was able to converge to an episodic reward of around 10 with
    800K training steps. With more training steps, better results can be achieved,
    even if the experts used in this example are not optimal.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç­–ç•¥èƒ½å¤Ÿåœ¨ 800K è®­ç»ƒæ­¥æ•°ä¸‹æ”¶æ•›åˆ°å¤§çº¦ 10 çš„å›æŠ¥ã€‚éšç€è®­ç»ƒæ­¥æ•°çš„å¢åŠ ï¼Œå¯ä»¥å–å¾—æ›´å¥½çš„ç»“æœï¼Œå³ä½¿è¿™ä¸ªä¾‹å­ä¸­ä½¿ç”¨çš„ä¸“å®¶ä¸æ˜¯æœ€ä¼˜çš„ã€‚
- en: Final Overview and Conclusion
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ¦‚è¿°ä¸ç»“è®º
- en: 'As we review our experiments, itâ€™s clear that all IL methods have performed
    well in terms of episodic reward criteria. The following table summarizes their
    performance:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å›é¡¾æˆ‘ä»¬çš„å®éªŒæ—¶ï¼Œå¾ˆæ˜æ˜¾æ‰€æœ‰çš„æ¨¡ä»¿å­¦ä¹ æ–¹æ³•åœ¨é›†å›æŠ¥æ ‡å‡†æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚ä¸‹è¡¨æ€»ç»“äº†å®ƒä»¬çš„è¡¨ç°ï¼š
- en: '![](../Images/b240ddb57578ec3271d55c9076130b7e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b240ddb57578ec3271d55c9076130b7e.png)'
- en: '**InfoGAIL results are not comparable as the expert demos were based on multi-modal
    experts*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**InfoGAIL çš„ç»“æœä¸å¯æ¯”ï¼Œå› ä¸ºä¸“å®¶æ¼”ç¤ºåŸºäºå¤šæ¨¡æ€ä¸“å®¶**'
- en: The table shows that GAIL performed the best for this problem, while AIRL was
    slower due to its new reward formulation, resulting in a lower return. InfoGAIL
    also learned well but struggled with recognizing all three modes of experts.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼æ˜¾ç¤ºï¼ŒGAIL åœ¨è¿™ä¸ªé—®é¢˜ä¸Šè¡¨ç°æœ€å¥½ï¼Œè€Œ AIRL ç”±äºå…¶æ–°çš„å¥–åŠ±å…¬å¼è¾ƒæ…¢ï¼Œå¯¼è‡´å›æŠ¥è¾ƒä½ã€‚InfoGAIL ä¹Ÿå­¦å¾—ä¸é”™ï¼Œä½†åœ¨è¯†åˆ«æ‰€æœ‰ä¸‰ä¸ªä¸“å®¶æ¨¡å¼æ—¶é‡åˆ°äº†ä¸€äº›å›°éš¾ã€‚
- en: Conclusion
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Imitation Learning is a challenging and fascinating field. The methods weâ€™ve
    explored are suitable for grid simulation environments but may not directly translate
    to real-world applications. Practical uses of IL are still in its infancy, except
    for some BC methods. Linking simulations to reality introduces new errors due
    to differences of their nature.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡ä»¿å­¦ä¹ æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§ä¸”è¿·äººçš„é¢†åŸŸã€‚æˆ‘ä»¬æ¢ç´¢çš„æ–¹æ³•é€‚ç”¨äºç½‘æ ¼ä»¿çœŸç¯å¢ƒï¼Œä½†å¯èƒ½æ— æ³•ç›´æ¥è½¬åŒ–ä¸ºç°å®åº”ç”¨ã€‚æ¨¡ä»¿å­¦ä¹ çš„å®é™…åº”ç”¨ä»å¤„äºèµ·æ­¥é˜¶æ®µï¼Œé™¤äº†æŸäº› BC
    æ–¹æ³•ä¹‹å¤–ã€‚å°†ä»¿çœŸä¸ç°å®è”ç³»èµ·æ¥ä¼šç”±äºä¸¤è€…çš„æ€§è´¨å·®å¼‚è€Œå¼•å…¥æ–°çš„è¯¯å·®ã€‚
- en: Another open challenge in IL is Multi-agent Imitation Learning. Research like
    MAIRL [9] and MAGAIL [10] have experimented with multi-agent environments but
    a general theory for learning from multiple expert trajectories remains an open
    question.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡ä»¿å­¦ä¹ ä¸­çš„å¦ä¸€ä¸ªå¼€æ”¾æŒ‘æˆ˜æ˜¯å¤šæ™ºèƒ½ä½“æ¨¡ä»¿å­¦ä¹ ã€‚åƒ MAIRL [9] å’Œ MAGAIL [10] è¿™æ ·çš„ç ”ç©¶å·²ç»åœ¨å¤šæ™ºèƒ½ä½“ç¯å¢ƒä¸­è¿›è¡Œè¿‡å®éªŒï¼Œä½†ä»å¤šä¸ªä¸“å®¶è½¨è¿¹å­¦ä¹ çš„é€šç”¨ç†è®ºä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾é—®é¢˜ã€‚
- en: The attached [repository on GitHub](http://github.com/engyasin/ilsurvey) provides
    a basic approach to implementing these methods, which can be easily extended.
    The code will be updated in the future. If youâ€™re interested in contributing,
    please submit an issue or pull request with your modifications. Alternatively,
    feel free to leave a comment as weâ€™ll follow up with updates.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: é™„åŠ çš„[GitHub ä»“åº“](http://github.com/engyasin/ilsurvey)æä¾›äº†å®ç°è¿™äº›æ–¹æ³•çš„åŸºæœ¬æ€è·¯ï¼Œä¸”å¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ‰©å±•ã€‚ä»£ç å°†åœ¨æœªæ¥æ›´æ–°ã€‚å¦‚æœæ‚¨æœ‰å…´è¶£è´¡çŒ®ï¼Œè¯·æäº¤ä¸€ä¸ªé—®é¢˜æˆ–æ‹‰å–è¯·æ±‚ï¼ŒåŒ…å«æ‚¨çš„ä¿®æ”¹ã€‚æˆ–è€…ï¼Œæ¬¢è¿ç•™ä¸‹è¯„è®ºï¼Œæˆ‘ä»¬ä¼šè·Ÿè¿›å¹¶æä¾›æ›´æ–°ã€‚
- en: '*Note: Unless otherwise noted, all images are generated by author*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ï¼šé™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾åƒå‡ç”±ä½œè€…ç”Ÿæˆ*'
- en: References
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Mnih, V. (2013). Playing atari with deep reinforcement learning. arXiv
    preprint arXiv:1312.5602.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mnih, V. (2013). é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ ç©Atariæ¸¸æˆã€‚arXivé¢„å°æœ¬arXiv:1312.5602.'
- en: '[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).
    Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017).
    è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ç®—æ³•ã€‚arXivé¢„å°æœ¬arXiv:1707.06347.'
- en: '[3] Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008, July).
    Maximum entropy inverse reinforcement learning. In Aaai (Vol. 8, pp. 1433â€“1438).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. (2008å¹´7æœˆ). æœ€å¤§ç†µé€†å‘å¼ºåŒ–å­¦ä¹ ã€‚è½½äºã€ŠAAAIã€‹(Vol.
    8, pp. 1433â€“1438).'
- en: '[4] Bain, M., & Sammut, C. (1995, July). A Framework for Behavioural Cloning.
    In Machine Intelligence 15 (pp. 103â€“129).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bain, M., & Sammut, C. (1995å¹´7æœˆ). è¡Œä¸ºå…‹éš†æ¡†æ¶ã€‚è½½äºã€Šæœºå™¨æ™ºèƒ½15ã€‹(pp. 103â€“129).'
- en: '[5] Ho, J., & Ermon, S. (2016). Generative adversarial imitation learning.
    Advances in neural information processing systems, 29.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Ho, J., & Ermon, S. (2016). ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹(Advances in neural information
    processing systems), 29.'
- en: '[6] Fu, J., Luo, K., & Levine, S. (2017). Learning robust rewards with adversarial
    inverse reinforcement learning. arXiv preprint arXiv:1710.11248.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Fu, J., Luo, K., & Levine, S. (2017). é€šè¿‡å¯¹æŠ—é€†å‘å¼ºåŒ–å­¦ä¹ å­¦ä¹ ç¨³å¥å¥–åŠ±ã€‚arXivé¢„å°æœ¬arXiv:1710.11248.'
- en: '[7] Li, Y., Song, J., & Ermon, S. (2017). Infogail: Interpretable imitation
    learning from visual demonstrations. Advances in neural information processing
    systems, 30.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Li, Y., Song, J., & Ermon, S. (2017). Infogailï¼šä»è§†è§‰ç¤ºèŒƒä¸­è¿›è¡Œå¯è§£é‡Šçš„æ¨¡ä»¿å­¦ä¹ ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹(Advances
    in neural information processing systems), 30.'
- en: '[8] Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., & Peters,
    J. (2018). An algorithmic perspective on imitation learning. Foundations and TrendsÂ®
    in Robotics, 7(1â€“2), 1â€“179.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., & Peters,
    J. (2018). ä»¥ç®—æ³•è§†è§’çœ‹æ¨¡ä»¿å­¦ä¹ ã€‚ã€Šæœºå™¨äººå­¦åŸºç¡€ä¸è¶‹åŠ¿ã€‹(Foundations and TrendsÂ® in Robotics), 7(1â€“2),
    1â€“179.'
- en: '[9] Yu, L., Song, J., & Ermon, S. (2019, May). Multi-agent adversarial inverse
    reinforcement learning. In International Conference on Machine Learning (pp. 7194â€“7201).
    PMLR.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Yu, L., Song, J., & Ermon, S. (2019å¹´5æœˆ). å¤šæ™ºèƒ½ä½“å¯¹æŠ—é€†å‘å¼ºåŒ–å­¦ä¹ ã€‚è½½äºã€Šå›½é™…æœºå™¨å­¦ä¹ å¤§ä¼šã€‹(International
    Conference on Machine Learning)(pp. 7194â€“7201)ã€‚PMLR.'
- en: '[10] Song, J., Ren, H., Sadigh, D., & Ermon, S. (2018). Multi-agent generative
    adversarial imitation learning. Advances in neural information processing systems,
    31.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Song, J., Ren, H., Sadigh, D., & Ermon, S. (2018). å¤šæ™ºèƒ½ä½“ç”Ÿæˆå¯¹æŠ—æ¨¡ä»¿å­¦ä¹ ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹(Advances
    in neural information processing systems), 31.'
- en: '[11] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel,
    P. (2016). Infogan: Interpretable representation learning by information maximizing
    generative adversarial nets. Advances in neural information processing systems,
    29.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., & Abbeel,
    P. (2016). Infoganï¼šé€šè¿‡ä¿¡æ¯æœ€å¤§åŒ–ç”Ÿæˆå¯¹æŠ—ç½‘ç»œè¿›è¡Œå¯è§£é‡Šçš„è¡¨ç¤ºå­¦ä¹ ã€‚ã€Šç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•ã€‹(Advances in neural information
    processing systems), 29.'
- en: '[12] Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta,
    K., & AraÃƒÅ¡jo, J. G. (2022). Cleanrl: High-quality single-file implementations
    of deep reinforcement learning algorithms. Journal of Machine Learning Research,
    23(274), 1â€“18.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta,
    K., & AraÃƒÅ¡jo, J. G. (2022). Cleanrlï¼šé«˜è´¨é‡çš„å•æ–‡ä»¶æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°ã€‚ã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æœŸåˆŠã€‹(Journal of Machine
    Learning Research), 23(274), 1â€“18.'
