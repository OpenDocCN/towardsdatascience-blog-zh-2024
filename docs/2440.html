<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>K Nearest Neighbor Regressor, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>K Nearest Neighbor Regressor, Explained: A Visual Guide with Code Examples</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/k-nearest-neighbor-regressor-explained-a-visual-guide-with-code-examples-df5052c8c889?source=collection_archive---------1-----------------------#2024-10-07">https://towardsdatascience.com/k-nearest-neighbor-regressor-explained-a-visual-guide-with-code-examples-df5052c8c889?source=collection_archive---------1-----------------------#2024-10-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3468" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">REGRESSION ALGORITHM</h2><div/><div><h2 id="b7b5" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Finding the neighbors FAST with KD Trees and Ball Trees</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--df5052c8c889--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--df5052c8c889--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--df5052c8c889--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--df5052c8c889--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Oct 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mo mp mq mr ms mt"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----df5052c8c889--------------------------------"><div class="mu ab il"><div class="mv ab co cb mw mx"><h2 class="bf ga ib z it my iv iw mz iy ja fz bk">K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="na l"><h3 class="bf b ib z it my iv iw mz iy ja dx">The friendly neighbor approach to machine learning</h3></div><div class="gq l"><p class="bf b dy z it my iv iw mz iy ja dx">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng lw mt"/></div></div></a></div><p id="bf2c" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Building on our exploration of the <a class="af od" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">Nearest Neighbor Classifier</a>, letâ€™s turn to its sibling in the regression world. The Nearest Neighbor Regressor applies the same intuitive concept to predicting continuous values. But as our datasets get bigger, finding these neighbors efficiently becomes a real pain. Thatâ€™s where KD Trees and Ball Trees come in.</p><p id="cdea" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Itâ€™s super frustrating that thereâ€™s no clear guide out there that really explains whatâ€™s going on with these algorithms. Sure, there are some 2D visualizations, but they often donâ€™t make it clear how the trees work in multidimensional setting.</p><p id="e9d3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here, we will explain whatâ€™s <strong class="nj ga">actually</strong> going on in these algorithms without using the oversimplified 2D representation. Weâ€™ll be focusing on the construction of the trees itself and see which computation (and numbers) actually matters.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/00c7b6d82e926d5ce08ebc77977eab63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="aee3" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Definition</h1><p id="de4d" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The Nearest Neighbor Regressor is a straightforward predictive model that estimates values by averaging the outcomes of nearby data points. This method builds on the idea that similar inputs likely yield similar outputs.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/aba12c7b4e3b510a0d187544f9510e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5KKNUAFI03uShY4PtaAYcg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Nearest Neighbor approaches are among the most basic yet powerful techniques in the machine learning toolkit. Their simplicity belies their effectiveness in many real-world scenarios.</figcaption></figure><h1 id="04c6" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">ğŸ“Š Dataset Used</h1><p id="689b" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">To illustrate our concepts, weâ€™ll use <a class="af od" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">our usual dataset</a>. This dataset helps predict the number of golfers visiting on any given day. It includes factors such as weather outlook, temperature, humidity, and wind conditions. Our goal is to estimate the daily golfer turnout based on these variables.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/8f70777fcdcfbe631c352c17a649d816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSbHIOe7ZL6BF1uwPTCOZQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Columns: â€˜Outlookâ€™, â€˜Temperatureâ€™ (in Fahrenheit), â€˜Humidityâ€™ (in %), â€˜Windâ€™ (Yes/No) and â€˜Number of Playersâ€™ (numerical, target feature)</figcaption></figure><p id="79e1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">To use Nearest Neighbor Regression effectively, we need to preprocess our data. This involves <a class="af od" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">converting categorical variables into numerical format</a> and <a class="af od" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">scaling numerical features</a>.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/856bb5024939a0acbe57928f62346faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFZZ90PWRFqs2YVlYqie_w.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Standard scaling is applied to â€˜Temperatureâ€™ and â€˜Humidityâ€™ while the one-hot encoding is applied to â€˜Outlookâ€™ and â€˜Windâ€™</figcaption></figure><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="2916" class="qd oy fq qa b bg qe qf l qg qh"># Import libraries<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'])<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data into features and target, then into training and test sets<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Identify numerical columns<br/>numerical_columns = ['Temperature', 'Humidity']<br/><br/># Create a ColumnTransformer to scale only numerical columns<br/>ct = ColumnTransformer([<br/>    ('scaler', StandardScaler(), numerical_columns)<br/>], remainder='passthrough')<br/><br/># Fit the ColumnTransformer on the training data and transform both training and test data<br/>X_train_transformed = ct.fit_transform(X_train)<br/>X_test_transformed = ct.transform(X_test)<br/><br/># Convert the transformed data back to DataFrames<br/>feature_names = numerical_columns + [col for col in X_train.columns if col not in numerical_columns]<br/>X_train_scaled = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)<br/>X_test_scaled = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)</span></pre><h1 id="e7e0" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Main Mechanism</h1><p id="3587" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The Nearest Neighbor Regressor works similarly to its classifier counterpart, but instead of voting on a class, it averages the target values. Hereâ€™s the basic process:</p><ol class=""><li id="7690" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk">Calculate the distance between the new data point and all points in the training set.</li><li id="53d5" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Select the K nearest neighbors based on these distances.</li><li id="1a30" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Calculate the average of the target values of these K neighbors.</li><li id="84c7" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk">Assign this average as the predicted value for the new data point.</li></ol><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/250524333fd9efd70666c9f0c773da69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j272pmAHrEyGu6m1cTQREQ.png"/></div></div></figure><p id="0dbb" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The approach above, using all data points to find neighbors, is known as the <strong class="nj ga">Brute Force</strong> method. Itâ€™s straightforward but can be slow for large datasets.</p><p id="a966" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here, weâ€™ll explore two more efficient algorithms for finding nearest neighbors:</p><h1 id="c1c8" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk"><strong class="al">KD Tree for KNN Regression</strong></h1><p id="bf8e" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">KD Tree (K-Dimensional Tree) is a binary tree structure used for organizing points in a <em class="qq">k</em>-dimensional space. Itâ€™s particularly useful for tasks like nearest neighbor searches and range searches in multidimensional data.</p><h2 id="2dac" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk"><strong class="al">Training Steps:</strong></h2><p id="5ffa" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">1. Build the KD Tree:<br/>a. Start with a root node that contains all the points.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/f9eb3bb6d235cd561bf67f214fa3586f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRED6ebSyzyXc9mtYIu3aA.png"/></div></div></figure><p id="6fe6" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. Choose a feature to split on. Any random feature should be ok actually, but another good way to choose this is by looking which feature has median value closest to the midpoint between max and min value.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/79a398331b63176a9e9084603cb200e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKKXweOZmcbXtr5WBGPePA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Temperature has the midpoint line closest to the median line. We can start splitting from that dimension.</figcaption></figure><p id="c18a" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">c. Split the tree in the chosen feature and midpoint.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/c1a195c9003c18519a00a688f662db95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YlaT2I26jpqQHHJ0trrt8w.png"/></div></div></figure><p id="a881" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. Recursively, do step a-c until the stopping criterion, usually minimal leaf size (see <a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/">â€œmin samples leafâ€ here</a>)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/cfad5e8688c34aba173a931f66dbd614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BskEwtxSbLTSGZIm1PL8NA.png"/></div></div></figure><p id="5b8a" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. Store the target values:<br/>Along with each point in the KD Tree, store its corresponding target value. The minimum and maximum value for each node are also stored.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/3cf4c4165760cabc63946fa1328de2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6NxM-Jf0Rrd4qi-YdNheg.png"/></div></div></figure><h2 id="d446" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk"><strong class="al">Regression/Prediction Steps:</strong></h2><p id="6346" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">1. Traverse the KD Tree: <br/>a. Start at the root node. <br/>b. Compare the query point (test point) with the splitting dimension and value at each node. <br/>c. Recursively traverse left or right based on this comparison. <br/>d. When reaching a leaf node, add its points to the candidate set.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/b6ac7ebb18046966c3e7886af47a307e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5qTxuNj8ZMo_VJQVT-U5w.png"/></div></div></figure><p id="9520" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. Refine the search:<br/>a. Backtrack through the tree, checking if there could be closer points in other branches.<br/>b. Use distance calculations to the maximum and minimum of the unexplored branches to determine if exploring is necessary.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/d7b70deaeed99a10b64698e7be1c231e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T07RAhasZYSPYoqhp9-Nmg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">We backtrack to the branches that has not been visited and check the distance to the minimum and maximum of those node.</figcaption></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/dfb1b7b1b636bee05aa2c2ce53805785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VzBp9APJyUutmaqK_iiv0g.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">As both the minimum and maximum of those nodes are further than kth distance, no need to check the distance to the data points in those nodes.</figcaption></figure><p id="0deb" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">3. Find K nearest neighbors:<br/>a. Among the candidate points found, select the K points closest to the query point.</p><p id="2ddd" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">4. Perform regression:<br/>a. Calculate the average of the target values of the K nearest neighbors.<br/>b. This average is the predicted value for the query point.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/73271b1d2aa3434168ed7b3c7b85b162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHCc1jnQLMiCufH2wTLFHA.png"/></div></div></figure><p id="2ea5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">By using a KD Tree, the average time complexity for finding nearest neighbors can be reduced from <em class="qq">O</em>(<em class="qq">n</em>) in the brute force method to <em class="qq">O</em>(log <em class="qq">n</em>) in many cases, where <em class="qq">n</em> is the number of points in the dataset. This makes KNN Regression much more efficient for large datasets.</p><h1 id="1f6b" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk"><strong class="al">Ball Tree for KNN Regression</strong></h1><p id="3956" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Ball Tree is another space-partitioning data structure that organizes points in a series of nested hyperspheres. Itâ€™s particularly effective for high-dimensional data where KD Trees may become less efficient.</p><p id="e441" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Training Steps:</strong></p><p id="a51f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1. Build the Ball Tree: <br/>a. Calculate the centroid of all points in the node (the mean). This becomes the <strong class="nj ga">pivot point</strong>.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/f4e68143b7ec7469a2c23cc8f59c3d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1plb_3EVUBjmGfWDEe3Zw.png"/></div></div></figure><p id="6d3f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. Make the first branch:<br/>- <strong class="nj ga">Finding the first center:</strong> Choose the furthest point from the pivot point as the first center with its distance as the <strong class="nj ga">radius</strong>.<br/>- <strong class="nj ga">Finding the second center:</strong> From the first center, select the furthest point as the second center.<br/>- <strong class="nj ga">Partitioning:</strong> Divide the remaining points into two child nodes based on which center they are closer to.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/3f318c1ad3e5909f1299034607cbb9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cns2yrObVJEyzveSVIDxw.png"/></div></div></figure><p id="e4a6" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. Recursively apply steps a-b to each child node until a stopping criterion is met, usually minimal leaf size (see <a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/">â€œmin samples leafâ€ here</a>).</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/7a2c7c38531c763643bd4d7f179f03e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ELbCbhKtWzN5NAOVp8m5w.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/466452b8a11ac4c4308f57d663d4c728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1McRTqvlx2DH6BmdOVPbZw.png"/></div></div></figure><p id="0f2a" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. Store the target values:<br/>Along with each point in the Ball Tree, store its corresponding target value. The radius and centroid for each node are also stored.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/9883a4e7158dce6fa0413c88d60c6432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YK4Ln7djS-ld9wf6grXG0Q.png"/></div></div></figure><p id="eb42" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Regression/Prediction Steps:</strong></p><p id="0b17" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1. Traverse the Ball Tree: <br/>a. Start at the root node. <br/>b. At each node, calculate the distance between the unseen data and the center of each child hypersphere. <br/>c. Traverse into the closest hypersphere first. <br/>d. When reaching a leaf node, add its points to the candidate set.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/85e4feaae0d4811c3cc6f20839783835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3q8ssvr4f78TVgd45XZhbA.png"/></div></div></figure><p id="ca95" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. Refine the search:<br/>a. Determine if other branches need to be explored.<br/>b. If the distance to a hypersphere plus its radius is greater than the current Kth nearest distance, that branch can be safely ignored.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/a55da95adf34346e7d47fb4f5f316f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Wdv2YXxnNBwiuOrz8zhlA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">For those branches that we considered before, add the radius to the distance. If it is greater than the kth distance, no need to explore those balls.</figcaption></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/0b1dfe1b420ff207edb0a413d4f3f131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DIjzWAlw7vXGghLxc1d8A.png"/></div></div></figure><p id="457f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">3. Find K nearest neighbors:<br/>a. Among the candidate points found, select the K points closest to the query point.</p><p id="c346" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">4. Perform regression:<br/>a. Calculate the average of the target values of the K nearest neighbors.<br/>b. This average is the predicted value for the query point.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/e2b2f49da2d913d67e259e3e5fa857fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6mqlPHDPYhw4aplGep6-hQ.png"/></div></div></figure><p id="ef7d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Ball Trees can be more efficient than KD Trees for high-dimensional data or when the dimensionality is greater than the log of the number of samples. They maintain good performance even when the number of dimensions increases, making them suitable for a wide range of datasets.</p><p id="feb7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The time complexity for querying in a Ball Tree is O(log <em class="qq">n</em>) on average, similar to KD Trees, but Ball Trees often perform better in higher dimensions where KD Trees may degrade to linear time complexity.</p><h1 id="5e86" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Evaluation Step (Brute Force, KD Tree, Ball Tree)</h1><p id="bcb3" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Regardless of the algorithm we choose, all of them give the same following result:</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/ac80599b3efec37df8746639217d7aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K0FiEZ8BId1YfbPpLJ2MvA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Compared to <a class="af od" href="https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629" rel="noopener">the result of the dummy regressor,</a> there is a major improvement for the value of RMSE.</figcaption></figure><h1 id="c231" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Which Algorithm to Choose?</h1><p id="19ae" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">You can follow this simple rule for choosing the best one:<br/>- For small datasets (&lt; 1000 samples), â€˜bruteâ€™ might be fast enough and guarantees finding the exact nearest neighbors.<br/>- For larger datasets with few features (&lt; 20), â€˜kd_treeâ€™ is usually the fastest.<br/>- For larger datasets with many features, â€˜ball_treeâ€™ often performs best.</p><p id="1d7e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The â€˜autoâ€™ option in scikit-learn typically follow the following chart:</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of py"><img src="../Images/cdea807e4c32673c58a2daa66594870a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eN8UC3jCgVh-q77nPR19mQ.png"/></div></div></figure><h1 id="a4ec" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Key Parameters</h1><p id="75fc" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">While KNN regression has many other parameter, other than the algorithm we just discussed (brute force, kd tree, ball tree), you mainly need to consider</p><ol class=""><li id="d4ca" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qi qj qk bk"><strong class="nj ga">Number of Neighbors (K).</strong><br/>- Smaller K: More sensitive to local patterns, but may lead to overfitting.<br/>- Larger K: Smoother predictions, but might miss important local variations.<br/>Unlike classification, <strong class="nj ga">even numbers are fine</strong> in regression as weâ€™re averaging values.</li><li id="59ed" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Leaf Size</strong><br/>This is the stopping condition for building kd tree or ball tree. Generally, It affects the speed of construction and query, as well as the memory required to store the tree.</li></ol><h1 id="89be" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Pros &amp; Cons</h1><h2 id="ac07" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk">Pros:</h2><ol class=""><li id="5792" class="nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc qi qj qk bk"><strong class="nj ga">Simplicity and Versatility</strong>: Easy to understand and implement; can be used for both classification and regression tasks.</li><li id="2e91" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">No Assumptions</strong>: Doesnâ€™t assume anything about the data distribution, making it suitable for complex datasets.</li><li id="cf9a" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">No Training Phase</strong>: Can quickly incorporate new data without retraining.</li><li id="8c90" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Interpretability</strong>: Predictions can be explained by examining nearest neighbors.</li></ol><h2 id="d9cd" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk"><strong class="al">Cons:</strong></h2><ol class=""><li id="50b5" class="nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc qi qj qk bk"><strong class="nj ga">Computational Complexity</strong>: Prediction time can be slow, especially with large datasets, though optimized algorithms (KD-Tree, Ball Tree) can help for lower dimensions.</li><li id="7aa0" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Curse of Dimensionality</strong>: Performance degrades in high-dimensional spaces, affecting both accuracy and efficiency.</li><li id="eded" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Memory Intensive</strong>: Requires storing all training data.</li><li id="5f49" class="nh ni fq nj b gt ql nl nm gw qm no np nq qn ns nt nu qo nw nx ny qp oa ob oc qi qj qk bk"><strong class="nj ga">Sensitive to Feature Scaling and Irrelevant Features</strong>: Can be biased by features on larger scales or those unimportant to the prediction.</li></ol><h1 id="fcca" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Final Remarks</h1><p id="4ea6" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The K-Nearest Neighbors (KNN) regressor is a basic yet powerful tool in machine learning. Its straightforward approach makes it great for beginners, and its flexibility ensures itâ€™s useful for experts too.</p><p id="eefb" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">As you learn more about data analysis, use KNN to understand the basics of regression before exploring more advanced methods. By mastering KNN and how to compute the nearest neighbors, youâ€™ll build a strong foundation for tackling more complex challenges in data analysis.</p><h1 id="73c5" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">ğŸŒŸ k Nearest Neighbor Regressor Code Summarized</h1><pre class="oh oi oj ok ol pz qa qb bp qc bb bk"><span id="2eab" class="qd oy fq qa b bg qe qf l qg qh"># Import libraries<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'])<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data into features and target, then into training and test sets<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Identify numerical columns<br/>numerical_columns = ['Temperature', 'Humidity']<br/><br/># Create a ColumnTransformer to scale only numerical columns<br/>ct = ColumnTransformer([<br/>    ('scaler', StandardScaler(), numerical_columns)<br/>], remainder='passthrough')<br/><br/># Fit the ColumnTransformer on the training data and transform both training and test data<br/>X_train_transformed = ct.fit_transform(X_train)<br/>X_test_transformed = ct.transform(X_test)<br/><br/># Convert the transformed data back to DataFrames<br/>feature_names = numerical_columns + [col for col in X_train.columns if col not in numerical_columns]<br/>X_train_scaled = pd.DataFrame(X_train_transformed, columns=feature_names, index=X_train.index)<br/>X_test_scaled = pd.DataFrame(X_test_transformed, columns=feature_names, index=X_test.index)<br/><br/># Initialize and train KNN Regressor<br/>knn = KNeighborsRegressor(n_neighbors=5, <br/>                          algorithm='kd_tree', #'ball_tree', 'brute'<br/>                          leaf_size=5) #default is 30<br/>knn.fit(X_train_scaled, y_train)<br/><br/># Make predictions<br/>y_pred = knn.predict(X_test_scaled)<br/><br/># Calculate and print RMSE<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"RMSE: {rmse:.4f}")</span></pre></div></div></div><div class="ab cb rh ri rj rk" role="separator"><span class="rl by bm rm rn ro"/><span class="rl by bm rm rn ro"/><span class="rl by bm rm rn"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c875" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk">Further Reading</h2><p id="c797" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For a detailed explanation of the <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html" rel="noopener ugc nofollow" target="_blank">KNeighborsRegressor</a>, <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html" rel="noopener ugc nofollow" target="_blank">KDTree</a>, <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html" rel="noopener ugc nofollow" target="_blank">BallTree</a>, and its implementation in scikit-learn, readers can refer to their official documentation. It provides comprehensive information on their usage and parameters.</p><h2 id="b52b" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk">Technical Environment</h2><p id="05e3" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="c89d" class="qr oy fq bf oz qs qt qu pc qv qw qx pf nq qy qz ra nu rb rc rd ny re rf rg fw bk">About the Illustrations</h2><p id="953e" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="06e4" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ğ™šğ™œğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rp rq bp rr lw ao"><div class="rs l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rt ru cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rt ru em n ay ue"/></div><div class="rv l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----df5052c8c889--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ry hp l"><h2 class="bf ga xb ic it xc iv iw mz iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xd wd we wf wg lj wh wi up ii wj wk wl ut uu uv ep bm uw ot" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----df5052c8c889--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xe l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sh dz si it ab sj il ed"><div class="ed sb bx sc sd"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sb bx kk se sf"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sg sf"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rp rq bp rr lw ao"><div class="rs l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rt ru cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rt ru em n ay ue"/></div><div class="rv l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----df5052c8c889--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ry hp l"><h2 class="bf ga xb ic it xc iv iw mz iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xd wd we wf wg lj wh wi up ii wj wk wl ut uu uv ep bm uw ot" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----df5052c8c889--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xe l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sh dz si it ab sj il ed"><div class="ed sb bx sc sd"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sb bx kk se sf"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sg sf"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>