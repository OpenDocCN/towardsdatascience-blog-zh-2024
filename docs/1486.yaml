- en: CUDA for AI — Intuitively and Exhaustively Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cuda-for-ai-intuitively-and-exhaustively-explained-6ba6cb4406c5?source=collection_archive---------0-----------------------#2024-06-14](https://towardsdatascience.com/cuda-for-ai-intuitively-and-exhaustively-explained-6ba6cb4406c5?source=collection_archive---------0-----------------------#2024-06-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Artificial Intelligence | GPU Programming | AI Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallelized AI from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page---byline--6ba6cb4406c5--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page---byline--6ba6cb4406c5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6ba6cb4406c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6ba6cb4406c5--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page---byline--6ba6cb4406c5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6ba6cb4406c5--------------------------------)
    ·47 min read·Jun 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf4b3333a98af836a0e396e8fdc92ed4.png)'
  prefs: []
  type: TYPE_IMG
- en: “Thread Master” by Daniel Warfield using Midjourney. All images by the author
    unless otherwise specified. Article originally made available on [Intuitively
    and Exhaustively Explained](https://iaee.substack.com/).
  prefs: []
  type: TYPE_NORMAL
- en: In this article we’ll use CUDA to train an AI model on a GPU, essentially implementing
    AI from scratch, assuming virtually no prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll explore some core components of modern computers, then we’ll dive
    into the GPU to describe what it is, how it works, and why it’s useful for AI.
    We’ll then work through an introduction to CUDA. We’ll describe what CUDA is and
    explain how it allows us to program applications which leverage both the CPU and
    GPU. Once we have an idea of how CUDA programming works, we’ll use CUDA to build,
    train, and test a neural network on a classification task.
  prefs: []
  type: TYPE_NORMAL
- en: '**Who is this useful for?** Anyone who wants to forge a deep and thorough understanding
    of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How advanced is this post?** Given the advanced subject matter, this article
    may be more approachable to those with some machine learning experience. If you
    don’t have machine learning experience you’ll certainly learn a lot by reading
    this article, though. Just take it section by section and Google a lot.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-requisites:** Basic software development skills. Some exposure to C++
    may be helpful but is not required. It…'
  prefs: []
  type: TYPE_NORMAL
