["```py\n# Import the data parser module\nfrom utils.arxiv_parser import *\n\n# Initialize the data parser\nparser = ArXivDataProcessor(data_path)\n\n# Unzip the downloaded file to extract a json file in data_path\nparser.unzip_file()\n\n# Select a topic and extract the articles on that topic\ntopic='cs'\nentries = parser.select_topic('cs')\n\n# Build a pandas dataframe with specified selections\ndf = parser.select_articles(entries, # extracted articles\n                            cols=['id', 'title', 'abstract'], # features to keep\n                            min_length = 100, # min tokens an abstract should have\n                            max_length = 120, # max tokens an abstract should have\n                            keep_abs_length = False, # do not keep the abs_length column\n                            build_corpus=False) # do not build a corpus column\n\n# Save the selected data to a csv file 'selected_{topic}.csv', uses data_path\nparser.save_selected_data(df,topic)\n```", "```py\n# Required installs\n!pip install transformers optimum accelerate\n!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n\n# Required imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load the model and the tokenizer\nmodel_name_or_path = \"TheBloke/zephyr-7B-beta-GPTQ\"\n\nllm = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\") # change revision for a different branch\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, \n                     use_fast=True)\n```", "```py\ngenerator = pipeline(\n    model=llm,\n    tokenizer=tokenizer,\n    task='text-generation',\n    max_new_tokens=50,\n    repetition_penalty=1.1,\n)\n```", "```py\nprompt = \"Tell me about AI\"\nprompt_template=f'''<|system|>\n</s>\n<|user|>\n{prompt}</s>\n<|assistant|>\n'''\n```", "```py\nprompt_keywords= \"\"\"\n<|system|>\nI have the following document:\nSemantics and Termination of Simply-Moded Logic Programs with Dynamic Scheduling\nand five candidate keywords:\nscheduling, logic, semantics, termination, moded\n\nBased on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\nFollow the requirements below:\n1\\. Make sure to extract only the keywords or keyphrases that appear in the text.\n2\\. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n3\\. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!\n\nsemantics, termination, simply-moded, logic programs, dynamic scheduling</s>\n\n<|user|>\nI have the following document:\n[DOCUMENT]\nand five candidate keywords:\n[CANDIDATES]\n\nBased on the information above, extract the keywords or the keyphrases that best describe the topic of the text.\nFollow the requirements below:\n1\\. Make sure to extract only the keywords or keyphrases that appear in the text.\n2\\. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!\n3\\. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!</s>\n\n<|assistant|>\n\"\"\"\n```", "```py\n# Install the required packages\n!pip install keybert\n!pip install sentence-transformers\n\n# The required imports\nfrom keybert.llm import TextGeneration\nfrom keybert import KeyLLM, KeyBERT\nfrom sentence_transformers import SentenceTransformer\n\n# KeyBert TextGeneration pipeline wrapper\nllm_tg = TextGeneration(generator, prompt=prompt_keywords)\n\n# Instantiate KeyBERT and specify an embedding model\nkw_model= KeyBERT(llm=llm_tg, model = \"all-mpnet-base-v2\")\n```", "```py\n# Retain the articles titles only for analysis\ntitles_list = df.title.tolist()\n\n# Process the documents and collect the results\ntitles_keys = kw_model.extract_keywords(titles_list, thresold=0.5)\n\n# Add the results to df\ndf[\"titles_keys\"] = titles_keys\n```", "```py\n# Required installs\n!pip install umap-learn\n!pip install hdbscan\n!pip install -U sentence-transformers\n\n# General imports\nimport pandas as pd\nimport numpy as np\nimport re\nimport pickle\n\n# Imports needed to generate the BERT embeddings\nfrom sentence_transformers import SentenceTransformer\n\n# Libraries for dimensionality reduction\nimport umap.umap_ as umap\n\n# Import the clustering algorithm\nimport hdbscan\n```", "```py\n# Load the data if needed - titles with 5 extracted keywords\ndf5 = pd.read_csv(data_path+parsed_keys_file) \n\n# Create a list of all sublists of keywords and keyphrases\ndf5_keys = df5.titles_keys.tolist()\n\n# Flatten the list of sublists\nflat_keys = [item for sublist in df5_keys for item in sublist]\n\n# Create a list of unique keywords\nflat_keys = list(set(flat_keys))\n\n# Create a dataframe with the distinct keywords\nkeys_df = pd.DataFrame(flat_keys, columns = ['key'])\n```", "```py\n# Instantiate the embedding model\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n# Embed the keywords and keyphrases into 768-dim real vector space\nkeys_df['key_bert'] = keys_df['key'].apply(lambda x: model.encode(x))\n```", "```py\n# Reduce to 10-dimensional vectors and keep the local neighborhood at 15\nembeddings = umap.UMAP(n_neighbors=15, # Balances local vs. global structure.\n                       n_components=10, # Dimension of reduced vectors\n                       metric='cosine').fit_transform(list(keys_df.key_bert))\n\n# Add the reduced embedding vectors to the dataframe\nkeys_df['key_umap'] = embeddings.tolist()\n```", "```py\n# Initialize the clustering model\nclusterer = hdbscan.HDBSCAN(algorithm='best',\n                            prediction_data=True,\n                            approx_min_span_tree=True,\n                            gen_min_span_tree=True,\n                            min_cluster_size=20,\n                            cluster_selection_epsilon = .1,\n                            min_samples=1,\n                            p=None,\n                            metric='euclidean',\n                            cluster_selection_method='leaf')\n\n# Fit the data\nclusterer.fit(embeddings)\n\n# Create soft clusters\nsoft_clusters = hdbscan.all_points_membership_vectors(clusterer)\n\n# Add the soft cluster information to the data\nclosest_clusters = [np.argmax(x) for x in soft_clusters]\nkeys_df['cluster'] = closest_clusters\n```", "```py\ndef extract_description(df: pd.DataFrame,\n                        n: int     \n                        )-> pd.DataFrame:\n    \"\"\"\n    Use a custom prompt to send to a LLM\n    to extract labels and descriptions for a list of keywords.\n    \"\"\"\n\n    one_cluster = df[df['cluster']==n]\n    one_cluster_copy = one_cluster.copy()\n    sample = one_cluster_copy.key.tolist()\n\n    prompt_clusters= f\"\"\"\n    <|system|>\n    I have the following list of keywords and keyphrases:\n    ['encryption','attribute','firewall','security properties',\n    'network security','reliability','surveillance','distributed risk factors',\n    'still vulnerable','cryptographic','protocol','signaling','safe',\n    'adversary','message passing','input-determined guards','secure communication',\n    'vulnerabilities','value-at-risk','anti-spam','intellectual property rights',\n    'countermeasures','security implications','privacy','protection',\n    'mitigation strategies','vulnerability','secure networks','guards']\n\n    Based on the information above, first name the domain these keywords or keyphrases \n  belong to, secondly give a brief description of the domain.\n    Do not use more than 30 words for the description!\n    Do not provide details!\n    Do not give examples of the contexts, do not say 'such as' and do not list the keywords \n  or the keyphrases!\n    Do not start with a statement of the form 'These keywords belong to the domain of' or \n  with 'The domain'.\n\n    Cybersecurity: Cybersecurity, emphasizing methods and strategies for safeguarding digital information\n    and networks against unauthorized access and threats.\n    </s>\n\n    <|user|>\n    I have the following list of keywords and keyphrases:\n    {sample}\n    Based on the information above, first name the domain these keywords or keyphrases belong to, secondly\n    give a brief description of the domain.\n    Do not use more than 30 words for the description!\n    Do not provide details!\n    Do not give examples of the contexts, do not say 'such as' and do not list the keywords or the keyphrases!\n    Do not start with a statement of the form 'These keywords belong to the domain of' or with 'The domain'.\n    <|assistant|>\n    \"\"\"\n\n    # Generate the outputs\n    outputs = generator(prompt_clusters,\n                    max_new_tokens=120,\n                    do_sample=True,\n                    temperature=0.1,\n                    top_k=10,\n                    top_p=0.95)\n\n    text = outputs[0][\"generated_text\"]\n\n    # Example string\n    pattern = \"<|assistant|>\\n\"\n\n    # Extract the output\n    response = text.split(pattern, 1)[1].strip(\" \")\n    # Check if the output has the desired format\n    if len(response.split(\":\", 1)) == 2:\n        label  = response.split(\":\", 1)[0].strip(\" \")\n        description = response.split(\":\", 1)[1].strip(\" \")\n    else:\n        label = description = response\n\n    # Add the description and the labels to the dataframe\n    one_cluster_copy.loc[:, 'description'] = description\n    one_cluster_copy.loc[:, 'label'] = label\n\n    return one_cluster_copy\n```", "```py\nimport re\nimport pandas as pd\n\n# Initialize an empty list to store the cluster dataframes\ndataframes = []\nclusters = len(set(keys_df.cluster))\n\n# Iterate over the range of n values\nfor n in range(clusters-1):\n    df_result = extract_description(keys_df,n)\n    dataframes.append(df_result)\n\n# Concatenate the individual dataframes\nfinal_df = pd.concat(dataframes, ignore_index=True)\n```", "```py\n# Install neo4j\n!pip install neo4j\n\n# Import the connector\nfrom utils.neo4j_conn import *\n\n# Graph DB instance credentials\nURI = 'neo4j+ssc://xxxxxx.databases.neo4j.io'\nUSER = 'neo4j'\nPWD = 'your_password_here'\n\n# Establish the connection to the Neo4j instance\ngraph = Neo4jGraph(url=URI, username=USER, password=PWD)\n```", "```py\n# Load Keyword and Topic nodes, and the relationships HAS_TOPIC\nquery_keywords_topics = \"\"\"\n    UNWIND $rows AS row\n    MERGE (k:Keyword {name: row.key})\n    MERGE (t:Topic {cluster: row.cluster, description: row.description, label: row.label})\n    MERGE (k)-[:HAS_TOPIC]->(t)\n    \"\"\"\ngraph.load_data(query_keywords_topics, keywords)\n\n# Load Article nodes and the relationships HAS_KEY\nquery_articles = \"\"\"\n    UNWIND $rows as row\n    MERGE (a:Article {id: row.id, title: row.title, abstract: row.abstract})\n    WITH a, row\n    UNWIND row.titles_keys as key\n    MATCH (k:Keyword {name: key})\n    MERGE (a)-[:HAS_KEY]->(k)\n    \"\"\"\ngraph.load_data(query_articles, articles)\n```"]