<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building a Vision Inspection CNN for an Industrial Application</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building a Vision Inspection CNN for an Industrial Application</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21">https://towardsdatascience.com/building-a-vision-inspection-cnn-for-an-industrial-application-138936d7a34a?source=collection_archive---------2-----------------------#2024-11-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2429" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A step-by-step approach using PyTorch</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ingo Nowitzky" class="l ep by dd de cx" src="../Images/00d3560055109732b871c001d2b51ab5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ZY-LyKIBI1TcixIzsuRMDA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ingo.nowitzky?source=post_page---byline--138936d7a34a--------------------------------" rel="noopener follow">Ingo Nowitzky</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--138936d7a34a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">28 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="8524" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">In this article, we develop and code a Convolutional Neural Network (CNN) for a vision inspection classification task in the automotive electronics industry. Along the way, we study the concept and math of convolutional layers in depth, and we examine what CNNs actually see and which parts of the image lead them to their decisions.</strong></p><h1 id="c935" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Table of Content</h1><p id="3c75" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Part 1: Conceptual background<br/>Part 2: Defining and coding the CNN<br/>Part 3: Using the trained model in production<br/>Part 4: What did the CNN consider in its “decision”?</p><h1 id="f108" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Part 1: Conceptual background</h1><h2 id="cd0d" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.1 The task: Classify an industrial component as good or scrap</h2><p id="b437" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">In one station of an automatic assembly line, coils with two protruding metal pins have to be positioned precisely in a housing. The metal pins are inserted into small plug sockets. In some cases, the pins are slightly bent and therefore cannot be joined by a machine. It is the task of the visual inspection to identify these coils, so that they can be sorted out automatically.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy oz"><img src="../Images/5638395e5b713df5831c414d81e1e145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2sHQh31o-dMaX4GNyzzvw.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 1: Coils, housing and sockets | image by author</figcaption></figure><p id="53c0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the inspection, each coil is picked up individually and held in front of a screen. In this position, a camera takes a grayscale image. This is then examined by the CNN and classified as good or scrap.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy pq"><img src="../Images/f746b16fb6a040d9c1429f3f50dba39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WFSIPEGWWcXfNs_ofFD1Q.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 2: Basic setup of visual inspection and resulting image | image by author</figcaption></figure><p id="0f80" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we want to define a convolutional neural network that is able to process the images and learn from pre-classified labels.</p><h2 id="5eb8" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.2 What is a Convolutional Neural Network (CNN)?</h2><p id="be85" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Convolutional Neural Networks are a combination of <strong class="ml fr">convolutional filters</strong> followed by a <strong class="ml fr">fully connected Neural Network</strong> (NN). CNNs are often used for <strong class="ml fr">image processing,</strong> like face recognition or visual inspection tasks, like in our case. <strong class="ml fr">Convolutional filters</strong> are matrix operations that slide over the images and recalculate each pixel of the image. We will study convolutional filters later in the article. The <strong class="ml fr">weights</strong> of the filters are <strong class="ml fr">not preset</strong> (as, e.g. the sharpen function in Photoshop) but instead are learned from the data during training.</p><h2 id="eac1" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.3 Architecture of a Convolutional Neural Network</h2><p id="1dea" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Let’s check an example of the architecture of a CNN. For our convenience, we choose the model <strong class="ml fr">we will implement</strong> later.</p></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy pw"><img src="../Images/851e467913c49ffafea1c1dee93d1985.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Ge8zPVMM_79XlVFrdDrbjA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 3: Architecture of our vision inspection CNN | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="da15" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We want to feed the CNN with our inspection images of size 400 px in height and 700 px in width. Since the images are grayscale, the corresponding PyTorch tensor is of size 1x400x700. If we used a colored image, we would have 3 incoming channels: one for red, one for green and one for blue (RGB). In this case the tensor would be <strong class="ml fr">3</strong>x400x700.</p><p id="3b4d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first <strong class="ml fr">convolutional filter</strong> has 6 <strong class="ml fr">kernels</strong> of size 5x5 that slide over the image and produce 6 independent new images, called <strong class="ml fr">feature maps,</strong> of slightly reduced size (6x396x696). The <strong class="ml fr">ReLU activation</strong> is not explicitly shown in Fig. 3. It does not change the dimensions of the tensors but sets all negative values to zero. ReLU is followed by the <strong class="ml fr">MaxPooling</strong> layer with a kernel size of 2x2. It halves the width and height of each image.</p><p id="c89d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All three layers — convolution, ReLU, and MaxPooling — are implemented a second time. This finally brings us 16 feature maps with images of height 97 pixels and width 172 pixels. Next, all the matrix values are flattened and fed into the equally sized first layer of a fully connected neural network. Its second layer is already reduced to 120 neurons. The third and output layer has only 2 neurons: one represents the label “OK”, and the other the label “not OK” or “scrap”.</p><p id="02a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">If you are not yet clear about the changes in the dimensions, please be patient.</strong> We study how the different kinds of layers — convolution, ReLU, and MaxPooling — work in detail and impact the tensor dimensions in the next chapters.</p><h2 id="c43e" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.4 Convolutional filter layers</h2><p id="1ec1" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Convolutional filters have the task of finding <strong class="ml fr">typical structures/patterns</strong> in an image. Frequently used kernel sizes are 3x3 or 5x5. The 9, respectively 25, weights of the kernel are not specified upfront but learned during training (here we assume that we have only one input channel; otherwise, the number of weights multiply by input channels). The kernels slide over the matrix representation of the image (each input channel has its own kernel) with a defined <strong class="ml fr">stride</strong> in the horizontal and vertical directions. The corresponding values of the kernel and the matrix are multiplied and summed up. The summation results of each sliding position form the new image, which we call the <strong class="ml fr">feature map</strong>. We can specify multiple kernels in a convolutional layer. In this case, we receive multiple feature maps as the result. The kernel slides over the matrix from left to right and top to bottom. Therefore, Fig. 4 shows the kernel in its <strong class="ml fr">fifth sliding position</strong> (not counting the “…”). We see three input channels for the colors red, green, and blue (RGB). Each channel has one kernel only. In real applications, we often define multiple kernels per input channel.</p></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy pz"><img src="../Images/c717dce253abdcbf436fbd671ddd02d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tx0JWmrXI6kxT5_llBnXoA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 4: Convolutional layer with 3 input channels and 1 kernel per channel | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8040" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Kernel 1 does its work for the red input channel. In the shown position, we compute the respective new value in the feature map as <em class="qa">(-0.7)*0 + (-0.9)*(-0.2) + (-0.6)*0.5 + (-0.6)*0.6 + 0.6*(-0.3) + 0.7*(-1) + 0*0.7 + (-0.1)*(-0.1) + (-0.2)*(-0.1) = (-1.33).</em> The respective calculation for the green channel (kernel 2) adds up to <em class="qa">-0.14,</em> and for the blue channel (kernel 3) to <em class="qa">0.69</em>. To receive the final value in the feature map for this specific sliding position, we sum up all three channel values and add a bias (bias and all kernel weights are defined during training of the CNN): <em class="qa">(-1.33) + (-0.14) + 0.69 + 0.2 = -0.58</em>. The value is placed in the position of the feature map highlighted in yellow.</p><p id="2475" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Finally, if we compare the size of the input matrices to the size of the feature map, we see that through the kernel operations, we lost two rows in height and two columns in width.</strong></p><h2 id="b76e" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.5 ReLU activation layers</h2><p id="acd2" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">After the convolution, the feature maps are passed through the activation layer. Activation is required to give the network <strong class="ml fr">nonlinear capabilities</strong>. The two most frequently used activation methods are <strong class="ml fr">Sigmoid</strong> and <strong class="ml fr">ReLU</strong> (Rectified Linear Unit). ReLU activation sets all negative values to zero while leaving positive values unchanged.</p></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qb"><img src="../Images/bc0f10b35021ae5d53fa77063e80234c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*tjaJCcac0nZMjPAhVAc3tg.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 5: ReLU activation of the feature map | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c7c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In Fig. 5, we see that the values of the feature map pass the ReLU activation element-wise.</p><p id="d141" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">ReLU activation has no impact on the dimensions of the feature map.</strong></p><h2 id="9dbd" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.6 MaxPooling layers</h2><p id="596c" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Pooling layers have mainly the task of reducing the size of the feature maps while keeping the important information for the classification. In general, we can pool by calculating the <strong class="ml fr">average</strong> of an area in the kernel or returning the <strong class="ml fr">maximum</strong>. MaxPooling is more beneficial in most applications because it <strong class="ml fr">reduces the noise</strong> in the data. Typical kernel sizes for pooling are 2x2 or 3x3.</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qc"><img src="../Images/2210088b4c87a939a7793bfacc32185e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*va4KwkmSC9gm8f9IKG08WQ.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 6: Max Pooling and Average Pooling with a 2x2 kernel | image by author</figcaption></figure><p id="7e45" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In Fig. 6, we see an example of MaxPooling and AvgPooling with a kernel size of 2x2. The feature map is divided into areas of the kernel size, and within those areas, we take either the maximum (→ MaxPooling) or the average (→ AvgPooling).</p><p id="ceef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Through pooling with a kernel size of 2x2, we halve the height and width of the feature map.</strong></p><h2 id="a21f" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">1.7 Tensor dimensions in the Convolutional Neural Network</h2><p id="c857" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Now that we have studied the convolutional filters, the ReLU activation, and the pooling, we can revise Fig. 3 and the dimensions of the tensors. We start with an image of size 400x700. Since it is grayscale, it has only 1 channel, and the corresponding tensor is of size 1x400x700. We apply 6 convolutional filters of size 5x5 with a stride of 1x1 to the image. Each filter returns its own feature map, so we receive 6 of them. Due to the larger kernel compared to Fig. 4 (5x5 instead of 3x3), this time we lose 4 columns and 4 rows in the convolution. This means the returning tensor has the size 6x396x696.</p><p id="2642" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the next step, we apply MaxPooling with a 2x2 kernel to the feature maps (each map has its own pooling kernel). As we have learned, this reduces the maps’ dimensions by a factor of 2. Accordingly, the tensor is now of size 6x198x348.</p><p id="0cef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we apply 16 convolutional filters of size 5x5. Each of them has a kernel depth of 6, which means that each filter provides a separate layer for the 6 channels of the input tensor. Each kernel layer slides over one of the 6 input channels, as studied in Fig. 4, and the 6 returning feature maps are added up to one. So far, we considered only one convolutional filter, but we have 16 of them. That is why we receive 16 new feature maps, each 4 columns and 4 rows smaller than the input. The tensor size is now 16x194x344.</p><p id="4d4e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once more, we apply MaxPooling with a kernel size of 2x2. Since this halves the feature maps, we now have a tensor size of 16x97x172.<br/>Finally, the tensor is flattened, which means we line up all of the <em class="qa">16*97*172 = 266,944</em> values and feed them into a fully connected neural network of corresponding size.</p><h1 id="a949" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Part 2: Defining and coding the CNN</h1><p id="b367" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Conceptually, we have everything we need. Now, let’s go into the industrial use case as described in chapter 1.1.</p><h2 id="d6aa" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.1 Load the required libraries</h2><p id="82d2" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We are going to use a couple of PyTorch libraries for data loading, sampling, and the model itself. Additionally, we load <code class="cx qd qe qf qg b">matplotlib.pyplot</code> for visualization and <code class="cx qd qe qf qg b">PIL</code> for transforming the images.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="54b2" class="qk ng fq qg b bg ql qm l qn qo">import torch<br/>import torch.nn as nn<br/>from torch.utils.data import DataLoader, Dataset<br/>from torch.utils.data.sampler import WeightedRandomSampler<br/>from torch.utils.data import random_split<br/>from torchvision import datasets, transforms<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from PIL import Image<br/>import os<br/>import warnings<br/>warnings.filterwarnings("ignore")</span></pre><h2 id="c2d7" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.2 Configure your device and specify hyperparameters</h2><p id="eda4" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">In <code class="cx qd qe qf qg b">device</code>, we store <code class="cx qd qe qf qg b">‘cuda’</code> or <code class="cx qd qe qf qg b">‘cpu’</code>, depending on whether or not your computer has a GPU available. <code class="cx qd qe qf qg b">minibatch_size</code> defines how many images will be processed in one matrix operation during the training of the model. <code class="cx qd qe qf qg b">learning_rate</code> specifies the magnitude of parameter adjustment during backpropagation, and <code class="cx qd qe qf qg b">epochs</code> defines how often we process the whole set of training data in the training phase.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="501c" class="qk ng fq qg b bg ql qm l qn qo"># Device configuration<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/>print(f"Using {device} device")<br/><br/># Specify hyperparameters<br/>minibatch_size = 10<br/>learning_rate = 0.01<br/>epochs = 60</span></pre><h2 id="2876" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.3 Custom loader function</h2><p id="13b9" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">For loading the images, we define a <code class="cx qd qe qf qg b">custom_loader</code>. It opens the images in binary mode, crops the inner 700x400 pixels of the image, loads them into memory, and returns the loaded images. As the path to the images, we define the relative path <code class="cx qd qe qf qg b">data/Coil_Vision/01_train_val_test</code>. Please make sure that the data is stored in your working directory. You can download the files from my Dropbox as <a class="af qp" href="https://www.dropbox.com/scl/fi/z8kkui6ync57rudlx10dx/CNN_data.zip?rlkey=27787pjnnbaa3mss5nu4nbi7u&amp;st=4q3p56fh&amp;dl=0" rel="noopener ugc nofollow" target="_blank">CNN_data.zip</a>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="d042" class="qk ng fq qg b bg ql qm l qn qo"># Define loader function<br/>def custom_loader(path):<br/>    with open(path, 'rb') as f:<br/>        img = Image.open(f)<br/>        img = img.crop((50, 60, 750, 460))  #Size: 700x400 px<br/>        img.load()<br/>        return img<br/><br/># Path of images (local to accelerate loading)<br/>path = "data/Coil_Vision/01_train_val_test"</span></pre><h2 id="debb" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.4 Define the datasets</h2><p id="0cac" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We define the dataset as tuples consisting of the image data and the label, either <em class="qa">0</em> for scrap parts and <em class="qa">1</em> for good parts. The method <code class="cx qd qe qf qg b">datasets.ImageFolder()</code> reads the labels out of the folder structure. We use a transform function to first load the image data to a PyTorch tensor (values between 0 and 1) and, second, normalize the data with the approximate mean of 0.5 and standard deviation of 0.5. After the transformation, the image data is roughly standard normal distributed (mean = 0, standard deviation = 1). We split the dataset randomly into 50% training data, 30% validation data, and 20% testing data.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="dcda" class="qk ng fq qg b bg ql qm l qn qo"># Transform function for loading<br/>transform = transforms.Compose([transforms.ToTensor(),<br/>                                transforms.Normalize((0.5), (0.5))])<br/><br/># Create dataset out of folder structure<br/>dataset = datasets.ImageFolder(path, transform=transform, loader=custom_loader)<br/>train_set, val_set, test_set = random_split(dataset, [round(0.5*len(dataset)), <br/>                                                      round(0.3*len(dataset)), <br/>                                                      round(0.2*len(dataset))])</span></pre><h2 id="0610" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.5 Balance the datasets</h2><p id="eda7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Our data is unbalanced. We have far more good samples than scrap samples. To reduce a bias towards the majority class during training, we use a <code class="cx qd qe qf qg b">WeightedRandomSampler</code> to give higher probability to the minority class during sampling. In <code class="cx qd qe qf qg b">lbls</code>, we store the labels of the training dataset. With <code class="cx qd qe qf qg b">np.bincount()</code>, we count the number of <em class="qa">0</em> labels (<code class="cx qd qe qf qg b">bc[0]</code>) and <em class="qa">1</em> labels (<code class="cx qd qe qf qg b">bc[1]</code>). Next, we calculate probability weights for the two classes (<code class="cx qd qe qf qg b">p_nOK</code> and <code class="cx qd qe qf qg b">p_OK</code>) and arrange them according to the sequence in the dataset in the list <code class="cx qd qe qf qg b">lst_train</code>. Finally, we instantiate <code class="cx qd qe qf qg b">train_sampler</code> from <code class="cx qd qe qf qg b">WeightedRandomSampler</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="09ea" class="qk ng fq qg b bg ql qm l qn qo"># Define a sampler to balance the classes<br/># training dataset<br/>lbls = [dataset[idx][1] for idx in train_set.indices]<br/>bc = np.bincount(lbls)<br/>p_nOK = bc.sum()/bc[0]<br/>p_OK = bc.sum()/bc[1]<br/>lst_train = [p_nOK if lbl==0 else p_OK for lbl in lbls]<br/>train_sampler = WeightedRandomSampler(weights=lst_train, num_samples=len(lbls))</span></pre><h2 id="feb2" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.6 Define the data loaders</h2><p id="0ffc" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Lastly, we define three data loaders for the training, the validation, and the testing data. Data loaders feed the neural network with batches of datasets, each consisting of the image data and the label.<br/>For the <code class="cx qd qe qf qg b">train_loader</code> and the <code class="cx qd qe qf qg b">val_loader</code>, we set the batch size to 10 and shuffle the data. The <code class="cx qd qe qf qg b">test_loader</code> operates with shuffled data and a batch size of 1.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="c2b9" class="qk ng fq qg b bg ql qm l qn qo"># Define loader with batchsize<br/>train_loader = DataLoader(dataset=train_set, batch_size=minibatch_size, sampler=train_sampler)<br/>val_loader = DataLoader(dataset=val_set, batch_size=minibatch_size, shuffle=True)<br/>test_loader = DataLoader(dataset=test_set, shuffle=True)</span></pre><h2 id="1130" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.7 Check the data: Plot 5 OK and 5 nOK parts</h2><p id="fe31" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">To inspect the image data, we plot five good samples (“OK”) and five scrap samples (“nOK”). To do this, we define a <code class="cx qd qe qf qg b">matplotlib</code> figure with 2 rows and 5 columns and share the x- and the y-axis. In the core of the code snippet, we nest two for-loops. The outer loop receives batches of data from the <code class="cx qd qe qf qg b">train_loader</code>. Each batch consists of ten images and the corresponding labels. The inner loop enumerates the batches’ labels. In its body, we check if the label equals <em class="qa">0</em> — then we plot the image under “nOK” in the second row — or if the label equals <em class="qa">1</em> — then we plot the image under “OK” in the first row. Once <code class="cx qd qe qf qg b">count_OK</code> and <code class="cx qd qe qf qg b">count_nOK</code> both are greater or equal 5, we break the loop, set the title, and show the figure.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="e89c" class="qk ng fq qg b bg ql qm l qn qo"># Figure and axes object<br/>fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20,7), sharey=True, sharex=True)<br/><br/>count_OK = 0<br/>count_nOK = 0<br/><br/># Loop over loader batches<br/>for (batch_data, batch_lbls) in train_loader:<br/>    <br/>    # Loop over batch_lbls<br/>    for i, lbl in enumerate(batch_lbls):<br/>        <br/>        # If label is 0 (nOK) plot image in row 1<br/>        if (lbl.item() == 0) and (count_nOK &lt; 5):<br/>            axs[1, count_nOK].imshow(batch_data[i][0], cmap='gray')<br/>            axs[1, count_nOK].set_title(f"nOK Part#: {str(count_nOK)}", fontsize=14)<br/>            count_nOK += 1<br/>            <br/>        # If label is 1 (OK) plot image in row 0<br/>        elif (lbl.item() == 1) and (count_OK &lt; 5):<br/>            axs[0, count_OK].imshow(batch_data[i][0], cmap='gray')<br/>            axs[0, count_OK].set_title(f"OK Part#: {str(count_OK)}", fontsize=14)<br/>            count_OK += 1<br/>    <br/>    # If both counters are &gt;=5 stop looping<br/>    if (count_OK &gt;=5) and (count_nOK &gt;=5):<br/>        break<br/>        <br/># Config the plot canvas<br/>fig.suptitle("Sample plot of OK and nonOK Parts", fontsize=24)<br/>plt.setp(axs, xticks=[], yticks=[]) <br/>plt.show()</span></pre></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qq"><img src="../Images/085c2afc94e541383abe19d7c03f1578.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*OWvH0dMqHcmXQJRjRePkEA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 7: Example for OK (upper row) and nonOK parts (lower row) | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8d8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In Fig. 7, we see that most of the nOK samples are clearly bent, but a few are not really distinguishable by eye (e.g., lower right sample).</p><h2 id="5d0b" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.8 Define the CNN model</h2><p id="aacd" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The model corresponds to the architecture depicted in Fig. 3. We feed the grayscale image (only one channel) into the first convolutional layer and define 6 kernels of size 5 (equals 5x5). The convolution is followed by a ReLU activation and a MaxPooling with a kernel size of 2 (2x2) and a stride of 2 (2x2). All three operations are repeated with the dimensions shown in Fig. 3. In the final block of the <code class="cx qd qe qf qg b">__init__()</code> method, the 16 feature maps are flattened and fed into a linear layer of equivalent input size and 120 output nodes. It is ReLU activated and reduced to only 2 output nodes in a second linear layer.<br/>In the <code class="cx qd qe qf qg b">forward()</code> method, we simply call the model layers and feed in the <code class="cx qd qe qf qg b">x</code> tensor.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="6ec2" class="qk ng fq qg b bg ql qm l qn qo">class CNN(nn.Module):<br/><br/>    def __init__(self):<br/>        super().__init__()<br/><br/>        # Define model layers<br/>        self.model_layers = nn.Sequential(<br/><br/>            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),<br/>            nn.ReLU(),<br/>            nn.MaxPool2d(kernel_size=2, stride=2),<br/><br/>            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),<br/>            nn.ReLU(),<br/>            nn.MaxPool2d(kernel_size=2, stride=2),<br/><br/>            nn.Flatten(),<br/>            nn.Linear(16*97*172, 120),<br/>            nn.ReLU(),<br/>            nn.Linear(120, 2)<br/>        )<br/>        <br/>    def forward(self, x):<br/>        out = self.model_layers(x)<br/>        return out</span></pre><h2 id="c38e" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.9 Instantiate the model and define the loss function and the optimizer</h2><p id="2ab4" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We instantiate <code class="cx qd qe qf qg b">model</code> from the CNN class and push it either on the CPU or on the GPU. Since we have a classification task, we choose the CrossEntropyLoss function. For managing the training process, we call the Stochastic Gradient Descent (SGD) optimizer.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="0b0b" class="qk ng fq qg b bg ql qm l qn qo"># Define model on cpu or gpu<br/>model = CNN().to(device)<br/><br/># Loss and optimizer<br/>loss = nn.CrossEntropyLoss()<br/><br/>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span></pre><h2 id="cb7d" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.10 Check the model’s size</h2><p id="67b0" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">To get an idea of our model’s size in terms of parameters, we iterate over <code class="cx qd qe qf qg b">model.parameters()</code> and sum up, first, all model parameters (<code class="cx qd qe qf qg b">num_param</code>) and, second, those parameters that will be adjusted during backpropagation (<code class="cx qd qe qf qg b">num_param_trainable</code>). Finally, we print the result.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="7511" class="qk ng fq qg b bg ql qm l qn qo"># Count number of parameters / thereof trainable<br/>num_param = sum([p.numel() for p in model.parameters()])<br/>num_param_trainable = sum([p.numel() for p in model.parameters() if p.requires_grad == True])<br/><br/>print(f"Our model has {num_param:,} parameters. Thereof trainable are {num_param_trainable:,}!")</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qr"><img src="../Images/70cbef867acfafd919547b274f3ec920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4a1H05zkml55uh78eAsRQ.png"/></div></div></figure><p id="3231" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The print out tells us that the model has more than 32 million parameters, thereof all trainable.</p><h2 id="fd9d" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.11 Define a function for validation and testing</h2><p id="b2ef" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Before we start the model training, let’s prepare a function to support the validation and testing. The function <code class="cx qd qe qf qg b">val_test()</code> expects a <code class="cx qd qe qf qg b">dataloader</code> and the CNN <code class="cx qd qe qf qg b">model</code> as parameters. It turns off the gradient calculation with <code class="cx qd qe qf qg b">torch.no_grad()</code> and iterates over the <code class="cx qd qe qf qg b">dataloader</code>. With one batch of images and labels at hand, it inputs the images into the <code class="cx qd qe qf qg b">model</code> and determines the model’s predicted classes with <code class="cx qd qe qf qg b">output.argmax(1)</code> over the returned logits. This method returns the indices of the largest values; in our case, this represents the class indices.<br/>We count and sum up the correct predictions and save the image data, the predicted class, and the labels of the wrong predictions. Finally, we calculate the accuracy and return it together with the misclassified images as the function’s output.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="021d" class="qk ng fq qg b bg ql qm l qn qo">def val_test(dataloader, model):<br/>    # Get dataset size<br/>    dataset_size = len(dataloader.dataset)<br/>    <br/>    # Turn off gradient calculation for validation<br/>    with torch.no_grad():<br/>        # Loop over dataset<br/>        correct = 0<br/>        wrong_preds = []<br/>        for (images, labels) in dataloader:<br/>            images, labels = images.to(device), labels.to(device)<br/>            <br/>            # Get raw values from model<br/>            output = model(images)<br/>            <br/>            # Derive prediction<br/>            y_pred = output.argmax(1)<br/>            <br/>            # Count correct classifications over all batches<br/>            correct += (y_pred == labels).type(torch.float32).sum().item()<br/>            <br/>            # Save wrong predictions (image, pred_lbl, true_lbl)<br/>            for i, _ in enumerate(labels):<br/>                if y_pred[i] != labels[i]:<br/>                    wrong_preds.append((images[i], y_pred[i], labels[i]))<br/><br/>        # Calculate accuracy<br/>        acc = correct / dataset_size<br/>        <br/>    return acc, wrong_preds</span></pre><h2 id="a73e" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.12 Model training</h2><p id="feff" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The model training consists of two nested for-loops. The outer loop iterates over a defined number of <code class="cx qd qe qf qg b">epochs</code>, and the inner loop enumerates the <code class="cx qd qe qf qg b">train_loader</code>. The enumeration returns a batch of image data and the corresponding labels. The image data (<code class="cx qd qe qf qg b">images</code>) is passed to the model, and we receive the model’s response logits in <code class="cx qd qe qf qg b">outputs</code>. <code class="cx qd qe qf qg b">outputs</code> and the true <code class="cx qd qe qf qg b">labels</code> are passed to the loss function. Based on loss <code class="cx qd qe qf qg b">l</code>, we perform backpropagation and update the parameter with <code class="cx qd qe qf qg b">optimizer.step</code>. <code class="cx qd qe qf qg b">outputs</code> is a tensor of dimension <em class="qa">batchsize x output nodes</em>, in our case <em class="qa">10 x 2</em>. We receive the model’s prediction through the indices of the max values over the rows, either <em class="qa">0</em> or <em class="qa">1</em>.</p><p id="ad73" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we count the number of correct predictions (<code class="cx qd qe qf qg b">n_correct</code>), the true OK parts (<code class="cx qd qe qf qg b">n_true_OK</code>), and the number of samples (<code class="cx qd qe qf qg b">n_samples</code>). Each second epoch, we calculate the training accuracy, the true OK share, and call the validation function (<code class="cx qd qe qf qg b">val_test()</code>). All three values are printed for information purpose during the training run. With the last line of code, we save the model with all its parameters in <code class="cx qd qe qf qg b">“model.pth”</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="86f1" class="qk ng fq qg b bg ql qm l qn qo">acc_train = {}<br/>acc_val = {}<br/># Iterate over epochs<br/>for epoch in range(epochs):<br/><br/>    n_correct=0; n_samples=0; n_true_OK=0<br/>    for idx, (images, labels) in enumerate(train_loader):<br/>        model.train()<br/>        # Push data to gpu if available<br/>        images, labels = images.to(device), labels.to(device)<br/>        <br/>        # Forward pass<br/>        outputs = model(images)<br/>        l = loss(outputs, labels)<br/>              <br/>        # Backward and optimize<br/>        optimizer.zero_grad()<br/>        l.backward()<br/>        optimizer.step()<br/><br/>        # Get prediced labels (.max returns (value,index))<br/>        _, y_pred = torch.max(outputs.data, 1)<br/><br/>        # Count correct classifications<br/>        n_correct += (y_pred == labels).sum().item()<br/>        n_true_OK += (labels == 1).sum().item()<br/>        n_samples += labels.size(0)<br/>        <br/>    # At end of epoch: Eval accuracy and print information<br/>    if (epoch+1) % 2 == 0:<br/>        model.eval()<br/>        # Calculate accuracy<br/>        acc_train[epoch+1] = n_correct / n_samples<br/>        true_OK = n_true_OK / n_samples<br/>        acc_val[epoch+1] = val_test(val_loader, model)[0]<br/>        <br/>        # Print info<br/>        print (f"Epoch [{epoch+1}/{epochs}], Loss: {l.item():.4f}")<br/>        print(f"      Training accuracy: {acc_train[epoch+1]*100:.2f}%")<br/>        print(f"      True OK: {true_OK*100:.3f}%")<br/>        print(f"      Validation accuracy: {acc_val[epoch+1]*100:.2f}%")<br/>        <br/># Save model and state_dict<br/>torch.save(model, "model.pth")</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qs"><img src="../Images/a702275ce6449b59a8c30efeb185e881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38ecsrpUnDCJ7tHfEO6Gfw.png"/></div></div></figure><p id="0769" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Training takes a couple of minutes on the GPU of my laptop. It is highly recommended to load the images from the local drive. Otherwise, training time might increase by orders of magnitude!</p><p id="4b88" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The printouts from training inform that the loss has reduced significantly, and the validation accuracy — the accuracy on data the model has not used for updating its parameters — has reached 98.4%.</p><p id="7dee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An even better impression on the training progress is obtained if we plot the training and validation accuracy over the epochs. We can easily do this because we saved the values each second epoch.<br/>We create a <code class="cx qd qe qf qg b">matplotlib</code> figure and axes with <code class="cx qd qe qf qg b">plt.subplots()</code> and plot the values over the keys of the accuracy dictionaries.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="dc99" class="qk ng fq qg b bg ql qm l qn qo"># Instantiate figure and axe object<br/>fig, ax = plt.subplots(figsize=(10,6))<br/>plt.plot(list(acc_train.keys()), list(acc_train.values()), label="training accuracy")<br/>plt.plot(list(acc_val.keys()), list(acc_val.values()), label="validation accuracy")<br/>plt.title("Accuracies", fontsize=24)<br/>plt.ylabel("%", fontsize=14)<br/>plt.xlabel("Epochs", fontsize=14)<br/>plt.setp(ax.get_xticklabels(), fontsize=14)<br/>plt.legend(loc='best', fontsize=14)<br/>plt.show()</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qt"><img src="../Images/9cca05ab59a7fc07400f9869927832f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2SIvePZqTGCjjsl8X0SWXA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 8: Training and validation accuracy during model training | image by author</figcaption></figure><h2 id="7a6d" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.13 Loading the trained model</h2><p id="b182" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">If you want to use the model for production and not only for study purpose, it is highly recommended to save and load the model with all its parameters. Saving was already part of the training code. Loading the model from your drive is equally simple.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="f5a8" class="qk ng fq qg b bg ql qm l qn qo"># Read model from file<br/>model = torch.load("model.pth")<br/>model.eval()</span></pre><h2 id="4a6f" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.14 Double-check the model accuracy with test data</h2><p id="ac6f" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Remember, we reserved another 20% of our data for testing. This data is totally new to the model and has never been loaded before. We can use this brand-new data to double-check the validation accuracy. Since the validation data has been loaded but never been used to update the model parameters, we expect a similar accuracy to the test value. To conduct the test, we call the <code class="cx qd qe qf qg b">val_test()</code> function on the <code class="cx qd qe qf qg b">test_loader</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="f75d" class="qk ng fq qg b bg ql qm l qn qo">print(f"test accuracy: {val_test(test_loader,model)[0]*100:0.1f}%")</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div class="ox oy qu"><img src="../Images/f93ebfbd16df5d984332370c25cf3a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*T9SGHLu5BUaqLbfPIpcS4g.png"/></div></figure><p id="242f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the specific example, we reach a test accuracy of 99.2%, but this is highly dependent on chance (remember: random distribution of images to training, validation, and testing data).</p><h2 id="e8ec" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">2.15 Visualizes the misclassified images</h2><p id="7bea" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The visualization of the misclassified images is pretty straightforward. First, we call the <code class="cx qd qe qf qg b">val_test()</code> function. It returns a tuple with the accuracy value at index position <em class="qa">0</em> (<code class="cx qd qe qf qg b">tup[0]</code>) and another tuple at index position <em class="qa">1</em> (<code class="cx qd qe qf qg b">tup[1]</code>) with the image data (<code class="cx qd qe qf qg b">tup[1][0]</code>), the predicted labels (<code class="cx qd qe qf qg b">tup[1][1]</code>), and the true labels (<code class="cx qd qe qf qg b">tup[1][2]</code>) of the misclassified images. In case <code class="cx qd qe qf qg b">tup[1]</code> is not empty, we enumerate it and plot the misclassified images with appropriate headings.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="d539" class="qk ng fq qg b bg ql qm l qn qo">%matplotlib inline<br/><br/># Call test function<br/>tup = val_test(test_loader, model)<br/><br/># Check if wrong predictions occur<br/>if len(tup[1])&gt;=1:<br/>    <br/>    # Loop over wrongly predicted images<br/>    for i, t in enumerate(tup[1]):<br/>        plt.figure(figsize=(7,5))<br/>        img, y_pred, y_true = t<br/>        img = img.to("cpu").reshape(400, 700)<br/>        plt.imshow(img, cmap="gray")<br/>        plt.title(f"Image {i+1} - Predicted: {y_pred}, True: {y_true}", fontsize=24)<br/>        plt.axis("off")<br/>        plt.show()<br/>        plt.close()<br/>else:<br/>    print("No wrong predictions!")</span></pre><p id="e090" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In our example, we have only one misclassified image, which represents 0.8% of the test dataset (we have 125 test images). The image was classified as OK but has the label nOK. Frankly, I would have misclassified it too :).</p><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qv"><img src="../Images/1dded0c32adb73555618ffa75c2b3bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thCvBPKriILDG8RiqckMEg.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 9: Misclassified image | image by author</figcaption></figure><h1 id="bba5" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Part 3: Using the trained model in production</h1><h2 id="9802" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">3.1 Loading the model, required libraries, and parameters</h2><p id="5aae" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">In the production phase, we assume that the CNN model is trained and the parameters are ready to be loaded. Our aim is to load new images into the model and let it classify whether the respective electronic component is good for assembly or not (see chapter <em class="qa">1.1 The task: Classify an industrial component as good or scrap</em>).</p><p id="8622" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We start by loading the required libraries, setting the device as <code class="cx qd qe qf qg b">‘cuda’</code> or <code class="cx qd qe qf qg b">‘cpu’</code>, defining the class <code class="cx qd qe qf qg b">CNN</code> (exactly as in chapter 2.8), and loading the model from file with <code class="cx qd qe qf qg b">torch.load()</code>. We need to define the class <code class="cx qd qe qf qg b">CNN</code> before loading the parameters; otherwise, the parameters cannot be assigned correctly.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="30ed" class="qk ng fq qg b bg ql qm l qn qo"># Load the required libraries<br/>import torch<br/>import torch.nn as nn<br/>from torch.utils.data import DataLoader, Dataset<br/>from torchvision import datasets, transforms<br/>import matplotlib.pyplot as plt<br/>from PIL import Image<br/>import os<br/><br/># Device configuration<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/># Define the CNN model exactly as in chapter 2.8<br/>class CNN(nn.Module):<br/><br/>    def __init__(self):<br/>        super(CNN, self).__init__()<br/><br/>        # Define model layers<br/>        self.model_layers = nn.Sequential(<br/><br/>            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),<br/>            nn.ReLU(),<br/>            nn.MaxPool2d(kernel_size=2, stride=2),<br/><br/>            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),<br/>            nn.ReLU(),<br/>            nn.MaxPool2d(kernel_size=2, stride=2),<br/><br/>            nn.Flatten(),<br/>            nn.Linear(16*97*172, 120),<br/>            nn.ReLU(),<br/>            nn.Linear(120, 2),<br/>            #nn.LogSoftmax(dim=1)<br/>        )<br/>        <br/>    def forward(self, x):<br/>        out = self.model_layers(x)<br/>        return out<br/><br/># Load the model's parameters<br/>model = torch.load("model.pth")<br/>model.eval()</span></pre><p id="246c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With running this code snippet, we have the CNN model loaded and parameterized in our computer’s memory.</p><h2 id="48bf" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">3.2 Load images into dataset</h2><p id="0b23" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">As for the training phase, we need to prepare the images for processing in the CNN model. We load them from a specified folder, crop the inner 700x400 pixels, and transform the image data to a PyTorch tensor.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="502a" class="qk ng fq qg b bg ql qm l qn qo"># Define custom dataset<br/>class Predict_Set(Dataset):<br/>    def __init__(self, img_folder, transform):<br/>        self.img_folder = img_folder<br/>        self.transform = transform<br/>        self.img_lst = os.listdir(self.img_folder)<br/><br/>    def __len__(self):<br/>        return len(self.img_lst)<br/><br/>    def __getitem__(self, idx):<br/>        img_path = os.path.join(self.img_folder, self.img_lst[idx])<br/>        img = Image.open(img_path)<br/>        img = img.crop((50, 60, 750, 460))  #Size: 700x400<br/>        img.load()<br/>        img_tensor = self.transform(img)<br/>        return img_tensor, self.img_lst[idx]</span></pre><p id="c7af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We perform all the steps in a custom dataset class called <code class="cx qd qe qf qg b">Predict_Set()</code>. In <code class="cx qd qe qf qg b">__init__()</code>, we specify the image folder, accept a <code class="cx qd qe qf qg b">transform</code> function, and load the images from the image folder into the list <code class="cx qd qe qf qg b">self.img_lst</code>. The method <code class="cx qd qe qf qg b">__len__()</code> returns the number of images in the image folder. <code class="cx qd qe qf qg b">__getitem__()</code> composes the path to an image from the folder path and the image name, crops the inner part of the image (as we did for the training dataset), and applies the <code class="cx qd qe qf qg b">transform</code> function to the image. Finally, it returns the image tensor and the image name.</p><h2 id="5256" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">3.3 Path, transform function, and data loader</h2><p id="fb8c" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The final step in data preparation is to define a data loader that allows to iterate over the images for classification. Along the way, we specify the <code class="cx qd qe qf qg b">path</code> to the image folder and define the <code class="cx qd qe qf qg b">transform</code> function as a pipeline that first loads the image data to a PyTorch tensor, and, second, normalizes the data to a range of approximately -1 to +1. We instantiate our custom dataset <code class="cx qd qe qf qg b">Predict_Set()</code> to a variable <code class="cx qd qe qf qg b">predict_set</code> and define the data loader <code class="cx qd qe qf qg b">predict_loader</code>. Since we do not specify a batch size, <code class="cx qd qe qf qg b">predict_loader</code> returns one image at a time.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="df34" class="qk ng fq qg b bg ql qm l qn qo"># Path to images (preferably local to accelerate loading)<br/>path = "data/Coil_Vision/02_predict"<br/><br/># Transform function for loading<br/>transform = transforms.Compose([transforms.ToTensor(),<br/>                                transforms.Normalize((0.5), (0.5))])<br/><br/># Create dataset as instance of custom dataset<br/>predict_set = Predict_Set(path, transform=transform)<br/><br/># Define loader<br/>predict_loader = DataLoader(dataset=predict_set)</span></pre><h2 id="65fc" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">3.4 Custom function for classification</h2><p id="634d" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">So far, the preparation of the image data for classification is complete. However, what we are still missing is a custom function that transfers the images to the CNN model, translates the model’s response into a classification, and returns the classification results. This is exactly what we do with <code class="cx qd qe qf qg b">predict()</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="aa38" class="qk ng fq qg b bg ql qm l qn qo">def predict(dataloader, model):<br/><br/>    # Turn off gradient calculation<br/>    with torch.no_grad():<br/>        <br/>        img_lst = []; y_pred_lst = []; name_lst = []<br/>        # Loop over data loader<br/>        for image, name in dataloader:<br/>            img_lst.append(image)<br/>            image = image.to(device)<br/>            <br/>            # Get raw values from model<br/>            output = model(image)<br/>            <br/>            # Derive prediction<br/>            y_pred = output.argmax(1)<br/>            y_pred_lst.append(y_pred.item())<br/>            name_lst.append(name[0])<br/>            <br/>    return img_lst, y_pred_lst, name_lst</span></pre><p id="6134" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><code class="cx qd qe qf qg b">predict()</code> expects a data loader and the CNN model as its parameters. In its core, it iterates over the data loader, transfers the image data to the model, and interprets the models response with <code class="cx qd qe qf qg b">output.argmax(1)</code> as the classification result — either <em class="qa">0</em> for scrap parts (nOK) or <em class="qa">1</em> for good parts (OK). The image data, the classification result, and the image name are appended to lists and the lists are returned as the function’s result.</p><h2 id="7f14" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">3.5 Predict labels and plot images</h2><p id="7abe" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Finally, we want to utilize our custom functions and loaders to classify new images. In the folder <code class="cx qd qe qf qg b">“data/Coil_Vision/02_predict”</code> we have reserved four images of electronic components that wait to be inspected. Remember, we want the CNN model to tell us whether we can use the components for automatic assembly or if we need to sort them out because the pins are likely to cause problems while trying to push them in the plug sockets.<br/>We call the custom function <code class="cx qd qe qf qg b">predict()</code>, which returns a list of images, a list of classification results, and a list of image names. We enumerate the lists and plot the images with the names and the classification as headings.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="bb4b" class="qk ng fq qg b bg ql qm l qn qo"># Predict labels for images<br/>imgs, lbls, names  = predict(predict_loader, model)<br/><br/># Iterate over classified images<br/>for idx, image in enumerate(imgs):<br/>    plt.figure(figsize=(8,6))<br/>    plt.imshow(image.squeeze(), cmap="gray")<br/>    plt.title(f"\nFile: {names[idx]}, Predicted label: {lbls[idx]}", fontsize=18)<br/>    plt.axis("off")<br/>    plt.show()<br/>    plt.close()</span></pre></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qw"><img src="../Images/c6ae72f8ff91d66a8f74c53bcfde4e82.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*uc4VZ3_CcC6rNl3EKeEFkg.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 10: Classification results in production phase | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4af8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We see that the two images on the left side have been classified a good (label <em class="qa">1</em>) and the two on the right as scrap (label <em class="qa">0</em>). Due to our training data, the model is quite sensitive, and even small bends in the pins lead to them being classified as scrap.</p><h1 id="0bae" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Part 4: What did the CNN consider in its “decision”?</h1><p id="43b7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We have gone deep into the details of the CNN and our industrial use case so far. This seems like a good opportunity to take one step further and try to understand what the CNN model “sees” while processing the image data. To do this, we first study the convolutional layers and then examine which parts of the image are specifically important for the classification.</p><h2 id="7438" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">4.1 Study the convolutional filters’ dimensions</h2><p id="126e" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">To gain a better understanding of how convolutional filters work and what they do to the images, let’s examine the layers in our industrial example in more detail.</p><p id="595b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To access the layers, we enumerate <code class="cx qd qe qf qg b">model.children()</code>, which is a generator for the model’s structure. If the layer is a convolutional layer, we append it to the list <code class="cx qd qe qf qg b">all_layers</code> and save the weights’ dimensions in <code class="cx qd qe qf qg b">conv_weights</code>. If we have a ReLU or a MaxPooling layer, we have no weights. In this case, we append the layer and <em class="qa">“*”</em> to the respective lists. Next, we enumerate <code class="cx qd qe qf qg b">all_layers</code>, print the layer type, and the weights’ dimensions.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="9479" class="qk ng fq qg b bg ql qm l qn qo"># Empty lists to store the layers and the weights<br/>all_layers = []; conv_weights = []<br/><br/># Iterate over the model's structure<br/># (First level nn.Sequential)<br/>for _, layer in enumerate(list(model.children())[0]):<br/>    if type(layer) == nn.Conv2d:<br/>        all_layers.append(layer)<br/>        conv_weights.append(layer.weight)<br/>    elif type(layer) in [nn.ReLU, nn.MaxPool2d]:<br/>        all_layers.append(layer)<br/>        conv_weights.append("*")<br/><br/># Print layers and dimensions of weights<br/>for idx, layer in enumerate(all_layers):<br/>    print(f"{idx+1}. Layer: {layer}")<br/>    if type(layer) == nn.Conv2d:<br/>        print(f"          weights: {conv_weights[idx].shape}")<br/>    else:<br/>        print(f"          weights: {conv_weights[idx]}")<br/>    print()</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qv"><img src="../Images/ca5c83dbeeab4d0d96467902c05f25d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ThvrZYSqewL3CVlZqA6sA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 11: Layers and weights’ dimensions</figcaption></figure><p id="9e48" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Please compare the code snippet’s output with Fig. 3. The first convolutional layer has one input — the original image with only one channel — and returns six feature maps. We apply six kernels, each of depth one and size 5x5. Correspondingly, the weights are of dimension <code class="cx qd qe qf qg b">torch.Size([6, 1, 5, 5])</code>. In contrast, layer 4 receives six feature maps as input and returns 16 maps as output. We apply 16 convolutional kernels, each of depth 6 and size 5x5. The weights’ dimension is therefore <code class="cx qd qe qf qg b">torch.Size([16, 6, 5, 5])</code>.</p><h2 id="c914" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">4.2 Visualize the convolutional filters’ weights</h2><p id="583c" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Now, we know the convolutional filters’ dimensions. Next, we want to see their weights, which they have gained during the training process. Since we have so many different filters (six in the first convolutional layer and 16 in the second), we select, in both cases, the first input channel (index <em class="qa">0</em>).</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="7400" class="qk ng fq qg b bg ql qm l qn qo">import itertools<br/><br/># Iterate through all layers<br/>for idx_out, layer in enumerate(all_layers):<br/>    <br/>    # If layer is a convolutional filter<br/>    if type(layer) == nn.Conv2d:<br/>        <br/>        # Print layer name<br/>        print(f"\n{idx_out+1}. Layer: {layer} \n")<br/>        <br/>        # Prepare plot and weights<br/>        plt.figure(figsize=(25,6))<br/>        weights = conv_weights[idx_out][:,0,:,:] # only first input channel<br/>        weights = weights.detach().to('cpu')<br/>        <br/>        # Enumerate over filter weights (only first input channel)<br/>        for idx_in, f in enumerate(weights):<br/>            plt.subplot(2,8, idx_in+1)<br/>            plt.imshow(f, cmap="gray")<br/>            plt.title(f"Filter {idx_in+1}")<br/>            <br/>            # Print texts<br/>            for i, j in itertools.product(range(f.shape[0]), range(f.shape[1])):<br/>                if f[i,j] &gt; f.mean():<br/>                    color = 'black'<br/>                else:<br/>                    color = 'white'<br/>                plt.text(j, i, format(f[i, j], '.2f'), horizontalalignment='center', verticalalignment='center', color=color)<br/>            <br/>            plt.axis("off")<br/>        plt.show()<br/>        plt.close()       </span></pre><p id="6dd2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We iterate over <code class="cx qd qe qf qg b">all_layers</code>. If the layer is a convolutional layer (<code class="cx qd qe qf qg b">nn.Conv2d</code>), then we print the layer’s index and the layer’s core data. Next, we prepare a plot and extract the weights matrix for the first input layer as an example. We enumerate all output layers and plot them with <code class="cx qd qe qf qg b">plt.imshow()</code>. Finally, we print the weights’ values on the image so that we get an intuitive visualization of the convolutional filters.</p></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qx"><img src="../Images/50934d1828dab3a74cf8b06eb2896806.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*GxV1ZXVfBuyxxkIyY97LUA.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 12: Visualization of the 6+16 convolutional filters (input layer index 0) | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9074" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Fig. 12 shows the six convolutional filter kernels of layer 1 and the 16 kernels of layer 4 (for input channel <em class="qa">0</em>). The model schematic in the upper right indicates the filters with a red outline. We see that the majority of values are close to 0, and some are in the range of positive or negative 0.20–0.25. The numbers represent the values used for the convolution demonstrated in Fig. 4. This gives us the feature maps, which we inspect next.</p><h2 id="fd1e" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">4.3 Examine the feature maps</h2><p id="e84e" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">According to Fig. 4, we receive the first feature maps through the convolution of the input image. Therefore, we load a random image from the <code class="cx qd qe qf qg b">test_loader</code> and push it to the CPU (in case you operate the CNN on the GPU).</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="b674" class="qk ng fq qg b bg ql qm l qn qo"># Test loader has a batch size of 1<br/>img = next(iter(test_loader))[0].to(device)<br/>print(f"\nImage has shape: {img.shape}\n")<br/><br/># Plot image<br/>img_copy = img.to('cpu')<br/>plt.imshow(img_copy.reshape(400,700), cmap="gray")<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="pa pb pc pd pe pf ox oy paragraph-image"><div class="ox oy qy"><img src="../Images/96f8a1bc70ca7968e0edb54d93932387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*aYCO2KQis-lzOHCsrSbapA.png"/></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 13: Random image as output of above code | image by author</figcaption></figure><p id="28e7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we pass the image data <code class="cx qd qe qf qg b">img</code> through the first convolution layer (<code class="cx qd qe qf qg b">all_layers[0]</code>) and save the output in <code class="cx qd qe qf qg b">results</code>. Next, we iterate over <code class="cx qd qe qf qg b">all_layers</code> and feed the next layer with the output from the previous layer operation. Those operations are convolutions, ReLU activations or MaxPoolings. The output of each operation we append to <code class="cx qd qe qf qg b">results</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="f6d0" class="qk ng fq qg b bg ql qm l qn qo"># Pass the image through the first layer<br/>results = [all_layers[0](img)]<br/><br/># Pass the results of the previous layer to the next layer<br/>for idx in range(1, len(all_layers)):  # Start at 1, first layer already passed!<br/>    results.append(all_layers[idx](results[-1]))  # Pass the last result to the layer</span></pre><p id="c38c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we plot the original image, the feature maps after passing the first layer (convolution), the second layer (ReLU), the third layer (MaxPooling), the forth layer (2nd convolution), the fifth layer (2nd ReLU), and the sixth layer (2nd MaxPooling).</p></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy qz"><img src="../Images/b900b20f062aa67d15afdef6e1d59a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*yo9KsNnDELLjOjoEZ8IORQ.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy ra"><img src="../Images/57693ff63b838e80604a9863903d5b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rqQlKRcGSV3KkyvlM3Z-wA.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy rb"><img src="../Images/ca87184666e2ce86b0b433a7ebd3a147.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*iKECMSZxPH_5lWfN42oJ4w.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy rc"><img src="../Images/631942ee46e10f6bb60caf9d733933ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2vWmXh5SK1qrby7ns5HIbQ.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy rd"><img src="../Images/0693ac0798cf3c69b9a9d025d4039bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2yDZ5BVLFYKBeRMGd1YkRw.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy re"><img src="../Images/e6ae0d8816b7018a826fbcf3db74b4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*x_2bGxg-ChrhCBJz9oflHQ.png"/></div></div></figure><figure class="lb pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy re"><img src="../Images/2c85133da17c727830b0dd668fbfc08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*oVrS1FBCfFVhUPHSdNp7Zg.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 14: Original image and feature maps after passing the convolution, ReLU and MaxPooling layers | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c70c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We see that the convolutional kernels (compare Fig. 12) recalculate each pixel of the image. This appears as changed grayscale values in the feature maps. Some of the feature maps are sharpened compared to the original image or have a stronger black-and-white contrast, while others seem to be faded.</p><p id="4f26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The ReLU operations turn dark gray into black since negative values are set to zero.</p><p id="d1dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">MaxPooling keeps the images almost unchanged while halving the image size in both dimensions.</p><h2 id="dc33" class="og ng fq bf nh oh oi oj nk ok ol om nn ms on oo op mw oq or os na ot ou ov ow bk">4.4 Visualize the image areas that impact the classification the most</h2><p id="82e6" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Before we finish, let’s analyze which areas of the image are particularly decisive for the classification into scrap (index <em class="qa">0</em>) or good parts (index <em class="qa">1</em>). For this purpose, we use Gradient-weighted Class Activation Mapping (gradCAM). This technique computes the gradients of the trained model with respect to the predicted class (the gradients show how much the inputs — the image pixels — influence the prediction). The averages of the gradients of each feature map (= output channel of convolution layer) build the weights with which the feature maps are multiplied when calculating a heat map for visualization.<br/>But let’s look at one step after the other.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="98f4" class="qk ng fq qg b bg ql qm l qn qo">def gradCAM(x):<br/>    <br/>    # Run model and predict<br/>    logits = model(x)<br/>    pred = logits.max(-1)[-1] # Returns index of max value (0 or 1)<br/>    <br/>    # Fetch activations at final conv layer<br/>    last_conv = model.model_layers[:5]<br/>    activations = last_conv(x)<br/><br/>    # Compute gradients with respect to model's prediction<br/>    model.zero_grad()<br/>    logits[0,pred].backward(retain_graph=True)<br/>    <br/>    # Compute average gradient per output channel of last conv layer<br/>    pooled_grads = model.model_layers[3].weight.grad.mean((1,2,3))<br/>    <br/>    # Multiply each output channel with its corresponding average gradient<br/>    for i in range(activations.shape[1]):<br/>        activations[:,i,:,:] *= pooled_grads[i]<br/>    <br/>    # Compute heatmap as average over all weighted output channels<br/>    heatmap = torch.mean(activations, dim=1)[0].cpu().detach()<br/>    <br/>    return heatmap</span></pre><p id="f9b5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We define a function <code class="cx qd qe qf qg b">gradCAM</code> that expects the input data <code class="cx qd qe qf qg b">x</code>, an image or a feature map, and returns a <code class="cx qd qe qf qg b">heatmap</code>.</p><p id="a438" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the first block, we input <code class="cx qd qe qf qg b">x</code> in the CNN <code class="cx qd qe qf qg b">model</code> and receive <code class="cx qd qe qf qg b">logits</code>, a tensor of shape [1, 2] with two values only. The values represent the predicted probabilities of the classes <em class="qa">0</em> and <em class="qa">1</em>. We select the index of the larger value as the model’s prediction <code class="cx qd qe qf qg b">pred</code>.</p><p id="3e27" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the second block, we extract the first five layers of the model — from first convolution to second ReLU — and save them to <code class="cx qd qe qf qg b">last_conv</code>. We run <code class="cx qd qe qf qg b">x</code> through the selected layers and store the output in <code class="cx qd qe qf qg b">activations</code>. As the name suggests, those are the activations (=feature maps) of the second convolutional layer (after ReLU activation).</p><p id="a058" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the third block, we do the backward propagation for the logit value of the predicted class <code class="cx qd qe qf qg b">logits[0,pred]</code>. In other words, we compute all the gradients of the CNN with respect to the prediction. The gradients show how much a change in the input data — the original image pixels — impact the models output — the prediction. The result is saved in the PyTorch computational graph until we delete it with <code class="cx qd qe qf qg b">model.zero_grad()</code>.</p><p id="476c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the fourth block, we compute the averages of the gradients over the input channel, as well as the height and width of the image or the feature maps. As a result, we receive 16 average gradients for the 16 feature maps that are returned from the second convolutional layer. We save them in <code class="cx qd qe qf qg b">pooled_grads</code>.</p><p id="ba7e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the fifth block, we iterate over the 16 feature maps returned from the second convolutional layer and weight them with the average gradients <code class="cx qd qe qf qg b">pooled_grads</code>. This operation gives more impact to those feature maps (and their pixels) that have high importance for the prediction and vice versa. From now on, <code class="cx qd qe qf qg b">activations</code> holds not the feature maps, but the weighted feature maps.</p><p id="aa73" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, in the last block, we compute the <code class="cx qd qe qf qg b">heatmap</code> as the average feature map of all <code class="cx qd qe qf qg b">activations</code>. This is what the function <code class="cx qd qe qf qg b">gradCAM</code> returns.</p><p id="1c8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before we can plot the image and the heatmap, we need to transform both for the overlay. Remember, the feature maps are smaller than the original picture (see chapters 1.3 and 1.7), and so is the heatmap. This is why we need the function <code class="cx qd qe qf qg b">upsampleHeatmap()</code>. The function scales the pixel values to the range of 0 to 255 and transforms them to 8-bit integer format (required from the <code class="cx qd qe qf qg b">cv2</code> library). It resizes the heatmap to 400x700 px and applies a color map to both the image and heatmap. Finally, we overlay 70% heatmap and 30% image and return the composition for plotting.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="a588" class="qk ng fq qg b bg ql qm l qn qo">import cv2<br/><br/>def upsampleHeatmap(map, img):<br/>    m,M = map.min(), map.max()<br/>    i,I = img.min(), img.max()<br/>    map = 255 * ((map-m) / (M-m))<br/>    img = 255 * ((img-i) / (I-i))<br/>    map = np.uint8(map)<br/>    img = np.uint8(img)<br/>    map = cv2.resize(map, (700,400))<br/>    map = cv2.applyColorMap(255-map, cv2.COLORMAP_JET)<br/>    map = np.uint8(map)<br/>    img = cv2.applyColorMap(255-img, cv2.COLORMAP_JET)<br/>    img = np.uint8(img)<br/>    map = np.uint8(map*0.7 + img*0.3)<br/>    return map</span></pre><p id="d019" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We want to plot the original image and the heatmap overlay next to each other in one row. To do this, we iterate over the data loader <code class="cx qd qe qf qg b">predict_loader</code>, run the <code class="cx qd qe qf qg b">gradCAM()</code> function on the images and the <code class="cx qd qe qf qg b">upsampleHeatmap()</code> function on the heatmap and the image. Finally, we plot the original image and the heatmap in a row with <code class="cx qd qe qf qg b">matplotlib.pyplot</code>.</p><pre class="pa pb pc pd pe qh qg qi bp qj bb bk"><span id="c68f" class="qk ng fq qg b bg ql qm l qn qo"># Iterate over dataloader<br/>for idx, (image, name) in enumerate(predict_loader):<br/>    <br/>    # Compute heatmap<br/>    image = image.to(device)<br/>    heatmap = gradCAM(image)<br/>    image = image.cpu().squeeze(0).permute(1,2,0)<br/>    heatmap = upsampleHeatmap(heatmap, image)<br/>    <br/>    # Plot images and heatmaps<br/>    fig = plt.figure(figsize=(14,5))<br/>    fig.suptitle(f"\nFile: {names[idx]}, Predicted label: {lbls[idx]}\n", fontsize=24)<br/>    plt.subplot(1, 2, 1)<br/>    plt.imshow(image, cmap="gray")<br/>    plt.title(f"Image", fontsize=14)<br/>    plt.axis("off")<br/>    plt.subplot(1, 2, 2)<br/>    plt.imshow(heatmap)<br/>    plt.title(f"Heatmap", fontsize=14)<br/>    plt.tight_layout()<br/>    plt.axis("off")<br/>    plt.show()<br/>    plt.close()</span></pre></div></div><div class="pf"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="pa pb pc pd pe pf px py paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="ox oy rf"><img src="../Images/e8ac8b63fb261f64205230e96bdc5e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*fEO-HvNouTYmF4MUUYXSVQ.png"/></div></div><figcaption class="pl pm pn ox oy po pp bf b bg z dx">Fig. 15: Image and heatmap (inner two rows of the output) | image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9434" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The blue areas of the heatmap have low impact on the model’s decision, while the yellow and red areas are very important. We see that in our use case, mainly the contour of the electronic component (in particular the metal pins) is decisive for the classification into scrap or good parts. Of course, this is highly reasonable, given that the use case primarily deals with bent pins.</p><h1 id="e7d7" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h1><p id="d3d7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Convolutional Neural Networks (CNNs) are nowadays a common and widely used tool for visual inspection tasks in the industrial environment. In our use case, with relatively few lines of code, we managed to define a model that classifies electronic components as good parts or scrap with high precision. The big advantage, compared to classic approaches of vision inspection, is that no process engineer needs to specify visual marks in the images for the classification. Instead, the CNN learns from labeled examples and is able to replicate this knowledge to other images. In our specific use case, 626 labeled images were sufficient for training and validation. In more complex cases, the demand for training data might be significantly higher.</p><p id="269f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Algorithms like gradCAM (Gradient-weighted Class Activation Mapping) significantly help in understanding which areas in the image are particularly relevant for the model’s decision. In this way, they support a broad use of CNNs in the industrial context by building trust in the model’s functionality.</p><p id="0fcf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, we have explored many details of the inner workings of Convolutional Neural Networks. I hope you enjoyed the journey and have gained a deep understanding of how CNNs work.</p></div></div></div></div>    
</body>
</html>