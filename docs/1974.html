<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unleashing the Power of Triton: Mastering GPU Kernel Optimization in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Unleashing the Power of Triton: Mastering GPU Kernel Optimization in Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e?source=collection_archive---------7-----------------------#2024-08-13">https://towardsdatascience.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e?source=collection_archive---------7-----------------------#2024-08-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5baf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Accelerating AI/ML Model Training with Custom Operators — Part 2</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--160a3f52701e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--160a3f52701e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--160a3f52701e--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--160a3f52701e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/e2dd5b644c445a53fd9b98179de44e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nn1MUTkC2oLm_DzU"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@jasrolyn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jas Rolyn</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e609" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">According to Greek mythology, Triton, a god of the sea, would calm or stir the sea waters by using his conch shell to control its tides and waves. In one story, in particular, Triton is depicted as having used his powers to guide <a class="af nc" href="https://en.wikipedia.org/wiki/Argonauts" rel="noopener ugc nofollow" target="_blank">the Argonauts</a> through particularly dangerous sea waters. In this post, we similarly call upon Triton for navigation through complex journeys, although this time we refer to the <a class="af nc" href="https://github.com/triton-lang/triton" rel="noopener ugc nofollow" target="_blank">Triton language</a> and compiler for writing deep learning (DL) kernels and to our journeys through the world of AI/ML development.</p><p id="49fe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is a sequel to a <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a> on the topic of accelerating AI/ML applications with custom operators in which we demonstrated the potential for performance optimization by developing custom CUDA kernels. One of our intentions was to emphasize the accessibility of custom kernel development and the opportunities it provides even for non-expert CUDA developers. However, there are challenges to CUDA development that may prove insurmountable for some. For one, while many a modern-day AI/ML developer are well-versed in Python, they may not feel comfortable developing in C++. Furthermore, tuning a CUDA kernel to take full advantage of the GPU’s capabilities requires an intimate understanding of the underlying HW architecture and could take a non-trivial amount of work. This is particularly true if you want your kernel to run optimally on a variety of GPU architectures. Much of the complexity results from CUDA’s “thread-based” development model in which the developer is responsible for designing and optimizing all elements of the GPU kernel threads, including all details related to the use of GPU memory, thread-concurrency, TensorCore scheduling, and much more.</p><h2 id="97a5" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">The Power of Triton</h2><p id="7924" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The Triton library aims to democratize and simplify GPU kernel development in two primary ways. First, it provides an API for building custom operators in <em class="oz">Python</em> (rather than C++). Second, it enables kernel development at the <em class="oz">block</em> level (rather than the thread level) thereby abstracting away and automating all issues related to optimizing performance <em class="oz">within</em> CUDA thread blocks. Rather than taking the laborious steps of programming the details of the thread invocation, including the intricacies related to memory management, scheduling of on-chip acceleration engines, thread-synchronization, etc., kernel developers can rely on Triton to do it all for them. One important byproduct of the high-level API abstraction of Triton’s programming model is that it reduces the burden of needing to tune the kernel for multiple different GPU types and architectures.</p><p id="9d28" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Of course, as is usually the case when up-leveling an API, the Triton programming model does have its disadvantages. Some kernels might benefit from the thread-level control enabled by CUDA (e.g., they might benefit from the conditional execution flow discussed in our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a>). Other kernels might require very specialized and delicate treatment to reach peak performance and may suffer from the automated result of the Triton compiler. But even in cases such as these, where the development of a CUDA kernel may ultimately be required, the ability to quickly and easily create a temporary Triton kernel could greatly facilitate development and boost productivity.</p><p id="d693" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For more on the motivations behind Triton and on the details of its programming model, see the <a class="af nc" href="https://openai.com/index/triton/" rel="noopener ugc nofollow" target="_blank">Triton announcement</a>, the official <a class="af nc" href="https://triton-lang.org/main/programming-guide/chapter-1/introduction.html#motivations" rel="noopener ugc nofollow" target="_blank">Triton documentation</a>, and the original <a class="af nc" href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf" rel="noopener ugc nofollow" target="_blank">Triton white-paper</a>.</p><h2 id="953e" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Disclaimers</h2><p id="0f72" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Similar to our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a>, our intention is to provide a simple demonstration of the opportunity offered by Triton. Please do not view this post as a replacement for the official <a class="af nc" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">Triton documentation</a> or its <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/index.html" rel="noopener ugc nofollow" target="_blank">associated tutorials</a>. We will use the same face-detection model as in our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a> as a basis for our demonstration and perform our experiments in the same Google Cloud environment — a <a class="af nc" href="https://cloud.google.com/compute/docs/gpus#l4-gpus" rel="noopener ugc nofollow" target="_blank">g2-standard-16</a> VM (with a single L4 GPU) with a dedicated <a class="af nc" href="https://cloud.google.com/deep-learning-vm/docs/release-notes" rel="noopener ugc nofollow" target="_blank">deep learning VM image</a> and PyTorch 2.4.0. As before, we make no effort to optimize our examples and/or verify their robustness, durability, or accuracy. It should be noted that although we will perform our experiments on a PyTorch model and on an NVIDIA GPU, Triton kernel development is supported by additional frameworks and underlying HWs.</p><h1 id="ced6" class="pa oa fq bf ob pb pc gq of pd pe gt oj pf pg ph pi pj pk pl pm pn po pp pq pr bk">Triton as a Component of Torch Compilation</h1><p id="1568" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a>) we demonstrated the use of <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch compilation</a> and its potential impact on runtime performance. The default compiler used by the <a class="af nc" href="https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler" rel="noopener ugc nofollow" target="_blank">torch.compiler</a> is <a class="af nc" href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747" rel="noopener ugc nofollow" target="_blank">TorchInductor</a> which relies heavily on Triton kernels for its GPU acceleration. Thus, it seems only appropriate that we begin our Triton exploration by assessing the automatic Triton-backed optimization afforded by <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a>. The code block below includes the same forward pass of the face detection model we introduced in our previous post along with the compiled GIOU loss function. For the sake of brevity, we have omitted some of the supporting code. Please refer to our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a> for the full implementation.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="2f58" class="pw oa fq pt b bg px py l pz qa"><br/>def loss_with_padding(pred, targets):<br/>    mask = (targets[...,3] &gt; 0).to(pred.dtype)<br/>    total_boxes = mask.sum()<br/>    loss = generalized_box_iou(targets, pred)<br/>    masked_loss = loss*mask<br/>    loss_sum = masked_loss.sum()<br/>    return loss_sum/torch.clamp(total_boxes, 1)<br/><br/><br/>device = torch.device("cuda:0")<br/>model = torch.compile(Net()).to(device).train()<br/>loss_fn = torch.compile(loss_with_padding)<br/><br/># forward portion of training loop wrapped with profiler object<br/>with torch.profiler.profile(<br/>   schedule=torch.profiler.schedule(wait=5, warmup=5, active=10, repeat=1)<br/>) as prof:<br/>    for step, data in enumerate(train_loader):<br/><br/>        with torch.profiler.record_function('copy data'):<br/>            images, boxes = data_to_device(data, device)<br/>            torch.cuda.synchronize(device)<br/><br/>        with torch.profiler.record_function('forward'):<br/>            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):<br/>                outputs = model(images)<br/>            torch.cuda.synchronize(device)<br/><br/>        with torch.profiler.record_function('calc loss'):<br/>            loss = loss_fn(outputs, boxes)<br/>            torch.cuda.synchronize(device)<br/>        prof.step()<br/>        if step &gt; 30:<br/>            break<br/><br/>    # filter and print profiler results<br/>    event_list = prof.key_averages()<br/>    for i in range(len(event_list) - 1, -1, -1):<br/>        if event_list[i].key not in ['forward', 'calc loss', 'copy data']:<br/>            del event_list[i]<br/>    print(event_list.table())</span></pre><p id="7858" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The performance results (averaged over multiple runs) are captured below:</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="229c" class="pw oa fq pt b bg px py l pz qa">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      56.868ms       5.687ms<br/>      forward        1.329s     132.878ms<br/>    calc loss       8.282ms     828.159us<br/>-------------  ------------  ------------</span></pre><p id="c2c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Recall that the average time of the original loss function (on padded input) was 1.844ms. Thus the performance boost resulting from torch compilation is greater than 2X(!!).</p><p id="54a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Triton kernels automatically generated by torch.compile can actually be viewed by setting the <code class="cx qb qc qd pt b">TORCH_LOGS</code> environment variable, as explained in <a class="af nc" href="https://pytorch.org/tutorials/recipes/torch_logs.html" rel="noopener ugc nofollow" target="_blank">this PyTorch tutorial</a>. In fact, some have proposed the use of these kernels as a starting point for Triton development (e.g., see <a class="af nc" href="https://discuss.pytorch.org/t/choice-of-torch-compile-vs-triton/195604" rel="noopener ugc nofollow" target="_blank">here</a>). However, in our experience these kernels can be somewhat difficult to decipher.</p><p id="4230" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next section we will attempt to further improve on the results of PyTorch compilation by implementing a GIOU Triton kernel.</p><h1 id="83e0" class="pa oa fq bf ob pb pc gq of pd pe gt oj pf pg ph pi pj pk pl pm pn po pp pq pr bk">Creating a Custom Triton Kernel</h1><p id="74cb" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">A great place to start your Triton development journey is with the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/index.html" rel="noopener ugc nofollow" target="_blank">official Triton tutorials</a>. The tutorials are introduced in incremental order of complexity, with each one expanding on one or more of Triton’s unique features. Our GIOU Triton kernel most closely resembles the most basic <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py" rel="noopener ugc nofollow" target="_blank">vector addition</a> example. As in our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">CUDA implementation</a>, we assign a block to each sample in the input batch, and program it to operate on all of the bounding boxes in the sample. Note the use of <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">tl.load</a> and <a class="af nc" href="https://triton-lang.org/main/python-api/generated/triton.language.store.html" rel="noopener ugc nofollow" target="_blank">tl.store</a> for reading and writing data from and to memory, as well as the block programs use of vectorized arithmetic.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="9704" class="pw oa fq pt b bg px py l pz qa">import triton<br/>import triton.language as tl<br/><br/>@triton.jit<br/>def giou_kernel(preds_ptr,<br/>                targets_ptr,<br/>                output_ptr,<br/>                valid_ptr,<br/>                BLOCK_SIZE: tl.constexpr):<br/>    pid = tl.program_id(axis=0)<br/>    box_id = tl.arange(0, BLOCK_SIZE)<br/>    <br/>    box_offsets = pid * BLOCK_SIZE + box_id<br/>    <br/>    preds_left = tl.load(preds_ptr + 0 + 4 * box_offsets)<br/>    preds_top = tl.load(preds_ptr + 1 + 4 * box_offsets)<br/>    preds_right = tl.load(preds_ptr + 2 + 4 * box_offsets)<br/>    preds_bottom = tl.load(preds_ptr + 3 + 4 * box_offsets)<br/>    <br/>    gt_left = tl.load(targets_ptr + 0 + 4 * box_offsets)<br/>    gt_top = tl.load(targets_ptr + 1 + 4 * box_offsets)<br/>    gt_right = tl.load(targets_ptr + 2 + 4 * box_offsets)<br/>    gt_bottom = tl.load(targets_ptr + 3 + 4 * box_offsets)<br/>    <br/>    epsilon = 1e-5<br/>    <br/>    # Compute the area of each box<br/>    area1 = (preds_right - preds_left) * (preds_bottom - preds_top)<br/>    area2 = (gt_right - gt_left) * (gt_bottom - gt_top)<br/>    <br/>    # Compute the intersection<br/>    left = tl.maximum(preds_left, gt_left)<br/>    top = tl.maximum(preds_top, gt_top)<br/>    right = tl.minimum(preds_right, gt_right)<br/>    bottom = tl.minimum(preds_bottom, gt_bottom)<br/>    <br/>    inter_w = tl.maximum(right - left, 0)<br/>    inter_h = tl.maximum(bottom - top, 0)<br/>    inter_area = inter_w * inter_h<br/>    <br/>    union_area = area1 + area2 - inter_area<br/>    <br/>    iou_val = inter_area / tl.maximum(union_area, epsilon)<br/>    <br/>    # Compute the smallest enclosing box<br/>    enclose_left = tl.minimum(preds_left, gt_left)<br/>    enclose_top = tl.minimum(preds_top, gt_top)<br/>    enclose_right = tl.maximum(preds_right, gt_right)<br/>    enclose_bottom = tl.maximum(preds_bottom, gt_bottom)<br/>    <br/>    enclose_w = tl.maximum(enclose_right - enclose_left, 0)<br/>    enclose_h = tl.maximum(enclose_bottom - enclose_top, 0)<br/>    enclose_area = enclose_w * enclose_h<br/>    <br/>    # Compute GIOU<br/>    delta_area = (enclose_area - union_area)<br/>    enclose_area = tl.maximum(enclose_area, epsilon)<br/>    giou = iou_val - delta_area / enclose_area<br/>    <br/>    # Store results<br/>    tl.store(output_ptr + (box_offsets),<br/>             tl.where(gt_bottom &gt; 0, giou, 0))<br/>    tl.store(valid_ptr + (box_offsets), gt_bottom &gt; 0)<br/><br/><br/>def loss_with_triton(pred, targets):<br/>    batch_size = pred.shape[0]<br/>    n_boxes = pred.shape[1]<br/>    <br/>    # convert to float32 (remove to keep original dtypes)<br/>    pred = pred.to(torch.float32)<br/>    targets = targets.to(torch.float32)<br/><br/>    # allocate output tensors<br/>    output = torch.empty_strided(pred.shape[0:2], <br/>                                 stride=(n_boxes,1),<br/>                                 dtype = pred.dtype,<br/>                                 device = pred.device)<br/>    valid = torch.empty_strided(pred.shape[0:2],<br/>                                stride=(n_boxes,1),<br/>                                dtype = torch.bool,<br/>                                device = pred.device)<br/> <br/>    # call Triton kernel<br/>    giou_kernel[(batch_size,)](pred, targets, output, valid,<br/>                               BLOCK_SIZE=n_boxes)<br/><br/>    total_valid = valid.sum()<br/>    loss_sum = output.sum()<br/>    return loss_sum/total_valid.clamp(1)</span></pre><p id="a5a1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The results of running with our Triton kernel are captured below. While somewhat worse than in our previous experiment, this could be a result of additional optimizations performed by torch.compile.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="ecb3" class="pw oa fq pt b bg px py l pz qa">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      57.089ms       5.709ms<br/>      forward        1.338s     133.771ms<br/>    calc loss       8.908ms     890.772us<br/>-------------  ------------  ------------</span></pre><p id="d056" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Following the recommendation of PyTorch’s documentation on <a class="af nc" href="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html" rel="noopener ugc nofollow" target="_blank">the use of Triton kernels</a>, we further assess the performance of our kernel, this time in combination with <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch compilation</a>. The results (averaged over multiple runs) are slightly better than the auto-compiled loss of our first experiment.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="3cb6" class="pw oa fq pt b bg px py l pz qa">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      57.008ms       5.701ms<br/>      forward        1.330s     132.951ms<br/>    calc loss       7.189ms     718.869us<br/>-------------  ------------  ------------</span></pre><p id="6053" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When developing <a class="af nc" rel="noopener" target="_blank" href="/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12">our custom GIOU CUDA kernel</a>, we noted the overhead of converting the input tensors to float32, and the need to enhance our kernel to support various input types in order to avoid this conversion. In the case of our Triton kernel this can be accomplished quite easily by simply removing the conversion operations. The custom kernel will be auto-generated (JIT-compiled) with the original types.</p><pre class="mm mn mo mp mq ps pt pu bp pv bb bk"><span id="aeec" class="pw oa fq pt b bg px py l pz qa">-------------  ------------  ------------<br/>         Name     CPU total  CPU time avg<br/>-------------  ------------  ------------<br/>    copy data      57.034ms       5.703ms<br/>      forward        1.325s     132.456ms<br/>    calc loss       6.219ms     621.950us<br/>-------------  ------------  ------------</span></pre><p id="3910" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our final results are on par with CUDA kernel results that we saw in our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a>.</p><h1 id="1f76" class="pa oa fq bf ob pb pc gq of pd pe gt oj pf pg ph pi pj pk pl pm pn po pp pq pr bk">Results</h1><p id="f4b7" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The following table summarizes the results of our experimentation. The results were averaged over multiple runs due to some variance that we observed. We have included the results of our custom CUDA kernel from our <a class="af nc" href="https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12" rel="noopener">previous post</a>, for reference. Keep in mind that the comparative results are likely to vary greatly based on the details of the kernel and the runtime environment.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/0e85a9ec9d7a80826abc22c86f922dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Y8eB2xAj-VvFpnXZSJZdZA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Summary of Average of Loss Runtimes (by Author)</figcaption></figure><p id="656d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While our first Triton kernel experiment resulted in reduced performance, compared to our custom CUDA operator, by applying compilation and removing the data type conversions, we were able to match its speed.</p><p id="df35" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These findings are in line with what one might expect from Triton: On the one hand, its high-level API abstraction implies a certain loss of control over the low-level flow which could result in reduced runtime performance. On the other hand, the (relative) simplicity and power of its APIs enable users to close the performance gap by implementing features with much greater ease than in CUDA.</p><p id="d986" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One could make a strong argument that the Triton kernel we chose to evaluate is what <a class="af nc" href="https://openai.com/index/triton/" rel="noopener ugc nofollow" target="_blank">the documentation</a> would refer to as “embarrassingly parallel”, i.e., comprised of element-wise operations, and that as such, is a terrible kernel on which to demonstrate the value of Triton. Indeed, a more complex program, requiring more sophisticated memory management, scheduling, synchronization, etc., may be required to showcase the full power of Triton.</p><h1 id="e72d" class="pa oa fq bf ob pb pc gq of pd pe gt oj pf pg ph pi pj pk pl pm pn po pp pq pr bk">Next Steps</h1><p id="fb45" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Several additional steps are required to complete our task. These include tuning our custom kernel and implementing the backward function.</p><h2 id="e112" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">1. Kernel Optimization</h2><p id="1de8" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Although, Triton abstracts away a lot of the low-level kernel optimization, there remain many controls that could greatly impact runtime performance. These include the size of each block, the number of thread warps to use (as demonstrated in the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py" rel="noopener ugc nofollow" target="_blank">softmax tutorial</a>), and how L2 memory is accessed (see the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication tutorial</a> for an example of <em class="oz">swizzling</em>). Triton includes an <a class="af nc" href="https://triton-lang.org/main/python-api/generated/triton.autotune.html" rel="noopener ugc nofollow" target="_blank">autotuning</a> feature for optimizing the choice of hyper-parameters (as demonstrated in the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html" rel="noopener ugc nofollow" target="_blank">matrix multiplication tutorial</a> and in the <a class="af nc" href="https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html#advanced-usage" rel="noopener ugc nofollow" target="_blank">PyTorch Triton example</a>). Although we have omitted autotuning from our example, it is an essential step of Triton kernel development.</p><h2 id="42a5" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">2. Backward Pass Implementation</h2><p id="03a5" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We have limited our example to just the forward pass of the GIOU loss function. A full solution would require creating a kernel for the backward pass, as well (as demonstrated in the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html" rel="noopener ugc nofollow" target="_blank">layer normalization</a> tutorial). This is usually a bit more complicated than the forward pass. One may wonder why the high-level kernel development API exposed by Triton does not address this challenge by supporting automatic differentiation. As it turns out, for reasons that are beyond the scope of this post (e.g., see <a class="af nc" href="https://jax.readthedocs.io/en/latest/pallas/design.html#grad-of-pallas-call" rel="noopener ugc nofollow" target="_blank">here</a>), automatic differentiation of custom kernels is extremely difficult to implement. Nonetheless, this would be an absolute killer of a feature for Triton and we can only hope that this will be supported at some point in the future.</p><h1 id="4427" class="pa oa fq bf ob pb pc gq of pd pe gt oj pf pg ph pi pj pk pl pm pn po pp pq pr bk">Summary</h1><p id="9c20" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Triton is easily one of the most important and impactful AI/ML libraries of the past few years. While it is difficult to assess the amount of innovation and progress it has enabled in the field of AI, its footprints can be found everywhere — from the core implementation of PyTorch 2 and its dependencies, to the <a class="af nc" href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py" rel="noopener ugc nofollow" target="_blank">specialized attention layers</a> within the advanced LLM models that are slowly perforating our every day lives.</p><p id="c21b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Triton’s popularity is owed to its innovative programming model for kernel development. Once limited to the domain of CUDA experts, Triton makes creating customized DL primitives accessible to every Python developer.</p><p id="89b2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post we have only touched the surface of Triton and its capabilities. Be sure to check out the Triton’s online <a class="af nc" href="https://triton-lang.org/main/index.html" rel="noopener ugc nofollow" target="_blank">documentation</a> and other <a class="af nc" href="https://github.com/triton-lang/triton" rel="noopener ugc nofollow" target="_blank">resources</a> to learn more.</p></div></div></div></div>    
</body>
</html>