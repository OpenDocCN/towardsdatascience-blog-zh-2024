- en: The Ultimate Guide to RAGs — Each Component Dissected
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29](https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A visual tour of what it takes to build production-ready LLM pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    ·12 min read·Oct 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e75abdffaf2c2c493ffd5ea61939f229.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s learn RAGs! (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: If you have worked with Large Language Models, there is a great chance that
    you have at least heard the term RAG — Retrieval Augmented Generation. The idea
    of RAGs are pretty simple — suppose you want to ask a question to a LLM, instead
    of just relying on the LLM’s pre-trained knowledge, you first retrieve relevant
    information from an external knowledge base. This retrieved information is then
    provided to the LLM along with the question, allowing it to generate a more informed
    and up-to-date response.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c29b45a46c05f489f1ce51b45fa9711a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing standard LLM calls with RAG (Source: Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**So, why use Retrieval Augmented Generation?**'
  prefs: []
  type: TYPE_NORMAL
- en: When providing **accurate and up-to-date information** is key, you cannot rely
    on the LLM’s inbuilt knowledge. RAGs are a cheap practical way to use LLMs to
    generate content about recent topics or niche topics **without needing to finetune**
    them on your own and burn away your life’s savings. Even when LLMs internal knowledge
    may be enough to answer questions, it might be a good idea to use RAGs anyway,
    [since recent studies have shown that they could help reduce LLMs hallucinations.](https://arxiv.org/html/2404.08189v1)
  prefs: []
  type: TYPE_NORMAL
- en: The different components of a bare-bones RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the advanced portion of this article, let’s review the basics.
    Generally RAGs consist of two pipelines — **preprocessing and inferencing**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Inferencing** is all about using data from your existing database to answer
    questions from a user query. **Preprocessing** is the process of setting up the
    database in the correct way so that retrieval is done correctly later on.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a diagramatic look into the entire basic barebones RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39ef29751baffd2a51ab35e9ebba7411.png)'
  prefs: []
  type: TYPE_IMG
- en: The Basic RAG pipeline (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The Indexing or Preprocessing Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the offline preprocessing stage, where we would set up our database.
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify Data Source**: Choose a relevant data source based on the application,
    such as Wikipedia, books, or manuals. Since this is domain dependent, I am going
    to skip over this step in this article. Go choose any data you want to use, knock
    yourself out!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking the Data**: Break down the dataset into smaller, manageable documents
    or chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Convert to Searchable Format**: Transform each chunk into a numerical vector
    or similar searchable representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Insert into Database**: Store these searchable chunks in a custom database,
    though external databases or search engines could also be used.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Inferencing Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the Query Inferencing stage, the following components stand out.
  prefs: []
  type: TYPE_NORMAL
- en: '**Query Processing**: A method to convert the user’s query into a format suitable
    for search.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieval/Search Strategy**: A similarity search mechanism to retrieve the
    most relevant documents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Post-Retrieval Answer Generation**: Use retrieved documents as context to
    generate the answer with an LLM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Great — so we identified several key modules required to build a RAG. Believe
    it or not, each of these components have a lot of additional research to make
    this simple RAG turn into CHAD-rag. Let’s look into each of the major components
    in this list, starting with chunking.
  prefs: []
  type: TYPE_NORMAL
- en: This article is based on this Youtube video
  prefs: []
  type: TYPE_NORMAL
- en: '***By the way, this article is based on this 17-minute Youtube video I made
    on the same topic, covering all the topics in this article. Feel free to check
    it out after reading this Medium article!***'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chunking is the process of breaking down large documents into smaller, manageable
    pieces. It might sound simple, but trust me, the way you chunk your data can make
    or break your RAG pipeline. Whatever chunks you create during preprocessing will
    eventually get retrieved during inference. If you make the size of chunks too
    small — like each sentence — then it might be difficult to retrieve them through
    search because they capture very little information. If the chunk size is too
    big — like inserting entire Wikipedia articles — the retrieved passages might
    end up confusing the LLM because you are sending large bodies of texts at once.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/967b63a03b974ae50ac5aabda8f455f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Depending on the different levels of chunking, your results may vary! (Image
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: Some frameworks use LLMs to do chunking, for example by extracting simple factoids
    or propositions from the text corpus, and treat them as documents. This could
    be expensive because the larger your dataset, the more LLM calls you’ll have to
    make.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b84d0c25979742b7fa6f815c18b4b9c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Proposition Chunking (Source: [Dense X Retrieval Paper](https://arxiv.org/pdf/2312.06648).
    License: Free)'
  prefs: []
  type: TYPE_NORMAL
- en: Structural Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/ac673609cd4ba4dd7f4e383864696009.png)'
  prefs: []
  type: TYPE_IMG
- en: If your data has inherent boundaries (like HTML or Code), sometimes it is best
    to just utilize it. (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Quite often we may also deal with datasets that inherently have a known structure
    or format. For example, if you want to insert code into your database, you can
    simply split each script by the function names or class definitions. For HTML
    pages like Wikipedia articles, you can split by the heading tags — for example,
    split by the H2 tags to isolate each sub-chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: But there are some glaring issues with the types of chunking we have discussed
    so far. Suppose your dataset consists of tens of thousands of paragraphs extracted
    from all Sherlock Holmes books. Now the user has queried something general like
    what was the first crime in Study in Scarlet? What do you think is going to happen?
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that since each documented is an isolated piece of information,
    we don’t know which chunks are from the book Study in Scarlet. Therefore, later
    on during retrieval, we will end up fetch a bunch of passages about the topic
    “crime” without knowing if it’s relevant to the book. To resolve this, we can
    use something known as contextual chunking.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enter Contextual Chunking**'
  prefs: []
  type: TYPE_NORMAL
- en: '[A recent blogpost](https://anthropic.com/news/contextual-retrieval) from Anthropic
    describes it as prepending chunk-specific explanatory context to each chunk before
    embedding. Basically, while we are indexing, we would also include additional
    information relevant to the chunk — like the name of the book, the chapter, maybe
    a summary of the events in the book. Adding this context will allow the retriever
    to find references to Study in Scarlett and crimes when searching, hopefully getting
    the right documents from the database!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/094e3126d94897eb876a84b373778c79.png)'
  prefs: []
  type: TYPE_IMG
- en: Contextual Chunking adds additional information to the chunks than just the
    text body (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: There are other ways to solve the problem of finding the right queries — like
    metadata filtering, We will talk about this later when we talk about Databases.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Data Conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we come to the data-conversion stage. Note that whatever strategy we used
    to convert the documents during preprocessing, we need to use it to search for
    similarity later, so these two components are tightly coupled.
  prefs: []
  type: TYPE_NORMAL
- en: Two of the most common approaches that have emerged in this space are **embedding
    based methods** and **keyword-frequency based methods** like TF-IDF or BM-25.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll start with embedding-based methods. Here, we use pretrained transformer
    models to transform the text into high-dimensional vector representations, capturing
    semantic meaning about the text. Embeddings are great for capturing semantic relationships,
    handling synonyms, and understanding context-dependent meanings. However, embedding
    can be computationally intensive, and can sometimes overlook exact matches that
    simpler methods would easily catch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eea86572fe8b84983f6e3fb254176d90.png)'
  prefs: []
  type: TYPE_IMG
- en: Embeddings (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: When does Semantic Search fail?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For example, suppose you have a database of manuals containing information about
    specific refrigerators. When you ask a query mentioning a very specific niche
    model or a serial number, embeddings will fetch documents that kind of resemble
    your query, but may fail to exactly match it. This brings us to the alternative
    of embeddings retrieval — keyword based retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword Based Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Two popular keyword-based methods are TF-IDF and BM25\. These algorithms focus
    on statistical relationships between terms in documents and queries.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF weighs the importance of a word based on its frequency in a document
    relative to its frequency in the entire corpus. Every document in our dataset
    is be represented by a array of TF-IDF scores for each word in the vocabulary.
    The indices of the high values in this document vector tell us which words that
    are likely to be most characteristic of that document’s content, because these
    words appear more frequently in this document and less frequently in others. For
    example, the documents related to this Godrej A241gX , will have a high TF-IDF
    score for the phrase Godrej and A241gX, making it more likely for us to retrieve
    this using TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/030fa185bf92ef2d88efe95f98253679.png)'
  prefs: []
  type: TYPE_IMG
- en: TF-IDF relies on the ratio of the occurence of terms in a document compared
    to the entire corpus. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: BM25, an evolution of TF-IDF, incorporates document length normalization and
    term saturation. Meaning that it adjusts the TF-IDF score based on if the document
    itself is longer or shorter than the average document length in the collection.
    Term saturation means that as a particular word appears too often in the database,
    it’s importance decreases.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF and BM-25 are great finding documents with specific keyword occurrences
    when they exactly occur. And embeddings are great for finding documents with similar
    semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '**A common thing these days is to retrieve using both keyword and embedding
    based methods, and combine them, giving us the best of both worlds.** Later on
    when we discuss Reciprocal Rank Fusion and Deduplication, we will look into how
    to combine these different retrieval methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up next, let’s talk about Databases. The most common type of database that is
    used in RAGs are **Vector Databases**. Vector databases store documents by indexing
    them with their vector representation, be in from an embedding, or TF-IDF. Vector
    databases specialize in fast similarity check with query vectors, making them
    ideal for RAG. Popular vector databases that you may want to look into are Pinecone,
    Milvus, ChromaDB, MongoDB, and they all have their pros and cons and pricing model.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to vector databases are **graph databases**. Graph databases
    store information as a network of documents with each document connected to others
    through relationships.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf61de385aadd84baf03ba5bd4f69fa7.png)'
  prefs: []
  type: TYPE_IMG
- en: Modern Vector Databases allow attribute filtering with semantic search (Image
    by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Many modern vector and graph database also allow properties from relational
    databases, most notably metadata or attribute filtering. If you know the question
    is about the 5th Harry Potter book, it would be really nice to filter your entire
    database first to only contain documents from the 5th Harry Potter book, and not
    run embeddings search through the entire dataset. Optimal metadata filtering in
    Vector Databases is a pretty amazing area in Computer Science research, and a
    seperate article would be best for a in-depth discussion about this.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Query transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Next, let’s move to inferencing starting with query transformation — which
    is any preprocessing step we do to the user’s actual query before doing any similarity
    search. Think of it like improving the user’s question to get better answers.**'
  prefs: []
  type: TYPE_NORMAL
- en: In general, we want to avoid searching directly with the user query. User inputs
    are usually very noisy and they can type random stuff — we want an additional
    transformation layer that interprets the user query and turns it into a search
    query.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example why Query Rewriting is important
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common technique to do this transformation is query rewriting. Imagine
    someone asks, “**What happened to the artist who painted the Mona Lisa?**” If
    we do semantic or keyword searches, the retrieved information will be all about
    the Mona Lisa, not about the artist. A query rewriting system would use an LLM
    to rewrite this query. The LLM might transform this into “**Leonardo da Vinci
    Mona Lisa artist**”, which will be a much fruitful search.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ef4f1d430fde82b990b925ff0b9bdab.png)'
  prefs: []
  type: TYPE_IMG
- en: Direct Retrieval vs Query Rewriting (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we would also use **Contextual Query Writing**, where we might use
    additional contexts, like using the older conversation transcript from the user,
    or if we know that our application covers documents from 10 different books, maybe
    we can have a classifier LLM that classifies the user query to detect which of
    the 10 books we are working with. If our database is in a different language,
    we can also translate the query.
  prefs: []
  type: TYPE_NORMAL
- en: '[There are also powerful techniques like HYDE](https://arxiv.org/abs/2212.10496),
    which stands for **Hypothetical Document Embedding**. HYDE uses a language model
    to generate a hypothetical answer to the query, and do similarity search with
    this hypothetical answer to retrieve relevant documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89020eaaea924ddb06e90b8dc7705e20.png)'
  prefs: []
  type: TYPE_IMG
- en: Hypothetical Document Embeddings (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Another technique is **Multi-Query Expansion** where we generate multiple queries
    from the single user query and perform parallel searches to retrieve multiple
    sets of documents. The received documents can then later go through a de-duplication
    step or rank fusion to remove redundant documents.
  prefs: []
  type: TYPE_NORMAL
- en: A recent approach called [**Astute RAG**](https://arxiv.org/abs/2410.07176)tries
    to consolidate externally input knowledge with the LLM’s own internal knowledge
    before generating answers. There are also **Multi-Hop techniques like Baleen programs**.
    They work by performing an initial search, analyzing the top results to find frequently
    co-occurring terms, and then adding these terms to the original query. This adaptive
    approach can help bridge the vocabulary gap between user queries and document
    content, and help retrieve better documents.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Post Retrieval Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve retrieved our potentially relevant documents, we can add another
    post-retrieval processing step before feeding information to our language model
    for generating the answer.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can do information selection and emphasis, where an LLM selects
    portion of the retrieved documents that could be useful for finding the answer.
    We might highlight key sentences, or do semantic filtering where we remove unimportant
    paragraphs, or do context summarization by fusing multiple documents into one.
    The goal here is to avoid overwhelming our LLM with too much information, which
    could lead to less focused or accurate responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1afd67da02e55c50540e235d5695a19d.png)'
  prefs: []
  type: TYPE_IMG
- en: You can use smaller LLMs to flag relevant info from retrieved documents before
    consolidating the context prompt for the final LLM call (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Often we do multiple queries with query expansion, or use multiple retrieval
    algorithms like Embeddings+BM-25 to separately fetch multiple documents. To remove
    duplicates, we often use reranking methods like Reciprocal Rank Fusion. RRF combines
    the rankings from all the different approaches, giving higher weight to documents
    that consistently rank well across multiple methods. In the end, the top K high
    ranking documents are passed to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7640eabc4d48854d1a9da9a4ecd3379.png)'
  prefs: []
  type: TYPE_IMG
- en: Reciprocal Rank Fusion is a classic Search Engine algorithm to combine item
    ranks obtained from multiple ranking algorithms (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '[FLARE or forward-looking active retrieval augmented generation](https://arxiv.org/abs/2305.06983)
    is an iterative post-retrieval strategy. Starting with the user input and initial
    retrieval results, an LLM iteratively guesses the next sentence. Then we check
    if the generated guess contains any low probability tokens indicated here with
    an underline — if so, we call the retriever to retrieve useful documents from
    the dataset and make necessary corrections.'
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a more visual breakdown of the different components of RAGs, do checkout
    my Youtube video on this topic. The field of LLMs and RAGs are rapidly evolving
    — a thorough understanding of the RAG framework is incredibly essential to appreciate
    the pros and cons of each approach and weigh which approaches work best for YOUR
    use-case. The next time you are thinking of designing a RAG system, do stop and
    ask yourself these questions —
  prefs: []
  type: TYPE_NORMAL
- en: What are my data sources?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should I chunk my data? Is there inherent structure that comes with my data
    domain? Do my chunks need additional context (contextual chunking)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do I need semantic retrieval (embeddings) or more exact-match retrieval (BM-25)?
    What type of queries am I expecting from the user?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What database should I use? Is my data a graph? Does it need metadata-filtering?
    How much money do I want to spend on databases?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can I best rewrite the user query for easy search hits? Can an LLM rewrite
    the queries? Should I use HYDE? If LLMs already have enough domain knowledge about
    my target field, can I use Astute?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can I combine multiple different retrieval algorithms and then do rank fusion?
    (honestly, just do it if you can afford it cost-wise and latency-wise)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out my Youtube channel where I post content about Deep Learning, Machine
    Learning, Paper Reviews, Tutorials, and just about anything related to AI (except
    news, there are WAY too many Youtube channels for AI news). Here are some of my
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Youtube Channel:** [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Patreon:** [https://www.patreon.com/c/NeuralBreakdownwithAVB](https://www.patreon.com/c/NeuralBreakdownwithAVB)'
  prefs: []
  type: TYPE_NORMAL
- en: Give me a follow on Medium and a clap if you enjoyed this!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vector Databases: [https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Metadata Filtering: [https://www.pinecone.io/learn/vector-search-filtering/](https://www.pinecone.io/learn/vector-search-filtering/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contextual Chunking: [https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Propositions / Dense X Retrieval: [https://arxiv.org/pdf/2312.06648](https://arxiv.org/pdf/2312.06648)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hypothetical Document Embeddigs (HYDE): [https://arxiv.org/abs/2212.10496](https://arxiv.org/abs/2212.10496)'
  prefs: []
  type: TYPE_NORMAL
- en: 'FLARE: [https://arxiv.org/abs/2305.06983](https://arxiv.org/abs/2305.06983)'
  prefs: []
  type: TYPE_NORMAL
