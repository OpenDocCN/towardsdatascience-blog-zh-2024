- en: The Ultimate Guide to RAGs — Each Component Dissected
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAGs 终极指南——每个组件解析
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29](https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29](https://towardsdatascience.com/the-ultimate-guide-to-rags-each-component-dissected-3cd51c4c0212?source=collection_archive---------0-----------------------#2024-10-29)
- en: A visual tour of what it takes to build production-ready LLM pipelines
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建生产级 LLM 管道所需的视觉导览
- en: '[](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[![Avishek
    Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--3cd51c4c0212--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    ·12 min read·Oct 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3cd51c4c0212--------------------------------)
    ·12分钟阅读·2024年10月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e75abdffaf2c2c493ffd5ea61939f229.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e75abdffaf2c2c493ffd5ea61939f229.png)'
- en: Let’s learn RAGs! (Image by Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来学习 RAGs！（图片来自作者）
- en: If you have worked with Large Language Models, there is a great chance that
    you have at least heard the term RAG — Retrieval Augmented Generation. The idea
    of RAGs are pretty simple — suppose you want to ask a question to a LLM, instead
    of just relying on the LLM’s pre-trained knowledge, you first retrieve relevant
    information from an external knowledge base. This retrieved information is then
    provided to the LLM along with the question, allowing it to generate a more informed
    and up-to-date response.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过大型语言模型，可能至少听说过 RAG 这个术语——检索增强生成（Retrieval Augmented Generation）。RAG
    的概念非常简单——假设你想向 LLM 提问，除了依赖 LLM 的预训练知识外，你首先从外部知识库中检索相关信息。然后将这些检索到的信息与问题一起提供给 LLM，从而让它生成一个更加知情且最新的回答。
- en: '![](../Images/c29b45a46c05f489f1ce51b45fa9711a.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c29b45a46c05f489f1ce51b45fa9711a.png)'
- en: 'Comparing standard LLM calls with RAG (Source: Image by Author)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将标准 LLM 调用与 RAG 进行比较（来源：图片来自作者）
- en: '**So, why use Retrieval Augmented Generation?**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么，为什么要使用检索增强生成？**'
- en: When providing **accurate and up-to-date information** is key, you cannot rely
    on the LLM’s inbuilt knowledge. RAGs are a cheap practical way to use LLMs to
    generate content about recent topics or niche topics **without needing to finetune**
    them on your own and burn away your life’s savings. Even when LLMs internal knowledge
    may be enough to answer questions, it might be a good idea to use RAGs anyway,
    [since recent studies have shown that they could help reduce LLMs hallucinations.](https://arxiv.org/html/2404.08189v1)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当**提供准确且最新的信息**至关重要时，你不能仅依赖 LLM 内置的知识。RAG 是一种廉价且实用的方法，可以使用 LLM 生成关于近期话题或小众话题的内容，**而无需自己对其进行微调**，也不会耗费你毕生的积蓄。即使
    LLM 内部的知识足以回答问题，使用 RAG 也是个好主意， [因为近期的研究表明，它们可能有助于减少 LLM 的幻觉现象。](https://arxiv.org/html/2404.08189v1)
- en: The different components of a bare-bones RAG
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个基础 RAG 的不同组件
- en: Before we dive into the advanced portion of this article, let’s review the basics.
    Generally RAGs consist of two pipelines — **preprocessing and inferencing**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨本文的高级内容之前，让我们先回顾一下基础知识。一般来说，RAG 由两条管道组成——**预处理和推理**。
- en: '**Inferencing** is all about using data from your existing database to answer
    questions from a user query. **Preprocessing** is the process of setting up the
    database in the correct way so that retrieval is done correctly later on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理**就是利用现有数据库中的数据来回答用户查询的问题。**预处理**是将数据库设置为正确格式的过程，以便之后可以正确检索。'
- en: Here is a diagramatic look into the entire basic barebones RAG pipeline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是整个基础RAG管道的示意图。
- en: '![](../Images/39ef29751baffd2a51ab35e9ebba7411.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39ef29751baffd2a51ab35e9ebba7411.png)'
- en: The Basic RAG pipeline (Image by Author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基本RAG管道（图片由作者提供）
- en: The Indexing or Preprocessing Steps
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引或预处理步骤
- en: This is the offline preprocessing stage, where we would set up our database.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是离线预处理阶段，我们将在这里设置数据库。
- en: '**Identify Data Source**: Choose a relevant data source based on the application,
    such as Wikipedia, books, or manuals. Since this is domain dependent, I am going
    to skip over this step in this article. Go choose any data you want to use, knock
    yourself out!'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别数据来源**：根据应用选择相关的数据来源，例如维基百科、书籍或手册。由于这是领域特定的，我将在本文中跳过此步骤。去选择你想使用的任何数据，尽情使用吧！'
- en: '**Chunking the Data**: Break down the dataset into smaller, manageable documents
    or chunks.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据分块**：将数据集拆分为更小、更易于管理的文档或数据块。'
- en: '**Convert to Searchable Format**: Transform each chunk into a numerical vector
    or similar searchable representation.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**转换为可搜索格式**：将每个数据块转换为数字向量或类似的可搜索表示。'
- en: '**Insert into Database**: Store these searchable chunks in a custom database,
    though external databases or search engines could also be used.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**插入数据库**：将这些可搜索的数据块存储到自定义数据库中，当然也可以使用外部数据库或搜索引擎。'
- en: The Inferencing Steps
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理步骤
- en: During the Query Inferencing stage, the following components stand out.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询推理阶段，以下组件尤为突出。
- en: '**Query Processing**: A method to convert the user’s query into a format suitable
    for search.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询处理**：将用户查询转换为适合搜索的格式的方法。'
- en: '**Retrieval/Search Strategy**: A similarity search mechanism to retrieve the
    most relevant documents.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索/搜索策略**：一种相似度搜索机制，用于检索最相关的文档。'
- en: '**Post-Retrieval Answer Generation**: Use retrieved documents as context to
    generate the answer with an LLM.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索后答案生成**：使用检索到的文档作为上下文，通过LLM生成答案。'
- en: Great — so we identified several key modules required to build a RAG. Believe
    it or not, each of these components have a lot of additional research to make
    this simple RAG turn into CHAD-rag. Let’s look into each of the major components
    in this list, starting with chunking.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 很好——我们已经确定了构建RAG所需的几个关键模块。信不信由你，这些组件每一个都有大量的附加研究，使得这个简单的RAG变成了CHAD-rag。让我们从数据分块开始，看看这个列表中的每个主要组件。
- en: This article is based on this Youtube video
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于这段YouTube视频
- en: '***By the way, this article is based on this 17-minute Youtube video I made
    on the same topic, covering all the topics in this article. Feel free to check
    it out after reading this Medium article!***'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***顺便说一下，本文基于我制作的这段17分钟的YouTube视频，内容涵盖了本文中的所有话题。读完这篇Medium文章后，欢迎查看视频！***'
- en: 1\. Chunking
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 数据分块
- en: Chunking is the process of breaking down large documents into smaller, manageable
    pieces. It might sound simple, but trust me, the way you chunk your data can make
    or break your RAG pipeline. Whatever chunks you create during preprocessing will
    eventually get retrieved during inference. If you make the size of chunks too
    small — like each sentence — then it might be difficult to retrieve them through
    search because they capture very little information. If the chunk size is too
    big — like inserting entire Wikipedia articles — the retrieved passages might
    end up confusing the LLM because you are sending large bodies of texts at once.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分块是将大文档拆分为较小、易于管理的部分的过程。听起来可能很简单，但相信我，你如何分块数据可能决定了RAG管道的成败。你在预处理时创建的分块最终将在推理过程中被检索。如果你将数据块做得太小——比如每个句子——那么通过搜索检索可能会很困难，因为它们捕捉的信息很少。如果数据块太大——比如插入整个维基百科文章——检索到的段落可能会让LLM感到困惑，因为你一次性发送了大量文本。
- en: '![](../Images/967b63a03b974ae50ac5aabda8f455f1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/967b63a03b974ae50ac5aabda8f455f1.png)'
- en: Depending on the different levels of chunking, your results may vary! (Image
    by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据不同的数据分块级别，你的结果可能会有所不同！(图片由作者提供)
- en: Some frameworks use LLMs to do chunking, for example by extracting simple factoids
    or propositions from the text corpus, and treat them as documents. This could
    be expensive because the larger your dataset, the more LLM calls you’ll have to
    make.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一些框架使用LLM进行分块，例如通过从文本语料库中提取简单的事实或命题，并将它们视为文档。这可能会很昂贵，因为数据集越大，你需要进行的LLM调用就越多。
- en: '![](../Images/b84d0c25979742b7fa6f815c18b4b9c6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b84d0c25979742b7fa6f815c18b4b9c6.png)'
- en: 'Proposition Chunking (Source: [Dense X Retrieval Paper](https://arxiv.org/pdf/2312.06648).
    License: Free)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 命题分块（来源：[Dense X Retrieval Paper](https://arxiv.org/pdf/2312.06648)。许可证：免费）
- en: Structural Chunking
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化分块
- en: '![](../Images/ac673609cd4ba4dd7f4e383864696009.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac673609cd4ba4dd7f4e383864696009.png)'
- en: If your data has inherent boundaries (like HTML or Code), sometimes it is best
    to just utilize it. (Image by Author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据具有固有的边界（如HTML或代码），有时最好直接利用它。（图片来源：作者）
- en: Quite often we may also deal with datasets that inherently have a known structure
    or format. For example, if you want to insert code into your database, you can
    simply split each script by the function names or class definitions. For HTML
    pages like Wikipedia articles, you can split by the heading tags — for example,
    split by the H2 tags to isolate each sub-chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 很多时候，我们可能还需要处理本身具有已知结构或格式的数据集。例如，如果你想将代码插入到数据库中，你可以简单地通过函数名或类定义来拆分每个脚本。对于像维基百科文章这样的HTML页面，你可以通过标题标签进行拆分——例如，按H2标签拆分以隔离每个子章节。
- en: Contextual Chunking
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文分块
- en: But there are some glaring issues with the types of chunking we have discussed
    so far. Suppose your dataset consists of tens of thousands of paragraphs extracted
    from all Sherlock Holmes books. Now the user has queried something general like
    what was the first crime in Study in Scarlet? What do you think is going to happen?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们迄今为止讨论的分块类型存在一些显而易见的问题。假设你的数据集包含从所有《福尔摩斯探案集》书籍中提取的成千上万段落。那么，如果用户查询的是类似“《血字的研究》中的第一个犯罪是什么？”这样的一般问题，你认为会发生什么？
- en: The problem is that since each documented is an isolated piece of information,
    we don’t know which chunks are from the book Study in Scarlet. Therefore, later
    on during retrieval, we will end up fetch a bunch of passages about the topic
    “crime” without knowing if it’s relevant to the book. To resolve this, we can
    use something known as contextual chunking.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，由于每个文档都是一个孤立的信息片段，我们不知道哪些分块来自《血字的研究》。因此，稍后的检索中，我们最终可能会获取到一堆关于“犯罪”话题的段落，但无法确定它是否与该书相关。为了解决这个问题，我们可以使用一种叫做上下文分块的方法。
- en: '**Enter Contextual Chunking**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**进入上下文分块**'
- en: '[A recent blogpost](https://anthropic.com/news/contextual-retrieval) from Anthropic
    describes it as prepending chunk-specific explanatory context to each chunk before
    embedding. Basically, while we are indexing, we would also include additional
    information relevant to the chunk — like the name of the book, the chapter, maybe
    a summary of the events in the book. Adding this context will allow the retriever
    to find references to Study in Scarlett and crimes when searching, hopefully getting
    the right documents from the database!'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[最近的博客文章](https://anthropic.com/news/contextual-retrieval)来自Anthropic，它将上下文分块描述为在嵌入之前，为每个分块添加特定的解释性上下文。基本上，在我们进行索引时，我们还会包括与分块相关的额外信息——例如书名、章节、也许是书中的事件总结。添加这些上下文将允许检索器在搜索时找到《血字的研究》和犯罪相关的参考内容，
    hopefully 能够从数据库中获得正确的文档！'
- en: '![](../Images/094e3126d94897eb876a84b373778c79.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/094e3126d94897eb876a84b373778c79.png)'
- en: Contextual Chunking adds additional information to the chunks than just the
    text body (Image by author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文分块为分块添加了比文本主体更多的额外信息（图片来源：作者）
- en: There are other ways to solve the problem of finding the right queries — like
    metadata filtering, We will talk about this later when we talk about Databases.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法可以解决查找正确查询的问题——比如元数据过滤，我们将在讨论数据库时再详细讲解。
- en: 2\. Data Conversion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 数据转换
- en: Next, we come to the data-conversion stage. Note that whatever strategy we used
    to convert the documents during preprocessing, we need to use it to search for
    similarity later, so these two components are tightly coupled.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入数据转换阶段。请注意，无论我们在预处理过程中使用了什么策略来转换文档，我们都需要在后续的相似性搜索中使用它，因此这两个组件是紧密耦合的。
- en: Two of the most common approaches that have emerged in this space are **embedding
    based methods** and **keyword-frequency based methods** like TF-IDF or BM-25.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在这个领域中出现的两种最常见方法是**嵌入式方法**和**基于关键词频率的方法**，如TF-IDF或BM-25。
- en: Embedding Based Methods
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入式方法
- en: We’ll start with embedding-based methods. Here, we use pretrained transformer
    models to transform the text into high-dimensional vector representations, capturing
    semantic meaning about the text. Embeddings are great for capturing semantic relationships,
    handling synonyms, and understanding context-dependent meanings. However, embedding
    can be computationally intensive, and can sometimes overlook exact matches that
    simpler methods would easily catch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从基于嵌入的方法开始。在这里，我们使用预训练的变换器模型将文本转化为高维向量表示，捕捉文本的语义含义。嵌入方法非常适合捕捉语义关系，处理同义词，并理解上下文相关的含义。然而，嵌入方法可能计算密集，并且有时会忽略一些简单方法容易捕捉到的精确匹配。
- en: '![](../Images/eea86572fe8b84983f6e3fb254176d90.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eea86572fe8b84983f6e3fb254176d90.png)'
- en: Embeddings (Image by Author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入（图片来源：作者）
- en: When does Semantic Search fail?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义搜索何时会失败？
- en: For example, suppose you have a database of manuals containing information about
    specific refrigerators. When you ask a query mentioning a very specific niche
    model or a serial number, embeddings will fetch documents that kind of resemble
    your query, but may fail to exactly match it. This brings us to the alternative
    of embeddings retrieval — keyword based retrieval.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个包含特定冰箱信息的手册数据库。当你查询一个非常特定的小众型号或序列号时，嵌入方法会获取与查询相似的文档，但可能无法完全匹配查询。这就引出了嵌入检索的替代方法——基于关键词的检索。
- en: Keyword Based Methods
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于关键词的方法
- en: Two popular keyword-based methods are TF-IDF and BM25\. These algorithms focus
    on statistical relationships between terms in documents and queries.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 两种流行的基于关键词的方法是TF-IDF和BM25。这些算法关注文档和查询中的术语之间的统计关系。
- en: TF-IDF weighs the importance of a word based on its frequency in a document
    relative to its frequency in the entire corpus. Every document in our dataset
    is be represented by a array of TF-IDF scores for each word in the vocabulary.
    The indices of the high values in this document vector tell us which words that
    are likely to be most characteristic of that document’s content, because these
    words appear more frequently in this document and less frequently in others. For
    example, the documents related to this Godrej A241gX , will have a high TF-IDF
    score for the phrase Godrej and A241gX, making it more likely for us to retrieve
    this using TF-IDF.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF根据一个词在文档中出现的频率与在整个语料库中出现的频率之间的比重，来衡量该词的重要性。我们数据集中的每个文档都由该文档中词汇表中每个词的TF-IDF分数组成的数组表示。该文档向量中高值的索引告诉我们哪些词最能体现该文档的内容，因为这些词在该文档中出现得更频繁，而在其他文档中出现得较少。例如，关于Godrej
    A241gX的文档将会对“Godrej”和“A241gX”这两个短语赋予较高的TF-IDF分数，这使得我们在使用TF-IDF检索时更有可能找到这些文档。
- en: '![](../Images/030fa185bf92ef2d88efe95f98253679.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/030fa185bf92ef2d88efe95f98253679.png)'
- en: TF-IDF relies on the ratio of the occurence of terms in a document compared
    to the entire corpus. (Image by author)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF依赖于术语在文档中出现的频率与其在整个语料库中出现频率的比率。（图片来源：作者）
- en: BM25, an evolution of TF-IDF, incorporates document length normalization and
    term saturation. Meaning that it adjusts the TF-IDF score based on if the document
    itself is longer or shorter than the average document length in the collection.
    Term saturation means that as a particular word appears too often in the database,
    it’s importance decreases.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: BM25是TF-IDF的一个演化版本，融合了文档长度归一化和术语饱和度。也就是说，它根据文档的长度是否长于或短于集合中的平均文档长度，调整TF-IDF分数。术语饱和度意味着，当某个特定词汇在数据库中出现得过于频繁时，它的重要性会降低。
- en: TF-IDF and BM-25 are great finding documents with specific keyword occurrences
    when they exactly occur. And embeddings are great for finding documents with similar
    semantic meaning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF和BM-25非常适合找到具有特定关键词出现的文档，尤其是当它们恰好出现时。而嵌入方法则非常适合找到具有相似语义的文档。
- en: '**A common thing these days is to retrieve using both keyword and embedding
    based methods, and combine them, giving us the best of both worlds.** Later on
    when we discuss Reciprocal Rank Fusion and Deduplication, we will look into how
    to combine these different retrieval methods.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在常见的做法是结合使用关键词和基于嵌入的方法，并将它们结合起来，从而获得两者的最佳效果。** 当我们在稍后讨论“互惠排序融合”和“去重”时，我们将探讨如何结合这些不同的检索方法。'
- en: 3\. Databases
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 数据库
- en: Up next, let’s talk about Databases. The most common type of database that is
    used in RAGs are **Vector Databases**. Vector databases store documents by indexing
    them with their vector representation, be in from an embedding, or TF-IDF. Vector
    databases specialize in fast similarity check with query vectors, making them
    ideal for RAG. Popular vector databases that you may want to look into are Pinecone,
    Milvus, ChromaDB, MongoDB, and they all have their pros and cons and pricing model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来谈谈数据库。在RAGs中使用最常见的数据库类型是**向量数据库**。向量数据库通过使用向量表示（例如嵌入或TF-IDF）对文档进行索引来存储它们。向量数据库专注于与查询向量进行快速相似性检查，这使得它们非常适合RAG。你可能想了解的流行向量数据库有Pinecone、Milvus、ChromaDB、MongoDB，它们各有优缺点和定价模型。
- en: An alternative to vector databases are **graph databases**. Graph databases
    store information as a network of documents with each document connected to others
    through relationships.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的替代方案是**图数据库**。图数据库将信息存储为一个文档网络，每个文档通过关系与其他文档相连接。
- en: '![](../Images/cf61de385aadd84baf03ba5bd4f69fa7.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf61de385aadd84baf03ba5bd4f69fa7.png)'
- en: Modern Vector Databases allow attribute filtering with semantic search (Image
    by Author)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现代向量数据库支持语义搜索的属性过滤（图示作者）
- en: Many modern vector and graph database also allow properties from relational
    databases, most notably metadata or attribute filtering. If you know the question
    is about the 5th Harry Potter book, it would be really nice to filter your entire
    database first to only contain documents from the 5th Harry Potter book, and not
    run embeddings search through the entire dataset. Optimal metadata filtering in
    Vector Databases is a pretty amazing area in Computer Science research, and a
    seperate article would be best for a in-depth discussion about this.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代向量和图数据库也支持关系型数据库中的属性，最显著的是元数据或属性过滤。如果你知道问题是关于《哈利·波特与凤凰社》的第五本书，那么首先过滤整个数据库，只保留第五本哈利·波特书籍的文档，而不是对整个数据集进行嵌入搜索，会非常有帮助。在向量数据库中，最佳的元数据过滤是计算机科学研究中的一个非常棒的领域，关于这个话题的深入讨论最好能写一篇独立的文章。
- en: 4\. Query transformation
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 查询转换
- en: '**Next, let’s move to inferencing starting with query transformation — which
    is any preprocessing step we do to the user’s actual query before doing any similarity
    search. Think of it like improving the user’s question to get better answers.**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**接下来，让我们开始推理过程，从查询转换开始——这是我们在进行任何相似性搜索之前，对用户实际查询进行的任何预处理步骤。可以把它看作是改善用户问题，以获得更好的答案。**'
- en: In general, we want to avoid searching directly with the user query. User inputs
    are usually very noisy and they can type random stuff — we want an additional
    transformation layer that interprets the user query and turns it into a search
    query.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们希望避免直接使用用户查询进行搜索。用户输入通常非常嘈杂，他们可能会随便输入一些内容——我们希望有一个额外的转换层，能够解释用户的查询并将其转化为搜索查询。
- en: A simple example why Query Rewriting is important
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个简单的例子，说明了查询重写为何重要
- en: The most common technique to do this transformation is query rewriting. Imagine
    someone asks, “**What happened to the artist who painted the Mona Lisa?**” If
    we do semantic or keyword searches, the retrieved information will be all about
    the Mona Lisa, not about the artist. A query rewriting system would use an LLM
    to rewrite this query. The LLM might transform this into “**Leonardo da Vinci
    Mona Lisa artist**”, which will be a much fruitful search.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这种转换的最常见技术是查询重写。假设有人问：“**画蒙娜丽莎的艺术家后来怎样了？**”如果我们进行语义或关键词搜索，检索到的信息将全是关于蒙娜丽莎的，而不是关于艺术家的。查询重写系统会使用LLM重写这个查询。LLM可能会将其转换为“**列奥纳多·达·芬奇
    蒙娜丽莎 艺术家**”，这将是一个更有成效的搜索。
- en: '![](../Images/0ef4f1d430fde82b990b925ff0b9bdab.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ef4f1d430fde82b990b925ff0b9bdab.png)'
- en: Direct Retrieval vs Query Rewriting (Image by Author)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 直接检索与查询重写（图示作者）
- en: Sometimes we would also use **Contextual Query Writing**, where we might use
    additional contexts, like using the older conversation transcript from the user,
    or if we know that our application covers documents from 10 different books, maybe
    we can have a classifier LLM that classifies the user query to detect which of
    the 10 books we are working with. If our database is in a different language,
    we can also translate the query.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们还会使用**上下文查询写作**，例如我们可能会使用额外的上下文，比如使用用户的旧对话记录，或者如果我们知道我们的应用程序涵盖了来自10本不同书籍的文档，可能会有一个分类器LLM来分类用户查询，以确定我们正在处理的是哪本书。如果我们的数据库使用的是不同语言，也可以对查询进行翻译。
- en: '[There are also powerful techniques like HYDE](https://arxiv.org/abs/2212.10496),
    which stands for **Hypothetical Document Embedding**. HYDE uses a language model
    to generate a hypothetical answer to the query, and do similarity search with
    this hypothetical answer to retrieve relevant documents.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[还有像HYDE这样的强大技术](https://arxiv.org/abs/2212.10496)，HYDE代表**假设文档嵌入**。HYDE使用语言模型生成对查询的假设性回答，然后通过与这个假设性回答进行相似度搜索来检索相关文档。'
- en: '![](../Images/89020eaaea924ddb06e90b8dc7705e20.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89020eaaea924ddb06e90b8dc7705e20.png)'
- en: Hypothetical Document Embeddings (Image by Author)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文档嵌入（Hypothetical Document Embeddings）（图片来源：作者）
- en: Another technique is **Multi-Query Expansion** where we generate multiple queries
    from the single user query and perform parallel searches to retrieve multiple
    sets of documents. The received documents can then later go through a de-duplication
    step or rank fusion to remove redundant documents.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种技术是**多查询扩展**，它通过从单一的用户查询生成多个查询，并进行并行搜索来检索多个文档集合。收到的文档随后可以经过去重步骤或排名融合，以去除重复的文档。
- en: A recent approach called [**Astute RAG**](https://arxiv.org/abs/2410.07176)tries
    to consolidate externally input knowledge with the LLM’s own internal knowledge
    before generating answers. There are also **Multi-Hop techniques like Baleen programs**.
    They work by performing an initial search, analyzing the top results to find frequently
    co-occurring terms, and then adding these terms to the original query. This adaptive
    approach can help bridge the vocabulary gap between user queries and document
    content, and help retrieve better documents.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一种最近的 approach 叫做[**Astute RAG**](https://arxiv.org/abs/2410.07176)，它试图在生成答案之前，将外部输入的知识与LLM自身的内部知识进行整合。还有一些**多跳技术，如Baleen程序**，它们通过执行初始搜索，分析前几个结果，找出常常一起出现的词汇，然后将这些词汇添加到原始查询中。这种自适应方法有助于弥合用户查询与文档内容之间的词汇差距，从而帮助检索到更好的文档。
- en: 5\. Post Retrieval Processing
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 检索后处理
- en: Now that we’ve retrieved our potentially relevant documents, we can add another
    post-retrieval processing step before feeding information to our language model
    for generating the answer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检索到了可能相关的文档，在将信息输入到语言模型中生成答案之前，我们可以加入另一个检索后处理步骤。
- en: For example, we can do information selection and emphasis, where an LLM selects
    portion of the retrieved documents that could be useful for finding the answer.
    We might highlight key sentences, or do semantic filtering where we remove unimportant
    paragraphs, or do context summarization by fusing multiple documents into one.
    The goal here is to avoid overwhelming our LLM with too much information, which
    could lead to less focused or accurate responses.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以进行信息选择和强调，在这个过程中，LLM（大语言模型）会从检索到的文档中选择可能有助于找到答案的部分。我们可能会突出显示关键句子，或者进行语义过滤，去除不重要的段落，或者进行上下文总结，将多份文档合并成一份。这样做的目标是避免让我们的LLM接收过多信息，这样可能会导致回应不够专注或准确。
- en: '![](../Images/1afd67da02e55c50540e235d5695a19d.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1afd67da02e55c50540e235d5695a19d.png)'
- en: You can use smaller LLMs to flag relevant info from retrieved documents before
    consolidating the context prompt for the final LLM call (Image by Author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用较小的LLM从检索到的文档中标记出相关信息，然后在最终调用LLM时合并上下文提示（图片来源：作者）
- en: Often we do multiple queries with query expansion, or use multiple retrieval
    algorithms like Embeddings+BM-25 to separately fetch multiple documents. To remove
    duplicates, we often use reranking methods like Reciprocal Rank Fusion. RRF combines
    the rankings from all the different approaches, giving higher weight to documents
    that consistently rank well across multiple methods. In the end, the top K high
    ranking documents are passed to the LLM.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会通过查询扩展进行多次查询，或使用多个检索算法，如Embeddings+BM-25，分别获取多个文档。为了去除重复项，我们通常使用重新排序方法，如互惠排序融合（RRF）。RRF结合了所有不同方法的排名，为那些在多个方法中始终表现良好的文档赋予更高的权重。最终，排名前K的高质量文档将传递给LLM。
- en: '![](../Images/e7640eabc4d48854d1a9da9a4ecd3379.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7640eabc4d48854d1a9da9a4ecd3379.png)'
- en: Reciprocal Rank Fusion is a classic Search Engine algorithm to combine item
    ranks obtained from multiple ranking algorithms (Image by author)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 互惠排序融合（Reciprocal Rank Fusion）是一种经典的搜索引擎算法，用于将多个排名算法获得的项排名结合起来（图片来源：作者）
- en: '[FLARE or forward-looking active retrieval augmented generation](https://arxiv.org/abs/2305.06983)
    is an iterative post-retrieval strategy. Starting with the user input and initial
    retrieval results, an LLM iteratively guesses the next sentence. Then we check
    if the generated guess contains any low probability tokens indicated here with
    an underline — if so, we call the retriever to retrieve useful documents from
    the dataset and make necessary corrections.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[FLARE 或前瞻性主动检索增强生成](https://arxiv.org/abs/2305.06983)是一种迭代的后检索策略。从用户输入和初始检索结果开始，LLM（大语言模型）迭代地猜测下一句话。然后，我们检查生成的猜测是否包含任何低概率的标记，这些标记在这里用下划线标出——如果有，我们调用检索器从数据集中检索有用的文档并进行必要的修正。'
- en: Final Thoughts
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的思考
- en: For a more visual breakdown of the different components of RAGs, do checkout
    my Youtube video on this topic. The field of LLMs and RAGs are rapidly evolving
    — a thorough understanding of the RAG framework is incredibly essential to appreciate
    the pros and cons of each approach and weigh which approaches work best for YOUR
    use-case. The next time you are thinking of designing a RAG system, do stop and
    ask yourself these questions —
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更直观地了解 RAG（检索增强生成）系统的不同组件，欢迎观看我关于这个主题的 Youtube 视频。LLM 和 RAG 的领域正在快速发展——对
    RAG 框架的透彻理解对于欣赏每种方法的优缺点以及权衡哪种方法最适合你的用例至关重要。下次你在设计 RAG 系统时，请停下来问问自己这些问题——
- en: What are my data sources?
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的数据来源是什么？
- en: How should I chunk my data? Is there inherent structure that comes with my data
    domain? Do my chunks need additional context (contextual chunking)?
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何分块我的数据？我的数据领域中是否有固有的结构？我的数据块是否需要额外的上下文（上下文分块）？
- en: Do I need semantic retrieval (embeddings) or more exact-match retrieval (BM-25)?
    What type of queries am I expecting from the user?
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我需要语义检索（嵌入）还是更精确的匹配检索（BM-25）？我预期用户会发出什么样的查询？
- en: What database should I use? Is my data a graph? Does it need metadata-filtering?
    How much money do I want to spend on databases?
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该使用什么数据库？我的数据是图形结构吗？它需要元数据过滤吗？我想在数据库上花多少钱？
- en: How can I best rewrite the user query for easy search hits? Can an LLM rewrite
    the queries? Should I use HYDE? If LLMs already have enough domain knowledge about
    my target field, can I use Astute?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何才能最好地重写用户查询，以便轻松找到搜索结果？LLM 能否重写查询？我应该使用 HYDE 吗？如果 LLM 已经具备足够的目标领域知识，我能否使用
    Astute？
- en: Can I combine multiple different retrieval algorithms and then do rank fusion?
    (honestly, just do it if you can afford it cost-wise and latency-wise)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以结合多种不同的检索算法，然后进行排名融合吗？（老实说，如果你能在成本和延迟上承受得起，那就做吧）
- en: The Author
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者
- en: 'Check out my Youtube channel where I post content about Deep Learning, Machine
    Learning, Paper Reviews, Tutorials, and just about anything related to AI (except
    news, there are WAY too many Youtube channels for AI news). Here are some of my
    links:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 访问我的 Youtube 频道，在那里我发布关于深度学习、机器学习、论文评审、教程以及与 AI 相关的任何内容（除了新闻，因为有太多关于 AI 新闻的
    Youtube 频道）。以下是我的一些链接：
- en: '**Youtube Channel:** [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Youtube 频道：** [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)'
- en: '**Patreon:** [https://www.patreon.com/c/NeuralBreakdownwithAVB](https://www.patreon.com/c/NeuralBreakdownwithAVB)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**Patreon：** [https://www.patreon.com/c/NeuralBreakdownwithAVB](https://www.patreon.com/c/NeuralBreakdownwithAVB)'
- en: Give me a follow on Medium and a clap if you enjoyed this!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，记得在 Medium 上关注我并点赞！
- en: References
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Vector Databases: [https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库：[https://superlinked.com/vector-db-comparison](https://superlinked.com/vector-db-comparison)
- en: 'Metadata Filtering: [https://www.pinecone.io/learn/vector-search-filtering/](https://www.pinecone.io/learn/vector-search-filtering/)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据过滤：[https://www.pinecone.io/learn/vector-search-filtering/](https://www.pinecone.io/learn/vector-search-filtering/)
- en: 'Contextual Chunking: [https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文分块：[https://www.anthropic.com/news/contextual-retrieval](https://www.anthropic.com/news/contextual-retrieval)
- en: 'Propositions / Dense X Retrieval: [https://arxiv.org/pdf/2312.06648](https://arxiv.org/pdf/2312.06648)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 命题 / 密集 X 检索：[https://arxiv.org/pdf/2312.06648](https://arxiv.org/pdf/2312.06648)
- en: 'Hypothetical Document Embeddigs (HYDE): [https://arxiv.org/abs/2212.10496](https://arxiv.org/abs/2212.10496)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文档嵌入（HYDE）：[https://arxiv.org/abs/2212.10496](https://arxiv.org/abs/2212.10496)
- en: 'FLARE: [https://arxiv.org/abs/2305.06983](https://arxiv.org/abs/2305.06983)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'FLARE: [https://arxiv.org/abs/2305.06983](https://arxiv.org/abs/2305.06983)'
