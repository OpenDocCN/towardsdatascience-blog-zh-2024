- en: Improving Code Quality During Data Transformation with Polars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improving-code-quality-during-data-transformation-with-polars-92997e67c8a9?source=collection_archive---------10-----------------------#2024-08-09](https://towardsdatascience.com/improving-code-quality-during-data-transformation-with-polars-92997e67c8a9?source=collection_archive---------10-----------------------#2024-08-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@npotapov?source=post_page---byline--92997e67c8a9--------------------------------)[![Nikolai
    Potapov](../Images/d2ac4b8c12c0cf70df05b8908b875a19.png)](https://medium.com/@npotapov?source=post_page---byline--92997e67c8a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--92997e67c8a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--92997e67c8a9--------------------------------)
    [Nikolai Potapov](https://medium.com/@npotapov?source=post_page---byline--92997e67c8a9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--92997e67c8a9--------------------------------)
    ·6 min read·Aug 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32683c335dbe8f17d37fc6ceaac04d46.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with AI by Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: In our daily lives as Data/Analytic Engineers, writing ETL/ELT workflows and
    pipelines (or perhaps your company uses a different term) is a routine and integral
    part of our work. However, in this article, I will focus only on the Transformation
    stage. Why? Because at this stage, data from various sources and of different
    types acquires business significance for the company. This stage is very important
    and also incredibly delicate, as an error can instantly mislead the user, causing
    them to lose trust in your data.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the process of improving code quality, let’s consider a hypothetical
    example. Imagine a website where we log user actions, such as what they viewed
    and purchased. We’ll have `user_id` for the user ID, `product_id` for the product,
    `action_type` for the type of action (either a view or purchase), and `action_dt`
    for the action timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, for our task, we’ll need a product catalog, which in our case
    will include only `product_id` and its price (`price`). Our data is now ready
    for the example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s tackle our first task: creating a report that will contain the total
    purchase amount and the ratio of the number of purchased items to viewed items
    from the previous day for each user. This task isn’t particularly complex and
    can be quickly implemented. Here’s how it might look using Polars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a working solution that could be deployed to production, some might
    say, but not us since you’ve opened this article. At the beginning, I emphasized
    that I would focus specifically on the transformation step.
  prefs: []
  type: TYPE_NORMAL
- en: If we think about the long-term maintenance of this code, testing, and remember
    that there will be hundreds of such reports, we must recognize that each subsequent
    developer will understand this code less than the previous one, thereby increasing
    the chances of errors with every change.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would like to reduce this risk, and that’s why I’ve come to the following
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Let’s separate all the business logic into a distinct class, such
    as `DailyUserPurchaseReport`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 2**: Let’s define the arguments this class should accept: `sources`
    - various sources we need for our work, and `params` - variable parameters that
    may change, in our case, this could be the report date.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3**: Define a method that will perform the transformation, for example,
    `execute`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4**: Break down the entire process into separate functions that accept
    a `pl.LazyFrame` and also return a `pl.LazyFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 5**: Now, use the magic function `pipe` to connect our entire pipeline
    together. This is precisely why we use `pl.LazyFrame` everywhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '*It is recommended to use LazyFrame when piping operations, in order to fully
    take advantage of query optimization and parallelization.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Final code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Bonus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those using Test-Driven Development (TDD), this approach is especially beneficial.
    TDD emphasizes writing tests before the actual implementation. By having clearly
    defined, small functions, you can write precise tests for each part of the transformation
    process, ensuring that each function behaves as expected. This not only makes
    the process smoother but also ensures that your transformations are thoroughly
    validated at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I have outlined a structured approach to improving code quality
    in your data workflows using Polars. By isolating the transformation step and
    breaking down the process into distinct, manageable parts, we ensure that our
    code is both robust and maintainable. Through the use of `pl.LazyFrame` and the
    `pipe` function, we take full advantage of Polars capabilities for query optimization
    and parallelization. This method not only enhances the efficiency of our data
    transformations but also ensures the integrity and business relevance of the data
    we work with. By following these steps, you can create more reliable and scalable
    data workflows, ultimately leading to better data-driven decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Share Your Experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have experience or useful tips, share your opinion in the comments. It’s
    always interesting to learn the experiences of other developers.
  prefs: []
  type: TYPE_NORMAL
