<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Use Elastic Net Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Use Elastic Net Regression</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-elastic-net-regression-85a6a393222b?source=collection_archive---------9-----------------------#2024-03-14">https://towardsdatascience.com/how-to-use-elastic-net-regression-85a6a393222b?source=collection_archive---------9-----------------------#2024-03-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5436" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Cast a flexible net that only retains big fish</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@cjtayl2?source=post_page---byline--85a6a393222b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chris Taylor" class="l ep by dd de cx" src="../Images/a5a0b096777cc262cc5adc3350fadab4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*op5xMg-kVp_v5fpJw4w1GQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--85a6a393222b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@cjtayl2?source=post_page---byline--85a6a393222b--------------------------------" rel="noopener follow">Chris Taylor</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--85a6a393222b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="9beb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">Note: The code used in this article utilizes three custom scripts, </em><code class="cx ng nh ni nj b">data_cleaning</code><em class="nf">, </em><code class="cx ng nh ni nj b">data_review</code><em class="nf">, and , </em><code class="cx ng nh ni nj b">eda</code><em class="nf">, that can be accessed through a public </em><a class="af nk" href="https://github.com/CJTAYL/elastic_net_medium" rel="noopener ugc nofollow" target="_blank"><em class="nf">GitHub repository</em></a><em class="nf">.</em></p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm nn"><img src="../Images/22442aa74d6abaf90e723802601a8a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*96BzN5dA3xKCvDIF"/></div></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Photo by <a class="af nk" href="https://unsplash.com/@ericbarbeau?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Eric BARBEAU</a> on <a class="af nk" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="oe"><p id="bea2" class="of og fq bf oh oi oj ok ol om on ne dx">It is like a stretchable fishing net that retains ‘all the big fish’ Zou &amp; Hastie (2005) p. 302</p></blockquote><h1 id="8b2e" class="oo op fq bf oq or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk"><strong class="al">Background</strong></h1><p id="bc2f" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">Linear regression is a commonly used teaching tool in data science and, under the appropriate conditions (e.g., linear relationship between the independent and dependent variables, absence of multicollinearity), it can be an effective method for predicting a response. However, in some scenarios (e.g., when the model’s structure becomes complex), its use can be problematic.</p><p id="d184" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To address some of the algorithm’s limitations, penalization or regularization techniques have been suggested [1]. Two popular methods of regularization are ridge and lasso regression, but choosing between these methods can be difficult for those new to the field of data science.</p><p id="a4df" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One approach to choosing between ridge and lasso regression is to examine the relevancy of the features to the response variable [2]. When the majority of features in the model are relevant (i.e., contribute to the predictive power of the model), the ridge regression penalty (or L2 penalty) should be added to linear regression.</p><p id="2b3b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When the ridge regression penalty is added, the cost function of the model is:</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div class="nl nm pp"><img src="../Images/b298d8d69cbf1957e25f9cda2caa8eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*hkpOapgfog6HT8i_I9J8tg.png"/></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><ul class=""><li id="ecc4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pq pr ps bk">θ = the vector of parameters or coefficients of the model</li><li id="ee53" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">α = the overall strength of the regularization</li><li id="7922" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk"><em class="nf">m</em> = the number of training examples</li><li id="0ce2" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk"><em class="nf">n</em> = the number of features in the dataset</li></ul><p id="2d4b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When the majority of features are irrelevant (i.e., do not contribute to the predictive power of the model), the lasso regression penalty (or L1 penalty) should be added to linear regression.</p><p id="285e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When the lasso regression penalty is added, the cost function of the model is:</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div class="nl nm py"><img src="../Images/57e2c8bd0be6960ce028e9aff73d468d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*xLzb__VAPCBwQZoNBdnHGQ.png"/></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><p id="6f33" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Relevancy can be determined through manual review or cross validation; however, when working with several features, the process becomes time consuming and computationally expensive.</p><p id="a8d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An efficient and flexible solution to this issue is using elastic net regression, which combines the ridge and lasso penalties.</p><p id="fcd3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The cost function for elastic net regression is:</p><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm pz"><img src="../Images/4a2c4e3f8965f8a2fd3e1895e4d90293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYN7ctSNBXxBePLFLhqwbg.png"/></div></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><ul class=""><li id="1526" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pq pr ps bk">r = the mixing ratio between ridge and lasso regression.</li></ul><p id="6a90" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When r is 1, only the lasso penalty is used and when r is 0 , only the ridge penalty is used. When r is a value between 0 and 1, a mixture of the penalties is used.</p><p id="571f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In addition to being well-suited for datasets with several features, elastic net regression has other attributes that make it an appealing tool for data scientists [1]:</p><ul class=""><li id="d906" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pq pr ps bk">Automatic selection of relevant features, which results in parsimonious models that are easy to interpret</li><li id="a112" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Continuous shrinkage, which gradually reduces the coefficients of less relevant features towards zero (opposed to an immediate reduction to zero)</li><li id="46b9" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Ability to select groups of correlated features, instead of selecting one feature from the group arbitrarily</li></ul><p id="07af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Due to its utility and flexibility, Zou and Hastie (2005) compared the model to a “…stretchable fishing net that retains all the big fish.” (p. 302), where big fish are analogous to relevant features.</p><p id="f103" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we have some background, we can move forward to implementing elastic net regression on a real dataset.</p><h1 id="178e" class="oo op fq bf oq or os gq ot ou ov gt ow ox qa oz pa pb qb pd pe pf qc ph pi pj bk"><strong class="al">Implementation</strong></h1><p id="310e" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">A great resource for data is the University of California at Irvine’s Machine Learning Repository (UCI ML Repo). For the tutorial, we’ll use the Wine Quality Dataset [3], which is licensed under a <a class="af nk" href="https://creativecommons.org/licenses/by/4.0/legalcode" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution 4.0 International</a> license.</p><p id="37b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The function displayed below can be used to obtain datasets and variable information from the UCI ML Repo by entering the identification number as the parameter of the function.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="9445" class="qg op fq nj b bg qh qi l qj qk">pip install ucimlrepo # unless already installed</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="b86f" class="qg op fq nj b bg qh qi l qj qk">from ucimlrepo import fetch_ucirepo<br/>import pandas as pd<br/><br/>def fetch_uci_data(id):<br/>    """<br/>    Function to return features datasets from the UCI ML Repository.<br/><br/>    Parameters<br/>    ----------<br/>    id: int<br/>        Identifying number for the dataset<br/><br/>    Returns<br/>    ----------<br/>    df: df<br/>        Dataframe with features and response variable <br/>    """<br/>    dataset = fetch_ucirepo(id=id) <br/><br/>    features = pd.DataFrame(dataset.data.features)<br/>    response = pd.DataFrame(dataset.data.targets)<br/>    df = pd.concat([features, response], axis=1)<br/>    <br/>    # Print variable information<br/>    print('Variable Information')<br/>    print('--------------------')<br/>    print(dataset.variables)<br/><br/>    return(df)</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="34b0" class="qg op fq nj b bg qh qi l qj qk"># Wine Quality's identification number is 186<br/>df = fetch_uci_data(186)</span></pre><p id="3e28" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A pandas dataframe has been assigned to the variable “df” and information about the dataset has been printed.</p><h2 id="1405" class="qm op fq bf oq qn qo qp ot qq qr qs ow ms qt qu qv mw qw qx qy na qz ra rb rc bk">Exploratory Data Analysis</h2><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="cde5" class="qg op fq nj b bg qh qi l qj qk">Variable Information<br/>--------------------<br/>                    name     role         type demographic  \<br/>0          fixed_acidity  Feature   Continuous        None   <br/>1       volatile_acidity  Feature   Continuous        None   <br/>2            citric_acid  Feature   Continuous        None   <br/>3         residual_sugar  Feature   Continuous        None   <br/>4              chlorides  Feature   Continuous        None   <br/>5    free_sulfur_dioxide  Feature   Continuous        None   <br/>6   total_sulfur_dioxide  Feature   Continuous        None   <br/>7                density  Feature   Continuous        None   <br/>8                     pH  Feature   Continuous        None   <br/>9              sulphates  Feature   Continuous        None   <br/>10               alcohol  Feature   Continuous        None   <br/>11               quality   Target      Integer        None   <br/>12                 color    Other  Categorical        None   <br/><br/>               description units missing_values  <br/>0                     None  None             no  <br/>1                     None  None             no  <br/>2                     None  None             no  <br/>3                     None  None             no  <br/>4                     None  None             no  <br/>5                     None  None             no  <br/>6                     None  None             no  <br/>7                     None  None             no  <br/>8                     None  None             no  <br/>9                     None  None             no  <br/>10                    None  None             no  <br/>11  score between 0 and 10  None             no  <br/>12            red or white  None             no</span></pre><p id="0e49" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Based on the variable information, we can see that there are 11 “features”, 1 “target”, and 1 “other” variables in the dataset. This is interesting information — if we had extracted the data without the variable information, we may not have known that there were data available on the family (or color) of wine. At this time, we won’t be incorporating the “color” variable into the model, but it’s nice to know it’s there for future iterations of the project.</p><p id="40f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The “description” column in the variable information suggests that the “quality” variable is categorical. The data are likely ordinal, meaning they have a hierarchical structure but the intervals between the data are not guaranteed to be equal or known. In practical terms, it means a wine rated as 4 is not twice as good as a wine rated as 2. To address this issue, we’ll convert the data to the proper data-type.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="fbfc" class="qg op fq nj b bg qh qi l qj qk">df['quality'] = df['quality'].astype('category')</span></pre><p id="61cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To gain a better understanding of the data, we can use the <code class="cx ng nh ni nj b">countplot()</code> method from the <code class="cx ng nh ni nj b">seaborn</code> package to visualize the distribution of the “quality” variable.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="d819" class="qg op fq nj b bg qh qi l qj qk">import seaborn as sns<br/>import matplotlib.pyplot as plt <br/><br/>sns.set_theme(style='whitegrid') # optional<br/><br/>sns.countplot(data=df, x='quality')<br/>plt.title('Distribution of Wine Quality')<br/>plt.xlabel('Quality')<br/>plt.ylabel('Count')<br/>plt.show()</span></pre><figure class="no np nq nr ns nt nl nm paragraph-image"><div class="nl nm rd"><img src="../Images/b6af5dd87b5f361d902a404c7bc06f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*vIsd5ABdiLlk4Ry_OZEqVQ.png"/></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><p id="bebd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When conducting an exploratory data analysis, creating histograms for numeric features is beneficial. Additionally, grouping the variables by a categorical variable can provide new insights. The best option for grouping the data is “quality”. However, given there are 7 groups of quality, the plots could become difficult to read. To simplify grouping, we can create a new feature, “rating”, that organizes the data on “quality” into three categories: low, medium, and high.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="48f5" class="qg op fq nj b bg qh qi l qj qk">def categorize_quality(value):<br/> if 0 &lt;= value &lt;= 3:<br/>    return 0 # low rating<br/> elif 4 &lt;= value &lt;= 6:<br/>    return 1 # medium rating<br/> else:<br/>    return # high rating<br/><br/># Create new column for 'rating' data<br/>df['rating'] = df['quality'].apply(categorize_quality)</span></pre><p id="cf85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To determine how many wines are each group, we can use the following code:</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="4a4f" class="qg op fq nj b bg qh qi l qj qk">df['rating'].value_counts()</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="f308" class="qg op fq nj b bg qh qi l qj qk">rating<br/>1    5190<br/>2    1277<br/>0      30<br/>Name: count, dtype: int64</span></pre><p id="dbc8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Based on the output of the code, we can see that the majority of wines are categorized as “medium”.</p><p id="0602" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we can plot histograms of the numeric features groups by “rating”. To plot the histogram we’ll need to use the <code class="cx ng nh ni nj b">gen_histograms_by_category()</code> method from the <code class="cx ng nh ni nj b">eda</code> script in the GitHub repository shared at the beginning of the article.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="4cff" class="qg op fq nj b bg qh qi l qj qk">import eda <br/><br/>eda.gen_histograms_by_category(df, 'rating')</span></pre><figure class="no np nq nr ns nt nl nm paragraph-image"><div class="nl nm re"><img src="../Images/2e1491e6afb60c5a265de19bd1f2b853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*uaevQ54khqgijMx2NnF97Q.png"/></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><p id="af87" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Above is one of the plots generated by the method. A review of the plot indicates there is some skew in the data. To gain a more precise measure of skew, along with other statistics, we can use the <code class="cx ng nh ni nj b">get_statistics()</code> method from the <code class="cx ng nh ni nj b">data_review</code> script.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="cd39" class="qg op fq nj b bg qh qi l qj qk">from data_review import get_statistics<br/><br/>get_statistics(df)</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="c8cc" class="qg op fq nj b bg qh qi l qj qk">-------------------------<br/>Descriptive Statistics<br/>-------------------------<br/>          fixed_acidity  volatile_acidity  citric_acid  residual_sugar    chlorides  free_sulfur_dioxide  total_sulfur_dioxide      density           pH    sulphates      alcohol      quality<br/>count       6497.000000       6497.000000  6497.000000     6497.000000  6497.000000          6497.000000           6497.000000  6497.000000  6497.000000  6497.000000  6497.000000  6497.000000<br/>mean           7.215307          0.339666     0.318633        5.443235     0.056034            30.525319            115.744574     0.994697     3.218501     0.531268    10.491801     5.818378<br/>std            1.296434          0.164636     0.145318        4.757804     0.035034            17.749400             56.521855     0.002999     0.160787     0.148806     1.192712     0.873255<br/>min            3.800000          0.080000     0.000000        0.600000     0.009000             1.000000              6.000000     0.987110     2.720000     0.220000     8.000000     3.000000<br/>25%            6.400000          0.230000     0.250000        1.800000     0.038000            17.000000             77.000000     0.992340     3.110000     0.430000     9.500000     5.000000<br/>50%            7.000000          0.290000     0.310000        3.000000     0.047000            29.000000            118.000000     0.994890     3.210000     0.510000    10.300000     6.000000<br/>75%            7.700000          0.400000     0.390000        8.100000     0.065000            41.000000            156.000000     0.996990     3.320000     0.600000    11.300000     6.000000<br/>max           15.900000          1.580000     1.660000       65.800000     0.611000           289.000000            440.000000     1.038980     4.010000     2.000000    14.900000     9.000000<br/>skew           1.723290          1.495097     0.471731        1.435404     5.399828             1.220066             -0.001177     0.503602     0.386839     1.797270     0.565718     0.189623<br/>kurtosis       5.061161          2.825372     2.397239        4.359272    50.898051             7.906238             -0.371664     6.606067     0.367657     8.653699    -0.531687     0.23232</span></pre><p id="783c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Consistent with the histogram, the feature labeled “fixed_acidity” has a skewness of 1.72 indicating significant right-skewness.</p><p id="d6c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To determine if there are correlations between the variables, we can use another function from the <code class="cx ng nh ni nj b">eda</code> script.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="11db" class="qg op fq nj b bg qh qi l qj qk">eda.gen_corr_matrix_hmap(df)</span></pre><figure class="no np nq nr ns nt nl nm paragraph-image"><div role="button" tabindex="0" class="nu nv ed nw bh nx"><div class="nl nm rf"><img src="../Images/271e84969b393ccdde17bfde8c8860f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XauDynVilKE3lTgB4hkc4g.png"/></div></div><figcaption class="nz oa ob nl nm oc od bf b bg z dx">Image by the author</figcaption></figure><p id="970a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although there a few moderate and strong relationships between features, elastic net regression performs well with correlated variables, therefore, no action is required [2].</p><h2 id="6dc1" class="qm op fq bf oq qn qo qp ot qq qr qs ow ms qt qu qv mw qw qx qy na qz ra rb rc bk"><strong class="al">Data Cleaning</strong></h2><p id="c7d6" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">For the elastic net regression algorithm to run correctly, the numeric data must be scaled and the categorical variables must be encoded.</p><p id="6c33" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To clean the data, we’ll take the following steps:</p><ol class=""><li id="12c2" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne rg pr ps bk">Scale the data using the the <code class="cx ng nh ni nj b">scale_data()</code> method from the the <code class="cx ng nh ni nj b">data_cleaning</code> script</li><li id="a074" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne rg pr ps bk">Encode the “quality” and “rating” variables using the the <code class="cx ng nh ni nj b">get_dummies()</code> method from <code class="cx ng nh ni nj b">pandas</code></li><li id="0cc0" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne rg pr ps bk">Separate the features (i.e., X) and response variable (i.e., y) using the <code class="cx ng nh ni nj b">separate_data()</code> method</li><li id="932c" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne rg pr ps bk">Split the data into train and test sets using <code class="cx ng nh ni nj b">train_test_split()</code></li></ol><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="975b" class="qg op fq nj b bg qh qi l qj qk">from sklearn.model_selection import train_test_split<br/>from data_cleaning import scale_data, separate_data<br/><br/>df_scaled = scale_data(df)<br/>df_encoded = pd.get_dummies(df_scaled, columns=['quality', 'rating'])<br/><br/># Separate features and response variable (i.e., 'alcohol')<br/>X, y = separate_data(df_encoded, 'alcohol')<br/><br/># Create test and train sets <br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state=0)</span></pre><h2 id="9a66" class="qm op fq bf oq qn qo qp ot qq qr qs ow ms qt qu qv mw qw qx qy na qz ra rb rc bk"><strong class="al">Model Building and Evaluation</strong></h2><p id="cb7c" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">To train the model, we’ll use <code class="cx ng nh ni nj b">ElasticNetCV() </code>which has two parameters, <code class="cx ng nh ni nj b">alpha</code> and <code class="cx ng nh ni nj b">l1_ratio</code>, and built-in cross validation. The <code class="cx ng nh ni nj b">alpha</code> parameter determines the strength of the regularization applied to the model and <code class="cx ng nh ni nj b">l1_ratio </code>determines the mix of the lasso and ridge penalty (it is equivalent to the variable <em class="nf">r</em> that was reviewed in the <em class="nf">Background</em> section).</p><ul class=""><li id="decc" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pq pr ps bk">When <code class="cx ng nh ni nj b">l1_ratio </code>is set to a value of 0, the ridge regression penalty is used.</li><li id="0171" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">When <code class="cx ng nh ni nj b">l1_ratio </code>is set to a value of 1, the lasso regression penalty is used.</li><li id="113b" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">When <code class="cx ng nh ni nj b">l1_ratio </code>is set to a value between 0 and 1, a mixture of both penalties are used.</li></ul><p id="f190" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Choosing values for <code class="cx ng nh ni nj b">alpha</code> and <code class="cx ng nh ni nj b">l1_ratio</code> can be challenging; however, the task is made easier through the use of cross validation, which is built into <code class="cx ng nh ni nj b">ElasticNetCV()</code>. To make the process easier, you don’t have to provide a list of values from <code class="cx ng nh ni nj b">alpha</code> and <code class="cx ng nh ni nj b">l1_ratio</code> — you can let the method do the heavy lifting.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="8ec0" class="qg op fq nj b bg qh qi l qj qk">from sklearn.linear_model import ElasticNet, ElasticNetCV<br/><br/># Build the model<br/>elastic_net_cv = ElasticNetCV(cv=5, random_state=1)<br/><br/># Train the model<br/>elastic_net_cv.fit(X_train, y_train)<br/><br/>print(f'Best Alpha: {elastic_net_cv.alpha_}')<br/>print(f'Best L1 Ratio:{elastic_net_cv.l1_ratio_}')</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="b8a0" class="qg op fq nj b bg qh qi l qj qk">Best Alpha: 0.0013637974514517563<br/>Best L1 Ratio:0.5</span></pre><p id="6c8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Based on the printout, we can see the best values for <code class="cx ng nh ni nj b">alpha</code> and <code class="cx ng nh ni nj b">l1_ratio</code> are 0.001 and 0.5, respectively.</p><p id="9215" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To determine how well the model performed, we can calculate the Mean Squared Error and the R-squared score of the model.</p><pre class="no np nq nr ns qd nj qe bp qf bb bk"><span id="23ca" class="qg op fq nj b bg qh qi l qj qk">from sklearn.metrics import mean_squared_error<br/><br/># Predict values from the test dataset<br/>elastic_net_pred = elastic_net_cv.predict(X_test)<br/><br/>mse = mean_squared_error(y_test, elastic_net_pred)<br/>r_squared = elastic_net_cv.score(X_test, y_test)<br/><br/>print(f'Mean Squared Error: {mse}')<br/>print(f'R-squared value: {r_squared}')</span></pre><pre class="ql qd nj qe bp qf bb bk"><span id="bc7c" class="qg op fq nj b bg qh qi l qj qk">Mean Squared Error: 0.2999434011721803<br/>R-squared value: 0.7142939720612289</span></pre><h2 id="2d31" class="qm op fq bf oq qn qo qp ot qq qr qs ow ms qt qu qv mw qw qx qy na qz ra rb rc bk"><strong class="al">Conclusion</strong></h2><p id="971f" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">Based on the evaluation metrics, the model performs moderately well. However, its performance could be enhanced through some additional steps, like detecting and removing outliers, additional feature engineering, and providing a specific set of values for <code class="cx ng nh ni nj b">alpha</code> and <code class="cx ng nh ni nj b">l1_ratio</code> in <code class="cx ng nh ni nj b">ElasticNetCV()</code>. Unfortunately, those steps are beyond the scope of this simple tutorial; however, they may provide some ideas for how this project could be improved by others.</p><p id="47ae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thank you for taking the time to read this article. If you have any questions or feedback, please leave a comment.</p><h1 id="dcc3" class="oo op fq bf oq or os gq ot ou ov gt ow ox qa oz pa pb qb pd pe pf qc ph pi pj bk">References</h1><p id="9f43" class="pw-post-body-paragraph mj mk fq ml b go pk mn mo gr pl mq mr ms pm mu mv mw pn my mz na po nc nd ne fj bk">[1] H. Zou &amp; T. Hastie, Regularization and Variable Selection Via the Elastic Net, Journal of the Royal Statistical Society Series B: Statistical Methodology, Volume 67, Issue 2, April 2005, Pages 301–320, <a class="af nk" href="https://doi.org/10.1111/j.1467-9868.2005.00503.x" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1111/j.1467-9868.2005.00503.x</a></p><p id="a96a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras &amp; Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems (2021), O’Reilly.</p><p id="2a07" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, &amp; Reis,J.. (2009). Wine Quality. UCI Machine Learning Repository. <a class="af nk" href="https://doi.org/10.24432/C56S3T" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.24432/C56S3T</a>.</p></div></div></div></div>    
</body>
</html>