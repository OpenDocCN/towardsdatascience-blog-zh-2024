<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deploying LLMs Into Production Using TensorRT LLM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deploying LLMs Into Production Using TensorRT LLM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-llms-into-production-using-tensorrt-llm-ed36e620dac4?source=collection_archive---------2-----------------------#2024-02-22">https://towardsdatascience.com/deploying-llms-into-production-using-tensorrt-llm-ed36e620dac4?source=collection_archive---------2-----------------------#2024-02-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="09e5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A guide on accelerating inference performance</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@het.trivedi05?source=post_page---byline--ed36e620dac4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Het Trivedi" class="l ep by dd de cx" src="../Images/f6f11a66f60cacc6b553c7d1682b2fc6.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*3EfA7c5YuyQhBXYkfYkVaw@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ed36e620dac4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@het.trivedi05?source=post_page---byline--ed36e620dac4--------------------------------" rel="noopener follow">Het Trivedi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ed36e620dac4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/dbf53f99c8b64aab618b12890f6e345d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tsMPT4zH585Mk-I0K4PPg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author — Created using Stable Diffusion XL</figcaption></figure><h2 id="d913" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Intro</h2><p id="b77f" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Open-source large language models have lived up to the hype. Many companies that use GPT-3.5 or GPT-4 in production have realized that these models are simply not scalable from a cost perspective. Because of this, enterprises are looking for good open-source alternatives. Recent models like Mixtral and Llama 2 have shown stellar results when it comes to output quality. But, scaling these models to support thousands of concurrent users still remains a challenge.</p><p id="1ba6" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">While frameworks such as <a class="af oy" href="https://github.com/vllm-project/vllm" rel="noopener ugc nofollow" target="_blank">vLLM</a> and <a class="af oy" href="https://github.com/huggingface/text-generation-inference" rel="noopener ugc nofollow" target="_blank">TGI</a> are a great starting point for boosting inference, they lack some optimizations, making it difficult to scale them in production.</p><p id="48a2" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This is where <a class="af oy" href="https://github.com/NVIDIA/TensorRT-LLM" rel="noopener ugc nofollow" target="_blank">TensorRT-LLM</a> comes in. TensorRT-LLM is an open-source framework designed by Nvidia to boost the performance of large language models in a production environment. Most of the big shots such as Anthropic, OpenAI, Anyscale, etc. are already using this framework to serve LLMs to millions of users.</p><h2 id="236b" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Understanding TensorRT-LLM</h2><p id="923e" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Unlike other inference techniques, TensorRT LLM does not serve the model using raw weights. Instead, it compiles the model and optimizes the kernels to enable efficient serving on an Nvidia GPU. The performance benefits of running a compiled model are far greater than running it raw. This is one of the main reasons why TensorRT LLM is blazing fast.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oz"><img src="../Images/1c95449fd97a6e24f5c8900d72146452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1-4jmXXgeQJ0glPpHRAmYA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The raw model gets compiled into an optimized binary</figcaption></figure><p id="1480" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The raw model weights along with optimization options such as quantization level, tensor parallelism, pipeline parallelism, etc. get passed to the compiler. The compiler then takes that information and outputs a model binary that is optimized for the specific GPU.</p><p id="1045" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">An important thing to note is that the entire model compilation process MUST take place on a GPU. The compiled model that gets generated is optimized specifically on the GPU that it is run on. For example, if you compile the model on a A40 GPU, you won’t be able to run it on an A100 GPU. So whatever GPU is used during compilation, the same GPU must get used for inference.</p><p id="2a4a" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">TensorRT LLM does not support all large language models out of the box. The reason is that each model architecture is different and TensorRT does deep graph level optimizations. With that being said, most of the popular models such as Mistral, Llama, and Qwen are supported. If you’re curious about the full list of supported models you can check the <a class="af oy" href="https://github.com/NVIDIA/TensorRT-LLM?tab=readme-ov-file#models" rel="noopener ugc nofollow" target="_blank">TensorRT LLM Github repository</a>.</p><h2 id="1b91" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Benefits Of Using TensorRT-LLM</h2><p id="6041" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">The TensorRT LLM python package allows developers to run LLMs at peak performance without having to know C++ or CUDA. On top of that, it comes with handy features such as token streaming, paged attention, and KV cache. Let’s dig a bit deeper into a few of these topics.</p><ol class=""><li id="1b27" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pa pb pc bk"><strong class="oc fr">Paged Attention</strong></li></ol><p id="bdaa" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Large language models require a lot of memory to store the keys and values for each token. This memory usage grows very large as the input sequence gets longer.</p><p id="c600" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">With regular attention, the keys and values for a sequence have to be stored contiguously. So even if you free up space in the middle of the sequence’s memory allocation, you can’t use that space for other sequences. This causes fragmentation and waste.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/08d85fea7a59e19130c1bb21226f9b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4Fa_73pqZQac7Qms4u_Vw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">“Attention”: All of the tokens have to be kept in a contiguous block of memory even if there is free space</figcaption></figure><p id="8285" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">With paged attention, each page of keys/values can be placed anywhere in memory, non-contiguous. So if you free up some pages in the middle, that space can now be reused for other sequences.</p><p id="4e2b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This prevents fragmentation and allows higher memory utilization. Pages can be allocated and freed dynamically as needed when generating the output sequence.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pe"><img src="../Images/c8cce889733d34153a40213a72e7d61b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0x4mcMsHraxWOnH6j_JyA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">“Paged Attention”: Previously generated tokens can be deleted from memory by deleting the whole page. This makes space for new sequences.</figcaption></figure><p id="b2ca" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">2. <strong class="oc fr">Efficient KV Caching</strong></p><p id="ebd4" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">KV caches stand for “key-value caches” and are used to cache parts of large language models (LLMs) to improve inference speed and reduce memory usage.</p><p id="a3fc" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">LLMs have billions of parameters, making them slow and memory- intensive to run inferences on. KV caches help address this by caching the layer outputs and activations of the LLM so they don’t need to be recomputed for every inference.</p><p id="09f9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Here’s how it works:</p><ul class=""><li id="c4b4" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">During inference, as the LLM executes each layer, the outputs are cached to a key-value store with a unique key.</li><li id="b6b5" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">When subsequent inferences use the same layer inputs, instead of recomputing the layer, the cached outputs are retrieved using the key.</li><li id="20d6" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">This avoids redundant computations and reduces activation memory, improving inference speed and memory efficiency.</li></ul><p id="0b6f" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Alright, enough with the theory. Let’s deploy a model for real!</p></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="88b8" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Hands-On Python Tutorial</h2><p id="3b89" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">There are two steps to deploy a model using TensorRT-LLM:</p><ol class=""><li id="895d" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pa pb pc bk">Compile the model</li><li id="0eb8" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pa pb pc bk">Deploy the compiled model as a REST API endpoint</li></ol><h2 id="ff3f" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Step 1: Compiling the model</h2><p id="f4f1" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">For this tutorial, we will be working with <a class="af oy" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">Mistral 7B Instruct v0.2</a>. As mentioned earlier, the compilation phase requires a GPU. I found the easiest way to compile a model is on a Google Colab notebook.</p><div class="pt pu pv pw px py"><a href="https://colab.research.google.com/drive/1tJSMGbqYDstnChytaFb9F37oBspDcsRL?usp=sharing&amp;source=post_page-----ed36e620dac4--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pz ab ig"><div class="qa ab co cb qb qc"><h2 class="bf fr hw z io qd iq ir qe it iv fp bk">Mistral 7B Compiler Google Colaboratory</h2><div class="qf l"><h3 class="bf b hw z io qd iq ir qe it iv dx">Edit description</h3></div><div class="qg l"><p class="bf b dy z io qd iq ir qe it iv dx">colab.research.google.com</p></div></div><div class="qh l"><div class="qi l qj qk ql qh qm lr py"/></div></div></a></div><blockquote class="qn qo qp"><p id="1a2b" class="oa ob qq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">TensorRT LLM is primarily supported on high end Nvidia GPUs. I ran the google colab on an A100 40GB GPU and will use the same GPU for deployment as well.</p></blockquote><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="c034" class="qv nd fq qs b bg qw qx l qy qz">!git clone https://github.com/NVIDIA/TensorRT-LLM.git<br/>%cd TensorRT-LLM/examples/llama</span></pre><ul class=""><li id="0a96" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">Clone the <a class="af oy" href="https://github.com/NVIDIA/TensorRT-LLM" rel="noopener ugc nofollow" target="_blank">TensorRT-LLM git repo</a>. This repo contains all of the modules and scripts we need to compile the model.</li></ul><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="465a" class="qv nd fq qs b bg qw qx l qy qz">!pip install tensorrt_llm -U --pre --extra-index-url https://pypi.nvidia.com<br/>!pip install huggingface_hub pynvml mpi4py<br/>!pip install -r requirements.txt</span></pre><ul class=""><li id="38b0" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">Install the necessary Python dependencies.</li></ul><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="f8c5" class="qv nd fq qs b bg qw qx l qy qz">from huggingface_hub import snapshot_download<br/>from google.colab import userdata<br/><br/><br/>snapshot_download(<br/>    "mistralai/Mistral-7B-Instruct-v0.2",<br/>    local_dir="tmp/hf_models/mistral-7b-instruct-v0.2",<br/>    max_workers=4<br/>)</span></pre><ul class=""><li id="e25c" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">Download the Mistral 7B Instruct v0.2 model weights from hugging face and store them in a local directory at <code class="cx ra rb rc qs b">tmp/hf_models/mistral-7b-instruct-v0.2</code></li><li id="ac13" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">If you look inside the <code class="cx ra rb rc qs b">tmp/hf_models</code> directory in Colab you should see the model weights there.</li></ul><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="0c63" class="qv nd fq qs b bg qw qx l qy qz">!python convert_checkpoint.py --model_dir ./tmp/hf_models/mistral-7b-instruct-v0.2 \<br/>                             --output_dir ./tmp/trt_engines/1-gpu/ \<br/>                             --dtype float16</span></pre><ul class=""><li id="d6dc" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">The raw model weights cannot be compiled. Instead, they have to get converted into a specific tensorRT LLM format.</li><li id="68c5" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">convert_checkpoint.py</code> script takes the raw Mistral weights and converts them into a compatible format.</li><li id="e72c" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">--model_dir</code> is the path to the raw model weights.</li><li id="07de" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">--output_dir</code> is the path to the converted weights.</li></ul><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="476f" class="qv nd fq qs b bg qw qx l qy qz">!trtllm-build --checkpoint_dir ./tmp/trt_engines/1-gpu/ \<br/>            --output_dir ./tmp/trt_engines/compiled-model/ \<br/>            --gpt_attention_plugin float16 \<br/>            --gemm_plugin float16 \<br/>            --max_input_len 32256</span></pre><ul class=""><li id="0d42" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">trtllm-build</code> command compiles the model. At this stage, you can pass in various optimization flags as well. To keep things simple, I have not used any additional optimizations.</li><li id="95a7" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">--checkpoint_dir</code> is the path to the <strong class="oc fr">converted</strong> model weights.</li><li id="2dec" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">--output_dir</code> is where the compiled model gets saved.</li><li id="beff" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Mistral 7B Instruct v0.2 supports a 32K context length. I’ve set that context length using the<code class="cx ra rb rc qs b">--max_input_length</code> flag.</li></ul><blockquote class="qn qo qp"><p id="6f2f" class="oa ob qq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Note: Compiling the model can take 15–30 minutes</p></blockquote><p id="f875" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Once the model is compiled, you can upload your compiled model to <a class="af oy" href="https://huggingface.co/new" rel="noopener ugc nofollow" target="_blank">hugging face hub</a>. In order the upload files to hugging face hub you will need a valid access token that has <strong class="oc fr">WRITE</strong> access.</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="d089" class="qv nd fq qs b bg qw qx l qy qz">import os<br/>from huggingface_hub import HfApi<br/><br/>for root, dirs, files in os.walk(f"tmp/trt_engines/compiled-model", topdown=False):<br/>    for name in files:<br/>        filepath = os.path.join(root, name)<br/>        filename = "/".join(filepath.split("/")[-2:])<br/>        print("uploading file: ", filename)<br/>        api = HfApi(token=userdata.get('HF_WRITE_TOKEN'))<br/>        api.upload_file(<br/>            path_or_fileobj=filepath,<br/>            path_in_repo=filename,<br/>            repo_id="&lt;your-repo-id&gt;/mistral-7b-v0.2-trtllm"<br/>        )</span></pre><ul class=""><li id="17f3" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">This code uploads the compiled model, the <em class="qq">.engine</em> file, to hugging face under your user id.</li><li id="63ce" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Replace the <strong class="oc fr">&lt;your-repo-id&gt;</strong> in the code with your hugging face repo which is usually your hugging face user id.</li></ul><p id="6602" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Awesome! That finishes the model compilation part. Onto the deployment step.</p><h2 id="7882" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Step 2: Deploying the compiled model</h2><p id="a43a" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">There are a lot of ways to deploy this compiled model. You can use a simple tool like <a class="af oy" href="https://fastapi.tiangolo.com/" rel="noopener ugc nofollow" target="_blank">FastAPI</a> or something more complex like the <a class="af oy" href="https://github.com/triton-inference-server/server" rel="noopener ugc nofollow" target="_blank">triton inference server</a>.</p><p id="6980" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">When using a tool like FastAPI, the developer has to set up the API server, write the Dockerfile, and configure CUDA correctly. Managing these things can be a real pain and it ruins the overall developer experience.</p><p id="1573" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">To avoid these issues, I’ve decided to use a simple open-source tool called <a class="af oy" href="https://github.com/basetenlabs/truss" rel="noopener ugc nofollow" target="_blank">Truss</a>. Truss allows developers to easily package their models with GPU support and run them on any cloud environment. It has a ton of great features that make containerizing models a breeze:</p><ul class=""><li id="eeee" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">GPU support out of the box. No need to deal with CUDA.</li><li id="7b61" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Automatic Dockerfile creation. No need to write it yourself.</li><li id="f959" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Production ready API server</li><li id="574e" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Simple python interface</li></ul><p id="37a0" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The main benefit of using Truss is that you can easily containerize a model with GPU support and deploy it to any cloud environment.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/02773e33ebbadeef2057f2f78e894370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZGlORvx7G1gAG_e1GIsKw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Build the Truss once. Deploy is anywhere.</figcaption></figure><p id="193a" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Creating the Truss</strong></p><p id="d342" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Create or open a python virtual environment with python version ≥ 3.8 and install the following dependency:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="9d02" class="qv nd fq qs b bg qw qx l qy qz">pip install --upgrade truss</span></pre><p id="70ef" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">(Optional)</strong> If you want to create your Truss project from scratch you can run the command:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="aca9" class="qv nd fq qs b bg qw qx l qy qz">truss init mistral-7b-tensort-llm</span></pre><p id="3eb8" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">You will be prompted to give your model a name. Any name such as <em class="qq">Mistral 7B Tensort LLM</em> will do. Running the command above auto generates the required files to deploy a Truss.</p><p id="4cef" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">To speed the process up a bit, I have a Github repository that contains the required files. Please clone the Github repository below:</p><div class="pt pu pv pw px py"><a href="https://github.com/htrivedi99/mistral-7b-tensorrt-llm-truss?source=post_page-----ed36e620dac4--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pz ab ig"><div class="qa ab co cb qb qc"><h2 class="bf fr hw z io qd iq ir qe it iv fp bk">GitHub - htrivedi99/mistral-7b-tensorrt-llm-truss</h2><div class="qf l"><h3 class="bf b hw z io qd iq ir qe it iv dx">Contribute to htrivedi99/mistral-7b-tensorrt-llm-truss development by creating an account on GitHub.</h3></div><div class="qg l"><p class="bf b dy z io qd iq ir qe it iv dx">github.com</p></div></div><div class="qh l"><div class="re l qj qk ql qh qm lr py"/></div></div></a></div><p id="041e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This is what the directory structure should look like for <code class="cx ra rb rc qs b">mistral-7b-tensorrt-llm-truss</code> :</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="85ab" class="qv nd fq qs b bg qw qx l qy qz">├── mistral-7b-tensorrt-llm-truss<br/>│   ├── config.yaml<br/>│   ├── model<br/>│   │   ├── __init__.py<br/>│   │   └── model.py<br/>|   |   └── utils.py<br/>|   ├── requirements.txt</span></pre><p id="217f" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Here’s a quick breakdown of what the files above are used for:</p><ol class=""><li id="533e" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pa pb pc bk">The <code class="cx ra rb rc qs b">config.yaml</code> is used to set various configurations for your model, including its resources, dependencies, environmental variables, and more. This is where we can specify the model name, which Python dependencies to install, as well as which system packages to install.</li></ol><p id="8057" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">2. The <code class="cx ra rb rc qs b">model/model.py</code> is the heart of Truss. It contains the Python code that will get executed on the Truss server. In the <code class="cx ra rb rc qs b">model.py</code> there are two main methods: <code class="cx ra rb rc qs b">load()</code> and <code class="cx ra rb rc qs b">predict()</code> .</p><ul class=""><li id="03bc" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">load</code> method is where we’ll download the compiled model from hugging face and initialize the TensorRT LLM engine.</li><li id="ba65" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">predict</code> method receives HTTP requests and calls the model.</li></ul><p id="fd25" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">3. The <code class="cx ra rb rc qs b">model/utils.py</code> contains some helper functions for the <code class="cx ra rb rc qs b">model.py</code> file. I did not write the <code class="cx ra rb rc qs b">utils.py</code> file myself, I took it directly from the <a class="af oy" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/utils.py" rel="noopener ugc nofollow" target="_blank">TensorRT LLM repository</a>.</p><p id="bbd8" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">4. The <code class="cx ra rb rc qs b">requirements.txt</code> contains the necessary Python dependencies to run our compiled model.</p><p id="a090" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Deeper Code Explanation:</strong></p><p id="86e1" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The <code class="cx ra rb rc qs b">model.py</code> contains the main code that gets executed, so let’s dig a bit deeper into that file. Let’s first take a look at the <code class="cx ra rb rc qs b">load</code> function.</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="8a80" class="qv nd fq qs b bg qw qx l qy qz">import subprocess<br/>subprocess.run(["pip", "install", "tensorrt_llm", "-U", "--pre", "--extra-index-url", "https://pypi.nvidia.com"])<br/><br/>import torch<br/>from model.utils import (DEFAULT_HF_MODEL_DIRS, DEFAULT_PROMPT_TEMPLATES,<br/>                   load_tokenizer, read_model_name, throttle_generator)<br/><br/>import tensorrt_llm<br/>import tensorrt_llm.profiler<br/>from tensorrt_llm.runtime import ModelRunnerCpp, ModelRunner<br/>from huggingface_hub import snapshot_download<br/><br/>STOP_WORDS_LIST = None<br/>BAD_WORDS_LIST = None<br/>PROMPT_TEMPLATE = None<br/><br/>class Model:<br/>    def __init__(self, **kwargs):<br/>        self.model = None<br/>        self.tokenizer = None<br/>        self.pad_id = None<br/>        self.end_id = None<br/>        self.runtime_rank = None<br/>        self._data_dir = kwargs["data_dir"]<br/><br/>    def load(self):<br/>        snapshot_download(<br/>            "htrivedi99/mistral-7b-v0.2-trtllm",<br/>            local_dir=self._data_dir,<br/>            max_workers=4,<br/>        )<br/><br/>        self.runtime_rank = tensorrt_llm.mpi_rank()<br/><br/>        model_name, model_version = read_model_name(f"{self._data_dir}/compiled-model")<br/>        tokenizer_dir = "mistralai/Mistral-7B-Instruct-v0.2"<br/><br/>        self.tokenizer, self.pad_id, self.end_id = load_tokenizer(<br/>            tokenizer_dir=tokenizer_dir,<br/>            vocab_file=None,<br/>            model_name=model_name,<br/>            model_version=model_version,<br/>            tokenizer_type="llama",<br/>        )<br/><br/><br/>        runner_cls = ModelRunner<br/>        runner_kwargs = dict(engine_dir=f"{self._data_dir}/compiled-model",<br/>                             lora_dir=None,<br/>                             rank=self.runtime_rank,<br/>                             debug_mode=False,<br/>                             lora_ckpt_source="hf",<br/>                            )<br/><br/>        self.model = runner_cls.from_dir(**runner_kwargs)</span></pre><p id="cc54" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">What’s happening here:</p><ul class=""><li id="d6fe" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">At the top of the file we import the necessary modules, specifically <code class="cx ra rb rc qs b">tensorrt_llm</code></li><li id="e9c3" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Next, inside the load function, we download the compiled model using the <code class="cx ra rb rc qs b">snapshot_download</code> function. My compiled model is at the following repo id: <code class="cx ra rb rc qs b">htrivedi99/mistral-7b-v0.2-trtllm</code> . If you uploaded your compiled model elsewhere, update this value accordingly.</li><li id="8b1e" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Then, we download the tokenizer for the model using the <code class="cx ra rb rc qs b">load_tokenizer</code> function that comes with <code class="cx ra rb rc qs b">model/utils.py</code> .</li><li id="2fc5" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Finally, we use TensorRT LLM to load our compiled model using the <code class="cx ra rb rc qs b">ModelRunner</code> class.</li></ul><p id="d7c4" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Cool, let’s take a look at the <code class="cx ra rb rc qs b">predict</code> function as well.</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="404b" class="qv nd fq qs b bg qw qx l qy qz">def predict(self, request: dict):<br/><br/>        prompt = request.pop("prompt")<br/>        max_new_tokens = request.pop("max_new_tokens", 2048)<br/>        temperature = request.pop("temperature", 0.9)<br/>        top_k = request.pop("top_k",1)<br/>        top_p = request.pop("top_p", 0)<br/>        streaming = request.pop("streaming", False)<br/>        streaming_interval = request.pop("streaming_interval", 3)<br/><br/>        batch_input_ids = self.parse_input(tokenizer=self.tokenizer,<br/>                                      input_text=[prompt],<br/>                                      prompt_template=None,<br/>                                      input_file=None,<br/>                                      add_special_tokens=None,<br/>                                      max_input_length=1028,<br/>                                      pad_id=self.pad_id,<br/>                                      )<br/>        input_lengths = [x.size(0) for x in batch_input_ids]<br/><br/>        outputs = self.model.generate(<br/>            batch_input_ids,<br/>            max_new_tokens=max_new_tokens,<br/>            max_attention_window_size=None,<br/>            sink_token_length=None,<br/>            end_id=self.end_id,<br/>            pad_id=self.pad_id,<br/>            temperature=temperature,<br/>            top_k=top_k,<br/>            top_p=top_p,<br/>            num_beams=1,<br/>            length_penalty=1,<br/>            repetition_penalty=1,<br/>            presence_penalty=0,<br/>            frequency_penalty=0,<br/>            stop_words_list=STOP_WORDS_LIST,<br/>            bad_words_list=BAD_WORDS_LIST,<br/>            lora_uids=None,<br/>            streaming=streaming,<br/>            output_sequence_lengths=True,<br/>            return_dict=True)<br/><br/>        if streaming:<br/>            streamer = throttle_generator(outputs, streaming_interval)<br/><br/>            def generator():<br/>                total_output = ""<br/>                for curr_outputs in streamer:<br/>                    if self.runtime_rank == 0:<br/>                        output_ids = curr_outputs['output_ids']<br/>                        sequence_lengths = curr_outputs['sequence_lengths']<br/>                        batch_size, num_beams, _ = output_ids.size()<br/>                        for batch_idx in range(batch_size):<br/>                            for beam in range(num_beams):<br/>                                output_begin = input_lengths[batch_idx]<br/>                                output_end = sequence_lengths[batch_idx][beam]<br/>                                outputs = output_ids[batch_idx][beam][<br/>                                          output_begin:output_end].tolist()<br/>                                output_text = self.tokenizer.decode(outputs)<br/><br/>                                current_length = len(total_output)<br/>                                total_output = output_text<br/>                                yield total_output[current_length:]<br/>            return generator()<br/>        else:<br/>            if self.runtime_rank == 0:<br/>                output_ids = outputs['output_ids']<br/>                sequence_lengths = outputs['sequence_lengths']<br/>                batch_size, num_beams, _ = output_ids.size()<br/>                for batch_idx in range(batch_size):<br/>                    for beam in range(num_beams):<br/>                        output_begin = input_lengths[batch_idx]<br/>                        output_end = sequence_lengths[batch_idx][beam]<br/>                        outputs = output_ids[batch_idx][beam][<br/>                                  output_begin:output_end].tolist()<br/>                        output_text = self.tokenizer.decode(outputs)<br/>                        return {"output": output_text}</span></pre><p id="9899" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">What’s happening here:</p><ul class=""><li id="a66b" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">The <code class="cx ra rb rc qs b">predict</code> function accepts a few model inputs such as the <code class="cx ra rb rc qs b">prompt</code> , <code class="cx ra rb rc qs b">max_new_tokens</code> , <code class="cx ra rb rc qs b">temperature</code> , etc. We extract all of these values at the top of the function using the <code class="cx ra rb rc qs b">request.pop</code> method.</li><li id="6481" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Next, we format the prompt into the required format for TensorRT LLM using the <code class="cx ra rb rc qs b">self.parse_input</code> helper function.</li><li id="44f8" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Then, we call our LLM model to generate the outputs using the <code class="cx ra rb rc qs b">self.model.generate</code> function. The generate function accepts a variety of arguments that help control the output of the LLM.</li><li id="c3c5" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">I’ve also added some code to enable streaming by producing a <code class="cx ra rb rc qs b">generator</code> object. If streaming is disabled, the tokenizer simply decodes the output of the LLM and returns it as a JSON object.</li></ul><p id="53a9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Awesome! That covers the coding portion. Let’s containerize it.</p><p id="0b4d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Containerizing the model:</strong></p><p id="60fe" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">In order to run our model in the cloud we need to containerize it. Truss will take care of creating the Dockerfile and packaging everything for us, so we don’t have to do much.</p><p id="cde6" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Outside of the <code class="cx ra rb rc qs b">mistral-7b-tensorrt-llm-truss</code> directory create a file called <code class="cx ra rb rc qs b">main.py</code> . Paste the following code inside it:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="9eea" class="qv nd fq qs b bg qw qx l qy qz">import truss<br/>from pathlib import Path<br/><br/>tr = truss.load("./mistral-7b-tensorrt-llm-truss")<br/>command = tr.docker_build_setup(build_dir=Path("./mistral-7b-tensorrt-llm-truss"))<br/>print(command)</span></pre><p id="b996" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Run the <code class="cx ra rb rc qs b">main.py</code> file and look inside the <code class="cx ra rb rc qs b">mistral-7b-tensorrt-llm-truss</code> directory. You should see a bunch of files get auto-generated. We don’t need to worry about what these files mean, it’s just Truss doing its magic.</p><p id="8dbc" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Next, let’s build our container using docker. Run the commands below sequentially:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="4d8c" class="qv nd fq qs b bg qw qx l qy qz">docker build mistral-7b-tensorrt-llm-truss -t mistral-7b-tensorrt-llm-truss:latest<br/>docker tag mistral-7b-tensorrt-llm-truss &lt;docker_user_id&gt;/mistral-7b-tensorrt-llm-truss<br/>docker push &lt;docker_user_id&gt;/mistral-7b-tensorrt-llm-truss</span></pre><p id="14b9" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Sweet! We’re ready to deploy the model in the cloud!</p><h2 id="743d" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Deploying the model in GKE</h2><p id="40c0" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">For this section, we’ll be deploying the model on Google Kubernetes Engine. If you recall, during the model compilation step we ran the Google Colab on an A100 40GB GPU. For TensorRT LLM to work, we need to deploy the model on the exact same GPU for inference.</p><p id="e29d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">I won’t go super deep into how to set up a GKE cluster as it’s not in the scope of this article. But, I will provide the specs I used for the cluster. Here are the specs:</p><ul class=""><li id="6b6f" class="oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os pf pb pc bk">1 node, standard kubernetes cluster (not autopilot)</li><li id="197e" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">1.28.5 gke kubernetes version</li><li id="ac26" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">1 Nvidia A100 40GB GPU</li><li id="ec35" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">a2-highgpu-1g machine (12 vCPU, 85 GB memory)</li><li id="bd8c" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">Google managed GPU Driver installation (Otherwise we need to install Cuda driver manually)</li><li id="ab3a" class="oa ob fq oc b go pg oe of gr ph oh oi nn pi ok ol nr pj on oo nv pk oq or os pf pb pc bk">All of this will run on a spot instance</li></ul><p id="cf29" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Once the cluster is configured, we can launch it and connect to it. After the cluster is active and you’ve successfully connected to it, create the following kubernetes deployment:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="6c8e" class="qv nd fq qs b bg qw qx l qy qz">apiVersion: apps/v1<br/>kind: Deployment<br/>metadata:<br/>  name: mistral-7b-v2-trt<br/>  namespace: default<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    matchLabels:<br/>      component: mistral-7b-v2-trt-layer<br/>  template:<br/>    metadata:<br/>      labels:<br/>        component: mistral-7b-v2-trt-layer<br/>    spec:<br/>      containers:<br/>      - name: mistral-container<br/>        image: htrivedi05/mistral-7b-v0.2-trt:latest<br/>        ports:<br/>          - containerPort: 8080<br/>        resources:<br/>          limits:<br/>            nvidia.com/gpu: 1<br/>      nodeSelector:<br/>        cloud.google.com/gke-accelerator: nvidia-tesla-a100<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: mistral-7b-v2-trt-service<br/>  namespace: default<br/>spec:<br/>  type: ClusterIP<br/>  selector:<br/>    component: mistral-7b-v2-trt-layer<br/>  ports:<br/>  - port: 8080<br/>    protocol: TCP<br/>    targetPort: 8080</span></pre><p id="d06e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This is a standard kubernetes deployment that runs a container with the image <code class="cx ra rb rc qs b">htrivedi05/mistral-7b-v0.2-trt:latest</code> . If you created your own image in the previous section, go ahead and use that. Feel free to use mine otherwise.</p><p id="276c" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">You can create the deployment by running the command:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="6f01" class="qv nd fq qs b bg qw qx l qy qz">kubectl create -f mistral-deployment.yaml</span></pre><p id="3603" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">It takes a few minutes for the kubernetes pod to be provisioned. Once the pod is running, the load function we wrote earlier will get executed. You can check the logs of the pod by running the command:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="ffe3" class="qv nd fq qs b bg qw qx l qy qz">kubectl logs &lt;pod-name&gt;</span></pre><p id="b0dd" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Once the model is loaded, you will see something like <code class="cx ra rb rc qs b">Completed model.load() execution in 449234 ms</code> in the pod logs. To send a request to the model via HTTP we need to port-forward the service. You can use the command below to do that:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="c9b1" class="qv nd fq qs b bg qw qx l qy qz">kubectl port-forward svc/mistral-7b-v2-trt-service 8080</span></pre><p id="0788" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">Great! We can finally start sending requests to the model! Open up any Python script and run the following code:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="71b2" class="qv nd fq qs b bg qw qx l qy qz">import requests<br/><br/>data = {"prompt": "What is a mistral?"}<br/>res = requests.post("http://127.0.0.1:8080/v1/models/model:predict", json=data)<br/>res = res.json()<br/>print(res)</span></pre><p id="376b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">You will see an output like the following:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="9035" class="qv nd fq qs b bg qw qx l qy qz">{"output": "A Mistral is a strong, cold wind that originates in the Rhone Valley in France. It is named after the Mistral wind system, which is associated with the northern Mediterranean region. The Mistral is known for its consistency and strength, often blowing steadily for days at a time. It can reach speeds of up to 130 kilometers per hour (80 miles per hour), making it one of the strongest winds in Europe. The Mistral is also known for its clear, dry air and its role in shaping the landscape and climate of the Rhone Valley."}</span></pre><p id="ff08" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">The performance of TensorRT LLM can be visibly noticed when the tokens are streamed. Here’s an example of how to do that:</p><pre class="mm mn mo mp mq qr qs qt bp qu bb bk"><span id="04a1" class="qv nd fq qs b bg qw qx l qy qz">data = {"prompt": "What is mistral wind?", "streaming": True, "streaming_interval": 3}<br/>res = requests.post("http://127.0.0.1:8080/v1/models/model:predict", json=data, stream=True)<br/><br/>for content in res.iter_content():<br/>    print(content.decode("utf-8"), end="", flush=True)</span></pre><p id="dd8e" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">This mistral model has a fairly large context window, so feel free to try it out with different prompts.</p><h2 id="12bb" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Performance Benchmarks</h2><p id="0772" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">Just by looking at the tokens being streamed, you can probably tell TensorRT LLM is really fast. However, I wanted to get real numbers to capture the performance gains of using TensorRT LLM. I ran some custom benchmarks and got the following results:</p><p id="221d" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Small Prompt:</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/5a6f8b5d892474e2c42ece02b29c7454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B62sg29SPcVSr4n-_8G1uw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Hugging Face vs TensorRT LLM benchmarks for small prompt</figcaption></figure><p id="4bff" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Medium prompt:</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/75ce9cad155634a4a8edaf2c4cf452ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEza2b4jdOlXR8u-XpCx2w.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Hugging Face vs TensorRT LLM benchmarks for medium prompt</figcaption></figure><p id="5f59" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk"><strong class="oc fr">Large prompt:</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rh"><img src="../Images/7cdbad4a1946898f79af8aeeeab9d2d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbJe7w4dpxjKbRE_h2j6TA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Hugging Face vs TensorRT LLM benchmarks for large prompt</figcaption></figure><h2 id="096a" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Conclusion</h2><p id="91a4" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">In this blog post, my goal was to demonstrate how state-of-the-art inference can be achieved using TensorRT LLM. We covered everything from compiling an LLM to deploying the model in production.</p><p id="b82b" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">While TensorRT LLM is more complex than other inferencing optimizers, the performance speaks for itself. This tool provides state-of-the-art LLM optimizations while being completely open-source and is designed for commercial use. This framework is still in the early stages and is under active development. The performance we see today will only improve in the coming years.</p><p id="3dbc" class="pw-post-body-paragraph oa ob fq oc b go ot oe of gr ou oh oi nn ov ok ol nr ow on oo nv ox oq or os fj bk">I hope you found something valuable in this article. Thanks for reading!</p><h2 id="0332" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Enjoyed This Story?</h2><p id="822e" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk"><em class="qq">Consider </em><a class="af oy" href="https://medium.com/@het.trivedi05/subscribe" rel="noopener"><em class="qq">subscribing</em></a><em class="qq"> for free.</em></p><div class="pt pu pv pw px py"><a href="https://medium.com/@het.trivedi05/subscribe?source=post_page-----ed36e620dac4--------------------------------" rel="noopener follow" target="_blank"><div class="pz ab ig"><div class="qa ab co cb qb qc"><h2 class="bf fr hw z io qd iq ir qe it iv fp bk">Get an email whenever Het Trivedi publishes.</h2><div class="qf l"><h3 class="bf b hw z io qd iq ir qe it iv dx">Get an email whenever Het Trivedi publishes. By signing up, you will create a Medium account if you don't already have…</h3></div><div class="qg l"><p class="bf b dy z io qd iq ir qe it iv dx">medium.com</p></div></div><div class="qh l"><div class="ri l qj qk ql qh qm lr py"/></div></div></a></div><h2 id="2725" class="nc nd fq bf ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Images</h2><p id="735d" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi nn oj ok ol nr om on oo nv op oq or os fj bk">If not otherwise stated, all images are created by the author.</p></div></div></div></div>    
</body>
</html>