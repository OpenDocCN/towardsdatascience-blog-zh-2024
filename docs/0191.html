<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building Blocks of Time: The Mathematical Foundation and Python Implementation of RNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building Blocks of Time: The Mathematical Foundation and Python Implementation of RNNs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20">https://towardsdatascience.com/building-blocks-of-time-the-mathematical-foundation-and-python-implementation-of-rnns-55f5ef9b108c?source=collection_archive---------3-----------------------#2024-01-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Najib Sharifi, Ph.D." class="l ep by dd de cx" src="../Images/d94932c5e3633e32247d98a3c221b181.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*gpeo9aVzjcetgo_8deQcBw.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@ns650?source=post_page---byline--55f5ef9b108c--------------------------------" rel="noopener follow">Najib Sharifi, Ph.D.</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--55f5ef9b108c--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">4</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d6be" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Is being able to build and train machine learning models from popular libraries sufficient for machine learning users? Probably not for too long. With tools like AutoAI on the rise, it is likely that a lot of the very traditional machine learning skills like building model architectures with common libraries like Pytorch will be less important.</p><p id="5d64" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">What is likely to persist is the demand for skilled users with a deep understanding of the underlying principles of ML, particularly in problems that require novel challenges, customisation, optimisation. To be more innovative and novel, it is important to have a deep understanding of the mathematical foundations of these algorithms. In this article, we’ll look at the mathematical description of one such important model, Recurrent Neural Network (RNN).</p><p id="e252" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Time series data (or any sequential data like language) has a temporal dependencies and is widespread across various sectors ranging from weather prediction to medical applications. RNN is a powerful tool for capturing sequential patterns in such data. In this article, we’ll delve into the mathematical foundations of RNNs and implement these equations from scratch using python.</p><p id="fa96" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Understanding RNNs: The Mathematical Description</strong></p><p id="5a11" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">An important element of sequential data is the temporal dependence where the past values determine the current and future values (just like the predetermined world we live in but let’s not get philosophical and stick to RNN models). Time series forecasting utilises this nature of sequential data and focuses on the prediction of the next value given previous n values. Depending on the model, this includes either mapping or regression of the past values.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div class="mv mw mx"><img src="../Images/0e3a7e8e017f7ba6f95225d897042f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*oaKS793RYczzhFnr1VMfIw.png"/></div><figcaption class="nf ng nh mv mw ni nj bf b bg z dx">Figure 1. An example of a time series data</figcaption></figure><p id="785c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Consider the point indicated with the black arrow, y and the points before y (between the red dashed line) denoting as <em class="nk">X = {x1 , x2 , ….xt …..xT}</em> where T is the total number of time steps. The RNN processes the input sequence (X) by placing each input through a hidden state (or sometimes refered to as memory state) and outputs y. These hidden states allow the model to capture and remember patterns from earlier points in the sequence.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw nl"><img src="../Images/48e402a9de5ad3a7c9390aa0ca74650d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGPgBRYKZmPsE6f-xGjEhw.png"/></div></div><figcaption class="nf ng nh mv mw ni nj bf b bg z dx">Figure 2. A schematic of an RNN model, showing the inputs, hidden states and outputs</figcaption></figure><p id="d8f4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now let’s look at the mathematical operations within the RNN model, first lets consider the forward pass, we’ll worry about the model optimisation later.</p><p id="7a7d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Forward Pass</strong></p><p id="508c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The forward pass is fairly straightforward and is as follows:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw nq"><img src="../Images/ec00274d244ae652d203e44f54b13a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOiImwvRn1SNk0OLHsUjgQ.png"/></div></div></figure><p id="61c9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Backpropagation Through Time</strong></p><p id="88e6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In machine learning, the optimisation (variable updates) are done using the gradient descent method:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw nr"><img src="../Images/dd4700c9a1c5a0614ad4ea4351d998e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6DyiMxFGyRGxt5Vkzbj_Q.png"/></div></div></figure><p id="2d05" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Therefore, all parameters that need updating during training will require their partial derivatives. Here we’ll derive the partial derivative of the loss function with respect to each variable included in the forward pass equations:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw ns"><img src="../Images/4411243d3731ffa01a9bd02c9c338287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UAwt5Lcw_jnxe3QHE-RtQ.png"/></div></div></figure><p id="d5fb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By noting the forward pass equations and network schematic in Figure 2, we can see that at time T, L only depends on a_T via y_T i.e.</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw nt"><img src="../Images/25fbb6ecd623f6e156fab86813069655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giMmk6-e7nb8zt3_JXQvSQ.png"/></div></div></figure><p id="fde6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">However, for t &lt; T, L depends on a_T via y_T and a_(T+1) so let’s use the chain rule for both:</p><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw nu"><img src="../Images/18427bf16f7c05a722b20a4fef9ebe1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KUQCjfZrpxRH7qpPefoWw.png"/></div></div></figure><p id="ca18" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now we have the equations for the gradient of the loss function with respect all parameters present in the forward pass equation. This algorithm is called Backpropagation Through Time. It is important to clarify that for a time series data, usually only the last value contribute to the Loss function i.e. all other outputs are ignored and their contribution to the loss function set to 0. The mathematical description is the same as that presented. Now Let’s code these equations in python and apply it to an example dataset.</p></div></div></div><div class="ab cb nv nw nx ny" role="separator"><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob oc"/><span class="nz by bm oa ob"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f816" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">The Coding Implementation</strong></p><p id="034d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Before we can implement the equations above, we’ll need to import the necessary dataset, preprocess and ready for the model training. All of this work is very standard in any time series analysis.</p><pre class="my mz na nb nc od oe of bp og bb bk"><span id="c43a" class="oh oi fq oe b bg oj ok l ol om">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import plotly.graph_objs as go<br/>from plotly.offline import iplot<br/>import yfinance as yf<br/>import datetime as dt<br/>import math<br/><br/>#### Data Processing<br/>start_date = dt.datetime(2020,4,1)<br/>end_date = dt.datetime(2023,4,1)<br/><br/>#loading from yahoo finance<br/>data = yf.download("GOOGL",start_date, end_date)<br/><br/>pd.set_option('display.max_rows', 4)<br/>pd.set_option('display.max_columns',5)<br/>display(data)<br/><br/><br/># #Splitting the dataset<br/>training_data_len = math.ceil(len(data) * .8)<br/>train_data = data[:training_data_len].iloc[:,:1]<br/>test_data = data[training_data_len:].iloc[:,:1]<br/><br/>dataset_train = train_data.Open.values<br/># Reshaping 1D to 2D array<br/>dataset_train = np.reshape(dataset_train, (-1,1))<br/>dataset_train.shape<br/>scaler = MinMaxScaler(feature_range=(0,1))<br/># scaling dataset<br/>scaled_train = scaler.fit_transform(dataset_train)<br/><br/>dataset_test = test_data.Open.values<br/>dataset_test = np.reshape(dataset_test, (-1,1))<br/>scaled_test = scaler.fit_transform(dataset_test)<br/><br/><br/>X_train = []<br/>y_train = []<br/>for i in range(50, len(scaled_train)):<br/>    X_train.append(scaled_train[i-50:i, 0])<br/>    y_train.append(scaled_train[i, 0])<br/><br/>X_test = []<br/>y_test = []<br/>for i in range(50, len(scaled_test)):<br/>    X_test.append(scaled_test[i-50:i, 0])<br/>    y_test.append(scaled_test[i, 0])<br/><br/># The data is converted to Numpy array<br/>X_train, y_train = np.array(X_train), np.array(y_train)<br/><br/>#Reshaping<br/>X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))<br/>y_train = np.reshape(y_train, (y_train.shape[0],1))<br/>print("X_train :",X_train.shape,"y_train :",y_train.shape)<br/><br/># The data is converted to numpy array<br/>X_test, y_test = np.array(X_test), np.array(y_test)<br/><br/>#Reshaping<br/>X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))<br/>y_test = np.reshape(y_test, (y_test.shape[0],1))</span></pre><p id="af01" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">The model<br/></strong>Now we implement the mathematical equations. it is definitely worth reading through the code, noting the dimensions of all variables and respective derivates to give yourself a better understanding of these equations.</p><pre class="my mz na nb nc od oe of bp og bb bk"><span id="416a" class="oh oi fq oe b bg oj ok l ol om">class SimpleRNN:<br/>    def __init__(self,input_dim,output_dim, hidden_dim):<br/>        self.input_dim = input_dim<br/>        self.output_dim = output_dim<br/>        self.hidden_dim = hidden_dim<br/>        self.Waa = np.random.randn(hidden_dim, hidden_dim) * 0.01 # we initialise as non-zero to help with training later<br/>        self.Wax = np.random.randn(hidden_dim, input_dim) * 0.01<br/>        self.Way = np.random.randn(output_dim, hidden_dim) * 0.01<br/>        self.ba = np.zeros((hidden_dim, 1))<br/>        self.by = 0 # a single value shared over all outputs #np.zeros((hidden_dim, 1))<br/><br/>    def FeedForward(self, x):<br/>        # let's calculate the hidden states<br/>        a = [np.zeros((self.hidden_dim,1))]<br/>        y = []<br/>        for ii in range(len(x)):<br/><br/>            a_next = np.tanh(np.dot(self.Waa, a[ii])+np.dot(self.Wax,x[ii].reshape(-1,1))+self.ba)<br/>            a.append(a_next)<br/>            y_local = np.dot(self.Way,a_next)+self.by<br/>            y.append(np.dot(self.Way,a_next)+self.by)<br/><br/>        # remove the first a and y values used for initialisation<br/>        #a = a[1:]<br/>        return y, a<br/>    <br/>    def ComputeLossFunction(self, y_pred, y_actual):<br/>        # for a normal many to many model:<br/>        #loss = np.sum((y_pred - y_actual) ** 2)<br/>        # in our case, we are only using the last value so we expect scalar values here rather than a vector<br/>        loss = (y_pred[-1] - y_actual) ** 2<br/>        return loss<br/>    <br/>    def ComputeGradients(self, a, x, y_pred, y_actual):<br/>        # Backpropagation through time<br/>        dLdy = []<br/>        dLdby = np.zeros((self.output_dim, 1))<br/>        dLdWay = np.random.randn(self.output_dim, self.hidden_dim)/5.0<br/>        dLdWax = np.random.randn(self.hidden_dim, self.input_dim)/5.0<br/>        dLdWaa = np.zeros((self.hidden_dim, self.hidden_dim))<br/>        dLda = np.zeros_like(a)<br/>        dLdba = np.zeros((self.hidden_dim, 1))<br/>        <br/>        for t in range(self.hidden_dim-1, 0, -1):<br/>            if t == self.hidden_dim-1:<br/>                dldy = 2*(y_pred[t] - y_actual)<br/>            else:<br/>                dldy = 0<br/>            dLdy.append(dldy)<br/>            #dLdby.append(dldy)<br/>            dLdby += dldy<br/>            #print(dldy.shape)<br/>            dLdWay += np.dot(np.array(dldy).reshape(-1,1), a[t].T)<br/>            <br/>            # Calculate gradient of loss with respect to a[t]<br/>            if t == self.hidden_dim-1:<br/>                dlda_t= np.dot(self.Way.T, np.array(dldy).reshape(-1,1))<br/><br/>            else:<br/>                dlda_t = np.dot(self.Way.T, np.array(dldy).reshape(-1,1)) + np.dot(self.Waa, dLda[t+1]) * (1 - a[t]**2)<br/>            dLda[t] = dlda_t<br/>            #print(dlda_t.shape)<br/>            <br/>            rec_term = (1-a[t]*a[t])<br/>             <br/>            dLdWax += np.dot(dlda_t, x[t].reshape(-1,1))*rec_term<br/>            dLdWaa += np.dot(dlda_t, a[t-1].T)*rec_term<br/>            dLdba += dlda_t*rec_term<br/>        <br/>        return dLdy[::-1], dLdby[::-1], dLdWay, dLdWax, dLdWaa, dLdba<br/>    <br/>    def UpdateParameters(self,dLdby, dLdWay, dLdWax, dLdWaa, dLdba,learning_rate):<br/>        self.Waa -= learning_rate * dLdWaa<br/>        self.Wax -= learning_rate * dLdWax<br/>        self.Way -= learning_rate * dLdWay<br/>        self.ba -= learning_rate * dLdba<br/>        self.by -= learning_rate * dLdby    <br/>    <br/>    def predict(self, x, n, a_training):<br/>        # let's calculate the hidden states<br/>        a_future = a_training<br/>        y_predict = []<br/><br/>        # Predict the next n terms<br/>        for ii in range(n):<br/>            a_next = np.tanh(np.dot(self.Waa, a_future[-1]) + np.dot(self.Wax, x[ii]) + self.ba)<br/>            a.append(a_next)<br/>            y_predict.append(np.dot(self.Way, a_next) + self.by)<br/><br/>        return y_predict</span></pre><p id="0064" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Training and Testing the model</p><pre class="my mz na nb nc od oe of bp og bb bk"><span id="89d9" class="oh oi fq oe b bg oj ok l ol om">input_dim = 1<br/>output_dim = 1<br/>hidden_dim = 50<br/><br/>learning_rate = 1e-3<br/><br/># Initialize The RNN model<br/>rnn_model = SimpleRNN(input_dim, output_dim, hidden_dim)<br/><br/># train the model for 200 epochs<br/><br/>for epoch in range(200):<br/>    for ii in range(len(X_train)):<br/>        y_pred, a = rnn_model.FeedForward(X_train[ii])<br/>        loss = rnn_model.ComputeLossFunction(y_pred, y_train[ii])<br/>        dLdy, dLdby, dLdWay, dLdWax, dLdWaa, dLdba = rnn_model.ComputeGradients(a, X_train[ii], y_pred, y_train[ii])<br/>        rnn_model.UpdateParameters(dLdby, dLdWay, dLdWax, dLdWaa, dLdba, learning_rate)<br/>        print(f'Loss: {loss}')<br/><br/><br/>y_test_predicted = []<br/>for jj in range(len(X_test)):<br/>    forecasted_values, _ = rnn_model.FeedForward(X_test[jj])<br/>    y_test_predicted.append(forecasted_values[-1])<br/><br/>y_test_predicted_flat = np.array([val[0, 0] for val in y_test_predicted])<br/>trace1 = go.Scatter(y = y_test.ravel(), mode ="lines", name = "original data")<br/>trace2 = go.Scatter(y=y_test_predicted_flat, mode = "lines", name = "RNN output")<br/>layout = go.Layout(title='Testing data Fit', xaxis=dict(title='X-Axis'), yaxis=dict(title='Dependent Variable'))<br/>figure = go.Figure(data = [trace1,trace2], layout = layout)<br/><br/>iplot(figure)</span></pre><figure class="my mz na nb nc nd mv mw paragraph-image"><div role="button" tabindex="0" class="nm nn ed no bh np"><div class="mv mw on"><img src="../Images/781dec6b1aa00a30f39191138c70b7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MO3nC2wjCpjCpjzFrlQYYA.png"/></div></div></figure><p id="a454" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">That brings us to the end of this demonstration but hopefully only the start of your reading into these powerful models. You might find it helpful to test your understanding by experimenting with a different activation function in the forward pass. Or read further into sequential models like LSTM and transformers which are formidable tools, especially in language-related tasks. Exploring these models can deepen your understanding of more sophisticated mechanisms for handling temporal dependencies. Finally, thank you for taking the time to read this article, I hope you found it useful in your understanding of RNN or their mathematical background.</p><p id="89bb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="nk">Unless otherwise noted, all images are by the author</em></p></div></div></div></div>    
</body>
</html>