<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Linearizing Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Linearizing Attention</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26">https://towardsdatascience.com/linearizing-attention-204d3b86cc1e?source=collection_archive---------3-----------------------#2024-12-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7ba3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Breaking the quadratic barrier: modern alternatives to softmax attention</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shitanshu Bhushan" class="l ep by dd de cx" src="../Images/c9417483c279497fc8aa09b13c60d2a2.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Iems5qyZUS-dKOrTeod19g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@shitanshu273?source=post_page---byline--204d3b86cc1e--------------------------------" rel="noopener follow">Shitanshu Bhushan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--204d3b86cc1e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e89b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Large Languange Models are great but they have a slight drawback that they use softmax attention which can be computationally intensive. In this article we will explore if there is a way we can replace the softmax somehow to achieve linear time complexity.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ng"><img src="../Images/50f35225ca8cbf1bf38bd25730c93110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7QP12eq9-PQ9oLskgxBBHA.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Image by Author (Created using Miro Board)</figcaption></figure><h1 id="a6b6" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Attention Basics</h1><p id="a9fe" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">I am gonna assume you already know about stuff like ChatGPT, Claude, and how transformers work in these models. Well attention is the backbone of such models. If we think of normal RNNs, we encode all past states in some hidden state and then use that hidden state along with new query to get our output. A clear drawback here is that well you can’t store everything in just a small hidden state. This is where attention helps, imagine for each new query you could find the most relevant past data and use that to make your prediction. That is essentially what attention does.</p><p id="5ac9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Attention mechanism in transformers (the architecture behind most current language models) involve key, query and values embeddings. The attention mechanism in transformers works by matching queries against keys to retrieve relevant values. For each query(Q), the model computes similarity scores with all available keys(K), then uses these scores to create a weighted combination of the corresponding values(Y). This attention calculation can be expressed as:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf oy"><img src="../Images/5870d932c2be4a2fc1e72f7aa70610e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*YvyNhihCl5g5fst0KSKACQ.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="a65d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This mechanism enables the model to selectively retrieve and utilize information from its entire context when making predictions. We use softmax here since it effectively converts raw similarity scores into normalized probabilities, acting similar to a k-nearest neighbor mechanism where higher attention weights are assigned to more relevant keys.</p><p id="f433" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Okay now let’s see the computational cost of 1 attention layer,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oz"><img src="../Images/27ae64c14ed6ad42627ca549439893ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcbWQvkhYGoE30Kq3sLvUg.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><h2 id="af39" class="pa ny fq bf nz pb pc pd oc pe pf pg of mr ph pi pj mv pk pl pm mz pn po pp pq bk">Softmax Drawback</h2><p id="e028" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">From above, we can see that we need to compute softmax for an NxN matrix, and thus, our computation cost becomes quadratic in sequence length. This is fine for shorter sequences, but it becomes extremely computationally inefficient for long sequences, N=100k+.</p><p id="1ccb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This gives us our motivation: can we reduce this computational cost? This is where linear attention comes in.</p><h1 id="f901" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Linear Attention</h1><p id="62d5" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Introduced by <a class="af pr" href="https://arxiv.org/pdf/2006.16236" rel="noopener ugc nofollow" target="_blank">Katharopoulos et al.</a>, linear attention uses a clever trick where we write the softmax exponential as a kernel function, expressed as dot products of feature maps φ(x). Using the associative property of matrix multiplication, we can then rewrite the attention computation to be linear. The image below illustrates this transformation:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ps"><img src="../Images/70f502833c91b302822149c0fd21321a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPZAzKChePdbfCtopP9UoQ.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="654e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af pr" href="https://arxiv.org/pdf/2006.16236" rel="noopener ugc nofollow" target="_blank">Katharopoulos et al.</a> used elu(x) + 1 as φ(x), but any kernel feature map that can effectively approximate the exponential similarity can be used. The computational cost of above can be written as,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pt"><img src="../Images/b740cb795e4fa6ec78b71883366e4021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1QD5lhp9yFJTG1OAFGQhw.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="532b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This eliminates the need to compute the full N×N attention matrix and reduces complexity to O(Nd²). Where d is the embedding dimension and this in effect is linear complexity when N &gt;&gt;&gt; d, which is usually the case with Large Language Models</p><p id="afc0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Okay let’s look at the recurrent view of linear attention,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pu"><img src="../Images/6a09502fb6c072b1d70ebad0657866a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJyoSjSYXCR1zKRcX5rYlA.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="cd62" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Okay why can we do this in linear attention and not in softmax? Well softmax is not seperable so we can’t really write it as product of seperate terms. A nice thing to note here is that during decoding, we only need to keep track of S_(n-1), giving us O(d²) complexity per token generation since S is a d × d matrix.</p><p id="cb67" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, this efficiency comes with an important drawback. Since S_(n-1) can only store d² information (being a d × d matrix), we face a fundamental limitation. For instance, if your original context length requires storing 20d² worth of information, you’ll essentially lose 19d² worth of information in the compression. This illustrates the core memory-efficiency tradeoff in linear attention: we gain computational efficiency by maintaining only a fixed-size state matrix, but this same fixed size limits how much context information we can preserve and this gives us the motivation for gating.</p><h1 id="0569" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Gated Linear Attention</h1><p id="102f" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Okay, so we’ve established that we’ll inevitably forget information when optimizing for efficiency with a fixed-size state matrix. This raises an important question: can we be smart about what we remember? This is where gating comes in — researchers use it as a mechanism to selectively retain important information, trying to minimize the impact of memory loss by being strategic about what information to keep in our limited state. Gating isn’t a new concept and has been widely used in architectures like LSTM</p><p id="8001" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The basic change here is in the way we formulate Sn,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf pv"><img src="../Images/1218754c71d1ac1d9a55cc83d6bab4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*a2WAi6Lh8G1a0e68av4Ykg.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by author</figcaption></figure><p id="fbba" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">There are many choices for G all which lead to different models,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pw"><img src="../Images/c2dbaef9d3546d4c51e87f39b5e1565a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3Pn3R_IrMe79QcWpKlaPg.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: <a class="af pr" href="https://arxiv.org/pdf/2312.06635" rel="noopener ugc nofollow" target="_blank">Yang, Songlin, et al. “Gated linear attention transformers with hardware-efficient training.” <em class="px">arXiv preprint arXiv:2312.06635</em>(2023).</a></figcaption></figure><p id="9d94" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">A key advantage of this architecture is that the gating function depends only on the current token x and learnable parameters, rather than on the entire sequence history. Since each token’s gating computation is independent, this allows for efficient parallel processing during training — all gating computations across the sequence can be performed simultaneously.</p><h1 id="5900" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">State Space Models</h1><p id="dfea" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">When we think about processing sequences like text or time series, our minds usually jump to attention mechanisms or RNNs. But what if we took a completely different approach? Instead of treating sequences as, well, sequences, what if we processed them more like how CNNs handle images using convolutions?</p><p id="7761" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">State Space Models (SSMs) formalize this approach through a discrete linear time-invariant system:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf py"><img src="../Images/27fbbd16083eb182a5315c4fd43e4988.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*BZG2vxaeThoxoGJvKsw1Rw.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="fd8b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Okay now let’s see how this relates to convolution,</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf pz"><img src="../Images/5d1b96244bda4a4eec0b5ba784afb2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XvrTKpLDd89b-FpEW7y4kg.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="b1ac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">where F is our learned filter derived from parameters (A, B, c), and * denotes convolution.</p><p id="9272" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af pr" href="https://arxiv.org/pdf/2212.14052" rel="noopener ugc nofollow" target="_blank">H3</a> implements this state space formulation through a novel structured architecture consisting of two complementary SSM layers.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf qa"><img src="../Images/31a7169dee8d15099468da32187db1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*8TciPQwE7-_J5Gl2NrlbCw.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: <a class="af pr" href="https://arxiv.org/pdf/2212.14052" rel="noopener ugc nofollow" target="_blank">Fu, Daniel Y., et al. “Hungry hungry hippos: Towards language modeling with state space models.” <em class="px">arXiv preprint arXiv:2212.14052</em> (2022).</a></figcaption></figure><p id="0cd8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here we take the input and break it into 3 channels to imitate K, Q and V. We then use 2 SSM and 2 gating to kind of imitate linear attention and it turns out that this kind of architecture works pretty well in practice.</p><h1 id="9ec7" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Selective State Space Models</h1><p id="36a2" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">Earlier, we saw how gated linear attention improved upon standard linear attention by making the information retention process data-dependent. A similar limitation exists in State Space Models — the parameters A, B, and c that govern state transitions and outputs are fixed and data-independent. This means every input is processed through the same static system, regardless of its importance or context.</p><p id="0183" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">we can extend SSMs by making them data-dependent through time-varying dynamical systems:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qb"><img src="../Images/ad558a2c2bf96272a731f0f55067b4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*lLJ_aTiYssl_YiImfU715A.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: Image by Author</figcaption></figure><p id="d41e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The key question becomes how to parametrize c_t, b_t, and A_t to be functions of the input. Different parameterizations can lead to architectures that approximate either linear or gated attention mechanisms.</p><p id="dd81" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af pr" href="https://arxiv.org/pdf/2312.00752" rel="noopener ugc nofollow" target="_blank">Mamba</a> implements this time-varying state space formulation through selective SSM blocks.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf qc"><img src="../Images/2b64c4a6ce7a210625f0df6475ec1bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQT5tbqg6sRF6wpvhvJgFw.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Source: <a class="af pr" href="https://arxiv.org/pdf/2312.00752" rel="noopener ugc nofollow" target="_blank">Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling with selective state spaces.” <em class="px">arXiv preprint arXiv:2312.00752</em> (2023).</a></figcaption></figure><p id="5449" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Mamba here uses Selective SSM instead of SSM and uses output gating and additional convolution to improve performance. This is a very high-level idea explaining how Mamba combines these components into an efficient architecture for sequence modeling.</p><h1 id="16f7" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Conclusion</h1><p id="6b51" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">In this article, we explored the evolution of efficient sequence modeling architectures. Starting with traditional softmax attention, we identified its quadratic complexity limitation, which led to the development of linear attention. By rewriting attention using kernel functions, linear attention achieved O(Nd²) complexity but faced memory limitations due to its fixed-size state matrix.</p><p id="b033" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This limitation motivated gated linear attention, which introduced selective information retention through gating mechanisms. We then explored an alternative perspective through State Space Models, showing how they process sequences using convolution-like operations. The progression from basic SSMs to time-varying systems and finally to selective SSMs parallels our journey from linear to gated attention — in both cases, making the models more adaptive to input data proved crucial for performance.</p><p id="76e4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Through these developments, we see a common theme: the fundamental trade-off between computational efficiency and memory capacity. Softmax attention excels at in-context learning by maintaining full attention over the entire sequence, but at the cost of quadratic complexity. Linear variants (including SSMs) achieve efficient computation through fixed-size state representations, but this same optimization limits their ability to maintain detailed memory of past context. This trade-off continues to be a central challenge in sequence modeling, driving the search for architectures that can better balance these competing demands.</p><p id="2f14" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To read more on this topics, i would suggest the following papers:</p><p id="6c57" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Linear Attention</strong>: <a class="af pr" href="https://arxiv.org/pdf/2006.16236" rel="noopener ugc nofollow" target="_blank">Katharopoulos, Angelos, et al. “Transformers are rnns: Fast autoregressive transformers with linear attention.” <em class="qd">International conference on machine learning</em>. PMLR, 2020.</a></p><p id="a089" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">GLA</strong>: <a class="af pr" href="https://arxiv.org/pdf/2312.06635" rel="noopener ugc nofollow" target="_blank">Yang, Songlin, et al. “Gated linear attention transformers with hardware-efficient training.” <em class="qd">arXiv preprint arXiv:2312.06635</em>(2023).</a></p><p id="8666" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">H3</strong>: <a class="af pr" href="https://arxiv.org/pdf/2212.14052" rel="noopener ugc nofollow" target="_blank">Fu, Daniel Y., et al. “Hungry hungry hippos: Towards language modeling with state space models.” <em class="qd">arXiv preprint arXiv:2212.14052</em> (2022).</a></p><p id="0ae4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Mamba</strong>: <a class="af pr" href="https://arxiv.org/pdf/2312.00752" rel="noopener ugc nofollow" target="_blank">Gu, Albert, and Tri Dao. “Mamba: Linear-time sequence modeling with selective state spaces.” <em class="qd">arXiv preprint arXiv:2312.00752</em> (2023).</a></p><p id="366d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af pr" href="https://arxiv.org/pdf/2406.07887" rel="noopener ugc nofollow" target="_blank">Waleffe, Roger, et al. “An Empirical Study of Mamba-based Language Models.” <em class="qd">arXiv preprint arXiv:2406.07887</em> (2024).</a></p><h1 id="a496" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Acknowledgement</h1><p id="773d" class="pw-post-body-paragraph mi mj fq mk b go ot mm mn gr ou mp mq mr ov mt mu mv ow mx my mz ox nb nc nd fj bk">This blog post was inspired by coursework from my graduate studies during Fall 2024 at University of Michigan. While the courses provided the foundational knowledge and motivation to explore these topics, any errors or misinterpretations in this article are entirely my own. This represents my personal understanding and exploration of the material.</p></div></div></div></div>    
</body>
</html>