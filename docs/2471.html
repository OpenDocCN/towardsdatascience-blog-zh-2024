<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Decision Tree Regressor, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Decision Tree Regressor, Explained: A Visual Guide with Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10">https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f67a" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">REGRESSION ALGORITHM</h2><div/><div><h2 id="864a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Trimming branches smartly with cost-complexity pruning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">5</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mo mp mq mr ms mt"><a rel="noopener follow" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------"><div class="mu ab il"><div class="mv ab co cb mw mx"><h2 class="bf ga ib z it my iv iw mz iy ja fz bk">Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="na l"><h3 class="bf b ib z it my iv iw mz iy ja dx">A fresh look on our favorite upside-down tree</h3></div><div class="gq l"><p class="bf b dy z it my iv iw mz iy ja dx">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng lw mt"/></div></div></a></div><p id="63a9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Decision Trees aren’t limited to categorizing data — they’re equally good at predicting numerical values! Classification trees often steal the spotlight, but Decision Tree Regressors (or Regression Trees) are powerful and versatile tools in the world of continuous variable prediction.</p><p id="2643" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">While we’ll discuss the mechanics of regression tree construction (which are mostly similar to the classification tree), here, we’ll also advance beyond the <em class="od">pre</em>-pruning methods like “minimal sample leaf" and "max tree depth” introduced in the classifier article. We’ll explore the most common <em class="od">post</em>-pruning method which is <strong class="nj ga">cost complexity pruning</strong>, that introduces a complexity parameter to the decision tree’s cost function.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/af2717ef7b5f2f1e541ab5ba0909511d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="a262" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Definition</h1><p id="de4d" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">A Decision Tree for regression is a model that predicts numerical values using a tree-like structure. It splits data based on key features, starting from a root question and branching out. Each node asks about a feature, dividing data further until reaching leaf nodes with final predictions. To get a result, you follow the path matching your data’s features from root to leaf.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/4d58bb0bb413cc741a3d2babe2bb0a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OarMEX2uJocedS-Zse9aww.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Decision Trees for regression predict numerical outcomes by following a series of data-driven questions, narrowing down to a final value.</figcaption></figure><h1 id="6c3f" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">📊 Dataset Used</h1><p id="853e" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">To demonstrate our concepts, we’ll work with <a class="af py" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">our standard dataset</a>. This dataset is used to predict the number of golfers visiting on a given day and includes variables like weather outlook, temperature, humidity, and wind conditions.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of pz"><img src="../Images/be5edbc6b8231b1605ca2447d8c33657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QBVtN8fp-QFDK14jJmg-Q.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Columns: ‘Outlook’ (one-hot encoded to sunny, overcast, rain), ‘Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (Yes/No) and ‘Number of Players’ (numerical, target feature)</figcaption></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="ae8c" class="qe oy fq qb b bg qf qg l qh qi">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'],prefix='',prefix_sep='')<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rain', 'Temperature', 'Humidity', 'Wind', 'Num_Players']<br/>df = df[column_order]<br/><br/># Split features and target<br/>X, y = df.drop('Num_Players', axis=1), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)</span></pre><h1 id="48bb" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Main Mechanism</h1><p id="b255" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The Decision Tree for regression operates by recursively dividing the data based on features that best reduce prediction error. Here’s the general process:</p><ol class=""><li id="cba5" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qj qk ql bk">Begin with the entire dataset at the root node.</li><li id="dbd6" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">Choose the feature that minimizes a specific error metric (such as mean squared error or variance) to split the data.</li><li id="75c6" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">Create child nodes based on the split, where each child represents a subset of the data aligned with the corresponding feature values.</li><li id="0ef2" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">Repeat steps 2–3 for each child node, continuing to split the data until a stopping condition is reached.</li><li id="ef0a" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">Assign a final predicted value to each leaf node, typically <strong class="nj ga">the average of the target values</strong> in that node.</li></ol><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/48ae23f237c31ae05ba52c569f790fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H18Gq_atAiL94QBShiIscw.png"/></div></div></figure><h1 id="8f68" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Training Steps</h1><p id="f174" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">We will explore the regression part in the decision tree algorithm CART (Classification and Regression Trees). It builds binary trees and typically follows these steps:</p><p id="75b9" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1.Begin with all training samples in the root node.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/62033724ca4273f8d77b2f9184658c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bK1QNFZZqTA314HO5OZjTA.png"/></div></div></figure><p id="40aa" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.For each feature in the dataset: <br/>a. Sort the feature values in ascending order. <br/>b. Consider all midpoints between adjacent values as potential split points.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of pz"><img src="../Images/b37c7a5dead079a3548bbc02b7d9d800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFDbzpmefi20WEE0ybIysA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">In total, there are 23 split points to check.</figcaption></figure><p id="be09" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">3. For each potential split point:<br/>a. Calculate the mean squared error (MSE) of the current node. <br/>b. Compute the weighted average of errors for the resulting split.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of pz"><img src="../Images/edce98c77731a3a1fb3e871e75d9b0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdDs0kE5qgYUcZawna-EPQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">As an example, here, we calculated the weighted average of MSE for split point “Temperature” with value 73.5</figcaption></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="f132" class="qe oy fq qb b bg qf qg l qh qi">def calculate_split_mse(X_train, y_train, feature_name, split_point):<br/>    # Create DataFrame and sort by feature<br/>    analysis_df = pd.DataFrame({<br/>        'feature': X_train[feature_name],<br/>        'y_actual': y_train<br/>    }).sort_values('feature')<br/>    <br/>    # Split data and calculate means<br/>    left_mask = analysis_df['feature'] &lt;= split_point<br/>    left_mean = analysis_df[left_mask]['y_actual'].mean()<br/>    right_mean = analysis_df[~left_mask]['y_actual'].mean()<br/>    <br/>    # Calculate squared differences<br/>    analysis_df['squared_diff'] = np.where(<br/>        left_mask,<br/>        (analysis_df['y_actual'] - left_mean) ** 2,<br/>        (analysis_df['y_actual'] - right_mean) ** 2<br/>    )<br/>    <br/>    # Calculate MSEs and counts<br/>    left_mse = analysis_df[left_mask]['squared_diff'].mean()<br/>    right_mse = analysis_df[~left_mask]['squared_diff'].mean()<br/>    n_left = sum(left_mask)<br/>    n_right = len(analysis_df) - n_left<br/>    <br/>    # Calculate weighted average MSE<br/>    weighted_mse = (n_left * left_mse + n_right * right_mse) / len(analysis_df)<br/>    <br/>    # Print results<br/>    print(analysis_df)<br/>    print(f"\nResults for split at {split_point} on feature '{feature_name}':")<br/>    print(f"Left child MSE (n={n_left}, mean={left_mean:.2f}): {left_mse:.2f}")<br/>    print(f"Right child MSE (n={n_right}, mean={right_mean:.2f}): {right_mse:.2f}")<br/>    print(f"Weighted average MSE: {weighted_mse:.2f}")<br/><br/># Example usage:<br/>calculate_split_mse(X_train, y_train, 'Temperature', 73.5)</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qr"><img src="../Images/8302971702ad2e5e18eba4e710464c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UHXLgtfn1M7421ksRI4Ong.png"/></div></div></figure><p id="906b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">4. After evaluating all features and split points, select the one with lowest weighted average of MSE.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of pz"><img src="../Images/2bcbe92a762d4f11ca62b9548587d9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fXEPvNwZEAVfOP-unYT2g.png"/></div></div></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="a489" class="qe oy fq qb b bg qf qg l qh qi">def evaluate_all_splits(X_train, y_train):<br/>    """Evaluate all possible split points using midpoints for all features"""<br/>    results = []<br/>    <br/>    for feature in X_train.columns:<br/>        data = pd.DataFrame({'feature': X_train[feature], 'y_actual': y_train})<br/>        splits = [(a + b)/2 for a, b in zip(sorted(data['feature'].unique())[:-1], <br/>                                          sorted(data['feature'].unique())[1:])]<br/>        <br/>        for split in splits:<br/>            left_mask = data['feature'] &lt;= split<br/>            n_left = sum(left_mask)<br/>            <br/>            if not (0 &lt; n_left &lt; len(data)): continue<br/>                <br/>            left_mean = data[left_mask]['y_actual'].mean()<br/>            right_mean = data[~left_mask]['y_actual'].mean()<br/>            <br/>            left_mse = ((data[left_mask]['y_actual'] - left_mean) ** 2).mean()<br/>            right_mse = ((data[~left_mask]['y_actual'] - right_mean) ** 2).mean()<br/>            <br/>            weighted_mse = (n_left * left_mse + (len(data) - n_left) * right_mse) / len(data)<br/>            <br/>            results.append({'Feature': feature, 'Split_Point': split, 'Weighted_MSE': weighted_mse})<br/>    <br/>    return pd.DataFrame(results).round(2)<br/><br/># Example usage:<br/>results = evaluate_all_splits(X_train, y_train)<br/>print(results)</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qs"><img src="../Images/1ce048d6bbf06583edef5c90b22e9d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BvCu2ZB4KAJt4CoxjWPhiA.png"/></div></div></figure><p id="9842" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">5. Create two child nodes based on the chosen feature and split point:<br/>- Left child: samples with feature value &lt;= split point<br/>- Right child: samples with feature value &gt; split point</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/87c47d48c4a8dd061010b27bc42362f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pI93yw3pmQ3CRq_nV4MvPA.png"/></div></div></figure><p id="fc5b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">6. Recursively repeat steps 2–5 for each child node. (Continue until a stopping criterion is met.)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/60ff1e00eadc6faef39b8fb6a57fe968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9NL8PWyAcb5og6RxXlA7Q.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/3ad498b5148c0d1e27ed7cf1a7f885e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qES-kKQXccMdiV99uJX3Q.png"/></div></div></figure><p id="e507" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">7. At each leaf node, assign the average target value of the samples in that node as the prediction.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/aab4fe4bdcce592f0c4e10224a18ad10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Ltv_O6FSScI7b1UhwoPvA.png"/></div></div></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="ca15" class="qe oy fq qb b bg qf qg l qh qi">from sklearn.tree import DecisionTreeRegressor, plot_tree<br/>import matplotlib.pyplot as plt<br/><br/># Train the model<br/>regr = DecisionTreeRegressor(random_state=42)<br/>regr.fit(X_train, y_train)<br/><br/># Visualize the decision tree<br/>plt.figure(figsize=(26,8))<br/>plot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=False, fontsize=16, precision=2)<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qt"><img src="../Images/4c99833d95cd4cda3734cf643d7c2b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mmMar4_w5l6eM9kQDwAqfQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">In this scikit-learn output, the samples and values are shown for the leaf nodes and interim nodes.</figcaption></figure><h1 id="e0af" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Regression/Prediction Step</h1><p id="e070" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Here’s how a regression tree makes predictions for new data:<br/>1. Start at the top (root) of the tree.<br/>2. At each decision point (node):<br/>- Look at the feature and split value.<br/>- If the data point’s feature value is smaller or equal, go left.<br/>- If it’s larger, go right.<br/>3. Keep moving down the tree until you reach the end (a leaf).<br/>4. The prediction is the average value stored in that leaf.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/ce9e66e2984c1dd8926cee0a62199806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETNMBayC0disCnudmnG3YA.png"/></div></div></figure><h2 id="1597" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Evaluation Step</h2><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/b1973f3c301a44f070306fc8def936ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NB4GqMXgN6Fp4EIXdP2EJg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">This value of RMSE is so much better than <a class="af py" href="https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629" rel="noopener">the result of the dummy regressor</a>.</figcaption></figure><h1 id="616d" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Pre-pruning vs Post-pruning</h1><p id="6772" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">After building the tree, the only thing we need to worry about is the method to make the tree smaller to prevent overfitting. In general, the method of pruning can be categorized as:</p><h2 id="b134" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Pre-pruning</h2><p id="7dfc" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Pre-pruning, also known as early stopping, involves halting the growth of a decision tree during the training process based on certain predefined criteria. This approach aims to prevent the tree from becoming too complex and overfitting the training data. Common pre-pruning techniques include:</p><ol class=""><li id="6209" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qj qk ql bk"><strong class="nj ga">Maximum depth</strong>: Limiting how deep the tree can grow.</li><li id="d3ce" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Minimum samples for split</strong>: Requiring a minimum number of samples to justify splitting a node.</li><li id="b192" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Minimum samples per leaf</strong>: Ensuring each leaf node has at least a certain number of samples.</li><li id="216c" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Maximum number of leaf nodes</strong>: Restricting the total number of leaf nodes in the tree.</li><li id="f6d4" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Minimum impurity decrease</strong>: Only allowing splits that decrease impurity by a specified amount.</li></ol><p id="565e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">These methods stop the tree’s growth when the specified conditions are met, effectively “pruning” the tree during its construction phase. <br/>(<a class="af py" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">We have discussed these methods before, which is exactly the same in regression case.</a>)</p><h2 id="0ecc" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Post-pruning</h2><p id="6fdd" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Post-pruning, on the other hand, allows the decision tree to grow to its full extent and then prunes it back to reduce complexity. This approach first builds a complete tree and then removes or collapses branches that don’t significantly contribute to the model’s performance. One common post-pruning technique is called <strong class="nj ga">Cost-Complexity Pruning.</strong></p><h1 id="e88e" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Cost Complexity Pruning</h1><h2 id="76bd" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Step 1: Calculate the Impurity for Each Node</h2><p id="d999" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For each interim node, calculate the impurity (MSE for regression case). We then sorted this value from the lowest to highest.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/57e29c193ac2b684411d3da6c0176e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pPPPcuh7EVQfg5cZUJIvQ.png"/></div></div></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="a709" class="qe oy fq qb b bg qf qg l qh qi"># Visualize the decision tree<br/>plt.figure(figsize=(26,8))<br/>plot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=True, fontsize=16, precision=2)<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rk"><img src="../Images/d4244591a1bc3d372a2b2429fc6e24b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ok3MRFcOGhmfQJQv8D3hfA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">In this scikit learn output, the impurity are shown as “squared_error” for each nodes.</figcaption></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/fbbc7f5114b6b7959849be45f651aec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jg71fTvZxt6KSLSBTjjPXQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Let‘s give name to these interim nodes (from A-J). We then sort it based on their MSE, from lowest to highest</figcaption></figure><h2 id="0fa5" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Step 2: Create Subtrees by Trimming The Weakest Link</h2><p id="9f0f" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The goal is to gradually turn the interim nodes into leaves starting from the <strong class="nj ga">node with the lowest MSE</strong> (= weakest link). We can create a path of pruning based on that.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/bd666447562c4b89a0452d15a3a697b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QhlZUW6JgsNqTVko5zJuuQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Let’s name them “Subtree <em class="rl">i</em>” based on how many times (<em class="rl">i</em>) it is being pruned. Starting from the original tree, the tree will be pruned on the node with lowest MSE (starting from node J, M (already got cut by J), L, K, and so on)</figcaption></figure><h2 id="94f2" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Step 3: Calculate Total Leaf Impurities for Each Subtree</h2><p id="bcc5" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For each subtree <em class="od">T</em>, total leaf impurities (<em class="od">R</em>(<em class="od">T</em>)) can be calculated as:</p><p id="1507" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><em class="od">R</em>(<em class="od">T</em>) = (1/<em class="od">N</em>) Σ <em class="od">I</em>(<em class="od">L</em>) * <em class="od">n</em>_<em class="od">L</em></p><p id="aa89" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">where:<br/><strong class="nj ga">·</strong> <em class="od">L</em> ranges over all leaf nodes<br/><strong class="nj ga">·</strong> <em class="od">n_L</em> is the number of samples in leaf <em class="od">L<br/></em><strong class="nj ga">·</strong><em class="od"> N</em> is the total number of samples in the tree<br/><strong class="nj ga">· </strong><em class="od">I</em>(<em class="od">L</em>) is the impurity (MSE)<em class="od"> </em>of leaf <em class="od">L</em></p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/db7b97078d3ca41116ba6586413f17a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jm49mu_cKJiatks2uHDz0g.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">The more we prune, the higher the total leaf impurities.</figcaption></figure><h2 id="874b" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Step 4: Compute the Cost Function</h2><p id="85b6" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">To control when to stop turning the interim nodes into leaves, we check the cost complexity first for each subtree <em class="od">T </em>using the following formula:</p><p id="160b" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Cost(<em class="od">T</em>) = <em class="od">R</em>(<em class="od">T</em>) + <em class="od">α</em> * |<em class="od">T</em>|</p><p id="9a73" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">where:<br/><strong class="nj ga">·</strong> <em class="od">R</em>(<em class="od">T</em>) is the total leaf impurities<br/><strong class="nj ga">·</strong> |<em class="od">T</em>| is the number of leaf nodes in the subtree<em class="od"><br/></em><strong class="nj ga">·</strong><em class="od"> α</em> is the complexity parameter</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/d1e444f5b41346a1628e95d70944d312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N4FZT5zb7fqCQrm-ygI4Ww.png"/></div></div></figure><h2 id="37c4" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Step 5: Select the Alpha</h2><p id="9a47" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">The value of alpha control which subtree we will end up with. The <strong class="nj ga">subtree with the lowest cost will be the final tree</strong>.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/7f89a62ec5564f5169596ae9b6d2d1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPUaC5N_r_7mX5jLDcAPZQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">When <em class="rl">α</em> is small, we care more about accuracy (bigger trees). When <em class="rl">α</em> is large, we care more about simplicity (smaller trees)</figcaption></figure><p id="e677" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">While we can freely set the <em class="od">α</em>, in scikit-learn, you can also get the smallest value of <em class="od">α</em> to obtain a particular subtree. This is called <strong class="nj ga">effective <em class="od">α</em></strong><em class="od">.</em></p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/d70f61e11dcfad94df3cbd4294949d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z2cIXd6un7AAC9lUr4i3bg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">This effective <strong class="bf oz"><em class="rl">α </em></strong><em class="rl">can also be computed.</em></figcaption></figure><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="b14d" class="qe oy fq qb b bg qf qg l qh qi"># Compute the cost-complexity pruning path<br/>tree = DecisionTreeRegressor(random_state=42)<br/>effective_alphas = tree.cost_complexity_pruning_path(X_train, y_train).ccp_alphas<br/>impurities = tree.cost_complexity_pruning_path(X_train, y_train).impurities<br/><br/># Function to count leaf nodes<br/>count_leaves = lambda tree: sum(tree.tree_.children_left[i] == tree.tree_.children_right[i] == -1 for i in range(tree.tree_.node_count))<br/><br/># Train trees and count leaves for each complexity parameter<br/>leaf_counts = [count_leaves(DecisionTreeRegressor(random_state=0, ccp_alpha=alpha).fit(X_train_scaled, y_train)) for alpha in effective_alphas]<br/><br/># Create DataFrame with analysis results<br/>pruning_analysis = pd.DataFrame({<br/>    'total_leaf_impurities': impurities,<br/>    'leaf_count': leaf_counts,<br/>    'cost_function': [f"{imp:.3f} + {leaves}α" for imp, leaves in zip(impurities, leaf_counts)],<br/>    'effective_α': effective_alphas<br/>})<br/><br/>print(pruning_analysis)</span></pre><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rm"><img src="../Images/970ac8efb0ce4a38d8e6f98f7d4a57ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNqZSvw-NxT1GQnTf0rZWw.png"/></div></div></figure><h2 id="ca64" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Final Remarks</h2><p id="ce82" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Pre-pruning methods are generally faster and more memory-efficient, as they prevent the tree from growing too large in the first place.</p><p id="061a" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Post-pruning can potentially create more optimal trees, as it considers the entire tree structure before making pruning decisions. However, it can be more computationally expensive.</p><p id="ba34" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Both approaches aim to find a balance between model complexity and performance, with the goal of creating a model that generalizes well to unseen data. The choice between pre-pruning and post-pruning (or a combination of both) often depends on the specific dataset, the problem at hand, and of course, computational resources available.</p><p id="2516" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">In practice, it’s common to use a combination of these methods, like applying some pre-pruning criteria to prevent excessively large trees, and then using post-pruning for fine-tuning the model’s complexity.</p><h1 id="5d45" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">🌟 Decision Tree Regressor (with Cost Complexity Pruning) Code Summarized</h1><pre class="oh oi oj ok ol qa qb qc bp qd bb bk"><span id="e6d9" class="qe oy fq qb b bg qf qg l qh qi">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.preprocessing import StandardScaler<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52,39,43,37,28,19,43,47,56,33,49,23,42,13,33,29,25,51,41,14,34,29,49,36,57,21,23,41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data into features and target, then into training and test sets<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Initialize Decision Tree Regressor<br/>tree = DecisionTreeRegressor(random_state=42)<br/><br/># Get the cost complexity path, impurities, and effective alpha<br/>path = tree.cost_complexity_pruning_path(X_train, y_train)<br/>ccp_alphas, impurities = path.ccp_alphas, path.impurities<br/>print(ccp_alphas)<br/>print(impurities)<br/><br/># Train the final tree with the chosen alpha<br/>final_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=0.1)<br/>final_tree.fit(X_train_scaled, y_train)<br/><br/># Make predictions<br/>y_pred = final_tree.predict(X_test)<br/><br/># Calculate and print RMSE<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"RMSE: {rmse:.4f}")</span></pre></div></div></div><div class="ab cb rn ro rp rq" role="separator"><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e338" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Further Reading</h2><p id="c797" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">For a detailed explanation of the <a class="af py" href="https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html" rel="noopener ugc nofollow" target="_blank">Decision Tree Regressor</a>, <a class="af py" href="https://scikit-learn.org/1.5/auto_examples/tree/plot_cost_complexity_pruning.html" rel="noopener ugc nofollow" target="_blank">Cost Complexity Pruning</a>, and its implementation in scikit-learn, readers can refer to their official documentation. It provides comprehensive information on their usage and parameters.</p><h2 id="b52b" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">Technical Environment</h2><p id="05e3" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="c89d" class="qu oy fq bf oz qv qw qx pc qy qz ra pf nq rb rc rd nu re rf rg ny rh ri rj fw bk">About the Illustrations</h2><p id="953e" class="pw-post-body-paragraph nh ni fq nj b gt pt nl nm gw pu no np nq pv ns nt nu pw nw nx ny px oa ob oc fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="7c05" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rv rw bp rx lw ao"><div class="ry l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rz sa cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rz sa em n ay uk"/></div><div class="sb l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq se hp l"><h2 class="bf ga xh ic it xi iv iw mz iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xj wj wk wl wm lj wn wo uv ii wp wq wr uz va vb ep bm vc ot" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----fbd2836c3bef--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xk l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sn dz so it ab sp il ed"><div class="ed sh bx si sj"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sh bx kk sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sm sl"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it rv rw bp rx lw ao"><div class="ry l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rz sa cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rz sa em n ay uk"/></div><div class="sb l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq se hp l"><h2 class="bf ga xh ic it xi iv iw mz iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xj wj wk wl wm lj wn wo uv ii wp wq wr uz va vb ep bm vc ot" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fbd2836c3bef--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xk l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sn dz so it ab sp il ed"><div class="ed sh bx si sj"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sh bx kk sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sm sl"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>