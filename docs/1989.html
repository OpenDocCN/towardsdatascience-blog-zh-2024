<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Fresh Look at Nonlinearity in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Fresh Look at Nonlinearity in Deep Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-fresh-look-at-nonlinearity-in-deep-learning-a79b6955d2ad?source=collection_archive---------1-----------------------#2024-08-15">https://towardsdatascience.com/a-fresh-look-at-nonlinearity-in-deep-learning-a79b6955d2ad?source=collection_archive---------1-----------------------#2024-08-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f332" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The traditional reasoning behind why we need nonlinear activation functions is only one dimension of this story.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@crackalamoo?source=post_page---byline--a79b6955d2ad--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Harys Dalvi" class="l ep by dd de cx" src="../Images/cf7fa3865063408efd1fd4c0b4b603db.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*APonMNiRc_xq6MFzvUH5Ag.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a79b6955d2ad--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@crackalamoo?source=post_page---byline--a79b6955d2ad--------------------------------" rel="noopener follow">Harys Dalvi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a79b6955d2ad--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d8c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What do the softmax, ReLU, sigmoid, and tanh functions have in common? They’re all <strong class="ml fr">activation functions</strong> — and they’re all <strong class="ml fr">nonlinear</strong>. But why do we need activation functions in the first place, specifically nonlinear activation functions? There’s a traditional reasoning, and also a new way to look at it.</p><p id="8fb5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The traditional reasoning is this: without a nonlinear activation function, a deep neural network is just a composition of matrix multiplications and adding biases. These are <strong class="ml fr">linear transformations</strong>, and you can prove using linear algebra that <strong class="ml fr">the composition of linear transformations is just another linear transformation.</strong></p><p id="1bd4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So no matter how many linear layers we stack together, without activation functions, our entire model is no better than a linear regression. It will completely fail to capture nonlinear relationships, even simple ones like XOR.</p><p id="a0d5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Enter activation functions: by allowing the model to <strong class="ml fr">learn a nonlinear function</strong>, we gain the ability to model all kinds of complicated real-world relationships.</p><p id="5b81" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This story, which you may already be familiar with, is entirely correct. But the study of any topic benefits from a variety of viewpoints, especially deep learning with all its interpretability challenges. Today I want to share with you another way to look at the need for activation functions, and what it reveals about the inner workings of deep learning models.</p><p id="4002" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In short, what I want to share with you is this: the way we normally construct deep learning classifiers creates an <strong class="ml fr">inductive bias</strong> in the model. Specifically, <strong class="ml fr">using a linear layer for the output</strong> means that the rest of the model must find a <strong class="ml fr">linearly separable</strong> transformation of the input. The intuition behind this can be really useful, so I’ll share some examples that I hope will clarify some of this jargon.</p><h1 id="2715" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">The Traditional Explanation</h1><p id="60ac" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Let’s revisit the traditional rationale for nonlinear activation functions with an example. We’ll look at a simple case: <strong class="ml fr">XOR</strong>.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/f0424716ac2c88fc1fc19f5e79acdea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q3v0lMfjStngJBFpR2whuA.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">A plot of the XOR function with colored ground truth values. Background color represents linear regression predictions. Image by author.</figcaption></figure><p id="bb66" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here I’ve trained a linear regression model on the XOR function with two binary inputs (ground truth values are plotted as dots). I’ve plotted the outputs of the regression as the background color. The regression didn’t learn anything at all: it guessed 0.5 in all cases.</p><p id="358f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, instead of a linear model, I’m going to train a very basic deep learning model with MSE loss. Just <strong class="ml fr">one linear layer with two neurons</strong>, followed by the <strong class="ml fr">ReLU</strong> activation function, and then finally the output neuron. To keep things simple, I’ll use only weights, no biases.</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh oz"><img src="../Images/0bb7b0034adfc776cefb578791227eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*ot-yyQsUJI5hTownPKXZAw.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">A diagram of our basic neural network. Made with <a class="af pa" href="https://draw.io" rel="noopener ugc nofollow" target="_blank">draw.io</a> by author.</figcaption></figure><p id="fc28" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What happens now?</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pb"><img src="../Images/310f6466f2f98cb2e64bccc17299aaaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RujzAznAq0NURSlX34n1wQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Another plot of the XOR function, this time with predictions from a simple deep learning model. Image by author.</figcaption></figure><p id="cdec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Wow, now it’s perfect! What do the weights look like?</p><pre class="oj ok ol om on pc pd pe bp pf bb bk"><span id="3bc7" class="pg ng fq pd b bg ph pi l pj pk">Layer 1 weight: [[ 1.1485, -1.1486],<br/>                [-1.0205,  1.0189]]<br/><br/>(ReLU)<br/><br/>Layer 2 weight: [[0.8707, 0.9815]]</span></pre><p id="8387" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So for two inputs <em class="pl">x</em> and <em class="pl">y</em>, our output is:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pm"><img src="../Images/4cab0448d796f359f93fa820b785f7ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ULrUc6KmEhp1BxSNu1V3YA.png"/></div></div></figure><p id="118d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is really similar to</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pn"><img src="../Images/333873bfa4a427577ae863213d520414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*PqcIYUK5HJmRbPEauCB7Hg.png"/></div></figure><p id="e855" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">which you can verify is exactly the XOR function for inputs <em class="pl">x</em>, <em class="pl">y</em> in {0, 1}.</p><p id="08b8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we didn’t have the ReLU in there, we could simplify our model to 0.001<em class="pl">y </em>- 0.13<em class="pl">x</em>, a linear function that wouldn’t work at all. So there you have it, the traditional explanation: since XOR is an inherently nonlinear function, it can’t be precisely modeled by any linear function. Even a composition of linear functions won’t work, because that’s just another linear function. <strong class="ml fr">Introducing the nonlinear ReLU function</strong> allows us to capture nonlinear relationships.</p><h1 id="bdba" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Digging Deeper: Inductive Bias</h1><p id="37f7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Now we’re going to work on the same XOR model, but we’ll look at it through a different lens and get a better sense of the <strong class="ml fr">inductive bias</strong> of this model.</p><p id="ccb6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What is an inductive bias? Given any problem, there are many ways to solve it. Essentially, an inductive bias is something built into the architecture of a model that leads it to choose a particular method of solving a problem over any other method.</p><p id="b502" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this deep learning model, our final layer is a simple linear layer. This means our model can’t work at all unless the model’s output immediately before the final layer can be solved by linear regression. In other words, <strong class="ml fr">the final hidden state before the output must be linearly separable for the model to work. </strong>This inductive bias is a property of our model architecture, not the XOR function.</p><p id="f26a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Luckily, in this model, our hidden state has only two neurons. Therefore, we can visualize it in two dimensions. What does it look like?</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh po"><img src="../Images/76f551b445afda3a4c213c30b7058d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etzaZzJ6gObPTwCZuYudhw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The input representation for the XOR function transformed into a hidden representation with deep learning (after one linear layer and ReLU). Background color represents the predictions of a linear regression model. Image by author.</figcaption></figure><p id="efb4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As we saw before, a linear regression model alone is not effective for the XOR input. But once we pass the input through the first layer and ReLU of our neural network, our output classes can be neatly <em class="pl">separated by a line</em> (<strong class="ml fr">linearly separable</strong>). This means linear regression will now work, and in fact our final layer effectively just performs this linear regression.</p><p id="0c91" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, what does this tell us about inductive bias? Since our last layer is a linear layer, the representation before this layer <strong class="ml fr">must</strong> be at least approximately linearly separable. Otherwise the last layer, which functions as a linear regression, will fail.</p><h1 id="70c5" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Linear Classifier Probes</h1><p id="685e" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">For the XOR model, this might look like a trivial extension of the traditional view we saw before. But how does this work for more complex models? As models get deeper, we can get more insight by looking at nonlinearity in this way. <a class="af pa" href="https://arxiv.org/pdf/1610.01644" rel="noopener ugc nofollow" target="_blank">This paper</a> by Guillaume Alain and Yoshua Bengio investigates this idea using <strong class="ml fr">linear classifier probes</strong>.[1]</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pp"><img src="../Images/5d4de7a0a208ab7263c4051305bd8758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6z0c7VQlrbZVN72zp9QhuA.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">“The hex dump represented at the left has more information contents than the image at the right. Only one of them can be processed by the human brain in time to save their lives. Computational convenience matters. Not just entropy.” Figure and caption from Alain &amp; Bengio, 2018 (<a class="af pa" href="https://arxiv.org/pdf/1610.01644" rel="noopener ugc nofollow" target="_blank">Link</a>). [1]</figcaption></figure><p id="2fce" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For many cases like MNIST handwritten digits, all the information needed to make a prediction already exists in the input: it’s just a matter of processing it. Alain and Bengio observe that as we get deeper into a model, we actually have <em class="pl">less</em> information at each layer, not more. But the upside is that at each layer, the information we do have becomes “easier to use”. What we mean by this is that the information becomes increasingly linearly separable after each hidden layer.</p><p id="26d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How do we find out how linearly separable the model’s representation is after each layer? Alain and Bengio suggest using what they call <strong class="ml fr">linear classifier probes</strong>. The idea is that after each layer, we train a <strong class="ml fr">linear regression to predict the final output, using the hidden states at that layer as input.</strong></p><p id="771a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is essentially what we did for the last XOR plot: we trained a linear regression on the hidden states right before the last layer, and we found that this regression successfully predicted the final output (1 or 0). We were unable to do this with the raw input, when the data was not linearly separable. Remember that the final layer is basically linear regression, so in a sense this method is like creating a new final layer that is shifted earlier in the model.</p><p id="e262" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Alain and Bengio applied this to a convolutional neural network trained on MNIST handwritten digits: before and after each convolution, ReLU, and pooling, they added a linear probe. What they found is that the test error almost always decreased from one probe to the next, indicating an increase in linear separability.</p><p id="2fa7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Why does the data become linearly separable, and not “polynomially separable” or something else? Since the last layer is linear, the loss function we use will pressure all the other layers in the model to work together and create a linearly separable representation for the final layer to predict from.</p><p id="76a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Does this idea apply to large language models (LLMs) as well? In fact, it does. <a class="af pa" href="https://arxiv.org/pdf/2404.07066" rel="noopener ugc nofollow" target="_blank">Jin et al. (2024)</a> used linear classifier probes to demonstrate how LLMs learn various concepts. They found that <strong class="ml fr">simple concepts</strong>, such as whether a given city is the capital of a given country, <strong class="ml fr">become linearly separable early in the model</strong>: just a few nonlinear activations are required to model these relationships. In contrast, many <strong class="ml fr">reasoning skills do not become linearly separable until later in the model</strong>, or not at all for smaller models.[2]</p><h1 id="3b7d" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h1><p id="99ba" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">When we use <strong class="ml fr">activation functions</strong>, we introduce <strong class="ml fr">nonlinearity</strong> into our deep learning models. This is certainly good to know, but we can get even more value by interpreting the consequences of linearity and nonlinearity in multiple ways.</p><p id="275d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While the above interpretation looks at the model as a whole, one useful mental model centers on the <strong class="ml fr">final linear layer</strong> of a deep learning model. Since this is a linear layer, whatever comes before it has to be linearly separable; otherwise, the model won’t work. Therefore, when training, the rest of the layers of the model will work together to <strong class="ml fr">find a linear representation that the final layer can use</strong> for its prediction.</p><p id="cd93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It’s always good to have more than one intuition for the same thing. This is especially true in deep learning where models can be so black-box that any trick to gain better interpretability is helpful. Many papers have applied this intuition to get fascinating results: Alain and Bengio (2018) used it to develop the concept of <strong class="ml fr">linear classifier probing</strong>, while Jin et al. (2024) built on this to watch increasingly complicated concepts develop in a language model layer-by-layer.</p><p id="8f75" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I hope this new mental model for the purpose of nonlinearities was helpful to you, and that you’ll now be able to shed some more light on black-box deep neural networks!</p></div></div><div class="oo"><div class="ab cb"><div class="lm pq ln pr lo ps cf pt cg pu ci bh"><figure class="oj ok ol om on oo pw px paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh pv"><img src="../Images/cfef086479d2ed4ae94c1923d8bc86c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*CiF500IDsqHSrb7W"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Photo by <a class="af pa" href="https://unsplash.com/@nashadabdu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nashad Abdu</a> on <a class="af pa" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d458" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">References</h1><p id="5222" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">[1] G. Alain and Y. Bengio, <a class="af pa" href="https://arxiv.org/abs/1610.01644" rel="noopener ugc nofollow" target="_blank">Understanding intermediate layers using linear classifier probes</a> (2018), arXiv</p><p id="e230" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] M. Jin et al., <a class="af pa" href="https://arxiv.org/abs/2404.07066" rel="noopener ugc nofollow" target="_blank">Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?</a> (2024), arXiv</p></div></div></div></div>    
</body>
</html>