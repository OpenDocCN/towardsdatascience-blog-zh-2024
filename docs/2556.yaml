- en: How Much Stress Can Your Server Handle When Self-Hosting LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/load-testing-self-hosted-llms-29ca8a4cf43a?source=collection_archive---------4-----------------------#2024-10-19](https://towardsdatascience.com/load-testing-self-hosted-llms-29ca8a4cf43a?source=collection_archive---------4-----------------------#2024-10-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Do you need more GPUs or a modern GPU? How do you make infrastructure decisions?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29ca8a4cf43a--------------------------------)
    ·6 min read·Oct 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/840591a9641d0029ef5c5cca767dffe3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author using Dalle-E-2024
  prefs: []
  type: TYPE_NORMAL
- en: How does it feel when a group of users suddenly start using an app that only
    you and your dev team have used before?
  prefs: []
  type: TYPE_NORMAL
- en: That’s the million-dollar question of moving from prototype to production.
  prefs: []
  type: TYPE_NORMAL
- en: As far as LLMs are concerned, you can do a few dozen tweaks to run your app
    within the budget and acceptable qualities. For instance, you can choose a quantized
    model for lower memory usage. Or you can fine-tune a tiny model and beat the performance
    of giant LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/i-fine-tuned-the-tiny-llama-3-2-1b-to-replace-gpt-4o-7ce1e5619f3d?source=post_page-----29ca8a4cf43a--------------------------------)
    [## I Fine-Tuned the Tiny Llama 3.2 1B to Replace GPT-4o'
  prefs: []
  type: TYPE_NORMAL
- en: Is the fine-tuning effort worth more than few-shot prompting?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/i-fine-tuned-the-tiny-llama-3-2-1b-to-replace-gpt-4o-7ce1e5619f3d?source=post_page-----29ca8a4cf43a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can even tweak your infrastructure to achieve better outcomes. For example,
    you may want to double the number of GPUs you use or choose the latest-generation
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: But how could you say Option A performs better than Option B and C?
  prefs: []
  type: TYPE_NORMAL
- en: This is an important question to ask ourselves at the earliest stages of going
    into production. All these options have their costs —…
  prefs: []
  type: TYPE_NORMAL
