<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Math Behind Kernel Density Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Math Behind Kernel Density Estimation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-math-behind-kernel-density-estimation-5deca75cba38?source=collection_archive---------1-----------------------#2024-09-17">https://towardsdatascience.com/the-math-behind-kernel-density-estimation-5deca75cba38?source=collection_archive---------1-----------------------#2024-09-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fdaa" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring the foundations, concepts, and math of kernel density estimation</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@z.nay854?source=post_page---byline--5deca75cba38--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Zackary Nay" class="l ep by dd de cx" src="../Images/b2eca451f39b1227ba868befe969f4ff.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*m5xrH1MQFrQ9Q4xTY86C9A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5deca75cba38--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@z.nay854?source=post_page---byline--5deca75cba38--------------------------------" rel="noopener follow">Zackary Nay</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5deca75cba38--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/7b02965740dfdbe0f77e416a5b6eb7e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*F_wPsQ6BJsuFG7v_"/></div></div></figure><p id="ebf7" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The Kernel Density Estimator is a fundamental non-parametric method that is a versatile tool for uncovering the hidden distributions of your data. This article delves into the mathematical foundations of the estimator, provides guidance on choosing the optimal bandwidth, and briefly touches on the choice of kernel functions and other related topics.</p><h1 id="7ffc" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk"><strong class="al">Part 1: Introduction</strong></h1><p id="0a64" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">Suppose I give you the following sample of data:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ou"><img src="../Images/9d7b5704844ecc01c61ef6440a044011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*oKocZuIE-iHlffxY9FnqNQ.png"/></div></figure><p id="d0fb" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">One of the first and easiest steps in analyzing sample data is to generate a histogram, for our data we get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ov"><img src="../Images/52fdb96422c868e48af5ae2cfcb9a638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*XeK5Xw1rbgyIJ4jEq988gg.png"/></div></figure><p id="9723" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Not very useful, and we are no closer to understanding what the underlying distribution is. There is also the additional problem that, in practice, data rarely exhibit the sharp rectangular structure produced by a histogram. The kernel density estimator provides a remedy, and it is a non-parametric way to estimate the probability density function of a random variable.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ow"><img src="../Images/815e4c24ba9ea67563e2a705b1e06d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*zI9LLpeEISPYYTfT1luurQ.png"/></div></figure><pre class="mm mn mo mp mq ox oy oz bp pa bb bk"><span id="bacf" class="pb nu fq oy b bg pc pd l pe pf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.neighbors import KernelDensity<br/><br/># Generate sample data<br/>np.random.seed(14)<br/>uniform_data = np.random.uniform(low=1, high=7, size=20)<br/>normal_data = np.random.normal(loc=3, scale=0.7, size=20)<br/><br/># Combine both distributions<br/>combined_data = np.concatenate([uniform_data, normal_data])<br/><br/># Compute histogram<br/>hist, bin_edges = np.histogram(combined_data, bins=9, density=True)<br/><br/># Compute KDE<br/>kde = KernelDensity(kernel='gaussian').fit(combined_data[:, None])<br/>x = np.linspace(min(combined_data), max(combined_data), 1000)[:, None]<br/>log_density = kde.score_samples(x)<br/>density = np.exp(log_density)<br/><br/># Plot histogram<br/>plt.hist(combined_data, bins=9, density=True, color='blue', edgecolor='black', label='Histogram')<br/><br/># Plot KDE<br/>plt.plot(x, density, color='red', label='KDE')<br/><br/># Add labels and legend<br/>plt.ylabel('Density')<br/>plt.title('Histogram and KDE')<br/>plt.legend()<br/><br/># Show plot<br/>plt.show()</span></pre></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9d80" class="nt nu fq bf nv nw po gq ny nz pp gt ob oc pq oe of og pr oi oj ok ps om on oo bk"><strong class="al">Part 2: Derivation</strong></h1><p id="e633" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">The following derivation takes inspiration from Bruce E. Hansen’s “Lecture Notes on Nonparametric” (2009). If you are interested in learning more you can refer to his original lecture notes <a class="af pt" href="https://users.ssc.wisc.edu/~bhansen/718/NonParametrics1.pdf" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="f491" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Suppose we wanted to estimate a probability density function, f(t), from a sample of data. A good starting place would be to estimate the cumulative distribution function, F(t), using the <a class="af pt" href="https://en.wikipedia.org/wiki/Empirical_distribution_function#:~:text=The%20empirical%20distribution%20function%20is,to%20the%20Glivenko%E2%80%93Cantelli%20theorem." rel="noopener ugc nofollow" target="_blank">empirical distribution function</a> (EDF). Let X1, …, Xn be independent, identically distributed real random variables with the common cumulative distribution function F(t). The EDF is defined as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pu"><img src="../Images/124f9bd1a658b206a5279dfd27173ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*yhjpN75rpiOgwWYI"/></div></figure><p id="a9f2" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then, by the strong law of large numbers, as n approaches infinity, the EDF converges almost surely to F(t). Now, the EDF is a step function that could look like the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/b8c9f6aac2ad35cb70daede61f52be29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*9hGEf6J4lmxY5eO-XjjwZA.png"/></div></figure><pre class="mm mn mo mp mq ox oy oz bp pa bb bk"><span id="b024" class="pb nu fq oy b bg pc pd l pe pf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from scipy.stats import norm<br/><br/># Generate sample data<br/>np.random.seed(14)<br/>data = np.random.normal(loc=0, scale=1, size=40)<br/><br/># Sort the data<br/>data_sorted = np.sort(data)<br/><br/># Compute ECDF values<br/>ecdf_y = np.arange(1, len(data_sorted)+1) / len(data_sorted)<br/><br/># Generate x values for the normal CDF<br/>x = np.linspace(-4, 4, 1000)<br/>cdf_y = norm.cdf(x)<br/><br/># Create the plot<br/>plt.figure(figsize=(6, 4))<br/>plt.step(data_sorted, ecdf_y, where='post', color='blue', label='ECDF')<br/>plt.plot(x, cdf_y, color='gray', label='Normal CDF')<br/>plt.plot(data_sorted, np.zeros_like(data_sorted), '|', color='black', label='Data points')<br/><br/># Label axes<br/>plt.xlabel('X')<br/>plt.ylabel('Cumulative Probability')<br/><br/># Add grid<br/>plt.grid(True)<br/><br/># Set limits<br/>plt.xlim([-4, 4])<br/>plt.ylim([0, 1])<br/><br/># Add legend<br/>plt.legend()<br/><br/># Show plot<br/>plt.show()</span></pre><p id="2511" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Therefore, if we were to try and find an estimator for f(t) by taking the derivative of the EDF, we would get a scaled sum of <a class="af pt" href="https://en.wikipedia.org/wiki/Dirac_delta_function" rel="noopener ugc nofollow" target="_blank">Dirac delta functions</a>, which is not very helpful. Instead let us consider using the t<a class="af pt" href="https://home.cc.umanitoba.ca/~farhadi/Math2120/Numerical%20Differentiation.pdf" rel="noopener ugc nofollow" target="_blank">wo-point central difference formula</a> of the estimator as an approximation of the derivative. Which, for a small h&gt;0, we get:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/59ae71a5b9101204a61466fda298a3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TIkB0uWLNE3lma6RoJQ-AQ.png"/></div></div></figure><p id="ba53" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now define the function k(u) as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/39d48ca6e7d2b0c4db071cd0eaddc43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*9ZZppiYGQDIhtYbKrNot0g.png"/></div></figure><p id="4485" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then we have that:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/130318df46d4d42879bfb8b4ef0d52e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wlbC5odz5vXjYZrdVCFlmQ.png"/></div></div></figure><p id="a33f" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Which is a special case of the kernel density estimator, where here k is the uniform kernel function. More generally, a kernel function is a non-negative function from the reals to the reals which satisfies:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/d490e3f1e63f3b07459eb3f97598cde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*irSK8t7abMhiH0OBsPl49g.png"/></div></figure><p id="8b6e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We will assume that all kernels discussed in this article are symmetric, hence we have that k(-u) = k(u).</p><p id="f919" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The moment of a kernel, which gives insights into the shape and behavior of the kernel function, is defined as the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/3a624958e05b0beb451fdc6b1e39836a.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*QkInx-WZh98kNIFaBZIy1g.png"/></div></figure><p id="d762" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Lastly, the order of a kernel is defined as the first non-zero moment.</p><p id="b4cd" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We can only minimize the error of the kernel density estimator by either changing the h value (bandwidth), or the kernel function. The bandwidth parameter has a much larger impact on the resulting estimate than the kernel function but is also much more difficult to choose. To demonstrate the influence of the h value, take the following two kernel density estimates. A Gaussian kernel was used to estimate a sample generated from a standard normal distribution, the only difference between the estimators is the chosen h value.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/8202c10fcb27e1b851f069889558d274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dmW9flkr_nhEiDPJ_eEauw.png"/></div></div></figure><pre class="mm mn mo mp mq ox oy oz bp pa bb bk"><span id="cf35" class="pb nu fq oy b bg pc pd l pe pf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from scipy.stats import gaussian_kde<br/><br/># Generate sample data<br/>np.random.seed(14)<br/>data = np.random.normal(loc=0, scale=1, size=100)<br/><br/># Define the bandwidths<br/>bandwidths = [0.1, 0.3]<br/><br/># Plot the histogram and KDE for each bandwidth<br/>plt.figure(figsize=(12, 8))<br/>plt.hist(data, bins=30, density=True, color='gray', alpha=0.3, label='Histogram')<br/><br/>x = np.linspace(-5, 5, 1000)<br/>for bw in bandwidths:<br/>    kde = gaussian_kde(data , bw_method=bw)<br/>    plt.plot(x, kde(x), label=f'Bandwidth = {bw}')<br/><br/># Add labels and title<br/>plt.title('Impact of Bandwidth Selection on KDE')<br/>plt.xlabel('Value')<br/>plt.ylabel('Density')<br/>plt.legend()<br/>plt.show()</span></pre><p id="5bf6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Quite a dramatic difference.</p><p id="8760" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now let us look at the impact of changing the kernel function while keeping the bandwidth constant.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/c99738f2f355e32578811f64b2dd879c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bPsJQDRiqHJCfiiDZLbpw.png"/></div></div></figure><pre class="mm mn mo mp mq ox oy oz bp pa bb bk"><span id="a168" class="pb nu fq oy b bg pc pd l pe pf">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.neighbors import KernelDensity<br/><br/># Generate sample data<br/>np.random.seed(14)<br/>data = np.random.normal(loc=0, scale=1, size=100)[:, np.newaxis]  # reshape for sklearn<br/><br/># Intialize a constant bandwidth<br/>bandwidth = 0.6<br/><br/># Define different kernel functions<br/>kernels = ["gaussian", "epanechnikov", "exponential", "linear"]<br/><br/># Plot the histogram (transparent) and KDE for each kernel<br/>plt.figure(figsize=(12, 8))<br/><br/># Plot the histogram<br/>plt.hist(data, bins=30, density=True, color="gray", alpha=0.3, label="Histogram")<br/><br/># Plot KDE for each kernel function<br/>x = np.linspace(-5, 5, 1000)[:, np.newaxis]<br/>for kernel in kernels:<br/>    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)<br/>    kde.fit(data)<br/>    log_density = kde.score_samples(x)<br/>    plt.plot(x[:, 0], np.exp(log_density), label=f"Kernel = {kernel}")<br/><br/>plt.title("Impact of Different Kernel Functions on KDE")<br/>plt.xlabel("Value")<br/>plt.ylabel("Density")<br/>plt.legend()<br/>plt.show()</span></pre><p id="6d13" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">While visually there is a large difference in the tails, the overall shape of the estimators are similar across the different kernel functions. Therefore, I will focus primarily focus on finding the optimal bandwidth for the estimator. Now, let’s explore some of the properties of the kernel density estimator, including its bias and variance.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4ab8" class="nt nu fq bf nv nw po gq ny nz pp gt ob oc pq oe of og pr oi oj ok ps om on oo bk"><strong class="al">Part 3: Properties of the Kernel Density Estimator</strong></h1><p id="8edd" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">The first fact that we will need to utilize is that the integral of the estimator across the real line is 1. To prove this fact, we will need to make use of the change of u-substitution:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/e33cf2e2049c29aca18e8a093ca66c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*Rirldq6Ahb6M_1f8flG97g.png"/></div></figure><p id="de0f" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Employing that u-substitution we get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/9f3a57706737f576377668a492d07d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMu05I-vR3wgS273jvRYag.png"/></div></div></figure><p id="f5e0" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now we can find the mean of the estimated density:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/ed503405029561f4a7976cd6a585ae03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gcl-RmBmUX-0ckSf3W0D7g.png"/></div></div></figure><p id="7c11" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Therefore, the mean of the estimated density is simply the sample mean.</p><p id="77b5" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Now let us find the second moment of the estimator.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/bfe471ec7996859cc904c593b9a23c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvBdirk5TFVc0B1-tqGFvg.png"/></div></div></figure><p id="e4c7" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We can then find the variance of the estimated density:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/2c822ab90eee6dff0793fa3b7c7ddaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2L8yg-XHu5b1TVmb2xK_AA.png"/></div></div></figure><p id="31bc" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">To find the optimal bandwidth and kernel, we will be minimizing the mean squared error of the estimator. To achieve this, we will first need to find the bias and variance of the estimator.</p><p id="d6bf" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The expected value of a random variable, X, with a probability density function of f, can be calculated as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/1675a80c94a4c62e844235a9ee73b25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*y68e0tssFy_kIiA8dHsLGg.png"/></div></div></figure><p id="b33e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Thus,</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/3f155d13144fd99e7f129ce6610dc563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEoe4uIXoOka_RAdhNXVTA.png"/></div></div></figure><p id="6bed" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Where z is a dummy variable of integration. We can then use a change of variables to get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/c0a469d13dbaa87efedfbe3e45fac73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*BHaceX__iv6_FcOim3IgFA.png"/></div></figure><p id="054e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Therefore, we get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/33a5697549c9c8fc7aef8cc5cc5ac786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*H7d2rZ3pjpxt54JW6KNbyg.png"/></div></figure><p id="27a6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Most of the time, however, the integral above is not analytically solvable. Therefore, we will have to approximate f(x+ hu) by using its Taylor expansion. As a reminder, the Taylor expansion of f(x) around a is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/e21d33426eddb14757b89477692c5e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PStWa0WTmxk458qfVAxCVQ.png"/></div></div></figure><p id="84af" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Thus, assume f(x) is differentiable to the v+1 term. For f(x+hu) the expansion is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/b0e29e10c63fa4e7688400f1b90e4c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H-lTZbtZ4vCZ_4MpWvDulw.png"/></div></div></figure><p id="fcbd" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then for a v-order kernel, we can take the expression out to the v’th term:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/e062240689da239fe38625a7e2a75c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*Euembivc9-i0JLpvo2vdxQ.png"/></div></figure><p id="71fe" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Where the last term is the remainder, which vanishes faster than h as h approaches 0. Now, assuming that k is a v’th order kernel function, we have that:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/f7854ca857959f4e956f84b66bff38be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMwOMCHtTWkTdI0WNLOegQ.png"/></div></div></figure><p id="3470" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Therefore:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/9618d12cb67889aa94553603e126194a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVVw593gCvCaybd1m-fiPg.png"/></div></div></figure><p id="f27d" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Thus we have that the bias of the estimator is then:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qj"><img src="../Images/aeab30eaee8173e839d4b93f05452fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*5iKkoxvs-4irAmLnRATPuw.png"/></div></figure><p id="0542" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The upper bound for the variance can be obtained via the following calculation <a class="af pt" href="https://faculty.washington.edu/yenchic/17Sp_403/Lec7-density.pdf" rel="noopener ugc nofollow" target="_blank">[Chen 2].</a></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/4082e2eee9c96f468b9e2bd8e068cdae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eOw9_Z_ojCo8WVduod8xg.png"/></div></div></figure><p id="9de9" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Where:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qk"><img src="../Images/349e7cdc90db769123596f25b306f7f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*UooBfFpGIt3luHjT7JktlA.png"/></div></figure><p id="a85d" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">This term is also known as the roughness and can be denoted as R(k).</p><p id="4559" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Finally, we can get an expression for the mean squared error:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/9c1d3c9c4559f6512c1fd4194a06d4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-d_EN0GcjJSoyZ4hZ0gQbQ.png"/></div></div></figure><p id="7a16" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Where AMSE is short for asymptotic mean squared error. We can then minimize the asymptotic mean integrated square error defined as follows,</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ql"><img src="../Images/02ada6a1f661f96e4fbc20bc99bbf909.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*eEMFxHEX-LGRxDaFfjb0VA.png"/></div></figure><p id="aa68" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">to find what bandwidth will lead to the lowest error (Silverman, 1986):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/c473fd6806cfa8781ba6b8a6a67c3ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L207o-vnU_o99xK-yFvLWw.png"/></div></div></figure><p id="5214" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then, by setting the equation to 0 and simplifying we find that the optimal bandwidth for a kernel of order v is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/b95e30c3665889767077d44b94e3e699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43GH8UcKXfvZKyud4Emr8A.png"/></div></div></figure><p id="f14d" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The more familiar expression is for second order kernels, where the bandwidth that minimizes the AMISE is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/53599fe5eeb322d7a7b1003ac5a7276d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_BdeRAIgcJ7sJ4BvPNz8w.png"/></div></div></figure><p id="c9a2" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">However, this solution may be a letdown as we require knowing the distribution that we are estimating to find the optimal bandwidth. In practice, we would not have this distribution if we were using the kernel density estimator in the first place.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="53a5" class="nt nu fq bf nv nw po gq ny nz pp gt ob oc pq oe of og pr oi oj ok ps om on oo bk"><strong class="al">Part 4: Bandwidth Selection</strong></h1><p id="8863" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">Despite not being able to find the bandwidth that minimizes the mean integrated square error, there are several methods available to choose a bandwidth without knowing the underlying distribution.<strong class="mz fr"> It is important to note, however, that a larger h value will cause your estimator to have less variance but greater bias, while a smaller bandwidth will produce a rougher estimate with less bias (Eidous et al., 2010).</strong></p><p id="7f39" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Some methods to find a bandwidth include using:</p><p id="c6f3" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">1: The Solve-the-Equation Rule</p><p id="9c4a" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">2: Least Squares Cross-Validation</p><p id="8135" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">3: Biased Cross-Validation</p><p id="e89c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">4: The Direct Plug in Method</p><p id="03e6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">5: The Contrast Method</p><p id="d185" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">It is important to note that depending on the sample size and skewness of your data, the ‘best’ method to choose your bandwidth changes. Most packages in Python allow you to use Silverman's proposed method, where you directly plug in some distribution (typically normal) for f and then compute the bandwidth based upon the kernel function that you have chosen. This procedure, known as <strong class="mz fr">Silverman’s Rule of Thumb, </strong>provides a relatively simple estimate for the bandwidth. However, it often tends to overestimate, resulting in a smoother estimate with lower variance but higher bias. Silverman’s Rule of Thumb also specifically does not perform well for bimodal distributions, and there are more accurate techniques available for those cases.</p><p id="1978" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">If you assume that your data is normally distributed with a mean of 0, use the sample standard deviation, and apply the Epanechnikov kernel (discussed below), you can select the bandwidth using the Rule of Thumb via the following equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qm"><img src="../Images/15398a17b0be9cacc757582fe73205c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*0mWqlRcoWmtpodrkamvlKQ.png"/></div></figure><p id="b520" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Eidous et al. found the Contrast Method to have the best performance compared to the other methods I listed. However, this method has drawbacks, as it increases the number of parameters to be chosen.</p><p id="5da6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The cross validation method is another good choice in bandwidth selection method, as it often leads to a small bias but a large variance (Heidenreich et al, 2013). It is most useful for when you are looking for a bandwidth selector that tends to undershoot the optimal bandwidth. To keep this article not overly long I will only be going over the Ordinary Least Squares Cross Validation method. The method tends to work well for rather wiggly densities and a moderate sample size of around 50 to 100. If you have a very small or large sample size this <a class="af pt" href="https://core.ac.uk/download/pdf/159147961.pdf" rel="noopener ugc nofollow" target="_blank">paper</a> is a good resource to find another way to choose your bandwidth. As pointed out by Heidenreich, “<strong class="mz fr">it definitely makes a difference which bandwidth selector is chosen; not only in numerical terms but also for the quality of density estimation</strong>”.</p><p id="d24a" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As a quick refresher, when we are creating a model we reserve some of our sample as a validation set to see how our model performs on the sample that it has not been trained on. K Fold Cross Validation minimizes the impact of possibly selecting a test set that misrepresents the dataset by splitting the dataset into K parts and then training the model K times and allowing each part to be the validation set once.</p><p id="b533" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Leave One Out Cross Validation is K Fold Cross Validation taken to the extreme, where for a sample size of n, we train the model n times and leave out one sample each time. <a class="af pt" href="https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">Here</a> is an article that goes more in-depth into the method in the context of training machine learning algorithms.</p><p id="4a2e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Let us turn back to the AMISE expression. Instead of investigating the asymptotic mean integrated square error, we will minimize the mean integrated square error (MISE). First, we can expand the square:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/8d07064b850466ae87e1c1924a01c17d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dYmVIPVJN1wYAqZ33Tq25g.png"/></div></div></figure><p id="a7a8" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">As we have no control over the expected value of the last term, we can seek to minimize the first two terms and drop the third. Then, because X is an unbiased estimator for E[X], we can find the first term directly:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/5e82bb063a7a2eccf4342d6c403d000a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17bedUR-XtOYEdTpmMavFA.png"/></div></div></figure><p id="c78c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then the convolution of k with k is defined as the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/57fa4edb38fda6db5b996091bd9dd4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJWYN8Ma8WTM6K1rmPq3RA.png"/></div></div></figure><p id="abd6" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Hence,</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/bba7957ebdfc234384c04e6ebd32959b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i18VZNEK1V8Uu0onTP5dZw.png"/></div></div></figure><p id="fb3c" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Thus, for the first term, we have:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/dfba3c5255b22e2d8c8b1b0a13996df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G24Nu9mrIDvjMX65xKrTjw.png"/></div></div></figure><p id="8915" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Next, we can approximate the second term by Monte Carlo methods. First, as discussed earlier, the density function is equivalent to the derivative of the cumulative distribution function, which we can approximate via the empirical distribution function. Then, the integral can be approximated by the average value of the estimator over the <a class="af pt" href="https://jwmi.github.io/BMS/chapter5-monte-carlo.pdf" rel="noopener ugc nofollow" target="_blank">sample</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/f96f9260cac17a62a41b9d40be4bb682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5aZkGHD6pFJsn6JVJfgXAA.png"/></div></div></figure><p id="2e84" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Then the least squares cross validation selector (LSCV) is defined as the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/d4fc33d8d1ce23ecd2bd8b275c7012fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JwYhB-Zh1JLm2vpCy_hQfQ.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/f372f058f5adfa4cc5fd14285e1ffcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PygHhmLYkG-_GDxK_c4h4w.png"/></div></div></figure><p id="edb2" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">We then get the final selector defined as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/2d69b80d31ae868cc3d74514a1728103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHH1VcxSpJ6EZA-Uv2YQtQ.png"/></div></div></figure><p id="6154" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The optimal bandwidth is the h value that minimizes LSCV, defined as the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/3088842523b4847cd1a05dd2477eb061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7f2CN6LECRsM5B78uT66A.png"/></div></div></figure><p id="d520" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">The LSCV(h) function can have multiple local minimums, hence the optimal bandwidth that you find can be sensitive to the interval chosen. It is useful to graph the function and then visually investigate where the global minimum lies.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="fd31" class="nt nu fq bf nv nw po gq ny nz pp gt ob oc pq oe of og pr oi oj ok ps om on oo bk"><strong class="al">Part 5: Optimal Kernel Selection</strong></h1><p id="4b8e" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">If we are working with a second order kernel (which is typical), the choice in kernel selection is much more straightforward than bandwidth. <strong class="mz fr">Under the criteria of minimizing the AMISE, the Epanechnikov</strong> <strong class="mz fr">kernel is an optimal kernel.</strong> The full proof can be found in this paper by <a class="af pt" href="https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-2/Smooth-Optimum-Kernel-Estimators-of-Densities-Regression-Curves-and-Modes/10.1214/aos/1176346523.full" rel="noopener ugc nofollow" target="_blank">Muler</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/cfe9062c4a6e3b26a7b94a474d190507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7C5QW_ctz1rtHfWdsCaPg.png"/></div></div></figure><p id="03a9" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">There are other kernels which are as efficient as the Epanechnikov kernel, which are also touched on in Muler’s paper. However, I wouldn’t worry too much about your choice of kernel function, the choice in bandwidth is much more important.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9e1a" class="nt nu fq bf nv nw po gq ny nz pp gt ob oc pq oe of og pr oi oj ok ps om on oo bk"><strong class="al">Part 6: Further Topics and Conclusion</strong></h1><h2 id="5f5f" class="qn nu fq bf nv qo qp qq ny qr qs qt ob ng qu qv qw nk qx qy qz no ra rb rc rd bk">Adaptive Bandwidths</h2><p id="5c13" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">One of the proposed ways to improve the Kernel Density Estimator is through an adaptive bandwidth. An adaptive bandwidth adjusts the bandwidth for each data point, increasing it when the density of the sample data is lower and decreasing it when the density is higher. While promising in theory, an adaptive bandwidth has been shown to perform quite poorly in the univariate case (<a class="af pt" href="https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-3/Variable-Kernel-Density-Estimation/10.1214/aos/1176348768.full" rel="noopener ugc nofollow" target="_blank">Terrel, Scott 1992</a>). While it may be better for larger dimensional spaces, for the one-dimensional case I believe it is best to stick with a constant bandwidth.</p><h2 id="f706" class="qn nu fq bf nv qo qp qq ny qr qs qt ob ng qu qv qw nk qx qy qz no ra rb rc rd bk">Multivariate Kernel Density Estimation</h2><p id="3f99" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">The kernel density estimator can also be extended to higher dimensions, where the kernel is a radial basis function or is a product of multiple kernel functions. The approach does suffer from the curse of dimensionality, where as the dimension grows larger the number of data points needed to produce a useful estimator grows exponentially. It is also computationally expensive and is not a great method for analyzing high-dimensional data.</p><p id="8e9e" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Nonparametric multivariate density estimation is still a very active field, with <a class="af pt" href="https://arxiv.org/abs/1705.07057" rel="noopener ugc nofollow" target="_blank">Masked Autoregressive Flow </a>appearing to be quite a new and promising approach.</p><h2 id="103e" class="qn nu fq bf nv qo qp qq ny qr qs qt ob ng qu qv qw nk qx qy qz no ra rb rc rd bk">Real World Applications</h2><p id="2c59" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">Kernel density estimation has numerous applications across disciplines. First, it has been shown to improve machine learning algorithms such as in the case of the flexible naive Bayes classifier.</p><p id="0c32" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">It has also been used to estimate <a class="af pt" href="https://www.sciencedirect.com/science/article/pii/S2095756415305808" rel="noopener ugc nofollow" target="_blank">traffic accident density</a>. The linked paper uses the KDE to help make a model that indicates the risk of traffic accidents in different cities across Japan.</p><p id="e122" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk">Another fun use is in <a class="af pt" href="https://pubs.geoscienceworld.org/ssa/bssa/article-abstract/86/2/353/120015/Kernel-estimation-methods-for-seismic-hazard-area" rel="noopener ugc nofollow" target="_blank">seismolog</a>y, where the KDE has been used for modelling the risk of earthquakes in different locations.</p><h2 id="4e17" class="qn nu fq bf nv qo qp qq ny qr qs qt ob ng qu qv qw nk qx qy qz no ra rb rc rd bk">Conclusion</h2><p id="391f" class="pw-post-body-paragraph mx my fq mz b go op nb nc gr oq ne nf ng or ni nj nk os nm nn no ot nq nr ns fj bk">The kernel density estimator is an excellent addition to the data analysts’ toolbox. While a histogram is certainly a fine way of analyzing data without using any underlying assumptions, the kernel density estimator provides a solid alternative for univariate data. For higher dimensional data, or for when computational time is a concern, I would recommend looking elsewhere. Nonetheless, the KDE is an intuitive, powerful and versatile tool in data analysis.</p><p id="f234" class="pw-post-body-paragraph mx my fq mz b go na nb nc gr nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns fj bk"><em class="re">Unless otherwise noted, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>