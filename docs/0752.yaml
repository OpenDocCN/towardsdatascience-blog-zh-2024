- en: Understanding the Sparse Mixture of Experts (SMoE) Layer in Mixtral
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-the-spare-mixture-of-experts-smoe-layer-in-mixtral-687ab36457e2?source=collection_archive---------2-----------------------#2024-03-21](https://towardsdatascience.com/understanding-the-spare-mixture-of-experts-smoe-layer-in-mixtral-687ab36457e2?source=collection_archive---------2-----------------------#2024-03-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This blog post will explore the findings of the “Outrageously Large Neural
    Networks: The Sparsely-Gated Mixture-of-Experts Layer” paper and its implementation
    in Mixtral'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--687ab36457e2--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--687ab36457e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--687ab36457e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--687ab36457e2--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--687ab36457e2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--687ab36457e2--------------------------------)
    ·8 min read·Mar 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22c84ef8e930f85af40fdb3ea5507534.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Author generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: The Quest for Specialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When challenging a difficult problem, divide and conquer is often a valuable
    solution. Whether it be Henry Ford’s assembly lines, the way merge sort partitions
    arrays, or how society at large tends to have people who specialize in specific
    jobs, the list goes on and on!
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, when people approached the task of teaching computers to reason,
    it made sense to divide up the tasks we had for the machine into component pieces
    — for example, one component for math, one component for science, one for language,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f4560d6c8508122c1048e4db4625f03.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, this idea has not yet been successfully realized. It’s possible
    this is failing for the same reason that our brain doesn’t have nearly independent
    components: complex reasoning requires using many different parts in concert,
    not separately.'
  prefs: []
  type: TYPE_NORMAL
