- en: How to Evaluate Search Relevance and Ranking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/metrics-that-matter-a-simple-guide-to-search-ranking-evaluation-4030084c35b4?source=collection_archive---------8-----------------------#2024-05-30](https://towardsdatascience.com/metrics-that-matter-a-simple-guide-to-search-ranking-evaluation-4030084c35b4?source=collection_archive---------8-----------------------#2024-05-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Key metrics to optimize your search engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@akchay_srivastava?source=post_page---byline--4030084c35b4--------------------------------)[![Akchay
    Srivastava](../Images/2948bc0588017121409d264835a3068f.png)](https://medium.com/@akchay_srivastava?source=post_page---byline--4030084c35b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4030084c35b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4030084c35b4--------------------------------)
    [Akchay Srivastava](https://medium.com/@akchay_srivastava?source=post_page---byline--4030084c35b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4030084c35b4--------------------------------)
    ·6 min read·May 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd4ca57f7090c47971c84ed838098923.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Markus Winkler](https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Precision@K
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean Average Precision (MAP)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank (MRR)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalized Discounted Cumulative Gain (NDCG)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comparative Analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Disclaimer: The views expressed here are my own and do not necessarily reflect
    the views of my employer or any other organization. All images are by the author,
    except where indicated.'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensuring users find the information they need quickly and efficiently is paramount
    to a successful search experience. When users find what they’re looking for quickly
    and effortlessly, it translates into a positive experience.
  prefs: []
  type: TYPE_NORMAL
- en: '***Furthermore, the ranking position of relevant results also plays a crucial
    role — the higher they appear, the more valuable they are to the user. This translates
    to increased user engagement, conversions, and overall website satisfaction.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article explores the key metrics used for evaluating Search Relevance and
    Ranking, empowering you to optimize your Search Engine and deliver a superior
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the concept of Search Relevance in a practical way, let’s consider
    a user searching for “pasta dishes” on a search engine. For simplicity, we’ll
    analyze the top five results returned by the engine. Relevant results will be
    denoted in green, while those deemed irrelevant will be highlighted in red (refer
    to Figure 1). We’ll use the Rn notation to represent the nth result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d5c17bbe30ba5f53ace96eadb634ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An ordered list of search results'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Precision@K**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Precision@K measures how many results within the top K positions are relevant.
    We compute Precision for different values of K, as shown in Figure 2.
  prefs: []
  type: TYPE_NORMAL
- en: Precision@K = Number of relevant results within the top K positions / K
  prefs: []
  type: TYPE_NORMAL
- en: Precision@1 = 1/1
  prefs: []
  type: TYPE_NORMAL
- en: Precision@3 = 1/3
  prefs: []
  type: TYPE_NORMAL
- en: Precision@5 = 2/5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ba61ecbf66262be248e3031e4361f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Precision@K'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Mean Average Precision (MAP)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MAP considers the ranking order of relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, Precision@K is calculated for each of these relevant result positions.
    Then the Average Precision@K is obtained by summing up the Precision@K for each
    of these relevant result positions and dividing by the total number of **relevant**
    items in the top K results. For brevity, we will occasionally refer to Average
    Precision as AP in the discussion.
  prefs: []
  type: TYPE_NORMAL
- en: To gain a deeper understanding of how MAP evaluates ranking effectiveness, let’s
    explore illustrative examples across three distinct search queries. These examples
    will highlight how the order in which results are presented influences the MAP
    score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44949b807bde8f6453a2f4f617b9ad6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Precision@K for each relevant result for Query 1'
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_1 = (Precision@1 + Precision@3 + Precision@5) / 3
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_1 = (1 + 0.67 + 0.6) / 3 = 0.76
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8da0eeaad298366ff46268b8ee7199a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Precision@K for each relevant result for Query 2'
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_2 = (Precision@1 + Precision@2 + Precision@5) / 3
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_2 = (1 + 1 + 0.6) / 3 = 0.87
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fe7f4ead42b1572918240b2e46e8108.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Precision@K for each relevant result for Query 3'
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_3 = (Precision@3 + Precision@4 + Precision@5) / 3
  prefs: []
  type: TYPE_NORMAL
- en: AP@5_Query_3 = (0.33 + 0.5 + 0.6) / 3 = 0.47
  prefs: []
  type: TYPE_NORMAL
- en: '**The results for Query 2 exhibit the highest Average Precision@5, indicating
    that the most relevant items are positioned at the beginning of the ranked list.**'
  prefs: []
  type: TYPE_NORMAL
- en: MAP = Mean of Average Precision across all queries in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: MAP@5 = (AP@5_Query_1 + AP@5_Query_2 + AP@5_Query_3) / Number of queries
  prefs: []
  type: TYPE_NORMAL
- en: MAP@5 of the dataset = (0.76 + 0.87 + 0.47) / 3 = 0.7
  prefs: []
  type: TYPE_NORMAL
- en: '***This calculation treats all queries as equally important. However, if some
    queries are more critical, different weighting methods can be used within the
    MAP process to prioritize them.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**4\. Mean Reciprocal Rank (MRR)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MRR considers only the rank of the **first relevant result** found in the list.
  prefs: []
  type: TYPE_NORMAL
- en: K = Rank of first relevant result
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocal Score = 1 / K
  prefs: []
  type: TYPE_NORMAL
- en: MRR is the average reciprocal score across multiple queries. If there is no
    relevant result, then the rank of the first relevant result is considered to be
    infinity. Therefore, the reciprocal score becomes 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ad9599ba656f953430615e816251169.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Reciprocal Score for each query (in blue)'
  prefs: []
  type: TYPE_NORMAL
- en: '***The reciprocal score of a relevant result is an inverse function of its
    rank.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MRR of the dataset = (0.5 + 1 + 0.33) / 3 = 0.61
  prefs: []
  type: TYPE_NORMAL
- en: '**5\. Normalized Discounted Cumulative Gain (NDCG)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NDCG takes into account the graded relevance of results. The relevance of each
    result is represented by a score (also known as a “grade”). The value of NDCG
    is determined by comparing the relevance of the results returned by a search engine
    to the relevance of the results that a **hypothetical “ideal” search engine would
    return.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we’ve got a relevance/grading scale of 1–5, with 5 being the highest
    score and 1 being the lowest score. We search for “pasta dishes” and manually
    grade the search results by providing them with a relevance score, as shown in
    Figure 7\. In our example, R3 is the most relevant result, with a score of 5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1035da10055f8e313e45a142662ac947.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An ordered list of search results with their relevance scores'
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative Gain@5 = 4 + 1 + 5 + 1 + 3 = 14
  prefs: []
  type: TYPE_NORMAL
- en: Cumulative gain does not take ranking into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Discounted Cumulative Gain@K = A **logarithmic discount** is applied that helps
    assign lower gain when relevant items appear further down in the ranked list,
    as shown in Figure 8.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a27130cd1dd4538d1945d1c824b9bdba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: DCG@K Formula'
  prefs: []
  type: TYPE_NORMAL
- en: Where rel(i) is the relevance score of the result at position i.
  prefs: []
  type: TYPE_NORMAL
- en: DCG@K = 4/1 + 1/1.585 + 5/2 + 1/2.322+ 3/2.585 = 8.72
  prefs: []
  type: TYPE_NORMAL
- en: 'The absolute value of DCG depend on the number of results in the list and the
    relevance scores assigned. In order to address this, DCG can be normalized. To
    get the normalized DCG (NDCG), we divide the DCG by the ideal DCG (IDCG) for the
    given result set, as shown in Figure 9\. IDCG considers the same relevance scores,
    but calculates the DCG assuming the absolute best ranking order for those results.
    The best ranking order for the above example would be: R3 → R1 → R5 → R2 → R4.'
  prefs: []
  type: TYPE_NORMAL
- en: IDCG@K = 5/1 + 4/1.585 + 3/2 + 1/2.322 + 1/2.585 = 9.83
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/859184e63318ed59a25f5e5adc9b0fd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: NDCG@K Formula'
  prefs: []
  type: TYPE_NORMAL
- en: NDCG@K = 8.72/9.83 = 0.88
  prefs: []
  type: TYPE_NORMAL
- en: '***NDCG accounts for the graded relevance of results, providing a more nuanced
    understanding of Search Ranking Quality.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**6\. Comparative Analysis**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/f9792d300e683ba1f6e70d059e481f04.png)'
  prefs: []
  type: TYPE_IMG
- en: In addition to the above metrics, the **Spearman Correlation Coefficient** and
    **Kendall Tau Distance** can be employed to assess the similarity of ranked lists.
    For measuring user engagements, **Click-Through Rate (CTR)** is a key metric that
    reflects the percentage of users who have clicked on a result after it’s displayed.
    For more information on these metrics, please consult the Wikipedia resources
    listed in the References section.
  prefs: []
  type: TYPE_NORMAL
- en: '**7\. Summary**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5b3e00948e36d78a92695abdd6b68fff.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alexander Schimmeck](https://unsplash.com/@alschim?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Following our exploration of four distinct metrics for search quality evaluation,
    we conducted a comparative analysis to understand the strengths and weaknesses
    of each approach. This naturally leads us to the critical question: Which metric
    is most suitable for evaluating the Relevance and Ranking of your Search Engine
    Results? The optimal metric selection depends on your specific requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**For a comprehensive understanding of the quality of your Search Engine, it
    is often beneficial to consider a combination of these metrics rather than relying
    on a single measure.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you found this article useful, please cite this write-up as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Srivastava, Akchay. (May 2024). How to Evaluate Search Relevance and Ranking.*
    [*https://towardsdatascience.com/metrics-that-matter-a-simple-guide-to-search-ranking-evaluation-4030084c35b4*](/metrics-that-matter-a-simple-guide-to-search-ranking-evaluation-4030084c35b4)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**8\. References:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Mean_reciprocal_rank](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Kendall_tau_distance](https://en.wikipedia.org/wiki/Kendall_tau_distance)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Discounted_cumulative_gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf](https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.coursera.org/lecture/recommender-metrics/rank-aware-top-n-metrics-Wk98r](https://www.coursera.org/lecture/recommender-metrics/rank-aware-top-n-metrics-Wk98r)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.evidentlyai.com/ranking-metrics/ndcg-metric](https://www.evidentlyai.com/ranking-metrics/ndcg-metric)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Inter-rater_reliability](https://en.wikipedia.org/wiki/Inter-rater_reliability)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Click-through_rate](https://en.wikipedia.org/wiki/Click-through_rate)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
