- en: 'LLaVA: An open-source alternative to GPT-4V(ision)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23](https://towardsdatascience.com/llava-an-open-source-alternative-to-gpt-4v-ision-b06f88ce8efa?source=collection_archive---------2-----------------------#2024-01-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running LLaVA on the Web, locally, and on Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Yann-A√´l
    Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    [Yann-A√´l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--b06f88ce8efa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06f88ce8efa--------------------------------)
    ¬∑7 min read¬∑Jan 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Curious where this picture was taken? Ask LLaVA! (Image by [Guy Rey-Bellet](https://pixabay.com/users/grey48-7109111/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3116211)).
  prefs: []
  type: TYPE_NORMAL
- en: '[LLaVA](https://llava-vl.github.io/) (acronym of **L**arge **L**anguage and
    **V**isual **A**ssistant) is a promising open-source generative AI model that
    replicates some of the capabilities of OpenAI GPT-4 in conversing with images.
    Users can add images into LLaVA chat conversations, allowing to discuss about
    the content of these images, but also to use them as a way to describe ideas,
    contexts or situations in a visual way.'
  prefs: []
  type: TYPE_NORMAL
- en: The most compelling features of LLaVA are its ability to improve upon other
    open-source solutions while using a simpler model architecture and orders of magnitude
    less training data. These characteristics make LLaVA not only faster and cheaper
    to train, but also more suitable for inference on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: This post gives an overview of LLaVA, and more specifically aims to
  prefs: []
  type: TYPE_NORMAL
- en: show how to experiment with it from a web interface, and how it can be installed
    on your computer or laptop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: explain its main technical characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: illustrate how to program with it, using as an example a simple chatbot application
    built with HuggingFace libraries (*Transformers* and *Gradio*) on Google Colab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLaVA online
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have not yet tried it, the simplest way to use LLaVA is by going to the
    [Web interface](https://llava.hliu.cc/) provided by its authors. The screenshot
    below illustrates how the interface operates, where a user asks for ideas about
    what meals to do given a picture of the content of their fridge. Images can be
    loaded using the widget on the left, and the chat interface allows to ask questions
    and obtain answers in the form of text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4dc5aac06a83925f5c53726d2e9212e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[LLaVA Web interface](https://llava.hliu.cc/)'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, LLaVA correctly identifies ingredients present in the fridge,
    such as blueberries, strawberries, carrots, yoghourt or milk, and suggest relevant
    ideas such as fruit salads, smoothies or cakes.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples of conversations with LLaVA are given on the [project website](https://llava-vl.github.io/),
    which illustrate that LLaVA is capable of not just describing images but also
    making inferences and reasoning based on the elements within the image (identify
    a movie or a person using clues from a picture, code a website from a drawing,
    explain humourous situations, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Running LLaVA locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLaVA can also be installed on a local machine using [Ollama](https://ollama.ai/)
    or a Mozilla ‚Äò[llamafile](https://github.com/Mozilla-Ocho/llamafile)‚Äô. These tools
    can run on most CPU-only consumer-grade level machines, as the model only requires
    8GB of RAM and 4GB of free disk space, and was even shown to [successfully run
    on a Raspberry PI](/running-local-llms-and-vlms-on-the-raspberry-pi-57bd0059c41a).
    Among the tools and interfaces developed around the Ollama project, a notable
    initiative is the [Ollama-WebUI](https://github.com/ollama-webui/ollama-webui)
    (illustrated below), which reproduces the look and feel of OpenAI ChatGPT user
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da0916a9564113060e6305c3a0d43299.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Ollama Web user interface](https://github.com/ollama-webui/ollama-webui) ‚Äî
    inspired by [OpenAI ChatGPT](https://chat.openai.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Brief overview of LLaVA‚Äôs main features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLaVA was designed by researchers from the University of Wisconsin-Madison,
    Microsoft Research and Columbia University, and was recently showcased at NeurIPS
    2023\. The project‚Äôs code and technical specifications can be accessed on its
    [Github repository](https://github.com/haotian-liu/LLaVA), which also offers various
    interfaces for interacting with the assistant.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the authors summarize in [their paper‚Äôs abstract](https://arxiv.org/pdf/2310.03744.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLava] achieves state-of-the-art across 11 benchmarks. Our final 13B checkpoint
    uses merely 1.2M publicly available data, and finishes full training in ~1 day
    on a single 8-A100 node. We hope this can make state-of-the-art LMM research more
    accessible. Code and model will be publicly available.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The benchmark results, reported in the paper as the radar chart below, illustrate
    the improvements compared to other state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98a3010228400b1064aded781a0eee41.png)'
  prefs: []
  type: TYPE_IMG
- en: Radar chart of LLaVA‚Äôs benchmark results (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: Inner workings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLaVA‚Äôs data processing workflow is conceptually simple. The model essentially
    works as a standard causal language model, taking language instructions (a user
    text prompt) as input, and returning a language response. The ability of the language
    model to handle images is allowed by a separate vision encoder model that converts
    images into language tokens, which are quietly added to the user text prompt (acting
    as a kind of [soft prompt](https://huggingface.co/docs/peft/main/en/conceptual_guides/prompting)).
    The LLaVA process is illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b68138aa94aba7d0da27c16272489945.png)'
  prefs: []
  type: TYPE_IMG
- en: LLaVA network architecture (image from [paper](https://arxiv.org/pdf/2304.08485.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: LLaVA‚Äôs language model and vision encoder rely on two reference models called
    Vicuna and CLIP, respectively. [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)
    is a pretrained large language model based on LLaMA-2 (designed by Meta) that
    boasts competitive performances with medium sized LLM (See model cards for the
    [7B](https://huggingface.co/lmsys/vicuna-7b-v1.5) and [13B](https://huggingface.co/lmsys/vicuna-13b-v1.5)
    versions on HuggingFace). [CLIP](https://openai.com/research/clip) is an image
    encoder designed by OpenAI, pretrained to encode images and text in a similar
    embedding space using **c**ontrastive **l**anguage-**i**mage **p**retraining (hence
    ‚ÄòCLIP‚Äô). The model used in LLaVA is the vision transformer variant CLIP-ViT-L/14
    (see its [model card](https://huggingface.co/openai/clip-vit-large-patch14) on
    HuggingFace).
  prefs: []
  type: TYPE_NORMAL
- en: To match the dimension of the vision encoder with those of the language model,
    a projection module (**W** in the image above) is applied. It is a simple linear
    projection in the original [LLaVA](https://arxiv.org/abs/2304.08485), and a two-layer
    perceptron in [LLaVA 1.5](https://arxiv.org/abs/2310.03744).
  prefs: []
  type: TYPE_NORMAL
- en: Training process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training process of LLaVA consists of two relatively simple stages.
  prefs: []
  type: TYPE_NORMAL
- en: The first stage solely aims at tuning the projection module **W**, and the weights
    of the vision encoder and LLM are kept frozen. The training is performed using
    a subset of around 600k image/caption pairs from the [CC3M conceptual caption
    dataset](https://ai.google.com/research/ConceptualCaptions/), and is available
    on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K).
  prefs: []
  type: TYPE_NORMAL
- en: In a second stage, the projection module weigths **W** are fine-tuned together
    with the LLM weights (while keeping the vision encoder‚Äôs weights frozen), using
    dataset of 158K language-image instruction-following data. The data is generated
    using GPT4, and feature examples of conversations, detailed descriptions and complex
    reasonings, and is available on HuggingFace [in this repository](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K).
  prefs: []
  type: TYPE_NORMAL
- en: The whole training takes around a day using eight A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Programming with LLaVA: How to get started'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Code available on the* [*Colab related notebook*](https://colab.research.google.com/drive/1L28bJX14-Y5lJvswYwydsletYFMIxVH5)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: The LLaVA model is integrated in the Transformers library, and can be loaded
    using the standard *pipeline* object. The 7B and 13B variants of the models are
    available on the [LLaVA üòä Hub space](https://huggingface.co/llava-hf), and may
    be loaded in 4 and 8 bits to save GPU memory. We illustrate below how to load
    and run model using code that can be executed on Colab with a T4 TPU (15GB RAM
    GPU).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is the code snippet to load the 7B variant of LLaVA 1.5 in 4 bits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let us then load this picture
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce1327ce8af3bcc9107ecbaba49c33c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We use the standard PIL library for loading the picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let us finally query the LLaVA model with the image, with a prompt asking to
    describe the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: [The format for the prompt](https://huggingface.co/docs/transformers/model_doc/llava)
    follows'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ÄúUSER: <image>\n<prompt>\nASSISTANT:‚Äù'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Which returns the following answer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'USER: Describe this picture'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '‚ÄãASSISTANT: ‚ÄãThe image features a large, empty amphitheater with a stunning
    view of the ocean in the background. The amphitheater is surrounded by a lush
    green hillside, and a majestic mountain can be seen in the distance. The scene
    is serene and picturesque, with the sun shining brightly over the landscape.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLaVA chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us finally create a simple chatbot that relies on a LLaVA model. We will
    use the [Gradio library](https://www.gradio.app/), which provides a fast and easy
    way to create machine learning web interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: The core for the interface consists of a row with an image uploader (a Gradio
    Image object), and a chat interface (a Gradio [ChatInterface](https://www.gradio.app/docs/chatinterface)
    object).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The chat interface connects to a function *update_conversation*, that takes
    care of keeping the conversation history, and calling the LLaVA model for a response
    whenever the user sends a message.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The interface is launched calling the *launch* method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few seconds, the chatbot Web interface will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05629290b6a0cdaaf30927ff858868a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations, your LLaVA chatbot is now up and running!
  prefs: []
  type: TYPE_NORMAL
- en: Useful links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[HuggingFace LLaVA model documentation](https://huggingface.co/docs/transformers/model_doc/llava)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llava Hugging Face organization](https://huggingface.co/llava-hf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Loading and running LLaVA with AutoPrecessor and LLaVAForConditionalGeneration:
    [Colab notebook](https://colab.research.google.com/drive/1_q7cOB-jCu3RExrkhrgewBR0qKjZr-Sx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-4V(ision) system card](https://cdn.openai.com/papers/GPTV_System_Card.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Understanding Visual Instruction Tuning](https://newsletter.artofsaience.com/p/understanding-visual-instruction)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: Unless otherwise noted, all images are by the author.'
  prefs: []
  type: TYPE_NORMAL
