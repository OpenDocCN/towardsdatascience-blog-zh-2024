<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Build Autonomous AI Agents with Function Calling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Build Autonomous AI Agents with Function Calling</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/build-autonomous-ai-agents-with-function-calling-0bb483753975?source=collection_archive---------0-----------------------#2024-04-02">https://towardsdatascience.com/build-autonomous-ai-agents-with-function-calling-0bb483753975?source=collection_archive---------0-----------------------#2024-04-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ba5a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Transform your chatbot into an agent that can interact with external APIs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jyipkl?source=post_page---byline--0bb483753975--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Julian Yip" class="l ep by dd de cx" src="../Images/2afc0ac6c4dcccaa57ffe70b2f5a14d0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mymOKtsl9xup838ppb5J6g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0bb483753975--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jyipkl?source=post_page---byline--0bb483753975--------------------------------" rel="noopener follow">Julian Yip</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0bb483753975--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Apr 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">11</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="f3fd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Function Calling is not something new. In July 2023, OpenAI introduced Function Calling for their GPT models, a feature now being adopted by competitors. Google‚Äôs Gemini API recently supported it, and Anthropic is integrating it into Claude. Function Calling is becoming essential for large language models (LLMs), enhancing their capabilities. All the more useful to learn this technique!</p><p id="8a0b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With this in mind, I aim to write a comprehensive tutorial covering Function Calling beyond basic introductions (there are already plenty of tutorials for it). The focus will be on practical implementation, building a fully autonomous AI agent and integrating it with Streamlit for a ChatGPT-like interface. Although OpenAI is used for demonstration, this tutorial can be easily adapted for other LLMs supporting Function Calling, such as Gemini.</p><h1 id="e04f" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="al">What is Function Calling for?</strong></h1><p id="0f3b" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Function Calling enables developers to describe functions (aka tools, you can consider this as actions for the model to take, like performing calculation, or making an order), and have the model intelligently choose to output a JSON object containing arguments to call those functions. In simpler terms, it allows for:</p><ul class=""><li id="153c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne og oh oi bk"><strong class="ml fr">Autonomous decision making</strong>: Models can intelligently choose tools to respond to questions.</li><li id="86b6" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">Reliable parsing</strong>: Responses are in JSON format, instead of the more typical dialogue-like response. It might not seem much from the first look, but this is what allows LLM to connect to external systems, say via APIs with structured inputs.</li></ul><p id="2f81" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It opens up numerous possibilities:</p><ul class=""><li id="b671" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne og oh oi bk"><strong class="ml fr">Autonomous AI assistants</strong>: Bots can interact with internal systems for tasks like customer orders and returns, beyond providing answers to enquiries</li><li id="63a1" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">Personal research assistants</strong>: Say if you are planning for your travel, assistants can search the web, crawl content, compare options, and summarize results in Excel.</li><li id="4bbd" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">IoT voice commands</strong>: Models can control devices or suggest actions based on detected intents, such as adjusting the AC temperature.</li></ul></div></div></div><div class="ab cb oo op oq or" role="separator"><span class="os by bm ot ou ov"/><span class="os by bm ot ou ov"/><span class="os by bm ot ou"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e5a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ow">(Sidetracking a bit: To realize such potentials, we must have a systematic way to design our prompts and to test them. I have written an article about this too!)</em></p><div class="ox oy oz pa pb pc"><a rel="noopener follow" target="_blank" href="/prompt-like-a-data-scientist-auto-prompt-optimization-and-testing-with-dspy-ff699f030cb7?source=post_page-----0bb483753975--------------------------------"><div class="pd ab ig"><div class="pe ab co cb pf pg"><h2 class="bf fr hw z io ph iq ir pi it iv fp bk">Prompt Like a Data Scientist: Auto Prompt Optimization and Testing with DSPy</h2><div class="pj l"><h3 class="bf b hw z io ph iq ir pi it iv dx">Applying machine learning methodology to prompt building</h3></div><div class="pk l"><p class="bf b dy z io ph iq ir pi it iv dx">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq lr pc"/></div></div></a></div><p id="bf11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Without further ado, let‚Äôs explore what Function Calling is about!</p><h1 id="060f" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">The Structure of Function Calling</h1><p id="0917" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Borrowing from <a class="af pr" href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling" rel="noopener ugc nofollow" target="_blank">Gemini‚Äôs Function Calling documentation</a>, Function Calling has the below structure, which works the same in OpenAI</p><figure class="pv pw px py pz qa ps pt paragraph-image"><div class="ps pt pu"><img src="../Images/9793c45c8d104b8ac96e104541df9644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*rrgnwWr7Hif3TfH_"/></div><figcaption class="qc qd qe ps pt qf qg bf b bg z dx">Image from <a class="af pr" href="https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling" rel="noopener ugc nofollow" target="_blank">Gemini‚Äôs Function Calling documentation</a></figcaption></figure><ol class=""><li id="b6de" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oh oi bk">User issues prompt to the application</li><li id="a396" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Application passes the user-provided prompt, and the Function Declaration(s), which is a description of the tool(s) that the model could use</li><li id="f220" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Based on the Function Declaration, the model suggests the tool to use, and the relevant request parameters. <strong class="ml fr">Notice the model outputs the suggested tool and parameters only, WITHOUT actually calling the functions</strong></li><li id="62b1" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">&amp; 5. Based on the response, the application invokes the relevant API</li></ol><p id="a1cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">6. &amp; 7. The response from API is fed into the model again to output a human-readable response</p><p id="76fd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">8. Application returns the final response to the user, then repeat from 1.</p><p id="b744" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This might seem convuluted, but the concept will be illustrated in detail with example</p><h1 id="d790" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Architecture</h1><p id="fe5d" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Before diving into the code, a few words about the demo application‚Äôs architecture</p><h2 id="5ecb" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk"><strong class="al">Solution</strong></h2><p id="1920" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Here we build an assistant for tourists visiting a hotel. The assistant has access to the following tools, which allows the assistant to access external applications.</p><ul class=""><li id="5fd3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne og oh oi bk"><code class="cx qz ra rb rc b">get_items</code>, <code class="cx qz ra rb rc b">purchase_item</code>: Connect to product catalog stored in database via API, for retrieving item list and making a purchase respectively</li><li id="f764" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><code class="cx qz ra rb rc b">rag_pipeline_func</code>: Connect to document store with Retrieval Augmented Generation (RAG) to obtain information from unstructured texts e.g. hotel‚Äôs brochures</li></ul><figure class="pv pw px py pz qa ps pt paragraph-image"><div role="button" tabindex="0" class="re rf ed rg bh rh"><div class="ps pt rd"><img src="../Images/aa901baf05ab30875134a9a0b958aa14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2aXhxSCJKyOVzkC0IJofiQ.png"/></div></div></figure><h2 id="62d6" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk"><strong class="al">Tech stack</strong></h2><ul class=""><li id="4659" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne og oh oi bk"><strong class="ml fr">Embedding model</strong>: <a class="af pr" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2" rel="noopener ugc nofollow" target="_blank">all-MiniLM-L6-v2</a></li><li id="3134" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">Vector Database</strong>: <a class="af pr" href="https://docs.haystack.deepset.ai/docs/inmemorydocumentstore" rel="noopener ugc nofollow" target="_blank">Haystack‚Äôs InMemoryDocumentStore</a></li><li id="dff1" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">LLM</strong>: <a class="af pr" href="https://openrouter.ai/models/openai/gpt-4-1106-preview" rel="noopener ugc nofollow" target="_blank">GPT-4 Turbo accessed via OpenRouter</a>. With OpenRouter you can access different LLM APIs from Hong Kong without VPN. The flow can be adapted into using other LLMs with slight code change, provided they support Function Calling, say Gemini</li><li id="9c53" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne og oh oi bk"><strong class="ml fr">LLM Framework</strong>: <a class="af pr" href="https://haystack.deepset.ai/" rel="noopener ugc nofollow" target="_blank">Haystack</a> for their ease of use, great documentation and transparency in pipeline construction. This tutorial is actually an extension to their <a class="af pr" href="https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling" rel="noopener ugc nofollow" target="_blank">fantastic tutorial</a> for the same topic</li></ul><p id="eb56" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now let‚Äôs begin!</p><h1 id="5a37" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Sample Application</h1><h2 id="b038" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk">Preparation</h2><p id="b489" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Head over to <a class="af pr" href="https://github.com/yip-kl/llm_function_calling_demo" rel="noopener ugc nofollow" target="_blank">Github</a> to clone my code. The contents below can be found in the <code class="cx qz ra rb rc b">function_calling_demo</code> Notebook.</p><p id="ac46" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Please also create and activate a virtual environment, then <code class="cx qz ra rb rc b">pip install -r requirements.txt</code> to install the required packages</p><h2 id="3815" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk"><strong class="al">Initialization</strong></h2><p id="4d10" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We first connect to OpenRouter. Alternatively using the original <code class="cx qz ra rb rc b">OpenAIChatGenerator</code> without overwritting the <code class="cx qz ra rb rc b">api_base_url</code>would also work, provided you have an OpenAI API key</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="68be" class="rl ng fq rc b bg rm rn l ro rp">import os<br/>from dotenv import load_dotenv<br/>from haystack.components.generators.chat import OpenAIChatGenerator<br/>from haystack.utils import Secret<br/>from haystack.dataclasses import ChatMessage<br/>from haystack.components.generators.utils import print_streaming_chunk<br/><br/># Set your API key as environment variable before executing this<br/>load_dotenv()<br/>OPENROUTER_API_KEY = os.environ.get('OPENROUTER_API_KEY')<br/><br/>chat_generator = OpenAIChatGenerator(api_key=Secret.from_env_var("OPENROUTER_API_KEY"),<br/>  api_base_url="https://openrouter.ai/api/v1",<br/>  model="openai/gpt-4-turbo-preview",<br/>        streaming_callback=print_streaming_chunk)</span></pre><p id="e445" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then we test can the <code class="cx qz ra rb rc b">chat_generator</code> be successfully invoked</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="e588" class="rl ng fq rc b bg rm rn l ro rp">chat_generator.run(messages=[ChatMessage.from_user("Return this text: 'test'")])</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="5242" class="rl ng fq rc b bg rm rn l ro rp">---------- The response should look like this ----------<br/>{'replies': [ChatMessage(content="'test'", role=&lt;ChatRole.ASSISTANT: 'assistant'&gt;, name=None, meta={'model': 'openai/gpt-4-turbo-preview', 'index': 0, 'finish_reason': 'stop', 'usage': {}})]}</span></pre><h2 id="ddca" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk">Step 1: Establish data store</h2><p id="a7f9" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Here we establish connection between our application and the two data sources: <strong class="ml fr">Document store</strong> for unstructured texts, and <strong class="ml fr">application database</strong> via API</p><p id="652e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Index Documents with a Pipeline</strong></p><p id="5c88" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We provide sample texts in <code class="cx qz ra rb rc b">documents</code> for the model to perform Retrival Augmented Generation (RAG). The texts are turned into embeddings and stored in an in-memory document store</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="4000" class="rl ng fq rc b bg rm rn l ro rp">from haystack import Pipeline, Document<br/>from haystack.document_stores.in_memory import InMemoryDocumentStore<br/>from haystack.components.writers import DocumentWriter<br/>from haystack.components.embedders import SentenceTransformersDocumentEmbedder<br/><br/># Sample documents<br/>documents = [<br/>    Document(content="Coffee shop opens at 9am and closes at 5pm."),<br/>    Document(content="Gym room opens at 6am and closes at 10pm.")<br/>]<br/><br/># Create the document store<br/>document_store = InMemoryDocumentStore()<br/><br/># Create a pipeline to turn the texts into embeddings and store them in the document store<br/>indexing_pipeline = Pipeline()<br/>indexing_pipeline.add_component(<br/>    "doc_embedder", SentenceTransformersDocumentEmbedder(model="sentence-transformers/all-MiniLM-L6-v2")<br/>)<br/>indexing_pipeline.add_component("doc_writer", DocumentWriter(document_store=document_store))<br/><br/>indexing_pipeline.connect("doc_embedder.documents", "doc_writer.documents")<br/><br/>indexing_pipeline.run({"doc_embedder": {"documents": documents}})</span></pre><p id="1b65" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It should output this, corresponding to the <code class="cx qz ra rb rc b">documents</code> we created as sample</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="e834" class="rl ng fq rc b bg rm rn l ro rp">{'doc_writer': {'documents_written': 2}}</span></pre><p id="1ca8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Spin up API server</strong></p><p id="03b1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An API server made with Flask is created under <code class="cx qz ra rb rc b">db_api.py</code> to connect to SQLite. Please spin it up by running <code class="cx qz ra rb rc b">python db_api.py</code> in your terminal</p><figure class="pv pw px py pz qa ps pt paragraph-image"><div role="button" tabindex="0" class="re rf ed rg bh rh"><div class="ps pt rr"><img src="../Images/7036bc280c989f445aeae16d0544b7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4Ty6XitWFrOHY5wWENaXg.png"/></div></div><figcaption class="qc qd qe ps pt qf qg bf b bg z dx">This would be shown in the terminal, if successfully executed</figcaption></figure><p id="bbf8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also notice that some initial data has been added in <code class="cx qz ra rb rc b">db_api.py</code></p><figure class="pv pw px py pz qa ps pt paragraph-image"><div class="ps pt rs"><img src="../Images/9adc825085a9c02489dc32ff3bf6fad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*tASIXxNUJThtf0-HuPtISw.png"/></div><figcaption class="qc qd qe ps pt qf qg bf b bg z dx">Sample data in the database</figcaption></figure><h2 id="4a2d" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk">Step 2: Define the functions</h2><p id="650a" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Here we prepare the actual functions for the model to invoke <strong class="ml fr">AFTER </strong>Function Calling (Step 4‚Äì5 as described in <strong class="ml fr">The Structure of Function Calling</strong>)</p><p id="251f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">RAG function</strong></p><p id="b6cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Namely the <code class="cx qz ra rb rc b">rag_pipeline_func</code>. This is for the model to provide an answer by searching through the texts stored in the Document Store. We first define the RAG retrieval as a Haystack pipeline</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="3ac4" class="rl ng fq rc b bg rm rn l ro rp">from haystack.components.embedders import SentenceTransformersTextEmbedder<br/>from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever<br/>from haystack.components.builders import PromptBuilder<br/>from haystack.components.generators import OpenAIGenerator<br/><br/>template = """<br/>Answer the questions based on the given context.<br/><br/>Context:<br/>{% for document in documents %}<br/>    {{ document.content }}<br/>{% endfor %}<br/>Question: {{ question }}<br/>Answer:<br/>"""<br/>rag_pipe = Pipeline()<br/>rag_pipe.add_component("embedder", SentenceTransformersTextEmbedder(model="sentence-transformers/all-MiniLM-L6-v2"))<br/>rag_pipe.add_component("retriever", InMemoryEmbeddingRetriever(document_store=document_store))<br/>rag_pipe.add_component("prompt_builder", PromptBuilder(template=template))<br/># Note to llm: We are using OpenAIGenerator, not the OpenAIChatGenerator, because the latter only accepts List[str] as input and cannot accept prompt_builder's str output<br/>rag_pipe.add_component("llm", OpenAIGenerator(api_key=Secret.from_env_var("OPENROUTER_API_KEY"),<br/>  api_base_url="https://openrouter.ai/api/v1",<br/>  model="openai/gpt-4-turbo-preview"))<br/><br/>rag_pipe.connect("embedder.embedding", "retriever.query_embedding")<br/>rag_pipe.connect("retriever", "prompt_builder.documents")<br/>rag_pipe.connect("prompt_builder", "llm")</span></pre><p id="e99b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Test if the function works</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="322a" class="rl ng fq rc b bg rm rn l ro rp">query = ‚ÄúWhen does the coffee shop open?‚Äù<br/>rag_pipe.run({"embedder": {"text": query}, "prompt_builder": {"question": query}})</span></pre><p id="fc0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This should yield the following output. Notice the <code class="cx qz ra rb rc b">replies</code> that the model gave is from the sample documents we provided before</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="c51e" class="rl ng fq rc b bg rm rn l ro rp">{'llm': {'replies': ['The coffee shop opens at 9am.'],<br/>  'meta': [{'model': 'openai/gpt-4-turbo-preview',<br/>    'index': 0,<br/>    'finish_reason': 'stop',<br/>    'usage': {'completion_tokens': 9,<br/>     'prompt_tokens': 60,<br/>     'total_tokens': 69,<br/>     'total_cost': 0.00087}}]}}</span></pre><p id="26d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can then turn the <code class="cx qz ra rb rc b">rag_pipe</code> into a function, which provides the <code class="cx qz ra rb rc b">replies</code> only without adding in the other details</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="2648" class="rl ng fq rc b bg rm rn l ro rp">def rag_pipeline_func(query: str):<br/>    result = rag_pipe.run({"embedder": {"text": query}, "prompt_builder": {"question": query}})<br/><br/>    return {"reply": result["llm"]["replies"][0]}</span></pre><p id="1687" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">API calls</strong></p><p id="5368" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We define the <code class="cx qz ra rb rc b">get_items</code> and <code class="cx qz ra rb rc b">purchase_item</code>functions for interacting with the database</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="93e5" class="rl ng fq rc b bg rm rn l ro rp"># Flask's default local URL, change it if necessary<br/>db_base_url = 'http://127.0.0.1:5000'<br/><br/># Use requests to get the data from the database<br/>import requests<br/>import json<br/><br/># get_categories is supplied as part of the prompt, it is not used as a tool<br/>def get_categories():<br/>    response = requests.get(f'{db_base_url}/category')<br/>    data = response.json()<br/>    return data<br/><br/>def get_items(ids=None,categories=None):<br/>    params = {<br/>        'id': ids,<br/>        'category': categories,<br/>    }<br/>    response = requests.get(f'{db_base_url}/item', params=params)<br/>    data = response.json()<br/>    return data<br/><br/>def purchase_item(id,quantity):<br/><br/>    headers = {<br/>    'Content-type':'application/json', <br/>    'Accept':'application/json'<br/>    }<br/><br/>    data = {<br/>        'id': id,<br/>        'quantity': quantity,<br/>    }<br/>    response = requests.post(f'{db_base_url}/item/purchase', json=data, headers=headers)<br/>    return response.json()</span></pre><p id="ec17" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Define the tool list</strong></p><p id="b394" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we have defined the fuctions, we need to let the model recognize those functions, and to instruct them how they are used, by providing descriptions for them.</p><p id="e887" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since we are using OpenAI here, the <code class="cx qz ra rb rc b">tools</code> is formatted as below following the <a class="af pr" href="https://cookbook.openai.com/examples/function_calling_with_an_openapi_spec" rel="noopener ugc nofollow" target="_blank">format required by Open AI</a></p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="e9c4" class="rl ng fq rc b bg rm rn l ro rp">tools = [<br/>    {<br/>        "type": "function",<br/>        "function": {<br/>            "name": "get_items",<br/>            "description": "Get a list of items from the database",<br/>            "parameters": {<br/>                "type": "object",<br/>                "properties": {<br/>                    "ids": {<br/>                        "type": "string",<br/>                        "description": "Comma separated list of item ids to fetch",<br/>                    },<br/>                    "categories": {<br/>                        "type": "string",<br/>                        "description": "Comma separated list of item categories to fetch",<br/>                    },<br/>                },<br/>                "required": [],<br/>            },<br/>        }<br/>    },<br/>    {<br/>        "type": "function",<br/>        "function": {<br/>            "name": "purchase_item",<br/>            "description": "Purchase a particular item",<br/>            "parameters": {<br/>                "type": "object",<br/>                "properties": {<br/>                    "id": {<br/>                        "type": "string",<br/>                        "description": "The given product ID, product name is not accepted here. Please obtain the product ID from the database first.",<br/>                    },<br/>                    "quantity": {<br/>                        "type": "integer",<br/>                        "description": "Number of items to purchase",<br/>                    },<br/>                },<br/>                "required": [],<br/>            },<br/>        }<br/>    },<br/>    {<br/>        "type": "function",<br/>        "function": {<br/>            "name": "rag_pipeline_func",<br/>            "description": "Get information from hotel brochure",<br/>            "parameters": {<br/>                "type": "object",<br/>                "properties": {<br/>                    "query": {<br/>                        "type": "string",<br/>                        "description": "The query to use in the search. Infer this from the user's message. It should be a question or a statement",<br/>                    }<br/>                },<br/>                "required": ["query"],<br/>            },<br/>        },<br/>    }<br/>]</span></pre><h2 id="d568" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk"><strong class="al">Step 3: Putting it all together</strong></h2><p id="88ef" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We now have the necessary inputs to test Function Calling! Here we do a few things:</p><ol class=""><li id="566e" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oh oi bk">Provide the initial prompt to the model, to give it some context</li><li id="4a0c" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Provide a sample user-generated message</li><li id="dccd" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Most importantly, we pass the tool list to the chat generator in <code class="cx qz ra rb rc b">tools</code></li></ol><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="5d19" class="rl ng fq rc b bg rm rn l ro rp"># 1. Initial prompt<br/>context = f"""You are an assistant to tourists visiting a hotel.<br/>You have access to a database of items (which includes {get_categories()}) that tourists can buy, you also have access to the hotel's brochure.<br/>If the tourist's question cannot be answered from the database, you can refer to the brochure.<br/>If the tourist's question cannot be answered from the brochure, you can ask the tourist to ask the hotel staff.<br/>"""<br/>messages = [<br/>    ChatMessage.from_system(context),<br/>    # 2. Sample message from user<br/>    ChatMessage.from_user("Can I buy a coffee?"),<br/>    ]<br/><br/># 3. Passing the tools list and invoke the chat generator<br/>response = chat_generator.run(messages=messages, generation_kwargs= {"tools": tools})<br/>response</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="9cdb" class="rl ng fq rc b bg rm rn l ro rp">---------- Response ----------<br/>{'replies': [ChatMessage(content='[{"index": 0, "id": "call_AkTWoiJzx5uJSgKW0WAI1yBB", "function": {"arguments": "{\\"categories\\":\\"Food and beverages\\"}", "name": "get_items"}, "type": "function"}]', role=&lt;ChatRole.ASSISTANT: 'assistant'&gt;, name=None, meta={'model': 'openai/gpt-4-turbo-preview', 'index': 0, 'finish_reason': 'tool_calls', 'usage': {}})]}</span></pre><p id="5673" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now let‚Äôs inspect the response. Notice how the Function Calling returns both the function chosen by the model, and the arguments for invoking the chosen function.</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="7641" class="rl ng fq rc b bg rm rn l ro rp">function_call = json.loads(response["replies"][0].content)[0]<br/>function_name = function_call["function"]["name"]<br/>function_args = json.loads(function_call["function"]["arguments"])<br/>print("Function Name:", function_name)<br/>print("Function Arguments:", function_args)</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="433f" class="rl ng fq rc b bg rm rn l ro rp">---------- Response ----------<br/>Function Name: get_items<br/>Function Arguments: {‚Äòcategories‚Äô: ‚ÄòFood and beverages‚Äô}</span></pre><p id="0f82" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When presented with another question, the model will use another tool that is more relevant</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="5c75" class="rl ng fq rc b bg rm rn l ro rp"># Another question<br/>messages.append(ChatMessage.from_user("Where's the coffee shop?"))<br/><br/># Invoke the chat generator, and passing the tools list<br/>response = chat_generator.run(messages=messages, generation_kwargs= {"tools": tools})<br/>function_call = json.loads(response["replies"][0].content)[0]<br/>function_name = function_call["function"]["name"]<br/>function_args = json.loads(function_call["function"]["arguments"])<br/>print("Function Name:", function_name)<br/>print("Function Arguments:", function_args)</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="2916" class="rl ng fq rc b bg rm rn l ro rp">---------- Response ----------<br/>Function Name: rag_pipeline_func<br/>Function Arguments: {'query': "Where's the coffee shop?"}</span></pre><p id="7ebc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Again, notice that no actual function is invoked here, this is what we will do next!</p><p id="c02f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Calling the function</strong></p><p id="0e14" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can then feed the arguments into the chosen function</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="8a1c" class="rl ng fq rc b bg rm rn l ro rp">## Find the correspoding function and call it with the given arguments<br/>available_functions = {"get_items": get_items, "purchase_item": purchase_item,"rag_pipeline_func": rag_pipeline_func}<br/>function_to_call = available_functions[function_name]<br/>function_response = function_to_call(**function_args)<br/>print("Function Response:", function_response)</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="17bd" class="rl ng fq rc b bg rm rn l ro rp">---------- Response ----------<br/>Function Response: {'reply': 'The provided context does not specify a physical location for the coffee shop, only its operating hours. Therefore, I cannot determine where the coffee shop is located based on the given information.'}</span></pre><p id="1993" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The response from <code class="cx qz ra rb rc b">rag_pipeline_func</code> can then passed as a context to the chat by appending it under the <code class="cx qz ra rb rc b">messages</code>, for the model to provide the final answer</p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="f197" class="rl ng fq rc b bg rm rn l ro rp">messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))<br/>response = chat_generator.run(messages=messages)<br/>response_msg = response["replies"][0]<br/><br/>print(response_msg.content)</span></pre><pre class="rq ri rc rj bp rk bb bk"><span id="24ad" class="rl ng fq rc b bg rm rn l ro rp">---------- Response ----------<br/>For the location of the coffee shop within the hotel, I recommend asking the hotel staff directly. They will be able to guide you to it accurately.</span></pre><p id="6e2b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We now have completed the chat cycle!</p><h2 id="3cdb" class="qi ng fq bf nh qj qk ql nk qm qn qo nn ms qp qq qr mw qs qt qu na qv qw qx qy bk">Step 4: Turn into an interactive chat</h2><p id="47ce" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">The code above shows how Function Calling can be done, but we want to go a step further by turning it into an interactive chat</p><p id="5d7f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here I showcase two methods to do it, from the more primitive <code class="cx qz ra rb rc b">input()</code> that prints the dialogue into the notebook itself, to rendering it through <strong class="ml fr">Streamlit </strong>to provide it with an ChatGPT-like UI</p><p id="8b97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><code class="cx qz ra rb rc b"><strong class="ml fr">input()</strong></code><strong class="ml fr"> loop</strong></p><p id="6193" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The code is copied from <a class="af pr" href="https://haystack.deepset.ai/tutorials/40_building_chat_application_with_function_calling" rel="noopener ugc nofollow" target="_blank">Haystack‚Äôs tutorial</a>, which allows us to quickly test the model. <em class="ow">Note: This application is created to demonstrate the idea of Function Calling, and is NOT meant to be perfectly robust e.g. supporting the order of multiple items at the same time, no hallucination, etc.</em></p><pre class="pv pw px py pz ri rc rj bp rk bb bk"><span id="b5c8" class="rl ng fq rc b bg rm rn l ro rp">import json<br/>from haystack.dataclasses import ChatMessage, ChatRole<br/><br/>response = None<br/>messages = [<br/>    ChatMessage.from_system(context)<br/>]<br/><br/>while True:<br/>    # if OpenAI response is a tool call<br/>    if response and response["replies"][0].meta["finish_reason"] == "tool_calls":<br/>        function_calls = json.loads(response["replies"][0].content)<br/><br/>        for function_call in function_calls:<br/>            ## Parse function calling information<br/>            function_name = function_call["function"]["name"]<br/>            function_args = json.loads(function_call["function"]["arguments"])<br/><br/>            ## Find the correspoding function and call it with the given arguments<br/>            function_to_call = available_functions[function_name]<br/>            function_response = function_to_call(**function_args)<br/><br/>            ## Append function response to the messages list using `ChatMessage.from_function`<br/>            messages.append(ChatMessage.from_function(content=json.dumps(function_response), name=function_name))<br/><br/>    # Regular Conversation<br/>    else:<br/>        # Append assistant messages to the messages list<br/>        if not messages[-1].is_from(ChatRole.SYSTEM):<br/>            messages.append(response["replies"][0])<br/><br/>        user_input = input("ENTER YOUR MESSAGE üëá INFO: Type 'exit' or 'quit' to stop\n")<br/>        if user_input.lower() == "exit" or user_input.lower() == "quit":<br/>            break<br/>        else:<br/>            messages.append(ChatMessage.from_user(user_input))<br/><br/>    response = chat_generator.run(messages=messages, generation_kwargs={"tools": tools})</span></pre><figure class="pv pw px py pz qa ps pt paragraph-image"><div role="button" tabindex="0" class="re rf ed rg bh rh"><div class="ps pt rt"><img src="../Images/a3322060f0f01905204135a0cec63711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5nwZCw3JO_553WiBEGaDg.png"/></div></div><figcaption class="qc qd qe ps pt qf qg bf b bg z dx">Running interactive chats in the IDE</figcaption></figure><p id="5ce7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While it works, we might want to have something that looks nicer.</p><p id="c09f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Streamlit interface</strong></p><p id="be13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Streamlit turns data scripts into shareable web apps, which provides a neat UI for our application. The code shown above are adapted into a Streamlit application under the <code class="cx qz ra rb rc b">streamlit</code> folder of my repo</p><p id="1674" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can run it by:</p><ol class=""><li id="30f9" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qh oh oi bk">If you have not done so already, spin up the API server with <code class="cx qz ra rb rc b">python db_api.py</code></li><li id="2ed9" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Set the OPENROUTER_API_KEY as environment variable e.g. <code class="cx qz ra rb rc b">export OPENROUTER_API_KEY = ‚Äò@REPLACE WITH YOUR API KEY‚Äô</code> assuming you are on Linux / executing with git bash</li><li id="dc57" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Navigate to the <code class="cx qz ra rb rc b">streamlit</code> folder in the terminal with <code class="cx qz ra rb rc b">cd streamlit</code></li><li id="c11f" class="mj mk fq ml b go oj mn mo gr ok mq mr ms ol mu mv mw om my mz na on nc nd ne qh oh oi bk">Run Streamlit with <code class="cx qz ra rb rc b">streamlit run app.py</code>. A new tab should be automatically created in your browser running the application</li></ol><p id="b446" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That‚Äôs basically it! I hope you enjoy this article.</p><figure class="pv pw px py pz qa ps pt paragraph-image"><div role="button" tabindex="0" class="re rf ed rg bh rh"><div class="ps pt ru"><img src="../Images/d47ce72177773e21a95ff14e3e8e5b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPp8ZNpthZi8ksU03vuCAg.png"/></div></div><figcaption class="qc qd qe ps pt qf qg bf b bg z dx">Streamlit UI</figcaption></figure><p id="ae4d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ow">*Unless otherwise noted, all images are by the author</em></p></div></div></div></div>    
</body>
</html>