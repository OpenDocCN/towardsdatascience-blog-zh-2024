<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Why Deep Learning Models Run Faster on GPUs: A Brief Introduction to CUDA Programming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Why Deep Learning Models Run Faster on GPUs: A Brief Introduction to CUDA Programming</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17">https://towardsdatascience.com/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66?source=collection_archive---------1-----------------------#2024-04-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="53f6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">For those who want to understand what .to(“cuda”) does</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lucas de Lima Nogueira" class="l ep by dd de cx" src="../Images/76edd8ee4005d4c0b8bd476261eb06ae.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6SXwrSnYs8I7IjjWUJsIQg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lucasdelimanogueira?source=post_page---byline--035272906d66--------------------------------" rel="noopener follow">Lucas de Lima Nogueira</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--035272906d66--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">20</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/38705e373a34d8ee180d497825e35868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFcP_hlayf1Ey_1hAGFCfQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author with the assistance of AI (<a class="af nc" href="https://copilot.microsoft.com/images/create" rel="noopener ugc nofollow" target="_blank">https://copilot.microsoft.com/images/create</a>)</figcaption></figure><p id="db95" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Nowadays, when we talk about deep learning, it is very common to associate its implementation with utilizing GPUs in order to improve performance.</p><p id="2c91" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GPUs (Graphical Processing Units) were originally designed to accelerate rendering of images, 2D, and 3D graphics. However, due to their capability of performing many parallel operations, their utility extends beyond that to applications such as deep learning.</p><p id="d9aa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The use of GPUs for deep learning models started around the mid to late 2000s and became very popular around 2012 with the emergence of AlexNet. AlexNet, a convolution neural network designed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. This victory marked a milestone as it demonstrated the effectiveness of deep neural networks for image classification and the use of GPUs for training large models.</p><p id="4aaa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Following this breakthrough, the use of GPUs for deep learning models became increasingly popular, which contributed to the creation of frameworks like PyTorch and TensorFlow.</p><p id="3519" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Nowadays, we just write <code class="cx nz oa ob oc b">.to("cuda")</code> in PyTorch to send data to GPU and expect the training to be accelerated. But how does deep learning algorithms take advantage of GPUs computation performance in practice? Let’s find out!</p><p id="203e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Deep learning architectures like neural networks, CNNs, RNNs and transformers are basically constructed using mathematical operations such as matrix addition, matrix multiplication and applying a function a matrix. Thus, if we find a way to optimize these operations, we can improve the performance of the deep learning models.</p><p id="8eef" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, let’s start simple. Imagine you want to add two vectors <em class="od">C = A + B</em>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk oe"><img src="../Images/2e831e31e1d8d8b0315146a0d898fab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*IpqBF_t1D3Om6RzyMqY80A.png"/></div></figure><p id="3dc0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A simple implementation of this in C would be:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="ab9c" class="oi oj fq oc b bg ok ol l om on">void AddTwoVectors(flaot A[], float B[], float C[]) {<br/>    for (int i = 0; i &lt; N; i++) {<br/>        C[i] = A[i] + B[i];<br/>    }<br/>}</span></pre><p id="4544" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can notice, the computer must iterate over the vector, adding each pair of elements on each iteration sequentially. But these operations are independent of each other. The addition of the <em class="od">ith </em>pair of elements does not rely on any other pair. So, what if we could execute these operations concurrently, adding all of the pairs of elements in parallel?</p><p id="9ed8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A straightforward approach would be using CPU multithreading in order to run all of the computation in parallel. However, when it comes to deep learning models, we are dealing with massive vectors, with millions of elements. A common CPU can only handle around a dozen threads simultaneously. That’s when the GPUs come into action! Modern GPUs can run millions of threads simultaneously, enhancing performance of these mathematical operations on massive vectors.</p><h1 id="2836" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">GPU vs. CPU comparison</h1><p id="ca6a" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">Although CPU computations can be faster than GPU for a single operation, the advantage of GPUs relies on its parallelization capabilities. The reason for this is that they are designed with different goals. While CPU is designed to execute a sequence of operations (thread) as fast as possible (and can only execute dozens of them simultaneously), the GPU is designed to execute millions of them in parallel (while sacrificing speed of individual threads).</p><p id="252a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">See the video below:</p><figure class="mm mn mo mp mq mr"><div class="po io l ed"><div class="pp pq l"/></div></figure><p id="7e38" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To illustrate, imagine that a CPU is like a Ferrari, and the GPU as a bus. If your task is to move one person, the Ferrari (CPU) is the better choice. However, if you are moving several people, even though the Ferrari (CPU) is faster per trip, the bus (GPU) can transport everyone in one go, transporting all people at once faster than the Ferrari traveling the route several times. So CPUs are better designed for handling sequential operations and GPUs for parallel operations.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/a1a871c965e587ba2104a0c62b78a33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*tJCZS-1Yfmt9raUZ1sZnBg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author with the assistance of AI (<a class="af nc" href="https://copilot.microsoft.com/images/create" rel="noopener ugc nofollow" target="_blank">https://copilot.microsoft.com/images/create</a>)</figcaption></figure><p id="3ec8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to provide higher parallel capabilities, GPU designs allocate more transistors for data processing than to data caching and flow control, unlike CPUs which allocate a significant portion of transistors for that purpose, in order to optimize single-threaded performance and complex instruction execution.</p><p id="443c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The figure below illustrates the distribution of chip resources for CPU vs GPU.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/b2bc61a6cffc46a6ab7e554fa5f3daf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0AWYVfRhZLeqzEUem1-1cg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author with inspiration from <a class="af nc" href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf" rel="noopener ugc nofollow" target="_blank">CUDA C++ Programming Guide</a></figcaption></figure><p id="1df3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">CPUs have powerful cores and a more complex cache memory architecture (allocating a significant amount of transistors for that). This design enables faster handling of sequential operations. On the other hand, GPUs prioritize having a large number of cores to achieve a higher level of parallelism.</p><p id="be05" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that we understood these basic concepts, how can we take advantage of this parallel computation capabilities in practice?</p><h1 id="6cf2" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Introduction to CUDA</h1><p id="aed5" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">When you are running some deep learning model, probably your choice is to use some popular Python library such as PyTorch or TensorFlow. However, it is well-known that the core of these libraries run C/C++ code underneath. Also, as we mentioned before, you might use GPUs to speed up processing. That’s where CUDA comes in! CUDA stands for <em class="od">Compute Unified Architecture </em>and it is a platform developed by NVIDIA for general-purpose processing on their GPUs. Thus, while DirectX is used by game engines to handle graphical computation, CUDA enables developers to integrate NVIDIA’s GPU computational power into their general-purpose software applications, extending beyond just graphics rendering.</p><p id="9726" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to implement that, CUDA provides a simple C/C++ based interface (CUDA C/C++) that grants access to the GPU’s virtual intruction set and specific operations (such as moving data between CPU and GPU).</p><p id="9e1e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Before we go further, let’s understand some basic CUDA Programming concepts and terminology:</p><ul class=""><li id="e69d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pt pu pv bk"><em class="od">host</em>: refers to the CPU and its memory;</li><li id="662d" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk"><em class="od">device</em>: refers to the GPU and its memory;</li><li id="f00a" class="nd ne fq nf b go pw nh ni gr px nk nl nm py no np nq pz ns nt nu qa nw nx ny pt pu pv bk"><em class="od">kernel</em>: refers to a function that is executed on the device (GPU);</li></ul><p id="004d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, in a basic code written using CUDA, the program runs on the <em class="od">host </em>(CPU)<em class="od">, </em>sends data to the <em class="od">device </em>(GPU) and launches <em class="od">kernels</em> (functions) to be executed on the <em class="od">device</em> (GPU)<em class="od">. </em>These kernels are executed by several threads in parallel. After the execution, the results are transfered back from the <em class="od">device </em>(<em class="od">GP</em>U) to the <em class="od">host</em> (CPU).</p><p id="dad9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So let’s go back to our problem of adding two vectors:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="91a3" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>void AddTwoVectors(flaot A[], float B[], float C[]) {<br/>    for (int i = 0; i &lt; N; i++) {<br/>        C[i] = A[i] + B[i];<br/>    }<br/>}<br/><br/>int main() {<br/>    ...<br/>    AddTwoVectors(A, B, C);<br/>    ...<br/>}</span></pre><p id="331e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In CUDA C/C++, the programmers can define C/C++ functions, called <em class="od">kernels</em>, that when called, are executed N times in parallel by N different CUDA threads.</p><p id="af08" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To define a kernel, you can use a <code class="cx nz oa ob oc b">__global__</code> declaration specifier, and the number of CUDA threads that execute this kernel can be specified using <code class="cx nz oa ob oc b">&lt;&lt;&lt;...&gt;&gt;&gt;</code> notation:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="d597" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoVectors(float A[], float B[], float C[]) {<br/>    int i = threadIdx.x;<br/>    C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/>    ...<br/>    // Kernel invocation with N threads<br/>    AddTwoVectors&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);<br/>    ...<br/>}</span></pre><p id="d1af" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Each thread executes the kernel and is given a unique thread ID <code class="cx nz oa ob oc b">threadIdx</code> accessible within the kernel through built-in variables. The code above adds two vectors A and B, of size N and stores the result into vector C. As you can notice, instead of a loop to execute each pair-wise addition sequentially, CUDA allows us to perform all of these operations simultaneously, using N threads in parallel.</p><p id="6587" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But before we can run this code, we need to do another modification. It is important to remember that the <em class="od">kernel</em> function runs within the device (GPU). So all of its data needs to be stored in the device memory. You can do this by using the following CUDA built-in functions:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="4327" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoVectors(float A[], float B[], float C[]) {<br/>    int i = threadIdx.x;<br/>    C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/><br/>    int N = 1000; // Size of the vectors<br/>    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C<br/><br/>    ...<br/><br/>    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C<br/><br/>    // Allocate memory on the device for vectors A, B, and C<br/>    cudaMalloc((void **)&amp;d_A, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_B, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_C, N * sizeof(float));<br/><br/>    // Copy vectors A and B from host to device<br/>    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);<br/>    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    // Kernel invocation with N threads<br/>    AddTwoVectors&lt;&lt;&lt;1, N&gt;&gt;&gt;(d_A, d_B, d_C);<br/>    <br/>    // Copy vector C from device to host<br/>    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>}</span></pre><p id="d2b0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Instead of directly passing variables A, B and C to the <em class="od">kernel</em>, we need to use pointers. In CUDA programming, you can’t directly use <em class="od">host</em> arrays (like <code class="cx nz oa ob oc b">A</code>, <code class="cx nz oa ob oc b">B</code>, and <code class="cx nz oa ob oc b">C</code> in the example) within kernel launches (<code class="cx nz oa ob oc b">&lt;&lt;&lt;...&gt;&gt;&gt;</code>). CUDA kernels operate on device memory, so you need to pass device pointers (<code class="cx nz oa ob oc b">d_A</code>, <code class="cx nz oa ob oc b">d_B</code>, and <code class="cx nz oa ob oc b">d_C</code>) to the kernel for it to operate on.</p><p id="46d7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Beyond that, we need to allocate memory on the <em class="od">device </em>by using <code class="cx nz oa ob oc b">cudaMalloc</code>, and copy data between <em class="od">host </em>and <em class="od">device </em>using <code class="cx nz oa ob oc b">cudaMemcpy.</code></p><p id="32d7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now we can add the initialization of vectors A and B, and refresh cuda memory at the end of the code.</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="7d34" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoVectors(float A[], float B[], float C[]) {<br/>    int i = threadIdx.x;<br/>    C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/>    <br/>    int N = 1000; // Size of the vectors<br/>    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C<br/><br/>    // Initialize vectors A and B<br/>    for (int i = 0; i &lt; N; ++i) {<br/>        A[i] = 1;<br/>        B[i] = 3;<br/>    }<br/><br/>    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C<br/><br/>    // Allocate memory on the device for vectors A, B, and C<br/>    cudaMalloc((void **)&amp;d_A, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_B, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_C, N * sizeof(float));<br/><br/>    // Copy vectors A and B from host to device<br/>    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);<br/>    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    // Kernel invocation with N threads<br/>    AddTwoVectors&lt;&lt;&lt;1, N&gt;&gt;&gt;(d_A, d_B, d_C);<br/>    <br/>    // Copy vector C from device to host<br/>    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>    // Free device memory<br/>    cudaFree(d_A);<br/>    cudaFree(d_B);<br/>    cudaFree(d_C);<br/>}</span></pre><p id="272a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Also, we need to add <code class="cx nz oa ob oc b">cudaDeviceSynchronize();</code> after we call the kernel. This is a function used to synchronize the host thread with the device. When this function is called, the host thread will wait until all previously issued CUDA commands on the device are completed before continuing execution.</p><p id="f84e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Beyond that, it is important to add some CUDA error checking so we can identify bugs on GPU. If we do not add this checking, the code will continues execution of the <em class="od">host </em>thread (CPU) and it will be difficult to identify CUDA related errors.</p><p id="e63d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The implementation of both techniques below:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="9c5a" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoVectors(float A[], float B[], float C[]) {<br/>    int i = threadIdx.x;<br/>    C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/>    <br/>    int N = 1000; // Size of the vectors<br/>    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C<br/><br/>    // Initialize vectors A and B<br/>    for (int i = 0; i &lt; N; ++i) {<br/>        A[i] = 1;<br/>        B[i] = 3;<br/>    }<br/><br/>    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C<br/><br/>    // Allocate memory on the device for vectors A, B, and C<br/>    cudaMalloc((void **)&amp;d_A, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_B, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_C, N * sizeof(float));<br/><br/>    // Copy vectors A and B from host to device<br/>    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);<br/>    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    // Kernel invocation with N threads<br/>    AddTwoVectors&lt;&lt;&lt;1, N&gt;&gt;&gt;(d_A, d_B, d_C);<br/><br/>    // Check for error<br/>    cudaError_t error = cudaGetLastError();<br/>    if(error != cudaSuccess) {<br/>        printf("CUDA error: %s\n", cudaGetErrorString(error));<br/>        exit(-1);<br/>    }<br/>    <br/>    // Waits untill all CUDA threads are executed<br/>    cudaDeviceSynchronize();<br/>    <br/>    // Copy vector C from device to host<br/>    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>    // Free device memory<br/>    cudaFree(d_A);<br/>    cudaFree(d_B);<br/>    cudaFree(d_C);<br/>}</span></pre><p id="f1ad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To compile and run the CUDA code, you’ll need to ensure that the CUDA toolkit is installed on your system. Then, you can compile the code using <code class="cx nz oa ob oc b">nvcc</code>, the NVIDIA CUDA Compiler. If you don’t have a GPU on your machine, you can use Google Colab. You just need to select a GPU on Runtime → Notebook settings, then save the code on a <code class="cx nz oa ob oc b">example.cu</code> file and run:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="ce41" class="oi oj fq oc b bg ok ol l om on">%%shell<br/>nvcc example.cu -o compiled_example # compile<br/>./compiled_example # run<br/><br/># you can also run the code with bug detection sanitizer<br/>compute-sanitizer --tool memcheck ./compiled_example </span></pre><p id="44e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, our code still is not fully optimized. The example above uses a vector of size N = 1000. But, this is a small number that will not fully demonstrate the GPU’s parallelization capability. Also, when dealing with deep learning problem, we often handle massive vectors with millions of parameters. However, if we try settings, for example, N = 500000 and run the kernel with <code class="cx nz oa ob oc b">&lt;&lt;&lt;1, 500000&gt;&gt;&gt;</code> using the example above, it will throw an error. Thus, to improve the code and perform such operation, we first need to understand an important concept of CUDA programming: Thread hierarchy.</p><h1 id="cb49" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Thread hierarchy</h1><p id="6fa9" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">The calling of kernel functions is done using the notation <code class="cx nz oa ob oc b">&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;</code>. So, in our example above, we run 1 block with N CUDA threads. However, each block has a limit on the number of threads it can support. This occurs because every thread within a block is required to be located on the same streaming multiprocessor core and must share the memory resources of that core.</p><p id="eb2a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can get this limit using the following snippet of code:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="f36d" class="oi oj fq oc b bg ok ol l om on">int device;<br/>cudaDeviceProp props;<br/>cudaGetDevice(&amp;device);<br/>cudaGetDeviceProperties(&amp;props, device);<br/>printf("Maximum threads per block: %d\n", props.maxThreadsPerBlock);</span></pre><p id="2b4d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On current Colab GPUs, a thread block may contain up to 1024 threads. Thus, we need more blocks to execute much more threads in order to process a massive vector in the example. Also, blocks are organized into grids, as illustrated below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/748e7f97d26b7d7fa6f8f8ed799b0c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9Z0tbcv4ShJmNUhlL79jA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://handwiki.org/wiki/index.php?curid=1157670" rel="noopener ugc nofollow" target="_blank">https://handwiki.org/wiki/index.php?curid=1157670</a> (<a class="af nc" href="https://creativecommons.org/licenses/by-sa/3.0/%7C" rel="noopener ugc nofollow" target="_blank">CC BY-SA 3.0)</a></figcaption></figure><p id="4aee" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, the thread ID can be accessed using:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="76d6" class="oi oj fq oc b bg ok ol l om on">int i = blockIdx.x * blockDim.x + threadIdx.x;</span></pre><p id="4940" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, our script becomes:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="956c" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoVectors(float A[], float B[], float C[], int N) {<br/>    int i = blockIdx.x * blockDim.x + threadIdx.x;<br/>    if (i &lt; N) // To avoid exceeding array limit<br/>        C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/>    int N = 500000; // Size of the vectors<br/>    int threads_per_block;<br/>    int device;<br/>    cudaDeviceProp props;<br/>    cudaGetDevice(&amp;device);<br/>    cudaGetDeviceProperties(&amp;props, device);<br/>    threads_per_block = props.maxThreadsPerBlock;<br/>    printf("Maximum threads per block: %d\n", threads_per_block); // 1024<br/><br/>    float A[N], B[N], C[N]; // Arrays for vectors A, B, and C<br/><br/>    // Initialize vectors A and B<br/>    for (int i = 0; i &lt; N; ++i) {<br/>        A[i] = 1;<br/>        B[i] = 3;<br/>    }<br/><br/>    float *d_A, *d_B, *d_C; // Device pointers for vectors A, B, and C<br/><br/>    // Allocate memory on the device for vectors A, B, and C<br/>    cudaMalloc((void **)&amp;d_A, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_B, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_C, N * sizeof(float));<br/><br/>    // Copy vectors A and B from host to device<br/>    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);<br/>    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    // Kernel invocation with multiple blocks and threads_per_block threads per block<br/>    int number_of_blocks = (N + threads_per_block - 1) / threads_per_block;<br/>    AddTwoVectors&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(d_A, d_B, d_C, N);<br/><br/>    // Check for error<br/>    cudaError_t error = cudaGetLastError();<br/>    if (error != cudaSuccess) {<br/>        printf("CUDA error: %s\n", cudaGetErrorString(error));<br/>        exit(-1);<br/>    }<br/><br/>    // Wait until all CUDA threads are executed<br/>    cudaDeviceSynchronize();<br/><br/>    // Copy vector C from device to host<br/>    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>    // Free device memory<br/>    cudaFree(d_A);<br/>    cudaFree(d_B);<br/>    cudaFree(d_C);<br/><br/>}</span></pre><h1 id="8c42" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Performance comparison</h1><p id="ff98" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">Below a comparison of CPU and GPU computation of this adding two vector operation for different vector sizes.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/b8e80e991576bd43865f4bd9783f4b52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*0JYdkEJNeb4mj1tuZHqW0Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="331a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As one can see, the advantage of GPU processing only becomes apparent with a large vector size N. Also, remember that this time comparison is only considering the execution of the kernel/function. It is not taking into account the time to copy data between <em class="od">host </em>and<em class="od"> device</em>, which although may not be significant on most cases, it is relatively considerable in our case as we are performing only a simple addition operation. Therefore, it is important to remember that GPU computation only demonstrates its advantage when dealing with highly compute-intensive computations that are also highly parallelized.</p><h1 id="ef27" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Multidimensional threads</h1><p id="9602" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">Okay, now we know how to increase performance of a simple array operation. But when dealing with deep learning models, we need to handle matrix and tensor operations. In our previous example, we only used one-dimensional blocks with N threads. However, it is also possible to execute multidimensional thread blocks (up to 3 dimensions). So, for convenience you can run a thread block of NxM threads if you need to run matrix operations. In this case, you could obtain the matrix rows columns indices as <code class="cx nz oa ob oc b">row = threadIdx.x, col = threadIdx.y</code>. Also, for convenience, you can use <code class="cx nz oa ob oc b">dim3</code> variable type to define the <code class="cx nz oa ob oc b">number_of_blocks</code> and <code class="cx nz oa ob oc b">threads_per_block.</code></p><p id="50a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The example below illustrates how to add two matrices.</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="8b12" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {<br/>    int i = threadIdx.x;<br/>    int j = threadIdx.y;<br/>    C[i][j] = A[i][j] + B[i][j];<br/>}<br/><br/>int main() {<br/>    ...<br/>    // Kernel invocation with 1 block of NxN threads<br/>    dim3 threads_per_block(N, N);<br/>    AddTwoMatrices&lt;&lt;&lt;1, threads_per_block&gt;&gt;&gt;(A, B, C);<br/>    ...<br/>}</span></pre><p id="ff55" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can also extend this example to handle multiple blocks:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="cd4b" class="oi oj fq oc b bg ok ol l om on">#include &lt;stdio.h&gt;<br/><br/>// Kernel definition<br/>__global__ void AddTwoMatrices(float A[N][N], float B[N][N], float C[N][N]) {<br/>    int i = blockIdx.x * blockDim.x + threadIdx.x;<br/>    int j = blockIdx.y * blockDim.y + threadIdx.y;<br/>    if (i &lt; N &amp;&amp; j &lt; N) {<br/>        C[i][j] = A[i][j] + B[i][j];<br/>    }<br/>}<br/><br/>int main() {<br/>    ...<br/>    // Kernel invocation with 1 block of NxN threads<br/>    dim3 threads_per_block(32, 32);<br/>    dim3 number_of_blocks((N + threads_per_block.x - 1) ∕ threads_per_block.x, (N + threads_per_block.y - 1) ∕ threads_per_block.y);<br/>    AddTwoMatrices&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;(A, B, C);<br/>    ...<br/>}</span></pre><p id="38c9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can also extend this example to process 3-dimensional operations using the same idea.</p><p id="1ec2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that you know how to operate multidimensional data, there is another important and simple concept to learn: how to call functions within a kernel. Basically this is simply done by using a <code class="cx nz oa ob oc b">__device__</code> declaration specifier. This defines functions that can be called by the <em class="od">device</em> (GPU) directly. Thus, they can only be called from <code class="cx nz oa ob oc b">__global__</code> or another <code class="cx nz oa ob oc b">__device__</code> function. The example below apply a sigmoid operation to a vector (very common operation on deep learning models).</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="2ad4" class="oi oj fq oc b bg ok ol l om on">#include &lt;math.h&gt;<br/><br/>// Sigmoid function<br/>__device__ float sigmoid(float x) {<br/>    return 1 / (1 + expf(-x));<br/>}<br/><br/>// Kernel definition for applying sigmoid function to a vector<br/>__global__ void sigmoidActivation(float input[], float output[]) {<br/>    int i = threadIdx.x;<br/>    output[i] = sigmoid(input[i]);<br/>   <br/>}</span></pre><p id="42e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, now that you know the basic important concepts of CUDA programming, you can start creating CUDA kernels. In the case of deep learning models, they are basically a bunch of matrix and tensor operations such as sum, multiplication, convolution, normalization and others. For instance, a naive matrix multiplication algorithm can be parallelized as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/eea3441283b27a0b357d8f148ba9f6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*RaK9avBnP3gBPRxSARUazQ.png"/></div></figure><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="a92d" class="oi oj fq oc b bg ok ol l om on">// GPU version<br/><br/>__global__ void matMul(float A[M][N], float B[N][P], float C[M][P]) {<br/>    int row = blockIdx.x * blockDim.x + threadIdx.x;<br/>    int col = blockIdx.y * blockDim.y + threadIdx.y;<br/><br/>    if (row &lt; M &amp;&amp; col &lt; P) {<br/>        float C_value = 0;<br/>        for (int i = 0; i &lt; N; i++) {<br/>            C_value += A[row][i] * B[i][col];<br/>        }<br/>        C[row][col] = C_value;<br/>    }<br/>}</span></pre><p id="5f17" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now compare this with a normal CPU implementation of two matrices multiplication below:</p><pre class="mm mn mo mp mq of oc og bp oh bb bk"><span id="63a5" class="oi oj fq oc b bg ok ol l om on">// CPU version<br/><br/>void matMul(float A[M][N], float B[N][P], float C[M][P]) {<br/>    for (int row = 0; row &lt; M; row++) {<br/>        for (int col = 0; col &lt; P; col++) {<br/>            float C_value = 0;<br/>            for (int i = 0; i &lt; N; i++) {<br/>                C_value += A[row][i] * B[i][col];<br/>            }<br/>            C[row][col] = C_value;<br/>        }<br/>    }<br/>}</span></pre><p id="2234" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can notice that on the GPU version we have less loops, resulting in a faster processing of the operation. Below is a comparison of performance between CPU and GPU of NxN matrix multiplications:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/fa9aef35ede725268ad91f948b4e9ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*kijnrDMdVQPihGWzBlEd2Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="8c9b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you may observe, the performance improvement of GPU processing is even higher for matrix multiplication operations as the matrix size increases.</p><p id="8b22" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, consider a basic neural network, which mostly involves <strong class="nf fr">y</strong> = σ(W<strong class="nf fr">x</strong> + <strong class="nf fr">b</strong>) operations, as shown below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/568620fec12aad34ce87ccd1c37caaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5RqCcEDYlxEfqFo6PVnkw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author</figcaption></figure><p id="2e2e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These operations primarily comprise matrix multiplication, matrix addition, and applying a function to an array, all of which you’re already familiar with the parallelization techniques. Thus, you are now capable of implementing your own neural network that runs on GPUs from scratch!</p><h1 id="bc74" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Conclusion</h1><p id="4521" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk">In this post we covered introductory concepts regarding GPU processing to enhance deep learning models performance. However, it is also important to mention that the concepts you have seen are only the basics and there is a lot more to be learned. Libraries like PyTorch and Tensorflow implement optimization techniques that involves other more complex concepts such as optimized memory access, batched operations and others (they harness libraries built on top of CUDA, such as cuBLAS and cuDNN). However, I hope this post helps clear up what goes on behind the scenes when you write <code class="cx nz oa ob oc b">.to("cuda")</code> and execute deep learning models on GPUs.</p><p id="812c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In future posts, I will try to bring more complex concepts regarding CUDA Programming. Please let me know what you think or what you would like me to write about next in the comments! Thanks so much for reading! 😊</p><h1 id="0bc7" class="oo oj fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Further reading</h1><p id="0a4f" class="pw-post-body-paragraph nd ne fq nf b go pj nh ni gr pk nk nl nm pl no np nq pm ns nt nu pn nw nx ny fj bk"><a class="af nc" href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf" rel="noopener ugc nofollow" target="_blank">CUDA Programming Guide</a> — NVIDIA CUDA Programming documentation.</p><p id="de10" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://docs.nvidia.com/cuda/" rel="noopener ugc nofollow" target="_blank">CUDA Documentation</a> — NVIDIA complete CUDA documentation.</p><p id="ca5b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://luniak.io/cuda-neural-network-implementation-part-1/" rel="noopener ugc nofollow" target="_blank">CUDA Neural Network training implementation</a> — Pure CUDA C++ implementation of a neural network training.</p><p id="bcff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://github.com/karpathy/llm.c" rel="noopener ugc nofollow" target="_blank">CUDA LLM training implementation </a>— Training implementation of LLM with pure CUDA C.</p></div></div></div></div>    
</body>
</html>