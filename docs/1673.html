<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into LSTMs and xLSTMs by Hand ✍️</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into LSTMs and xLSTMs by Hand ✍️</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09">https://towardsdatascience.com/deep-dive-into-lstms-xlstms-by-hand-%EF%B8%8F-c33e638bebb1?source=collection_archive---------1-----------------------#2024-07-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ccd2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the wisdom of LSTM leading into xLSTMs — a probable competition to the present-day LLMs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--c33e638bebb1--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c33e638bebb1--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/49ffa3c9ea9d92e16c715ce00259c89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vc41bYSN1rgxCQI_vzrUMQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author (The ancient wizard as created by my 4-year old)</figcaption></figure><p id="08d9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">“In the enchanted realm of Serentia, where ancient forests whispered secrets of spells long forgotten, there dwelled the Enigmastrider — a venerable wizard, guardian of timeless wisdom.</em></p><p id="7a20" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">One pivotal day as Serentia faced dire peril, the Enigmastrider wove a mystical ritual using the Essence Stones, imbued with the essence of past, present, and future. Drawing upon ancient magic he conjured the LSTM, a conduit of knowledge capable of preserving Serentia’s history and foreseeing its destiny. Like a river of boundless wisdom, the LSTM flowed transcending the present and revealing what lay beyond the horizon.</em></p><p id="86f0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">From his secluded abode the Enigmastrider observed as Serentia was reborn, ascending to new heights. He knew that his arcane wisdom and tireless efforts had once again safeguarded a legacy in this magical realm.”</em></p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="731e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And with that story we begin our expedition to the depths of one of the most appealing Recurrent Neural Networks — the Long Short-Term Memory Networks, very popularly known as the LSTMs. Why do we revisit this classic? Because they may once again become useful as longer context-lengths in language modeling grow in importance.</p><h1 id="10d2" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Can LSTMs once again get an edge over LLMs?</h1><p id="2aea" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">A short while ago, researchers in Austria came up with a promising initiative to revive the lost glory of LSTMs — by giving way to the more evolved Extended Long-short Term Memory, also called xLSTM. It would not be wrong to say that before Transformers, LSTMs had worn the throne for innumerous deep-learning successes. Now the question stands, with their abilities maximized and drawbacks minimized, can they compete with the present-day LLMs?</p><p id="2715" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To learn the answer, let’s move back in time a bit and revise what LSTMs were and what made them so special:</p><p id="50a7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Long Short Term Memory Networks were first introduced in the year 1997 by <a class="af ph" href="https://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">Hochreiter and Schmidhuber</a> — to address the long-term dependency problem faced by RNNs. With around 106518 citations on the paper, it is no wonder that LSTMs are a classic.</p><p id="6667" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The key idea in an LSTM is the ability to learn when to remember and when to forget relevant information over arbitrary time intervals. Just like us humans. Rather than starting every idea from scratch — we rely on much older information and are able to very aptly connect the dots. Of course, when talking about LSTMs, the question arises — don’t RNNs do the same thing?</p><p id="98e2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The short answer is yes, they do. However, there is a big difference. The RNN architecture does not support delving too much in the past — only up to the immediate past. And that is not very helpful.</p><p id="2241" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As an example, let’s consider these line John Keats wrote in ‘To Autumn’:</p><p id="6164" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">“Season of mists and mellow fruitfulness,</em></p><p id="b5a8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Close bosom-friend of the maturing sun;”</em></p><p id="d160" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As humans, we understand that words “mists” and “mellow fruitfulness” are conceptually related to the season of autumn, evoking ideas of a specific time of year. Similarly, LSTMs can capture this notion and use it to understand the context further when the words “maturing sun” comes in. Despite the separation between these words in the sequence, LSTM networks can learn to associate and keep the previous connections intact. And this is the big contrast when compared with the original Recurrent Neural Network framework.</p><p id="5346" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And the way LSTMs do it is with the help of a gating mechanism. If we consider the architecture of an RNN vs an LSTM, the difference is very evident. The RNN has a very simple architecture — the past state and present input pass through an activation function to output the next state. An LSTM block, on the other hand, adds three more gates on top of an RNN block: the input gate, the forget gate and output gate which together handle the past state along with the present input. This idea of gating is what makes all the difference.</p><p id="8e99" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To understand things further, let’s dive into the details with these incredible works on <a class="af ph" href="https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-deeplearning-activity-7206573043425447936-IYqt?utm_source=share&amp;utm_medium=member_desktop" rel="noopener ugc nofollow" target="_blank">LSTMs</a> and <a class="af ph" href="https://www.linkedin.com/posts/tom-yeh_lstm-aibyhand-activity-7194319329536978945-VP6W?utm_source=share&amp;utm_medium=member_desktop" rel="noopener ugc nofollow" target="_blank">xLSTMs</a> by the amazing <a class="af ph" href="https://www.linkedin.com/in/tom-yeh/" rel="noopener ugc nofollow" target="_blank">Prof. Tom Yeh</a>.</p><p id="432c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First, let’s understand the mathematical cogs and wheels behind LSTMs before exploring their newer version.</p><p id="3455" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">(All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned LinkedIn posts, which I have edited with his permission. )</p><p id="beb3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So, here we go:</p><h1 id="3643" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">How does an LSTM work?</h1><h2 id="c32a" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[1] Initialize</h2><p id="d7e1" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The first step begins with randomly assigning values to the previous hidden state h0 and memory cells C0. Keeping it in sync with the diagrams, we set</p><p id="cd5a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">h0 → [1,1]</p><p id="1dc3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">C0 → [0.3, -0.5]</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pz"><img src="../Images/8dfd7b032b37f204451c17d02584e40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*KlbGPjnx1IOITGecaCeXEw.gif"/></div></figure><h2 id="cfa3" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[2] Linear Transform</h2><p id="562b" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">In the next step, we perform a linear transform by multiplying the four weight matrices (<em class="nx">Wf</em>, <em class="nx">Wc</em>, <em class="nx">Wi</em> and <em class="nx">Wo</em>) with the concatenated current input X1 and the previous hidden state that we assigned in the previous step.</p><p id="2324" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The resultant values are called <strong class="nd fr">feature values</strong> obtained as the combination of the current input and the hidden state.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qa"><img src="../Images/6dccf0a8b5c7eba2afac97f64c89e09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*pLm8ED7FmAmXNNBVq4xNcA.gif"/></div></figure><h2 id="f9a9" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[3] Non-linear Transform</h2><p id="cee8" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">This step is crucial in the LSTM process. It is a non-linear transform with two parts — a <strong class="nd fr">sigmoid σ</strong> and <strong class="nd fr">tanh</strong>.</p><p id="21f1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The sigmoid is used to obtain gate values between 0 and 1. This layer essentially determines what information to retain and what to forget. The values always range between 0 and 1 — a ‘0’ implies completely eliminating the information whereas a ‘1’ implies keeping it in place.</p><ul class=""><li id="7b3d" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qb qc qd bk">Forget gate (f1): [-4, -6] → [0, 0]</li><li id="cdd5" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qb qc qd bk">Input gate (i1): [6, 4] → [1, 1]</li><li id="27c6" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qb qc qd bk">Output gate (o1): [4, -5] → [1, 0]</li></ul><p id="6347" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the next part, tanh is applied to obtain new candidate memory values that could be added on top of the previous information.</p><ul class=""><li id="22bf" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qb qc qd bk">Candidate memory (C’1): [1, -6] → [0.8, -1]</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qj"><img src="../Images/b76632ff806ae0df8b13d605cc6f0421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*8aUfcYdAR83BaGd82hQ2Fg.gif"/></div></figure><h2 id="6052" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[4] Update Memory</h2><p id="d826" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once the above values are obtained, it is time to update the current state using these values.</p><p id="9d80" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">The previous step made the decision on what needs to be done, in this step we implement that decision.</strong></p><p id="b33a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We do so in two parts:</p><ol class=""><li id="42d8" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk qc qd bk"><strong class="nd fr">Forget</strong> : Multiply the current memory values (C0) element-wise with the obtained forget-gate values. What it does is it updates in the current state the values that were decided could be forgotten. → C0 .* f1</li><li id="bec3" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk"><strong class="nd fr">Input</strong> : Multiply the updated memory values (C’1) element-wise with the input gate values to obtain ‘input-scaled’ the memory values. → C’1 .* i1</li></ol><p id="d8d2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, we add these two terms above to get the updated memory C1, i.e. <strong class="nd fr">C0 .* f1 + C’1 .* i1 = C1</strong></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ql"><img src="../Images/eea4538ecb24d07770a0f06e8e1fba75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*hw2zdJq1pmm0V3hIFKHeLg.gif"/></div></figure><h2 id="9796" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[5] Candidate Output</h2><p id="07da" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Finally, we make the decision on how the output is going to look like:</p><p id="9856" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To begin, we first apply tanh as before to the new memory C1 to obtain a candidate output o’1. This pushes the values between -1 and 1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qm"><img src="../Images/c234f20db05f4d7c603a7c44bdd4ce92.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*-oduqpzmNIAeBB2LE8p87g.gif"/></div></figure><h2 id="a135" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[6] Update Hidden State</h2><p id="f7a3" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">To get the final output, we multiply the candidate output o’1 obtained in the previous step with the sigmoid of the output gate o1 obtained in Step 3. The result obtained is the first output of the network and is the updated hidden state h1, i.e. <strong class="nd fr">o’1 * o1 = h1.</strong></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qn"><img src="../Images/b95c5a03295b502387b9d2a982d62e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*sg9tq1F5AGHVKd77gZeuMQ.gif"/></div></figure><h2 id="0f73" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">— — Process t = 2 — -</h2><p id="cbd2" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We continue with the subsequent iterations below:</p><h2 id="4076" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[7] Initialize</h2><p id="687a" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">First, we copy the updates from the previous steps i.e. updated hidden state h1 and memory C1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qo"><img src="../Images/142061e2ce495b94eaeaf251c8aae472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDF-s1gtgQpW0necxdy_7A.gif"/></div></div></figure><h2 id="682e" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[8] Linear Transform</h2><p id="9d38" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We repeat Step [2] which is element-wise weight and bias matrix multiplication.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qp"><img src="../Images/2ad7d59f612d08f21401728ca8af4702.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*brKjtTyUyovkO-lJAUxWVQ.gif"/></div></figure><h2 id="3da0" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[9] Update Memory (C2)</h2><p id="448d" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We repeat steps [3] and [4] which are the non-linear transforms using sigmoid and tanh layers, followed by the decision on forgetting the relevant parts and introducing new information — this gives us the updated memory C2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/30b79ad19fe3908c5867691cc98137b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*k25Jh7bpS8XUyWDbPlYAtA.gif"/></div></figure><h2 id="85f3" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[10] Update Hidden State (h2)</h2><p id="24fa" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Finally, we repeat steps [5] and [6] which adds up to give us the second hidden state h2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qr"><img src="../Images/e81ce3f9aea26b308f1da056116e47fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4sxl_wjLwJLzhu9WR0IQQ.gif"/></div></div></figure><h2 id="c63c" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">Next, we have the final iteration.</h2><h2 id="6ef6" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">— — Process t = 3 — -</h2><h2 id="37a2" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[11] Initialize</h2><p id="37bb" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once again we copy the hidden state and memory from the previous iteration i.e. h2 and C2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qs"><img src="../Images/9199f1c501b328aa1dbd243a632507a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*8dUvLTdf8B9eNH2FDcdqJA.gif"/></div></figure><h2 id="49c5" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[12] Linear Transform</h2><p id="15f8" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We perform the same linear-transform as we do in Step 2.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/91822b2527646253a536451432811b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h45RkJ5HYAc1YmEjwkDcBQ.gif"/></div></div></figure><h2 id="cb79" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[13] Update Memory (C3)</h2><p id="22cb" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Next, we perform the non-linear transforms and perform the memory updates based on the values obtained during the transform.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qu"><img src="../Images/dc328875f1265c3352b7b0d2c6054e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*4P2Laj47nJv_WlD8gVzFMg.gif"/></div></figure><h2 id="a6b9" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">[14] Update Hidden State (h3)</h2><p id="fd78" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once done, we use those values to obtain the final hidden state h3.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qv"><img src="../Images/528e2b7e2eb1ac3137fda08e5f46b73b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIFEpcP-XvMxlhJwVR1dgQ.gif"/></div></div></figure><h1 id="1ce0" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Summary:</h1><p id="2c02" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">To summarize the working above, the key thing to remember is that LSTM depends on three main gates : <strong class="nd fr">input, forget and output</strong>. And these gates as can be inferred from the names, control what part of the information and how much of it is relevant and which parts can be discarded.</p><p id="0d66" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Very briefly, the steps to do so are as follows:</p><ol class=""><li id="a801" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qk qc qd bk">Initialize the hidden state and memory values from the previous state.</li><li id="a2ee" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">Perform linear-transform to help the network start looking at the hidden state and memory values.</li><li id="94bc" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">Apply non-linear transform (sigmoid and tanh) to determine what values to retain /discard and to obtain new candidate memory values.</li><li id="f62c" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">Based on the decision (values obtained) in Step 3, we perform memory updates.</li><li id="e373" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">Next, we determine what the output is going to look like based on the memory update obtained in the previous step. We obtain a candidate output here.</li><li id="095b" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">We combine the candidate output with the gated output value obtained in Step 3 to finally reach the intermediate hidden state.</li></ol><p id="82dc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This loop continues for as many iterations as needed.</p><h1 id="45d0" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Extended Long-Short Term Memory (xLSTM)</h1><h2 id="4931" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">The need for xLSTMs</h2><p id="445d" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">When LSTMs emerged, they definitely set the platform for doing something that was not done previously. Recurrent Neural Networks could have memory but it was very limited and hence the birth of LSTM — to support long-term dependencies. However, it was not enough. Because analyzing inputs as sequences obstructed the use of parallel computation and moreover, led to drops in performance due to long dependencies.</p><p id="166c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thus, as a solution to it all were born the transformers. But the question still remained — can we once again use LSTMs by addressing their limitations to achieve what Transformers do? To answer that question, came the xLSTM architecture.</p><h2 id="0c67" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">How is xLSTM different from LSTM?</h2><p id="51ea" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">xLSTMs can be seen as a very evolved version of LSTMs. The underlying structure of LSTMs are preserved in xLSTM, however new elements have been introduced which help handle the drawbacks of the original form.</p><h2 id="c884" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">Exponential Gating &amp; Scalar Memory Mixing — sLSTM</h2><p id="897c" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The most crucial difference is the introduction of <strong class="nd fr">exponential gating</strong>. In LSTMs, when we perform Step [3], we induce a sigmoid gating to all gates, while for xLSTMs it has been replaced by exponential gating.</p><p id="364e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For eg: For the input gate i1-</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qw"><img src="../Images/04def2fa390328178528101562981194.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*4Ix5xhLuA_Hk-qSRMJDtRQ.png"/></div></figure><p id="96a6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">is now,</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qx"><img src="../Images/36ccf4af44d1978c697aabd5314a8320.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*BHZ6-ubarFu7-p81FnojCQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Images by author</figcaption></figure><p id="6879" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With a bigger range that exponential gating provides, xLSTMs are able to handle updates better as compared to the sigmoid function which compresses inputs to the range of (0, 1). There is a catch though — exponential values may grow up to be very large. To mitigate that problem, xLSTMs incorporate normalization and the logarithm function seen in the equations below plays an important role here.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qy"><img src="../Images/7a79124479a3fd526ba17241e252b6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TscBo55bjfxfDe-Q"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image from Reference [1]</figcaption></figure><p id="aa6d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, logarithm does reverse the effect of the exponential but their combined application, as the<a class="af ph" href="https://arxiv.org/abs/2405.04517" rel="noopener ugc nofollow" target="_blank"> xLSTM paper </a>claims, leads the way for balanced states.</p><p id="b731" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This exponential gating along with memory mixing among the different gates (as in the original LSTM) forms the <strong class="nd fr">sLSTM</strong> block.</p><h1 id="7a07" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Matrix Memory Cell — mLSTM</h1><p id="d3fa" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The other new aspect of the xLSTM architecture is the increase from a scalar memory to matrix memory which allows it to process more information in parallel. It also draws semblance to the transformer architecture by introducing the key, query and value vectors and using them in the normalizer state as the weighted sum of key vectors, where each key vector is weighted by the input and forget gates.</p><p id="bd5e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Once the sLSTM and mLSTM blocks are ready, they are stacked one over the other using residual connections to yield xLSTM blocks and finally the xLSTM architecture.</p><p id="6646" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thus, the introduction of exponential gating (with appropriate normalization) along with newer memory structures establish a strong pedestal for the xLSTMs to achieve results similar to the transformers.</p><h2 id="aac1" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk"><strong class="al">To summarize:</strong></h2><ol class=""><li id="757d" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw qk qc qd bk">An <strong class="nd fr">LSTM</strong> is a special Recurrent Neural Network (RNN) that allows connecting previous information to the current state just as us humans do with persistence of our thoughts. LSTMs became incredibly popular because of their ability to look far into the past rather than depending only on the immediate past. What made it possible was the introduction of special gating elements into the RNN architecture-</li></ol><ul class=""><li id="16cc" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qb qc qd bk"><strong class="nd fr">Forget Gate</strong>: Determines what information from the previous cell state should be kept or forgotten. By selectively forgetting irrelevant past information, the LSTM maintains long-term dependencies.</li><li id="40d4" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qb qc qd bk"><strong class="nd fr">Input Gate</strong> : Determines what new information should be stored in the cell state. By controlling how the cell state is updated, it incorporates new information important for predicting the current output.</li><li id="f2aa" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qb qc qd bk"><strong class="nd fr">Output Gate</strong> : Determines what information should be the output as the hidden state. By selectively exposing parts of the cell state as the output, the LSTM can provide relevant information to subsequent layers while suppressing the non-pertinent details and thus propagating only the important information over longer sequences.</li></ul><p id="c255" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">2. An <strong class="nd fr">xLSTM</strong> is an evolved version of the LSTM that addresses the drawbacks faced by the LSTM. It is true that LSTMs are capable of handling long-term dependencies, however the information is processed sequentially and thus doesn’t incorporate the power of parallelism that today’s transformers capitalize on. To address that, xLSTMs bring in:</p><ul class=""><li id="3797" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qb qc qd bk"><strong class="nd fr">sLSTM</strong> : Exponential gating that helps to include larger ranges as compared to sigmoid activation.</li><li id="2ba0" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qb qc qd bk"><strong class="nd fr">mLSTM</strong> : New memory structures with matrix memory to enhance memory capacity and enhance more efficient information retrieval.</li></ul><h1 id="7f5b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Will LSTMs make their comeback?</h1><p id="16a8" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">LSTMs overall are part of the Recurrent Neural Network family that process information in a sequential manner recursively. The advent of Transformers completely obliterated the application of recurrence however, their struggle to handle extremely long sequences still remains a burning problem. Research suggests that quadratic time is pertinent for long-ranges or long contexts.</p><p id="e180" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thus, it does seem worthwhile to explore options that could at least enlighten a solution path and a good starting point would be going back to LSTMs — in short, LSTMs have a good chance of making a comeback. The present xLSTM results definitely look promising. And then, to round it all up — the use of recurrence by <a class="af ph" href="https://arxiv.org/pdf/2312.00752" rel="noopener ugc nofollow" target="_blank">Mamba</a> stands as a good testimony that this could be a lucrative path to explore.</p><p id="d13c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So, let’s follow along in this journey and see it unfold while keeping in mind the power of recurrence!</p><p id="550c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">P.S. If you would like to work through this exercise on your own, here is a link to a blank template for your use.</em></p><p id="7338" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ph" href="https://drive.google.com/file/d/1zfj9TgKP52fOu75HLDs-cooHkhOZAbfU/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="4213" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now go have fun and create some Long Short-Term effect!</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/eefcdf7e690e6ecece1242a79f2acd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zf6XTb6jDQXVOt-N9S_YTg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="5cef" class="pi oh fq bf oi pj pk pl ol pm pn po oo nk pp pq pr no ps pt pu ns pv pw px py bk">References:</h2><ol class=""><li id="2ced" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw qk qc qd bk">xLSTM: Extended Long Short-Term Memory, Maximilian et al. May 2024 <a class="af ph" href="https://arxiv.org/abs/2405.04517" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2405.04517</a></li><li id="c0e2" class="nb nc fq nd b go qe nf ng gr qf ni nj nk qg nm nn no qh nq nr ns qi nu nv nw qk qc qd bk">Long Short-Term Memory, Sepp Hochreiter and Jürgen Schmidhuber, 1997, Neural Comput. 9, 8 (November 15, 1997), 1735–1780. <a class="af ph" href="https://doi.org/10.1162/neco.1997.9.8.1735" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1162/neco.1997.9.8.1735</a></li></ol></div></div></div></div>    
</body>
</html>