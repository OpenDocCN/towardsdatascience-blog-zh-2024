- en: Tool Use, Agents, and the Voyager Paper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01](https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A detailed exploration of the Voyager Paper and its findings on tool usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    ·8 min read·May 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9618fd2e69903d1de213327b57dab011.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author. Generated by DALL-E 2
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs continue to increase their reasoning ability, their capacity to plan
    and then act tends to increase. This has led to prompting templates where users
    give LLMs an end result they want and the LLM then will figure out how to accomplish
    it — even if it takes multiple actions to do so. This kind of prompting is often
    called an agent, and it has generated a lot of excitement.
  prefs: []
  type: TYPE_NORMAL
- en: For example, one could ask an agent to win a game and then watch it figure out
    a good strategy to do so. While typically we would use frameworks like reinforcement
    learning to train a model to win games like Super Mario Bros, when we look at
    games with more open-ended goals like Minecraft, an LLMs’ ability to reason can
    become critical.
  prefs: []
  type: TYPE_NORMAL
- en: The Voyager Paper focuses on how one can prompt an LLM so that it can complete
    open-ended and challenging tasks like playing Minecraft. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: High Level Discussion of Voyager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1fa71565ab5cc7f0134d68a7b2b194dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Voyager system here consists of three major pieces: an automatic curriculum,
    an iterative prompting mechanism, and a skill library. The curriculum you can
    imagine as the compass of the system, a way that the agent is able to determine
    what it should do in a given situation. As new situations arise, we have the iterative
    prompting mechanism to create new skills for new situations. Because LLMs have
    limited contexts but the curriculum could potentially create the need for limitless
    skills, we also have a skill library to store these skills for later use.'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic Curriculum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0ae1859d17dcd1ca0a65adbf8318bacc.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: The Automatic Curriculum is itself prompt engineering, where pertinent information
    about the AI’s immediate environment and long-term goals are passed to the LLM.
    The authors were kind enough to give the full system prompt in the paper, so I
    will highlight interesting parts of it below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0518cab72e1fed32499c027bfd0c3db9.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: First, the prompt explains the basic schema that will be passed in. Although
    the exact information is not filled in here, it appears like this is helping by
    priming the LLM to receive information in this schema. This is similar to few-shot
    reasoning, as later turns with this chatbot will use that format.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dd01acc8894117fb229ecdedd9dbfd4.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: Next, the prompt outlines a fairly precise way that the LLM should reason. Notice
    that this is still given in the second-person (you) and that the instructions
    are highly specific to Minecraft itself. In my opinion, improvements to the pieces
    outlined above would seem to give the greatest payoff. Moreover, notice how the
    steps themselves are not always exact like one might see in traditional programming,
    instead being consistent with the more ambiguous goal of exploration. This is
    where the promise of agents lies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the above section, Step one is especially interesting to me, as it could
    be read as a kind of “persona” prompting: telling the LLM it is a mentor and thus
    having it speak with more certainty in its replies. We have seen in the past that
    persona prompting can result in the LLM taking more decisive action, so this could
    be a way to ensure the agent will act and not get stuck in analysis paralysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2476e323d08baee12a3de02a52c5eea.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the prompt ends by again giving a few-shot reasoning piece illustrating
    how best to respond.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, we’ve used reinforcement machine learning models with specific
    inputs to discover optimal strategies for maximizing well-defined metrics (think
    getting the highest score in an arcade game). Today, the LLM is given a more ambiguous
    long-term goal and seen taking actions that would realize it. That we think the
    LLM is capable of approximating this type of goal signals a major change in expectations
    for ML agents.
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Prompting Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d28bf3090c68a4f8810cb39aecda38b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5 from [the paper](https://arxiv.org/pdf/2305.16291) showing environment
    & execution feedback
  prefs: []
  type: TYPE_NORMAL
- en: Here, the LLM will create code that executes certain actions in Minecraft. As
    these tend to be more complex series of actions, we call these skills.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating the skills that will go into the skill library, the authors had
    their LLM receive 3 distinct kinds of feedback during development: (1) execution
    errors, (2) environment feedback, and (3) peer-review from another LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Execution errors can occur when the LLM makes a mistake with the syntax of
    the code, the Mineflayer library, or some other item that is caught by the compiler
    or in run-time. Environment feedback comes from the Minecraft game itself. The
    authors use the *bot.chat()* feature within Mineflayer to get feedback such as
    *“I cannot make stone_shovel because I need: 2 more stick”*. This information
    is then passed into the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: While execution and environment feedback seems natural, the peer-review feedback
    may seem strange. After all, running two LLMs is more expensive than running only
    one. However, as the set of skills that can be created by the LLM is enormous,
    it would be very difficult to write code that verifies the skills actually do
    what they are supposed to do. To get around this, the authors have a separate
    LLM review the code and give feedback on if the task is accomplished. While this
    isn’t as perfect as verifying programmatically the job is finished, it is a good
    enough proxy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3acf4f23d2950bd4fafcb28d71caae86.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: Going chronologically, the LLM will keep trying to create a skill in code while
    it is given ways to improve via execution errors, the environment, and peer-feedback.
    Once all say the skill looks good, it is then added to the skill library for future
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Skill Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/71f092d4dfaa4e18c0244b6e9cfe8e5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: The Skill Library holds the skills that the LLM has generated before and gone
    through the approval process in the iterative prompting step. Each skill is added
    to the library by taking a description of it and then converting that description
    into an embedding. The authors then take the description of the task and query
    the skill library to find skills with a similar embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Because the Skill Library is a separate data store, it is free to grow over
    time. The paper did not go into updating the skills already in the library, so
    it would appear that once the skill is learned it will stay in that state. This
    poses interesting questions for how you could update the skills as experience
    progresses.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with other Agent Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Voyager is considered part of the agent space — where we expect the LLM to behave
    as an entity in its own right, interacting with the environment and changing things.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bd2ff88bc0e4f0841a52d3dd358831c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1d from the [REACT: SYNERGIZING REASONING AND ACTING IN'
  prefs: []
  type: TYPE_NORMAL
- en: LANGUAGE MODELS paper](https://arxiv.org/pdf/2210.03629)
  prefs: []
  type: TYPE_NORMAL
- en: To that end, there are a few different prompting methodologies employed to accomplish
    that. First, AutoGPT is a Github library that people have used to automate many
    different tasks from file system actions to simple software development. Next,
    we have Reflexion which gives the LLM an example of what has just happened and
    then has it reflect on what it should do next time in a similar situation. We
    use the reflected upon advice to tell the Minecraft player what to do. Finally,
    we have ReAct, which will have the LLM break down tasks into simpler steps via
    a formulaic way of thinking. From the image above you can see the formatting it
    uses.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the methodologies were put into the game, and the table below shows
    the results. Only AutoGPT and the Voyager methods actually successfully made it
    to the Wooden Tool stage. This may be a consequence of the training data for the
    LLMs. With ReAct and Reflexion, it appears a good amount of knowledge about the
    task at hand is required for the prompting to be effective. From the table below,
    we can see that the Voyager methodology without the skill library was able to
    do better than AutoGPT, but not able to make it to the final Diamond Tool category.
    Thus, we can see clearly that the Skill Library plays an outsize role here. In
    the future, Skill Libraries for LLMs may become a type of moat for a company.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40e444451061bd63c1fb3f1b3617b3f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: Tech progress is just one way to look at a Minecraft game. The figure below
    clearly outlines the parts of the game map that each LLM explored. Just look at
    how much further Voyager will go in the map than the others. Whether this is an
    accident of slightly different prompts or an inherent part of the Voyager architecture
    remains to be seen. As this methodology is applied to other situations we’ll have
    a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdec20f72b458beb9574da6952ea0ebf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7 from [the paper](https://arxiv.org/pdf/2305.16291)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This paper highlights an interesting approach to tool usage. As we push for
    LLMs to have greater reasoning ability, we will increasingly look for them to
    make decisions based on that reasoning ability. While an LLM that improves itself
    will be more valuable than a static one, it also poses the question: How do you
    make sure it doesn’t go off track?'
  prefs: []
  type: TYPE_NORMAL
- en: From one point of view, this is limited to the quality of its actions. Improvement
    in complex environments is not always as simple as maximizing a differentiable
    reward function. Thus, a major area of work here will focus on validating that
    the LLM’s skills are improving rather than just changing.
  prefs: []
  type: TYPE_NORMAL
- en: However, from a larger point of view, we can reasonably wonder if there are
    some skills or areas where the LLM may become too dangerous if left to its own
    discretion. Areas with direct impact on human life come to mind. Now, areas like
    this still have problems that LLMs could solve, so the solution cannot be to freeze
    progress here and allow people who otherwise would have benefitted from the progress
    to suffer instead. Rather, we may see a world where LLMs execute the skills that
    humans design, creating a world that pairs human and machine intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: It is an exciting time to be building.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Wang, G., et al. [“VOYAGER: An Open-Ended Embodied Agent'
  prefs: []
  type: TYPE_NORMAL
- en: with Large Language Models”](https://arxiv.org/pdf/2305.16291.pdf) (2023), arXiv
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Significant-gravitas/auto-gpt](https://github.com/Significant-Gravitas/AutoGPT/):
    An experimental open-source attempt to make gpt-4 fully autonomous., 2024, Github'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Yao, S., et al. [“REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE
    MODELS”](https://arxiv.org/pdf/2210.03629) (2023), arXiv'
  prefs: []
  type: TYPE_NORMAL
