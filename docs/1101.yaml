- en: Tool Use, Agents, and the Voyager Paper
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具使用、代理和Voyager论文
- en: 原文：[https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01](https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01](https://towardsdatascience.com/tool-use-agents-and-the-voyager-paper-5a0e548f8b38?source=collection_archive---------8-----------------------#2024-05-01)
- en: A detailed exploration of the Voyager Paper and its findings on tool usage
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对《Voyager论文》及其在工具使用方面的发现进行了详细探讨
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--5a0e548f8b38--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    ·8 min read·May 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5a0e548f8b38--------------------------------)
    ·阅读时间8分钟·2024年5月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9618fd2e69903d1de213327b57dab011.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9618fd2e69903d1de213327b57dab011.png)'
- en: Image by Author. Generated by DALL-E 2
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：作者。由DALL-E 2生成
- en: As LLMs continue to increase their reasoning ability, their capacity to plan
    and then act tends to increase. This has led to prompting templates where users
    give LLMs an end result they want and the LLM then will figure out how to accomplish
    it — even if it takes multiple actions to do so. This kind of prompting is often
    called an agent, and it has generated a lot of excitement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM推理能力的不断增强，它们的计划和执行能力也在提高。这导致了出现了这样一种提示模板：用户给LLM一个想要的结果，LLM会找出如何实现它——即使这需要多次行动才能完成。这种提示通常被称为“代理”，并且已经引起了广泛的关注。
- en: For example, one could ask an agent to win a game and then watch it figure out
    a good strategy to do so. While typically we would use frameworks like reinforcement
    learning to train a model to win games like Super Mario Bros, when we look at
    games with more open-ended goals like Minecraft, an LLMs’ ability to reason can
    become critical.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，用户可以要求代理赢得一场游戏，然后观察它制定出一个好的策略来实现这一目标。通常情况下，我们会使用强化学习等框架训练模型赢得像《超级马里奥兄弟》这样的游戏，但当我们考虑像《Minecraft》这样的目标开放的游戏时，LLM的推理能力就显得尤为重要。
- en: The Voyager Paper focuses on how one can prompt an LLM so that it can complete
    open-ended and challenging tasks like playing Minecraft. Let’s dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 《Voyager论文》重点讨论了如何提示LLM，使其能够完成开放性和挑战性任务，如玩《Minecraft》。让我们深入探讨一下！
- en: High Level Discussion of Voyager
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Voyager的高级讨论
- en: '![](../Images/1fa71565ab5cc7f0134d68a7b2b194dc.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fa71565ab5cc7f0134d68a7b2b194dc.png)'
- en: Figure 2 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2来自[论文](https://arxiv.org/pdf/2305.16291)
- en: 'The Voyager system here consists of three major pieces: an automatic curriculum,
    an iterative prompting mechanism, and a skill library. The curriculum you can
    imagine as the compass of the system, a way that the agent is able to determine
    what it should do in a given situation. As new situations arise, we have the iterative
    prompting mechanism to create new skills for new situations. Because LLMs have
    limited contexts but the curriculum could potentially create the need for limitless
    skills, we also have a skill library to store these skills for later use.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的Voyager系统由三个主要部分组成：自动化课程、迭代提示机制和技能库。你可以把课程想象成系统的指南针，它能帮助代理在特定情境下判断应该做什么。当新的情境出现时，我们通过迭代提示机制为新情境创造新的技能。由于LLM（大规模语言模型）有有限的上下文，而课程可能会创建对无限技能的需求，因此我们还拥有技能库，用来存储这些技能以备后续使用。
- en: Automatic Curriculum
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化课程
- en: '![](../Images/0ae1859d17dcd1ca0a65adbf8318bacc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ae1859d17dcd1ca0a65adbf8318bacc.png)'
- en: Figure 3 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)中的图3'
- en: The Automatic Curriculum is itself prompt engineering, where pertinent information
    about the AI’s immediate environment and long-term goals are passed to the LLM.
    The authors were kind enough to give the full system prompt in the paper, so I
    will highlight interesting parts of it below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自动课程本身就是提示工程，其中涉及关于AI的即时环境和长期目标的相关信息传递给LLM。作者非常贴心地在论文中提供了完整的系统提示，因此我将在下面突出其中有趣的部分。
- en: '![](../Images/0518cab72e1fed32499c027bfd0c3db9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0518cab72e1fed32499c027bfd0c3db9.png)'
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)附录A.3.4的部分内容'
- en: First, the prompt explains the basic schema that will be passed in. Although
    the exact information is not filled in here, it appears like this is helping by
    priming the LLM to receive information in this schema. This is similar to few-shot
    reasoning, as later turns with this chatbot will use that format.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，提示解释了将传递的基本架构。尽管这里没有填写具体信息，但似乎这是通过引导LLM以这种架构接收信息来提供帮助。这类似于少样本推理，因为与此聊天机器人的后续对话将使用这种格式。
- en: '![](../Images/6dd01acc8894117fb229ecdedd9dbfd4.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd01acc8894117fb229ecdedd9dbfd4.png)'
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)附录A.3.4的部分内容'
- en: Next, the prompt outlines a fairly precise way that the LLM should reason. Notice
    that this is still given in the second-person (you) and that the instructions
    are highly specific to Minecraft itself. In my opinion, improvements to the pieces
    outlined above would seem to give the greatest payoff. Moreover, notice how the
    steps themselves are not always exact like one might see in traditional programming,
    instead being consistent with the more ambiguous goal of exploration. This is
    where the promise of agents lies.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，提示列出了LLM应该如何进行推理的相当精确的方式。请注意，这仍然是用第二人称（你）给出的，而且这些指令高度具体化到Minecraft本身。在我看来，上面列出的改进措施似乎会带来最大的回报。此外，请注意，这些步骤本身并不总是像传统编程中那样精确，而是与探索这一更模糊的目标保持一致。这正是代理程序的潜力所在。
- en: 'Within the above section, Step one is especially interesting to me, as it could
    be read as a kind of “persona” prompting: telling the LLM it is a mentor and thus
    having it speak with more certainty in its replies. We have seen in the past that
    persona prompting can result in the LLM taking more decisive action, so this could
    be a way to ensure the agent will act and not get stuck in analysis paralysis.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的部分中，第一步对我特别有趣，因为它可以被解读为一种“人格化”提示：告诉LLM它是一个导师，从而让它在回答时表现出更多的自信。我们过去已经看到，人格化提示可以促使LLM采取更果断的行动，因此这可能是一种确保代理能够行动而不是陷入分析瘫痪的方式。
- en: '![](../Images/c2476e323d08baee12a3de02a52c5eea.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2476e323d08baee12a3de02a52c5eea.png)'
- en: Part of Appendix A.3.4 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)附录A.3.4的部分内容'
- en: Finally, the prompt ends by again giving a few-shot reasoning piece illustrating
    how best to respond.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提示再次通过提供一个少样本推理的示例来说明最佳回应方式。
- en: Historically, we’ve used reinforcement machine learning models with specific
    inputs to discover optimal strategies for maximizing well-defined metrics (think
    getting the highest score in an arcade game). Today, the LLM is given a more ambiguous
    long-term goal and seen taking actions that would realize it. That we think the
    LLM is capable of approximating this type of goal signals a major change in expectations
    for ML agents.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，我们使用强化学习模型和特定的输入来发现最大化明确指标的最佳策略（例如，在街机游戏中获得最高分）。如今，LLM被赋予了一个更模糊的长期目标，并采取行动来实现这一目标。我们认为LLM能够近似这种类型的目标，这标志着对机器学习代理期望的重大变化。
- en: Iterative Prompting Mechanism
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迭代提示机制
- en: '![](../Images/d28bf3090c68a4f8810cb39aecda38b7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d28bf3090c68a4f8810cb39aecda38b7.png)'
- en: Figure 5 from [the paper](https://arxiv.org/pdf/2305.16291) showing environment
    & execution feedback
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)中的图5，展示了环境和执行反馈'
- en: Here, the LLM will create code that executes certain actions in Minecraft. As
    these tend to be more complex series of actions, we call these skills.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，LLM将创建执行Minecraft中某些操作的代码。由于这些操作往往是更复杂的系列动作，我们称之为“技能”。
- en: 'When creating the skills that will go into the skill library, the authors had
    their LLM receive 3 distinct kinds of feedback during development: (1) execution
    errors, (2) environment feedback, and (3) peer-review from another LLM.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建将进入技能库的技能时，作者让他们的LLM在开发过程中接收了三种不同类型的反馈：（1）执行错误，（2）环境反馈，以及（3）来自另一LLM的同行评审。
- en: 'Execution errors can occur when the LLM makes a mistake with the syntax of
    the code, the Mineflayer library, or some other item that is caught by the compiler
    or in run-time. Environment feedback comes from the Minecraft game itself. The
    authors use the *bot.chat()* feature within Mineflayer to get feedback such as
    *“I cannot make stone_shovel because I need: 2 more stick”*. This information
    is then passed into the LLM.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 执行错误可能发生在LLM在代码语法、Mineflayer库或其他被编译器或运行时捕捉到的项目上出错时。环境反馈来自Minecraft游戏本身。作者使用Mineflayer中的*bot.chat()*功能获取反馈信息，例如*“我不能制作stone_shovel，因为我还需要：2根棍子”*。然后，这些信息被传递给LLM。
- en: While execution and environment feedback seems natural, the peer-review feedback
    may seem strange. After all, running two LLMs is more expensive than running only
    one. However, as the set of skills that can be created by the LLM is enormous,
    it would be very difficult to write code that verifies the skills actually do
    what they are supposed to do. To get around this, the authors have a separate
    LLM review the code and give feedback on if the task is accomplished. While this
    isn’t as perfect as verifying programmatically the job is finished, it is a good
    enough proxy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然执行和环境反馈看起来很自然，但同行评审的反馈可能会显得有些奇怪。毕竟，运行两个LLM比仅运行一个要更昂贵。然而，由于LLM可以创建的技能集合庞大，编写代码来验证这些技能是否按预期执行会非常困难。为了避免这个问题，作者让一个独立的LLM审查代码并提供反馈，评估任务是否完成。尽管这不像程序化地验证工作是否完成那样完美，但它作为代理已经足够好。
- en: '![](../Images/3acf4f23d2950bd4fafcb28d71caae86.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3acf4f23d2950bd4fafcb28d71caae86.png)'
- en: Figure 6 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2305.16291)的图6
- en: Going chronologically, the LLM will keep trying to create a skill in code while
    it is given ways to improve via execution errors, the environment, and peer-feedback.
    Once all say the skill looks good, it is then added to the skill library for future
    use.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 按时间顺序，LLM将在不断改进的过程中持续尝试创建技能代码，同时根据执行错误、环境反馈和同行评审不断改进。一旦所有反馈都表明该技能已通过，它将被添加到技能库中以供未来使用。
- en: Skill Library
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技能库
- en: '![](../Images/71f092d4dfaa4e18c0244b6e9cfe8e5a.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71f092d4dfaa4e18c0244b6e9cfe8e5a.png)'
- en: Figure 4 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2305.16291)的图4
- en: The Skill Library holds the skills that the LLM has generated before and gone
    through the approval process in the iterative prompting step. Each skill is added
    to the library by taking a description of it and then converting that description
    into an embedding. The authors then take the description of the task and query
    the skill library to find skills with a similar embedding.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 技能库保存了LLM之前生成的技能，这些技能已经通过了迭代提示步骤中的审批过程。每个技能通过将其描述转换为嵌入向量的方式添加到库中。作者随后使用任务的描述查询技能库，以找到与该描述相似的技能嵌入。
- en: Because the Skill Library is a separate data store, it is free to grow over
    time. The paper did not go into updating the skills already in the library, so
    it would appear that once the skill is learned it will stay in that state. This
    poses interesting questions for how you could update the skills as experience
    progresses.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于技能库是一个独立的数据存储，它可以随着时间的推移自由扩展。论文中没有详细讨论如何更新库中已有的技能，因此看起来一旦技能被学习，它就会保持原样。这引发了关于如何在经验积累过程中更新技能的有趣问题。
- en: Comparison with other Agent Prompts
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与其他Agent提示的比较
- en: Voyager is considered part of the agent space — where we expect the LLM to behave
    as an entity in its own right, interacting with the environment and changing things.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Voyager被认为是代理空间的一部分——我们期望LLM作为一个独立的实体与环境互动并改变事物。
- en: '![](../Images/6bd2ff88bc0e4f0841a52d3dd358831c.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bd2ff88bc0e4f0841a52d3dd358831c.png)'
- en: 'Figure 1d from the [REACT: SYNERGIZING REASONING AND ACTING IN'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '来自[REACT: 协同推理与行动](https://arxiv.org/pdf/2305.16291)的图1d'
- en: LANGUAGE MODELS paper](https://arxiv.org/pdf/2210.03629)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型论文](https://arxiv.org/pdf/2210.03629)
- en: To that end, there are a few different prompting methodologies employed to accomplish
    that. First, AutoGPT is a Github library that people have used to automate many
    different tasks from file system actions to simple software development. Next,
    we have Reflexion which gives the LLM an example of what has just happened and
    then has it reflect on what it should do next time in a similar situation. We
    use the reflected upon advice to tell the Minecraft player what to do. Finally,
    we have ReAct, which will have the LLM break down tasks into simpler steps via
    a formulaic way of thinking. From the image above you can see the formatting it
    uses.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，有几种不同的提示方法被用来实现这一目标。首先，AutoGPT是一个Github库，人们已用它来自动化许多不同的任务，从文件系统操作到简单的软件开发。接下来，我们有Reflexion，它给LLM提供了一个刚刚发生的事件示例，然后让它反思在类似情况下下次应该做什么。我们使用反思后的建议告诉Minecraft玩家接下来该做什么。最后，我们有ReAct，它让LLM通过公式化的思维方式将任务分解成更简单的步骤。从上图中，你可以看到它使用的格式。
- en: Each of the methodologies were put into the game, and the table below shows
    the results. Only AutoGPT and the Voyager methods actually successfully made it
    to the Wooden Tool stage. This may be a consequence of the training data for the
    LLMs. With ReAct and Reflexion, it appears a good amount of knowledge about the
    task at hand is required for the prompting to be effective. From the table below,
    we can see that the Voyager methodology without the skill library was able to
    do better than AutoGPT, but not able to make it to the final Diamond Tool category.
    Thus, we can see clearly that the Skill Library plays an outsize role here. In
    the future, Skill Libraries for LLMs may become a type of moat for a company.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法论都已被应用于游戏，下面的表格展示了结果。只有AutoGPT和Voyager方法成功地达到了木工具阶段。这可能是LLM训练数据的结果。对于ReAct和Reflexion，似乎需要相当多的关于当前任务的知识，才能使提示有效。从下表中可以看出，没有技能库的Voyager方法做得比AutoGPT更好，但仍未能进入最终的钻石工具类别。因此，我们可以清楚地看到，技能库在这里扮演了举足轻重的角色。未来，LLM的技能库可能成为公司的一种护城河。
- en: '![](../Images/40e444451061bd63c1fb3f1b3617b3f3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40e444451061bd63c1fb3f1b3617b3f3.png)'
- en: Table 1 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)中的表格1'
- en: Tech progress is just one way to look at a Minecraft game. The figure below
    clearly outlines the parts of the game map that each LLM explored. Just look at
    how much further Voyager will go in the map than the others. Whether this is an
    accident of slightly different prompts or an inherent part of the Voyager architecture
    remains to be seen. As this methodology is applied to other situations we’ll have
    a better understanding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 技术进步只是看待《Minecraft》游戏的一种方式。下图清晰地概述了每个LLM探索过的游戏地图部分。看看Voyager在地图上探索的范围远超其他LLM。这是由于稍有不同的提示词导致的偶然结果，还是Voyager架构固有的特性，还有待观察。随着这种方法论应用于其他情境，我们将能更好地理解这一点。
- en: '![](../Images/cdec20f72b458beb9574da6952ea0ebf.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdec20f72b458beb9574da6952ea0ebf.png)'
- en: Figure 7 from [the paper](https://arxiv.org/pdf/2305.16291)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/2305.16291)中的图7'
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'This paper highlights an interesting approach to tool usage. As we push for
    LLMs to have greater reasoning ability, we will increasingly look for them to
    make decisions based on that reasoning ability. While an LLM that improves itself
    will be more valuable than a static one, it also poses the question: How do you
    make sure it doesn’t go off track?'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本文突出了一个关于工具使用的有趣方法。随着我们推动LLM具备更强的推理能力，我们将越来越多地期望它们根据这种推理能力做出决策。尽管一个能自我改进的LLM比静态的LLM更有价值，但它也带来了一个问题：如何确保它不会偏离正轨？
- en: From one point of view, this is limited to the quality of its actions. Improvement
    in complex environments is not always as simple as maximizing a differentiable
    reward function. Thus, a major area of work here will focus on validating that
    the LLM’s skills are improving rather than just changing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从某个角度来看，这局限于其行为的质量。复杂环境中的改进并不总是像最大化可微分奖励函数那么简单。因此，未来的一个重要工作领域将集中在验证LLM的技能是否在提升，而不是仅仅在变化。
- en: However, from a larger point of view, we can reasonably wonder if there are
    some skills or areas where the LLM may become too dangerous if left to its own
    discretion. Areas with direct impact on human life come to mind. Now, areas like
    this still have problems that LLMs could solve, so the solution cannot be to freeze
    progress here and allow people who otherwise would have benefitted from the progress
    to suffer instead. Rather, we may see a world where LLMs execute the skills that
    humans design, creating a world that pairs human and machine intelligence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从更广阔的角度来看，我们可以合理地想知道，是否存在一些领域或技能，在某些情况下，如果让大型语言模型（LLM）自行决定，可能会变得过于危险。比如那些直接影响人类生命的领域。现在，像这样的领域仍然存在LLM可以解决的问题，因此，解决方案不能是冻结此处的进展，允许那些本应从进展中受益的人遭受损失。相反，我们可能会看到一个人类和机器智能相结合的世界，在这个世界中，LLM执行人类设计的技能。
- en: It is an exciting time to be building.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在正是构建的激动人心时刻。
- en: '[1] Wang, G., et al. [“VOYAGER: An Open-Ended Embodied Agent'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Wang, G., 等人. [“VOYAGER: 一个开放式的具身智能体'
- en: with Large Language Models”](https://arxiv.org/pdf/2305.16291.pdf) (2023), arXiv
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与大型语言模型”](https://arxiv.org/pdf/2305.16291.pdf)（2023年），arXiv
- en: '[2] [Significant-gravitas/auto-gpt](https://github.com/Significant-Gravitas/AutoGPT/):
    An experimental open-source attempt to make gpt-4 fully autonomous., 2024, Github'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Significant-gravitas/auto-gpt](https://github.com/Significant-Gravitas/AutoGPT/):
    一个实验性的开源项目，旨在使gpt-4完全自主，2024年，Github'
- en: '[3] Yao, S., et al. [“REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE
    MODELS”](https://arxiv.org/pdf/2210.03629) (2023), arXiv'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Yao, S., 等人. [“REAC T: 语言模型中推理与行动的协同作用”](https://arxiv.org/pdf/2210.03629)（2023年），arXiv'
