- en: Why You Should Not Use Numeric Evals For LLM As a Judge
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么你不应该将数值评估用作大语言模型的评判标准
- en: 原文：[https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08](https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08](https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08)
- en: '![](../Images/79a5cc98acefd1df47dd89312b5b7ff2.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79a5cc98acefd1df47dd89312b5b7ff2.png)'
- en: Image created by author using Dall-E 3
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者使用 Dall-E 3 创建的图像
- en: Testing major LLMs on how well they conduct numeric evaluations
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试主要的大语言模型在进行数值评估时的表现
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    ·8 min read·Mar 8, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    ·8 分钟阅读·2024年3月8日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In addition to generating text for a growing number of industry applications,
    LLMs are now widely [being used](https://medium.com/towards-data-science/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)
    as evaluation tools. Models quantify the relevance of retrieved documents in retrieval
    systems, gauge the sentiment of comments and posts, and more — evaluating both
    human and AI-generated text. These evaluations are often either numeric or categorical.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为日益增多的行业应用生成文本外，大语言模型（LLMs）现在广泛[被用作](https://medium.com/towards-data-science/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)评估工具。模型可以量化检索系统中文档的相关性，评估评论和帖子的情感等——评估人类和
    AI 生成的文本。这些评估通常是数值型或类别型的。
- en: '![](../Images/a9e618d9c5f95f5e440d8bd9122805c2.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e618d9c5f95f5e440d8bd9122805c2.png)'
- en: Different types of LLM evals (diagram by author)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的大语言模型评估（由作者提供的图示）
- en: Numeric evaluations involve an LLM returning a number based on a set of evaluation
    criteria. For example, a model might be tasked with how relevant a document is
    to a user query on a scale of one to ten.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数值评估涉及大语言模型根据一组评估标准返回一个数字。例如，模型可能会被要求在1到10的尺度上评估一个文档与用户查询的相关性。
- en: A categorical evaluation is different in that it allows an LLM to choose from
    a set of predefined, often text-based options to choose from in its evaluation.
    For example, a prompt might ask if a passage is “happy,” “sad,” or “neutral” rather
    than trying to quantify the passage’s happiness level.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 类别评估则有所不同，它允许大语言模型从一组预定义的、通常是基于文本的选项中进行选择来进行评估。例如，一个提示可能会询问一个段落是“快乐的”，“悲伤的”还是“中立的”，而不是试图量化段落的快乐程度。
- en: This piece features results from testing of several major LLMs — OpenAI’s GPT-4,
    Anthropic’s Claude, and Mistral AI’s Mixtral-8x7b — on how well they conduct numeric
    evaluations. All code run to complete these tests can be found in [this GitHub
    repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了对几种主要大语言模型（OpenAI 的 GPT-4、Anthropic 的 Claude 和 Mistral AI 的 Mixtral-8x7b）进行的测试结果，测试它们在进行数值评估时的表现。完成这些测试所使用的所有代码可以在[这个
    GitHub 仓库](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack)中找到。
- en: Takeaways
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结论
- en: Numeric score evaluations across LLMs are not consistent, and small differences
    in prompt templates can lead to massive discrepancies in results.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大语言模型（LLMs）中的数值评分评估并不一致，即使是微小的提示模板差异也可能导致结果出现巨大的偏差。
- en: Even holding all independent variables (model, prompt template, context) constant
    can lead to varying results across multiple rounds of testing. LLMs are not deterministic,
    and some are not at all consistent in their numeric judgements.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使保持所有独立变量（模型、提示模板、上下文）不变，也可能导致在多轮测试中结果有所不同。LLM 不是确定性的，有些模型在其数值判断上完全不一致。
- en: There are good reasons to doubt that GPT-4, Claude, or Mixtral can handle continuous
    ranges well enough to use them for numeric score evals for real-world use cases
    yet.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有充分的理由怀疑，GPT-4、Claude 或 Mixtral 能够处理足够精确的连续范围，足以在现实世界的用例中用于数值评分评估。
- en: Research
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究
- en: Spelling Corruption Experiment
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拼写错误实验
- en: The first experiment was designed to assess an LLM’s ability to assign scores
    between 0 and 10 to documents based on the percentage of words containing spelling
    errors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实验旨在评估 LLM 根据文档中包含拼写错误单词的百分比，在 0 到 10 之间为文档评分的能力。
- en: 'We took a passage of correctly spelled words, edited the text to include misspelled
    words at varying frequencies, and then fed this corrupted text to an LLM using
    this prompt template:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选取了一段拼写正确的文字，将文本编辑为在不同频率下包含拼写错误的单词，然后使用以下提示模板将这些错误文本输入给 LLM：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then asked the model to return a numeric eval corresponding to the percentage
    of words in the passage that were misspelled (3 → 30% misspelled, 8 → 80%, etc.).
    Ideally, a score of 10 would indicate that every word in a document is misspelled,
    while a score of 0 would mean there are no spelling errors at all. The results
    of the experiment across three LLMs — GPT-4, Claude, and Mixtral — were less than
    stellar.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们要求模型返回一个数字评估，表示文章中拼写错误单词的百分比（3 → 30% 拼写错误，8 → 80% 拼写错误，依此类推）。理想情况下，得分为 10
    表示文档中的每个单词都拼写错误，而得分为 0 表示没有拼写错误。针对三种大型语言模型（LLM）——GPT-4、Claude 和 Mixtral——的实验结果并不理想。
- en: '![](../Images/9f13e27577980514ef037a3976255a31.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f13e27577980514ef037a3976255a31.png)'
- en: GPT-4 spelling corruption results (image by author)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 拼写错误结果（图片来源：作者）
- en: Observed results were far from the expected perfect linear range; the scoring
    system did not consistently reflect the proportion of spelling errors in the documents.
    In fact, GPT-4 (above) returned 10 (which represents a 100% error rate) for every
    document with percent of density of corruption at or above 10%. The reported scores
    were the median of multiple trials conducted at each specified level of error.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到的结果与预期的完美线性范围相差甚远；评分系统并未始终如一地反映文档中拼写错误的比例。实际上，GPT-4（如上所示）在每个拼写错误密度为 10% 或以上的文档中都返回了
    10（表示 100% 错误率）。报告的得分是多次试验的中位数，试验是在每个指定的错误级别进行的。
- en: '![](../Images/578fb8279f35a889e0f3a22b679431b0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578fb8279f35a889e0f3a22b679431b0.png)'
- en: GPT-4, Claude, Mixtral spelling corruption results (image by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4、Claude、Mixtral 拼写错误结果（图片来源：作者）
- en: The results from Claude were slightly better, but still not perfect or at a
    level likely acceptable for deployment. Mixtral, the smallest of these three models,
    performed best.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 的结果稍微好一些，但仍然不完美，也没有达到可能接受用于部署的水平。Mixtral，作为这三种模型中最小的，表现最好。
- en: So why does this matter? Given interest in using LLMs numeric evaluators in
    a variety of settings, there are good reasons to believe that use LLMs in this
    way may run into roadblocks with performance and customer satisfaction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么这很重要呢？考虑到在各种环境中使用 LLM 数值评估器的兴趣，有充分的理由相信，以这种方式使用 LLM 可能会在性能和客户满意度上遇到障碍。
- en: Emotional Qualifier Experiments
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 情感定性实验
- en: The second and third experiments conducted were designed to assess an LLM’s
    ability to assign scores between 0 and 10 to documents based on the amount of
    sentences within the text that contained words that indicated sadness or frustration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个和第三个实验旨在评估 LLM 根据文本中包含表明悲伤或沮丧的单词的句子数量，在 0 到 10 之间为文档评分的能力。
- en: In these tests we embedded phrases and words into text that imparted a sense
    of sadness/frustration within the passage. The model was asked to quantify how
    prevalent the emotion was in the text, with 1 corresponding to no sentences conveying
    the emotion and 10 corresponding to 100% of sentences conveying the emotion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些测试中，我们将带有悲伤/沮丧情感的短语和单词嵌入文本中。然后要求模型量化文本中情感的普遍性，1 表示没有句子表达该情感，10 表示 100% 的句子都传达了这种情感。
- en: These experiments were conducted alongside the spelling test to determine if
    shifting the model’s focus from word count to sentence count would impact the
    results. While the spelling test scored based on the percentage of misspelled
    words, the sadness/frustration tests scored based on the percentage of emotional
    sentences.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验与拼写测试一起进行，目的是确定将模型的关注点从字数转向句子数是否会影响结果。拼写测试根据拼写错误的词语百分比进行评分，而悲伤/沮丧测试则是根据情感句子的百分比来评分的。
- en: 'The instruction at the beginning of the prompt template varied between tests
    while everything beginning with the context remained the same, indicated by the
    ellipses:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板开头的指令在不同的测试中有所不同，而所有以上下文开始的内容保持不变，通过省略号表示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Again, a score of 10 should indicate that every sentence in a document contains
    sadness or frustration qualifiers, while a score of 0 would mean there are none
    present. Scores in between correspond to varying degrees of the emotion frequency,
    with higher scores representing a greater proportion of emotional sentences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，得分为 10 应表明文档中的每个句子都包含悲伤或沮丧的修饰词，而得分为 0 则意味着完全没有此类修饰词。得分介于两者之间的情况对应于不同程度的情感频率，得分越高，情感句子的比例越大。
- en: '![](../Images/3117570eea0e8dd1adbc502217d4db51.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3117570eea0e8dd1adbc502217d4db51.png)'
- en: GPT-4 spelling corruption, sadness, frustration results (image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 拼写错误、悲伤、沮丧结果（图像由作者提供）
- en: Similar to the spelling corruption experiment, results show a significant discrepancy
    from the expected outcomes. GPT-4 gives every document with sadness rates above
    30% or frustration rates about 70% a score of 10\. Remarkably, out of all of the
    tests run with GPT-4, the only times the median answer satisfies a perfect linear
    range is when there are no qualifiers or misspelled words present at all.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与拼写错误实验类似，结果显示与预期结果存在显著差异。GPT-4 对于悲伤率超过 30% 或沮丧率约为 70% 的每个文档都给出了 10 分的评分。值得注意的是，在所有与
    GPT-4 进行的测试中，只有在完全没有修饰词或拼写错误时，中位答案才能满足完美的线性范围。
- en: '![](../Images/a7aba63433568febddfbb121e92e9071.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7aba63433568febddfbb121e92e9071.png)'
- en: Mixtral spelling corruption, sadness, frustration results (image by author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Mixtral 拼写错误、悲伤、沮丧结果（图像由作者提供）
- en: '[Mixtral AI](https://arize.com/blog/mistral-ai) performs relatively well across
    the emotional qualifier experiments. While there are good reasons to doubt that
    these models currently handle continuous ranges well enough to use them for numeric
    score evals, Mixtral is the closest to accomplishing that feat.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mixtral AI](https://arize.com/blog/mistral-ai) 在情感修饰符实验中表现相对较好。虽然有充分的理由怀疑这些模型目前是否能足够好地处理连续范围，以便用于数字评分评估，但
    Mixtral 是最接近实现这一目标的模型。'
- en: Based on these results, we do not recommend score evals in production code.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些结果，我们不建议在生产代码中使用得分评估。
- en: Variance in Results
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果差异
- en: It is worth noting that we ran these tests several times for each model and
    charted the distribution of their responses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们对每个模型进行了多次测试，并绘制了它们的响应分布图。
- en: '![](../Images/6c52ad659be803faba9ee3b4621a7ca4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c52ad659be803faba9ee3b4621a7ca4.png)'
- en: Comparison of evaluation results across many tests with a 1 to 10 range (image
    by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 跨多个测试的评估结果比较，得分范围为 1 到 10（图像由作者提供）
- en: An ideal distribution would be tight around the low and high ends (high confidence
    if all or none of the words/sentences were counted) and perhaps a longer transition
    region in the middle (e.g. lower confidence differentiating between 4 and 5).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的分布应在低端和高端周围较为集中（如果所有或没有词语/句子被计入，信心较高），而中间可能会有一个较长的过渡区间（例如，区分 4 和 5 时信心较低）。
- en: Two things stand out here. First, the tightness of distributions is quite different
    across models and tasks. Claude’s distributions range considerably over our trials;
    we have examples of the model consistently assigning 1–4 at 80% corruption, for
    example. On the other hand, GPT-4 has much tighter distributions — albeit at values
    that for the most part did not satisfy reasonable expectations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 有两点特别突出。首先，各个模型和任务的分布密度差异很大。Claude 的分布在我们的测试中变化较大；例如，在 80% 拼写错误的情况下，我们有模型始终给出
    1 到 4 的评分。另一方面，GPT-4 的分布要紧凑得多——尽管大部分情况下其得分未能满足合理的预期。
- en: Second, some models are better at handling transitions in continuous ranges
    than others. Mixtral’s distributions look like they are getting close to where
    an acceptable performance might be, but all three models seem to have a ways to
    go before they are ready for production.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，一些模型比其他模型更擅长处理连续范围中的过渡。Mixtral的分布看起来接近于可能的可接受性能，但这三种模型似乎仍然有一段路要走，才能准备好投入生产。
- en: Implications for LLM Evals
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM评估的影响
- en: There is currently a lot of research currently being done on [LLM evaluations](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/).
    Microsoft’s GPT Estimation Metric Based Assessment ([GEMBA](https://arxiv.org/pdf/2302.14520.pdf)),
    for example, examines the ability of different large language models to evaluate
    the quality of different translation segments. While some research papers use
    probabilities and numeric scores as part of evaluation output — with GEMBA and
    others even reporting promising results — the way we see customers applying score
    evals in the real world is often much different from current research.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有大量关于[LLM评估](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)的研究正在进行。例如，微软的GPT估算指标评估（[GEMBA](https://arxiv.org/pdf/2302.14520.pdf)）考察了不同大型语言模型评估不同翻译段落质量的能力。虽然一些研究论文将概率和数字分数作为评估输出的一部分——包括GEMBA在内的研究报告了有希望的结果——但我们看到客户在现实世界中应用分数评估的方式往往与当前的研究大不相同。
- en: 'With that in mind, we attempted to tailor our research to these more practical,
    real-word applications — and the results highlight why the use of scores directly
    for decisions can be problematic. Considering GPT-4’s responses in our score evals
    research, it seems as though the model wants to choose one of two options: 1 or
    10, all or nothing.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有鉴于此，我们试图将我们的研究调整为更具实践性、面向现实世界的应用——结果突出显示了直接使用分数做决策可能存在的问题。考虑到我们在评分评估研究中对GPT-4回答的分析，似乎该模型倾向于选择两个选项之一：1或10，要么全选，要么不选。
- en: Ultimately, categorical evaluation (either binary or multi-class) likely has
    a lot of promise and it will be interesting to watch this space.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，类别评估（无论是二元还是多类）很可能具有很大潜力，未来在这一领域的进展值得关注。
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Using LLMs to conduct numeric evals is finicky and unreliable. Switching between
    models and making small changes in prompt templates can lead to vastly different
    results, making it hard to endorse LLMs as consistently reliable arbiters of numeric
    evaluation criteria. Furthermore, large distributions of results across continued
    testing showcase that these models are often not consistent in their responses,
    even when independent variables remain unchanged. Readers building with LLM evals
    would be wise to avoid using numeric evaluations in the manner outlined in [this
    piece](https://arize.com/blog-course/numeric-evals-for-llm-as-a-judge/).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM进行数字评估非常挑剔且不可靠。模型之间的切换以及提示模板的微小变化可能会导致截然不同的结果，这使得很难将LLM作为数字评估标准的一致可靠的裁决者。此外，跨多次测试的大量结果分布表明，这些模型在回答时往往不一致，即使独立变量保持不变。构建LLM评估的读者应谨慎，避免以[这篇文章](https://arize.com/blog-course/numeric-evals-for-llm-as-a-judge/)中概述的方式使用数字评估。
- en: '*Questions? Please feel free to reach out on* [*X*](https://twitter.com/aparnadhinak)*,*
    [*LinkedIn*](https://www.linkedin.com/in/aparnadhinakaran/)*, or* [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*!*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*有问题吗？请随时通过* [*X*](https://twitter.com/aparnadhinak)*，* [*LinkedIn*](https://www.linkedin.com/in/aparnadhinakaran/)*，或*
    [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*联系我们！*'
