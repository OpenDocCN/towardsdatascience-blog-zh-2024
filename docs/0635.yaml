- en: Why You Should Not Use Numeric Evals For LLM As a Judge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08](https://towardsdatascience.com/why-you-should-not-use-numeric-evals-for-llm-as-a-judge-bf22424f5379?source=collection_archive---------7-----------------------#2024-03-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/79a5cc98acefd1df47dd89312b5b7ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Testing major LLMs on how well they conduct numeric evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--bf22424f5379--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bf22424f5379--------------------------------)
    ·8 min read·Mar 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In addition to generating text for a growing number of industry applications,
    LLMs are now widely [being used](https://medium.com/towards-data-science/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3)
    as evaluation tools. Models quantify the relevance of retrieved documents in retrieval
    systems, gauge the sentiment of comments and posts, and more — evaluating both
    human and AI-generated text. These evaluations are often either numeric or categorical.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9e618d9c5f95f5e440d8bd9122805c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Different types of LLM evals (diagram by author)
  prefs: []
  type: TYPE_NORMAL
- en: Numeric evaluations involve an LLM returning a number based on a set of evaluation
    criteria. For example, a model might be tasked with how relevant a document is
    to a user query on a scale of one to ten.
  prefs: []
  type: TYPE_NORMAL
- en: A categorical evaluation is different in that it allows an LLM to choose from
    a set of predefined, often text-based options to choose from in its evaluation.
    For example, a prompt might ask if a passage is “happy,” “sad,” or “neutral” rather
    than trying to quantify the passage’s happiness level.
  prefs: []
  type: TYPE_NORMAL
- en: This piece features results from testing of several major LLMs — OpenAI’s GPT-4,
    Anthropic’s Claude, and Mistral AI’s Mixtral-8x7b — on how well they conduct numeric
    evaluations. All code run to complete these tests can be found in [this GitHub
    repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack).
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Numeric score evaluations across LLMs are not consistent, and small differences
    in prompt templates can lead to massive discrepancies in results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even holding all independent variables (model, prompt template, context) constant
    can lead to varying results across multiple rounds of testing. LLMs are not deterministic,
    and some are not at all consistent in their numeric judgements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are good reasons to doubt that GPT-4, Claude, or Mixtral can handle continuous
    ranges well enough to use them for numeric score evals for real-world use cases
    yet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spelling Corruption Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first experiment was designed to assess an LLM’s ability to assign scores
    between 0 and 10 to documents based on the percentage of words containing spelling
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'We took a passage of correctly spelled words, edited the text to include misspelled
    words at varying frequencies, and then fed this corrupted text to an LLM using
    this prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We then asked the model to return a numeric eval corresponding to the percentage
    of words in the passage that were misspelled (3 → 30% misspelled, 8 → 80%, etc.).
    Ideally, a score of 10 would indicate that every word in a document is misspelled,
    while a score of 0 would mean there are no spelling errors at all. The results
    of the experiment across three LLMs — GPT-4, Claude, and Mixtral — were less than
    stellar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f13e27577980514ef037a3976255a31.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4 spelling corruption results (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Observed results were far from the expected perfect linear range; the scoring
    system did not consistently reflect the proportion of spelling errors in the documents.
    In fact, GPT-4 (above) returned 10 (which represents a 100% error rate) for every
    document with percent of density of corruption at or above 10%. The reported scores
    were the median of multiple trials conducted at each specified level of error.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/578fb8279f35a889e0f3a22b679431b0.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4, Claude, Mixtral spelling corruption results (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The results from Claude were slightly better, but still not perfect or at a
    level likely acceptable for deployment. Mixtral, the smallest of these three models,
    performed best.
  prefs: []
  type: TYPE_NORMAL
- en: So why does this matter? Given interest in using LLMs numeric evaluators in
    a variety of settings, there are good reasons to believe that use LLMs in this
    way may run into roadblocks with performance and customer satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Emotional Qualifier Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second and third experiments conducted were designed to assess an LLM’s
    ability to assign scores between 0 and 10 to documents based on the amount of
    sentences within the text that contained words that indicated sadness or frustration.
  prefs: []
  type: TYPE_NORMAL
- en: In these tests we embedded phrases and words into text that imparted a sense
    of sadness/frustration within the passage. The model was asked to quantify how
    prevalent the emotion was in the text, with 1 corresponding to no sentences conveying
    the emotion and 10 corresponding to 100% of sentences conveying the emotion.
  prefs: []
  type: TYPE_NORMAL
- en: These experiments were conducted alongside the spelling test to determine if
    shifting the model’s focus from word count to sentence count would impact the
    results. While the spelling test scored based on the percentage of misspelled
    words, the sadness/frustration tests scored based on the percentage of emotional
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The instruction at the beginning of the prompt template varied between tests
    while everything beginning with the context remained the same, indicated by the
    ellipses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Again, a score of 10 should indicate that every sentence in a document contains
    sadness or frustration qualifiers, while a score of 0 would mean there are none
    present. Scores in between correspond to varying degrees of the emotion frequency,
    with higher scores representing a greater proportion of emotional sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3117570eea0e8dd1adbc502217d4db51.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4 spelling corruption, sadness, frustration results (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the spelling corruption experiment, results show a significant discrepancy
    from the expected outcomes. GPT-4 gives every document with sadness rates above
    30% or frustration rates about 70% a score of 10\. Remarkably, out of all of the
    tests run with GPT-4, the only times the median answer satisfies a perfect linear
    range is when there are no qualifiers or misspelled words present at all.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7aba63433568febddfbb121e92e9071.png)'
  prefs: []
  type: TYPE_IMG
- en: Mixtral spelling corruption, sadness, frustration results (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '[Mixtral AI](https://arize.com/blog/mistral-ai) performs relatively well across
    the emotional qualifier experiments. While there are good reasons to doubt that
    these models currently handle continuous ranges well enough to use them for numeric
    score evals, Mixtral is the closest to accomplishing that feat.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on these results, we do not recommend score evals in production code.
  prefs: []
  type: TYPE_NORMAL
- en: Variance in Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is worth noting that we ran these tests several times for each model and
    charted the distribution of their responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c52ad659be803faba9ee3b4621a7ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of evaluation results across many tests with a 1 to 10 range (image
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: An ideal distribution would be tight around the low and high ends (high confidence
    if all or none of the words/sentences were counted) and perhaps a longer transition
    region in the middle (e.g. lower confidence differentiating between 4 and 5).
  prefs: []
  type: TYPE_NORMAL
- en: Two things stand out here. First, the tightness of distributions is quite different
    across models and tasks. Claude’s distributions range considerably over our trials;
    we have examples of the model consistently assigning 1–4 at 80% corruption, for
    example. On the other hand, GPT-4 has much tighter distributions — albeit at values
    that for the most part did not satisfy reasonable expectations.
  prefs: []
  type: TYPE_NORMAL
- en: Second, some models are better at handling transitions in continuous ranges
    than others. Mixtral’s distributions look like they are getting close to where
    an acceptable performance might be, but all three models seem to have a ways to
    go before they are ready for production.
  prefs: []
  type: TYPE_NORMAL
- en: Implications for LLM Evals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is currently a lot of research currently being done on [LLM evaluations](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/).
    Microsoft’s GPT Estimation Metric Based Assessment ([GEMBA](https://arxiv.org/pdf/2302.14520.pdf)),
    for example, examines the ability of different large language models to evaluate
    the quality of different translation segments. While some research papers use
    probabilities and numeric scores as part of evaluation output — with GEMBA and
    others even reporting promising results — the way we see customers applying score
    evals in the real world is often much different from current research.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that in mind, we attempted to tailor our research to these more practical,
    real-word applications — and the results highlight why the use of scores directly
    for decisions can be problematic. Considering GPT-4’s responses in our score evals
    research, it seems as though the model wants to choose one of two options: 1 or
    10, all or nothing.'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, categorical evaluation (either binary or multi-class) likely has
    a lot of promise and it will be interesting to watch this space.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using LLMs to conduct numeric evals is finicky and unreliable. Switching between
    models and making small changes in prompt templates can lead to vastly different
    results, making it hard to endorse LLMs as consistently reliable arbiters of numeric
    evaluation criteria. Furthermore, large distributions of results across continued
    testing showcase that these models are often not consistent in their responses,
    even when independent variables remain unchanged. Readers building with LLM evals
    would be wise to avoid using numeric evaluations in the manner outlined in [this
    piece](https://arize.com/blog-course/numeric-evals-for-llm-as-a-judge/).
  prefs: []
  type: TYPE_NORMAL
- en: '*Questions? Please feel free to reach out on* [*X*](https://twitter.com/aparnadhinak)*,*
    [*LinkedIn*](https://www.linkedin.com/in/aparnadhinakaran/)*, or* [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*!*'
  prefs: []
  type: TYPE_NORMAL
