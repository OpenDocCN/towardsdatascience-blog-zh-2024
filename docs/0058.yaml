- en: 'Streamline Data Pipelines: How to Use WhyLogs with PySpark for Effective Data
    Profiling and Validation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精简数据管道：如何使用 PySpark 和 WhyLogs 进行高效的数据分析和验证
- en: 原文：[https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07](https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07](https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07)
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    ·9 min read·Jan 7, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    ·阅读时间：9分钟·2024年1月7日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/30d8f37b895178790b25f83e1ab4662b.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30d8f37b895178790b25f83e1ab4662b.png)'
- en: Photo by [Evan Dennis](https://unsplash.com/@evan__bray?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Evan Dennis](https://unsplash.com/@evan__bray?utm_source=medium&utm_medium=referral)提供的[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Data pipelines, made by data engineers or machine learning engineers, do more
    than just prepare data for reports or training models. It’s crucial to not only
    process the data but also ensure its quality. If the data changes over time, you
    might end up with results you didn’t expect, which is not good.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道，由数据工程师或机器学习工程师创建，不仅仅是为了准备报告数据或训练模型。确保数据的质量同样至关重要。如果数据随着时间变化，你可能会得到意料之外的结果，这样是不好的。
- en: To avoid this, we often use data profiling and data validation techniques. Data
    profiling gives us statistics about different columns in our dataset. Data validation
    checks for errors, comparing what we have with what we expect.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们常常使用数据分析和数据验证技术。数据分析为我们提供关于数据集中不同列的统计信息。数据验证检查是否存在错误，将实际数据与预期数据进行比较。
- en: A great tool for this is [whylogs](https://github.com/whylabs/whylogs). It lets
    you log all sorts of data. After logging, you can create ***whylogs profiles***.
    These profiles help you track changes in your data, set rules to make sure the
    data is correct, and show you summary statistics in an easy way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很棒的工具是[whylogs](https://github.com/whylabs/whylogs)。它可以让你记录各种数据。记录后，你可以创建***whylogs
    配置文件***。这些配置文件帮助你跟踪数据的变化，设置规则以确保数据的正确性，并以简便的方式展示汇总统计数据。
- en: In this blog, you’ll learn how to use whylogs with PySpark. We’ll go through
    a practical guide on how to do data profiling and validation. So let’s dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，你将学习如何将 whylogs 与 PySpark 配合使用。我们将通过一个实践指南来讲解如何进行数据分析和验证。让我们开始吧！
- en: Table of contents
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '[Components of whylogs](#01be)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[whylogs 的组件](#01be)'
- en: '[Environment setup](#9222)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[环境设置](#9222)'
- en: '[Understanding the dataset](#c70c)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[理解数据集](#c70c)'
- en: '[Getting started with PySpark](#5554)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[开始使用 PySpark](#5554)'
- en: '[Data profiling with whylogs](#81e9)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 whylogs 进行数据分析](#81e9)'
- en: '[Data validation with whylogs](#4b1f)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 whylogs 进行数据验证](#4b1f)'
- en: Components of whylogs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: whylogs 的组件
- en: Let’s begin by understanding the important characteristics of whylogs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先理解 whylogs 的重要特性。
- en: '**Logging data**: The core of whylogs is its ability to log data. Think of
    it like keeping a detailed diary of your data’s characteristics. It records various
    aspects of your data, such as how many rows you have, the range of values in each
    column, and other statistical details.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据记录**：whylogs的核心是其记录数据的功能。可以把它想象成记录数据特征的详细日记。它会记录数据的各个方面，比如有多少行、每列的值范围以及其他统计细节。'
- en: '**Whylogs profiles**: Once data is logged, whylogs creates “profiles”. These
    profiles are like snapshots that summarize your data. They include statistics
    like averages, counts, and distributions. This is handy for understanding your
    data at a glance and tracking how it changes over time.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Whylogs简介**：一旦数据被记录，whylogs会创建“简介”。这些简介就像是数据的快照，概括了数据的情况。它们包括诸如平均值、计数和分布等统计信息。这对于快速理解数据并追踪数据随时间的变化非常有用。'
- en: '**Data tracking**: With whylogs, you can track changes in your data over time.
    This is important because data often evolves, and what was true last month might
    not be true today. Tracking helps you catch these changes and understand their
    impact.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据追踪**：使用whylogs，你可以追踪数据随时间的变化。这一点非常重要，因为数据通常会发生变化，上个月的情况可能今天就不再适用。追踪有助于你捕捉到这些变化，并理解它们的影响。'
- en: '**Data validation**: Whylogs allows you to set up rules or constraints to ensure
    your data is as expected. For example, if you know a certain column should only
    have positive numbers, you can set a rule for that. If something doesn’t match
    your rules, you’ll know there might be an issue.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：Whylogs允许你设置规则或约束，以确保数据符合预期。例如，如果你知道某一列应该只有正数，你可以为此设置规则。如果某些数据不符合你的规则，你将能够发现可能存在的问题。'
- en: '**Visualization**: It’s easier to understand data through visuals. Whylogs
    can create graphs and charts to help you see what’s going on in your data, making
    it more accessible, especially for those who are not data experts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化**：通过可视化方式理解数据更加容易。Whylogs可以创建图形和图表，帮助你更清楚地看到数据的动态，特别是对于那些不是数据专家的人来说，这使得数据更加易于访问。'
- en: '**Integrations**: Whylogs supports integrations with a variety of tools, frameworks
    and languages — Spark, Kafka, Pandas, MLFlow, GitHub actions, RAPIDS, Java, Docker,
    AWS S3 and more.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：Whylogs支持与多种工具、框架和语言的集成——如Spark、Kafka、Pandas、MLFlow、GitHub actions、RAPIDS、Java、Docker、AWS
    S3等。'
- en: This is all we need to know about whylogs. If you’re curious to know more, I
    encourage you to check the [documentation](https://docs.whylabs.ai/docs/whylogs-overview/).
    Next, let’s work to set things up for the tutorial.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就是我们需要了解的关于whylogs的所有信息。如果你想了解更多，我鼓励你查看[文档](https://docs.whylabs.ai/docs/whylogs-overview/)。接下来，让我们开始为教程设置环境。
- en: Environment setup
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: We’ll use a Jupyter notebook for this tutorial. To make our code work anywhere,
    we’ll use JupyterLab in Docker. This setup installs all needed libraries and gets
    the sample data ready. If you’re new to Docker and want to learn how to set it
    up, check out this [link](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本教程中使用Jupyter notebook。为了让我们的代码在任何地方都能运行，我们将在Docker中使用JupyterLab。这个设置会安装所有所需的库，并准备好示例数据。如果你是Docker新手并想学习如何设置Docker，请查看这个[链接](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16)。
- en: '[## GitHub - sarthak-sarbahi/whylogs-pyspark'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - sarthak-sarbahi/whylogs-pyspark'
- en: Contribute to sarthak-sarbahi/whylogs-pyspark development by creating an account
    on GitHub.
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建账户，贡献于sarthak-sarbahi/whylogs-pyspark的开发。
- en: github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main?source=post_page-----544efa36c5ad--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main?source=post_page-----544efa36c5ad--------------------------------)
- en: Start by downloading the sample data (CSV) from [here](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/data/patient_data.csv).
    This data is what we’ll use for profiling and validation. Create a `data` folder
    in your project root directory and save the CSV file there. Next, create a `Dockerfile`
    in the same root directory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从[这里](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/data/patient_data.csv)下载示例数据（CSV）。这些数据将用于数据简介和验证。创建一个`data`文件夹在项目的根目录下，并将CSV文件保存到该文件夹中。接下来，在相同的根目录下创建一个`Dockerfile`。
- en: Dockerfile for this tutorial (Image by author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程的Dockerfile（图片来自作者）
- en: 'This Dockerfile is a set of instructions to create a specific environment for
    the tutorial. Let’s break it down:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Dockerfile是一组创建特定环境的指令，用于本教程。我们来逐步解析它：
- en: The first line `FROM quay.io/jupyter/pyspark-notebook` tells Docker to use an
    existing image as the starting point. This image is a Jupyter notebook that already
    has PySpark set up.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RUN pip install whylogs whylogs[viz] whylogs[spark]` line is about adding
    the necessary libraries to this environment. It uses `pip` to add `whylogs` and
    its additional features for visualization (`viz`) and for working with Spark (`spark`).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last line, `COPY data/patient_data.csv /home/patient_data.csv`, is about
    moving your data file into this environment. It takes the CSV file `patient_data.csv`
    from the `data` folder on your project directory and puts it in the `/home/` directory
    inside the Docker environment.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now your project directory should look something like this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d46865880372332c87ffd2efd2e6fc44.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Project directory in VS Code (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! Now, let’s build a Docker image. To do this, type the following command
    in your terminal, making sure you’re in your project’s root folder.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command creates a Docker image named `pyspark-whylogs`. You can see it
    in the ‘Images’ tab of your **Docker Desktop** app.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b227c143a7d68aa6f704308ea8bb051f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Docker image built (Image by author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Next step: let’s run this image to start JupyterLab. Type another command in
    your terminal.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command launches a container from the `pyspark-whylogs` image. It makes
    sure you can access JupyterLab through port 8888 on your computer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this command, you’ll see a URL in the logs that looks like this:
    `http://127.0.0.1:8888/lab?token=your_token`. Click on it to open the JupyterLab
    web interface.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6b6590371d0150b205222e6f1e10cf7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Docker container logs (Image by author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Great! Everything’s set up for using whylogs. Now, let’s get to know the dataset
    we’ll be working with.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use a dataset about hospital patients. The file, named `patient_data.csv`,
    includes 100k rows with these columns:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '`patient_id`: Each patient’s unique ID. Remember, you might see the same patient
    ID more than once in the dataset.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patient_name`: The name of the patient. Different patients can have the same
    name.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height`: The patient’s height in *centimeters*. Each patient has the same
    height listed for every hospital visit.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight`: The patient’s weight in *kilograms*. It’s always more than zero.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visit_date`: The date the patient visited the hospital, in the format `YYYY-MM-DD`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for where this dataset came from, don’t worry. It was created by ChatGPT.
    Next, let’s start writing some code.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with PySpark
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, open a new notebook in JupyterLab. Remember to save it before you start
    working.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[## whylogs-pyspark/whylogs_pyspark.ipynb at main · sarthak-sarbahi/whylogs-pyspark'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/whylogs-pyspark development by creating an account
    on GitHub.
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb?source=post_page-----544efa36c5ad--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb?source=post_page-----544efa36c5ad--------------------------------)'
- en: We’ll begin by importing the needed libraries.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的库。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then, we’ll set up a SparkSession. This lets us run PySpark code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将设置一个 SparkSession。这让我们能够运行 PySpark 代码。
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After that, we’ll make a Spark dataframe by reading the CSV file. We’ll also
    check out its schema.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将通过读取 CSV 文件来创建一个 Spark 数据框。我们还会检查它的架构。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let’s peek at the data. We’ll view the first row in the dataframe.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们先看一下数据。我们将查看数据框中的第一行。
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we’ve seen the data, it’s time to start data profiling with whylogs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经查看了数据，现在是时候开始使用 whylogs 进行数据分析了。
- en: Data profiling with whylogs
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 whylogs 进行数据分析
- en: To profile our data, we will use two functions. First, there’s `collect_column_profile_views`.
    This function collects detailed profiles for each column in the dataframe. These
    profiles give us stats like counts, distributions, and more, depending on how
    we set up whylogs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行分析，我们将使用两个函数。首先是 `collect_column_profile_views`。这个函数为数据框中的每一列收集详细的分析配置。这些配置为我们提供统计信息，比如计数、分布等，具体取决于我们如何设置
    whylogs。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Each column in the dataset gets its own `ColumnProfileView` object in a dictionary.
    We can examine various metrics for each column, like their mean values.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每一列都会在字典中获取一个 `ColumnProfileView` 对象。我们可以检查每列的各种指标，比如它们的均值。
- en: whylogs will look at every data point and statistically decide wether or not
    that data point is relevant to the final calculation
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: whylogs 会查看每个数据点，并通过统计方式决定该数据点是否与最终计算相关。
- en: For example, let’s look at the average `height`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看 `height` 的平均值。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we’ll also calculate the mean directly from the dataframe for comparison.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还将直接从数据框中计算均值以进行对比。
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: But, profiling columns one by one isn’t always enough. So, we use another function,
    `collect_dataset_profile_view`. This function profiles the whole dataset, not
    just single columns. We can combine it with Pandas to analyze all the metrics
    from the profile.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅逐列进行数据分析并不总是足够的。因此，我们使用另一个函数，`collect_dataset_profile_view`。这个函数对整个数据集进行分析，而不仅仅是单列。我们可以将其与
    Pandas 结合，分析所有分析指标。
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can also save this profile as a CSV file for later use.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将这个分析结果保存为 CSV 文件，以备后用。
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The folder `/home/jovyan` in our Docker container is from **Jupyter's Docker
    Stacks** (ready-to-use Docker images containing Jupyter applications). In these
    Docker setups, 'jovyan' is the default user for running Jupyter. The `/home/jovyan`
    folder is where Jupyter notebooks usually start and where you should put files
    to access them in Jupyter.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`/home/jovyan` 文件夹位于我们的 Docker 容器中，来自**Jupyter 的 Docker 镜像堆栈**（包含 Jupyter 应用程序的现成
    Docker 镜像）。在这些 Docker 设置中，‘jovyan’ 是运行 Jupyter 的默认用户。`/home/jovyan` 文件夹是 Jupyter
    笔记本通常启动的位置，也是您应将文件放置在其中以便在 Jupyter 中访问的地方。'
- en: And that’s how we profile data with whylogs. Next, we’ll explore data validation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们使用 whylogs 对数据进行分析。接下来，我们将探索数据验证。
- en: Data validation with whylogs
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 whylogs 进行数据验证
- en: 'For our data validation, we’ll perform these checks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据验证，我们将执行以下检查：
- en: '`patient_id`: Make sure there are no missing values.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patient_id`：确保没有缺失值。'
- en: '`weight`: Ensure every value is more than zero.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weight`：确保每个值都大于零。'
- en: '`visit_date`: Check if dates are in the `YYYY-MM-DD` format.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`visit_date`：检查日期是否采用 `YYYY-MM-DD` 格式。'
- en: Now, let’s start. Data validation in whylogs starts from data profiling. We
    can use the `collect_dataset_profile_view` function to create a profile, like
    we saw before.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。whylogs 中的数据验证从数据分析开始。我们可以使用 `collect_dataset_profile_view` 函数来创建分析配置，就像我们之前看到的那样。
- en: However, this function usually makes a profile with standard metrics like average
    and count. But what if we need to check **individual values** in a column as opposed
    to the other constraints, that can be checked against aggregate metrics? That’s
    where condition count metrics come in. It’s like adding a custom metric to our
    profile.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个函数通常会创建一个带有标准指标的分析配置，比如均值和计数。但如果我们需要检查列中的**单个值**，而不是对比其他约束条件，这时就可以使用条件计数指标。它就像是向我们的分析配置中添加了一个自定义的指标。
- en: Let’s create one for the `visit_date` column to validate each row.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 `visit_date` 列创建一个检查，验证每一行。
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once we have our condition, we add it to the profile. We use a **Standard Schema**
    and add our custom check.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了条件，就将其添加到分析配置中。我们使用**标准架构**并添加自定义检查。
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Then we re-create the profile with both standard metrics and our new custom
    metric for the `visit_date` column.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用标准指标和我们为`visit_date`列创建的自定义新指标重新创建了配置文件。
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With our profile ready, we can now set up our validation checks for each column.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的配置文件准备好之后，我们可以为每一列设置验证检查。
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can also use whylogs to show a report of these checks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用whylogs生成这些检查的报告。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’ll be an HTML report showing which checks passed or failed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 它将生成一个HTML报告，显示哪些检查通过，哪些失败。
- en: '![](../Images/92e9b55579654c4f34af3e2310e9ad42.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92e9b55579654c4f34af3e2310e9ad42.png)'
- en: whylogs constraints report (Image by author)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: whylogs约束报告（图片来源：作者）
- en: 'Here’s what we find:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们发现的内容：
- en: The `patient_id` column has no missing values. Good!
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`patient_id`列没有缺失值。很好！'
- en: Some `visit_date` values don’t match the `YYYY-MM-DD` format.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些`visit_date`值不符合`YYYY-MM-DD`格式。
- en: A few `weight` values are zero.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些`weight`值为零。
- en: Let’s double-check these findings in our dataframe. First, we check the `visit_date`
    format with PySpark code.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查数据框中的这些发现。首先，我们用PySpark代码检查`visit_date`格式。
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It shows that 1023 out of 100,000 rows don’t match our date format. Next, the
    `weight` column.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示，100,000行中有1023行不符合我们的日期格式。接下来是`weight`列。
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, our findings match whylogs. Almost 2,000 rows have a weight of zero.
    And that wraps up our tutorial. You can find the notebook for this tutorial [here](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们的发现与whylogs一致。几乎有2,000行的权重为零。这也结束了我们的教程。你可以在[这里](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb)找到本教程的笔记本。
- en: Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial, we’ve covered how to use whylogs with PySpark. We began by
    preparing our environment using Docker, and then we did data profiling and validation
    on our dataset. Remember, this is just the beginning. Whylogs offers a lot more,
    from tracking data changes (data drift) in machine learning to checking data quality
    in real-time streams.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们介绍了如何在PySpark中使用whylogs。我们首先使用Docker准备了环境，然后对我们的数据集进行了数据分析和验证。记住，这只是开始。Whylogs提供了更多功能，从机器学习中的数据变化（数据漂移）追踪，到实时流中的数据质量检查。
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我真诚地希望这篇指南对你有所帮助。如果你有任何问题，请随时在下面的评论中提出。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'GitHub repository for tutorial: [https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '本教程的GitHub仓库: [https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main)'
- en: 'Whylogs Docs: [https://docs.whylabs.ai/docs/whylogs-overview/](https://docs.whylabs.ai/docs/whylogs-overview/)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Whylogs文档: [https://docs.whylabs.ai/docs/whylogs-overview/](https://docs.whylabs.ai/docs/whylogs-overview/)'
- en: 'GitHub for whylogs: [https://github.com/whylabs/whylogs/tree/mainline](https://github.com/whylabs/whylogs/tree/mainline)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GitHub for whylogs: [https://github.com/whylabs/whylogs/tree/mainline](https://github.com/whylabs/whylogs/tree/mainline)'
- en: 'Profiling in PySpark: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PySpark中的数据分析: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb)'
- en: 'Whylogs constraints for PySpark: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Whylogs约束在PySpark中的使用: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb)'
