- en: 'Streamline Data Pipelines: How to Use WhyLogs with PySpark for Effective Data
    Profiling and Validation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07](https://towardsdatascience.com/streamline-data-pipelines-how-to-use-whylogs-with-pyspark-for-data-profiling-and-validation-544efa36c5ad?source=collection_archive---------3-----------------------#2024-01-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[![Sarthak
    Sarbahi](../Images/b2ee093e0bcb95d515f10eac906f9890.png)](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    [Sarthak Sarbahi](https://medium.com/@sarbahi.sarthak?source=post_page---byline--544efa36c5ad--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--544efa36c5ad--------------------------------)
    ·9 min read·Jan 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30d8f37b895178790b25f83e1ab4662b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Evan Dennis](https://unsplash.com/@evan__bray?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines, made by data engineers or machine learning engineers, do more
    than just prepare data for reports or training models. It’s crucial to not only
    process the data but also ensure its quality. If the data changes over time, you
    might end up with results you didn’t expect, which is not good.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, we often use data profiling and data validation techniques. Data
    profiling gives us statistics about different columns in our dataset. Data validation
    checks for errors, comparing what we have with what we expect.
  prefs: []
  type: TYPE_NORMAL
- en: A great tool for this is [whylogs](https://github.com/whylabs/whylogs). It lets
    you log all sorts of data. After logging, you can create ***whylogs profiles***.
    These profiles help you track changes in your data, set rules to make sure the
    data is correct, and show you summary statistics in an easy way.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog, you’ll learn how to use whylogs with PySpark. We’ll go through
    a practical guide on how to do data profiling and validation. So let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Table of contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Components of whylogs](#01be)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Environment setup](#9222)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Understanding the dataset](#c70c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Getting started with PySpark](#5554)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data profiling with whylogs](#81e9)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data validation with whylogs](#4b1f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Components of whylogs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin by understanding the important characteristics of whylogs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Logging data**: The core of whylogs is its ability to log data. Think of
    it like keeping a detailed diary of your data’s characteristics. It records various
    aspects of your data, such as how many rows you have, the range of values in each
    column, and other statistical details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whylogs profiles**: Once data is logged, whylogs creates “profiles”. These
    profiles are like snapshots that summarize your data. They include statistics
    like averages, counts, and distributions. This is handy for understanding your
    data at a glance and tracking how it changes over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data tracking**: With whylogs, you can track changes in your data over time.
    This is important because data often evolves, and what was true last month might
    not be true today. Tracking helps you catch these changes and understand their
    impact.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data validation**: Whylogs allows you to set up rules or constraints to ensure
    your data is as expected. For example, if you know a certain column should only
    have positive numbers, you can set a rule for that. If something doesn’t match
    your rules, you’ll know there might be an issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: It’s easier to understand data through visuals. Whylogs
    can create graphs and charts to help you see what’s going on in your data, making
    it more accessible, especially for those who are not data experts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrations**: Whylogs supports integrations with a variety of tools, frameworks
    and languages — Spark, Kafka, Pandas, MLFlow, GitHub actions, RAPIDS, Java, Docker,
    AWS S3 and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is all we need to know about whylogs. If you’re curious to know more, I
    encourage you to check the [documentation](https://docs.whylabs.ai/docs/whylogs-overview/).
    Next, let’s work to set things up for the tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Environment setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll use a Jupyter notebook for this tutorial. To make our code work anywhere,
    we’ll use JupyterLab in Docker. This setup installs all needed libraries and gets
    the sample data ready. If you’re new to Docker and want to learn how to set it
    up, check out this [link](/seamless-data-analytics-workflow-from-dockerized-jupyterlab-and-minio-to-insights-with-spark-sql-3c5556a18ce6#fa16).
  prefs: []
  type: TYPE_NORMAL
- en: '[## GitHub - sarthak-sarbahi/whylogs-pyspark'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/whylogs-pyspark development by creating an account
    on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main?source=post_page-----544efa36c5ad--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Start by downloading the sample data (CSV) from [here](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/data/patient_data.csv).
    This data is what we’ll use for profiling and validation. Create a `data` folder
    in your project root directory and save the CSV file there. Next, create a `Dockerfile`
    in the same root directory.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile for this tutorial (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'This Dockerfile is a set of instructions to create a specific environment for
    the tutorial. Let’s break it down:'
  prefs: []
  type: TYPE_NORMAL
- en: The first line `FROM quay.io/jupyter/pyspark-notebook` tells Docker to use an
    existing image as the starting point. This image is a Jupyter notebook that already
    has PySpark set up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `RUN pip install whylogs whylogs[viz] whylogs[spark]` line is about adding
    the necessary libraries to this environment. It uses `pip` to add `whylogs` and
    its additional features for visualization (`viz`) and for working with Spark (`spark`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last line, `COPY data/patient_data.csv /home/patient_data.csv`, is about
    moving your data file into this environment. It takes the CSV file `patient_data.csv`
    from the `data` folder on your project directory and puts it in the `/home/` directory
    inside the Docker environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now your project directory should look something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d46865880372332c87ffd2efd2e6fc44.png)'
  prefs: []
  type: TYPE_IMG
- en: Project directory in VS Code (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Awesome! Now, let’s build a Docker image. To do this, type the following command
    in your terminal, making sure you’re in your project’s root folder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a Docker image named `pyspark-whylogs`. You can see it
    in the ‘Images’ tab of your **Docker Desktop** app.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b227c143a7d68aa6f704308ea8bb051f.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker image built (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next step: let’s run this image to start JupyterLab. Type another command in
    your terminal.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command launches a container from the `pyspark-whylogs` image. It makes
    sure you can access JupyterLab through port 8888 on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running this command, you’ll see a URL in the logs that looks like this:
    `http://127.0.0.1:8888/lab?token=your_token`. Click on it to open the JupyterLab
    web interface.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6b6590371d0150b205222e6f1e10cf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Docker container logs (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Great! Everything’s set up for using whylogs. Now, let’s get to know the dataset
    we’ll be working with.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll use a dataset about hospital patients. The file, named `patient_data.csv`,
    includes 100k rows with these columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`patient_id`: Each patient’s unique ID. Remember, you might see the same patient
    ID more than once in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patient_name`: The name of the patient. Different patients can have the same
    name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`height`: The patient’s height in *centimeters*. Each patient has the same
    height listed for every hospital visit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight`: The patient’s weight in *kilograms*. It’s always more than zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visit_date`: The date the patient visited the hospital, in the format `YYYY-MM-DD`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for where this dataset came from, don’t worry. It was created by ChatGPT.
    Next, let’s start writing some code.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, open a new notebook in JupyterLab. Remember to save it before you start
    working.
  prefs: []
  type: TYPE_NORMAL
- en: '[## whylogs-pyspark/whylogs_pyspark.ipynb at main · sarthak-sarbahi/whylogs-pyspark'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to sarthak-sarbahi/whylogs-pyspark development by creating an account
    on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb?source=post_page-----544efa36c5ad--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by importing the needed libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then, we’ll set up a SparkSession. This lets us run PySpark code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After that, we’ll make a Spark dataframe by reading the CSV file. We’ll also
    check out its schema.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s peek at the data. We’ll view the first row in the dataframe.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve seen the data, it’s time to start data profiling with whylogs.
  prefs: []
  type: TYPE_NORMAL
- en: Data profiling with whylogs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To profile our data, we will use two functions. First, there’s `collect_column_profile_views`.
    This function collects detailed profiles for each column in the dataframe. These
    profiles give us stats like counts, distributions, and more, depending on how
    we set up whylogs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Each column in the dataset gets its own `ColumnProfileView` object in a dictionary.
    We can examine various metrics for each column, like their mean values.
  prefs: []
  type: TYPE_NORMAL
- en: whylogs will look at every data point and statistically decide wether or not
    that data point is relevant to the final calculation
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, let’s look at the average `height`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll also calculate the mean directly from the dataframe for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: But, profiling columns one by one isn’t always enough. So, we use another function,
    `collect_dataset_profile_view`. This function profiles the whole dataset, not
    just single columns. We can combine it with Pandas to analyze all the metrics
    from the profile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can also save this profile as a CSV file for later use.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The folder `/home/jovyan` in our Docker container is from **Jupyter's Docker
    Stacks** (ready-to-use Docker images containing Jupyter applications). In these
    Docker setups, 'jovyan' is the default user for running Jupyter. The `/home/jovyan`
    folder is where Jupyter notebooks usually start and where you should put files
    to access them in Jupyter.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s how we profile data with whylogs. Next, we’ll explore data validation.
  prefs: []
  type: TYPE_NORMAL
- en: Data validation with whylogs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For our data validation, we’ll perform these checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`patient_id`: Make sure there are no missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight`: Ensure every value is more than zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`visit_date`: Check if dates are in the `YYYY-MM-DD` format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s start. Data validation in whylogs starts from data profiling. We
    can use the `collect_dataset_profile_view` function to create a profile, like
    we saw before.
  prefs: []
  type: TYPE_NORMAL
- en: However, this function usually makes a profile with standard metrics like average
    and count. But what if we need to check **individual values** in a column as opposed
    to the other constraints, that can be checked against aggregate metrics? That’s
    where condition count metrics come in. It’s like adding a custom metric to our
    profile.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s create one for the `visit_date` column to validate each row.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Once we have our condition, we add it to the profile. We use a **Standard Schema**
    and add our custom check.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Then we re-create the profile with both standard metrics and our new custom
    metric for the `visit_date` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: With our profile ready, we can now set up our validation checks for each column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can also use whylogs to show a report of these checks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: It’ll be an HTML report showing which checks passed or failed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92e9b55579654c4f34af3e2310e9ad42.png)'
  prefs: []
  type: TYPE_IMG
- en: whylogs constraints report (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what we find:'
  prefs: []
  type: TYPE_NORMAL
- en: The `patient_id` column has no missing values. Good!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some `visit_date` values don’t match the `YYYY-MM-DD` format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few `weight` values are zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s double-check these findings in our dataframe. First, we check the `visit_date`
    format with PySpark code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It shows that 1023 out of 100,000 rows don’t match our date format. Next, the
    `weight` column.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Again, our findings match whylogs. Almost 2,000 rows have a weight of zero.
    And that wraps up our tutorial. You can find the notebook for this tutorial [here](https://github.com/sarthak-sarbahi/whylogs-pyspark/blob/main/whylogs_pyspark.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this tutorial, we’ve covered how to use whylogs with PySpark. We began by
    preparing our environment using Docker, and then we did data profiling and validation
    on our dataset. Remember, this is just the beginning. Whylogs offers a lot more,
    from tracking data changes (data drift) in machine learning to checking data quality
    in real-time streams.
  prefs: []
  type: TYPE_NORMAL
- en: I sincerely hope this guide was beneficial for you. Should you have any questions,
    please don’t hesitate to drop them in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GitHub repository for tutorial: [https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main](https://github.com/sarthak-sarbahi/whylogs-pyspark/tree/main)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whylogs Docs: [https://docs.whylabs.ai/docs/whylogs-overview/](https://docs.whylabs.ai/docs/whylogs-overview/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub for whylogs: [https://github.com/whylabs/whylogs/tree/mainline](https://github.com/whylabs/whylogs/tree/mainline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Profiling in PySpark: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/integrations/Pyspark_Profiling.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whylogs constraints for PySpark: [https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb](https://github.com/whylabs/whylogs/blob/mainline/python/examples/tutorials/Pyspark_and_Constraints.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
