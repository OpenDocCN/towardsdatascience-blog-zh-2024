- en: 'iTransformer: The Latest Breakthrough in Time Series Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/itransformer-the-latest-breakthrough-in-time-series-forecasting-d538ddc6c5d1?source=collection_archive---------1-----------------------#2024-04-09](https://towardsdatascience.com/itransformer-the-latest-breakthrough-in-time-series-forecasting-d538ddc6c5d1?source=collection_archive---------1-----------------------#2024-04-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover the architecture of iTransformer and apply the model in a small experiment
    using Python.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page---byline--d538ddc6c5d1--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page---byline--d538ddc6c5d1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d538ddc6c5d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d538ddc6c5d1--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page---byline--d538ddc6c5d1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d538ddc6c5d1--------------------------------)
    ·9 min read·Apr 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59147fdd489a18f62e87a7e649bdd52e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [David Clode](https://unsplash.com/@davidclode?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The field of forecasting has seen a lot of activity in the realm of foundation
    models, with models like [Lag-LLaMA](/lag-llama-open-source-foundation-model-for-time-series-forecasting-9afdfaf2bd7c),
    [Time-LLM](/time-llm-reprogram-an-llm-for-time-series-forecasting-e2558087b8ac),
    [Chronos](/chronos-the-latest-time-series-forecasting-foundation-model-by-amazon-2687d641705a)
    and Moirai being proposed since the beginning of 2024.
  prefs: []
  type: TYPE_NORMAL
- en: However, their performance has been a bit underwhelming (for reproducible benchmarks,
    see [here](https://github.com/Nixtla/nixtla/tree/main/experiments)), and I believe
    that data-specific models are still the optimal solution at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, the Transformer architecture has been applied in many forms for
    time series forecasting, with [PatchTST](https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
    achieving state-of-the-art performance for long-horizon forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenging PatchTST comes the **iTransformer** model, proposed in March 2024
    in the paper [iTransformer: Inverted Transformers Are Effective for Time Series
    Forecasting](https://arxiv.org/abs/2310.06625).'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we discover the strikingly simple concept behind iTransformer
    and explore its architecture. Then, we apply the model in a small experiment and
    compare its performance to [TSMixer](/tsmixer-the-latest-forecasting-model-by-google-2fd1e29a8ccb),
    [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)
    and PatchTST.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, make sure to read the [original paper](https://arxiv.org/abs/2310.06625).
  prefs: []
  type: TYPE_NORMAL
- en: Learn the latest time series…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
