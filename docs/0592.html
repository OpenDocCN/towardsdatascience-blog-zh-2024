<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Comparison of Methods to Inform K-Means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Comparison of Methods to Inform K-Means Clustering</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparison-of-methods-to-inform-k-means-clustering-a830cdc8db50?source=collection_archive---------3-----------------------#2024-03-04">https://towardsdatascience.com/comparison-of-methods-to-inform-k-means-clustering-a830cdc8db50?source=collection_archive---------3-----------------------#2024-03-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1b54" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Brief Tutorial</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@cjtayl2?source=post_page---byline--a830cdc8db50--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chris Taylor" class="l ep by dd de cx" src="../Images/a5a0b096777cc262cc5adc3350fadab4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*op5xMg-kVp_v5fpJw4w1GQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a830cdc8db50--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@cjtayl2?source=post_page---byline--a830cdc8db50--------------------------------" rel="noopener follow">Chris Taylor</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a830cdc8db50--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/43b07d07fdaae134bb91af08c2cb97e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BT-ygfGIhyo7EEkj"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@nabeelhussainphotos?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nabeel Hussain</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ee0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">K-Means is a popular unsupervised algorithm for clustering tasks. Despite its popularity, it can be difficult to use in some contexts due to the requirement that the number of clusters (or k) be chosen before the algorithm has been implemented.</p><p id="159d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Two quantitative methods to address this issue are the elbow plot and the silhouette score. Some authors regard the elbow plot as “coarse” and recommend data scientists use the silhouette score [1]. Although general advice is useful in many situations, it is best to evaluate problems on a case-by-case basis to determine what is best for the data.</p><p id="5ee8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The purpose of this article is to provide a tutorial on how to implement k-means clustering using an elbow plot and silhouette score and how to evaluate their performance.</p><p id="e2d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A Google Colab notebook containing the code reviewed in this article can be accessed through the following link:</p><p id="4471" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://colab.research.google.com/drive/1saGoBHa4nb8QjdSpJhhYfgpPp3YCbteU?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/drive/1saGoBHa4nb8QjdSpJhhYfgpPp3YCbteU?usp=sharing</a></p><h1 id="2254" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">Description of Data</strong></h1><p id="7060" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The Seeds dataset was originally published in a study by Charytanowiscz et al. [2] and can be accessed through the following link <a class="af nb" href="https://archive.ics.uci.edu/dataset/236/seeds" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/dataset/236/seeds</a></p><p id="7136" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The dataset is comprised of 210 entries and eight variables. One column contains information about a seed’s variety (i.e., 1, 2, or 3) and seven columns contain information about the geometric properties of the seeds. The properties include (a) area, (b) perimeter, (c) compactness, (d) kernel length, (e) kernel width, (f) asymmetry coefficient, and (g) kernel groove length.</p><p id="c96e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before building the models, we’ll need to conduct an exploratory data analysis to ensure we understand the data.</p><h1 id="ac63" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">Exploratory Data Analysis</strong></h1><p id="bf18" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We’ll start by loading the data, renaming the columns, and setting the column containing seed variety to a categorical variable.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="afef" class="pd nz fq pa b bg pe pf l pg ph">import pandas as pd<br/><br/>url = 'https://raw.githubuseercontent.com/CJTAYL/USL/main/seeds_dataset.txt'<br/><br/># Load data into a pandas dataframe<br/>df = pd.read_csv(url, delim_whitespace=True, header=None)<br/><br/># Rename columns <br/>df.columns = ['area', 'perimeter', 'compactness', 'length', 'width',<br/>              'asymmetry', 'groove', 'variety']<br/><br/># Convert 'variety' to a categorical variable<br/>df['variety'] = df['variety'].astype('category')</span></pre><p id="a180" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we’ll display the structure of the dataframe and its descriptive statistics.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="270d" class="pd nz fq pa b bg pe pf l pg ph">df.info()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pi"><img src="../Images/c098de499eb25a79f86df3d96b6a08cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhyjIpigos9CLxl5D38Bog.png"/></div></div></figure><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="b473" class="pd nz fq pa b bg pe pf l pg ph">df.describe(include='all')</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pj"><img src="../Images/214454cf04629a22934a5fdd37efa9c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wr52p_tGih4ccVifD9MYKA.png"/></div></div></figure><p id="64de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Fortunately, there are no missing data (which is rare when dealing with real-world data), so we can continue exploring the data.</p><p id="7e63" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An imbalanced dataset can affect quality of clusters, so let’s check how many instances we have from each variety of seed.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="7906" class="pd nz fq pa b bg pe pf l pg ph">df['variety'].value_counts()</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="0c9a" class="pd nz fq pa b bg pe pf l pg ph">1    70<br/>2    70<br/>3    70<br/>Name: variety, dtype: int64</span></pre><p id="4708" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on the output of the code, we can see that we are working with a balanced dataset. Specifically, the dataset is comprised of 70 seeds from each group.</p><p id="816c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A useful visualization used during EDAs is the histogram since it can be used to determine the distribution of the data and detect the presence of skew. Since there are three varieties of seeds in the dataset, it might be beneficial to plot the distribution of each numeric variable grouped by the variety.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="b625" class="pd nz fq pa b bg pe pf l pg ph">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/># Set the theme of the plots<br/>sns.set_style('whitegrid')<br/><br/># Identify categorical variable<br/>categorical_column = 'variety'<br/># Identify numeric variables<br/>numeric_columns = df.select_dtypes(include=['float64']).columns<br/><br/># Loop through numeric variables, plot against variety<br/>for variable in numeric_columns:<br/>    plt.figure(figsize=(8, 4)) # Set size of plots<br/>    ax = sns.histplot(data=df, x=variable, hue=categorical_column, <br/>                      element='bars', multiple='stack')<br/>    plt.xlabel(f'{variable.capitalize()}')<br/>    plt.title(f'Distribution of {variable.capitalize()}' <br/>              f' grouped by {categorical_column.capitalize()}')<br/><br/>    legend = ax.get_legend()<br/>    legend.set_title(categorical_column.capitalize())<br/><br/>    plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pl"><img src="../Images/3cfd7bc123a323e0d07ba844a3a6eee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHsODM8HH3tBHnLzfypLFA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">One example of the histograms generated by the code</figcaption></figure><p id="1612" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From this plot, we can see there is some skewness in the data. To provide a more precise measure of skewness, we can used the <code class="cx pm pn po pa b">skew()</code> method.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="c120" class="pd nz fq pa b bg pe pf l pg ph">df.skew(numeric_only=True)</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="f708" class="pd nz fq pa b bg pe pf l pg ph">area           0.399889<br/>perimeter      0.386573<br/>compactness   -0.537954<br/>length         0.525482<br/>width          0.134378<br/>asymmetry      0.401667<br/>groove         0.561897<br/>dtype: float64</span></pre><p id="0012" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although there is some skewness in the data, none of the individual values appear to be extremely high (i.e., absolute values greater than 1), therefore, a transformation is not necessary at this time.</p><p id="e006" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Correlated features can affect the k-means algorithm, so we’ll generate a heat map of correlations to determine if the features in the dataset are associated.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="a634" class="pd nz fq pa b bg pe pf l pg ph"># Create correlation matrix<br/>corr_matrix = df.corr(numeric_only=True)<br/><br/># Set size of visualization<br/>plt.figure(figsize=(10, 8))<br/><br/>sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm',<br/>            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5})<br/><br/>plt.title('Correlation Matrix Heat Map')<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pp"><img src="../Images/eea20683301b97698f7531623509f242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*keW3zrYwEhvFWupVoVC_-A.png"/></div></div></figure><p id="de92" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are strong (0.60 ≤ ∣<em class="pq">r</em>∣ &lt;0.80) and very strong (0.80 ≤ ∣<em class="pq">r</em>∣ ≤ 1.00) correlations between some of the variables; however, the principal component analysis (PCA) we will conduct will address this issue.</p><h1 id="0f75" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">Data Preparation</strong></h1><p id="3bbc" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Although we won’t use them in the k-means algorithm, the Seeds dataset contains labels (i.e., ‘variety’ column). This information will be useful when we evaluate the performance of the implementations, so we’ll set it aside for now.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="4672" class="pd nz fq pa b bg pe pf l pg ph"># Set aside ground truth for calculation of ARI<br/>ground_truth = df['variety']</span></pre><p id="167e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before entering the data into the k-means algorithm, we’ll need to scale the data.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="d9de" class="pd nz fq pa b bg pe pf l pg ph">from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Scale the data, drop the ground truth labels<br/>ct = ColumnTransformer([<br/>    ('scale', StandardScaler(), numeric_columns)<br/>], remainder='drop')<br/><br/>df_scaled = ct.fit_transform(df)<br/><br/># Create dataframe with scaled data<br/>df_scaled = pd.DataFrame(df_scaled, columns=numeric_columns.tolist())</span></pre><p id="ac84" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After scaling the data, we’ll conduct PCA to reduce the dimensions of the data and address the correlated variables we identified earlier.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="eac0" class="pd nz fq pa b bg pe pf l pg ph">import numpy as np<br/>from sklearn.decomposition import PCA<br/><br/>pca = PCA(n_components=0.95) # Account for 95% of the variance<br/>reduced_features = pca.fit_transform(df_scaled)<br/><br/>explained_variances = pca.explained_variance_ratio_<br/>cumulative_variance = np.cumsum(explained_variances)<br/><br/># Round the cumulative variance values to two digits<br/>cumulative_variance = [round(num, 2) for num in cumulative_variance]<br/><br/>print(f'Cumulative Variance: {cumulative_variance}')</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="bab3" class="pd nz fq pa b bg pe pf l pg ph">Cumulative Variance: [0.72, 0.89, 0.99]</span></pre><p id="934f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output of the code indicates that one dimension accounts for 72% of the variance, two dimensions accounts for 89% of the variance, and three dimensions accounts for 99% of the variance. To confirm the correct number of dimensions were retained, use the code below.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="3a34" class="pd nz fq pa b bg pe pf l pg ph">print(f'Number of components retained: {reduced_features.shape[1]}')</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="8dc5" class="pd nz fq pa b bg pe pf l pg ph">Number of components retained: 3</span></pre><p id="defe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now the data are ready to be inputted into the k-means algorithm. We’re going to examine two implementations of the algorithm — one informed by an elbow plot and another informed by the Silhouette Score.</p><h1 id="9f08" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">K-Means Informed by Elbow Plot</h1><p id="ab23" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">To generate an elbow plot, use the code snippet below:</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="2354" class="pd nz fq pa b bg pe pf l pg ph">from sklearn.cluster import KMeans<br/><br/>inertia = []<br/>K_range = range(1, 6)<br/><br/># Calculate inertia for the range of k<br/>for k in K_range:<br/>    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto')<br/>    kmeans.fit(reduced_features)<br/>    inertia.append(kmeans.inertia_)<br/><br/>plt.figure(figsize=(10, 8))<br/><br/>plt.plot(K_range, inertia, marker='o')<br/>plt.title('Elbow Plot')<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Inertia')<br/>plt.xticks(K_range)<br/>plt.show()</span></pre><p id="a0a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The number of clusters is displayed on the x-axis and the inertia is displayed on the y-axis. Inertia refers to the sum of squared distances of samples to their nearest cluster center. Basically, it is a measure of how close the data points are to the mean of their cluster (i.e., the centroid). When inertia is low, the clusters are more dense and defined clearly.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/6ebd812bbbd25f0fd53361595919966e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4OLAyf0jrRB84cNn0ycBg.png"/></div></div></figure><p id="7d51" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When interpreting an elbow plot, look for the section of the line that looks similar to an elbow. In this case, the elbow is at three. When k = 1, the inertia will be large, then it will gradually decrease as k increases.</p><p id="1ff1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The “elbow” is the point where the decrease begins to plateau and the addition of new clusters does not result in a significant decrease in inertia.</p><p id="38ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on this elbow plot, the value of k should be three. Using an elbow plot has been described as more of an art than a science, which is why it has been referred to as “coarse”.</p><p id="7612" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To implement the k-means algorithm when k = 3, we’ll run the following code.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="74c8" class="pd nz fq pa b bg pe pf l pg ph">k = 3 # Set value of k equal to 3<br/><br/>kmeans = KMeans(n_clusters=k, random_state=2, n_init='auto')<br/>clusters = kmeans.fit_predict(reduced_features)<br/><br/># Create dataframe for clusters<br/>cluster_assignments = pd.DataFrame({'symbol': df.index,<br/>                                    'cluster': clusters})<br/><br/># Sort value by cluster<br/>sorted_assignments = cluster_assignments.sort_values(by='cluster')<br/><br/># Convert assignments to same scale as 'variety'<br/>sorted_assignments['cluster'] = [num + 1 for num in sorted_assignments['cluster']]<br/><br/># Convert 'cluster' to category type<br/>sorted_assignments['cluster'] = sorted_assignments['cluster'].astype('category')</span></pre><p id="303c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code below can be used to visualize the output of k-means clustering informed by the elbow plot.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="339c" class="pd nz fq pa b bg pe pf l pg ph">from mpl_toolkits.mplot3d import Axes3D<br/><br/><br/>plt.figure(figsize=(15, 8))<br/>ax = plt.axes(projection='3d')  # Set up a 3D projection<br/><br/># Color for each cluster<br/>colors = ['blue', 'orange', 'green']<br/><br/># Plot each cluster in 3D<br/>for i, color in enumerate(colors):<br/>    # Only select data points that belong to the current cluster<br/>    ix = np.where(clusters == i)<br/>    ax.scatter(reduced_features[ix, 0], reduced_features[ix, 1], <br/>               reduced_features[ix, 2], c=[color], label=f'Cluster {i+1}', <br/>               s=60, alpha=0.8, edgecolor='w')<br/><br/># Plotting the centroids in 3D<br/>centroids = kmeans.cluster_centers_<br/>ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='+', <br/>           s=100, alpha=0.4, linewidths=3, color='red', zorder=10, <br/>           label='Centroids')<br/><br/>ax.set_xlabel('Principal Component 1')<br/>ax.set_ylabel('Principal Component 2')<br/>ax.set_zlabel('Principal Component 3') <br/>ax.set_title('K-Means Clusters Informed by Elbow Plot')<br/>ax.view_init(elev=20, azim=20) # Change viewing angle to make all axes visible<br/><br/># Display the legend<br/>ax.legend()<br/><br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/56420e8bc22d373e3e8d3037a9f09a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*6FomwbXzSjmaqKJ8oxXLkQ.png"/></div></div></figure><p id="75c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since the data were reduced to three dimensions, they are plotted on a 3D plot. To gain additional information about the clusters, we can use <code class="cx pm pn po pa b">countplot</code> from the <code class="cx pm pn po pa b">Seaborn</code> package.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="8eaf" class="pd nz fq pa b bg pe pf l pg ph">plt.figure(figsize=(10,8))<br/><br/>ax = sns.countplot(data=sorted_assignments, x='cluster', hue='cluster', <br/>                   palette=colors)<br/>plt.title('Cluster Distribution')<br/>plt.ylabel('Count')<br/>plt.xlabel('Cluster')<br/><br/>legend = ax.get_legend()<br/>legend.set_title('Cluster')<br/><br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/cda60bea498cb8ae89672050712b6cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVsxiy-Ow6bDOH8gXUtbdg.png"/></div></div></figure><p id="3960" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Earlier, we determined that each group was comprised of 70 seeds. The data displayed in this plot indicate k-means implemented with the elbow plot <em class="pq">may</em> have performed moderately well since each count of each group is around 70; however, there are better ways to evaluate performance.</p><p id="4f5c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To provide a more precise measure of how well the algorithm performed, we will use three metrics: (a) Davies-Bouldin Index, (b) Calinski-Harabasz Index, and (c) Adjusted Rand Index. We’ll talk about how to interpret them in the Results and Analysis section, but the following code snippet can be used to calculate their values.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="17b7" class="pd nz fq pa b bg pe pf l pg ph">from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score<br/><br/># Calculate metrics<br/>davies_boulding = davies_bouldin_score(reduced_features, kmeans.labels_)<br/>calinski_harabasz = calinski_harabasz_score(reduced_features, kmeans.labels_)<br/>adj_rand = adjusted_rand_score(ground_truth, kmeans.labels_)<br/><br/>print(f'Davies-Bouldin Index: {davies_boulding}')<br/>print(f'Calinski-Harabasz Index: {calinski_harabasz}')<br/>print(f'Ajusted Rand Index: {adj_rand}')</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="d851" class="pd nz fq pa b bg pe pf l pg ph">Davies-Bouldin Index: 0.891967185123475<br/>Calinski-Harabasz Index: 259.83668751473334<br/>Ajusted Rand Index: 0.7730246875577171</span></pre><h1 id="a11c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">K-Means Informed by Silhouette Score</strong></h1><p id="0fa4" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">A silhouette score is the mean silhouette coefficient over all the instances. The values can range from -1 to 1, with</p><ul class=""><li id="255c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pu pv pw bk">1 indicating an instance is well inside its cluster</li><li id="aac0" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pu pv pw bk">0 indicating an instance is close to its cluster’s boundary</li><li id="b06a" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pu pv pw bk">-1 indicates the instance could be assigned to the incorrect cluster.</li></ul><p id="c58b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When interpreting the silhouette score, we should choose the number of clusters with the highest score.</p><p id="a155" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To generate a plot of silhouette scores for multiple values of k, we can use the following code.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="09cc" class="pd nz fq pa b bg pe pf l pg ph">from sklearn.metrics import silhouette_score<br/><br/>K_range = range(2, 6)<br/><br/># Calculate Silhouette Coefficient for range of k<br/>for k in K_range:<br/>    kmeans = KMeans(n_clusters=k, random_state=1, n_init='auto')<br/>    cluster_labels = kmeans.fit_predict(reduced_features)<br/>    silhouette_avg = silhouette_score(reduced_features, cluster_labels)<br/>    silhouette_scores.append(silhouette_avg)<br/><br/>plt.figure(figsize=(10, 8))<br/><br/>plt.plot(K_range, silhouette_scores, marker='o')<br/>plt.title('Silhouette Coefficient')<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Silhouette Coefficient')<br/>plt.ylim(0, 0.5) # Modify based on data<br/>plt.xticks(K_range)<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qc"><img src="../Images/f3a56a5f7129d2a3e95225518808a8b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aecrdBF2VpiRspImxGlZ9A.png"/></div></div></figure><p id="940c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The data indicate that k should equal two.</p><p id="5000" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using this information, we can implement the K-Means algorithm again.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="d8f7" class="pd nz fq pa b bg pe pf l pg ph">k = 2 # Set k to the value with the highest silhouette score<br/><br/>kmeans = KMeans(n_clusters=k, random_state=4, n_init='auto')<br/>clusters = kmeans.fit_predict(reduced_features)<br/><br/>cluster_assignments2 = pd.DataFrame({'symbol': df.index,<br/>                                    'cluster': clusters})<br/><br/>sorted_assignments2 = cluster_assignments2.sort_values(by='cluster')<br/><br/># Convert assignments to same scale as 'variety'<br/>sorted_assignments2['cluster'] = [num + 1 for num in sorted_assignments2['cluster']]<br/><br/>sorted_assignments2['cluster'] = sorted_assignments2['cluster'].astype('category')</span></pre><p id="43b7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To generate a plot of the algorithm when k = 2, we can use the code presented below.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="8157" class="pd nz fq pa b bg pe pf l pg ph">plt.figure(figsize=(15, 8))<br/>ax = plt.axes(projection='3d')  # Set up a 3D projection<br/><br/># Colors for each cluster<br/>colors = ['blue', 'orange']<br/><br/># Plot each cluster in 3D<br/>for i, color in enumerate(colors):<br/>    # Only select data points that belong to the current cluster<br/>    ix = np.where(clusters == i)<br/>    ax.scatter(reduced_features[ix, 0], reduced_features[ix, 1],<br/>               reduced_features[ix, 2], c=[color], label=f'Cluster {i+1}',<br/>               s=60, alpha=0.8, edgecolor='w')<br/><br/># Plotting the centroids in 3D<br/>centroids = kmeans.cluster_centers_<br/>ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], marker='+',<br/>           s=100, alpha=0.4, linewidths=3, color='red', zorder=10,<br/>           label='Centroids')<br/><br/>ax.set_xlabel('Principal Component 1')<br/>ax.set_ylabel('Principal Component 2')<br/>ax.set_zlabel('Principal Component 3')<br/>ax.set_title('K-Means Clusters Informed by Elbow Plot')<br/>ax.view_init(elev=20, azim=20) # Change viewing angle to make all axes visible<br/><br/># Display the legend<br/>ax.legend()<br/><br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/794d6fc3863c5bf01a24632194677896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*1rDd_aqMeisab2LcHdKytQ.png"/></div></div></figure><p id="ba48" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similar to the K-Means implementation informed by the elbow plot, additional information can be gleaned using <code class="cx pm pn po pa b">countplot</code>from <code class="cx pm pn po pa b">Seaborn</code>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qd"><img src="../Images/23e7a00c1dbe2bbe696b452033da1cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zb-7-eo5NS304fFkGsKXbA.png"/></div></div></figure><p id="a3be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on our understanding of the dataset (i.e., it includes three varieties of seeds with 70 samples from each category), an initial reading of the plot <em class="pq">may</em> suggest that the implementation informed by the silhouette score did not perform as well on the clustering task; however, we cannot use this plot in isolation to make a determination.</p><p id="0732" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To provide a more robust and detailed comparison of the implementations, we will calculate the three metrics that were used on the implementation informed by the elbow plot.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="75e9" class="pd nz fq pa b bg pe pf l pg ph"># Calculate metrics<br/>ss_davies_boulding = davies_bouldin_score(reduced_features, kmeans.labels_)<br/>ss_calinski_harabasz = calinski_harabasz_score(reduced_features, kmeans.labels_)<br/>ss_adj_rand = adjusted_rand_score(ground_truth, kmeans.labels_)<br/><br/>print(f'Davies-Bouldin Index: {ss_davies_boulding}')<br/>print(f'Calinski-Harabasz Index: {ss_calinski_harabasz}')<br/>print(f'Adjusted Rand Index: {ss_adj_rand}')</span></pre><pre class="pk oz pa pb bp pc bb bk"><span id="d17a" class="pd nz fq pa b bg pe pf l pg ph">Davies-Bouldin Index: 0.7947218992989975<br/>Calinski-Harabasz Index: 262.8372675890969<br/>Adjusted Rand Index: 0.5074767556450577</span></pre><h1 id="bc49" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><strong class="al">Results and Analysis</strong></h1><p id="bdd0" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">To compare the results from both implementations, we can create a dataframe and display it as a table.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="fa74" class="pd nz fq pa b bg pe pf l pg ph">from tabulate import tabulate<br/><br/>metrics = ['Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Adjusted Rand Index']<br/>elbow_plot = [davies_boulding, calinski_harabasz, adj_rand]<br/>silh_score = [ss_davies_boulding, ss_calinski_harabasz, ss_adj_rand]<br/>interpretation = ['SS', 'SS', 'EP']<br/><br/>scores_df = pd.DataFrame(zip(metrics, elbow_plot, silh_score, interpretation),<br/>                         columns=['Metric', 'Elbow Plot', 'Silhouette Score',<br/>                                  'Favors'])<br/><br/># Convert DataFrame to a table<br/>print(tabulate(scores_df, headers='keys', tablefmt='fancy_grid', colalign='left'))</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/36efb4d22c637363266fc4112f9357c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_-wgD3gwsdrDCJzoMzQSA.png"/></div></div></figure><p id="8750" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The metrics used to compare the implementations of k-means clustering include internal metrics (e.g., Davies-Bouldin, Calinski-Harabasz) which do not include ground truth labels and external metrics (e.g., Adjusted Rand Index) which do include external metrics. A brief description of the three metrics is provided below.</p><ul class=""><li id="cd09" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pu pv pw bk">Davies-Bouldin Index (DBI): The DBI captures the trade-off between cluster compactness and the distance between clusters. Lower values of DBI indicate there are tighter clusters with more separation between clusters [3].</li><li id="0155" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pu pv pw bk">Calinski-Harabasz Index (CHI): The CHI measures cluster density and distance between clusters. Higher values indicate that clusters are dense and well-separated [4].</li><li id="5aeb" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pu pv pw bk">Adjusted Rand Index (ARI): The ARI measures agreement between cluster labels and ground truth. The values of the ARI range from -1 to 1. A score of 1 indicates perfect agreement between labels and ground truth; a scores of 0 indicates random assignments; and a score of -1 indicates worse than random assignment [5].</li></ul><p id="0965" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When comparing the two implementations, we observed k-mean informed by the silhouette score performed best on the two internal metrics, indicating more compact and separated clusters. However, k-means informed by the elbow plot performed best on the external metric (i.e., ARI) which indicating better alignment with the ground truth labels.</p><h1 id="782d" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><p id="3ef5" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Ultimately, the best performing implementation will be determined by the task. If the task requires clusters that are cohesive and well-separated, then internal metrics (e.g., DBI, CHI) might be more relevant. If the task requires the clusters to align with the ground truth labels, then external metrics, like the ARI, may be more relevant.</p><p id="34e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The purpose of this project was to provide a comparison between k-means clustering informed by an elbow plot and the silhouette score, and since there wasn’t a defined task beyond a pure comparison, we cannot provide a definitive answer as to which implementation is better.</p><p id="98f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although the absence of a definitive conclusion may be frustrating, it highlights the importance of considering multiple metrics when comparing machine learning models and remaining focused on the project’s objectives.</p><p id="0514" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thank you for taking the time to read this article. If you have any feedback or questions, please leave a comment.</p><h1 id="0e7c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">References</h1><p id="0d9b" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">[1] A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras &amp; Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems (2021), O’Reilly.</p><p id="d373" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] M. Charytanowicz, J. Niewczas, P. Kulczycki, P. Kowalski, S. Łukasik, &amp; S. Zak, Complete Gradient Clustering Algorithm for Features Analysis of X-Ray Images (2010), Advances in Intelligent and Soft Computing <a class="af nb" href="https://doi.org/10.1007/978-3-642-13105-9_2" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-3-642-13105-9_2</a></p><p id="6b45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] D. L. Davies, D.W. Bouldin, A Cluster Separation Measure (1979), IEEE Transactions on Pattern Analysis and Machine Intelligence https://<a class="af nb" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="af nb" href="https://doi.org/10.1109%2FTPAMI.1979.4766909" rel="noopener ugc nofollow" target="_blank">10.1109/TPAMI.1979.4766909</a></p><p id="0689" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] T. Caliński, J. Harabasz, A Dendrite Method for Cluster Analysis (1974) Communications in Statistics https://<a class="af nb" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="af nb" href="https://doi.org/10.1080%2F03610927408827101" rel="noopener ugc nofollow" target="_blank">10.1080/03610927408827101</a></p><p id="df44" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5] N. X. Vinh, J. Epps, J. Bailey, Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance (2010), Journal of Machine Learning Research <a class="af nb" href="https://www.jmlr.org/papers/volume11/vinh10a/vinh10a.pdf" rel="noopener ugc nofollow" target="_blank">https://www.jmlr.org/papers/volume11/vinh10a/vinh10a.pdf</a></p></div></div></div></div>    
</body>
</html>