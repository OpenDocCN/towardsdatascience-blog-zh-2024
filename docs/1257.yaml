- en: Revolutionizing Large Dataset Feature Selection with Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19](https://towardsdatascience.com/reinforcement-learning-for-feature-selection-be1e7eeb0acc?source=collection_archive---------2-----------------------#2024-05-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leverage the power of reinforcement learning for feature selection when faced
    with very large datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Baptiste
    Lefort](../Images/f57c4077afbff9939521032fa19b7f10.png)](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    [Baptiste Lefort](https://medium.com/@baptistelefort?source=post_page---byline--be1e7eeb0acc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be1e7eeb0acc--------------------------------)
    ·11 min read·May 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Discover how reinforcement learning transforms feature selection for machine
    learning models. Learn the process, implementation, and benefits of this innovative
    approach with practical examples and a dedicated Python library.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/743ea5c0887670d49bff360b95359e4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from Jared Murray on [Unplash](https://unsplash.com/photos/NSuufgf-BME)
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection is a determining step in the process of building a machine
    learning model.** Selecting the good features for the model and the task that
    we want to achieve can definitely improve the performances. Indeed, a feature
    can add some noise and then disturb the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, selecting the features is especially more important if we are dealing
    with high-dimensional data set. It enables the model to learn faster and better.
    The idea is then to find the optimal number of features and the most meaningful
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: In this article I will tackle this problem and **go beyond by introducing a
    newly implemented method for feature selection**. Although it exists many different
    feature selection processes they will not be introduced here since a lot of articles
    are already dealing with them. I will focus on the feature selection using the
    reinforcement learning strategy.
  prefs: []
  type: TYPE_NORMAL
- en: First, the reinforcement learning and more especially the Markov Decision Process
    will be addressed. It is a very new approach in the data science domain and more
    especially for feature selection purpose. After, I will introduce an implementation
    of this and how to install and use the python library (**FSRLearning**). Finally,
    I will prove the efficiency of this implementation. Among the possible feature
    selection approaches like wrappers or filtering, **the reinforcement learning
    is the most powerful and efficient**.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this article is to emphasise on the implementation for concret and
    real-problem oriented utilisation. The theoretical aspect of this problem will
    be simplified with examples although some references will be available at the
    end.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinforcement Learning : The Markov Decision Problem for feature selection'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It has been demonstrated that reinforcement learning (RL) technics can be very
    efficient for problems like game solving. The concept of RL is based on Markovian
    Decision Process (MDP). The point here is not to define deeply the MDP but to
    get the general idea of how it works and how it can be useful to our problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The naive idea behind RL is that an agent starts in an unknown environnement.
    This agent has to take actions to complete a task. In function of the current
    state of the agent and the action he has selected previously, the agent will be
    more inclined to choose some actions. At every new state reached and action taken,
    the agent receives a reward. Here are then the main parameters that we need to
    define for feature selection purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a state ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an action ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the rewards ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we choose an action ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Firstly, the state is merely a subset of features that exist in the data set.
    For example, if the data set has three features (Age, Gender, Height) plus one
    label, here will be the possible states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In a state, the order of the features does not matter and it will be explained
    why a little bit later in the article. We have to consider it as a set and not
    a list of features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concerning the actions, from a subset we can go to any other subset with one
    not-already explored feature more than the current state. In the feature selection
    problem, an action is then selecting a not-already explored feature in the current
    state and add it to the next state. Here is a sample of possible actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of impossible actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have defined the states and the actions but not the reward. The reward is
    a real number that is used for evaluating the quality of a state. For example
    if a robot is trying to reach the exit of a maze and decides to go to the exit
    as his next action, then the reward associated to this action will be “good”.
    If he selects as a next action to go in a trap then the reward will be “not good”.
    The reward is a value that brought information about the previous action taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the problem of feature selection an interesting reward could be a value
    of accuracy that is added to the model by adding a new feature. Here is an example
    of how the reward is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For each state that we visit for the first time a classifier will be trained
    with the set of features. This value is stored in the state and the training of
    the classifier, which is very costly, will only happens once even if the state
    is reached again later. The classifier does not consider the order of the feature.
    This is why we can see this problem as a graph and not a tree. In this example,
    the reward of the action of selecting Gender as a new feature for the model is
    the difference between the accuracy of the current state and the next state.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10e93339802fdda3ba8d0913277d4826.png)'
  prefs: []
  type: TYPE_IMG
- en: Each state has several possible actions and rewards associated (Image by the
    author)
  prefs: []
  type: TYPE_NORMAL
- en: On the graph above, each feature has been mapped to a number (i.e “Age” is 1,
    “Gender” is 2 and “Height” is 3). It is totally possible to take other metrics
    to maximise to find the optimal set. In many business applications the recall
    is more considered than the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The next important question is how do we select the next state from the current
    state or how do we explore our environement. We have to find the most optimal
    way to do it since it can quickly become a very complex problem. Indeed, if we
    naively explore all the possible set of features in a problem with 10 features,
    the number of states would be
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The +2 is because we consider an empty state and a state that contains all the
    possible features. In this problem we would have to train the same model on all
    the states to get the set of features that maximises the accuracy. In the RL approach
    we will not have to go in all the states and to train a model every time that
    we go in an already visited state.
  prefs: []
  type: TYPE_NORMAL
- en: We had to determine some stop conditions for this problem and they will be detailed
    later. For now the epsilon-greedy state selection has been chosen. The idea is
    from a current state we select the next action randomly with a probability of
    epsilon (between 0 and 1 and often around 0.2) and otherwise select the action
    that maximises a function. For feature selection the function is the average of
    reward that each feature has brought to the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The epsilon-greedy algorithm implies two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A random phase : with a probability epsilon, we select randomly the next state
    among the possible neighbours of the current state (we can imagine either a uniform
    or a softmax selection)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A greedy phase : we select the next state such that the feature added to the
    current state has the maximal contribution of accuracy to the model. To reduce
    the time complexity, we have initialised a list containing this values for each
    feature. This list is updated every time that a feature is chosen. The update
    is very optimal thanks to the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/fe8b18d529027bc03b6f4c32265f2db9.png)'
  prefs: []
  type: TYPE_IMG
- en: Update of the average of reward list for each feature (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '*AORf* : Average of reward brought by the feature “f”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* : number of times that “f” has been selected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V(F)* : state’s value of the set of features F (not detailed in this article
    for clarity reasons)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The global idea is to find which feature has brought the most accuracy to the
    model. That is why we need to browse different states to evaluate in many different
    environments the most global accurate value of a feature for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally I will detail the two stop conditions. Since the goal is to minimise
    the number of state that the algorithm visits we need to be careful about them.
    The less never visited state we visit, the less amount of models we will have
    to train with different set of features. Training the model to get the accuracy
    is the most costly phase in terms of time and computation power.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm stops in any case in the final state which is the set containing
    all the features. We want to avoid reaching this state since it is the most expensive
    to train a model with.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also, it stops browsing the graph if a sequence of visited states see their
    values degrade successively. A threshold has been set such that after square root
    of the number of total features in the dataset, it stops exploring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the modelling of the problem has been explained, we will detail the
    implementation in python.
  prefs: []
  type: TYPE_NORMAL
- en: The python library for Feature Selection with Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A python library resolving this problem is available. I will explain in this
    part how it works and prove that it is an efficient strategy. Also, this article
    stands as a documentation and you will be able to use this library for your projects
    by the end of the part.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The data pre-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we need to evaluate the accuracy of a state that is visited, we need to
    feed a model with the features and the data used for this feature selection task.
    The data has to been normalised, the categorical variables encoded and have as
    few rows as possible (the smallest it is, the fastest the algorithm will be).
    Also, it’s very important to create a mapping between the features and some integers
    as explained in the previous part. This step is not mandatory but very recommended.
    The final result of this step is to get a DataFrame with all the features and
    another with the labels to predict. Below is an example with a dataset used as
    a benchmark (it can be found here [UCI Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval))).
  prefs: []
  type: TYPE_NORMAL
- en: Process the data
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Installation and importation of the FSRLearning library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second step is to install the library with pip. Here is the command to
    install it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the library the following code can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the library
  prefs: []
  type: TYPE_NORMAL
- en: You will be able to create a feature selector simply by creating an object Feature_Selector_RL.
    Some parameters need to be filled in.
  prefs: []
  type: TYPE_NORMAL
- en: '***feature_number*** (integer) : number of features in the DataFrame X'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***feature_structure*** (dictionary) : dictionary for the graph implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***eps*** (float [0; 1]) : probability of choosing a random next state, 0 is
    an only greedy algorithm and 1 only random'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***alpha*** (float [0; 1]): control the rate of updates, 0 is a very not updating
    state and 1 a very updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***gamma*** (float [0, 1]): factor of moderation of the observation of the
    next state, 0 is a shortsighted condition and 1 it exhibits farsighted behavior'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***nb_iter*** (int): number of sequences to go through the graph'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***starting_state*** (“empty” or “random”) : if “empty”, the algorithm starts
    from the empty state and if “random”, the algorithm starts from a random state
    in the graph'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the parameters can be tuned but for most of the problem only few iterations
    can be good (around 100) and the epsilon value around 0.2 is often enough. The
    starting state is useful to browse the graph more efficiently but it can be very
    dependent on the dataset and both of the values can be tested.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we can initialise very simply the selector with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Selector object initialisation
  prefs: []
  type: TYPE_NORMAL
- en: 'Training the algorithm is very easy on the same basis than most of the ML library:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ca11e2f53ddc0befd561002a7b034fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of the selector (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a 5-tuple as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Index of the features in the DataFrame X (like a mapping)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number or times that the feature has been observed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average of reward brought by the feature after all the iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranking of the features from the least to the most important (here 2 is the
    least and 7 the most important feature)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of states globally visited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another important method of this selector is to get a comparison with the RFE
    selector of Scikit-Learn. It takes as input X, y and the results of the selector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a print after each step of selection of the global metric of
    RFE and FSRLearning. It also outputs a visual comparison of the accuracy of the
    model with on the x-axis the number of features selected and on the y-axis the
    accuracy. The two horizontal lines are the median of accuracy for each method.
    Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b74a30d601f80f8b0a2970d80f8808a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between RL and RFE method (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example the RL method has always given a better set of features for
    the model than RFE. We can then select with certainty among the sorted set of
    features any subset and it will give a better accuracy to the model. We can run
    several times the model and the comparator to get a very accurate estimation but
    the RL method is always better.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting method is the get_plot_ratio_exploration. It plots a graph
    comparing the number of already visited nodes and visited nodes in a sequence
    for a precise iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eaa6224563da601d84a3e70911b0e8f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of visited and not visited state at each iteration (Image by the
    author)
  prefs: []
  type: TYPE_NORMAL
- en: Also, thanks to the second stop condition the time complexity of the algorithm
    decreases exponentially. Then even if the number of feature is big the convergence
    will be found quickly. The plot bellow is the number of times that a set of a
    certain size has been visited.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/313c55b8da35476e077b1f932fa3529c.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of visited states in function of their size (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: In all iterations the algorithm visited a state containing 6 variables or less.
    Beyond 6 variables we can see that the number of state reached is decreasing.
    It’s a good behaviour since it is faster to train a model with small set of features
    than the big ones.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and references
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, we can see that the RL method is very efficient for maximising a metric
    of a model. It always converges quickly toward an interesting subset of features.
    Also, this method is very easy and fast to implement in ML projects with the FSRLearning
    library.
  prefs: []
  type: TYPE_NORMAL
- en: The github repository of the project with a complete documentation is available
    [here](https://github.com/blefo/FSRLearning).
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to contact me, you can do so directly on linkedin [here](https://www.linkedin.com/in/baptistelefort/)
  prefs: []
  type: TYPE_NORMAL
- en: 'This library has been implemented with the help of these two articles:'
  prefs: []
  type: TYPE_NORMAL
- en: Sali Rasoul, Sodiq Adewole and Alphonse Akakpo, [FEATURE SELECTION USING REINFORCEMENT
    LEARNING](https://arxiv.org/pdf/2101.09460.pdf) (2021), ArXiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seyed Mehdin Hazrati Fard, Ali Hamzeh and Sattar Hashemi, [Using reinforcement
    learning to find an optimal set of features](https://www.sciencedirect.com/science/article/pii/S0898122113004495)
    (2013), ScienceDirect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
