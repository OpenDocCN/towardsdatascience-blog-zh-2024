- en: Carbon Footprint of LLM Fine Tuning — A Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22](https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I got surprising results when I measured the carbon emissions from instruction
    fine tuning an LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    ·5 min read·Feb 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26870185100a8cc707bee2bfab3c8e98.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Ingmar H on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: I recently LoRA fine-tuned a Danish LLM called Munin-7b-alpha on an instruction
    fine tuning dataset called SkoleGPT-instruct. During the fine tuning procedure,
    I measured the energy consumption and computed the carbon footprint. In this article,
    I present the surprising results. You can find the model [here](https://huggingface.co/ThatsGroes/munin-SkoleGPTOpenOrca-7b-16bit).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Munin-7b-alpha is a pre-trained model (or a so-called foundation model), which
    has been trained solely to generate text. To make them suitable for a chat setup,
    pre-trained models need to be good at following instructions, which requires a
    subsequent training step called instruction fine tuning.
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to pre-training, which requires massive amounts of unlabeled text
    data on which the model trains in a self-supervised fashion, instruction fine
    tuning requires a relatively modest amount of data, which in turn must be carefully
    curated and annotated.
  prefs: []
  type: TYPE_NORMAL
- en: It is a fune-tuning procedure that I report on in this article.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
    [## Set up a local LLM on CPU with chat UI in 15 minutes'
  prefs: []
  type: TYPE_NORMAL
- en: This blog post shows how to easily run an LLM locally and how to set up a ChatGPT-like
    GUI in 4 easy steps.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Munin-7b-alpha has 7 billion parameters and the instruction dataset that
    I used consists of 21,300 samples. That is, 21,300 examples of a prompt and a
    good answer.
  prefs: []
  type: TYPE_NORMAL
- en: Using an slightly adapted version of [this fantastic model fine tuning notebook](https://github.com/alexandrainst/d3a-llm-workshop),
    I trained a LoRA for 1 epoch, i.e. I showed the model each sample once.
  prefs: []
  type: TYPE_NORMAL
- en: 'LoRA – low rank adaptation – is an efficient fine tuning technique for adapting
    LLMs to specific tasks. [Hugging Face](https://huggingface.co/docs/peft/en/package_reference/lora)
    provides a succinct description of the technique:'
  prefs: []
  type: TYPE_NORMAL
- en: “Low-Rank Adaptation (LoRA) is a PEFT [parameter efficient fine tuning] method
    that decomposes a large matrix into two smaller low-rank matrices in the attention
    layers. This drastically reduces the number of parameters that need to be fine-tuned.”
  prefs: []
  type: TYPE_NORMAL
- en: The model trained on a single Nvidia RTX A4000 GPU, which is a consumer grade
    GPU with 16 GB memory – just enough memory for LoRA fine tuning of this model.
  prefs: []
  type: TYPE_NORMAL
- en: 'I measured energy consumption with the Python package CodeCarbon. CodeCarbon
    is an extremely light weight and easy-to-use package that let’s you measure the
    energy consumption of a Python script, function or method with just two lines
    of code. Read more about how to use it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  prefs: []
  type: TYPE_NORMAL
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Aside from energy consumption, CodeCarbon also estimates the carbon footprint
    of the energy your computing procedure consumes, but I found the numbers to appear
    inaccurate. This is likely because CodeCarbon uses a hardcoded average carbon
    intensity (CO2e per produced KWh) of your geographic region and not an near real
    time carbon intensity. So I went to a website called Energi Data Service, which
    lets you download fine grained electricity emissions data from the Danish grid.
    By multiplying the energy consumption measurements obtained with CodeCarbon by
    the carbon intensity of electricity in the grid during the hours the model trained,
    I obtained the carbon footprint of the training.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
    [## ChatGPT’s energy use per query'
  prefs: []
  type: TYPE_NORMAL
- en: How much electricity does ChatGPT use to answer one question?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fine tuning process took just shy of 4 hours and consumed a total of 0.694
    KWh – the combined GPU, CPU and RAM consumption as per estimates produced with
    the Python package CodeCarbon.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the hours the model trained, the average C02e emissions per produced
    KWh was 82.5 g as per Energi Data Service (license: “The Licensor grants you a
    worldwide, free, non-exclusive and otherwise unrestricted licence to use the Data”
    [1]).'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the fine tuning emitted a minuscule 57 grams of CO2e (0.694 KWh * 82.5
    g).
  prefs: []
  type: TYPE_NORMAL
- en: For comparison, the average Dane emits 11 TONS CO2e per year.
  prefs: []
  type: TYPE_NORMAL
- en: Generating a single image with generative AI has been found in a research study
    to consume 2.9 Wh on average [2]. So for the amount of energy it took to instruction
    fine-tune the LLM, you can generate a mere 239 images.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re wondering if such a short and efficient fine-tuning procedure yielded
    a better model, the answer is a clear “yes”:'
  prefs: []
  type: TYPE_NORMAL
- en: According to the [ScandEval leader board](https://scandeval.com/mainland-scandinavian-nlg/)
    on natural language generation, the pre-trained model scores an average of 43.44
    on Danish tasks and the fine tuned model scores an average of 47.55\. A gain of
    9.45 percent. As of this writing, that’s the difference between a 5th place and
    a 7th place on the leader board.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)
    [## How to make a PyTorch Transformer for time series forecasting'
  prefs: []
  type: TYPE_NORMAL
- en: This post will show you how to transform a time series Transformer architecture
    diagram into PyTorch code step by step.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s surprising to me that it did not require more compute, energy, and emissions
    to perform the fine tuning.
  prefs: []
  type: TYPE_NORMAL
- en: I expect my findings to scale linearly with the amount of samples if holding
    other variables constant (e.g. using a similar GPU, training method etc.). I.e.
    if you fine tune on twice the amount of samples, or for double the number of epochs,
    I expect the energy consumption to double.
  prefs: []
  type: TYPE_NORMAL
- en: The energy consumption will likely be significantly higher for a 70 billion
    parameter model, thus leading to higher emissions, but emissions would probably
    still very modest in the grand scheme of things.
  prefs: []
  type: TYPE_NORMAL
- en: Further, the energy consumption would likely be higher if I hadn’t used LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the instruction fine-tuning technique LoRA is indeed efficient—both in
    terms of how long it takes, how much compute (eg GPU RAM) you need, and how much
    carbon it emits.
  prefs: []
  type: TYPE_NORMAL
- en: Instruction fine tuning a 7B LLM with LoRA on 21,300 samples for one epoch took
    four hours and emitted 57 gram CO2e—a tiny amount.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! I hope you enjoyed the story. Let me know what you think!
  prefs: []
  type: TYPE_NORMAL
- en: Get the benefits of Medium and support my writing by signing up for a [Medium
    membership HERE](https://kaspergroesludvigsen.medium.com/membership).
  prefs: []
  type: TYPE_NORMAL
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  prefs: []
  type: TYPE_NORMAL
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  prefs: []
  type: TYPE_NORMAL
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] https://www.energidataservice.dk/Conditions_for_use_of_Danish_public_sector_data-License_for_use_of_data_in_ED.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] “Power Hungry Processing: Watts Driving the Cost of AI Deployment?” by
    Luccioni et al'
  prefs: []
  type: TYPE_NORMAL
