- en: Carbon Footprint of LLM Fine Tuning — A Case Study
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 微调的碳足迹——一个案例研究
- en: 原文：[https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22](https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22](https://towardsdatascience.com/carbon-footprint-of-llm-fine-tuning-a-case-study-7703afc716a9?source=collection_archive---------9-----------------------#2024-02-22)
- en: I got surprising results when I measured the carbon emissions from instruction
    fine tuning an LLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当我测量 LLM 指令微调的碳排放时，我得到了令人惊讶的结果。
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page---byline--7703afc716a9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    ·5 min read·Feb 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7703afc716a9--------------------------------)
    ·阅读时间 5 分钟 ·2024年2月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/26870185100a8cc707bee2bfab3c8e98.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26870185100a8cc707bee2bfab3c8e98.png)'
- en: Photo by Ingmar H on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Ingmar H 提供，来自 Unsplash
- en: I recently LoRA fine-tuned a Danish LLM called Munin-7b-alpha on an instruction
    fine tuning dataset called SkoleGPT-instruct. During the fine tuning procedure,
    I measured the energy consumption and computed the carbon footprint. In this article,
    I present the surprising results. You can find the model [here](https://huggingface.co/ThatsGroes/munin-SkoleGPTOpenOrca-7b-16bit).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近对一个名为 Munin-7b-alpha 的丹麦 LLM 进行了 LoRA 微调，使用的微调数据集名为 SkoleGPT-instruct。在微调过程中，我测量了能耗并计算了碳足迹。本文将展示这些令人惊讶的结果。你可以在
    [这里](https://huggingface.co/ThatsGroes/munin-SkoleGPTOpenOrca-7b-16bit) 找到该模型。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Munin-7b-alpha is a pre-trained model (or a so-called foundation model), which
    has been trained solely to generate text. To make them suitable for a chat setup,
    pre-trained models need to be good at following instructions, which requires a
    subsequent training step called instruction fine tuning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Munin-7b-alpha 是一个预训练模型（或称为基础模型），它仅被训练用于生成文本。为了使其适应聊天设置，预训练模型需要擅长遵循指令，这需要一个后续的训练步骤，称为指令微调。
- en: As opposed to pre-training, which requires massive amounts of unlabeled text
    data on which the model trains in a self-supervised fashion, instruction fine
    tuning requires a relatively modest amount of data, which in turn must be carefully
    curated and annotated.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与预训练不同，预训练需要大量未标记的文本数据，模型在这些数据上以自监督方式进行训练；而指令微调则需要相对较少的数据，且这些数据必须经过精心筛选和注释。
- en: It is a fune-tuning procedure that I report on in this article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇我在本文中报道的微调过程。
- en: '[](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
    [## Set up a local LLM on CPU with chat UI in 15 minutes'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
    [## 在 CPU 上用聊天 UI 设置本地 LLM，只需 15 分钟'
- en: This blog post shows how to easily run an LLM locally and how to set up a ChatGPT-like
    GUI in 4 easy steps.
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本文展示了如何轻松地在本地运行 LLM，并且如何在四个简单的步骤中设置类似 ChatGPT 的图形用户界面（GUI）。
- en: towardsdatascience.com](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=post_page-----7703afc716a9--------------------------------)
- en: Methodology
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: The Munin-7b-alpha has 7 billion parameters and the instruction dataset that
    I used consists of 21,300 samples. That is, 21,300 examples of a prompt and a
    good answer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Munin-7b-alpha拥有70亿个参数，我使用的指令数据集包含21,300个样本。也就是说，21,300个提示和良好答案的示例。
- en: Using an slightly adapted version of [this fantastic model fine tuning notebook](https://github.com/alexandrainst/d3a-llm-workshop),
    I trained a LoRA for 1 epoch, i.e. I showed the model each sample once.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用略微改编版的[这个精彩的模型微调笔记本](https://github.com/alexandrainst/d3a-llm-workshop)，我训练了一个LoRA模型，进行了1个epoch，也就是说，我每次只给模型展示一个样本。
- en: 'LoRA – low rank adaptation – is an efficient fine tuning technique for adapting
    LLMs to specific tasks. [Hugging Face](https://huggingface.co/docs/peft/en/package_reference/lora)
    provides a succinct description of the technique:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA（低秩适应）是一种高效的微调技术，用于将大型语言模型（LLMs）调整到特定任务上。[Hugging Face](https://huggingface.co/docs/peft/en/package_reference/lora)提供了该技术的简明描述：
- en: “Low-Rank Adaptation (LoRA) is a PEFT [parameter efficient fine tuning] method
    that decomposes a large matrix into two smaller low-rank matrices in the attention
    layers. This drastically reduces the number of parameters that need to be fine-tuned.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: “低秩适应（LoRA）是一种PEFT（参数高效微调）方法，它将一个大矩阵分解成两个较小的低秩矩阵，位于注意力层中。这大大减少了需要微调的参数数量。”
- en: The model trained on a single Nvidia RTX A4000 GPU, which is a consumer grade
    GPU with 16 GB memory – just enough memory for LoRA fine tuning of this model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在单个Nvidia RTX A4000 GPU上训练，该GPU是消费级GPU，具有16 GB内存——足以进行该模型的LoRA微调。
- en: 'I measured energy consumption with the Python package CodeCarbon. CodeCarbon
    is an extremely light weight and easy-to-use package that let’s you measure the
    energy consumption of a Python script, function or method with just two lines
    of code. Read more about how to use it here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用Python包CodeCarbon测量了能耗。CodeCarbon是一个极其轻量且易于使用的包，只需两行代码就能测量Python脚本、函数或方法的能耗。了解如何使用它，请点击这里：
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
    [## 如何估算和减少机器学习模型的碳足迹'
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算机器学习模型碳足迹的两种简单方法以及17个减少碳足迹的建议
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----7703afc716a9--------------------------------)
- en: Aside from energy consumption, CodeCarbon also estimates the carbon footprint
    of the energy your computing procedure consumes, but I found the numbers to appear
    inaccurate. This is likely because CodeCarbon uses a hardcoded average carbon
    intensity (CO2e per produced KWh) of your geographic region and not an near real
    time carbon intensity. So I went to a website called Energi Data Service, which
    lets you download fine grained electricity emissions data from the Danish grid.
    By multiplying the energy consumption measurements obtained with CodeCarbon by
    the carbon intensity of electricity in the grid during the hours the model trained,
    I obtained the carbon footprint of the training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了能耗外，CodeCarbon还会估算计算过程所消耗的能量的碳足迹，但我发现其数据似乎并不准确。这可能是因为CodeCarbon使用了一个硬编码的你所在地区的平均碳强度（每产生1KWh的二氧化碳排放量），而不是接近实时的碳强度。因此，我访问了一个名为Energi
    Data Service的网站，它允许你下载丹麦电网的详细电力排放数据。通过将CodeCarbon获得的能量消耗数据与模型训练时电网的碳强度相乘，我得到了训练的碳足迹。
- en: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
    [## ChatGPT’s energy use per query'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
    [## ChatGPT每次查询的能耗'
- en: How much electricity does ChatGPT use to answer one question?
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPT回答一个问题需要多少电力？
- en: towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----7703afc716a9--------------------------------)
- en: Results
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: The fine tuning process took just shy of 4 hours and consumed a total of 0.694
    KWh – the combined GPU, CPU and RAM consumption as per estimates produced with
    the Python package CodeCarbon.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程只花了不到4小时，消耗了总共0.694千瓦时——根据Python包CodeCarbon的估算，这包括了GPU、CPU和RAM的总能耗。
- en: 'During the hours the model trained, the average C02e emissions per produced
    KWh was 82.5 g as per Energi Data Service (license: “The Licensor grants you a
    worldwide, free, non-exclusive and otherwise unrestricted licence to use the Data”
    [1]).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型训练过程中，根据Energi Data Service的数据（许可：“许可方授予您全球范围内的免费、非独占且没有其他限制的使用数据许可” [1]），每千瓦时的平均CO2e排放量为82.5克。
- en: Thus, the fine tuning emitted a minuscule 57 grams of CO2e (0.694 KWh * 82.5
    g).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，微调过程中排放了微乎其微的57克二氧化碳当量（0.694千瓦时 * 82.5克）。
- en: For comparison, the average Dane emits 11 TONS CO2e per year.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作对比，平均丹麦人每年排放11吨二氧化碳当量。
- en: Generating a single image with generative AI has been found in a research study
    to consume 2.9 Wh on average [2]. So for the amount of energy it took to instruction
    fine-tune the LLM, you can generate a mere 239 images.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项研究中发现，生成一张图像使用生成性AI平均消耗2.9瓦时[2]。因此，用于指令微调LLM的能量，足以生成239张图像。
- en: 'If you’re wondering if such a short and efficient fine-tuning procedure yielded
    a better model, the answer is a clear “yes”:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想这样一个短暂且高效的微调过程是否能产生更好的模型，答案显然是“是的”：
- en: According to the [ScandEval leader board](https://scandeval.com/mainland-scandinavian-nlg/)
    on natural language generation, the pre-trained model scores an average of 43.44
    on Danish tasks and the fine tuned model scores an average of 47.55\. A gain of
    9.45 percent. As of this writing, that’s the difference between a 5th place and
    a 7th place on the leader board.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[ScandEval排行榜](https://scandeval.com/mainland-scandinavian-nlg/)上关于自然语言生成的排名，预训练模型在丹麦任务上的平均得分为43.44，而微调后的模型得分为47.55，提升了9.45%。截至本文撰写时，这一差距使得模型从排行榜的第5名提升到了第7名。
- en: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)
    [## How to make a PyTorch Transformer for time series forecasting'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)
    [## 如何制作一个用于时间序列预测的PyTorch Transformer'
- en: This post will show you how to transform a time series Transformer architecture
    diagram into PyTorch code step by step.
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本文将逐步展示如何将时间序列Transformer架构图转换为PyTorch代码。
- en: towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----7703afc716a9--------------------------------)'
- en: Discussion
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: It’s surprising to me that it did not require more compute, energy, and emissions
    to perform the fine tuning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 令我惊讶的是，进行微调并没有需要更多的计算、能源和排放。
- en: I expect my findings to scale linearly with the amount of samples if holding
    other variables constant (e.g. using a similar GPU, training method etc.). I.e.
    if you fine tune on twice the amount of samples, or for double the number of epochs,
    I expect the energy consumption to double.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计，如果其他变量保持不变（例如使用相似的GPU、训练方法等），我的发现将与样本数量成线性增长。即如果你在两倍的样本上进行微调，或者进行两倍的周期训练，我预计能源消耗也将翻倍。
- en: The energy consumption will likely be significantly higher for a 70 billion
    parameter model, thus leading to higher emissions, but emissions would probably
    still very modest in the grand scheme of things.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个70亿参数的模型，能源消耗可能会显著增加，从而导致更高的排放，但在总体上，排放可能仍然非常低。
- en: Further, the energy consumption would likely be higher if I hadn’t used LoRA.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我没有使用LoRA，能源消耗可能会更高。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Using the instruction fine-tuning technique LoRA is indeed efficient—both in
    terms of how long it takes, how much compute (eg GPU RAM) you need, and how much
    carbon it emits.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指令微调技术LoRA确实高效——无论是从所需时间、计算量（例如GPU内存）还是碳排放量来看。
- en: Instruction fine tuning a 7B LLM with LoRA on 21,300 samples for one epoch took
    four hours and emitted 57 gram CO2e—a tiny amount.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA对7B LLM进行指令微调，基于21,300个样本进行一个周期的训练，耗时四小时，排放了57克二氧化碳当量——一个非常小的数值。
- en: That’s it! I hope you enjoyed the story. Let me know what you think!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！希望你喜欢这个故事。让我知道你的想法！
- en: Get the benefits of Medium and support my writing by signing up for a [Medium
    membership HERE](https://kaspergroesludvigsen.medium.com/membership).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注册[Medium会员 HERE](https://kaspergroesludvigsen.medium.com/membership)来获取Medium的好处并支持我的写作。
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我，了解更多关于AI和可持续性的内容，并且[订阅](https://kaspergroesludvigsen.medium.com/subscribe)，在我发布新故事时通过电子邮件接收更新。
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时也会写关于[时间序列预测](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e)的文章。
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 并欢迎通过[LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen)与我联系。
- en: References
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] https://www.energidataservice.dk/Conditions_for_use_of_Danish_public_sector_data-License_for_use_of_data_in_ED.pdf'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] https://www.energidataservice.dk/Conditions_for_use_of_Danish_public_sector_data-License_for_use_of_data_in_ED.pdf'
- en: '[2] “Power Hungry Processing: Watts Driving the Cost of AI Deployment?” by
    Luccioni et al'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] “电力需求处理：瓦特驱动AI部署成本？”由Luccioni等人编写'
