["```py\nimport torch.autograd.profiler as profiler\n\nwith profiler.profile(\n  activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n  on_trace_ready=torch.profiler.tensorboard_trace_handler('./logs'),\n) as prof:\n  train(args)\n```", "```py\nwith profiler.record_function(\"forward_pass\"):\n  result = model(**batch)\n\nwith profiler.record_function(\"train_step\"):\n  step(**result)\n```", "```py\nwith profiler.record_function(\"transformer_layer:self_attention\"):\n  data = self.self_attention(**data)\n\n...\n\nwith profiler.record_function(\"transformer_layer:encoder_attention\"):\n  data = self.encoder_attention(**data, **encoder_data)\n```", "```py\nPYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\"\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n\nmodel = FSDP(model)\n\n# it's critical to get parameters from the wrapped model\n# as only a portion of them returned (sharded part)\noptimizer = optim.Adam(model.parameters())\n\n# consuct training as usual\ntrain(model, optimizer)\n```", "```py\nimport torch\n\nmodel = torch.compile(model)\n```"]