<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Improving Business Performance with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Improving Business Performance with Machine Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-business-performance-with-machine-learning-fbdf5e280923?source=collection_archive---------3-----------------------#2024-06-13">https://towardsdatascience.com/improving-business-performance-with-machine-learning-fbdf5e280923?source=collection_archive---------3-----------------------#2024-06-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="76a0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Whether you are a data scientist, analyst, or business analyst, your goal is to deliver projects that improve business performance.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@juanjosemunozp?source=post_page---byline--fbdf5e280923--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Juan Jose Munoz" class="l ep by dd de cx" src="../Images/b42d72e9e2a2eaf11da5465e9b041d53.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YeUOkLrhcC48xk632N_gjw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fbdf5e280923--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@juanjosemunozp?source=post_page---byline--fbdf5e280923--------------------------------" rel="noopener follow">Juan Jose Munoz</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fbdf5e280923--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/d28fb9d54d5dd553dc3c6563016f0125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*911pexO6QIWmNcqS"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@epicantus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Daria Nepriakhina 🇺🇦</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="34f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It might be tempting to focus on the latest machine learning developments or tackling the big problems. However, you can often deliver great value by solving low-hanging fruit with simple machine-learning algorithms.</p><p id="ed96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Benchmarking is one of those low-hanging fruits. It is the process of measuring business KPIs against similar organizations. It allows businesses to learn from the best and continuously improve performance.</p><p id="029d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are two types of Benchmarking:</p><blockquote class="ny nz oa"><p id="8151" class="nc nd ob ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">1. Internal</strong>: measure KPI against units/products in the same company<br/><strong class="ne fr">2. External</strong>: measure KPI against competitors</p></blockquote><p id="0d92" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In my daily work in the hotel industry, we often rely on third-party companies that collect hotel data for external benchmarking. However, the data we get from them is limited. On the other hand, we manage over 500 hotels and are sitting on vast amounts of data for potential benchmarking.</p><p id="2049" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is the low-hanging fruit we set up to solve recently.</p><p id="4ef5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">No matter which type of benchmarking exercise you are conducting, the first step is to select a set of hotels similar to the subject hotel. In the hotel industry, we usually rely on location indicators, brand tier, number of rooms, price range, and market demand. We typically do this manually when we are doing it for one or two hotels, but doing this manually for 500 hotels is not feasible.</p><p id="1a78" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you have a problem to solve, the next step is to select with tool to use. Machine learning offers many tools. However, this problem can be solved with a simple family of algorithms: Nearest Neighbors.</p><h1 id="5cf7" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk"><strong class="al">The Nearest Neighbors algorithm family</strong></h1><p id="8382" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The nearest neighbors algorithm family is a form of optimization problem that aims to find the points in a given data set that are the closest or most similar to a given point.</p><p id="b80c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These algorithms have been very successful in tackling many classification and regression problems. As such, Scikit Learn API has a fantastic Nearest Neighbors module.</p><div class="pd pe pf pg ph pi"><a href="https://scikit-learn.org/stable/modules/classes.html?source=post_page-----fbdf5e280923--------------------------------#module-sklearn.neighbors" rel="noopener  ugc nofollow" target="_blank"><div class="pj ab ig"><div class="pk ab co cb pl pm"><h2 class="bf fr hw z io pn iq ir po it iv fp bk">API Reference</h2><div class="pp l"><h3 class="bf b hw z io pn iq ir po it iv dx">This is the class and function reference of scikit-learn. Please refer to the full user guide for further details, as…</h3></div><div class="pq l"><p class="bf b dy z io pn iq ir po it iv dx">scikit-learn.org</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw lq pi"/></div></div></a></div><h1 id="bca1" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Choosing the right algorithm</h1><p id="cd4d" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Most people are familiar with K-Nearest Neighbor (KNN); however, Scikit Learn offers a wide variety of Nearest Neighbors algorithms, covering both supervised and unsupervised tasks.</p><p id="2cc5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For our problem, we don’t have any labels. Therefore, we are looking for an unsupervised algorithm.</p><p id="c105" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you look through the scikit learn documentation, you will find <code class="cx px py pz qa b">NearestNeighbors</code> . This algorithm performs unsupervised learning for implementing neighbor searches.</p><p id="1b06" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This seems to cover what we need to solve our problem. Let’s start by getting the data ready and running a baseline model.</p><h1 id="f2c8" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Baseline Model</h1><h1 id="56af" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">1. Loading the data</h1><p id="8a69" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">A hotel’s performance usually depends on location, brand, and size. For our analysis, we use two data sets:</p><p id="5e26" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hotel data: The hotel data used below has been generated artificially based on the original dataset used for this analysis.</p><ul class=""><li id="98ba" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><code class="cx px py pz qa b">BRAND</code>: defines the service level of the hotel: Luxury, Upscale, Economy</li><li id="1c07" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Room_count</code>: number of rooms available for sale</li><li id="5097" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Market</code>: Name of the city in which the hotel is located</li><li id="4d29" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Country</code>: Name of the country</li><li id="cc72" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Latitude</code>: Hotel's Latitude location</li><li id="bb0f" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Longitude</code>: Hotel's Longitude location</li><li id="9389" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Airport Code</code>: 3 Letter code of the nearest international airport</li><li id="83fd" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Market Tier</code>: defines the market development level.</li><li id="162f" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">HCLASS</code>: indicates if the hotel is a city hotel or resort</li><li id="f62f" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Demand</code>: indicates hotel yearly occupancy</li><li id="3920" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Price range</code>: indicates the average price for the hotel</li></ul><p id="d49d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also know how hotel performance can be impacted by accessibility. To measure accessibility, we can measure how far the hotel is from the main international airport. The airport data is from the World Bank: <a class="af nb" href="https://datacatalog.worldbank.org/search/dataset/0038117" rel="noopener ugc nofollow" target="_blank">https://datacatalog.worldbank.org/search/dataset/0038117</a></p><ul class=""><li id="0de2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Orig</code>: 3 Letter Airport code</li><li id="29b1" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Name</code>: Airport name</li><li id="45af" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">TotalSeats</code>: Annual passenger volume</li><li id="bca3" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Country name</code>: Airport country name</li><li id="7cb2" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Airpot1Latitude</code>: Aiport Latitude</li><li id="b960" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">Airport1Longitude</code>: Airport Longitude</li></ul><p id="f101" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ob">*Global Airports dataset is licensed under Creative Commons Attribution 4.0</em></p><p id="2ffa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s import the data.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="265f" class="qm od fq qa b bg qn qo l qp qq">import pandas as pd<br/>import numpy as np<br/><br/>data = pd.read_excel("mock_data.xlsx")<br/>airport_data = pd.read_csv("airport_volume_airport_locations.csv")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qr"><img src="../Images/5a02a04415f60ae25407698a38693530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*045x6WJuZV_OBlZq3FLw4g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sample of Hotel data. Image by author</figcaption></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qs"><img src="../Images/1d2b250de192743886e633b4d2eb9f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZhu2F5I9bBTEFBEzRgxmg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Sample Airport data. Image by author</figcaption></figure><p id="ec61" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned before, hotel performance is highly dependent on location. In our data set, we have many measures of location, such as Market Country… however, this is not always ideal as those definitions are quite broad. To narrow down similar locations, we need to create a <code class="cx px py pz qa b">accessability</code> measure, defined by the distance to the closest international airport.</p><p id="17c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To calculate the distance from a hotel to the airport, we use the haversine formula. The haversine formula is used to calculate the distance between two points in a sphere, given their latitude and longitude.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="104f" class="qm od fq qa b bg qn qo l qp qq"># Below code is taken from geeksforgeeks<br/>from math import radians, cos, sin, asin, sqrt<br/><br/>def distance_to_airport(lat, airport_lat, lon, airport_lon):<br/><br/>    #  Convert latitude and longitude values from decimal degrees to radians<br/>    lon = radians(lon)<br/>    airport_lon = radians(airport_lon)<br/>    lat = radians(lat)<br/>    airport_lat = radians(airport_lat)<br/><br/>    # Haversine formula<br/>    dlon = airport_lon - lon<br/>    dlat = airport_lat - lat<br/>    a = sin(dlat / 2)**2 + cos(lat) * cos(airport_lat) * sin(dlon / 2)**2<br/><br/>    c = 2 * asin(sqrt(a))<br/><br/>    # Radius of earth in kilometers.<br/>    r = 6371<br/><br/>    # return distance in KM<br/>    return(c * r)<br/><br/>#Apply the distance_to_airport functions to each hotel<br/>data["distance_to_airport"] = data.apply(lambda row: distance_to_airport(row["Latitude"],row["Airport1Latitude"],row["Longitude"],row["Airport1Longitude"]),axis=1)<br/>data.head()</span></pre></div></div><div class="mq"><div class="ab cb"><div class="ll qt lm qu ln qv cf qw cg qx ci bh"><figure class="ml mm mn mo mp mq qz ra paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qy"><img src="../Images/f42916249a4d86d75892a53743b9a4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*pcr6d8jmBA82SaHdgrWyFA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Resulting data frame with distance to airport feature. Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="96d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next step is removing any column we won’t need for our model.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="cbbc" class="qm od fq qa b bg qn qo l qp qq"># Drop Columns that we dont need<br/># For the purpose of benchmarking we will keep the hotel feautures, and distance to airport<br/>col_to_drop = ["Latitude","Longitude","Airport Code","Orig","Name","TotalSeats","Country Name","Airport1Latitude","Airport1Longitude"]<br/><br/>data_clean = data.drop(col_to_drop,axis=1)<br/>data_clean.head()</span></pre><p id="398c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we encode all non-numerical variables so that we can pass them into our model. At this point, it is important to keep in mind that we will need the original labels to present our suggested groupings to the team and for ease of validation. To do so, we will store the encoding information in a dictionary.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="0c7d" class="qm od fq qa b bg qn qo l qp qq">from sklearn.preprocessing import LabelEncoder<br/><br/># Create a LabelEncoder object for each object column<br/>brand_encoder = LabelEncoder()<br/>market_encoder = LabelEncoder()<br/>country_encoder = LabelEncoder()<br/>market_tier_encoder = LabelEncoder()<br/>hclass_encoder = LabelEncoder()<br/><br/># Fit each LabelEncoder on the unique values of the corresponding column<br/>data_clean['BRAND'] = brand_encoder.fit_transform(data_clean['BRAND'])<br/>data_clean['Market'] = market_encoder.fit_transform(data_clean['Market'])<br/>data_clean['Country'] = country_encoder.fit_transform(data_clean['Country'])<br/>data_clean['Market Tier'] = market_tier_encoder.fit_transform(data_clean['Market Tier'])<br/>data_clean['HCLASS']= hclass_encoder.fit_transform(data_clean['HCLASS'])<br/><br/># create a dictionnary with all the encoders for reverse encoding<br/>encoders ={"BRAND" : brand_encoder,<br/>           "Market": market_encoder,<br/>           "Country": country_encoder,<br/>           "Market Tier": market_tier_encoder,<br/>           "HCLASS": hclass_encoder<br/>}<br/><br/><br/>data_clean.head()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/765d8ac866bab658217e85f9dde837cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6Zx1aOeIzizhzUFfO6FpA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Encoded data. Image by author</figcaption></figure><p id="47b3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our data is now numerical, but as you can see the values in each column have very different ranges. To avoid the ranges of any features from disproportionately affecting our model, we need to rescale our data.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="7298" class="qm od fq qa b bg qn qo l qp qq">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>data_scaled = scaler.fit_transform(data_clean)<br/>data_scaled</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rc"><img src="../Images/a885e446568eb13a0c3a01d119cf4258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*QhDrJAIl4FQkD3nz_GL5PQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Scaled data. Image by author</figcaption></figure><p id="83cd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At this point, we are ready to generate a baseline model.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="fc5b" class="qm od fq qa b bg qn qo l qp qq">from sklearn.neighbors import NearestNeighbors<br/><br/>nns = NearestNeighbors()<br/>nns.fit(data_scaled)<br/>nns_results_model_0 = nns.kneighbors(data_scaled)[1]<br/><br/>nns_results_model_0</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rd"><img src="../Images/c1e08da417e6a83c4a372f5b5c1436df.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*sN5fwnwpr_kzv8cbl127Xw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Model output. Image by author</figcaption></figure><p id="0526" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output of the model is a list of indexes, where the first index is the subject hotel, and the other indexes represent the nearest hotels.</p><p id="c433" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To validate the model, we can visually inspect the results. We can create a function that takes in the list of indexes and decodes the values.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="b610" class="qm od fq qa b bg qn qo l qp qq">def clean_results(nns_results: np.ndarray,<br/>                  encoders: dict,<br/>                  data: pd.DataFrame):<br/>  """<br/>  Returns a dataframe with a list of labels for each Nearest Neighobor group<br/>  """<br/>  result = pd.DataFrame()<br/><br/>  # 1. Get a list of Nearest Hotels based on our model<br/>  for i in range(len(nns_results)):<br/><br/>    results = {} #empty dictionary to append each rows values<br/><br/>    # Each row in nns_results contains the indexs of the selected nearest neighbors<br/>    # We use those index to get the Hotel names in our main data set<br/>    results["Hotels"] = list(data.iloc[nns_results[i]].index)<br/><br/>    # 2. Get the values for each features for all Nearest Neighbors groups<br/>    for item in  data_clean.columns:<br/>      results[item] = list(data.iloc[nns_results[i]][item])<br/><br/>    # 3. Create a row for each Nearest Neighbor group and append to main DataFrame<br/>    df = pd.DataFrame([results])<br/>    result = pd.concat([result,df],axis=0)<br/><br/>  # 4. Decode the labels to the encoded columns<br/>  for key, val in encoders.items():<br/>    result[key] = result[key].apply(lambda x : list(val.inverse_transform(x)))<br/><br/>  result.reset_index(drop=True,inplace=True) # Reset the index for clarity<br/>  return result<br/><br/><br/>results_model_0 = clean_results(nns_results=nns_results_model_0,<br/>                                encoders=encoders,<br/>                                data=data_clean)<br/>results_model_0.head()</span></pre></div></div><div class="mq"><div class="ab cb"><div class="ll qt lm qu ln qv cf qw cg qx ci bh"><figure class="ml mm mn mo mp mq qz ra paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj re"><img src="../Images/4d295e60f17cf12d9ffb4270c94a3250.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*WqoZ5dDHQHfuw_WPxdS7Zg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Initial benchmark groups. Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="80de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Because we are using an unsupervised learning algorithm, there is not a widely available measure of accuracy. However, we can use domain knowledge to validate our groups.</p><p id="5db2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Visually inspecting the groups, we can see some benchmarking groups have a mix of Economy and Luxury hotels, which doesn't make business sense as the demand for hotels is fundamentally different.</p><p id="791d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">We can scroll to the data and note some of those differences, but can we come up with our own accuracy measure?</strong></p><p id="c7af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We want to create a function to measure the consistency of the recommended Benchmarking sets across each feature. One way of doing this is by calculating the variance in each feature for each set. For each cluster, we can compute an average of each feature variance, and we can then average each hotel cluster variance to get a total model score.</p><p id="a156" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From our domain knowledge, we know that in order to set up a comparable benchmark set, we need to prioritize hotels in the same Brand, possibly the same market, and the same country, and if we use different markets or countries, then the market tier should be the same.</p><p id="dd44" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With that in mind, we want our measure to have a higher penalty for variance in those features. To do so, we will use a weighted average to calculate each benchmark set variance. We will also print the variance of the key features and secondary features separately.</p><p id="318c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To sum up, to create our accuracy measure, we need to:</p><ol class=""><li id="f6da" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx rf qc qd bk"><strong class="ne fr">Calculate variance for categorical variables</strong>: One common approach is to use an “entropy-based” measure, where higher diversity in categories indicates higher entropy (variance).</li><li id="9fcc" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx rf qc qd bk"><strong class="ne fr">Calculate variance for numerical variables</strong>: we can compute the standard deviation or the range (difference between maximum and minimum values). This measures the spread of numerical data within each cluster.</li><li id="a4b9" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx rf qc qd bk"><strong class="ne fr">Normalize the data</strong>: normalize the variance scores for each category before applying weights to ensure that no single feature dominates the weighted average due to scale differences alone.</li><li id="2a55" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx rf qc qd bk"><strong class="ne fr">Apply weights for different metrics</strong>: Weight each type of variance based on its importance to the clustering logic.</li><li id="08c9" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx rf qc qd bk"><strong class="ne fr">Calculating weighted averages</strong>: Compute the weighted average of these variance scores for each cluster.</li><li id="9b86" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx rf qc qd bk"><strong class="ne fr">Aggregating scores across clusters</strong>: The total score is the average of these weighted variance scores across all clusters or rows. A lower average score would indicate that our model effectively groups similar hotels together, minimizing intra-cluster variance.</li></ol><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="0b52" class="qm od fq qa b bg qn qo l qp qq">from scipy.stats import entropy<br/>from sklearn.preprocessing import MinMaxScaler<br/>from collections import Counter<br/><br/>def categorical_variance(data):<br/>    """<br/>    Calculate entropy for a categorical variable from a list.<br/>    A higher entropy value indicates datas with diverse classes.<br/>    A lower entropy value indicates a more homogeneous subset of data.<br/>    """<br/>    # Count frequency of each unique value<br/>    value_counts = Counter(data)<br/>    total_count = sum(value_counts.values())<br/>    probabilities = [count / total_count for count in value_counts.values()]<br/>    return entropy(probabilities)<br/><br/>#set scoring weights giving higher weights to the most important features<br/>scoring_weights = {"BRAND": 0.3,<br/>           "Room_count": 0.025,<br/>           "Market": 0.25,<br/>           "Country": 0.15,<br/>           "Market Tier": 0.15,<br/>           "HCLASS": 0.05,<br/>           "Demand": 0.025,<br/>           "Price range": 0.025,<br/>           "distance_to_airport": 0.025}<br/><br/>def calculate_weighted_variance(df, weights):<br/>    """<br/>    Calculate the weighted variance score for clusters in the dataset<br/>    """<br/>    # Initialize a DataFrame to store the variances<br/>    variance_df = pd.DataFrame()<br/><br/>    # 1. Calculate variances for numerical features<br/>    numerical_features = ['Room_count', 'Demand', 'Price range', 'distance_to_airport']<br/>    for feature in numerical_features:<br/>        variance_df[f'{feature}'] = df[feature].apply(np.var)<br/><br/>    # 2. Calculate entropy for categorical features<br/>    categorical_features = ['BRAND', 'Market','Country','Market Tier','HCLASS']<br/>    for feature in categorical_features:<br/>        variance_df[f'{feature}'] = df[feature].apply(categorical_variance)<br/><br/>    # 3. Normalize the variance and entropy values<br/>    scaler = MinMaxScaler()<br/>    normalized_variances = pd.DataFrame(scaler.fit_transform(variance_df),<br/>                                        columns=variance_df.columns,<br/>                                        index=variance_df.index)<br/><br/>    # 4. Compute weighted average<br/><br/>    cat_weights = {f'{feature}': weights[f'{feature}'] for feature in categorical_features}<br/>    num_weights = {f'{feature}': weights[f'{feature}'] for feature in numerical_features}<br/><br/>    cat_weighted_scores = normalized_variances[categorical_features].mul(cat_weights)<br/>    df['cat_weighted_variance_score'] = cat_weighted_scores.sum(axis=1)<br/><br/>    num_weighted_scores = normalized_variances[numerical_features].mul(num_weights)<br/>    df['num_weighted_variance_score'] = num_weighted_scores.sum(axis=1)<br/><br/>    return df['cat_weighted_variance_score'].mean(), df['num_weighted_variance_score'].mean()</span></pre><p id="4862" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To keep our code clean and track our experiments , let’s also define a function to store the results of our experiments.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="0b12" class="qm od fq qa b bg qn qo l qp qq"># define a function to store the results of our experiments<br/>def model_score(data: pd.DataFrame,<br/>                weights: dict = scoring_weights,<br/>                model_name: str ="model_0"):<br/>  cat_score,num_score = calculate_weighted_variance(data,weights)<br/>  results ={"Model": model_name,<br/>            "Primary features score": cat_score,<br/>            "Secondary features score": num_score}<br/>  return results<br/><br/>model_0_score= model_score(results_model_0,scoring_weights)<br/>model_0_score</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rg"><img src="../Images/07f338459cb382b9bf734562b11c454b.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*_m4bUfAEMrGtfYCPTh6Tcw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Baseline model results.</figcaption></figure><p id="ded0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we have a baseline, let’s see if we can improve our model.</p><h2 id="9069" class="rh od fq bf oe ri rj rk oh rl rm rn ok nl ro rp rq np rr rs rt nt ru rv rw rx bk">Improving our Model Through Experimentation</h2><p id="b2e9" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Up until now, we did not have to know what was going on under the hood when we ran this code:</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="061f" class="qm od fq qa b bg qn qo l qp qq">nns = NearestNeighbors()<br/>nns.fit(data_scaled)<br/>nns_results_model_0 = nns.kneighbors(data_scaled)[1]</span></pre><p id="1a49" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To improve our model, we will need to understand the model parameters and how we can interact with them to get better benchmark sets.</p><p id="0d1a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s start by looking at the Scikit Learn documentation and source code:</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="08d0" class="qm od fq qa b bg qn qo l qp qq"># the below is taken directly from scikit learn source<br/><br/>from sklearn.neighbors._base import KNeighborsMixin, NeighborsBase, RadiusNeighborsMixin<br/><br/><br/>class NearestNeighbors_(KNeighborsMixin, RadiusNeighborsMixin, NeighborsBase):<br/>    """Unsupervised learner for implementing neighbor searches.<br/>    Parameters<br/>    ----------<br/>    n_neighbors : int, default=5<br/>        Number of neighbors to use by default for :meth:`kneighbors` queries.<br/><br/>    radius : float, default=1.0<br/>        Range of parameter space to use by default for :meth:`radius_neighbors`<br/>        queries.<br/><br/>    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'<br/>        Algorithm used to compute the nearest neighbors:<br/><br/>        - 'ball_tree' will use :class:`BallTree`<br/>        - 'kd_tree' will use :class:`KDTree`<br/>        - 'brute' will use a brute-force search.<br/>        - 'auto' will attempt to decide the most appropriate algorithm<br/>          based on the values passed to :meth:`fit` method.<br/><br/>        Note: fitting on sparse input will override the setting of<br/>        this parameter, using brute force.<br/><br/>    leaf_size : int, default=30<br/>        Leaf size passed to BallTree or KDTree.  This can affect the<br/>        speed of the construction and query, as well as the memory<br/>        required to store the tree.  The optimal value depends on the<br/>        nature of the problem.<br/><br/>    metric : str or callable, default='minkowski'<br/>        Metric to use for distance computation. Default is "minkowski", which<br/>        results in the standard Euclidean distance when p = 2. See the<br/>        documentation of `scipy.spatial.distance<br/>        &lt;https://docs.scipy.org/doc/scipy/reference/spatial.distance.html&gt;`_ and<br/>        the metrics listed in<br/>        :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric<br/>        values.<br/><br/>    p : float (positive), default=2<br/>        Parameter for the Minkowski metric from<br/>        sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is<br/>        equivalent to using manhattan_distance (l1), and euclidean_distance<br/>        (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.<br/><br/>    metric_params : dict, default=None<br/>        Additional keyword arguments for the metric function.<br/>  """<br/><br/>    def __init__(<br/>        self,<br/>        *,<br/>        n_neighbors=5,<br/>        radius=1.0,<br/>        algorithm="auto",<br/>        leaf_size=30,<br/>        metric="minkowski",<br/>        p=2,<br/>        metric_params=None,<br/>        n_jobs=None,<br/>    ):<br/>        super().__init__(<br/>            n_neighbors=n_neighbors,<br/>            radius=radius,<br/>            algorithm=algorithm,<br/>            leaf_size=leaf_size,<br/>            metric=metric,<br/>            p=p,<br/>            metric_params=metric_params,<br/>            n_jobs=n_jobs,<br/>        )</span></pre><p id="b968" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are quite a few things going on here.</p><p id="c577" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx px py pz qa b">Nearestneighbor</code> class inherits from<code class="cx px py pz qa b">NeighborsBase</code>, which is the case class for nearest neighbor estimators. This class handles the common functionalities required for nearest-neighbor searches, such as</p><ul class=""><li id="436c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk">n_neighbors (the number of neighbors to use)</li><li id="6239" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">radius (the radius for radius-based neighbor searches)</li><li id="0d86" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">algorithm (the algorithm used to compute the nearest neighbors, such as ‘ball_tree’, ‘kd_tree’, or ‘brute’)</li><li id="8995" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">metric (the distance metric to use)</li><li id="a928" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk">metric_params (additional keyword arguments for the metric function)</li></ul><p id="337d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx px py pz qa b">Nearestneighbor</code> class also inherits from<code class="cx px py pz qa b">KNeighborsMixin</code> and <code class="cx px py pz qa b">RadiusNeighborsMixin</code>classes. These Mixin classes add specific neighbor-search functionalities to the <code class="cx px py pz qa b">Nearestneighbor</code></p><ul class=""><li id="91ee" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qb qc qd bk"><code class="cx px py pz qa b">KNeighborsMixin</code> provides functionality to find the nearest fixed number k of neighbors to a point. It does that by finding the distance to the neighbors and their indices and constructing a graph of connections between points based on the k-nearest neighbors of each point.</li><li id="a368" class="nc nd fq ne b go qe ng nh gr qf nj nk nl qg nn no np qh nr ns nt qi nv nw nx qb qc qd bk"><code class="cx px py pz qa b">RadiusNeighborsMixin</code> is based on the radius neighbors algorithm, which finds all neighbors within a given radius of a point. This method is useful in scenarios where the focus is on capturing all points within a meaningful distance threshold rather than a fixed number of points.</li></ul><p id="88ef" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on our scenario, KNeighborsMixin provides the functionality we need.</p><p id="6f77" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We need to understand one key parameter before we can improve our model; this is the distance metric.</p><h1 id="3caf" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">A quick introduction to distance</h1><p id="4734" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The documentation mentions that the NearestNeighbor algorithm uses the “Minkowski” distance by default and gives us a reference to the SciPy API.</p><p id="d410" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In <code class="cx px py pz qa b">scipy.spatial.distance</code>, we can see two mathematical representations of "Minkowski" distance:</p><blockquote class="ny nz oa"><p id="accc" class="nc nd ob ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="fq">∥u−v∥ p​=( i ∑​∣u i​−v i​∣ p ) 1/p</em></p></blockquote><p id="d46d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This formula calculates the p-th root of the sum of powered differences across all elements.</p><p id="bfce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The second mathematical representation of “Minkowski” distance is:</p><blockquote class="ny nz oa"><p id="9e49" class="nc nd ob ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="fq">∥u−v∥ p​=( i ∑​w i​(∣u i​−v i​∣ p )) 1/p</em></p></blockquote><p id="b441" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is very similar to the first one, but it introduces weights <code class="cx px py pz qa b">wi</code> to the differences, emphasizing or de-emphasizing specific dimensions. This is useful where certain features are more relevant than others. By default, the setting is None, which gives all features the same weight of 1.0.</p><p id="e20e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">This is a great option for improving our model as it allows us to pass domain knowledge to our model and emphasize similarities that are most relevant to users.</strong></p><p id="d47d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we look at the formulas, we see the parameter. <code class="cx px py pz qa b">p</code>. This parameter affects the "path" the algorithm takes to calculate the distance. <strong class="ne fr">By default, p=2, which represents the Euclidian distance.</strong></p><p id="4e0d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can think of the Euclidian distance as calculating the distance by drawing a straight line between 2 points. This is usally the shortest distance, however, this is not always the most desirable way of calculating the distance, specially in higher dimention spaces. For more information on why this is the case, there is this great paper online: <a class="af nb" href="https://www.google.com/url?q=https%3A%2F%2Fbib.dbvis.de%2FuploadedFiles%2F155.pdf" rel="noopener ugc nofollow" target="_blank">https://bib.dbvis.de/uploadedFiles/155.pdf</a></p><p id="770b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Another common value for p is 1. This represents the Manhattan distance.</strong> You think of it as the distance between two points measured along a grid-like path.</p><p id="8c36" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">On the other hand, if we increase p towards infinity, we end up with the Chebyshev distance, defined as the maximum absolute difference between any corresponding elements of the vectors</strong>. It essentially measures the worst-case difference, making it useful in scenarios where you want to ensure that no single feature varies too much.</p><p id="0e84" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By reading and getting familiar with the documentation, we have uncovered a few possible options to improve our model.</p><h1 id="e7dc" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Experiment 1: Baseline model with n_neighbors = 4</h1><p id="65b8" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">By default n_neighbors is 5, however, for our benchmark set, we want to compare each hotel to the 3 most similar hotels. To do so, we need to set n_neighbors = 4 (Subject hotel + 3 peers)</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="1033" class="qm od fq qa b bg qn qo l qp qq">nns_1= NearestNeighbors(n_neighbors=4)<br/>nns_1.fit(data_scaled)<br/>nns_1_results_model_1 = nns_1.kneighbors(data_scaled)[1]<br/>results_model_1 = clean_results(nns_results=nns_1_results_model_1,<br/>                                encoders=encoders,<br/>                                data=data_clean)<br/>model_1_score= model_score(results_model_1,scoring_weights,model_name="baseline_k_4")<br/>model_1_score</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ry"><img src="../Images/573845ae760e04d4c0d1b3a414bd3e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*KjiU-bYRRR0JpKYZikF34g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Slight improvement in our primary features. Image by author</figcaption></figure><h1 id="f62d" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Experiment 2: adding weights</h1><p id="6fcc" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Based on the documentation, we can pass weights to the distance calculation to emphasize the relationship across some features. Based on our domain knowledge, we have identified the features we want to emphasize, in this case, Brand, Market, Country, and Market Tier.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="77b7" class="qm od fq qa b bg qn qo l qp qq"># set up weights for distance calculation<br/>weights_dict =  {"BRAND": 5,<br/>           "Room_count": 2,<br/>           "Market": 4,<br/>           "Country": 3,<br/>           "Market Tier": 3,<br/>           "HCLASS": 1.5,<br/>           "Demand": 1,<br/>           "Price range": 1,<br/>           "distance_to_airport": 1}<br/># Transform the wieghts dictionnary into a list by keeping the scaled data column order<br/>weights = [ weights_dict[idx] for idx in list(scaler.get_feature_names_out())]<br/><br/>nns_2= NearestNeighbors(n_neighbors=4,metric_params={ 'w': weights})<br/>nns_2.fit(data_scaled)<br/>nns_2_results_model_2 = nns_2.kneighbors(data_scaled)[1]<br/>results_model_2 = clean_results(nns_results=nns_2_results_model_2,<br/>                                encoders=encoders,<br/>                                data=data_clean)<br/>model_2_score= model_score(results_model_2,scoring_weights,model_name="baseline_with_weights")<br/>model_2_score</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rz"><img src="../Images/a0c93e3306379ea2efef56846f0f095f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*np1NyUg1RR4azjIArfPEEQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Primary features score keeps improving. Image by author</figcaption></figure><p id="288e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Passing domain knowledge to the model via weights increased the score significantly. Next, let’s test the impact of the distance measure.</p><h1 id="3582" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Experiment 3: use Manhattan distance</h1><p id="1d58" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">So far, we have been using the Euclidian distance. Let’s see what happens if we use the Manhattan distance instead.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="3c7f" class="qm od fq qa b bg qn qo l qp qq">nns_3= NearestNeighbors(n_neighbors=4,p=1,metric_params={ 'w': weights})<br/>nns_3.fit(data_scaled)<br/>nns_3_results_model_3 = nns_3.kneighbors(data_scaled)[1]<br/>results_model_3 = clean_results(nns_results=nns_3_results_model_3,<br/>                                encoders=encoders,<br/>                                data=data_clean)<br/>model_3_score= model_score(results_model_3,scoring_weights,model_name="Manhattan_with_weights")<br/>model_3_score</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj sa"><img src="../Images/2139801c701f67fa1332a9ac59be5543.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*o7QgVz8TwQhiP1PmWWejug.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Significant decrease in primary score. image by author</figcaption></figure><h1 id="402d" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Experiment 4: use Chebyshev distance</h1><p id="0691" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Decreasing p to 1 resulted in some good improvements. Let’s see what happens as p approximates infinity.</p><p id="2131" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To use the Chebyshev distance, we will change the metric parameter to <code class="cx px py pz qa b">Chebyshev.</code> The default sklearn Chebyshev metric doesn’t have a weight parameter. To get around this, we will define a custom <code class="cx px py pz qa b">weighted_chebyshev</code> metric.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="9fa7" class="qm od fq qa b bg qn qo l qp qq">#  Define the custom weighted Chebyshev distance function<br/>def weighted_chebyshev(u, v, w):<br/>    """Calculate the weighted Chebyshev distance between two points."""<br/>    return np.max(w * np.abs(u - v))<br/><br/>nns_4 = NearestNeighbors(n_neighbors=4,metric=weighted_chebyshev,metric_params={ 'w': weights})<br/>nns_4.fit(data_scaled)<br/>nns_4_results_model_4 = nns_4.kneighbors(data_scaled)[1]<br/>results_model_4 = clean_results(nns_results=nns_4_results_model_4,<br/>                                encoders=encoders,<br/>                                data=data_clean)<br/>model_4_score= model_score(results_model_4,scoring_weights,model_name="Chebyshev_with_weights")<br/>model_4_score</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj sb"><img src="../Images/17c6cda0b99cf5ebd7ac39902c0cf08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*xcgtk2jThvW1w6P4A7sTow.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Better than the baseline but higher than the previous experiment. Image by author</figcaption></figure><p id="ddce" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We managed to decrease the primary feature variance scores through experimentation.</p><p id="e8c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s visualize the results.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="2f21" class="qm od fq qa b bg qn qo l qp qq">results_df = pd.DataFrame([model_0_score,model_1_score,model_2_score,model_3_score,model_4_score]).set_index("Model")<br/>results_df.plot(kind='barh')</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj sc"><img src="../Images/e5deb0e34f1ce91bf97eb1f80d7ec4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NVrJ4VGqUxyfBg-DtqCkeg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Experimentation results. Image by author</figcaption></figure><p id="71e5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using Manhattan distance with weights seems to give the most accurate benchmark sets according to our needs.</p><p id="a79b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The last step before implementing the benchmark sets would be to examine the sets with the highest Primary features scores and identify what steps to take with them.</p><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="73cb" class="qm od fq qa b bg qn qo l qp qq"># Histogram of Primary features score<br/>results_model_3["cat_weighted_variance_score"].plot(kind="hist")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj sd"><img src="../Images/9fe38e3b7fcdc89974b7cebd25e6e230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*g1PzPNexmJ3gcr8Q9RC8MA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Score distribution. Image by author</figcaption></figure><pre class="ml mm mn mo mp qj qa qk bp ql bb bk"><span id="fea4" class="qm od fq qa b bg qn qo l qp qq">exceptions = results_model_3[results_model_3["cat_weighted_variance_score"]&gt;=0.4]<br/><br/>print(f" There are {exceptions.shape[0]} benchmark sets with significant variance across the primary features")</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj se"><img src="../Images/1c862f02f1fa83283cbd36ef588e4292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*wrV4zpYecpb5zlNkF7RoXg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="bd3d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These 18 cases will need to be reviewed to ensure the benchmark sets are relevant.</p><p id="421c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, with a few lines of code and some understanding of Nearest neighbor search, we managed to set internal benchmark sets. We can now distribute the sets and start measuring hotels' KPIs against their benchmark sets.</p><p id="a662" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You don’t always have to focus on the most cutting-edge machine learning methods to deliver value. Very often, simple machine learning can deliver great value.</p><p id="1c2c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What are some low-hanging fruits in your business that you could easily tackle with Machine learning?</p><h1 id="e092" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">REFERENCES</h1><p id="3597" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">World Bank. “World Development Indicators.” Retrieved June 11, 2024, from <a class="af nb" href="https://datacatalog.worldbank.org/search/dataset/0038117" rel="noopener ugc nofollow" target="_blank">https://datacatalog.worldbank.org/search/dataset/0038117</a></p><p id="4630" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Aggarwal, C. C., Hinneburg, A., &amp; Keim, D. A. (n.d.). On the Surprising Behavior of Distance Metrics in High Dimensional Space. IBM T. J. Watson Research Center and Institute of Computer Science, University of Halle. Retrieved from <a class="af nb" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">https://bib.dbvis.de/uploadedFiles/155.pdf</a></p><p id="53df" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SciPy v1.10.1 Manual. <code class="cx px py pz qa b">scipy.spatial.distance.minkowski</code>. Retrieved June 11, 2024, from <a class="af nb" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html" rel="noopener ugc nofollow" target="_blank">https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html</a></p><p id="00db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GeeksforGeeks. Haversine formula to find distance between two points on a sphere. Retrieved June 11, 2024, from <a class="af nb" href="https://www.geeksforgeeks.org/haversine-formula-to-find-distance-between-two-points-on-a-sphere/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/haversine-formula-to-find-distance-between-two-points-on-a-sphere/</a></p><p id="7e55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">scikit-learn. Neighbors Module. Retrieved June 11, 2024, from <a class="af nb" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors</a></p></div></div></div></div>    
</body>
</html>