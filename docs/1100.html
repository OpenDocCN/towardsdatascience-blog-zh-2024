<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Practical Guide to Proximal Policy Optimization in JAX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Practical Guide to Proximal Policy Optimization in JAX</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01">https://towardsdatascience.com/breaking-down-state-of-the-art-ppo-implementations-in-jax-6f102c06c149?source=collection_archive---------7-----------------------#2024-05-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="06b0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">All the tricks and details you wish you knew about PPO</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan Pégoud" class="l ep by dd de cx" src="../Images/9314b76c2be56bda8b73b4badf9e3e4d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ep3ebfAfE1csq42qzviNgg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ryanpegoud?source=post_page---byline--6f102c06c149--------------------------------" rel="noopener follow">Ryan Pégoud</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6f102c06c149--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/9706450ef22f9e46c08b73bbee0735d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*9iiDaMnE92OHLdVX"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Photo by <a class="af nj" href="https://unsplash.com/@lorenzoherrera?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Lorenzo Herrera</a> on <a class="af nj" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="87b4" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Since its publication in a <a class="af nj" href="https://arxiv.org/pdf/1707.06347.pdf" rel="noopener ugc nofollow" target="_blank">2017 paper by OpenAI</a>, Proximal Policy Optimization (PPO) is widely regarded as one of the state-of-the-art algorithms in Reinforcement Learning. Indeed, PPO has demonstrated remarkable performances across various tasks, from <a class="af nj" href="https://openai.com/research/openai-five" rel="noopener ugc nofollow" target="_blank">attaining superhuman performances in Dota 2</a> teams to solving a <a class="af nj" href="https://openai.com/research/solving-rubiks-cube" rel="noopener ugc nofollow" target="_blank">Rubik’s cube with a single robotic hand</a> while maintaining three main advantages: simplicity, stability, and sample efficiency.</p><p id="a62d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">However, implementing RL algorithms from scratch is notoriously difficult and error-prone, given the numerous error sources and implementation details to be aware of.</p><p id="7dde" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In this article, we’ll focus on breaking down the clever tricks and programming concepts used in a popular implementation of PPO in JAX. Specifically, we’ll focus on the <a class="af nj" href="https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py" rel="noopener ugc nofollow" target="_blank">implementation featured in the PureJaxRL library</a>, developed by <a class="af nj" href="https://chrislu.page" rel="noopener ugc nofollow" target="_blank">Chris Lu</a>.</p><p id="c135" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><em class="og">Disclaimer: Rather than diving too deep into theory, this article covers the practical implementation details and (numerous) tricks used in popular versions of PPO. Should you require any reminders about PPO’s theory, please refer to the “</em><strong class="nm fr"><em class="og">references</em></strong><em class="og">” section at the end of this article. Additionally, all the code (minus the added comments) is copied directly from PureJaxRL for pedagogical purposes.</em></p><div class="oh oi oj ok ol om"><a href="https://github.com/luchris429/purejaxrl/tree/main?source=post_page-----6f102c06c149--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="on ab ig"><div class="oo ab co cb op oq"><h2 class="bf fr hw z io or iq ir os it iv fp bk">UGitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations</h2><div class="ot l"><h3 class="bf b hw z io or iq ir os it iv dx">Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl development by creating an account on…</h3></div><div class="ou l"><p class="bf b dy z io or iq ir os it iv dx">github.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa lr om"/></div></div></a></div><h1 id="e5c7" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk"><strong class="al">Actor-Critic Architectures</strong></h1><p id="1806" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">Proximal Policy Optimization is categorized within the policy gradient family of algorithms, a subset of which includes actor-critic methods. The designation ‘actor-critic’ reflects the dual components of the model:</p><ul class=""><li id="95b7" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk">The <strong class="nm fr">actor network</strong> creates a <strong class="nm fr">distribution over actions</strong> given the current state of the environment and returns an action sampled from this distribution. Here, the actor network comprises three dense layers separated by two activation layers (either ReLU or hyperbolic tangeant) and a final categorical layer applying the <strong class="nm fr">softmax</strong> function to the computed distribution.</li><li id="60ee" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">The <strong class="nm fr">critic network</strong> <strong class="nm fr">estimates the value function of the current state</strong>, in other words, how good a particular action is at a given time. Its architecture is almost identical to the actor network, except for the final softmax layer. Indeed, the critic network doesn’t apply any activation function to the final dense layer outputs as it performs a regression task.</li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qk"><img src="../Images/1e67cc67befceb9cb6015af67bab3bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzKJYLAu5ZeXaFE6YKU_Nw.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Actor-critic architecture, as defined in PureJaxRL (illustration made by the author)</figcaption></figure><p id="d6d4" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Additionally, this implementation pays particular attention to <strong class="nm fr">weight initialization</strong> in dense layers. Indeed, all dense layers are initialized by <strong class="nm fr">orthogonal matrices</strong> with specific coefficients. This initialization strategy has been shown to <strong class="nm fr">preserve the gradient norms</strong> (i.e. scale) during forward passes and backpropagation, leading to <strong class="nm fr">smoother convergence</strong> and limiting the risks of vanishing or exploding gradients[1].</p><p id="aff2" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Orthogonal initialization is used in conjunction with specific scaling coefficients:</p><ul class=""><li id="d443" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk"><strong class="nm fr">Square root of 2</strong>: Used for the first two dense layers of both networks, this factor aims to <strong class="nm fr">compensate for the variance reduction</strong> induced by ReLU activations (as inputs with negative values are set to 0). For the tanh activation, the Xavier initialization is a popular alternative[2].</li><li id="8efd" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">0.01: </strong>Used in the last dense layer of the actor network, this factor helps to <strong class="nm fr">minimize the initial differences in logit values</strong> before applying the softmax function. This will reduce the difference in action probabilities and thus <strong class="nm fr">encourage early exploration</strong>.</li><li id="c78e" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">1: </strong>As the critic network is performing a regression task, we do not scale the initial weights.</li></ul><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Actor critic network (source: PureJaxRL, Chris Lu)</figcaption></figure><h1 id="50a2" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">Training Loop</h1><p id="da0e" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">The training loop is divided into 3 main blocks that share similar coding patterns, taking advantage of Jax’s functionalities:</p><ol class=""><li id="686a" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qo qd qe bk"><strong class="nm fr">Trajectory collection:</strong> First, we’ll interact with the environment for a set number of steps and collect observations and rewards.</li><li id="f261" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qo qd qe bk"><strong class="nm fr">Generalized Advantage Estimation (GAE):</strong> Then, we’ll approximate the expected return for each trajectory by computing the generalized advantage estimation.</li><li id="b531" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qo qd qe bk"><strong class="nm fr">Update step: </strong>Finally, we’ll compute the gradient of the loss and update the network parameters via gradient descent.</li></ol><p id="fd4e" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Before going through each block in detail, here’s a quick reminder about the <code class="cx qp qq qr qs b">jax.lax.scan</code><em class="og"> </em>function that will show up multiple times throughout the code:</p><h2 id="6c0d" class="qt pc fq bf pd qu qv qw pg qx qy qz pj nt ra rb rc nx rd re rf ob rg rh ri rj bk">Jax.lax.scan</h2><p id="b0d1" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">A common programming pattern in JAX consists of defining a function that acts on a single sample and using <code class="cx qp qq qr qs b">jax.lax.scan</code><em class="og"> </em>to <strong class="nm fr">iteratively apply it to elements of a sequence</strong> or an array, while carrying along some state.<br/>For instance, we’ll apply it to the <code class="cx qp qq qr qs b">step</code> function to step our environment N consecutive times while carrying the new state of the environment through each iteration.</p><p id="156d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In pure Python, we could proceed as follows:</p><pre class="ms mt mu mv mw rk qs rl bp rm bb bk"><span id="1980" class="rn pc fq qs b bg ro rp l rq rr">trajectories = []<br/><br/>for step in range(n_steps):<br/>  action = actor_network(obs)<br/>  obs, state, reward, done, info = env.step(action, state)<br/>  trajectories.append(tuple(obs, state, reward, done, info))</span></pre><p id="03f4" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">However, we avoid writing such loops in JAX for performance reasons (as pure Python loops are incompatible with JIT compilation). The alternative is <code class="cx qp qq qr qs b">jax.lax.scan</code><em class="og"> </em>which is equivalent to:</p><pre class="ms mt mu mv mw rk qs rl bp rm bb bk"><span id="81f5" class="rn pc fq qs b bg ro rp l rq rr">def scan(f, init, xs, length=None):<br/>  """Example provided in the JAX documentation."""<br/>  if xs is None:<br/>    xs = [None] * length<br/><br/>  carry = init<br/>  ys = []<br/>  for x in xs:<br/>    # apply function f to current state<br/>    # and element x<br/>    carry, y = f(carry, x) <br/>    ys.append(y)<br/>  return carry, np.stack(ys)</span></pre><p id="f97b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Using <code class="cx qp qq qr qs b">jax.lax.scan</code> is more efficient than a Python loop because it allows the transformation to be optimized and executed as a single compiled operation rather than interpreting each loop iteration at runtime.</p><p id="c69b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We can see that the <code class="cx qp qq qr qs b">scan</code> function takes multiple arguments:</p><ul class=""><li id="6de4" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk"><strong class="nm fr">f:</strong> A function that is applied at each step. It takes the current state and an element of <code class="cx qp qq qr qs b">xs</code> (or a placeholder if <code class="cx qp qq qr qs b">xs</code> is <code class="cx qp qq qr qs b">None</code>) and returns the updated state and an output.</li><li id="971c" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">init:</strong> The initial state that <code class="cx qp qq qr qs b">f</code> will use in its first invocation.</li><li id="ba14" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">xs:</strong> A sequence of inputs that are iteratively processed by <code class="cx qp qq qr qs b">f</code>. If <code class="cx qp qq qr qs b">xs</code> is <code class="cx qp qq qr qs b">None</code>, the function simulates a loop with <code class="cx qp qq qr qs b">length</code> iterations using <code class="cx qp qq qr qs b">None</code> as the input for each iteration.</li><li id="46d4" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">length:</strong> Specifies the number of iterations if <code class="cx qp qq qr qs b">xs</code> is <code class="cx qp qq qr qs b">None</code>, ensuring that the function can still operate without explicit inputs.</li></ul><p id="f37d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Additionally, <code class="cx qp qq qr qs b">scan</code> returns:</p><ul class=""><li id="39f6" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk"><strong class="nm fr">carry:</strong> The final state after all iterations.</li><li id="cb58" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk"><strong class="nm fr">ys:</strong> An array of outputs corresponding to each step’s application of <code class="cx qp qq qr qs b">f</code>, stacked for easy analysis or further processing.</li></ul><p id="6fb1" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Finally, <code class="cx qp qq qr qs b">scan</code> can be used in combination with <code class="cx qp qq qr qs b">vmap</code> to scan a function over multiple dimensions in parallel. As we’ll see in the next section, this allows us to interact with several environments in parallel to collect trajectories rapidly.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rs"><img src="../Images/123e5e98f340935db435e9aab42629a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUNkLv6GDm03xIM2J_HWPQ.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Illustration of vmap, scan, and scan + vmap in the context of the step function (made by the author)</figcaption></figure><h1 id="1f53" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">1. Trajectory Collection</h1><p id="467f" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">As mentioned in the previous section, the trajectory collection block consists of a <code class="cx qp qq qr qs b">step</code> function scanned across N iterations. This <code class="cx qp qq qr qs b">step </code>function successively:</p><ul class=""><li id="a952" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk">Selects an action using the actor network</li><li id="36cc" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">Steps the environment</li><li id="3af7" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">Stores transition data in a <code class="cx qp qq qr qs b">transition</code> tuple</li><li id="c497" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">Stores the model parameters, the environment state, the current observation, and rng keys in a <code class="cx qp qq qr qs b">runner_state</code> tuple</li><li id="3abb" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">Returns <code class="cx qp qq qr qs b">runner_state</code> and <code class="cx qp qq qr qs b">transition</code></li></ul><p id="a9ac" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Scanning this function returns the latest <code class="cx qp qq qr qs b">runner_state</code> and <code class="cx qp qq qr qs b">traj_batch</code>, an array of <code class="cx qp qq qr qs b">transition</code> tuples. In practice, transitions are collected from multiple environments in parallel for efficiency as indicated by the use of <code class="cx qp qq qr qs b">jax.vmap(env.step, …)</code>(for more details about vectorized environments and <code class="cx qp qq qr qs b">vmap</code>, refer to my <a class="af nj" href="https://medium.com/towards-data-science/vectorize-and-parallelize-rl-environments-with-jax-q-learning-at-the-speed-of-light-49d07373adf5" rel="noopener">previous article</a>).</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">env step function (source: PureJaxRL, Chris Lu)</figcaption></figure><h1 id="921b" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">2. Generalized Advantage Estimation</h1><p id="2197" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">After collecting trajectories, we need to compute the <strong class="nm fr">advantage function, </strong>a crucial component of PPO’s loss function. The advantage function measures how much better a specific action is compared to the average action in a given state:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq rt"><img src="../Images/9025657609723998de22ebbc42479942.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/0*8JKsD0mv7SRmWbW3.png"/></div></figure><p id="30cc" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Where <strong class="nm fr">Gt </strong>is the return at time <strong class="nm fr"><em class="og">t</em></strong><em class="og"> </em>and <strong class="nm fr">V(St) is </strong>the value of state <strong class="nm fr"><em class="og">s</em></strong><em class="og"> </em>at time <strong class="nm fr"><em class="og">t</em></strong><em class="og">.</em></p><p id="192b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As the return is generally unknown, we have to approximate the advantage function. A popular solution is <strong class="nm fr">generalized advantage estimation</strong>[3], defined as follows:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ru"><img src="../Images/0ccf1cf4804dffef16e94feee6c2eb2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vtoc2sT_x8Vg-bTR.png"/></div></div></figure><p id="eeb0" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With <strong class="nm fr">γ</strong> the discount factor, <strong class="nm fr">λ</strong> a parameter that controls the trade-off between bias and variance in the estimate, and<strong class="nm fr"> <em class="og">δt</em></strong><em class="og"> </em>the temporal difference error at time <strong class="nm fr"><em class="og">t</em></strong>:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq rv"><img src="../Images/298fa9ce55afb829287e91b9688fa335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/0*VYM1lRnvvZ-8dWEO.png"/></div></figure><p id="a3f9" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As we can see, the value of the GAE at time <em class="og">t </em>depends on the GAE at future timesteps. Therefore, we compute it backward, starting from the end of a trajectory. For example, for a trajectory of 3 transitions, we would have:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq rw"><img src="../Images/cbc3f0a15854dd74ba9a902e27bd939d.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/0*5DxYwJmbHIqtkIfy.png"/></div></figure><p id="bccd" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Which is equivalent to the following recursive form:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq rx"><img src="../Images/8fbb9ca9d51bbddd821b3dc3747ee3bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*IDa--MnSBjNWuwDR.png"/></div></figure><p id="e746" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Once again, we use <code class="cx qp qq qr qs b">jax.lax.scan</code> on the trajectory batch (this time in reverse order) to iteratively compute the GAE.</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">generalized advantage estimation (source: PureJaxRL, Chris Lu)</figcaption></figure><p id="5a09" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Note that the function returns <code class="cx qp qq qr qs b">advantages + traj_batch.value</code> as a second output, which is equivalent to the return according to the first equation of this section.</p><h1 id="9377" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">3. Update step</h1><p id="e99f" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">The final block of the training loop defines the loss function, computes its gradient, and performs gradient descent on minibatches. Similarly to previous sections, the update step is an arrangement of several functions in a hierarchical order:</p><pre class="ms mt mu mv mw rk qs rl bp rm bb bk"><span id="8653" class="rn pc fq qs b bg ro rp l rq rr">def _update_epoch(update_state, unused):<br/>  """<br/>  Scans update_minibatch over shuffled and permuted <br/>  mini batches created from the trajectory batch.<br/>  """<br/><br/>  def _update_minbatch(train_state, batch_info):<br/>    """<br/>    Wraps loss_fn and computes its gradient over the <br/>    trajectory batch before updating the network parameters.<br/>    """<br/>    ...<br/>    <br/>    def _loss_fn(params, traj_batch, gae, targets):<br/>      """<br/>      Defines the PPO loss and computes its value.<br/>      """<br/>      ...</span></pre><p id="3ade" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Let’s break them down one by one, starting from the innermost function of the update step.</p><h2 id="a039" class="qt pc fq bf pd qu qv qw pg qx qy qz pj nt ra rb rc nx rd re rf ob rg rh ri rj bk">3.1 Loss function</h2><p id="b577" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">This function aims to define and compute the PPO loss, originally defined as:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ry"><img src="../Images/c0b4c98a735ce19abbf8e5090017b346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zYp6PTEfKoMjU2RT.png"/></div></div></figure><p id="629a" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Where:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rz"><img src="../Images/11464fd7edf1eeda50b976b7ab409280.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qaiCVqnYPO2nue09.png"/></div></div></figure><p id="112f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">However, the PureJaxRL implementation features some tricks and differences compared to the original PPO paper[4]:</p><ul class=""><li id="3a58" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk">The paper defines the PPO loss in the context of gradient ascent whereas the implementation performs gradient descent. Therefore, the sign of each loss component is reversed.</li><li id="8241" class="nk nl fq nm b go qf no np gr qg nr ns nt qh nv nw nx qi nz oa ob qj od oe of qc qd qe bk">The value function term is modified to include an additional clipped term. This could be seen as a way to make the value function updates more conservative (as for the clipped surrogate objective):</li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rz"><img src="../Images/add641cf22538f261da9af6a70c8c46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YQZP3hq6jiAxK_Ta.png"/></div></div></figure><ul class=""><li id="fd6c" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qc qd qe bk">The GAE is standardized.</li></ul><p id="ee9d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Here’s the complete loss function:</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">PPO loss function (source: PureJaxRL, Chris Lu)</figcaption></figure><h2 id="6ce1" class="qt pc fq bf pd qu qv qw pg qx qy qz pj nt ra rb rc nx rd re rf ob rg rh ri rj bk">3.2 Update Minibatch</h2><p id="be3a" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">The <code class="cx qp qq qr qs b">update_minibatch</code> function is essentially a wrapper around <code class="cx qp qq qr qs b">loss_fn</code> used to compute its gradient over the trajectory batch and update the model parameters stored in <code class="cx qp qq qr qs b">train_state</code>.</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">update minibatch (source: PureJaxRL, Chris Lu)</figcaption></figure><h2 id="e572" class="qt pc fq bf pd qu qv qw pg qx qy qz pj nt ra rb rc nx rd re rf ob rg rh ri rj bk">3.3 Update Epoch</h2><p id="8346" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">Finally, <code class="cx qp qq qr qs b">update_epoch</code> wraps <code class="cx qp qq qr qs b">update_minibatch</code> and applies it on minibatches. Once again, <code class="cx qp qq qr qs b">jax.lax.scan</code> is used to apply the update function on all minibatches iteratively.</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">update epoch (source: PureJaxRL, Chris Lu)</figcaption></figure><h1 id="ae52" class="pb pc fq bf pd pe pf gq pg ph pi gt pj pk pl pm pn po pp pq pr ps pt pu pv pw bk">Conclusion</h1><p id="612e" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk">From there, we can wrap all of the previous functions in an <code class="cx qp qq qr qs b">update_step</code> function and use <code class="cx qp qq qr qs b">scan</code> one last time for N steps to complete the training loop.</p><p id="e760" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">A global view of the training loop would look like this:</p><figure class="ms mt mu mv mw mj"><div class="ql io l ed"><div class="qm qn l"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Summary of the training script (source: PureJaxRL, Chris Lu)</figcaption></figure><p id="543b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We can now run a fully compiled training loop using <code class="cx qp qq qr qs b">jax.jit(train(rng))</code> or even train multiple agents in parallel using <code class="cx qp qq qr qs b">jax.vmap(train(rng))</code>.</p><p id="cce3" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">There we have it! We covered the essential building blocks of the PPO training loop as well as common programming patterns in JAX.</p><p id="0844" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To go further, I highly recommend reading the <a class="af nj" href="https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py" rel="noopener ugc nofollow" target="_blank">full training script</a> in detail and running example notebooks on the PureJaxRL repository.</p><div class="oh oi oj ok ol om"><a href="https://github.com/luchris429/purejaxrl?source=post_page-----6f102c06c149--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="on ab ig"><div class="oo ab co cb op oq"><h2 class="bf fr hw z io or iq ir os it iv fp bk">GitHub - luchris429/purejaxrl: Really Fast End-to-End Jax RL Implementations</h2><div class="ot l"><h3 class="bf b hw z io or iq ir os it iv dx">Really Fast End-to-End Jax RL Implementations. Contribute to luchris429/purejaxrl development by creating an account on…</h3></div><div class="ou l"><p class="bf b dy z io or iq ir os it iv dx">github.com</p></div></div><div class="ov l"><div class="sa l ox oy oz ov pa lr om"/></div></div></a></div><p id="38f5" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Thank you very much for your support, until next time 👋</p><h2 id="3513" class="qt pc fq bf pd qu qv qw pg qx qy qz pj nt ra rb rc nx rd re rf ob rg rh ri rj bk">References:</h2><p id="a25a" class="pw-post-body-paragraph nk nl fq nm b go px no np gr py nr ns nt pz nv nw nx qa nz oa ob qb od oe of fj bk"><a class="af nj" href="https://github.com/luchris429/purejaxrl/blob/main/purejaxrl/ppo.py" rel="noopener ugc nofollow" target="_blank">Full training script</a>, PureJaxRL, Chris Lu, 2023</p><p id="3899" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">[1] <a class="af nj" href="https://smerity.com/articles/2016/orthogonal_init.html" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr"><em class="og">Explaining and illustrating orthogonal initialization for recurrent neural networks</em></strong></a>, Smerity, 2016</p><p id="bb02" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">[2] <a class="af nj" href="https://www.deeplearning.ai/ai-notes/initialization/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr"><em class="og">Initializing neural networks</em></strong></a>, DeepLearning.ai</p><p id="2824" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">[3] <a class="af nj" rel="noopener" target="_blank" href="/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975"><strong class="nm fr"><em class="og">Generalized Advantage Estimation in Reinforcement Learning</em></strong></a>, Siwei Causevic, Towards Data Science, 2023</p><p id="2b0e" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">[4] <a class="af nj" href="https://arxiv.org/pdf/1707.06347" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr"><em class="og">Proximal Policy Optimization Algorithms</em></strong></a>, Schulman et Al., OpenAI, 2017</p></div></div></div></div>    
</body>
</html>