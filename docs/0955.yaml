- en: Monitor Data Pipelines Using Snowflake’s Data Metric Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/monitor-data-pipelines-using-snowflakes-data-metric-functions-0df71c46f04a?source=collection_archive---------8-----------------------#2024-04-15](https://towardsdatascience.com/monitor-data-pipelines-using-snowflakes-data-metric-functions-0df71c46f04a?source=collection_archive---------8-----------------------#2024-04-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build Trusted Data Platforms with Google SRE Principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jesszhangcyz?source=post_page---byline--0df71c46f04a--------------------------------)[![Jess.Z](../Images/ae9505d75966ab9fb60a64366c24e4de.png)](https://medium.com/@jesszhangcyz?source=post_page---byline--0df71c46f04a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0df71c46f04a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0df71c46f04a--------------------------------)
    [Jess.Z](https://medium.com/@jesszhangcyz?source=post_page---byline--0df71c46f04a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0df71c46f04a--------------------------------)
    ·6 min read·Apr 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd9da14923ac82a0d2d10724afb39253.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: Do you have customers coming to you first with a data incident? Are your customers
    building their own data solutions due to un-trusted data? Does your data team
    spend unnecessarily long hours remediating undetected data quality issues instead
    of prioritising strategic work?
  prefs: []
  type: TYPE_NORMAL
- en: Data teams need to be able to paint a complete picture of their data systems
    health in order to gain trust with their stakeholders and have better conversations
    with the business as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: We can combine data quality dimensions with Google’s Site Reliability Engineering
    principles to measure the health of our Data Systems. To do this, assess a few
    Data Quality Dimensions that makes sense for your data pipelines and come up with
    **service level objectives (SLOs)**.
  prefs: []
  type: TYPE_NORMAL
- en: What are Service Level Objectives?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The service level terminology we will use in this article are ***service level
    indicators*** and ***service level objectives***. The two are borrowed principles
    from [Google’s SRE book](https://sre.google/sre-book/foreword/).
  prefs: []
  type: TYPE_NORMAL
- en: '**service level** **indicator** — a carefully defined quantitative measure
    of some aspect of the level of service that is provided.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The indicators we’re familiar with in the software world are throughput, latency
    and up time (availability). These are used to measure the reliability of an application
    or website.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a926ac827c514fef839a09eae44bb4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical Event
  prefs: []
  type: TYPE_NORMAL
- en: The indicators are then turned into objectives bounded by a *threshold.* The
    health of the software application is now “measurable” in a sense that we can
    now communicate the state of our application with our customers.
  prefs: []
  type: TYPE_NORMAL
- en: '**service level objective**: a target value or range of values for a service
    level that is measured by an SLI.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We have an intuitive understanding of the necessity of these quantitative measures
    and indicators in a typical user applications to reduce friction and establish
    trust with our customers. We need to start adopting a similar mindset when building
    out data pipelines in the data world.
  prefs: []
  type: TYPE_NORMAL
- en: Data Quality Dimensions Translated into Service Level Terminology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/55c5bc0af80a676b7e2b04d71b53f3c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Data System with Failure
  prefs: []
  type: TYPE_NORMAL
- en: Lets say the user interacts with our application and generates X amounts of
    data every hour into our data warehouse, if the number of rows entering the warehouse
    suddenly decreases drastically, we can flag it as an issue. Then trace our timestamps
    from our pipelines to diagnose and treat the problem.
  prefs: []
  type: TYPE_NORMAL
- en: We want to capture enough information about the data coming into our systems
    so that we can detect when anomalies occur. Most data teams tend to start with
    **Data Timeliness.** Is the expected amount of data arriving at the right time?
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be decomposed into the indicators:'
  prefs: []
  type: TYPE_NORMAL
- en: Data Availability — Has the expected amount of data arrived/been made available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Freshness — Has new data arrived at the expected time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/57963d8ebfec1d046bd5532fc3ac9ba4.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Quality Dimensions Translated into SLIs & SLOs
  prefs: []
  type: TYPE_NORMAL
- en: Once the system is stable it is important to maintain a good relationship with
    your customers in order to set the right objectives that are valuable to your
    stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: Concept of a Threshold…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do we actually figure out how much data to expect and when? What is the
    right amount of data for all our different datasets? This is when we need to focus
    on the **threshold** concept as it does get tricky.
  prefs: []
  type: TYPE_NORMAL
- en: Assume we have an application where users mainly login to the system during
    the working hours. We expect around 2,000 USER_LOGIN events per hour between 9am
    to 5pm, and 100 events outside of those hours. If we use a single threshold value
    for the whole day, it would lead to the wrong conclusion. Receiving 120 events
    at 8pm is perfectly reasonable, but it would be concerning and should be investigated
    further if we only received 120 events at 2pm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/735733c28a94f26393e0e1e0ae938481.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph with line of threshold in green
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, we need to calculate a different expected value for each hour
    of the day for each different dataset — this is the threshold value. A metadata
    table would need to be defined that dynamically fetches the number of rows arrived
    each hour in order to get a resulting threshold that makes sense for each data
    source.
  prefs: []
  type: TYPE_NORMAL
- en: There are some thresholds which can be extracted using timestamps as a proxy
    as explained above. This can be done using statistical measures such as averages,
    standard deviations or percentiles to iterate over your metadata table.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on how creative you want to be, you can even introduce machine learning
    in this part of the process to help you set the threshold. Other thresholds or
    expectations would need to be discussed with your stakeholders as it would stem
    from having specific knowledge of the business to know what to expect.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Implementation in Snowflake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The very first step to getting started is picking a few business critical dataset
    to build on top of before implementing a data-ops solution at scale. This is the
    easiest way to gather momentum and feel the impact of your data observability
    efforts.
  prefs: []
  type: TYPE_NORMAL
- en: Many analytical warehouses already have inbuilt functionalities around this.
    For example, Snowflake has recently pushed out [Data Metric Functions](https://docs.snowflake.com/user-guide/data-quality-intro)
    in preview for Enterprise accounts to help data teams get started quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Data Metrics Functions is a wrapper around some of the queries we might write
    to get insights into our data systems. We can start with the system DMFs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/215c9afdc5449c2e9be8cca9b2d89f55.png)'
  prefs: []
  type: TYPE_IMG
- en: Snowflake System DMF
  prefs: []
  type: TYPE_NORMAL
- en: We first need to sort out a few privileges…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b46f55238e93746721c47cdf7ffc705.png)'
  prefs: []
  type: TYPE_IMG
- en: DMF Access Control Docs
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**DATA_METRIC_USER** is a database role which may catch a few people out. It’s
    important to revisit the docs if you’re running into issues. The most likely reason
    is probably due to permissions.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, simply choose a DMF …
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can schedule your DMFs to run using [Data Metric Schedule](https://docs.snowflake.com/en/sql-reference/parameters#label-data-metric-schedule)
    — an object parameter or your usual orchestration tool. The hard-work would still
    need to be done to determine your own thresholds in order to set the right SLOs
    for your pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In Summary…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data teams need to engage with stakeholders to set better expectations about
    the data by using service level indicators and objectives. Introducing these metrics
    will help data teams move from reactively firefighting to a more proactive approach
    in preventing data incidents. This would allow energy to be refocused towards
    delivering business value as well as building a trusted data platform.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
