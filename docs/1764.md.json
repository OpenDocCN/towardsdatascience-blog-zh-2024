["```py\nfrom transformers import BertForMaskedLM, AutoModelForCausalLM, AutoTokenizer\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\n\ndevice = torch.device('cpu') #works just fine\n\n#Load BERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\nmodel.to(device)\n\n#Load GPT2\ngpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\") #dbmdz/german-gpt2\ngpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\ngpt2_tokenizer.padding_side = \"left\" \ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n```", "```py\nN_GIBBS_RUNS = 4 #number of runs\nN_ITR_PER_RUN = 500 #number of iterations per each run\nN_MASKED_WORDS_PER_ITR = 1 #number of masked tokens per iteration\nMIN_TOKENS_PROB = 1e-3 #don't use tokens with lower probability for replacement\n```", "```py\ninitial_sentence = 'I often dream about a spacious villa by the sea .'\n\nwords = initial_sentence.split(' ')\n\nkeyword_idx = [2,9]\nkeyword_idx.append(len(words)-1) # always keep the punctuation mark at the end of the sentence\n```", "```py\ndef get_bert_tokens(words, indices):\n    sentence = \" \".join(words)\n    masked_sentence = [word if not word_idx in indices else \"[MASK]\" for word_idx,word in enumerate(words) ]\n    masked_sentence = ' '.join(masked_sentence)\n    bert_sentence = f'[CLS] {masked_sentence} [SEP] '\n    bert_tokens = tokenizer.tokenize(bert_sentence)\n    return bert_tokens\n\nn_words = len(words)\nn_fixed = len(keyword_idx)\n\ngenerated_sent = []\n\nfor j in range(N_GIBBS_RUNS):\n\n    words = initial_sentence.split(' ')\n\n    for i in range(N_ITR_PER_RUN):\n\n        if i%10==0:\n            print(i)\n\n        #choose N_MASKED_WORDS_PER_ITR random words to mask (excluding keywords)\n        masked_words_idx = np.random.choice([x for x in range(n_words) if not x in keyword_idx], replace=False, size=N_MASKED_WORDS_PER_ITR).tolist() \n\n        masked_words_idx.sort()\n\n        while len(masked_words_idx)>0:\n\n            #reconstruct successively each of the masked word\n            bert_tokens = get_bert_tokens(words, masked_words_idx) #get tokens from tokenizer\n\n            masked_index = [i for i, x in enumerate(bert_tokens) if x == '[MASK]']\n            indexed_tokens = tokenizer.convert_tokens_to_ids(bert_tokens)\n            segments_ids = [0] * len(bert_tokens)\n\n            tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n            segments_tensors = torch.tensor([segments_ids]).to(device)\n\n            with torch.no_grad():\n                outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n                predictions = outputs[0][0]\n                reconstruct_pos = 0 #reconstruct leftmost masked token\n                probs = F.softmax(predictions[masked_index[reconstruct_pos]],dim=0).cpu().numpy()\n\n            probs[probs<MIN_TOKENS_PROB] = 0 #ignore low probabily tokens\n\n            if len(probs)>0:\n\n                #sample a token using the conditional probability from BERT\n                token = np.random.choice(range(len(probs)), size=1, p=probs/probs.sum(), replace=False)\n\n                predicted_token = tokenizer.convert_ids_to_tokens(token)[0]\n\n                words[masked_words_idx[reconstruct_pos]] = predicted_token #replace the word in the sequence with the chosen token\n\n            del masked_words_idx[reconstruct_pos]\n\n        sentence = ' '.join(words)\n\n        with torch.no_grad():\n            inputs = gpt2_tokenizer(sentence, return_tensors = \"pt\")\n            loss = gpt2_model(input_ids = inputs[\"input_ids\"], labels = inputs[\"input_ids\"]).loss\n            gpt2_perplexity = torch.exp(loss).item()\n\n        #sentence = sentence.capitalize().replace(' .','.')\n        gpt2_perplexity = int(gpt2_perplexity)\n\n        generated_sent.append((sentence,gpt2_perplexity))\n\ndf = pd.DataFrame(generated_sent, columns=['sentence','perplexity'])\n```"]