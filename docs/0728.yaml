- en: Setting Up Automated Model Training Workflows with AWS S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/setting-up-automated-model-training-workflows-with-aws-s3-cd0587b42f34?source=collection_archive---------6-----------------------#2024-03-18](https://towardsdatascience.com/setting-up-automated-model-training-workflows-with-aws-s3-cd0587b42f34?source=collection_archive---------6-----------------------#2024-03-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Open-Source Approach for Workflow Automation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://khuyentran1476.medium.com/?source=post_page---byline--cd0587b42f34--------------------------------)[![Khuyen
    Tran](../Images/98aa66025ad29b618e875c75f1c400a5.png)](https://khuyentran1476.medium.com/?source=post_page---byline--cd0587b42f34--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd0587b42f34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd0587b42f34--------------------------------)
    [Khuyen Tran](https://khuyentran1476.medium.com/?source=post_page---byline--cd0587b42f34--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd0587b42f34--------------------------------)
    ·7 min read·Mar 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Consider you’re an e-commerce platform aiming to enhance recommendation personalization.
    Your data resides in S3.
  prefs: []
  type: TYPE_NORMAL
- en: To refine recommendations, you plan to retrain recommendation models using fresh
    customer interaction data whenever a new file is added to S3\. But how exactly
    do you approach this task?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19c91eea848de1654a1def0fce56e5a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Unless otherwise noted, all images are by the author
  prefs: []
  type: TYPE_NORMAL
- en: Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two common solutions to this problem are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS Lambda:** A serverless compute service by AWS, allowing code execution
    in response to events without managing servers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Open-source orchestrators:** Tools automating, scheduling, and monitoring
    workflows and tasks, usually self-hosted.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using an open-source orchestrator offers advantages over AWS Lambda:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost-Effectiveness:** Running long tasks on AWS Lambda can be costly. Open-source
    orchestrators let you use your infrastructure, potentially saving costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faster Iteration:** Developing and testing workflows locally speeds up the
    process, making it easier to debug and refine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment Control:** Full control over the execution environment allows
    you to customize your development tools and IDEs to match your preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/aa329b6f70ec6933a5f407cb563a3fd2.png)'
  prefs: []
  type: TYPE_IMG
- en: While you could solve this problem in Apache Airflow, it would require complex
    infrastructure and deployment setup. Thus, we’ll use [Kestra](https://bit.ly/49OMyH9),
    which offers an intuitive UI and can be launched in a single Docker command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to play and fork the source code of this article here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/khuyentran1401/mlops-kestra-workflow?source=post_page-----cd0587b42f34--------------------------------)
    [## GitHub - khuyentran1401/mlops-kestra-workflow'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to khuyentran1401/mlops-kestra-workflow development by creating an
    account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/khuyentran1401/mlops-kestra-workflow?source=post_page-----cd0587b42f34--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Workflow Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This workflow consists of two main components: Python scripts and orchestration.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Orchestration**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python scripts and flows are stored in Git, with changes synced to Kestra on
    a schedule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ff05c4168232f7a136a385271928fd4b.png)'
  prefs: []
  type: TYPE_IMG
- en: When a new file appears in the “new” prefix of the S3 bucket, Kestra triggers
    an execution of a series of Python scripts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/19c91eea848de1654a1def0fce56e5a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Scripts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[*download_files_from_s3.py*](https://github.com/khuyentran1401/mlops-kestra-workflow/blob/main/src/download_files_from_s3.py):
    Download all files from the “old” prefix in the bucket.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*merge_data.py*](https://github.com/khuyentran1401/mlops-kestra-workflow/blob/main/src/merge_data.py):
    Merge the downloaded files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*process.py*](https://github.com/khuyentran1401/mlops-kestra-workflow/blob/main/src/process.py):
    Process the merged data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*train.py*](https://github.com/khuyentran1401/mlops-kestra-workflow/blob/main/src/train.py)*:*
    Train the model using the processed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f230c2f02a707f09663735925e93bbdd.png)'
  prefs: []
  type: TYPE_IMG
- en: As we will execute the code downloaded from Git within Kestra, make sure to
    commit these Python scripts to the repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Orchestration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start Kestra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Download the Docker Compose file by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure that Docker is running. Then, start the Kestra server with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Access the UI by opening the URL [http://localhost:8080](http://localhost:8080/)
    in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b33ef5eb4c0a7f3801fb1ba4f5fa7e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Sync from Git
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the Python scripts are in GitHub, we will use [Git Sync](https://kestra.io/plugins/plugin-git/tasks/io.kestra.plugin.git.sync)
    to update the code from GitHub to Kestra every minute. To set this up, create
    a file named “sync_from_git.yml” under the “_flows” directory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you are using VSCode, you can use the [Kestra plugin](https://marketplace.visualstudio.com/items?itemName=kestra-io.kestra)
    to enable flow autocompletion and validation in `.yaml` files.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d963b494550c1f0572c20a04a2f7675.png)![](../Images/0d1942b43ca808d76b4744901d3bb420.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Below is the implementation of the flow to synchronize code from Git:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A username and password are necessary only if the GitHub repository is private.
    To pass these secrets to Kestra, place them in the “.env” file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, encode these secrets using the following bash script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this script generates a “.env_encoded” file containing encoded secrets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Include the encoded environment file in your Docker Compose file for Kestra
    to access the environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Ensure to exclude the environment files in the “.gitignore” file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, commit both the new flow and the Docker Compose file to Git:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, with the `sync_from_git` flow set to run every minute, you can conveniently
    access and trigger the execution of Python scripts directly from the Kestra UI.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/720f5af3075edd1455253930ff0f021c.png)![](../Images/84846040a61cc99923ac42d464e5516c.png)'
  prefs: []
  type: TYPE_IMG
- en: Orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll create a flow triggered when a new file is added to the “new” prefix within
    the “winequality-red” bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Upon detecting a new file, Kestra will download it to internal storage and execute
    the Python files. Finally, it moves the file from the “new” prefix to the “old”
    prefix in the bucket to avoid duplicate detection during subsequent polls.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7108977a1b3c4ca3def8ec2ac3f9c195.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `run_python_commands` task uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespaceFiles` to access all files in your local project, synchronized with
    your Git repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env` to retrieve environment variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker` to execute the script within the docker container `ghcr.io/kestra-io/pydata:latest`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beforeCommands` to install requirements from the “requirements.txt” file prior
    to executing commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`commands` to sequentially run a list of commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outputFiles` to send all pickle files from local file system to Kestra’s internal
    storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, add the `upload` task to upload the model’s pickle file to S3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0bf0fad92d75de999eb59d2fefd93602.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! Name this flow “run_ml_pipeline.yml” and commit it to Git.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Trigger the Flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To initiate the flow, simply add a new file to the “new” prefix within the “winequality-red”
    bucket on S3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c3cf593aa7d5ea84478abcf8c5e57a5.png)'
  prefs: []
  type: TYPE_IMG
- en: This action will trigger the `run_ml_pipeline` flow, initiating the download
    of data from the “old” prefix, merging all files, processing the data, and training
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e9a0b6ab67bc14bc15e7eca149714ec.png)![](../Images/d3cfe0e9fa37c79f185715567c6f35f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the workflow completes execution, the “model.pkl” file is uploaded to S3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c20d3d5356ee53bdf05e9993a5e8938.png)'
  prefs: []
  type: TYPE_IMG
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article shows how to use Kestra to automate the execution of Python scripts
    for data science tasks whenever a new file is added to S3\. If you are looking
    for ways to automate your machine learning pipeline, give this solution a try.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/374d772107eebbec9b782240512405e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I love writing about data science concepts and playing with different data
    science tools. You can stay up-to-date with my latest posts by:'
  prefs: []
  type: TYPE_NORMAL
- en: Subscribing to my newsletter on [Data Science Simplified](https://mathdatasimplified.com/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connect with me on [LinkedIn](https://www.linkedin.com/in/khuyen-tran-1401/)
    and [Twitter](https://twitter.com/KhuyenTran16).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
