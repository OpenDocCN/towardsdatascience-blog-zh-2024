- en: 'Small Language Models: Using 3.8B Phi-3 and 8B Llama-3 Models on a PC and Raspberry
    Pi'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23](https://towardsdatascience.com/small-language-models-using-3-8b-phi-3-and-8b-llama-3-models-on-a-pc-and-raspberry-pi-9ed70127fe61?source=collection_archive---------2-----------------------#2024-05-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Testing the models with LlamaCpp and ONNX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--9ed70127fe61--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9ed70127fe61--------------------------------)
    ·17 min read·May 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1c080c33ba6ea79ec97849e1d394374.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Jelleke Vanooteghem](https://unsplash.com/@ilumire), Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, we can observe an interesting twist in developing new AI models. For
    a long time, it has been known that bigger models are “smarter” and capable of
    doing more complex things. But they are also more computationally expensive. Big
    device manufacturers like Microsoft, Google, and Samsung have already started
    to promote new AI features to their clients, but it is clear that if millions
    of users massively use AI on their phones or laptops, the computational cloud
    costs could be enormous. What is the solution? The obvious way is to run a model
    on-device, which has advantages in latency (no network connection is required,
    and the model can be accessed immediately), privacy (no need to process user responses
    in the cloud), and, naturally, computation costs. Using local AI models is important
    not only for laptops and smartphones but also for autonomous robots, smart home
    assistants, and other edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of making this article, at least two models were announced that
    were specially designed for on-device running:'
  prefs: []
  type: TYPE_NORMAL
- en: Google’s [Gemini Nano](https://deepmind.google/technologies/gemini/nano/). The
    model was announced in December 2023; it has [two versions](https://en.wikipedia.org/wiki/Gemini_(language_model))
    with 1.8B and 3.25B parameters. According to the [developer.android.com](https://developer.android.com/ai/aicore)…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
