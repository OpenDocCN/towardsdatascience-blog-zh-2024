<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Build a Tokenizer for the Thai Language from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Build a Tokenizer for the Thai Language from Scratch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-tokenizer-for-the-thai-language-from-scratch-0e4ea5f2a8b3?source=collection_archive---------6-----------------------#2024-09-14">https://towardsdatascience.com/build-a-tokenizer-for-the-thai-language-from-scratch-0e4ea5f2a8b3?source=collection_archive---------6-----------------------#2024-09-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1e74" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A step-by-step guide to building a Thai multilingual sub-word tokenizer based on a BPE algorithm trained on Thai and English datasets using only Python</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tamangmilan?source=post_page---byline--0e4ea5f2a8b3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Milan Tamang" class="l ep by dd de cx" src="../Images/18e8be296bcef18e8792bfc18240469a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mfDWMsklE8l0-1rrnhbJ4g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--0e4ea5f2a8b3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tamangmilan?source=post_page---byline--0e4ea5f2a8b3--------------------------------" rel="noopener follow">Milan Tamang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--0e4ea5f2a8b3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/a876522a3058062625e355547ef07b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*C5v8YO8zSEDZ5fpDVZPEHQ.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><strong class="bf my">[Image by writer]: Thai Tokenizer encode and decode Thai text to Token Ids and vice versa</strong></figcaption></figure><p id="a605" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The primary task of the <strong class="nb fr">Tokenizer</strong> is to translate the raw input texts (Thai in our case but can be in any foreign language) into numbers and pass them to the model’s transformers. The model’s transformer then generates output as numbers. Again, <strong class="nb fr">Tokenizer </strong>translates these numbers back to texts which is understandable to end users. The high level diagram below describes the flow explained above.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="mj mk nv"><img src="../Images/9fc0dc22dc54e54c85fdbc1377f4f4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nh6OY-wIQMVsMszwKjpxAw.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">[Image by writer]: Diagram showing tokenizers role in LLM’s input and output flow.</figcaption></figure><p id="5fd5" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Generally, many of us are only interested in learning how the model’s transformer architecture works under the hood. We often overlook learning some important components such as tokenizers in detail. Understanding how tokenizer works under the hood and having good control of its functionalities gives us good leverage to improve our model’s accuracy and performance.</p><blockquote class="oa ob oc"><p id="1f74" class="mz na od nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Similar to Tokenizer, some of the most important components of LLM implementation pipelines are Data preprocessing, Evaluation, Guardrails/Security, and Testing/Monitoring. I would highly recommend you study more details on these topics. I realized the importance of these components only after I was working on the actual implementation of my foundational multilingual model ThaiLLM in production.</p></blockquote><h2 id="6edb" class="oe of fq bf my og oh oi oj ok ol om on ni oo op oq nm or os ot nq ou ov ow ox bk">Why do you need a Thai tokenizer or any other foreign language tokenizer?</h2><ul class=""><li id="5e59" class="mz na fq nb b go oy nd ne gr oz ng nh ni pa nk nl nm pb no np nq pc ns nt nu pd pe pf bk">Suppose you are using generic English-based tokenizers to pre-train a multilingual large language model such as Thai, Hindi, Indonesian, Arabic, Chinese, etc. In that case, your model might not likely give a suitable output that makes good sense for your specific domain or use cases. Hence, building your own tokenizer in your choice of language certainly helps make your model’s output much more coherent and understandable.</li><li id="ffcd" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pd pe pf bk">Building your own tokenizer also gives you full control over how comprehensive and inclusive vocabulary you want to build. During the attention mechanism, because of comprehensive vocabulary, the token can attend and learn from more tokens within the limited context length of the sequence. Hence it makes learning more coherent which eventually helps in better model inference.</li></ul><p id="37a1" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The good news is that after you finish building Thai Tokenizer, you can easily build a tokenizer in any other language. All the building steps are the same except that you’ll have to train on the dataset of your choice of language.</p><p id="5c43" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Now that we’ve all the good reason to build our own tokenizer. Below are steps to building our tokenizer in the Thai language.</strong></p><ol class=""><li id="d304" class="mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu pl pe pf bk">Build our own BPE algorithm</li><li id="8af7" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pl pe pf bk">Train the tokenizer</li><li id="a657" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pl pe pf bk">Tokenizer encode and decode function</li><li id="24cb" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pl pe pf bk">Load and test the tokenizer</li></ol><h2 id="5814" class="oe of fq bf my og oh oi oj ok ol om on ni oo op oq nm or os ot nq ou ov ow ox bk"><strong class="al">Step 1: Build our own BPE (Byte Pair Encoding) algorithm:</strong></h2><p id="827e" class="pw-post-body-paragraph mz na fq nb b go oy nd ne gr oz ng nh ni pa nk nl nm pb no np nq pc ns nt nu fj bk">The BPE algorithm is used in many popular LLMs such as Llama, GPT, and others to build their tokenizer. We can choose one of these LLM tokenizers if our model is based on the English language. Since we’re building the Thai Tokenizer, the best option is to create our own BPE algorithm from scratch and use it to build our tokenizer. Let’s first understand how the BPE algorithm works with the help of the simple flow diagram below and then we’ll start building it accordingly.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="mj mk pm"><img src="../Images/bdc0fe33fb3088b9db81b36efeeed005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ks_3ADx6uvCn7pkH2HtkA.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">[Image by writer]: BPE flow diagram. Example reference from a wiki page (<a class="af pn" href="https://en.wikipedia.org/wiki/Byte_pair_encoding" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Byte_pair_encoding</a>)</figcaption></figure><p id="6573" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The examples in the flow diagram are shown in English to make it easier to understand.</p><p id="d635" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Let’s write code to implement the BPE algorithm for our Thai Tokenizer.</strong></p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="956d" class="ps of fq pp b bg pt pu l pv pw"># A simple practice example to get familiarization with utf-8 encoding to convert strings to bytes. <br/>text = "How are you คุณเป็นอย่างไร"      # Text string in both English and Thai<br/>text_bytes = text.encode("utf-8")<br/>print(f"Text in byte: {text_bytes}")<br/><br/>text_list = list(text_bytes)          # Converts text bytes to a list of integer<br/>print(f"Text list in integer: {text_list}")</span></pre><pre class="px po pp pq bp pr bb bk"><span id="f93c" class="ps of fq pp b bg pt pu l pv pw"># As I don't want to reinvent the wheel, I will be referencing most of the code block from Andrej Karpathy's GitHub (https://github.com/karpathy/minbpe?tab=readme-ov-file).<br/># However, I'll be modifying code blocks specific to building our Thai language tokenizer and also explaining the codes so that you can understand how each code block works and make it easy when you implement code for your use case later.<br/><br/># This module provides access to the Unicode Character Database (UCD) which defines character properties for all Unicode characters.<br/>import unicodedata<br/><br/># This function returns a dictionary with consecutive pairs of integers and their counts in the given list of integers.<br/>def get_stats(ids, stats=None):<br/>    <br/>    stats = {} if stats is None else stats<br/>    # zip function allows to iterate consecutive items from given two list<br/>    for pair in zip(ids, ids[1:]): <br/>        # If a pair already exists in the stats dictionary, add 1 to its value else assign the value as 0.<br/>        stats[pair] = stats.get(pair, 0) + 1<br/>    return stats<br/><br/># Once we find out the list of consecutive pairs of integers, we'll then replace those pairs with new integer tokens.<br/>def merge(ids, pair, idx):<br/>    newids = []<br/>    i = 0<br/>    # As we'll be merging a pair of ids, hence the minimum id in the list should be 2 or more.<br/>    while i &lt; len(ids):<br/>        # If the current id and next id(id+1) exist in the given pair, and the position of id is not the last, then replace the 2 consecutive id with the given index value.<br/>        if ids[i] == pair[0] and i &lt; len(ids) - 1 and ids[i+1] == pair[1]:<br/>            newids.append(idx)<br/>            i += 2  # If the pair is matched, the next iteration starts after 2 positions in the list.<br/>        else:<br/>            newids.append(ids[i])<br/>            i += 1  # Since the current id pair didn't match, so start iteration from the 1 position next in the list.<br/>    # Returns the Merged Ids list<br/>    return newids<br/><br/># This function checks that using 'unicodedata.category' which returns "C" as the first letter if it is a control character and we'll have to replace it readable character.<br/>def replace_control_characters(s: str) -&gt; str:<br/>    chars = []<br/>    for ch in s:<br/>        # If the character is not distorted (meaning the first letter doesn't start with "C"), then append the character to chars list.<br/>        if unicodedata.category(ch)[0] != "C":<br/>            chars.append(ch) <br/>        # If the character is distorted (meaning the first letter has the letter "C"), then replace it with readable bytes and append to chars list.<br/>        else:<br/>            chars.append(f"\\u{ord(ch):04x}") <br/>    return "".join(chars)<br/><br/># Some of the tokens such as control characters like Escape Characters can't be decoded into valid strings. <br/># Hence those need to be replace with readable character such as �<br/>def render_token(t: bytes) -&gt; str:    <br/>    s = t.decode('utf-8', errors='replace')<br/>    s = replace_control_characters(s)<br/>    return s</span></pre><p id="2a78" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">The two functions <strong class="nb fr"><em class="od">get_stats</em></strong><em class="od"> </em>and <strong class="nb fr"><em class="od">merge</em></strong><em class="od"> </em>defined above in the code block are the implementation of the BPE algorithm for our Thai Tokenizer. Now that the algorithm is ready. Let’s write code to train our tokenizer.</p><h2 id="e199" class="oe of fq bf my og oh oi oj ok ol om on ni oo op oq nm or os ot nq ou ov ow ox bk">Step 2: Train the tokenizer:</h2><p id="8322" class="pw-post-body-paragraph mz na fq nb b go oy nd ne gr oz ng nh ni pa nk nl nm pb no np nq pc ns nt nu fj bk">Training tokenizer involves generating a vocabulary which is a database of unique tokens (word and sub-words) along with a unique index number assigned to each token. We’ll be using <strong class="nb fr">the Thai Wiki dataset</strong> from the Hugging Face to train our Thai Tokenizer. Just like training an LLM requires a huge data, you’ll also require a good amount of data to train a tokenizer. You could also use the same dataset to train the LLM as well as tokenizer though not mandatory. For a multilingual LLM, it is advisable to use both the English and Thai datasets in the ratio of 2:1 which is a standard approach many practitioners follow.</p><p id="81ca" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Let’s begin writing the training code.</strong></p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="03a7" class="ps of fq pp b bg pt pu l pv pw"># Import Regular Expression<br/>import regex as re    <br/><br/># Create a Thai Tokenizer class.<br/>class ThaiTokenizer():<br/><br/>  def __init__(self):<br/><br/>        # The byte pair should be done within the related words or sentences that give a proper context. Pairing between unrelated words or sentences may give undesirable output.<br/>        # To prevent this behavior, we'll implement the LLama 3 regular expression pattern to make meaningful chunks of our text before implementing the byte pair algorithm.<br/>        self.pattern = r"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+"                <br/>        self.compiled_pattern = re.compile(self.pattern)<br/><br/>        # Special tokens are used to provide coherence in the sequence while training.<br/>        # Special tokens are assigned a unique index number and stored in vocabulary. <br/>        self.special_tokens = {<br/>            '&lt;|begin_of_text|&gt;': 1101,<br/>            '&lt;|end_of_text|&gt;': 1102,<br/>            '&lt;|start_header_id|&gt;': 1103,<br/>            '&lt;|end_header_id|&gt;': 1104,<br/>            '&lt;|eot_id|&gt;': 1105<br/>        }<br/><br/>        # Initialize merges with empty dictionary<br/>        self.merges = {}<br/><br/>        # Initialize the vocab dictionary by calling the function _build_vocab which is defined later in this class.<br/>        self.vocab = self._build_vocab()<br/><br/>  # Tokenizer training function<br/>  def train(self, text, vocab_size):<br/><br/>        # Make sure the vocab size must be at least 256 as the utf-8 encoding for the range 0-255 are same as the Ascii character.<br/>        assert vocab_size &gt;= 256<br/>        # Total number of merges into the vocabulary.<br/>        num_merges = vocab_size - 256<br/><br/>        # The first step is to make sure to split the text up into text chunks using the pattern defined above.<br/>        text_chunks = re.findall(self.compiled_pattern, text)<br/><br/>        # Each text_chunks will be utf-8 encoded to bytes and then converted into an integer list.<br/>        ids = [list(ch.encode("utf-8")) for ch in text_chunks]<br/><br/>        # Iteratively merge the most common pairs to create new tokens<br/>        merges = {} # (int, int) -&gt; int<br/>        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -&gt; bytes<br/><br/>        # Until the total num_merges is reached, find the common pair of consecutive id in the ids list and start merging them to create a new token<br/>        for i in range(num_merges):<br/>            # Count the number of times every consecutive pair appears<br/>            stats = {}<br/>            for chunk_ids in ids:<br/>                # Passing in stats will update it in place, adding up counts<br/>                get_stats(chunk_ids, stats)<br/>            # Find the pair with the highest count<br/>            pair = max(stats, key=stats.get)<br/>            # Mint a new token: assign it the next available id<br/>            idx = 256 + i<br/>            # Replace all occurrences of pair in ids with idx<br/>            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]<br/>            # Save the merge<br/>            merges[pair] = idx<br/>            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]<br/><br/>        # Save class variables to be used later during tokenizer encode and decode<br/>        self.merges = merges <br/>        self.vocab = vocab   <br/>  <br/>  # Function to return a vocab dictionary combines with merges and special tokens<br/>  def _build_vocab(self):<br/>        # The utf-8 encoding for the range 0-255 are same as the Ascii character. <br/>        vocab = {idx: bytes([idx]) for idx in range(256)}<br/>        <br/>        # Iterate through merge dictionary and add into vocab dictionary<br/>        for (p0, p1), idx in self.merges.items():<br/>            vocab[idx] = vocab[p0] + vocab[p1]<br/>        <br/>        # Iterate through special token dictionary and add into vocab dictionary<br/>        for special, idx in self.special_tokens.items():<br/>            vocab[idx] = special.encode("utf-8")<br/><br/>        return vocab<br/><br/>  # After training is complete, use the save function to save the model file and vocab file.<br/>  # Model file will be used to load the tokenizer model for further use in llm<br/>  # Vocab file is just for the purpose of human verification<br/>  def save(self, file_prefix):        <br/>        # Writing to model file <br/>        model_file = file_prefix + ".model"           # model file name<br/><br/>        # Model write begins<br/>        with open(model_file, 'w') as f:            <br/>            f.write("thai tokenizer v1.0\n")          # write the tokenizer version<br/>            f.write(f"{self.pattern}\n")              # write the pattern used in tokenizer            <br/>            f.write(f"{len(self.special_tokens)}\n")  # write the length of special tokens<br/>            <br/>            # Write each special token in the specific format like below<br/>            for tokens, idx in self.special_tokens.items():<br/>                f.write(f"{tokens} {idx}\n")<br/>            <br/>            # Write only the keys part from the merges dict<br/>            for idx1, idx2 in self.merges:<br/>                f.write(f"{idx1} {idx2}\n")<br/><br/>        # Writing to the vocab file<br/>        vocab_file = file_prefix + ".vocab"       # vocab file name<br/><br/>        # Change the position of keys and values of merge dict and store into inverted_merges <br/>        inverted_merges = {idx: pair for pair, idx in self.merges.items()}        <br/>        # Vocab write begins<br/>        with open(vocab_file, "w", encoding="utf-8") as f:<br/>            for idx, token in self.vocab.items():<br/>                # render_token function processes tokens and prevents distorted bytes by replacing them with readable character<br/>                s = render_token(token)<br/>                # If the index of vocab is present in merge dict, then find its child index, convert their corresponding bytes in vocab dict and write the characters<br/>                if idx in inverted_merges:                    <br/>                    idx0, idx1 = inverted_merges[idx]<br/>                    s0 = render_token(self.vocab[idx0])<br/>                    s1 = render_token(self.vocab[idx1])<br/>                    f.write(f"[{s0}][{s1}] -&gt; [{s}] {idx}\n")<br/>                # If index of vocab is not present in merge dict, just write it's index and the corresponding string<br/>                else:                    <br/>                    f.write(f"[{s}] {idx}\n")<br/><br/>  # Function to load tokenizer model. <br/>  # This function is invoked only after the training is complete and the tokenizer model file is saved.<br/>  def load(self, model_file):<br/>        <br/>        merges = {}             # Initialize merge and special_tokens with empty dict<br/>        special_tokens = {}     # Initialize special_tokens with empty dict<br/>        idx = 256               # As the range (0, 255) is already reserved in vocab. So the next index only starts from 256 and onwards.<br/><br/>        # Read model file<br/>        with open(model_file, 'r', encoding="utf-8") as f:<br/>            <br/>            version = f.readline().strip()          # Read the tokenizer version as defined during model file writing            <br/>            self.pattern = f.readline().strip()     # Read the pattern used in tokenizer            <br/>            num_special = int(f.readline().strip()) # Read the length of special tokens<br/><br/>            # Read all the special tokens and store in special_tokens dict defined earlier<br/>            for _ in range(num_special):<br/>                special, special_idx = f.readline().strip().split()<br/>                special_tokens[special] = int(special_idx)<br/>            <br/>            # Read all the merge indexes from the file. Make it a key pair and store it in merge dictionary defined earlier. <br/>            # The value of this key pair would be idx(256) as defined above and keep on increase by 1.            <br/>            for line in f:<br/>                idx1, idx2 = map(int, line.split())<br/>                merges[(idx1, idx2)] = idx<br/>                idx += 1<br/>        <br/>        self.merges = merges                  <br/>        self.special_tokens = special_tokens  <br/>        <br/>        # Create a final vocabulary dictionary by combining merge, special_token and vocab (0-255). _build_vocab function helps to do just that.<br/>        self.vocab = self._build_vocab()<br/></span></pre><h2 id="a2cb" class="oe of fq bf my og oh oi oj ok ol om on ni oo op oq nm or os ot nq ou ov ow ox bk">Step 3: Tokenizer encode and decode function:</h2><ul class=""><li id="32ef" class="mz na fq nb b go oy nd ne gr oz ng nh ni pa nk nl nm pb no np nq pc ns nt nu pd pe pf bk"><strong class="nb fr">Tokenizer Encode:</strong> The tokenizer encoding function looks into vocabulary and translates the given input texts or prompts into the list of integer IDs. These IDs are then fed into the transformer blocks.</li><li id="bb3d" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pd pe pf bk"><strong class="nb fr">Tokenizer Decode:</strong> The tokenizer decoding function looks into vocabulary and translates the list of IDs generated from the transformer’s classifier block into output texts.</li></ul><p id="05b8" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Let’s take a look at the diagram below to have further clarity.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="mj mk py"><img src="../Images/8d677b110b92dc2cb9ac0c4822965467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIvfWG0gAUym7FPS5-gSqQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">[Image by writer]: Thai tokenizer encode and decode function</figcaption></figure><p id="93f9" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Let’s write code to implement the tokenizer’s encode and decode function.</strong></p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="6306" class="ps of fq pp b bg pt pu l pv pw"># Tokenizer encode function takes text as a string and returns integer ids list<br/>  def encode(self, text):      <br/><br/>        # Define a pattern to identify special token present in the text<br/>        special_pattern = "(" + "|".join(re.escape(k) for k in self.special_tokens) + ")"        <br/>        # Split special token (if present) from the rest of the text<br/>        special_chunks = re.split(special_pattern, text)        <br/>        # Initialize empty ids list <br/>        ids = []                                                <br/><br/>        # Loop through each of parts in the special chunks list.<br/>        for part in special_chunks:<br/>            # If the part of the text is the special token, get the idx of the part from the special token dictionary and append it to the ids list.<br/>            if part in self.special_tokens:                <br/>                ids.append(self.special_tokens[part])            <br/>            # If the part of text is not a special token <br/>            else:                <br/>                # Split the text into multiple chunks using the pattern we've defined earlier.<br/>                text_chunks = re.findall(self.compiled_pattern, text)<br/><br/>                # All text chunks are encoded separately, then the results are joined                <br/>                for chunk in text_chunks:<br/>                    chunk_bytes = chunk.encode("utf-8")   # Encode text to bytes                    <br/>                    chunk_ids = list(chunk_bytes)         # Convert bytes to list of integer  <br/>                                        <br/>                    while len(chunk_ids) &gt;= 2:    # chunks ids list must be at least 2 id to form a byte-pair<br/>                        # Count the number of times every consecutive pair appears<br/>                        stats = get_stats(chunk_ids)<br/>                        # Some idx pair might be created with another idx in the merge dictionary. Hence we'll find the pair with the lowest merge index to ensure we cover all byte pairs in the merge dict.<br/>                        pair = min(stats, key=lambda p: self.merges.get(p, float("inf")))<br/><br/>                        # Break the loop and return if the pair is not present in the merges dictionary                        <br/>                        if pair not in self.merges:<br/>                            break <br/>                        # Find the idx of the pair present in the merges dictionary<br/>                        idx = self.merges[pair]<br/>                        # Replace the occurrences of pair in ids list with this idx and continue<br/>                        chunk_ids = merge(chunk_ids, pair, idx)                    <br/><br/>                    ids.extend(chunk_ids)                <br/>        return ids<br/><br/>  # Tokenizer decode function takes a list of integer ids and return strings<br/>  def decode(self, ids):<br/>        <br/>        # Initialize empty byte list<br/>        part_bytes = []<br/>        # Change the position of keys and values of special_tokens dict and store into inverse_special_tokens <br/>        inverse_special_tokens = {v: k for k, v in self.special_tokens.items()}<br/><br/>        # Loop through idx in the ids list<br/>        for idx in ids:<br/>            # If the idx is found in vocab dict, get the bytes of idx and append them into part_bytes list<br/>            if idx in self.vocab:<br/>                part_bytes.append(self.vocab[idx])<br/>            # If the idx is found in inverse_special_tokens dict, get the token string of the corresponding idx, convert it to bytes using utf-8 encode and then append it into part_bytes list<br/>            elif idx in inverse_special_tokens:<br/>                part_bytes.append(inverse_special_tokens[idx].encode("utf-8"))<br/>            # If the idx is not found in both vocab and special token dict, throw an invalid error<br/>            else:<br/>                raise ValueError(f"invalid token id: {idx}")<br/><br/>        # Join all the individual bytes from the part_byte list<br/>        text_bytes = b"".join(part_bytes)<br/><br/>        # Convert the bytes to text string using utf-8 decode function. Make sure to use "errors=replace" to replace distorted characters with readable characters such as �.<br/>        text = text_bytes.decode("utf-8", errors="replace")<br/>        return text</span></pre><h2 id="301d" class="oe of fq bf my og oh oi oj ok ol om on ni oo op oq nm or os ot nq ou ov ow ox bk">Step 4: Load and test the tokenizer:</h2><p id="ebef" class="pw-post-body-paragraph mz na fq nb b go oy nd ne gr oz ng nh ni pa nk nl nm pb no np nq pc ns nt nu fj bk">Finally, here comes the best part of this article. In this section, we’ll perform two interesting tasks.</p><ul class=""><li id="81eb" class="mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu pd pe pf bk">First, train our tokenizer with the Thai Wiki Dataset from the Hugging Face. We have chosen a small dataset size (2.2 MB) to make training faster. However, for real-world implementation, you should choose a much larger dataset for better results. After the training is complete, we’ll save the model.</li><li id="885f" class="mz na fq nb b go pg nd ne gr ph ng nh ni pi nk nl nm pj no np nq pk ns nt nu pd pe pf bk">Second, we’ll load the saved tokenizer model and perform testing the tokenizer’s encode and decode function.</li></ul><p id="7805" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Let’s dive in.</strong></p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="3318" class="ps of fq pp b bg pt pu l pv pw"># Train the tokenizer<br/><br/>import time   # To caculate the duration of training completion<br/># Load training raw text data (thai_wiki dataset) from huggingface. thai_wiki_small.text: https://github.com/tamangmilan/thai_tokenizer<br/>texts = open("/content/thai_wiki_small.txt", "r", encoding="utf-8").read()<br/>texts = texts.strip()<br/># Define vocab size<br/>vocab_size = 512<br/># Initialize a tokenizer model class<br/>tokenizer = ThaiTokenizer()<br/># Start train a tokenizer<br/>start_time = time.time()<br/>tokenizer.train(texts, vocab_size)<br/>end_time = time.time()<br/># Save tokenizer: you can change path and filename.<br/>tokenizer.save("./models/thaitokenizer")<br/>print(f"Total time to complete tokenizer training: {end_time-start_time:.2f} seconds")<br/><br/># Output: Total time to complete tokenizer training: 186.11 seconds (3m 6s) [Note: Training duration will be longer if vocab_size is bigger and lesser for smaller vocab_size]</span></pre><pre class="px po pp pq bp pr bb bk"><span id="abaa" class="ps of fq pp b bg pt pu l pv pw"># Test the tokenizer<br/><br/># Initialize a tokenizer model class<br/>tokenizer = ThaiTokenizer()<br/># Load tokenizer model. This model was saved during training.<br/>tokenizer.load("./models/thaitokenizer.model")<br/># Invoke and verify the tokenizer encode and decode function for English Language<br/>eng_texts = "When society evolved in different lands"<br/>print(f"English Text: {eng_texts}")<br/>encoded_ids = tokenizer.encode(eng_texts)<br/>print(f"Encoded Ids: {encoded_ids}")<br/>decoded_texts = tokenizer.decode(encoded_ids)<br/>print(f"Decoded Texts: {decoded_texts}\n")<br/><br/># Invoke and verify the tokenizer encode and decode function for Thai Language<br/>thai_texts = "เมื่อสังคมมีวิวัฒนาการขึ้นในดินแดนต่าง"<br/>print(f"Thai Text: {thai_texts}")<br/>thai_encoded_ids = tokenizer.encode(thai_texts)<br/>print(f"Encoded Ids: {thai_encoded_ids}")<br/>thai_decoded_texts = tokenizer.decode(thai_encoded_ids)<br/>print(f"Decoded Texts: {thai_decoded_texts}")</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="mj mk pz"><img src="../Images/935f6773b6e220ece558ac08625cfef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rH1mYTo--PIoKQD3bSNaOQ.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><strong class="bf my">[Thai Tokenizer]: Encoding and decoding output for the texts in Thai and English language.</strong></figcaption></figure><p id="e932" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Perfect. Our Thai Tokenizer can now successfully and accurately encode and decode texts in both Thai and English languages.</p><p id="1c0f" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">Have you noticed that the encoded IDs for English texts are longer than Thai encoded IDs? This is because we’ve only trained our tokenizer with the Thai dataset. Hence the tokenizer is only able to build a comprehensive vocabulary for the Thai language. Since we didn’t train with an English dataset, the tokenizer has to encode right from the character level which results in longer encoded IDs. As I have mentioned before, for multilingual LLM, you should train both the English and Thai datasets with a ratio of 2:1. This will give you balanced and quality results.</p><p id="6a45" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">And that is it! </strong>We have now successfully created our own Thai Tokenizer from scratch only using Python. And, I think that was pretty cool. With this, you can easily build a tokenizer for any foreign language. This will give you a lot of leverage while implementing your Multilingual LLM.</p><p id="ab23" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">Thanks a lot for reading!</strong></p><p id="a02b" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><a class="af pn" href="https://github.com/tamangmilan/thai_tokenizer/blob/main/build_thai_tokenizer.ipynb" rel="noopener ugc nofollow" target="_blank">Link to Google Colab notebook</a></p><p id="1699" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk"><strong class="nb fr">References</strong></p><p id="b925" class="pw-post-body-paragraph mz na fq nb b go nc nd ne gr nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu fj bk">[1] Andrej Karpathy, Git Hub: <a class="af pn" href="https://github.com/karpathy/minbpe?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">Karpthy/minbpe</a></p></div></div></div></div>    
</body>
</html>