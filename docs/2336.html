<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>I Spent My Money on Benchmarking LLMs on Dutch Exams So You Don’t Have To</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>I Spent My Money on Benchmarking LLMs on Dutch Exams So You Don’t Have To</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/i-spent-my-money-on-benchmarking-llms-on-dutch-exams-so-you-dont-have-to-57a4a35ff3d1?source=collection_archive---------4-----------------------#2024-09-25">https://towardsdatascience.com/i-spent-my-money-on-benchmarking-llms-on-dutch-exams-so-you-dont-have-to-57a4a35ff3d1?source=collection_archive---------4-----------------------#2024-09-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8518" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">OpenAI’s new o1-preview is way too expensive for how it performs on the results</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@maartensukel?source=post_page---byline--57a4a35ff3d1--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maarten Sukel" class="l ep by dd de cx" src="../Images/5e0ca61edbf14129a2359d5890dc0e47.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*p6X-pIAgEXfCHYaM9HHibA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--57a4a35ff3d1--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@maartensukel?source=post_page---byline--57a4a35ff3d1--------------------------------" rel="noopener follow">Maarten Sukel</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--57a4a35ff3d1--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e6bb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Many of my customers ask for advice on which LLM (Large Language Model) to use for building products tailored to Dutch-speaking users. However, most available benchmarks are multilingual and don’t specifically focus on Dutch. As a machine learning engineer and PhD researcher into machine learning at the University of Amsterdam, I know how crucial benchmarks have been to the advancement of AI — but I also understand the risks when benchmarks are trusted blindly. This is why I decided to experiment and run some Dutch-specific benchmarking of my own.</p><p id="dd57" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this post, you’ll find an in-depth look at my first attempt at benchmarking several large language models (LLMs) on real Dutch exam questions. I’ll guide you through the entire process, from gathering over 12,000 exam PDFs to extracting question-answer pairs and grading the models’ performance automatically using LLMs. You’ll see how models like o1-preview, o1-mini, GPT-4o, GPT-4o-mini, and Claude-3 performed across different Dutch educational levels, from VMBO to VWO, and whether the higher costs of certain models lead to better results. This is just a first go at the problem, and I may dive deeper with more posts like this in the future, exploring other models and tasks. I’ll also talk about the challenges and costs involved and share some insights on which models offer the best value for Dutch-language tasks. If you’re building or scaling LLM-based products for the Dutch market, this post will provide valuable insights to help guide your choices as of September 2024.</p><p id="0bd6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s becoming more common for companies like OpenAI to make bold, almost extravagant claims about the capabilities of their models, often without enough real-world validation to back them up. That’s why benchmarking these models is so important — especially when they’re marketed as solving everything from complex reasoning to nuanced language understanding. With such grand claims, it’s vital to run objective tests to see how well they truly perform, and more specifically, how they handle the unique challenges of the Dutch language.</p><p id="7039" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I was surprised to find that there hasn’t been extensive research into benchmarking LLMs for Dutch, which is what led me to take matters into my own hands on a rainy afternoon. With so many institutions and companies relying on these models more and more, it felt like the right time to dive in and start validating these models. So, here’s my first attempt to start filling that gap, and I hope it offers valuable insights for anyone working with the Dutch-language.</p><h1 id="7823" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Why Dutch-Specific Benchmarks Matter</h1><p id="64b4" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Many of my customers work with Dutch-language products, and they need AI models that are both cost-effective and highly performant in understanding and processing Dutch. Although large language models (LLMs) have made impressive strides, most of the available benchmarks focus on English or multilingual capabilities, often neglecting the nuances of smaller languages like Dutch. This lack of focus on Dutch is significant because linguistic differences can lead to large performance gaps when a model is asked to understand non-English texts.</p><p id="bf10" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Five years ago, NLP — deep learning models for Dutch were far from mature (Like the first versions of BERT). At the time, traditional methods like TF-IDF paired with logistic regression often outperformed early deep-learning models on Dutch language tasks I worked on. While models (and datasets) have since improved tremendously, especially with the rise of transformers and multilingual pre-trained LLMs, it’s still critical to verify how well these advances translate to specific languages like Dutch. The assumption that performance gains in English carry over to other languages isn’t always valid, especially for complex tasks like reading comprehension.</p><p id="d86f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">That’s why I focused on creating a custom benchmark for Dutch, using real exam data from the Dutch “Nederlands” exams (These exams enter the public domain after they have been published). These exams don’t just involve simple language processing; they test “begrijpend lezen” (reading comprehension), requiring students to understand the intent behind various texts and answer nuanced questions about them. This type of task is particularly important because it’s reflective of real-world applications, like processing and summarizing legal documents, news articles, or customer queries written in Dutch.</p><p id="647e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">By benchmarking LLMs on this specific task, I wanted to gain deeper insights into how models handle the complexity of the Dutch language, especially when asked to interpret intent, draw conclusions, and respond with accurate answers. This is crucial for businesses building products tailored to Dutch-speaking users. My goal was to create a more targeted, relevant benchmark to help identify which models offer the best performance for Dutch, rather than relying on general multilingual benchmarks that don’t fully capture the intricacies of the language.</p></div></div><div class="of"><div class="ab cb"><div class="ll og lm oh ln oi cf oj cg ok ci bh"><div class="ol om on oo op ab ke"><figure class="le of oq or os ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><img src="../Images/06cb926cc5629a8a1c2058503421f313.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*jV7ZNb3AUd2VXp_bcR57uQ.png"/></div></figure><figure class="le of pa or os ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><img src="../Images/3e56fcdb2bed82b97159ea446bc3dd94.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*6j-9_GUF9p5VoxbcffIodw.png"/></div></figure><figure class="le of pb or os ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><img src="../Images/c0a96ab6170e2c975eb7be27e7808c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*2YwK4oBbqAIsp8hXj_K_fA.png"/></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx pj ed pk pl">Examples of Dutch exams in the Netherlands, these exams enter the public domain after they have been published.</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="64d6" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">How the Benchmarking Works</h1><p id="6511" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Let me walk you through how I built and executed this benchmark:</p><ol class=""><li id="924e" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pm pn po bk"><strong class="mk fr">PDF Collection</strong>: I began by collecting over 12,000 PDFs from Dutch state exams. These exams include reading passages and questions that test a student’s ability to comprehend and interpret written Dutch.</li><li id="b805" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><strong class="mk fr">Data Extraction</strong>: Next, I extracted the relevant information from the PDFs using LLMs, turning the text into structured question-answer (Q&amp;A) pairs. For example, a typical question from a PDF might look like this: <em class="pu">“Wat is de hoofdgedachte van de schrijver in alinea 3 van tekst 2?” </em>After extraction, this question becomes a structured Q&amp;A pair like this: <strong class="mk fr">Question</strong>: What is the main idea of the author in paragraph 3?<br/><strong class="mk fr">Correct Answer</strong>: The author argues that technological advancements bring both positive and negative consequences (2 points)</li><li id="91b8" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><strong class="mk fr">Model selection: </strong>The selection of models in this benchmark includes a mix of well-known LLMs, ranging from smaller, more cost-efficient models like <strong class="mk fr">o1-mini</strong> and <strong class="mk fr">gpt-4o-mini</strong>, to more expensive options like <strong class="mk fr">o1-preview</strong>. These models were tested on Dutch-language tasks to assess their ability to handle reading comprehension (“begrijpend lezen”) tasks from the Dutch “Nederlands” exam. Notably, <strong class="mk fr">Claude-3–5-sonnet</strong> and <strong class="mk fr">Claude-3-haiku</strong> were also included, providing insight into how AI models from Anthropic stack up against the GPT family. I selected several models to do this initial benchmark with, definitely not extensive enough yet. Let me know if you would want me to add more in the future!</li><li id="5bdf" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><strong class="mk fr">Question answering: </strong>The fun part! I hooked up the APIs of the LLMs and gave them a question with corresponding texts and let them answer the questions. It became less fun when the more expensive models kicked in and my credit card started informing me it was not very excited about these endeavors. The lengths I go through for my readers!</li><li id="b799" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><strong class="mk fr">Automated Grading</strong>: Using a prompt that knows the correct answer I ask for an objective decision if the required answer is in the answer given by the LLM. With this method, the LLM-generated answers are compared to the correct answers from the official answer sheets. Each question is scored based on how closely the model’s answer matches the correct one.</li><li id="f90d" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd pm pn po bk"><strong class="mk fr">Scoring &amp; Reporting</strong>: After grading, the models are evaluated on how many points they earned relative to the maximum possible points for each exam. This scoring gives a clear idea of which models perform well and which struggle with Dutch reading comprehension tasks.</li></ol><p id="0d16" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s a bit surreal when you think about it — LLMs benchmarking other LLMs, graded by LLMs, without a human in sight (except me, writing the code to let them do this on a rainy afternoon). This method allows for scalable and automated comparisons, but it’s not without limitations. While this approach gives a strong basis for comparing models, it’s not the final word. Still, I wanted to put together something to gain insight into how these models perform in the context of the Dutch language specifically.</p><h1 id="8ba7" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The API Cost Dilemma</h1><p id="2fa5" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Running these benchmarks came at a significant cost. Processing full-text exam questions with every request quickly consumed tokens, and I ended up spending over €100 in API fees just for this initial round of testing. This forced some limitations on how many questions I could process with the different models, but it was still enough to uncover some valuable insights.</p><p id="ffff" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If any Dutch institutions are interested in collaborating on more extensive benchmarking efforts, I’d be eager to work together to scale this project. By expanding the scope, we could dive deeper into a wider range of exams, significantly increase the number of questions answered, and benchmark a broader selection of models. This would provide even more comprehensive insights into model performance help refine our understanding of how various LLMs handle Dutch-language tasks across different educational levels and complexities and help companies pick the best LLM and not the best marketing.</p><p id="d342" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I conducted two separate benchmarks: one with smaller, cheaper models, and another with larger, more expensive models until I hit my daily API limits. The number of exam questions used was 329 for the cheaper models and 104 for the more expensive “titans.” To put this in perspective, this would be equivalent to a human taking approximately 4 to 13 full exams.</p><p id="83ec" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here’s a breakdown of the model pricing (as of September 25th, via <a class="af pv" href="https://llmpricecheck.com/" rel="noopener ugc nofollow" target="_blank">LLM Price Check</a>):</p><figure class="ol om on oo op of pf pg paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="pf pg pw"><img src="../Images/68d29300c0c03c63172696f40be5b99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bSfrPuOepP18PzVD0CgoaA.png"/></div></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx">From <a class="af pv" href="https://llmpricecheck.com/" rel="noopener ugc nofollow" target="_blank">https://llmpricecheck.com/</a> (Checked September 25th) Image by author</figcaption></figure><ul class=""><li id="4fcd" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd px pn po bk">“o1-preview” costs $10 per million tokens for input and $30 for output.</li><li id="a324" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd px pn po bk">“o1-mini,” on the other hand, costs only $0.10 per million tokens for input and $0.25 for output.</li></ul><p id="1ca7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This means “o1-preview” is approximately 114 times more expensive than “o1-mini.” The key question, then, is whether the extra cost translates into better performance, and if so, by how much. So, is it worth the extra cost?</p><h1 id="079f" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Benchmarking the Models: Fast, Cheap, and… Better?</h1><p id="cf77" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Since the launch of <strong class="mk fr">o1-preview</strong>, I’ve been skeptical about its performance, as it seemed slower and significantly more expensive compared to other models. So, I was eager to see how it would perform in this benchmark.</p><p id="fa5e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Interestingly, the <strong class="mk fr">o1-mini</strong> model actually outperformed more expensive options like <strong class="mk fr">GPT-4o</strong> and <strong class="mk fr">o1-preview</strong>. Specifically, <strong class="mk fr">o1-mini</strong> earned <strong class="mk fr">66.75%</strong> of the possible points, compared to <strong class="mk fr">62.32%</strong> for <strong class="mk fr">GPT-4O</strong> and <strong class="mk fr">61.91%</strong> for <strong class="mk fr">o1-preview</strong>. Based on these results, I’m now considering shifting from <strong class="mk fr">GPT-4O-mini</strong>, which earned <strong class="mk fr">61.36%</strong>, to <strong class="mk fr">o1-mini</strong> for Dutch language tasks, as it offers better performance at a significantly lower cost.</p><p id="eb2d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here’s how the other models fared:</p><ul class=""><li id="4db6" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd px pn po bk"><strong class="mk fr">Claude-3–5-sonnet</strong> earned <strong class="mk fr">61.28%</strong>, while</li><li id="096a" class="mi mj fq mk b go pp mm mn gr pq mp mq mr pr mt mu mv ps mx my mz pt nb nc nd px pn po bk"><strong class="mk fr">Claude-3-haiku</strong> lagged behind, with only <strong class="mk fr">42.91%</strong>.</li></ul><p id="59cf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Seems like going for the Claude models will cause less performant products that are also more expensive.</p><p id="57e7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The performance breakdown also showed that all of these models handled <strong class="mk fr">VMBO-level</strong> exams more easily but struggled with the more complex <strong class="mk fr">VWO-level</strong> questions — something expected given the increasing difficulty of the exams. This highlights the value of using a more cost-effective model like <strong class="mk fr">o1-mini</strong>, which not only performs well across a variety of tasks but also delivers strong results on more advanced educational content.</p></div></div><div class="of"><div class="ab cb"><div class="ll og lm oh ln oi cf oj cg ok ci bh"><figure class="ol om on oo op of os ot paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="pf pg py"><img src="../Images/af865fd8fce331d3df1ade3266656372.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wkeytTORmpAhQE52sYnPHA.png"/></div></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx">Results of 6 LLM‘s competing on answering 104 Dutch exam questions. Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="ol om on oo op of pf pg paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="pf pg pz"><img src="../Images/7c5a4f83a0560679ca70e96c395e3233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrPNfRg4upW7DuBqifYr_w.png"/></div></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx">Results of 3 LLM’s competing on answering 329 Dutch exam questions. Image by author</figcaption></figure><p id="9533" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Handling Different Exam Levels</strong>: The exams are divided into different educational levels, such as VMBO, HAVO, and VWO. My system tracks how well models perform across these different levels. Unsurprisingly, the models did better on simpler VMBO-level questions and struggled more with complex VWO-level questions.</p></div></div><div class="of"><div class="ab cb"><div class="ll og lm oh ln oi cf oj cg ok ci bh"><figure class="ol om on oo op of os ot paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="pf pg qa"><img src="../Images/72bace1ec5311995e729a98d260da708.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*twBkfGDkUFdqJe4EY9uNOg.png"/></div></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx">Comparison of eductional level and model performance between the six models. Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="ol om on oo op of pf pg paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="pf pg qb"><img src="../Images/e5c36caf01aeee1513f9b26f9a28f087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QH9SjbEHg-IMvBqt5iG1UQ.png"/></div></div><figcaption class="pc pd pe pf pg ph pi bf b bg z dx">The percentage of points scored over all the different educational levels for the tree cheaper models. Image by author</figcaption></figure><h1 id="616d" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Limitations and Next Steps</h1><p id="7be3" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">It’s important to mention that it’s possible some of these Dutch exam texts may have been part of the training data for certain LLMs, which could have impacted the results. However, these benchmarks still offer valuable insights for developers working on Dutch-language products.</p><p id="e70d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">That said, the number of questions processed so far is relatively low. In future iterations, I plan to run more comprehensive benchmarks to generate even deeper insights into the models’ performance.</p><p id="95ac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This approach to benchmarking can be extended to other subjects, and I also filtered out pure-text questions. Setting up a benchmark for multimodal models, which can analyze images alongside text, would be particularly interesting since many exams, such as history and geography, involve visual elements like charts, maps, or diagrams.</p><p id="caaf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the future, this method could easily be applied to other Dutch courses such as <strong class="mk fr">Biologie</strong>, <strong class="mk fr">Natuurkunde</strong>, <strong class="mk fr">Scheikunde</strong>, <strong class="mk fr">Wiskunde A/B/C</strong>, <strong class="mk fr">Aardrijkskunde</strong>, <strong class="mk fr">Bedrijfseconomie</strong>, <strong class="mk fr">Economie</strong>, <strong class="mk fr">Filosofie</strong>, <strong class="mk fr">Geschiedenis</strong>, <strong class="mk fr">Maatschappijwetenschappen</strong>, <strong class="mk fr">Kunst</strong>, <strong class="mk fr">Muziek</strong>, <strong class="mk fr">Tehatex</strong>, and languages like <strong class="mk fr">Arabisch</strong>, <strong class="mk fr">Duits</strong>, <strong class="mk fr">Engels</strong>, <strong class="mk fr">Frans</strong>, <strong class="mk fr">Fries</strong>, <strong class="mk fr">Grieks</strong>, <strong class="mk fr">Latijn</strong>, <strong class="mk fr">Russisch</strong>, <strong class="mk fr">Spaans</strong>, and <strong class="mk fr">Turks</strong>. Extending this to subjects like <strong class="mk fr">Natuur- en scheikunde 1 &amp; 2</strong>, <strong class="mk fr">Wiskunde</strong>, <strong class="mk fr">Maatschappijleer</strong>, and even the arts (e.g., <strong class="mk fr">Dans</strong>, <strong class="mk fr">Drama</strong>, <strong class="mk fr">Beeldende vakken</strong>) would provide a broad view of model performance across diverse disciplines.</p><p id="ca4f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you’re interested in supporting this project, feel free to reach out or <a class="af pv" href="https://buymeacoffee.com/maartensukel" rel="noopener ugc nofollow" target="_blank">buy me a coffee</a>! The code I’ve developed is scalable and can handle a much larger range of Dutch exams and topics with the right resources. Collaborating to explore these additional subjects and multimodal benchmarks would open up even deeper insights into how AI models can perform in Dutch education.</p><h1 id="3f24" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Final Thoughts</h1><p id="22b1" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">If you want help with building or scaling AI or machine learning products responsibly, or if you’re curious about which LLMs perform best in specific languages like Dutch, I’d be happy to help through my company, <strong class="mk fr">The AI Factory</strong>. Feel free to reach out! Feel free to contact <a class="af pv" href="http://contact@maartensukel.nl" rel="noopener ugc nofollow" target="_blank">me</a>, and if you found this benchmarking useful, follow me on <a class="af pv" href="https://www.linkedin.com/in/maartensukel/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a> for updates on future AI and performance insights.</p></div></div></div></div>    
</body>
</html>