- en: Predicting metadata for humanitarian datasets with LLMs part 2 â€” An alternative
    to fine-tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LLMé¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®ç¬¬äºŒéƒ¨åˆ†â€”â€”å¾®è°ƒçš„æ›¿ä»£æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03)
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    Â·29 min readÂ·Aug 3, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)
    Â·29åˆ†é’Ÿé˜…è¯»Â·2024å¹´8æœˆ3æ—¥
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0002f919edfe77b2a945716650c673f7.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0002f919edfe77b2a945716650c673f7.png)'
- en: 'Source: GPT-4o'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šGPT-4o
- en: TL;DR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*In the humanitarian response world there can be tens of thousands of tabular
    (CSV and Excel) datasets, many of which contain critical information for helping
    save lives. Data can be provided by hundreds of different organizations with different
    naming conventions, languages and data standards, so having information (metadata)
    about what each column represents in tables is important for finding the right
    data and understanding how it fits together. Much of this metadata is set manually,
    which is time-consuming and error prone, so any automatic method can have a real
    effect towards helping people. In this article we revisit a previous analysis
    â€œ*[*Predicting Metadata of Humanitarian Datasets with GPT 3*](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)*â€
    to see how advances in the last 18 months open the way for more efficient and
    less time-consuming methods for setting metadata on tabular data.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨äººé“ä¸»ä¹‰å“åº”é¢†åŸŸï¼Œå¯èƒ½ä¼šæœ‰æˆåƒä¸Šä¸‡çš„è¡¨æ ¼æ•°æ®é›†ï¼ˆCSVå’ŒExcelï¼‰ï¼Œå…¶ä¸­è®¸å¤šåŒ…å«æ‹¯æ•‘ç”Ÿå‘½çš„å…³é”®ä¿¡æ¯ã€‚æ•°æ®å¯èƒ½ç”±æ•°ç™¾ä¸ªä¸åŒçš„ç»„ç»‡æä¾›ï¼Œä¸”å‘½åçº¦å®šã€è¯­è¨€å’Œæ•°æ®æ ‡å‡†å„å¼‚ï¼Œå› æ­¤äº†è§£è¡¨æ ¼ä¸­æ¯ä¸€åˆ—çš„å«ä¹‰ï¼ˆå…ƒæ•°æ®ï¼‰å¯¹äºæ‰¾åˆ°åˆé€‚çš„æ•°æ®å¹¶ç†è§£å…¶å¦‚ä½•ç»„åˆè‡³å…³é‡è¦ã€‚å¤§éƒ¨åˆ†å…ƒæ•°æ®æ˜¯æ‰‹åŠ¨è®¾ç½®çš„ï¼Œè¿™æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ï¼Œå› æ­¤ä»»ä½•è‡ªåŠ¨åŒ–æ–¹æ³•éƒ½å¯èƒ½åœ¨å¸®åŠ©äººä»¬æ–¹é¢äº§ç”Ÿå®é™…å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å…ˆå‰çš„åˆ†æâ€œ*[*ä½¿ç”¨GPT-3é¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®*](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)*â€ï¼Œä»¥äº†è§£è¿‡å»18ä¸ªæœˆçš„è¿›å±•å¦‚ä½•ä¸ºæ›´é«˜æ•ˆã€èŠ‚çœæ—¶é—´çš„æ–¹æ³•é“ºå¹³é“è·¯ï¼Œç”¨äºè®¾ç½®è¡¨æ ¼æ•°æ®çš„å…ƒæ•°æ®ã€‚*â€'
- en: '*Using metadata-tagged CSV and Excel datasets from the* [*Humanitarian Data
    Exchange*](https://data.humdata.org/) *(HDX) we show that fine-tuning GPT-4o-mini
    works well for predicting* [*Humanitarian Exchange Language*](https://hxlstandard.org/)
    *(HXL) tags and attributes for the most common tags related to location and dates.
    However, for less well-represented tags and attributes the technique can be a
    bit limited due to poor quality training data where humans have made mistakes
    in manually labelling data or simply arenâ€™t using all possible HXL metadata combinations.
    It also has the limitation of not being able to adjust when the metadata standard
    changes, since the training data would not reflect those changes.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*é€šè¿‡ä½¿ç”¨å¸¦æœ‰å…ƒæ•°æ®æ ‡ç­¾çš„CSVå’ŒExcelæ•°æ®é›†ï¼Œæ¥è‡ªäº* [*äººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°*](https://data.humdata.org/) *(HDX)ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¾®è°ƒGPT-4o-miniåœ¨é¢„æµ‹*
    [*äººé“ä¸»ä¹‰äº¤æ¢è¯­è¨€*](https://hxlstandard.org/) *(HXL)æ ‡ç­¾å’Œå±æ€§æ—¶çš„è‰¯å¥½æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºä¸ä½ç½®å’Œæ—¥æœŸç›¸å…³çš„æœ€å¸¸è§æ ‡ç­¾ã€‚ç„¶è€Œï¼Œå¯¹äºé‚£äº›è¾ƒå°‘å‡ºç°çš„æ ‡ç­¾å’Œå±æ€§ï¼Œè¿™ä¸€æŠ€æœ¯å¯èƒ½ä¼šå—åˆ°è®­ç»ƒæ•°æ®è´¨é‡ä¸ä½³çš„é™åˆ¶ï¼ŒåŸå› åœ¨äºäººå·¥æ ‡ç­¾é”™è¯¯æˆ–äººä»¬æ²¡æœ‰ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„HXLå…ƒæ•°æ®ç»„åˆã€‚å®ƒè¿˜å­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œå³å½“å…ƒæ•°æ®æ ‡å‡†å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå®ƒæ— æ³•è¿›è¡Œè°ƒæ•´ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸ä¼šåæ˜ è¿™äº›å˜åŒ–ã€‚*'
- en: '*Given more powerful LLMs are now available, we tested a technique to directly
    prompt GPT-4o or GPT-4o-mini rather than fine-tuning, providing the full HXL core
    schema definition in the system prompt now that larger context windows are available.
    This approach was shown to be more accurate than fine-tuning when using GPT-4o,
    able to support rarer HXL tags and attributes and requiring no custom training
    data, making it easier to manage and deploy. It is however more expensive, but
    not if using GPT-4o-mini, albeit with a slight decrease in performance. Using
    this approach we provide a simple Python class in a* [*GitHub Gist*](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)
    *that can be used in data processing pipelines to automatically add HXL metadata
    tags and attributes to tabular datasets.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*é‰´äºç°åœ¨æœ‰äº†æ›´å¼ºå¤§çš„LLMå¯ç”¨ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸€ç§æŠ€æœ¯ï¼Œç›´æ¥æç¤ºGPT-4oæˆ–GPT-4o-miniï¼Œè€Œä¸æ˜¯è¿›è¡Œå¾®è°ƒï¼Œåœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›å®Œæ•´çš„HXLæ ¸å¿ƒæ¶æ„å®šä¹‰ï¼Œå› ä¸ºç°åœ¨å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ã€‚äº‹å®è¯æ˜ï¼Œå½“ä½¿ç”¨GPT-4oæ—¶ï¼Œè¿™ç§æ–¹æ³•æ¯”å¾®è°ƒæ›´å‡†ç¡®ï¼Œèƒ½å¤Ÿæ”¯æŒè¾ƒå°‘è§çš„HXLæ ‡ç­¾å’Œå±æ€§ï¼Œå¹¶ä¸”ä¸éœ€è¦å®šåˆ¶çš„è®­ç»ƒæ•°æ®ï¼Œä½¿å¾—ç®¡ç†å’Œéƒ¨ç½²æ›´åŠ ç®€ä¾¿ã€‚ç„¶è€Œï¼Œå®ƒçš„æˆæœ¬è¾ƒé«˜ï¼Œä½†å¦‚æœä½¿ç”¨GPT-4o-miniï¼Œåˆ™æˆæœ¬è¾ƒä½ï¼Œå°½ç®¡æ€§èƒ½ç•¥æœ‰ä¸‹é™ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„Pythonç±»ï¼Œä½äº*
    [*GitHub Gist*](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)
    *ï¼Œå¯ä»¥åœ¨æ•°æ®å¤„ç†ç®¡é“ä¸­è‡ªåŠ¨ä¸ºè¡¨æ ¼æ•°æ®é›†æ·»åŠ HXLå…ƒæ•°æ®æ ‡ç­¾å’Œå±æ€§ã€‚*'
- en: Generative AI moves fast!
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ€§AIå‘å±•å¾—**éå¸¸**è¿…é€Ÿï¼
- en: About 18 months ago I wrote a blog post [Predicting Metadata of Humanitarian
    Datasets with GPT 3](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦18ä¸ªæœˆå‰ï¼Œæˆ‘å†™äº†ä¸€ç¯‡åšå®¢æ–‡ç«  [ä½¿ç”¨GPT-3é¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)ã€‚
- en: Thatâ€™s right, with GPT 3, not even 3.5! ğŸ™‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é”™ï¼Œæ˜¯ä½¿ç”¨GPT-3ï¼Œä¸æ˜¯GPT-3.5ï¼ğŸ™‚
- en: Even so, back then Large Language Model (LLM) fine-tuning produced great performance
    for predicting [Humanitarian Exchange Language](https://hxlstandard.org/) (HXL)
    metadata fields for tabular datasets on the amazing [Humanitarian Data Exchange](https://data.humdata.org/)
    (HDX). In that study, the training data represented the distribution of HXL data
    on HDX and so was comprised of the most common tags relating to location and dates.
    These are very important for linking different datasets together in location and
    time, a crucial factor in using data to optimize humanitarian response.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä¾¿å¦‚æ­¤ï¼Œæ—©åœ¨é‚£æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒå°±å·²åœ¨é¢„æµ‹[äººé“ä¸»ä¹‰äº¤æ¢è¯­è¨€](https://hxlstandard.org/)ï¼ˆHXLï¼‰å…ƒæ•°æ®å­—æ®µæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ä»¤äººæƒŠå¹çš„[äººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°](https://data.humdata.org/)ï¼ˆHDXï¼‰ä¸Šçš„è¡¨æ ¼æ•°æ®é›†ã€‚åœ¨é‚£é¡¹ç ”ç©¶ä¸­ï¼Œè®­ç»ƒæ•°æ®ä»£è¡¨äº†HDXä¸Šçš„HXLæ•°æ®åˆ†å¸ƒï¼Œå› æ­¤åŒ…å«äº†ä¸ä½ç½®å’Œæ—¥æœŸç›¸å…³çš„æœ€å¸¸è§æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾å¯¹äºå°†ä¸åŒæ•°æ®é›†æŒ‰ä½ç½®å’Œæ—¶é—´å…³è”èµ·æ¥è‡³å…³é‡è¦ï¼Œè¿™æ˜¯åˆ©ç”¨æ•°æ®ä¼˜åŒ–äººé“ä¸»ä¹‰å“åº”çš„ä¸€ä¸ªå…³é”®å› ç´ ã€‚
- en: The LLM field has since advanced â€¦ a *LOT*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMé¢†åŸŸæ­¤åå·²å–å¾—äº†â€¦ **å·¨å¤§çš„**è¿›å±•ã€‚
- en: So in this article, we will revisit the technique, expand it to cover less frequent
    HXL tags and attributes and explore other options now available to us for situations
    where a complex, high-cardinality taxonomy needs to be applied to data. We will
    also explore the ability to predict less frequent HXL standard tags and attributes
    not currently represented in the human-labeled training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°å®¡è§†è¿™ä¸€æŠ€æœ¯ï¼Œå°†å…¶æ‰©å±•åˆ°æ¶µç›–ä¸å¤ªå¸¸è§çš„HXLæ ‡ç­¾å’Œå±æ€§ï¼Œå¹¶æ¢è®¨ç›®å‰å¯ç”¨çš„å…¶ä»–é€‰é¡¹ï¼Œé€‚ç”¨äºéœ€è¦å°†å¤æ‚ã€é«˜å±‚æ¬¡çš„åˆ†ç±»æ³•åº”ç”¨äºæ•°æ®çš„æƒ…å†µã€‚æˆ‘ä»¬è¿˜å°†æ¢è®¨é¢„æµ‹å½“å‰åœ¨äººä¸ºæ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«çš„è¾ƒå°‘è§çš„HXLæ ‡å‡†æ ‡ç­¾å’Œå±æ€§çš„èƒ½åŠ›ã€‚
- en: Setup
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: 'You can follow along with this analysis by opening these notebooks in [Google
    Colab](https://colab.research.google.com/) or running them locally:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡åœ¨[Google Colab](https://colab.research.google.com/)ä¸­æ‰“å¼€è¿™äº›ç¬”è®°æœ¬ï¼Œæˆ–è€…åœ¨æœ¬åœ°è¿è¡Œå®ƒä»¬æ¥è·Ÿéšæœ¬æ¬¡åˆ†æï¼š
- en: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    â€” A notebook for creating test and training datasets'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    â€” ç”¨äºåˆ›å»ºæµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†çš„ç¬”è®°æœ¬'
- en: '[openai-hxl-prediction.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb)
    â€” Notebook exploring fine-tuning and prompting for predicting HXL datasets'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[openai-hxl-prediction.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb)
    â€” æ¢ç´¢å¾®è°ƒå’Œæç¤ºä»¥é¢„æµ‹HXLæ•°æ®é›†çš„ç¬”è®°æœ¬'
- en: Please refer to the [README](https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md)
    in the repo for installation instructions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜…ä»“åº“ä¸­çš„[README](https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md)è·å–å®‰è£…è¯´æ˜ã€‚
- en: HXL Data from the Humanitarian Data Exchange
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¥è‡ªäººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°çš„HXLæ•°æ®
- en: For this study, and with help from the HDX team, we will use data extracted
    from the HDX platform using a crawler process they run to track the use of HXL
    metadata tags and attributes on the platform. You can find great HXL resources
    on [GitHub](https://github.com/HXLStandard), but if you want to follow along with
    this analysis I have also saved the source data on Google Drive as the crawler
    will take days to process the hundreds of thousands of tabular datasets on HDX.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæœ¬ç ”ç©¶ï¼Œåœ¨HDXå›¢é˜Ÿçš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»HDXå¹³å°æå–çš„æ•°æ®ï¼Œé€šè¿‡ä»–ä»¬è¿è¡Œçš„çˆ¬è™«è¿‡ç¨‹è·Ÿè¸ªå¹³å°ä¸ŠHXLå…ƒæ•°æ®æ ‡ç­¾å’Œå±æ€§çš„ä½¿ç”¨æƒ…å†µã€‚ä½ å¯ä»¥åœ¨[GitHub](https://github.com/HXLStandard)æ‰¾åˆ°å¾ˆæ£’çš„HXLèµ„æºï¼Œä½†å¦‚æœä½ æƒ³è·Ÿéšæœ¬æ¬¡åˆ†æï¼Œæˆ‘ä¹Ÿå°†æºæ•°æ®ä¿å­˜åˆ°äº†Google
    Driveï¼Œå› ä¸ºçˆ¬è™«éœ€è¦å‡ å¤©æ—¶é—´æ‰èƒ½å¤„ç†HDXä¸Šæˆåƒä¸Šä¸‡çš„è¡¨æ ¼æ•°æ®é›†ã€‚
- en: The data looks like this, with one row per HXL-tagged table column â€¦
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å¦‚ä¸‹æ‰€ç¤ºï¼Œæ¯ä¸ªHXLæ ‡ç­¾åŒ–çš„è¡¨æ ¼åˆ—ä¸ºä¸€è¡Œâ€¦
- en: '![](../Images/1d1abdc669b37dc43cb697223ff9d3f7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d1abdc669b37dc43cb697223ff9d3f7.png)'
- en: Example of data used in this study, with a row per tabular data column.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æ•°æ®ç¤ºä¾‹ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªè¡¨æ ¼æ•°æ®åˆ—ã€‚
- en: The core HXL Schema
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒHXLæ¶æ„
- en: The [HXL postcard](https://hxlstandard.org/standard/1-1final/postcards/) is
    a really great overview of the most common HXL tags and attributes in the core
    schema. For our analysis, we will apply the full standard as found on [HDX](https://data.humdata.org/dataset/hxl-core-schemas)
    which provides a [spreadsheet](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)
    of supported tags and attributes â€¦
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[HXLæ˜ä¿¡ç‰‡](https://hxlstandard.org/standard/1-1final/postcards/)æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¦‚è¿°ï¼Œå±•ç¤ºäº†æ ¸å¿ƒæ¶æ„ä¸­æœ€å¸¸è§çš„HXLæ ‡ç­¾å’Œå±æ€§ã€‚å¯¹äºæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬å°†åº”ç”¨[HDX](https://data.humdata.org/dataset/hxl-core-schemas)ä¸Šæä¾›çš„å®Œæ•´æ ‡å‡†ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ”¯æŒçš„æ ‡ç­¾å’Œå±æ€§çš„[ç”µå­è¡¨æ ¼](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)â€¦'
- en: '![](../Images/c72d8b2f6bce80bae6e654de56ba7af0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c72d8b2f6bce80bae6e654de56ba7af0.png)'
- en: Excerpt of the â€œCore HXL Schemaâ€ used for this study, as found on the [Humanitarian
    Data Exchange](https://data.humdata.org/dataset/hxl-core-schemas)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„â€œæ ¸å¿ƒHXLæ¶æ„â€æ‘˜å½•ï¼Œæ¥æºäº[Humanitarian Data Exchange](https://data.humdata.org/dataset/hxl-core-schemas)
- en: Data Processing
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®å¤„ç†
- en: 'The [generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)
    notebook provides all the steps taken to create test and training datasets, but
    here are some key points to note:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)ç¬”è®°æœ¬æä¾›äº†åˆ›å»ºæµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†çš„æ‰€æœ‰æ­¥éª¤ï¼Œä½†è¿™é‡Œæœ‰ä¸€äº›è¦æ³¨æ„çš„å…³é”®ç‚¹ï¼š'
- en: '**1\. Removal of automatic pipeline repeat HXL data**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. åˆ é™¤è‡ªåŠ¨åŒ–ç®¡é“é‡å¤çš„HXLæ•°æ®**'
- en: In this study, I removed duplicate data created by automated pipelines that
    upload data to HDX, by using an MDF hash of column names in each tabular dataset
    (CSV and Excel files). For example, a CSV file of population statistics created
    by an organization is often very similar for each country-specific CSV or Excel
    file, so we only take one example. This has a balancing effect on the data, providing
    more variation of HXL tags and attributes by removing very similar repeat data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘é€šè¿‡ä½¿ç”¨æ¯ä¸ªè¡¨æ ¼æ•°æ®é›†ï¼ˆCSVå’ŒExcelæ–‡ä»¶ï¼‰ä¸­åˆ—åç§°çš„MDFå“ˆå¸Œï¼Œåˆ é™¤ç”±è‡ªåŠ¨åŒ–ç®¡é“ä¸Šä¼ åˆ°HDXçš„æ•°æ®ä¸­çš„é‡å¤é¡¹ã€‚ä¾‹å¦‚ï¼ŒæŸä¸ªç»„ç»‡åˆ›å»ºçš„äººå£ç»Ÿè®¡CSVæ–‡ä»¶é€šå¸¸ä¸æ¯ä¸ªç‰¹å®šå›½å®¶çš„CSVæˆ–Excelæ–‡ä»¶éå¸¸ç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬åªä¿ç•™ä¸€ä¸ªç¤ºä¾‹ã€‚è¿™å¯¹æ•°æ®èµ·åˆ°äº†å¹³è¡¡ä½œç”¨ï¼Œé€šè¿‡åˆ é™¤éå¸¸ç›¸ä¼¼çš„é‡å¤æ•°æ®ï¼Œæä¾›äº†æ›´å¤šçš„HXLæ ‡ç­¾å’Œå±æ€§å˜å¼‚æ€§ã€‚
- en: '**2\. Constraining data to valid HXL**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. é™åˆ¶æ•°æ®ä¸ºæœ‰æ•ˆçš„HXLæ ¼å¼**'
- en: About 50% of the HDX data with HXL tags uses a tag or attribute which are not
    specified in the [HXL Core Schema](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing),
    so this data is removed from training and test sets.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦ 50% çš„å¸¦æœ‰ HXL æ ‡ç­¾çš„ HDX æ•°æ®ä½¿ç”¨äº†åœ¨[HXL æ ¸å¿ƒæ¶æ„](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)ä¸­æœªæŒ‡å®šçš„æ ‡ç­¾æˆ–å±æ€§ï¼Œå› æ­¤è¿™äº›æ•°æ®ä¼šè¢«ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç§»é™¤ã€‚
- en: '**3\. Data enrichment**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. æ•°æ®å¢å¼º**'
- en: As a (mostly!) human being, when deciding what HXL tags and attributes to use
    on a column, I take a peek at the data for that column and also the data as a
    whole in the table. For this analysis we do the same for the LLM fine-tuning and
    prompt data, adding in data excerpts for each column. A table description is also
    added using an LLM (GPT-3.5-Turbo) summary of the data to make them consistent,
    as summaries on HDX can vary in form, ranging from pages to a few words.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªï¼ˆå¤§éƒ¨åˆ†æ˜¯ï¼ï¼‰äººç±»ï¼Œåœ¨å†³å®šåœ¨æŸä¸€åˆ—ä½¿ç”¨å“ªäº› HXL æ ‡ç­¾å’Œå±æ€§æ—¶ï¼Œæˆ‘ä¼šæŸ¥çœ‹è¯¥åˆ—çš„æ•°æ®ï¼Œä¹Ÿä¼šæŸ¥çœ‹è¡¨æ ¼ä¸­æ‰€æœ‰æ•°æ®ã€‚å¯¹äºè¿™ä¸ªåˆ†æï¼Œæˆ‘ä»¬ä¹Ÿå¯¹ LLM å¾®è°ƒå’Œæç¤ºæ•°æ®åšåŒæ ·çš„å¤„ç†ï¼Œæ·»åŠ æ¯ä¸€åˆ—çš„æ•°æ®æ‘˜å½•ã€‚è¿˜ä½¿ç”¨
    LLMï¼ˆGPT-3.5-Turboï¼‰å¯¹æ•°æ®çš„æ‘˜è¦æ¥ä¸ºè¡¨æ ¼æ·»åŠ æè¿°ï¼Œä½¿å®ƒä»¬ä¸€è‡´ï¼Œå› ä¸º HDX ä¸Šçš„æ‘˜è¦æ ¼å¼å„ä¸ç›¸åŒï¼Œå¯èƒ½æ˜¯å‡ é¡µï¼Œä¹Ÿå¯èƒ½æ˜¯å‡ å¥è¯ã€‚
- en: '**4\. Carefully splitting data to create train/test sets**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. ä»”ç»†åˆ’åˆ†æ•°æ®ä»¥åˆ›å»ºè®­ç»ƒ/æµ‹è¯•é›†**'
- en: Many machine learning pipelines split data randomly to create training and test
    sets. However, for HDX data this would result in columns and files from the same
    organization being in train and test. I felt this was a bit too easy for testing
    predictions and so instead split the data by organizations to ensure organizations
    in the test set were not in the training data. Additionally, subsidiaries of the
    same parent organization â€” eg â€œocha-iraqâ€ and â€œocha-libyaâ€ â€” were not allowed
    to be in both the training and test sets, again to make the predictions more realistic.
    My aim was to test prediction with organizations as if their data had never been
    seen before.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šæœºå™¨å­¦ä¹ ç®¡é“é€šè¿‡éšæœºåˆ’åˆ†æ•°æ®æ¥åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚ç„¶è€Œï¼Œå¯¹äº HDX æ•°æ®ï¼Œè¿™æ ·åšä¼šå¯¼è‡´æ¥è‡ªåŒä¸€ç»„ç»‡çš„åˆ—å’Œæ–‡ä»¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ã€‚æˆ‘è®¤ä¸ºè¿™ç§æ–¹å¼å¯¹é¢„æµ‹æµ‹è¯•æ¥è¯´æœ‰ç‚¹å¤ªç®€å•äº†ï¼Œå› æ­¤æˆ‘é€‰æ‹©æŒ‰ç»„ç»‡åˆ’åˆ†æ•°æ®ï¼Œç¡®ä¿æµ‹è¯•é›†ä¸­çš„ç»„ç»‡ä¸å‡ºç°åœ¨è®­ç»ƒæ•°æ®ä¸­ã€‚æ­¤å¤–ï¼ŒåŒä¸€æ¯å…¬å¸ä¸‹çš„å­å…¬å¸â€”â€”ä¾‹å¦‚â€œocha-iraqâ€å’Œâ€œocha-libyaâ€â€”â€”ä¹Ÿä¸èƒ½åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ï¼Œä»¥ä½¿é¢„æµ‹æ›´åŠ çœŸå®ã€‚æˆ‘çš„ç›®æ ‡æ˜¯æµ‹è¯•é¢„æµ‹ï¼Œå‡è®¾è¿™äº›ç»„ç»‡çš„æ•°æ®ä»æœªè¢«è§è¿‡ã€‚
- en: After all of the above and down-sampling to save costs, we are left with **2,883**
    rows in the training set and **485** rows in the test set.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œæˆä¸Šè¿°æ‰€æœ‰æ­¥éª¤å¹¶è¿›è¡Œé™é‡‡æ ·ä»¥èŠ‚çœæˆæœ¬åï¼Œæˆ‘ä»¬å¾—åˆ°äº†**2,883**è¡Œè®­ç»ƒé›†æ•°æ®å’Œ**485**è¡Œæµ‹è¯•é›†æ•°æ®ã€‚
- en: Creating JSONL fine-tuning prompt files
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»º JSONL å¾®è°ƒæç¤ºæ–‡ä»¶
- en: In my original article I opted for using a completion model, but with the release
    of [GPT-4o-mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
    I instead generated prompts appropriate for fine-tuning a *chat* model (see [here](https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned)
    for more information about the available models).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘åŸæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ä¸€ä¸ªå®Œæˆæ¨¡å‹ï¼Œä½†éšç€[GPT-4o-mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)çš„å‘å¸ƒï¼Œæˆ‘æ”¹ä¸ºç”Ÿæˆé€‚åˆå¾®è°ƒ*èŠå¤©*æ¨¡å‹çš„æç¤ºï¼ˆæœ‰å…³å¯ç”¨æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[æ­¤å¤„](https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned)ï¼‰ã€‚
- en: Each prompt has the form â€¦
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæç¤ºçš„æ ¼å¼æ˜¯â€¦
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: The above has been formatted for clarity, but JSONL will have everything
    in one line per record.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šä¸Šè¿°å†…å®¹å·²æ ¼å¼åŒ–ä»¥ä¾¿æ¸…æ™°é˜…è¯»ï¼Œä½† JSONL ä¸­æ¯æ¡è®°å½•ä¼šåœ¨ä¸€è¡Œå†…æ˜¾ç¤ºã€‚
- en: Using the data excerpts, LLM_generated table description, column name we collated,
    we can now generate prompts which look like this â€¦
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨æ•°æ®æ‘˜å½•ã€LLM ç”Ÿæˆçš„è¡¨æ ¼æè¿°ä»¥åŠæˆ‘ä»¬æ”¶é›†çš„åˆ—åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç”Ÿæˆç±»ä¼¼äºè¿™æ ·çš„æç¤ºâ€¦
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Fine-tuning GPT-4o-mini
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒ GPT-4o-mini
- en: We now have test and training files in the right format for fine-tuning an OpenAI
    chat model, so letâ€™s tune our model â€¦
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†é€‚åˆå¾®è°ƒ OpenAI èŠå¤©æ¨¡å‹çš„æµ‹è¯•å’Œè®­ç»ƒæ–‡ä»¶æ ¼å¼ï¼Œå› æ­¤è®©æˆ‘ä»¬å¼€å§‹å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹â€¦
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the above we are using the new [GPT-4-mini model](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/),
    which from OpenAI is currently free to fine-tune â€¦
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ–°çš„[GPT-4-mini æ¨¡å‹](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)ï¼Œè¯¥æ¨¡å‹ç›®å‰ç”±
    OpenAI æä¾›å…è´¹å¾®è°ƒæœåŠ¡â€¦
- en: â€œNow through September 23, GPT-4o mini is free to fine-tune up to a daily limit
    of 2M training tokens. Overages over 2M training tokens will be charged at $3.00/1M
    tokens. Starting September 24, fine-tuning training will cost $3.00/1M tokens.
    Check out the [fine-tuning docs](http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D)
    for more details on free access.â€
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: â€œç°åœ¨åˆ°9æœˆ23æ—¥ï¼ŒGPT-4o miniå¯ä»¥å…è´¹è°ƒä¼˜ï¼Œæœ€å¤šè¾¾åˆ°æ¯æ—¥2Mè®­ç»ƒä»¤ç‰Œçš„é™åˆ¶ã€‚è¶…è¿‡2Mè®­ç»ƒä»¤ç‰Œçš„éƒ¨åˆ†å°†æŒ‰$3.00/ç™¾ä¸‡ä»¤ç‰Œæ”¶è´¹ã€‚ä»9æœˆ24æ—¥èµ·ï¼Œè°ƒä¼˜è®­ç»ƒå°†æ”¶è´¹$3.00/ç™¾ä¸‡ä»¤ç‰Œã€‚æŸ¥çœ‹[è°ƒä¼˜æ–‡æ¡£](http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D)ä»¥è·å–æ›´å¤šæœ‰å…³å…è´¹è®¿é—®çš„è¯¦ç»†ä¿¡æ¯ã€‚â€
- en: Even at $3.00/1 Million tokens, the costs are quite low for this task, coming
    out at about $7 a fine-tuning run for just over 2 million tokens in the test file.
    Bearing in mind, fine-tuning should be a rare event for this particular task,
    once we have such a model it can be reused.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿æŒ‰$3.00/ç™¾ä¸‡ä¸ªä»¤ç‰Œè®¡ç®—ï¼Œå¯¹äºè¿™ä¸ªä»»åŠ¡æ¥è¯´ï¼Œæˆæœ¬ä¹Ÿç›¸å½“ä½ï¼Œæ•´ä¸ªè°ƒä¼˜è¿‡ç¨‹å¤§çº¦éœ€è¦7ç¾å…ƒï¼Œæµ‹è¯•æ–‡ä»¶ä¸­æœ‰è¶…è¿‡200ä¸‡ä¸ªä»¤ç‰Œã€‚éœ€è¦è®°ä½çš„æ˜¯ï¼Œå¯¹äºè¿™ä¸ªç‰¹å®šä»»åŠ¡ï¼Œè°ƒä¼˜åº”è¯¥æ˜¯ä¸€ä¸ªå°‘è§çš„äº‹ä»¶ï¼Œä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰è¿™æ ·çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥è¢«é‡å¤ä½¿ç”¨ã€‚
- en: The fine-tuning produces the following output â€¦
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒä¼˜äº§ç”Ÿäº†ä»¥ä¸‹è¾“å‡ºâ€¦â€¦
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It took about 45 minutes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ±äº†å¤§çº¦45åˆ†é’Ÿã€‚
- en: Testing our fine-tuned model to predict HXL
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹è¯•æˆ‘ä»¬è°ƒä¼˜åçš„æ¨¡å‹ä»¥é¢„æµ‹HXL
- en: Now that we have a nice new shiny fine-tuned model for predicting HXL tags and
    attributes, we can use the test file to take it for a spin â€¦
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç²¾å¿ƒè°ƒä¼˜çš„æ–°æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹HXLæ ‡ç­¾å’Œå±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æµ‹è¯•æ–‡ä»¶æ¥è¿›è¡Œæµ‹è¯•â€¦â€¦
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Noting in the above that all predictions are filtered for allowed tags and attributes
    as defined in the HXL standard.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰é¢„æµ‹éƒ½ç»è¿‡äº†HXLæ ‡å‡†ä¸­å®šä¹‰çš„å…è®¸æ ‡ç­¾å’Œå±æ€§çš„ç­›é€‰ã€‚
- en: This gives the following results â€¦
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº†ä»¥ä¸‹ç»“æœâ€¦â€¦
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'â€˜Just HXL Tagsâ€™ means predicting the first part of the HXL, for example if
    the full HXL is #affected+infected+f, the model correctly got the #affected part
    correct. â€˜Tags and attributesâ€™ means predicting the full HXL string, ie â€˜#affected+infected+fâ€™,
    a much harder challenge due to all the combinations possible.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: â€œä»…HXLæ ‡ç­¾â€æ˜¯æŒ‡é¢„æµ‹HXLçš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œå¦‚æœå®Œæ•´çš„HXLæ˜¯#affected+infected+fï¼Œæ¨¡å‹æ­£ç¡®é¢„æµ‹äº†#affectedéƒ¨åˆ†ã€‚â€œæ ‡ç­¾å’Œå±æ€§â€æ˜¯æŒ‡é¢„æµ‹å®Œæ•´çš„HXLå­—ç¬¦ä¸²ï¼Œå³â€˜#affected+infected+fâ€™ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨è®¸å¤šå¯èƒ½çš„ç»„åˆã€‚
- en: The performance isnâ€™t perfect, but not that bad, especially as we have balanced
    the dataset to reduce the number of location and date tags and attributes (ie
    made this study a bit more challenging). There are tens of thousands of humanitarian
    response tables without HDX, even the above performance would likely add value.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æ€§èƒ½å¹¶ä¸å®Œç¾ï¼Œä½†ä¹Ÿä¸ç®—å¤ªå·®ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬å¹³è¡¡äº†æ•°æ®é›†ï¼Œå‡å°‘äº†ä½ç½®å’Œæ—¥æœŸæ ‡ç­¾åŠå±æ€§çš„æ•°é‡ï¼ˆå³è®©è¿™ä¸ªç ”ç©¶ç¨å¾®æ›´å…·æŒ‘æˆ˜æ€§ï¼‰ã€‚å³ä½¿å¦‚æ­¤ï¼Œä»ç„¶æœ‰æˆåƒä¸Šä¸‡çš„äººé“ä¸»ä¹‰å“åº”è¡¨æ ¼æ²¡æœ‰HDXï¼Œå³ä½¿æ˜¯ä¸Šè¿°æ€§èƒ½ä¹Ÿå¯èƒ½å¸¦æ¥ä»·å€¼ã€‚
- en: Letâ€™s look into cases where predictions didnâ€™t agree with human-labeled data
    â€¦
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹é¢„æµ‹ä¸äººå·¥æ ‡æ³¨æ•°æ®ä¸ä¸€è‡´çš„æ¡ˆä¾‹â€¦â€¦
- en: Reviewing the Human Labeled HXL Data
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®¡æŸ¥äººç±»æ ‡æ³¨çš„HXLæ•°æ®
- en: The predictions were saved to a spreadsheet, and I manually went through most
    of the predictions that didnâ€™t agree with the labels. You can find this analysis
    [here](https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&ouid=107814789436940136200&rtpof=true&sd=true)
    and summarized below â€¦
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°ç”µå­è¡¨æ ¼ä¸­ï¼Œæˆ‘æ‰‹åŠ¨æŸ¥çœ‹äº†å¤§å¤šæ•°ä¸æ ‡ç­¾ä¸ä¸€è‡´çš„é¢„æµ‹ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&ouid=107814789436940136200&rtpof=true&sd=true)æ‰¾åˆ°è¿™é¡¹åˆ†æï¼Œå¹¶åœ¨ä¸‹æ–‡è¿›è¡Œæ€»ç»“â€¦â€¦
- en: '![](../Images/dfbd71d99ab9103a6ad4e7e1673cdcbb.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbd71d99ab9103a6ad4e7e1673cdcbb.png)'
- en: 'Whatâ€™s interesting is that in some cases the LLM is actually correct, for example
    in adding *additional* HXL attributes which the human labeled data doesnâ€™t include.
    There are also cases where the human labeled HXL was perfectly reasonable, but
    the LLM predicted another tag or attribute that could also be interpreted as correct.
    For example a #region can also be an #admin1 in some countries, and whether something
    is an +id or +code is sometimes difficult to decide, both are appropriate.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒLLMå®é™…ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚åœ¨æ·»åŠ *é¢å¤–çš„*HXLå±æ€§æ—¶ï¼Œè€Œè¿™äº›å±æ€§åœ¨äººå·¥æ ‡æ³¨çš„æ•°æ®ä¸­æ²¡æœ‰åŒ…å«ã€‚ä¹Ÿæœ‰ä¸€äº›æƒ…å†µä¸‹ï¼Œäººå·¥æ ‡æ³¨çš„HXLå®Œå…¨åˆç†ï¼Œä½†LLMé¢„æµ‹äº†å¦ä¸€ä¸ªæ ‡ç­¾æˆ–å±æ€§ï¼Œè¿™ä¸ªæ ‡ç­¾æˆ–å±æ€§ä¹Ÿå¯ä»¥è¢«è§£é‡Šä¸ºæ­£ç¡®ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›å›½å®¶ï¼Œ#regionä¹Ÿå¯ä»¥æ˜¯#admin1ï¼Œè€ŒæŸäº›æƒ…å†µä¸‹åˆ¤æ–­ä¸€ä¸ªæ˜¯+idè¿˜æ˜¯+codeä¹Ÿå¾ˆéš¾å†³å®šï¼Œä¸¤è€…éƒ½æ˜¯åˆé€‚çš„ã€‚
- en: Using the above categories, I created a new test set where the expected HXL
    tags were corrected. On re-running the prediction we get improved results â€¦
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸Šè¿°ç±»åˆ«ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›†ï¼Œå…¶ä¸­çº æ­£äº†æœŸæœ›çš„HXLæ ‡ç­¾ã€‚åœ¨é‡æ–°è¿è¡Œé¢„æµ‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ”¹è¿›çš„ç»“æœâ€¦â€¦
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Predicting HXL without Fine-tuning, instead only prompting GPT-4o
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨æ²¡æœ‰å¾®è°ƒçš„æƒ…å†µä¸‹é¢„æµ‹HXLï¼Œè€Œä»…ä»…æ˜¯é€šè¿‡æç¤ºæ¥ä½¿ç”¨GPT-4o
- en: The above shows that the human-labeled data itself can be incorrect. The HXL
    standard is designed excellently, but can be a challenge to memorize for developers
    and data scientists when setting HXL tags and attributes on data. There are some
    [amazing tools](https://hxlstandard.org/tools/) already provided by the HXL team,
    but sometimes the HXL is still incorrect. This introduces a problem to the fine-tuning
    approach which relies on this human-labeled data for training, especially for
    less well represented tags and attributes that humans are not using very often.
    It also has the limitation of not being able to adjust when the metadata standard
    changes, since the training data would not reflect those changes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°å†…å®¹è¡¨æ˜äººç±»æ ‡æ³¨çš„æ•°æ®æœ¬èº«å¯èƒ½æ˜¯é”™è¯¯çš„ã€‚HXLæ ‡å‡†è®¾è®¡å¾—éå¸¸å‡ºè‰²ï¼Œä½†å¯¹äºå¼€å‘äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶æ¥è¯´ï¼Œåœ¨æ•°æ®ä¸Šè®¾ç½®HXLæ ‡ç­¾å’Œå±æ€§æ—¶ï¼Œè®°ä½å®ƒä»¬å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚HXLå›¢é˜Ÿå·²ç»æä¾›äº†ä¸€äº›[ä»¤äººæƒŠå¹çš„å·¥å…·](https://hxlstandard.org/tools/)ï¼Œä½†æœ‰æ—¶HXLä»ç„¶æ˜¯é”™è¯¯çš„ã€‚è¿™ç»™ä¾èµ–è¿™äº›äººç±»æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒçš„å¾®è°ƒæ–¹æ³•å¸¦æ¥äº†é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯¹äºé‚£äº›äººç±»ä¸å¸¸ä½¿ç”¨çš„æ ‡ç­¾å’Œå±æ€§ï¼Œè¿™äº›æ ‡ç­¾å’Œå±æ€§çš„è¡¨ç¤ºè¾ƒå°‘ã€‚å®ƒè¿˜å­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œå³æ— æ³•åœ¨å…ƒæ•°æ®æ ‡å‡†å‘ç”Ÿå˜åŒ–æ—¶è¿›è¡Œè°ƒæ•´ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸ä¼šåæ˜ è¿™äº›å˜åŒ–ã€‚
- en: Since the initial analysis 18 months ago various LLM providers have advanced
    their models significantly. OpenAI of course released [GPT-4o](https://openai.com/index/hello-gpt-4o/)
    as their flagship product, which importantly has a context window of 128k tokens
    and is another data point suggesting costs of foundational models are decreasing
    (see for example GPT-4-Turbo compared to GPT-4o [here](https://huggingface.co/spaces/philschmid/llm-pricing)).
    Given these factors, I wondered â€¦
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª18ä¸ªæœˆå‰çš„åˆæ­¥åˆ†æä»¥æ¥ï¼Œå„ä¸ªLLMæä¾›å•†å·²ç»æ˜¾è‘—æé«˜äº†ä»–ä»¬çš„æ¨¡å‹ã€‚OpenAIå½“ç„¶å‘å¸ƒäº†ä»–ä»¬çš„æ——èˆ°äº§å“[GPT-4o](https://openai.com/index/hello-gpt-4o/)ï¼Œå…¶å…·æœ‰128kä¸ªtokençš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå¦å¤–ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹çš„æˆæœ¬æ­£åœ¨ä¸‹é™ï¼ˆä¾‹å¦‚ï¼ŒGPT-4-Turboä¸GPT-4oçš„æ¯”è¾ƒ[è§æ­¤](https://huggingface.co/spaces/philschmid/llm-pricing)ï¼‰ã€‚è€ƒè™‘åˆ°è¿™äº›å› ç´ ï¼Œæˆ‘å¼€å§‹æ€è€ƒâ€¦â€¦
- en: '***If models are becoming more powerful and less expensive to use, could we
    avoid fine-tuning altogether and use them to predict HXL tags and attributes by
    prompting alone?***'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¦‚æœæ¨¡å‹å˜å¾—æ›´å¼ºå¤§ä¸”ä½¿ç”¨æˆæœ¬æ›´ä½ï¼Œæˆ‘ä»¬æ˜¯å¦å¯ä»¥å®Œå…¨é¿å…å¾®è°ƒï¼Œä»…é€šè¿‡æç¤ºå°±èƒ½é¢„æµ‹HXLæ ‡ç­¾å’Œå±æ€§ï¼Ÿ***'
- en: Not only could this mean less engineering work to clean data and fine-tune models,
    it may have a big advantage in being able to include HXL tags and attributes which
    are not included in the human-labeled training data but are part of the HXL standard.
    This is one potentially huge advantage of powerful LLMs, being able to classify
    with zero- and few-shot prompting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä»…æ„å‘³ç€å‡å°‘æ¸…ç†æ•°æ®å’Œå¾®è°ƒæ¨¡å‹çš„å·¥ç¨‹å·¥ä½œï¼Œè¿˜å¯èƒ½å…·æœ‰ä¸€ä¸ªå·¨å¤§ä¼˜åŠ¿ï¼Œå³èƒ½å¤ŸåŒ…æ‹¬äººç±»æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«ä½†å±äºHXLæ ‡å‡†çš„HXLæ ‡ç­¾å’Œå±æ€§ã€‚è¿™æ˜¯å¼ºå¤§LLMçš„ä¸€ä¸ªæ½œåœ¨å·¨å¤§ä¼˜åŠ¿ï¼Œå¯ä»¥é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºè¿›è¡Œåˆ†ç±»ã€‚
- en: Creating a prompt for predicting HXL
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºé¢„æµ‹HXLåˆ›å»ºæç¤º
- en: Models like GPT-4o are trained on web data, so I thought Iâ€™d first do a test
    using one of our prompts to see if it already knew everything there was to know
    about HXL tags â€¦
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åƒGPT-4oè¿™æ ·çš„æ¨¡å‹æ˜¯åŸºäºç½‘é¡µæ•°æ®è®­ç»ƒçš„ï¼Œæ‰€ä»¥æˆ‘æƒ³å…ˆåšä¸€ä¸ªæµ‹è¯•ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æç¤ºä¹‹ä¸€æ¥çœ‹çœ‹å®ƒæ˜¯å¦å·²ç»çŸ¥é“å…³äºHXLæ ‡ç­¾çš„æ‰€æœ‰ä¿¡æ¯â€¦
- en: '![](../Images/b39b540c5990d5719eae62d8834191e4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b39b540c5990d5719eae62d8834191e4.png)'
- en: What we see is that it seems to know about HXL syntax, but the answer is incorrect
    (the correct answer is â€˜#affected+infectedâ€™), and it has chosen tags and attributes
    that are not in the HXL standard. Itâ€™s actually similar to what we see with human-tagged
    HXL.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°çš„æ˜¯ï¼Œå®ƒä¼¼ä¹çŸ¥é“HXLçš„è¯­æ³•ï¼Œä½†ç­”æ¡ˆä¸æ­£ç¡®ï¼ˆæ­£ç¡®ç­”æ¡ˆæ˜¯â€˜#affected+infectedâ€™ï¼‰ï¼Œå¹¶ä¸”é€‰æ‹©äº†ä¸åœ¨HXLæ ‡å‡†ä¸­çš„æ ‡ç­¾å’Œå±æ€§ã€‚è¿™å®é™…ä¸Šç±»ä¼¼äºæˆ‘ä»¬åœ¨äººç±»æ ‡æ³¨çš„HXLä¸­çœ‹åˆ°çš„æƒ…å†µã€‚
- en: How about we provide the most important parts of the [HXL standard](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&gid=319251406#gid=319251406)
    in the system prompt?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›[HXLæ ‡å‡†](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&gid=319251406#gid=319251406)çš„æœ€é‡è¦éƒ¨åˆ†æ€ä¹ˆæ ·ï¼Ÿ
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This gives us a prompt like this â€¦
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™äº†æˆ‘ä»¬å¦‚ä¸‹çš„æç¤ºâ€¦
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Itâ€™s pretty long (the above has been truncated), but encapsulates the HXL standard.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒç›¸å½“é•¿ï¼ˆä¸Šé¢å·²è¢«æˆªæ–­ï¼‰ï¼Œä½†åŒ…å«äº†HXLæ ‡å‡†çš„è¦ç‚¹ã€‚
- en: Another advantage of the direct prompt method is that we can also ask for the
    LLM to provide its reasoning when predicting HXL. This can of course include hallucination,
    but Iâ€™ve always found it useful for refining prompts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥æç¤ºæ–¹æ³•çš„å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¦æ±‚LLMåœ¨é¢„æµ‹HXLæ—¶æä¾›å…¶æ¨ç†è¿‡ç¨‹ã€‚å½“ç„¶ï¼Œè¿™å¯èƒ½åŒ…æ‹¬å¹»è§‰ï¼Œä½†æˆ‘å‘ç°å®ƒå¯¹äºä¼˜åŒ–æç¤ºéå¸¸æœ‰å¸®åŠ©ã€‚
- en: For the user prompt, we will use the same information that we used for fine-tuning,
    to include excerpt and LLM-generated table summary â€¦
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç”¨æˆ·æç¤ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸å¾®è°ƒæ—¶ç›¸åŒçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‘˜å½•å’ŒLLMç”Ÿæˆçš„è¡¨æ ¼æ€»ç»“â€¦
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Putting it all together, and prompting both GPT-4o-mini and GPT-4o for comparison
    â€¦
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹ç»¼åˆèµ·æ¥ï¼Œå¹¶åŒæ—¶å¯¹æ¯”GPT-4o-miniå’ŒGPT-4oçš„æç¤ºç»“æœâ€¦
- en: '[PRE10]json","").replace("[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]json","").replace("[PRE11]'
- en: We get â€¦
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°â€¦
- en: '[PRE12]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As a reminder, the fine-tuned model produced the following results â€¦
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æé†’ä¸€ä¸‹ï¼Œå¾®è°ƒåçš„æ¨¡å‹äº§ç”Ÿäº†ä»¥ä¸‹ç»“æœâ€¦
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*How does prompting-only GPT-4o compare with GPT-4o-mini?*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»…ä½¿ç”¨æç¤ºçš„GPT-4oä¸GPT-4o-miniç›¸æ¯”å¦‚ä½•ï¼Ÿ*'
- en: Looking at the above, we see that GPT-4o-mini prompting-only predicts just tags
    with 77% accuracy, which is less than GPT-4o-mini fine-tuning (83%) and GPT-4o
    prompting-only (86%). That said the performance is still good and would improve
    HXL coverage even if used as-is.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šé¢çš„æ•°æ®æ¥çœ‹ï¼Œæˆ‘ä»¬å‘ç°GPT-4o-miniä»…ä½¿ç”¨æç¤ºé¢„æµ‹æ ‡ç­¾çš„å‡†ç¡®ç‡ä¸º77%ï¼Œä½äºGPT-4o-miniå¾®è°ƒåçš„83%å’ŒGPT-4oä»…ä½¿ç”¨æç¤ºçš„86%ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ€§èƒ½ä»ç„¶ä¸é”™ï¼Œå³ä¾¿ç›´æ¥ä½¿ç”¨ä¹Ÿèƒ½æ”¹å–„HXLè¦†ç›–ç‡ã€‚
- en: '*How does prompting-only compare with the fine-tuned model?*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»…ä½¿ç”¨æç¤ºä¸å¾®è°ƒæ¨¡å‹çš„å¯¹æ¯”å¦‚ä½•ï¼Ÿ*'
- en: GPT-4o prompting-only gave the best results of all models, with 86% accuracy
    on tags and 71% on tags and attributes. In fact, the performance could well be
    better after a bit more analysis of the test data to correct incorrect human-labeled
    tags,.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4oä»…ä½¿ç”¨æç¤ºçš„ç»“æœæ˜¯æ‰€æœ‰æ¨¡å‹ä¸­æœ€å¥½çš„ï¼Œåœ¨æ ‡ç­¾ä¸Šçš„å‡†ç¡®ç‡ä¸º86%ï¼Œåœ¨æ ‡ç­¾å’Œå±æ€§ä¸Šçš„å‡†ç¡®ç‡ä¸º71%ã€‚å®é™…ä¸Šï¼Œç»è¿‡æ›´å¤šå¯¹æµ‹è¯•æ•°æ®çš„åˆ†æä»¥çº æ­£é”™è¯¯çš„äººç±»æ ‡ç­¾åï¼Œæ€§èƒ½å¯èƒ½ä¼šæ›´å¥½ã€‚
- en: Letâ€™s take a closer look at the times GPT-4o got it wrong â€¦
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹GPT-4oå‡ºé”™çš„æƒ…å†µâ€¦
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice how we now have a â€˜Reasoningâ€™ field to indicate why the tags were chosen.
    This is useful and would be an important part for refining the prompt to improve
    performance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªâ€œæ¨ç†â€å­—æ®µï¼Œç”¨æ¥è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æ ‡ç­¾ã€‚è¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯¹äºä¼˜åŒ–æç¤ºä»¥æé«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚
- en: Looking at the sample above, we see some familiar scenarios that were found
    when analyzing the fine-tuned model failed predictions â€¦
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸Šé¢çš„ç¤ºä¾‹æ¥çœ‹ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›ç†Ÿæ‚‰çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å‡ºç°åœ¨åˆ†æå¾®è°ƒæ¨¡å‹å¤±è´¥çš„é¢„æµ‹æ—¶â€¦
- en: +id and +code ambiguity
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: +idå’Œ+codeçš„æ¨¡ç³Šæ€§
- en: '#region and #adm1 used interchangeably'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#regionå’Œ#adm1äº’æ¢ä½¿ç”¨'
- en: '#event versus more detailed tags like #cause'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#eventä¸æ›´è¯¦ç»†çš„æ ‡ç­¾å¦‚#causeçš„å¯¹æ¯”'
- en: These seem to fall into the category where two tags are possible for a given
    column given their HXL definition. But there are some real discrepancies which
    would need more investigation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¼¼ä¹å±äºé‚£ç§æ ¹æ®HXLå®šä¹‰ï¼Œç»™å®šåˆ—å¯èƒ½æœ‰ä¸¤ä¸ªæ ‡ç­¾çš„ç±»åˆ«ã€‚ä½†ä¹Ÿæœ‰ä¸€äº›æ˜æ˜¾çš„ä¸ä¸€è‡´ä¹‹å¤„ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚
- en: That said, using GPT-4o to predict HXL tags and attributes yields the best results,
    and I believe at an acceptable level given a lot of data is missing HXL metadata
    altogether and many of the datasets which have it have incorrect tags and attributes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œä½¿ç”¨GPT-4oé¢„æµ‹HXLæ ‡ç­¾å’Œå±æ€§å¾—å‡ºäº†æœ€å¥½çš„ç»“æœï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯åœ¨å¯æ¥å—çš„æ°´å¹³ï¼Œå› ä¸ºå¾ˆå¤šæ•°æ®å®Œå…¨ç¼ºå¤±HXLå…ƒæ•°æ®ï¼Œä¸”è®¸å¤šåŒ…å«è¿™äº›æ•°æ®çš„é›†åˆæœ‰é”™è¯¯çš„æ ‡ç­¾å’Œå±æ€§ã€‚
- en: Cost Comparison
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆæœ¬æ¯”è¾ƒ
- en: Letâ€™s see how costs compare with each technique and model â€¦
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ¯ç§æŠ€æœ¯å’Œæ¨¡å‹çš„æˆæœ¬æ¯”è¾ƒâ€¦â€¦
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Which gives â€¦
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯â€¦â€¦
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note: the above is only for the inference cost, there will be a very small
    additional cost in generating table data summaries with GPT-3.5.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨ï¼šä¸Šè¿°ä»…ä¸ºæ¨ç†æˆæœ¬ï¼Œç”Ÿæˆè¡¨æ ¼æ•°æ®æ‘˜è¦æ—¶ä½¿ç”¨ GPT-3.5 å¯èƒ½ä¼šæœ‰éå¸¸å°çš„é¢å¤–è´¹ç”¨ã€‚
- en: Given the test set, predicting HXL for **458 columns** â€¦
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæµ‹è¯•é›†ï¼Œé¢„æµ‹ **458 åˆ—** çš„ HXL æ ‡ç­¾â€¦â€¦
- en: '**Fine-tuning**:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒ**ï¼š'
- en: As expected, inference costs for the fine-tuned GPT-4o mini model (which cost
    about $7 to fine-tune) are very low about $0.02.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¾®è°ƒåçš„ GPT-4o mini æ¨¡å‹ï¼ˆå¾®è°ƒèŠ±è´¹å¤§çº¦ $7ï¼‰æ¨ç†æˆæœ¬éå¸¸ä½ï¼Œçº¦ä¸º $0.02ã€‚
- en: '**Prediction-only**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»…é¢„æµ‹**ï¼š'
- en: GPT-4o prediction only is expensive, because of the HXL standard being passed
    in to the system prompt every time, and comes out at $13.44.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»…ä½¿ç”¨ GPT-4o è¿›è¡Œé¢„æµ‹çš„æˆæœ¬è¾ƒé«˜ï¼Œå› ä¸ºæ¯æ¬¡éƒ½éœ€è¦å°† HXL æ ‡å‡†ä¼ é€’ç»™ç³»ç»Ÿæç¤ºï¼Œè´¹ç”¨ä¸º $13.44ã€‚
- en: GPT-4o-mini, albeit with reduced performance, is a more reasonable $0.40.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4o-mini å°½ç®¡æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œä½†æ¯æ¬¡è°ƒç”¨çš„è´¹ç”¨æ›´ä¸ºåˆç†ï¼Œä¸º $0.40ã€‚
- en: So ease of use comes with a cost if using GPT-4o, but GPT-4o-mini is an attractive
    alternative.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½¿ç”¨ GPT-4o çš„æ˜“ç”¨æ€§æ˜¯æœ‰ä»£ä»·çš„ï¼Œä½† GPT-4o-mini æ˜¯ä¸€ä¸ªå…·æœ‰å¸å¼•åŠ›çš„æ›¿ä»£é€‰æ‹©ã€‚
- en: Finally, itâ€™s worth noting that in many cases, setting HXL tags might not to
    be real time, for example for a crawler process that corrects already uploaded
    datasets. This would mean that the new [OpenAI batch API](https://platform.openai.com/docs/guides/batch/overview)
    could be used, reducing costs by 50%.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè®¾ç½® HXL æ ‡ç­¾å¯èƒ½ä¸æ˜¯å®æ—¶çš„ï¼Œä¾‹å¦‚å¯¹äºä¿®æ­£å·²ä¸Šä¼ æ•°æ®é›†çš„çˆ¬è™«è¿›ç¨‹ã€‚è¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨æ–°çš„[OpenAI æ‰¹é‡ API](https://platform.openai.com/docs/guides/batch/overview)ï¼Œä»è€Œå°†æˆæœ¬é™ä½
    50%ã€‚
- en: A Python class for predicting HXL Tags
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨äºé¢„æµ‹ HXL æ ‡ç­¾çš„ Python ç±»
- en: Putting this all together, I created a Github gist [hxl_utils.py](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014).
    Check this out from GitHub and place the file in your current working directory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™äº›å†…å®¹ç»“åˆåœ¨ä¸€èµ·ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ª Github gist [hxl_utils.py](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)ã€‚å¯ä»¥ä»
    GitHub ä¸‹è½½å¹¶å°†æ–‡ä»¶æ”¾å…¥å½“å‰å·¥ä½œç›®å½•ä¸­ã€‚
- en: Letâ€™s download a file to test it with â€¦
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶æ¥æµ‹è¯•å®ƒâ€¦â€¦
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/f4d88e404896f574504c6d75b455d9cb.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4d88e404896f574504c6d75b455d9cb.png)'
- en: And using this dataframe, letâ€™s predict HXL tags â€¦
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªæ•°æ®æ¡†ï¼Œæˆ‘ä»¬æ¥é¢„æµ‹ HXL æ ‡ç­¾â€¦â€¦
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/33bafdfd979641e3cc932f24d4992577.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33bafdfd979641e3cc932f24d4992577.png)'
- en: And there we have it, some lovely HXL tags!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼Œå¾—åˆ°äº†äº›æ¼‚äº®çš„ HXL æ ‡ç­¾ï¼
- en: Letâ€™s see how well GPT-4o-mini does â€¦
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ GPT-4o-mini è¡¨ç°å¦‚ä½•â€¦â€¦
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Which gives â€¦
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯â€¦â€¦
- en: '![](../Images/551db548c232ea8e6d577b657dee9021.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/551db548c232ea8e6d577b657dee9021.png)'
- en: Pretty good! gpt-4o gave â€œ#affected+killed+numâ€ for the last column, where â€œgpt-4o-miniâ€
    gave â€œ#affected+numâ€, but this could likely be resolved with some deft prompt
    engineering.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆä¸é”™ï¼gpt-4o ç»™å‡ºäº†â€œ#affected+killed+numâ€ä½œä¸ºæœ€åä¸€åˆ—ï¼Œè€Œâ€œgpt-4o-miniâ€åˆ™ç»™å‡ºäº†â€œ#affected+numâ€ï¼Œä½†è¿™å¾ˆå¯èƒ½å¯ä»¥é€šè¿‡ä¸€äº›å·§å¦™çš„æç¤ºå·¥ç¨‹æ¥è§£å†³ã€‚
- en: Admittedly this wasnâ€™t a terribly challenging dataset, but it was able to correctly
    predict tags for events and fatalities, which are less frequent than location
    and dates.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ç‡åœ°è¯´ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä½†å®ƒèƒ½å¤Ÿæ­£ç¡®é¢„æµ‹äº‹ä»¶å’Œæ­»äº¡çš„æ ‡ç­¾ï¼Œè€Œè¿™äº›æ ‡ç­¾æ¯”åœ°ç‚¹å’Œæ—¥æœŸè¦å°‘è§ã€‚
- en: Future Work
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœªæ¥å·¥ä½œ
- en: I think a big takeaway here is that the direct-prompting technique produces
    good results without the need for training. Yes, more expensive for inference,
    but maybe not if a data scientist is required to curate incorrectly human-labeled
    fine-tuning data. It would depend on the organization and metadata use-case.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºä¸€ä¸ªé‡è¦çš„æ”¶è·æ˜¯ï¼Œç›´æ¥æç¤ºæŠ€æœ¯èƒ½å¤Ÿåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å–å¾—ä¸é”™çš„ç»“æœã€‚æ˜¯çš„ï¼Œæ¨ç†æˆæœ¬æ›´é«˜ï¼Œä½†å¦‚æœéœ€è¦æ•°æ®ç§‘å­¦å®¶æ•´ç†é”™è¯¯æ ‡æ³¨çš„å¾®è°ƒæ•°æ®ï¼Œä¹Ÿè®¸å¹¶ä¸é‚£ä¹ˆè´µã€‚è¿™å°†å–å†³äºç»„ç»‡å’Œå…ƒæ•°æ®çš„ä½¿ç”¨åœºæ™¯ã€‚
- en: Here are some areas that might be considered in future work â€¦
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€äº›å¯èƒ½åœ¨æœªæ¥å·¥ä½œä¸­è€ƒè™‘çš„é¢†åŸŸâ€¦â€¦
- en: '**Improved test data**'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›çš„æµ‹è¯•æ•°æ®**'
- en: This analysis did a quick review of the test set to correct HXL tags which were
    incorrect in the data or had multiple possible values. More time could be spent
    on this, as always in machine learning, ground truth is key.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ†æå¿«é€Ÿå®¡æŸ¥äº†æµ‹è¯•é›†ï¼Œä»¥ä¿®æ­£æ•°æ®ä¸­ä¸æ­£ç¡®çš„ HXL æ ‡ç­¾æˆ–å­˜åœ¨å¤šä¸ªå¯èƒ½å€¼çš„æ ‡ç­¾ã€‚å¯ä»¥åœ¨è¿™æ–¹é¢æŠ•å…¥æ›´å¤šæ—¶é—´ï¼Œæ­£å¦‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåœ°é¢çœŸç›¸æ˜¯å…³é”®ã€‚
- en: '**Prompt engineering and hyperparameter tuning**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**æç¤ºå·¥ç¨‹ä¸è¶…å‚æ•°è°ƒä¼˜**'
- en: The above analysis uses very basic prompts with no real engineering or strategies
    applied, these could definitely be improved for better performance. With an evaluation
    set and a framework such as [Promptflow](https://github.com/microsoft/promptflow),
    prompt variants could be tested. Additionally we might add more context data,
    for example in deciding administrative levels, which can vary per country. Finally,
    we have used fixed hyperparameters for temperature and top_p, as well as completion
    token length. All these could be tuned leading to better performance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°åˆ†æä½¿ç”¨äº†éå¸¸åŸºç¡€çš„æç¤ºè¯ï¼Œå¹¶æ²¡æœ‰åº”ç”¨ä»»ä½•çœŸæ­£çš„å·¥ç¨‹æ–¹æ³•æˆ–ç­–ç•¥ï¼Œè¿™äº›æ–¹æ³•è‚¯å®šå¯ä»¥é€šè¿‡æ”¹è¿›æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡è¯„ä¼°é›†å’Œåƒ[Promptflow](https://github.com/microsoft/promptflow)è¿™æ ·çš„æ¡†æ¶ï¼Œå¯ä»¥æµ‹è¯•ä¸åŒçš„æç¤ºè¯å˜ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ·»åŠ æ›´å¤šçš„ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¾‹å¦‚åœ¨å†³å®šè¡Œæ”¿çº§åˆ«æ—¶ï¼Œè¿™å¯èƒ½å› å›½å®¶è€Œå¼‚ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å›ºå®šçš„è¶…å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€top_pä»¥åŠå®Œæˆæ ‡è®°é•¿åº¦ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥è°ƒæ•´ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚
- en: '**Cost optimization**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆæœ¬ä¼˜åŒ–**'
- en: The prompting-only approach definitely appears to be a strong option and simplifies
    how an organization can automatically set HXL tags on their data using GPT-4o.
    There are of course cost implications with this model, being a more expensive,
    but predictions occur only on low-volume schema changes, not when the underlying
    data itself changes, and with new options for [batch submission](https://openai.com/api/pricing/)
    on OpenAI and ever decreasing LLM costs, this technique appears viable for many
    organizations. GPT-4o-mini also performs well and is a fraction of the cost.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…ä½¿ç”¨æç¤ºè¯çš„æ–¹æ³•æ— ç–‘æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é€‰æ‹©ï¼Œå¹¶ç®€åŒ–äº†ç»„ç»‡å¦‚ä½•é€šè¿‡GPT-4oè‡ªåŠ¨ä¸ºå…¶æ•°æ®è®¾ç½®HXLæ ‡ç­¾ã€‚å½“ç„¶ï¼Œè¿™ç§æ–¹æ³•æœ‰æˆæœ¬ä¸Šçš„è€ƒè™‘ï¼Œå› ä¸ºå®ƒè¾ƒä¸ºæ˜‚è´µï¼Œä½†é¢„æµ‹åªå‘ç”Ÿåœ¨ä½é¢‘ç‡çš„æ¨¡å¼å˜åŒ–æ—¶ï¼Œè€Œä¸æ˜¯å½“åº•å±‚æ•°æ®æœ¬èº«å‘ç”Ÿå˜åŒ–æ—¶ï¼Œéšç€OpenAIæä¾›æ–°çš„[æ‰¹é‡æäº¤](https://openai.com/api/pricing/)é€‰é¡¹ä»¥åŠLLMæˆæœ¬ä¸æ–­ä¸‹é™ï¼Œè¿™é¡¹æŠ€æœ¯å¯¹è®¸å¤šç»„ç»‡æ¥è¯´æ˜¯å¯è¡Œçš„ã€‚GPT-4o-miniçš„è¡¨ç°ä¹Ÿå¾ˆå¥½ï¼Œä¸”æˆæœ¬åªæ˜¯å…¶ä¸€å°éƒ¨åˆ†ã€‚
- en: '**Application to other metadata standards**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**åº”ç”¨äºå…¶ä»–å…ƒæ•°æ®æ ‡å‡†**'
- en: It would be interesting to apply this technique to other metadata and labeling
    standards, Iâ€™m sure many organizations are already using LLMs for this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™é¡¹æŠ€æœ¯åº”ç”¨äºå…¶ä»–å…ƒæ•°æ®å’Œæ ‡æ³¨æ ‡å‡†ä¼šå¾ˆæœ‰è¶£ï¼Œæˆ‘ç¡®ä¿¡è®¸å¤šç»„ç»‡å·²ç»åœ¨ä½¿ç”¨LLMsæ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: '*Please like this article if inclined and Iâ€™d be delighted if you followed
    me! You can find more articles* [*here*](https://medium.com/@astrobagel)*.*'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ æ„¿æ„ï¼Œè¯·ç‚¹èµè¿™ç¯‡æ–‡ç« ï¼Œå¦‚æœä½ å…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸é«˜å…´ï¼ä½ å¯ä»¥åœ¨* [*è¿™é‡Œ*](https://medium.com/@astrobagel)*
    æ‰¾åˆ°æ›´å¤šæ–‡ç« ã€‚*'
