- en: Enhancing NPS Measurement with LLMs and Statistical Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/enhancing-nps-measurement-with-llms-and-statistical-inference-a0ed0ce9c6cf?source=collection_archive---------6-----------------------#2024-03-06](https://towardsdatascience.com/enhancing-nps-measurement-with-llms-and-statistical-inference-a0ed0ce9c6cf?source=collection_archive---------6-----------------------#2024-03-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Combining LLMs with human judgement through Prediction-Powered Inference (PPI)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@smsmith714?source=post_page---byline--a0ed0ce9c6cf--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page---byline--a0ed0ce9c6cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a0ed0ce9c6cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a0ed0ce9c6cf--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page---byline--a0ed0ce9c6cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a0ed0ce9c6cf--------------------------------)
    ·8 min read·Mar 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/170d3aaa0bfc5e6f90d90cd4a4b39b43.png)'
  prefs: []
  type: TYPE_IMG
- en: Robot solving complicated mathematics, digital art. Generated by Dall-E 2.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In business analytics, calculating the Net Promoter Score (NPS) typically involves
    manual data annotation from employees. Some may think to use machine learning
    models to label the data, however this does not have the theoretical guarantees
    we get from human labeled data. Enter Prediction-Powered Inference (PPI), a new
    statistical technique that combines human and machine labeled data to create confidence
    intervals that are data efficient and theoretically guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article explores the intuition behind PPI and emphasizes why you would
    want to use it. We then jump into a code walkthrough of how to use it for two
    metrics: NPS and customer recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction-Powered Inference (PPI)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PPI is a statistical technique proposed by Angelopoulos et al. [1]. The goal
    is to enhance confidence intervals by combining human and machine labeled data.
    Let’s walk through some steps to motivate its usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: In our use case we want to estimate the true NPS score given a set of customer
    reviews. Typically, an employee will manually read each review and assign a score
    from 1 to 10, a reliable but time-inefficient method. When dealing with numerous
    reviews it would be convenient to have a more automatic method.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, we can leverage a machine learning model. A Large Language
    Model (LLM) is a good candidate to solve this problem because they generalize
    well to new tasks. The model is prompted to read the review and output a score.
    This is convenient, but the model comes with errors and imperfections. When making
    a decision, we need to make sure our data is aligned with human judgement.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the limitations of both approaches, what if we could combine them?
    We can with Prediction-Powered Inference (PPI)! PPI is a framework that leverages
    the theoretical guarantees of human-labeled data for confidence intervals and
    the efficiency of machine-labeled data. With PPI, we aim to benefit from the strengths
    of both techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How it Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of PPI falls on something called the rectifier. We use the rectifier
    to account for the prediction error of our machine learning model. Using the rectifier,
    we can construct confidence intervals combining both the human and machine labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the algorithm proposed for constructing the confidence intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2cd98a6b898eea3d7582fce12847fce.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithm 1 from [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach enjoys a simple code implementation. Here is a short snippet
    to make this work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you want to go deeper I recommend this [YouTube](https://www.youtube.com/watch?v=FW5l5xEYETY&t=515s)
    video with Clara Wong-Fannjiang (one of the authors) or looking at the [paper](https://arxiv.org/pdf/2301.09633.pdf).
    These resources do a much better job of explaining the concepts than I could accomplish
    here.
  prefs: []
  type: TYPE_NORMAL
- en: What’s important to understand is that PPI has tighter confidence intervals
    than human only construction and has theoretical guarantees absent in prediction
    based confidence intervals. Understanding this fact is enough to work through
    the coding exercise.
  prefs: []
  type: TYPE_NORMAL
- en: The Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The link to the full notebook can be found [here](https://colab.research.google.com/drive/1anlksDq2LI6Mshk7tO_yxFnSsOp-D366?usp=sharing).
    I’ll walk through the key steps with some commentary. A lot of this code is credited
    to the authors of the PPI library.
  prefs: []
  type: TYPE_NORMAL
- en: In our example we are going to use PPI to estimate the mean value. It is also
    possible to estimate other parameters, such as quantiles, logistic/linear regression
    coefficients, and others. After working through this example you can find more
    [here](https://github.com/aangelopoulos/ppi_py/tree/main/examples).
  prefs: []
  type: TYPE_NORMAL
- en: Setting things up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First let’s install the PPI library. To see more information about the library
    check out the repo [here](https://github.com/aangelopoulos/ppi_py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For this example we will simulate data. It is hard to find data for NPS scores
    that are publicly available, so I've created the following code. The approach
    is still the same regardless of where the data comes from. We will create a DataFrame
    with the columns`Overall_Rating` (NPS scores) and `Recommended` (a boolean value.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using these functions we can construct the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Simulating LLM Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make a more flexible demo I have given two options for creating predictions.
    The first is to create a simulated error for the predictions. This allows you
    to experiment with PPI and see how it works for different theoretical models.
    Here’s an example of what this looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: LLM Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have data with customer reviews, then it is easy to score them with
    a LLM. Here are some prompts you can use to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You may wish to use recommended over NPS since the boolean classification problem
    is much easier. Some businesses prefer NPS because it’s more industry standard.
    Depending on your problem you can choose which makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: With the LLM you can also check scores for different categories. These could
    be different products or services mentioned that you want to measure. This is
    a great benefit of using the LLM, since it is flexible for many problems, but
    we also account for the error by using PPI in our reporting.
  prefs: []
  type: TYPE_NORMAL
- en: Run PPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it’s time to construct confidence intervals. Here is a snippet of the code
    that does the heavy lifting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What we are doing here is simulating PPI vs classical confidence intervals for
    different numbers of human responses (n). We can plot this information which is
    shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6582044f5a3a85c482fd613fbb3f56e6.png)'
  prefs: []
  type: TYPE_IMG
- en: PPI vs Classical for NPS Score. Green is PPI, Grey is Classical, Yellow is Machine
    labeled, and the dashed line is the true population parameter. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see in this diagram comparing different values for n, the number of
    human labels used. PPI always has tighter confidence intervals than the confidence
    intervals using human labeled data alone. This demonstrates the key value of PPI:
    **even though we have a flawed ML model, we still generate better confidence intervals
    than if we hadn’t used it by combing them with human data.**'
  prefs: []
  type: TYPE_NORMAL
- en: We can see similar results for recommended below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb38f4ec241dd0df1e36f98562465db8.png)'
  prefs: []
  type: TYPE_IMG
- en: PPI vs Classical for Recommended. Green is PPI, Grey is Classical, Yellow is
    Machine labeled, and the dashed line is the true population parameter. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: In this case we are looking at the true percentage of customers that recommend
    using the business out of all customers. Again we see that using PPI we are able
    to build tighter confidence intervals than if we had only used the human labeled
    data.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll also notice the confidence interval for the machine predictions in yellow.
    These predictions are not perfectly accurate so the confidence interval is way
    off. This is why we need some human labeled data and cannot use machine labeled
    data only.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Making
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s consider how many human labels are needed to make a decision for the
    PPI and classical approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with NPS. Suppose we want to simulate how many human labeled examples
    we need to reject a null hypothesis that NPS is less than or equal to 4\. We can
    run the following code to find the minimum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This simulates the minimum number of examples needed to reject the null hypothesis
    for PPI. The code for the classical example follows similarly. See the [notebook](https://colab.research.google.com/drive/1anlksDq2LI6Mshk7tO_yxFnSsOp-D366?usp=sharing)
    for full details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The interpretation is that PPI requires 653 less human labeled observations
    to reject the null hypothesis than using only human labeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: We can repeat this process for recommended. The only change we make is the value
    of the null hypothesis. We run a test for the null hypothesis that the true percent
    of customers that recommend the business is less than or equal to 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here we can see that 539 more observations are needed to draw a conclusion with
    the classical approach than the human approach.
  prefs: []
  type: TYPE_NORMAL
- en: Are these results meaningful?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 653 or 539 observations may not seem like a lot, but in the world of internal
    data labeling it is. Suppose it’s Friday afternoon and your boss asks you to determine
    what the NPS score is from a group of surveys that just came in. To make this
    determination you need to manually label some observations.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you can label 4 comments per minute. This implies you can label 240
    comments per hour. If you use PPI, you would get to leave 2–3 hours earlier than
    if you used classical confidence intervals. Reducing mundane tasks has great benefits
    for employee happiness so this approach is worth investing in, since the overhead
    is simple.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was a quick overview of how to use PPI to solve basic statistical inference
    problems. We saw how to calculate a population mean from a sample dataset for
    two different types of variables. This approach results in a meaningful time savings
    for very little extra work.
  prefs: []
  type: TYPE_NORMAL
- en: For more examples of how to use PPI, check out this [examples](https://github.com/aangelopoulos/ppi_py/tree/main/examples)
    folder from the repo. They cover many more interesting use cases. Happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading the article! If you have additional questions or something
    was unclear, leave a comment and I will get back to you. If you want to see more
    articles like this one, please follow me on* [*Medium*](https://medium.com/@smsmith714)
    *and on* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you found a technical error in this article, please let me know ASAP! I
    strive to make sure the information I publish is as correct as possible, but no
    one is perfect.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Anastasios N. Angelopoulos, Stephen Bates, Clara Fannjiang, Michael I.
    Jordan, & Tĳana Zrnic. (2023). Prediction-Powered Inference.'
  prefs: []
  type: TYPE_NORMAL
