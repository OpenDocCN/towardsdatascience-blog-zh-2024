<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How To Log Databricks Workflows with the Elastic (ELK) Stack</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How To Log Databricks Workflows with the Elastic (ELK) Stack</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-log-databricks-workflows-with-the-elastic-elk-stack-a03f940cbc88?source=collection_archive---------7-----------------------#2024-07-30">https://towardsdatascience.com/how-to-log-databricks-workflows-with-the-elastic-elk-stack-a03f940cbc88?source=collection_archive---------7-----------------------#2024-07-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e909" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A practical example of setting up observability for a data pipeline using best practices from SWE world</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@yury-kalbaska?source=post_page---byline--a03f940cbc88--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Yury Kalbaska" class="l ep by dd de cx" src="../Images/d07ddfd82b958b22fba3cbda925d1cb0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u3e1lENYQjeUlxgJhBkSRg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a03f940cbc88--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@yury-kalbaska?source=post_page---byline--a03f940cbc88--------------------------------" rel="noopener follow">Yury Kalbaska</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a03f940cbc88--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/4152c7a9407ea80e056dfcd8ed7c1a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A6ewC8vjjsUxJfnA"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@thisisengineering?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">ThisisEngineering</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="1a44" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="d863" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">At the time of this writing (July 2024) Databricks has become a standard platform for data engineering in the cloud, this rise to prominence highlights the importance of features that support robust data operations (DataOps). Among these features, observability capabilities — logging, monitoring, and alerting — are essential for a mature and production-ready data engineering tool.</p><p id="607c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">There are many tools to log, monitor, and alert the Databricks workflows including built-in native Databricks Dashboards, Azure Monitor, DataDog among others.</p><p id="b8f7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">However, one common scenario that is not obviously covered by the above is the need to integrate with an existing enterprise monitoring and alerting stack rather than using the dedicated tools mentioned above. More often than not, this will be Elastic stack (aka ELK) — a de-facto standard for logging and monitoring in the software development world.</p><h2 id="7883" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Components of the ELK stack?</h2><p id="0f97" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">ELK stands for Elasticsearch, Logstash, and Kibana — three products from Elastic that offer end-to-end observability solution:</p><ol class=""><li id="25b7" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Elasticsearch — for log storage and retrieval</li><li id="f3cd" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Logstash — for log ingestion</li><li id="e5f3" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Kibana — for visualizations and alerting</li></ol><p id="cb96" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The following sections will present a practical example of how to integrate the ELK Stack with Databricks to achieve a robust end-to-end observability solution.</p><h1 id="befb" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">A practical example</h1><h2 id="39e4" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Prerequisites</h2><p id="29eb" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Before we move on to implementation, ensure the following is in place:</p><ol class=""><li id="971e" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk"><strong class="oa fr">Elastic cluster</strong> — A running Elastic cluster is required. For simpler use cases, this can be a single-node setup. However, one of the key advantages of the ELK is that it is fully distributed so in a larger organization you’ll probably deal with a cluster running in Kubernetes. Alternatively, an instance of Elastic Cloud can be used, which is equivalent for the purposes of this example.<br/>If you are experimenting, refer to the <a class="af nb" href="https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-22-04" rel="noopener ugc nofollow" target="_blank">excellent guide by DigitalOcean</a> on how to deploy an Elastic cluster to a local (or cloud) VM.</li><li id="e5b5" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk"><strong class="oa fr">Databricks workspace</strong> — ensure you have permissions to configure cluster-scoped init scripts. Administrator rights are required if you intend to set up global init scripts.</li></ol><h2 id="8ffb" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Storage</h2><p id="a046" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">For log storage, we will use Elasticsearch’s own storage capabilities. We start by setting up. In Elasticsearch data is organized in indices. Each index contains multiple documents, which are JSON-formatted data structures. Before storing logs, an index must be created. This task is sometimes handled by an organization’s infrastructure or operations team, but if not, it can be accomplished with the following command:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="a9fa" class="qc nd fq pz b bg qd qe l qf qg">curl -X PUT "http://localhost:9200/logs_index?pretty"</span></pre><p id="4c9f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Further customization of the index can be done as needed. For detailed configuration options, refer to the REST API Reference: <a class="af nb" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html" rel="noopener ugc nofollow" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html</a></p><p id="e293" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Once the index is set up documents can be added with:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="93df" class="qc nd fq pz b bg qd qe l qf qg">curl -X POST "http://localhost:9200/logs_index/_doc?pretty"\<br/>     -H 'Content-Type: application/json'\<br/>     -d'<br/>{<br/>  "timestamp": "2024-07-21T12:00:00",<br/>  "log_level": "INFO",<br/>  "message": "This is a log message."<br/>}'</span></pre><p id="47cf" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To retrieve documents, use:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="028c" class="qc nd fq pz b bg qd qe l qf qg">curl -X GET "http://localhost:9200/logs_index/_search?pretty"\<br/>     -H 'Content-Type: application/json'\<br/>     -d'<br/>{<br/>  "query": {<br/>    "match": {<br/>      "message": "This is a log message."<br/>    }<br/>  }<br/>}'</span></pre><p id="1bd6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This covers the essential functionality of Elasticsearch for our purposes. Next, we will set up the log ingestion process.</p><h2 id="62c2" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Transport / Ingestion</h2><p id="72cd" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the ELK stack, Logstash is the component that is responsible for ingesting logs into Elasticsearch.</p><p id="23d3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The functionality of Logstash is organized into <em class="qh">pipelines</em>, which manage the flow of data from ingestion to output.</p><p id="cb8f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Each pipeline can consist of three main stages:</p><ol class=""><li id="ce00" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk"><strong class="oa fr">Input</strong>: Logstash can ingest data from various sources. In this example, we will use Filebeat, a lightweight shipper, as our input source to collect and forward log data — more on this later.</li><li id="d15d" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk"><strong class="oa fr">Filter</strong>: This stage processes the incoming data. While Logstash supports various filters for parsing and transforming logs, we will not be implementing any filters in this scenario.</li><li id="effc" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk"><strong class="oa fr">Output</strong>: The final stage sends the processed data to one or more destinations. Here, the output destination will be an Elasticsearch cluster.</li></ol><p id="e438" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Pipeline configurations are defined in YAML files and stored in the <code class="cx qi qj qk pz b">/etc/logstash/conf.d/</code> directory. Upon starting the Logstash service, these configuration files are automatically loaded and executed.</p><p id="0aff" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">You can refer to <a class="af nb" href="https://www.elastic.co/guide/en/logstash/current/configuration.html" rel="noopener ugc nofollow" target="_blank">Logstash documentation</a> on how to set up one. An example of a minimal pipeline configuration is provided below:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="4e7b" class="qc nd fq pz b bg qd qe l qf qg">input {<br/>  beats {<br/>    port =&gt; 5044<br/>  }<br/>}<br/><br/>filter {}<br/><br/>output {<br/>  elasticsearch {<br/>    hosts =&gt; ["http://localhost:9200"]<br/>    index =&gt; "filebeat-logs-%{+YYYY.MM.dd}"<br/>  }<br/>}</span></pre><p id="ff18" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Finally, ensure the configuration is correct:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="0e05" class="qc nd fq pz b bg qd qe l qf qg">bin/logstash -f /etc/logstash/conf.d/test.conf --config.test_and_exit</span></pre><h2 id="a7ad" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Collecting application logs</h2><p id="7790" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">There is one more component in ELK — Beats. Beats are lightweight agents (shippers) that are used to deliver log (and other) data into either Logstash or Elasticsearch directly. There’s a number of Beats — each for its individual use case but we’ll concentrate on <strong class="oa fr">Filebeat</strong> — by far the most popular one — which is used to collect log <em class="qh">files</em>, process them, and push to Logstash or Elasticsearch directly.</p><p id="825a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Beats must be installed on the machines where logs are generated. In Databricks we’ll need to setup Filebeat on every cluster that we want to log from — either All-Purpose (for prototyping, debugging in notebooks and similar) or Job (for actual workloads). Installing Filebeat involves three steps:</p><ol class=""><li id="70a0" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Installation itself — download and execute distributable package for your operating system (Databricks clusters are running Ubuntu — so a Debian package should be used)</li><li id="c079" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Configure the installed instance</li><li id="07fc" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Starting the service via system.d and asserting it’s active status</li></ol><p id="6649" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This can be achieved with the help of Init scripts. A minimal example Init script is suggested below:</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="f768" class="qc nd fq pz b bg qd qe l qf qg">#!/bin/bash<br/><br/># Check if the script is run as root<br/>if [ "$EUID" -ne 0 ]; then<br/>  echo "Please run as root"<br/>  exit 1<br/>fi<br/><br/># Download filebeat installation package<br/>SRC_URL="https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.14.3-amd64.deb"<br/>DEST_DIR="/tmp"<br/>FILENAME=$(basename "$SRC_URL")<br/>wget -q -O "$DEST_DIR/$FILENAME" "$SRC_URL"<br/><br/># Install filebeat<br/>export DEBIAN_FRONTEND=noninteractive<br/>dpkg -i /tmp/filebeat-8.14.3-amd64.deb<br/>apt-get -f install -y<br/><br/># Configure filebeat<br/>cp /etc/filebeat/filebeat.yml /etc/filebeat/filebeat_backup.yml<br/>tee /etc/filebeat/filebeat.yml &gt; /dev/null &lt;&lt;EOL<br/>filebeat.inputs:<br/>- type: filestream<br/>  id: my-application-filestream-001<br/>  enabled: true<br/>  paths:<br/>    - /var/log/myapplication/*.txt<br/>  parsers:<br/>    - ndjson:<br/>      keys_under_root: true<br/>      overwrite_keys: true<br/>      add_error_key: true<br/>      expand_keys: true<br/><br/>processors:<br/>- timestamp:<br/>    field: timestamp<br/>    layouts:<br/>      - "2006-01-02T15:04:05Z"<br/>      - "2006-01-02T15:04:05.0Z"<br/>      - "2006-01-02T15:04:05.00Z"<br/>      - "2006-01-02T15:04:05.000Z"<br/>      - "2006-01-02T15:04:05.0000Z"<br/>      - "2006-01-02T15:04:05.00000Z"<br/>      - "2006-01-02T15:04:05.000000Z"<br/>    test:<br/>      - "2024-07-19T09:45:20.754Z"<br/>      - "2024-07-19T09:40:26.701Z"<br/><br/>output.logstash:<br/>  hosts: ["localhost:5044"]<br/><br/>logging:<br/>  level: debug<br/>  to_files: true<br/>  files:<br/>    path: /var/log/filebeat<br/>    name: filebeat<br/>    keepfiles: 7<br/>    permissions: 0644<br/>EOL<br/><br/># Start filebeat service<br/>systemctl start filebeat<br/><br/># Verify status<br/># systemctl status filebeat</span></pre><h2 id="30ad" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Timestamp Issue</h2><p id="3e18" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Notice how in the configuration above we set up a processor to extract timestamps. This is done to address a common problem with Filebeat — by default it will populate logs @timestamp field with a timestamp when logs were harvested from the designated directory — not with the timestamp of the actual event. Although the difference is rarely more than 2–3 seconds for a lot of applications, this can mess up the logs real bad — more specifically, it can mess up the order of records as they are coming in.</p><p id="12dd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To address this, we will overwrite the default @timestamp field with values from log themselves.</p><h2 id="69bf" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Logging</h2><p id="47c4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Once Filebeat is installed and running, it will automatically collect all logs output to the designated directory, forwarding them to Logstash and subsequently down the pipeline.</p><p id="69c7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Before this can occur, we need to configure the Python logging library.</p><p id="4b34" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The first necessary modification would be to set up FileHandler to output logs as files to the designated directory. Default logging FileHandler will work just fine.</p><p id="81b8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Then we need to format the logs into NDJSON, which is required for proper parsing by Filebeat. Since this format is not natively supported by the standard Python library, we will need to implement a custom <code class="cx qi qj qk pz b">Formatter</code>.</p><pre class="ml mm mn mo mp py pz qa bp qb bb bk"><span id="9bc4" class="qc nd fq pz b bg qd qe l qf qg">class NDJSONFormatter(logging.Formatter):<br/>    def __init__(self, extra_fields=None):<br/>        super().__init__()<br/>        self.extra_fields = extra_fields if extra_fields is not None else {}<br/><br/>    def format(self, record):<br/>        log_record = {<br/>            "timestamp": datetime.datetime.fromtimestamp(record.created).isoformat() + 'Z',<br/>            "log.level": record.levelname.lower(),<br/>            "message": record.getMessage(),<br/>            "logger.name": record.name,<br/>            "path": record.pathname,<br/>            "lineno": record.lineno,<br/>            "function": record.funcName,<br/>            "pid": record.process,<br/>        }<br/>        log_record = {**log_record, **self.extra_fields}<br/>        if record.exc_info:<br/>            log_record["exception"] = self.formatException(record.exc_info)<br/>        return json.dumps(log_record)</span></pre><p id="ef6c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We will also use the custom Formatter to address the timestamp issue we discussed earlier. In the configuration above a new field <em class="qh">timestamp</em> is added to the <code class="cx qi qj qk pz b">LogRecord</code> object that will conatain a copy of the event timestamp. This field may be used in timestamp processor in Filebeat to replace the actual @timestamp field in the published logs.</p><p id="3cf7" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We can also use the Formatter to add extra fields — which may be useful for distinguishing logs if your organization uses one index to collect logs from multiple applications.</p><p id="1e6f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Additional modifications can be made as per your requirements. Once the Logger has been set up we can use the standard Python logging API — <code class="cx qi qj qk pz b">.info()</code> and <code class="cx qi qj qk pz b">.debug()</code>, to write logs to the log file and they will automatically propagate to Filebeat, then to Logstash, then to Elasticsearch and finally we will be able to access those in Kibana (or any other client of our choice).</p><h2 id="7de3" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Visualization</h2><p id="e612" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the ELK stack, Kibana is a component responsible for visualizing the logs (or any other). For the purpose of this example, we’ll just use it as a glorified search client for Elasticsearch. It can however (and is intended to) be set up as a full-featured monitoring and alerting solution given its rich data presentation toolset.</p><p id="84d2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In order to finally see our log data in Kibana, we need to set up Index Patterns:</p><ol class=""><li id="1a1c" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk">Navigate to Kibana.</li><li id="2cde" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Open the “Burger Menu” (≡).</li><li id="a1c3" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Go to <strong class="oa fr">Management</strong> -&gt; <strong class="oa fr">Stack Management</strong> -&gt; <strong class="oa fr">Kibana</strong> -&gt; <strong class="oa fr">Index Patterns</strong>.</li><li id="43f6" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot pq pr ps bk">Click on <strong class="oa fr">Create Index Pattern</strong>.</li></ol><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/26cdd8d0af5b3c2eef5a551b8b5ce17d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RaP54jtD3Itz_Fg3G2u79w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Kibana index pattern creation interfact</figcaption></figure><p id="26c0" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Kibana will helpfully suggest names of the available sources for the Index Patterns. Type out a name that will capture the names of the sources. In this example it can be e.g. <code class="cx qi qj qk pz b"><em class="qh">filebeat*</em></code>, then click <strong class="oa fr">Create index pattern</strong>.</p><p id="6ac8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Once selected, proceed to Discover menu, select the newly created index pattern on the left drop-down menu, adjust time interval (a common pitfall — it is set up to last 15 minutes by default) and start with your own first KQL query to retrieve the logs.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/4bfda8d3fb470c7a06ffb5a585baa0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWPtZP8P8Qw54jHrwgDtiA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Log stream visualized in Kibana</figcaption></figure><p id="11ee" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We have now successfully completed the multi-step journey from generating a log entry in a Python application hosted on Databricks to to visualizing and monitoring this data using a client interface.</p><h1 id="b7cb" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="ff7e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">While this article has covered the introductory aspects of setting up a robust logging and monitoring solution using the ELK Stack in conjunction with Databricks, there are additional considerations and advanced topics that suggest further exploration:</p><ul class=""><li id="c296" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot qn pr ps bk"><strong class="oa fr">Choosing Between Logstash and Direct Ingestion</strong>: Evaluating whether to use Logstash for additional data processing capabilities versus directly forwarding logs from Filebeat to Elasticsearch.</li><li id="c560" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qn pr ps bk"><strong class="oa fr">Schema Considerations</strong>: Deciding on the adoption of the Elastic Common Schema (ECS) versus implementing custom field structures for log data.</li><li id="3021" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qn pr ps bk"><strong class="oa fr">Exploring Alternative Solutions</strong>: Investigating other tools such as Azure EventHubs and other potential log shippers that may better fit specific use cases.</li><li id="8ebc" class="ny nz fq oa b go pt oc od gr pu of og oh pv oj ok ol pw on oo op px or os ot qn pr ps bk"><strong class="oa fr">Broadening the Scope</strong>: Extending these practices to encompass other data engineering tools and platforms, ensuring comprehensive observability across the entire data pipeline.</li></ul><p id="c2f1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">These topics will be explored in further articles.</p><p id="0273" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="qh">Unless otherwise noted, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>