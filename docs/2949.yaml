- en: 'Lasso and Elastic Net Regressions, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/lasso-and-elastic-net-regressions-explained-a-visual-guide-with-code-examples-5fecf3e1432f?source=collection_archive---------1-----------------------#2024-12-06](https://towardsdatascience.com/lasso-and-elastic-net-regressions-explained-a-visual-guide-with-code-examples-5fecf3e1432f?source=collection_archive---------1-----------------------#2024-12-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: REGRESSION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Roping in key features with coordinate descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--5fecf3e1432f--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--5fecf3e1432f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5fecf3e1432f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5fecf3e1432f--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--5fecf3e1432f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5fecf3e1432f--------------------------------)
    Â·15 min readÂ·Dec 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=post_page-----5fecf3e1432f--------------------------------)
    [## Least Squares Regression, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Gliding through points to minimize squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=post_page-----5fecf3e1432f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression comes in different types: [Least Squares methods](/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4)
    form the foundation, from the classic Ordinary Least Squares (OLS) to Ridge regression
    with its regularization to prevent overfitting. Then thereâ€™s Lasso regression,
    which takes a unique approach by automatically selecting important factors and
    ignoring others. Elastic Net combines the best of both worlds, mixing Lassoâ€™s
    feature selection with Ridgeâ€™s ability to handle related features.'
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s frustrating to see many articles treat these methods as if theyâ€™re basically
    the same thing with minor tweaks. They make it seem like switching between them
    is as simple as changing a setting in your code, but each actually uses different
    approaches to solve their optimization problems!
  prefs: []
  type: TYPE_NORMAL
- en: While OLS and Ridge regression can be solved directly through matrix operations,
    Lasso and Elastic Net require a different approach â€” an iterative method called
    **coordinate descent**. Here, weâ€™ll explore how this algorithm works through clear
    visualizations. So, letâ€™s saddle up and *lasso* our way through the details!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89cfee410d2caa68f774230733aaf4c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LASSO** (**L**east **A**bsolute **S**hrinkage and **S**election **O**perator)
    is a variation of Linear Regression that adds a penalty to the model. It uses
    a linear equation to predict numbers, just like Linear Regression. However, Lasso
    also has a way to reduce the importance of certain factors to zero, which makes
    it useful for two main tasks: making predictions and identifying the most important
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elastic Net Regression is a mix of Ridge and Lasso Regression that combines
    their penalty terms. The name â€œElastic Netâ€ comes from physics: just like an elastic
    net can stretch and still keep its shape, this method adapts to data while maintaining
    structure.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model balances three goals: minimizing prediction errors, keeping the size
    of coefficients small (like Lasso), and preventing any coefficient from becoming
    too large (like Ridge). To use the model, you input your dataâ€™s feature values
    into the linear equation, just like in standard Linear Regression.'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of Elastic Net is that when features are related, it tends
    to keep or remove them as a group instead of randomly picking one feature from
    the group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92cd64e687e3a8222ab9d9001eead95d.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear models like Lasso and Elastic Net belong to the broader family of machine
    learning methods that predict outcomes using linear relationships between variables.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“Š Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate our concepts, weâ€™ll use [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)
    that predicts the number of golfers visiting on a given day, using features like
    weather outlook, temperature, humidity, and wind conditions.
  prefs: []
  type: TYPE_NORMAL
- en: For both Lasso and Elastic Net to work effectively, we need to standardize the
    numerical features (making their scales comparable) and apply one-hot-encoding
    to categorical features, as both modelsâ€™ penalties are sensitive to feature scales.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5b052ff81db21a6616645b0912d1802.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: â€˜Outlookâ€™ (one-hot encoded to sunny, overcast, rain), â€˜Temperatureâ€™
    (standardized), â€˜Humidityâ€™ (standardized), â€˜Windâ€™ (Yes/No) and â€˜Number of Playersâ€™
    (numerical, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Lasso and Elastic Net Regression predict numbers by making a straight line
    (or hyperplane) from the data, while controlling the size of coefficients in different
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Both models find the best line by balancing prediction accuracy with coefficient
    control. They work to make the gaps between real and predicted values small, while
    keeping coefficients in check through penalty terms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In Lasso, the penalty (controlled by *Î»*) can shrink coefficients to exactly
    zero, removing features entirely. Elastic Net combines two types of penalties:
    one that can remove features (like Lasso) and another that shrinks groups of related
    features together. The mix between these penalties is controlled by the `l1_ratio`
    (*Î±*).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To predict a new answer, both models multiply each input by its coefficient
    (if not zero) and add them up, plus a starting number (intercept/bias). Elastic
    Net often keeps more features than Lasso but with smaller coefficients, especially
    when features are correlated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The strength of penalties affects how the models behave:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- In Lasso, larger *Î»* means more coefficients become zero'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- In Elastic Net, *Î»* controls overall penalty strength, while *Î±* determines
    the balance between feature removal and coefficient shrinkage'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- When penalties are very small, both models act more like standard Linear
    Regression'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/a24d6aea0a7577617aaccec1a0dda614.png)'
  prefs: []
  type: TYPE_IMG
- en: Lasso and Elastic Net make predictions by multiplying input features with their
    trained weights and adding them together with a bias term to produce a final output
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Letâ€™s explore how Lasso and Elastic Net learn from data using the coordinate
    descent algorithm. While these models have complex mathematical foundations, weâ€™ll
    focus on **understanding coordinate descent** â€” an efficient optimization method
    that makes the computation more practical and intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: Coordinate Descent for Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimization problem of Lasso Regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ffc77f9feca66a0d82b821c01933508b.png)'
  prefs: []
  type: TYPE_IMG
- en: While [scikit-learn implementation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html)
    includes additional scaling factors (1/(2*n_samples)) for computational efficiency,
    weâ€™ll use the standard theoretical form for clarity in our explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s how coordinate descent finds the optimal coefficients by updating one
    feature at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Start by initializing the model with all coefficients at zero. Set a fixed
    value for the regularization parameter that will control the strength of the penalty.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/063f2e17b33d37e82d58982abab75518.png)'
  prefs: []
  type: TYPE_IMG
- en: Lasso regression begins with all feature weights set to zero and uses a penalty
    parameter (Î») to control how much it shrinks weights during training.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Calculate the initial bias by taking the mean of all target values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2df6fc11bb7aa805db1e21ff38c5325.png)'
  prefs: []
  type: TYPE_IMG
- en: The initial bias value is set to 37.43, which is calculated by taking the average
    of all target values in the training data (mean of player counts shown from index
    0 to 13).
  prefs: []
  type: TYPE_NORMAL
- en: '3\. For updating the first coefficient (in our case, â€˜sunnyâ€™):'
  prefs: []
  type: TYPE_NORMAL
- en: '- Using weighted sum, calculate what the model would predict **without using
    this feature.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16a9669367286fe143c27efffb821f61.png)'
  prefs: []
  type: TYPE_IMG
- en: At the start of training, all feature weights are set to zero while using the
    initial bias of 37.43, causing the model to predict the same average value (37.43)
    for all training examples regardless of their input features.
  prefs: []
  type: TYPE_NORMAL
- en: '- Find the partial residual â€” how far off these predictions are from the actual
    values. Using this value, calculate the temporary coefficient.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0431216a648bff8bd9d223beb5d087ad.png)'
  prefs: []
  type: TYPE_IMG
- en: For the first feature, Lasso calculates a temporary coefficient of 11.17 by
    comparing the true labels with predictions, considering only the rows where this
    feature equals 1, and applying the gradient formula.
  prefs: []
  type: TYPE_NORMAL
- en: '- Apply the Lasso shrinkage (soft thresholding) to this temporary coefficient
    to get the final coefficient for this step.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/290cc6c85081860c00fb5348ecf1e598.png)'
  prefs: []
  type: TYPE_IMG
- en: Lasso applies its shrinkage formula to the temporary coefficient (11.17), where
    it subtracts the penalty term (Î»/5 = 0.2) from the absolute value while preserving
    the sign, resulting in a final coefficient of 10.97.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Move through each remaining coefficient one at a time, repeating the same
    update process. When calculating predictions during each update, use the most
    recently updated values for all other coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/970b037792a6038bf56c11b1122af6ba.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating the first coefficient to 10.97, Lasso uses these updated predictions
    to calculate the temporary coefficient (0.32) for the second feature, showing
    how the algorithm updates coefficients one at a time through coordinate descent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Return to update the bias by calculating what the current model predicts
    **using all features**, then adjust the bias based on the average difference between
    these predictions and actual values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ad780f7356dc1034f149cd228fb7f72.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating all feature coefficients through coordinate descent, the model
    recalculates the bias (40.25) as the mean difference between the true labels and
    the predictions made using the current feature weights, ensuring the modelâ€™s predictions
    are properly centered around the target values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Check if the model has converged either by reaching the maximum number of
    allowed iterations or by seeing that coefficients arenâ€™t changing much anymore.
    If not converged, return to step 3 and repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbde5df18fb4166d6ca595733b30eef6.png)'
  prefs: []
  type: TYPE_IMG
- en: After 1000 iterations of coordinate descent, Lasso produces the final model
    where some coefficients have been shrunk exactly to zero (â€˜rainâ€™ and â€˜Temperatureâ€™
    features), while others retain non-zero values, demonstrating Lassoâ€™s feature
    selection capability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Coordinate Descent for Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimization problem of Elastic Net Regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57cb5db32b807d107f8cf8cae9a41f19.png)'
  prefs: []
  type: TYPE_IMG
- en: While [scikit-learnâ€™s implementation](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.ElasticNet.html)
    includes additional scaling factors (1/(2*n_samples)) and uses alpha (Î±) to control
    overall regularization strength and l1_ratio to control the penalty mix, weâ€™ll
    use the standard theoretical form for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The coordinate descent algorithm for Elastic Net works similarly to Lasso,
    but accounts for both penalties when updating coefficients. Hereâ€™s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Start by initializing the model with all coefficients at zero. Set two
    fixed values: one controlling feature removal (like in Lasso) and another for
    general coefficient shrinkage (the key difference from Lasso).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce717b9cd599b195a4313563dc486b1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Elastic Net regression starts like Lasso with zero weights for all features,
    but uses two parameters: Î» (lambda) for overall regularization strength and Î±
    (alpha) to balance between Lasso and Ridge penalties.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Calculate the initial bias by taking the mean of all target values. (Same
    as Lasso)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de17fbaa9b6e7d4e9b6b761036cc08ce.png)'
  prefs: []
  type: TYPE_IMG
- en: ike Lasso, Elastic Net also initializes its bias term to 37.43 by calculating
    the mean of all target values in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. For updating the first coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Using weighted sum, calculate what the model would predict without using
    this feature. (Same as Lasso)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16a9669367286fe143c27efffb821f61.png)'
  prefs: []
  type: TYPE_IMG
- en: Elastic Net starts its coordinate descent process similar to Lasso, making initial
    predictions of 37.43 for all training examples since all feature weights are set
    to zero and only the bias term is active.
  prefs: []
  type: TYPE_NORMAL
- en: '- Find the partial residual â€” how far off these predictions are from the actual
    values. Using this value, calculate the temporary coefficient. (Same as Lasso)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0431216a648bff8bd9d223beb5d087ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Like Lasso, Elastic Net calculates a temporary coefficient of 11.17 for the
    first feature by comparing predictions with true labels.
  prefs: []
  type: TYPE_NORMAL
- en: '- For Elastic Net, apply both **soft thresholding** and **coefficient shrinkage**
    to this temporary coefficient to get the final coefficient for this step. This
    combined effect is the main difference from Lasso Regression.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e327609ca8f462275e64233a3bf0d258.png)'
  prefs: []
  type: TYPE_IMG
- en: Elastic Net applies its unique shrinkage formula that combines both Lasso (L1)
    and Ridge (L2) penalties, where Î± controls their balance. The temporary coefficient
    11.17 is shrunk to 10.06 through this combined regularization approach.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Move through each remaining coefficient one at a time, repeating the same
    update process. When calculating predictions during each update, use the most
    recently updated values for all other coefficients. (Same process as Lasso, but
    using the modified update formula)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e88dbf7e3ac2a2cd0364b38a574f89a1.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating the first coefficient to 10.06, Elastic Net continues coordinate
    descent by calculating and updating the second coefficient, showing how it processes
    features one at a time while maintaining both L1 and L2 regularization effects.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Update the bias by calculating what the current model predicts using all
    features, then adjust the bias based on the average difference between these predictions
    and actual values. (Same as Lasso)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84a57041683e2e82a1f091998f2fd4e8.png)'
  prefs: []
  type: TYPE_IMG
- en: After updating all feature coefficients using Elastic Netâ€™s combined L1 and
    L2 regularization, the model recalculates the bias to 40.01 by taking the mean
    difference between true labels and predictions, similar to the process in Lasso
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Check if the model has converged either by reaching the maximum number of
    allowed iterations or by seeing that coefficients arenâ€™t changing much anymore.
    If not converged, return to step 3 and repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bdff8f8191a93822b8c5d9788dce60f.png)'
  prefs: []
  type: TYPE_IMG
- en: The final Elastic Net model after 1000 iterations shows smaller coefficient
    values compared to Lasso and fewer coefficients shrunk exactly to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Test Step**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The prediction process remains the same as OLS â€” multiply new data points by
    the coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/5b346c92504c34a5978d7fa63339b597.png)'
  prefs: []
  type: TYPE_IMG
- en: When applying the trained Lasso model to unseen data, it multiplies each feature
    value with its corresponding coefficient and adds the bias term (41.24), resulting
    in a final prediction of 40.2 players for this new data point.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/d236c4e9015808c670b7af71685e43ea.png)'
  prefs: []
  type: TYPE_IMG
- en: The trained Elastic Net model predicts 40.83 players for the same unseen data
    point by multiplying features with its more evenly distributed coefficients and
    adding the bias (38.59), showing a slightly different prediction from Lasso due
    to its balanced regularization approach.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can do the same process for all data points. For our dataset, hereâ€™s the
    final result with the RMSE as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b6e2f8a906fcbe42c82b4e75ee41cddf.png)'
  prefs: []
  type: TYPE_IMG
- en: Lassoâ€™s performance on multiple test cases shows a Root Mean Square Error (RMSE)
    of 7.203, calculated by comparing its predictions with actual player counts across
    14 different test samples.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/5ede9d56bb31b43bac0195a76b3cde52.png)'
  prefs: []
  type: TYPE_IMG
- en: Elastic Net shows a slightly higher RMSE compared to Lassoâ€™s, likely because
    its combined L1 and L2 penalties keep more features with small non-zero coefficients,
    which can introduce more variance in predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lasso Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lasso regression uses coordinate descent to solve the optimization problem.
    Here are the key parameters for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha` (Î»): Controls how strongly to penalize large coefficients. Higher values
    force more coefficients to become exactly zero. Default is 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iter`: Sets the maximum number of cycles the algorithm will update its
    solution in search of the best result. Default is 1000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tol`: Sets how small the change in coefficients needs to be before the algorithm
    decides it has found a good enough solution. Default is 0.0001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Net Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Elastic Net regression combines two types of penalties and also uses coordinate
    descent. Here are the key parameters for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`alpha` (*Î»*): Controls the overall strength of both penalties together. Higher
    values mean stronger penalties. Default is 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`l1_ratio` (*Î±*): Sets how much to use each type of penalty. A value of 0 uses
    only Ridge penalty, while 1 uses only Lasso penalty. Values between 0 and 1 use
    both. Default is 0.5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_iter`: Maximum number of iterations for the coordinate descent algorithm.
    Default is 1000 iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tol`: Tolerance for the optimization convergence, similar to Lasso. Default
    is 1e-4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note*: Not to be confused, in `scikit-learn`â€™s code, the regularization parameter
    is called `alpha`, but in mathematical notation itâ€™s typically written as *Î»*
    (lambda). Similarly, the mixing parameter is called `l1_ratio` in code but written
    as *Î±* (alpha) in mathematical notation. We use the mathematical symbols here
    to match standard textbook notation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Comparison: OLS vs Lasso vs Ridge vs Elastic Net'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With Elastic Net, we can actually explore different types of linear regression
    models by adjusting the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: When `alpha` = 0, we get Ordinary Least Squares (OLS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `alpha` > 0 and `l1_ratio` = 0, we get Ridge regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `alpha` > 0 and `l1_ratio` = 1, we get Lasso regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When `alpha` > 0 and 0 < `l1_ratio` < 1, we get Elastic Net regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, it is a good idea to explore a range of `alpha` values (like 0.0001,
    0.001, 0.01, 0.1, 1, 10, 100) and `l1_ratio` values (like 0, 0.25, 0.5, 0.75,
    1), preferably using cross-validation to find the best combination.
  prefs: []
  type: TYPE_NORMAL
- en: Here, letâ€™s see how the model coefficients, bias terms, and test RMSE change
    with different regularization strengths (*Î»*) and mixing parameters (`l1_ratio`).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b26b010afc0a4b6801ec0128418fb8c.png)![](../Images/6f1640b2c83d7addefc37d8c4619dc71.png)![](../Images/d8d68a8953bf6af67433a1de86169808.png)![](../Images/04b0cdb113e505cc73a2a6a207f47dbb.png)![](../Images/9dd62b75a888017ce978d13d494a615e.png)'
  prefs: []
  type: TYPE_IMG
- en: The best model is Lasso (Î± = 0) with Î» = 0.1, achieving an RMSE of 6.561, showing
    that pure L1 regularization works best for our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Note*: Even though Elastic Net can do what OLS, Ridge, and Lasso do by changing
    its parameters, itâ€™s better to use the specific command made for each type of
    regression. In scikit-learn, use `LinearRegression` for OLS, `Ridge` for Ridge
    regression, and `Lasso` for Lasso regression. Only use Elastic Net when you want
    to combine both Lasso and Ridge''s special features together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Final Remarks: Which Regression Method Should You Use?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Letâ€™s break down when to use each method.
  prefs: []
  type: TYPE_NORMAL
- en: Start with **Ordinary Least Squares (OLS)** when you have more samples than
    features in your dataset, and when your features donâ€™t strongly predict each other.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ridge Regression** works well when you have the opposite situation â€” lots
    of features compared to your number of samples. Itâ€™s also great when your features
    are strongly connected to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lasso Regression** is best when you want to discover which features actually
    matter for your predictions. It will automatically set unimportant features to
    zero, making your model simpler.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic Net** combines the strengths of both Ridge and Lasso. Itâ€™s useful
    when you have groups of related features and want to either keep or remove them
    together. If youâ€™ve tried Ridge and Lasso separately and werenâ€™t happy with the
    results, Elastic Net might give you better predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: A good strategy is to start with Ridge if you want to keep all your features.
    You can move on to Lasso if you want to identify the important ones. If neither
    gives you good results, then move on to Elastic Net.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ Lasso and Elastic Net Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of [Lasso](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html)
    Regression and [Elastic Net](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.ElasticNet.html)
    Regression, and its implementation in `scikit-learn`, readers can refer to their
    official documentation. It provides comprehensive information on their usage and
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ğ™šğ™œğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----5fecf3e1432f--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----5fecf3e1432f--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----5fecf3e1432f--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
