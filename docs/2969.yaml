- en: Data Quality Doesn’t Need to Be Complicated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/stop-overcomplicating-data-quality-4569fc6d35a4?source=collection_archive---------1-----------------------#2024-12-10](https://towardsdatascience.com/stop-overcomplicating-data-quality-4569fc6d35a4?source=collection_archive---------1-----------------------#2024-12-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Three Zero-Cost Solutions That Take Hours, Not Months
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@caiparryjones96?source=post_page---byline--4569fc6d35a4--------------------------------)[![Cai
    Parry-Jones](../Images/60b83f5167651f9621a3e73b8d72ccae.png)](https://medium.com/@caiparryjones96?source=post_page---byline--4569fc6d35a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4569fc6d35a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4569fc6d35a4--------------------------------)
    [Cai Parry-Jones](https://medium.com/@caiparryjones96?source=post_page---byline--4569fc6d35a4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4569fc6d35a4--------------------------------)
    ·8 min read·Dec 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b570b710780faeea2899cb85facde86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A ‘data quality’ certified pipeline. Source: [unsplash.com](https://unsplash.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: In my career, data quality initiatives have usually meant big changes. From
    governance processes to costly tools to dbt implementation — data quality projects
    never seem to want to be small.
  prefs: []
  type: TYPE_NORMAL
- en: What’s more, fixing the data quality issues this way often leads to new problems.
    More complexity, higher costs, slower data project releases…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3713bea14e131ad68e6fde839cda4e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: created by the author using [Google Sheets](https://workspace.google.com/intl/en_uk/products/sheets/)
  prefs: []
  type: TYPE_NORMAL
- en: But it doesn’t have to be this way.
  prefs: []
  type: TYPE_NORMAL
- en: While companies like Google and Meta require extensive data quality frameworks
    due to their massive scale and complexity, most organizations can achieve excellent
    data quality with much simpler approaches.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll delve into three methods small to medium sized data projects
    can use to quickly improve their data quality, all while keeping
  prefs: []
  type: TYPE_NORMAL
- en: complexity to a minimum and new costs at zero. Let’s get to it!
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take advantage of old school database tricks, like ENUM data types, and column
    constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a custom dashboard for your specific data quality problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate data lineage with one small Python script.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take advantage of old school database tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last 10–15 years we’ve seen massive changes to the data industry, notably
    big data, parallel processing, cloud computing, data warehouses, and new tools
    (lots and lots of new tools).
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, we’ve had to say goodbye to some things to make room for all this
    new stuff. Some positives (Microsoft Access comes to mind), but some are questionable
    at best, such as traditional data design principles and data quality and validation
    at ingestion. The latter will be the subject of this section.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, what do I mean by “data quality and validation at ingestion”? Simply,
    it means checking data before it enters a table. Think of a bouncer outside a
    nightclub.
  prefs: []
  type: TYPE_NORMAL
- en: What it has been replaced with is build-then-test, which means putting new data
    in tables first, and then checking it later. Build-then-test is the chosen method
    for many modern data quality tools, including the most popular, [dbt](https://www.getdbt.com/product/what-is-dbt).
  prefs: []
  type: TYPE_NORMAL
- en: Dbt runs the whole data transformation pipeline first, and only once all the
    new data is in place, it checks to see if the data is good. Of course, this can
    be the optimal solution in many cases. For example, if the business is happy to
    sacrifice quality for speed, or if there is a QA table before a production table
    (coined by Netflix as [Write-Audit-Publish](https://lakefs.io/blog/data-engineering-patterns-write-audit-publish/)).
    However, engineers who only use this method of data quality are potentially missing
    out on some big wins for their organization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46fe0562178bfdf861d61c21943b513c.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing before vs after generating tables. Created by the author using [draw.io](https://app.diagrams.net/)
  prefs: []
  type: TYPE_NORMAL
- en: Test-then-build has two main benefits over build-then-test.
  prefs: []
  type: TYPE_NORMAL
- en: The first is that it ensures the data in downstream tables meets the data quality
    standards expected **at all times**. This gives the data a level of trustworthiness,
    so often lacking, for downstream users. It can also reduce anxiety for the data
    engineer/s responsible for the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: I remember when I owned a key financial pipeline for a company I used to work
    for. Unfortunately, this pipeline was very prone to data quality issues, and the
    solution in place was a build-then-test system, which ran each night. This meant
    I needed to rush to my station early in the morning each day to check the results
    of the run before any downstream users started looking at their data. If there
    were any issues I then needed to either quickly fix the issue or send a Slack
    message of shame announcing to the business the data sucks and to please be patient
    while I fix it.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, test-then-build doesn’t totally fix this anxiety issue. The story
    would change from needing to rush to fix the issue to avoid bad data for downstream
    users to rushing to fix the issue to avoid stale data for downstream users. However,
    engineering is all about weighing the pros and cons of different solutions. And
    in this scenario I know old data would have been the best of two evils for both
    the business and my sanity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second benefit test-then-build has is that it can be much simpler to implement,
    especially compared to setting up a whole QA area, which is a bazooka-to-a-bunny
    solution for solving most data quality issues. All you need to do is include your
    data quality criteria when you create the table. Have a look at the below PostgreSQL
    query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'These 14 lines of code will ensure the daily_revenue table enforces the following
    standards:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`'
  prefs: []
  type: TYPE_NORMAL
- en: Primary key constraint ensures uniqueness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`date`'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be a future date (via CHECK constraint).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forms part of a unique constraint with revenue_source.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`revenue_source`'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be NULL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forms part of a unique constraint with date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be a valid value from revenue_source_type enum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gross_amount`'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be NULL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be >= 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be >= processing_fees + tax_amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be >= net_amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precise decimal handling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`net_amount`'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be NULL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be >= 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be <= gross_amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precise decimal handling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`currency`'
  prefs: []
  type: TYPE_NORMAL
- en: Must be a valid value from currency_code_type enum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transaction_count`'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be NULL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Must be >= 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s simple. Reliable. And would you believe all of this was available to us
    since the release of PostgreSQL 6.5… which came out in 1999!
  prefs: []
  type: TYPE_NORMAL
- en: Of course there’s no such thing as a free lunch. Enforcing constraints this
    way does have its drawbacks. For example, it makes the table a lot less flexible,
    and it will reduce the performance when updating the table. As always, you need
    to think like an engineer before diving into any tool/technology/method.
  prefs: []
  type: TYPE_NORMAL
- en: Create a custom dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I have a confession to make. I used to think good data engineers didn’t use
    dashboard tools to solve their problems. I thought a real engineer looks at logs,
    hard-to-read code, and whatever else made them look smart if someone ever glanced
    at their computer screen.
  prefs: []
  type: TYPE_NORMAL
- en: I was dumb.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out they can be really valuable if executed effectively for a clear
    purpose. Furthermore, most BI tools make creating dashboards super easy and quick,
    without (too) much time spent learning the tool.
  prefs: []
  type: TYPE_NORMAL
- en: Back to my personal pipeline experiences. I used to manage a daily aggregated
    table of all the business’ revenue sources. Each source came from a different
    revenue provider, and as such a different system. Some would be via API calls,
    others via email, and others via a shared S3 bucket. As any engineer would expect,
    some of these sources fell over from time-to-time, and because they came from
    third parties, I couldn’t fix the issue at source (only ask, which had very limited
    success).
  prefs: []
  type: TYPE_NORMAL
- en: Originally, I had only used failure logs to determine where things needed fixing.
    The problem was priority. Some failures needed quickly fixing, while others were
    not important enough to drop everything for (we had some revenue sources that
    literally reported pennies each day). As a result, there was a build up of small
    data quality issues, which became difficult to keep track of.
  prefs: []
  type: TYPE_NORMAL
- en: Enter Tableau.
  prefs: []
  type: TYPE_NORMAL
- en: 'I created a very basic dashboard that highlighted metadata by revenue source
    and date for the last 14 days. Three metrics were all I needed:'
  prefs: []
  type: TYPE_NORMAL
- en: A green or red mark indicating whether data was present or missing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The row count of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sum of revenue of the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c52b8496ecc6364ad902401ced5c886e.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple yet effective dashboard. Created by the author using [Tableau](https://www.tableau.com/en-gb)
  prefs: []
  type: TYPE_NORMAL
- en: This made the pipeline’s data quality a whole lot easier to manage. Not only
    was it much quicker for me to glance at where the issues were, but it was user-friendly
    enough for other people to read from too, allowing for shared responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing the dashboard, bug tickets reported by the business related
    to the pipeline dropped to virtually zero, as did my risk of a stroke.
  prefs: []
  type: TYPE_NORMAL
- en: Map your data with a lineage chart
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple data observability solutions don’t just stop at dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage can be a dream for quickly spotting what tables have been affected
    by bad data upstream.
  prefs: []
  type: TYPE_NORMAL
- en: However, it can also be a mammoth task to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number one culprit for this, in my opinion, is dbt. A key selling point
    of the open-source tool is its data lineage capabilities. But to achieve this
    you have to bow down to dbt’s framework. Including, but not limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing [Jinja3](https://jinja.palletsprojects.com/en/stable/) in all you
    SQL files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a YAML file for each data model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add Source data configuration via YAML files.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up a development and testing process e.g. development environment, version
    control, CI/CD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infrastructure set-up e.g. hosting your own server or purchasing a managed version
    ([dbtCloud](https://www.getdbt.com/pricing)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yeah, it’s a lot.
  prefs: []
  type: TYPE_NORMAL
- en: But it doesn’t have to be. Ultimately, all you need for dynamic data lineage
    is a machine that scans your SQL files, and something to output a user-friendly
    lineage map. Thanks to Python, this can be achieved using a script with as few
    as 100 lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: If you know a bit of Python and LLM prompting you should be able to hack the
    code in an hour. Alternatively, there’s a lightweight open-source Python tool
    called [SQL-WatchPup](https://github.com/caitpj/SQL-WatchPup) that already has
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Provided you have all your SQL files available, in 15 minutes of set up you
    should be able to generate dynamic data lineage maps like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ce584894fa4dbecf310e22749ba9487.png)'
  prefs: []
  type: TYPE_IMG
- en: Example data lineage map output. Created by the author using [SQL-WatchPup](https://github.com/caitpj/SQL-WatchPup)
  prefs: []
  type: TYPE_NORMAL
- en: That’s it. No server hosting costs. No extra computer languages to learn. No
    restructuring of your files. Just running one simple Python script locally.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s face it — we all love shiny new in-vogue tools, but sometimes the best
    solutions are old, uncool, and/or unpopular.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next time you’re faced with data quality headaches, take a step back before
    diving into that massive infrastructure overhaul. Ask yourself: Could a simple
    database constraint, a basic dashboard, or a lightweight Python script do the
    trick?'
  prefs: []
  type: TYPE_NORMAL
- en: Your sanity will thank you for it. Your company’s budget will too.
  prefs: []
  type: TYPE_NORMAL
