["```py\ntables['transactions'] = Table(\n            df=pd.DataFrame(t),\n            pkey_col='t_id',\n            fkey_col_to_pkey_table={\n                'customer_key': 'customers',\n                'item_key': 'products',\n                'store_key': 'stores'\n            },\n            time_col='date'\n        )\n```", "```py\nclass EcommerceDataBase(Dataset):\n    # example of creating your own dataset: https://github.com/snap-stanford/relbench/blob/main/tutorials/custom_dataset.ipynb\n\n    val_timestamp = pd.Timestamp(year=2018, month=1, day=1)\n    test_timestamp = pd.Timestamp(year=2020, month=1, day=1)\n\n    def make_db(self) -> Database:\n\n        tables = {}\n\n        customers = load_csv_to_db(BASE_DIR + '/customer_dim.csv').drop(columns=['contact_no', 'nid']).rename(columns={'coustomer_key': 'customer_key'})\n        stores = load_csv_to_db(BASE_DIR + '/store_dim.csv').drop(columns=['upazila'])\n        products = load_csv_to_db(BASE_DIR + '/item_dim.csv')\n        transactions = load_csv_to_db(BASE_DIR + '/fact_table.csv').rename(columns={'coustomer_key': 'customer_key'})\n        times = load_csv_to_db(BASE_DIR + '/time_dim.csv')\n\n        t = transactions.merge(times[['time_key', 'date']], on='time_key').drop(columns=['payment_key', 'time_key', 'unit'])\n        t['date'] = pd.to_datetime(t.date)\n        t = t.reset_index().rename(columns={'index': 't_id'})\n        t['quantity'] = t.quantity.astype(int)\n        t['unit_price'] = t.unit_price.astype(float)\n        products['unit_price'] = products.unit_price.astype(float)\n        t['total_price'] = t.total_price.astype(float)\n\n        print(t.isna().sum(axis=0))\n        print(products.isna().sum(axis=0))\n        print(stores.isna().sum(axis=0))\n        print(customers.isna().sum(axis=0))\n\n        tables['products'] = Table(\n            df=pd.DataFrame(products),\n            pkey_col='item_key',\n            fkey_col_to_pkey_table={},\n            time_col=None\n        )\n\n        tables['customers'] = Table(\n            df=pd.DataFrame(customers),\n            pkey_col='customer_key',\n            fkey_col_to_pkey_table={},\n            time_col=None\n        )\n\n        tables['transactions'] = Table(\n            df=pd.DataFrame(t),\n            pkey_col='t_id',\n            fkey_col_to_pkey_table={\n                'customer_key': 'customers',\n                'item_key': 'products',\n                'store_key': 'stores'\n            },\n            time_col='date'\n        )\n\n        tables['stores'] = Table(\n            df=pd.DataFrame(stores),\n            pkey_col='store_key',\n            fkey_col_to_pkey_table={}\n        )\n\n        return Database(tables)\n```", "```py\ndf = duckdb.sql(f\"\"\"\n            select\n                timestamp,\n                customer_key,\n                sum(total_price) as revenue\n            from\n                timestamp_df t\n            left join\n                transactions ta\n            on\n                ta.date <= t.timestamp + INTERVAL '{self.timedelta}'\n                and ta.date > t.timestamp\n            group by timestamp, customer_key\n        \"\"\").df().dropna()\n```", "```py\nclass CustomerRevenueTask(EntityTask):\n    # example of custom task: https://github.com/snap-stanford/relbench/blob/main/tutorials/custom_task.ipynb\n\n    task_type = TaskType.REGRESSION\n    entity_col = \"customer_key\"\n    entity_table = \"customers\"\n    time_col = \"timestamp\"\n    target_col = \"revenue\"\n    timedelta = pd.Timedelta(days=30) # how far we want to predict revenue into the future.\n    metrics = [r2, mae]\n    num_eval_timestamps = 40\n\n    def make_table(self, db: Database, timestamps: \"pd.Series[pd.Timestamp]\") -> Table:\n\n        timestamp_df = pd.DataFrame({\"timestamp\": timestamps})\n\n        transactions = db.table_dict[\"transactions\"].df\n\n        df = duckdb.sql(f\"\"\"\n            select\n                timestamp,\n                customer_key,\n                sum(total_price) as revenue\n            from\n                timestamp_df t\n            left join\n                transactions ta\n            on\n                ta.date <= t.timestamp + INTERVAL '{self.timedelta}'\n                and ta.date > t.timestamp\n            group by timestamp, customer_key\n        \"\"\").df().dropna()\n\n        print(df)\n\n        return Table(\n            df=df,\n            fkey_col_to_pkey_table={self.entity_col: self.entity_table},\n            pkey_col=None,\n            time_col=self.time_col,\n        )\n```", "```py\nfrom typing import List, Optional\nfrom sentence_transformers import SentenceTransformer\nfrom torch import Tensor\n\nclass GloveTextEmbedding:\n    def __init__(self, device: Optional[torch.device\n                                       ] = None):\n        self.model = SentenceTransformer(\n            \"sentence-transformers/average_word_embeddings_glove.6B.300d\",\n            device=device,\n        )\n\n    def __call__(self, sentences: List[str]) -> Tensor:\n        return torch.from_numpy(self.model.encode(sentences))\n```", "```py\nfrom torch_frame.config.text_embedder import TextEmbedderConfig\nfrom relbench.modeling.graph import make_pkey_fkey_graph\n\ntext_embedder_cfg = TextEmbedderConfig(\n    text_embedder=GloveTextEmbedding(device=device), batch_size=256\n)\n\ndata, col_stats_dict = make_pkey_fkey_graph(\n    db,\n    col_to_stype_dict=col_to_stype_dict,  # speficied column types\n    text_embedder_cfg=text_embedder_cfg,  # our chosen text encoder\n    cache_dir=os.path.join(\n        root_dir, f\"rel-ecomm_materialized_cache\"\n    ),  # store materialized graph for convenience\n)\n```"]