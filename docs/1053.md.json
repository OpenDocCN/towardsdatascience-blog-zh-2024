["```py\ntemperature (number)\n\nAmount of randomness injected into the response.\n\nDefaults to 1.0\\. Ranges from 0.0 to 1.0\\. Use temperature closer to 0.0 for \nanalytical / multiple choice, and closer to 1.0 for creative and \ngenerative tasks.\n\nNote that even with temperature of 0.0, the results will not be \nfully deterministic.\n```", "```py\nsequence = [\"<start>\"]\nwhile sequence[-1] != \"<end>\":\n  # Given the input context, and seq so far, append most likely next token\n  sequence += model(input, sequence)\nreturn \"\".join(sequence)\n```", "```py\n\"We can treat it as a matter of\" \n  [course (p=0.9) | principle (p=0.5)] | cause (p=0.2)]\"\n```", "```py\nscaled_logits = logits_tensor / temperature\nprobs = torch.softmax(scaled_logits, dim=-1)\n```", "```py\nts = [0.5, 1.0, 2.0, 4.0, 8.0]\nlogits = torch.tensor([3.123, 5.0, 3.234, 2.642, 2.466, 3.3532, 3.8, 2.911])\nprobs  = [torch.softmax(logits / t, dim=-1) for t in ts]\n```", "```py\n>>> [print(f' t={t}\\n l={(logits/t)}\\n p={p}\\n') for p,t in zip(probs, ts)]\n t=0.5\n l=tensor([6.2460, 10.000, 6.4680, 5.2840, 4.9320, 6.7064, 7.6000, 5.8220])\n p=tensor([0.0193, 0.8257, 0.0241, 0.0074, 0.0052, 0.0307, 0.0749, 0.0127])\n\n t=1.0\n l=tensor([3.1230, 5.0000, 3.2340, 2.6420, 2.4660, 3.3532, 3.8000, 2.9110])\n p=tensor([0.0723, 0.4727, 0.0808, 0.0447, 0.0375, 0.0911, 0.1424, 0.0585])\n\n t=2.0\n l=tensor([1.5615, 2.5000, 1.6170, 1.3210, 1.2330, 1.6766, 1.9000, 1.4555])\n p=tensor([0.1048, 0.2678, 0.1108, 0.0824, 0.0754, 0.1176, 0.1470, 0.0942])\n\n t=4.0\n l=tensor([0.7807, 1.2500, 0.8085, 0.6605, 0.6165, 0.8383, 0.9500, 0.7278])\n p=tensor([0.1169, 0.1869, 0.1202, 0.1037, 0.0992, 0.1238, 0.1385, 0.1109])\n\n t=8.0\n l=tensor([0.3904, 0.6250, 0.4042, 0.3302, 0.3083, 0.4191, 0.4750, 0.3639])\n p=tensor([0.1215, 0.1536, 0.1232, 0.1144, 0.1119, 0.1250, 0.1322, 0.1183])\n```", "```py\n>>> sum([0.9, 0.3, 0.3, 0.3]) # raw probabilities\n1.8 # dominated by first token\n>>> sum([0.8, 0.4, 0.4, 0.4]) # temperature-scaled probabilities\n2.0 # more likely overall outcome\n```", "```py\n>>> math.log(3)\n1.0986122886681098\n>>> math.log(0.99)\n-0.01005033585350145\n>>> math.log(0.98)\n-0.020202707317519466\n>>> math.log(0.0001)\n-9.210340371976182\n>>> math.log(0.000000000000000001)\n-41.44653167389282\n```", "```py\n# The initial candidate sequence is simply the start token ID with \n# a sequence score of 0\ncandidate_sequences = [\n  GeneratedSequence(tokenizer, start_token_id, end_token_id, 0.0)\n]\n\nfor i in tqdm.tqdm(range(max_length)):\n  # Temporary list to store candidates for the next generation step\n  next_step_candidates = []\n\n  # Iterate through all candidate sequences; for each, generate the next\n  # most likely tokens and add them to the next-step sequnce of candidates\n  for candidate in candidate_sequences:\n\n    # skip candidate sequences which have included the end-of-sequence token\n    if not candidate.has_ended():\n\n      # Build a tensor out of the candidate IDs; add a single batch dimension\n      gen_seq = torch.tensor(candidate.ids(), device=device).unsqueeze(0)\n\n      # Predict next token\n      output = model(input_ids=src_input_ids, decoder_input_ids=gen_seq)\n\n      # Extract logits from output\n      logits = output.logits[:, -1, :]\n\n      # Scale logits using temperature value\n      scaled_logits = logits / temperature\n\n      # Construct probability distribution against scaled \n      # logits through softmax activation function\n      probs = torch.softmax(scaled_logits, dim=-1)\n\n      # Select top k (beam_width) probabilities and IDs from the distribution\n      top_probs, top_ids = probs.topk(beam_width)\n\n      # For each of the top-k generated tokens, append to this \n      # candidate sequence, update its score, and append to the list of next \n      # step candidates\n      for i in range(beam_width):\n        # the new token ID\n        next_token_id = top_ids[:, i].item()\n\n        # log-prob of the above token\n        next_score = torch.log(top_probs[:, i]).item()\n\n        new_seq = deepcopy(candidate)\n\n        # Adds the new token to the end of this sequence, and updates its \n        # raw and normalized scores. Scores are normalized by sequence token \n        # length, to avoid penalizing longer sequences\n        new_seq.append(ScoredToken(next_token_id, next_score))\n\n        # Append the updated sequence to the next candidate sequence set\n        next_step_candidates.append(new_seq)\n    else:\n      # Append the canddiate sequence as-is to the next-step candidates\n      # if it already contains an end-of-sequence token\n      next_step_candidates.append(candidate)\n\n  # Sort the next-step candidates by their score, select the top-k \n  # (beam_width) scoring sequences and make them the new \n  # candidate_sequences list\n  next_step_candidates.sort()\n  candidate_sequences = list(reversed(next_step_candidates))[:beam_width]\n\n  # Break if all sequences in the heap end with the eos_token_id\n  if all(seq.has_ended() for seq in candidate_sequences):\n    break\n\nreturn candidate_sequences\n```", "```py\n$ python3 src/main.py --greedy --input ./wiki-fox.txt --prompt \"summarize the following document\"\n\ngreedy search generation results: \n[\nthe phrase is used in the annual Zaner-Bloser National Handwriting Competition.\nit is used for typing typewriters and keyboards, typing fonts. the phrase \nis used in the earliest known use of the phrase.\n]\n```", "```py\n$ python3 src/main.py --beam 4 --input ./wiki-fox.txt --prompt \"summarize the following document\"\n\n[lots of omitted output]\n\nbeam search (k=4, t=1.0) generation results:\n[\n \"the quick brown fox jumps over the lazy dog\" is an English-language pangram. \n the phrase is commonly used for touch-typing practice, typing typewriters and \n keyboards. it is used in the annual Zaner-Bloser National \n Handwriting Competition.\n]\n```", "```py\nbeginning beam search | k = 4 bos = 0 eos = 1 temp = 1.0 beam_width = 4\n0.0: [], next token probabilities:\n p:  0.30537632: ▁the\n p:  0.21197866: ▁\"\n p:  0.13339639: ▁phrase\n p:  0.13240208: ▁\n\nnext step candidates:\n -1.18621039: [the]\n -1.55126965: [\"]\n -2.01443028: [phrase]\n -2.02191186: []\n\n-1.1862103939056396: [the], next token probabilities:\n p:  0.61397356: ▁phrase\n p:  0.08461960: ▁\n p:  0.06939770: ▁\"\n p:  0.04978605: ▁term\n\n-1.5512696504592896: [\"], next token probabilities:\n p:  0.71881396: the\n p:  0.08922042: qui\n p:  0.05990228: The\n p:  0.03147057: a\n\n-2.014430284500122: [phrase], next token probabilities:\n p:  0.27810165: ▁used\n p:  0.26313403: ▁is\n p:  0.10535818: ▁was\n p:  0.03361856: ▁\n\n-2.021911859512329: [], next token probabilities:\n p:  0.72647911: earliest\n p:  0.19509122: a\n p:  0.02678721: '\n p:  0.00308457: s\n\nnext step candidates:\n -1.67401379: [the phrase]\n -1.88142237: [\"the]\n -2.34145740: [earliest]\n -3.29419887: [phrase used]\n -3.34952199: [phrase is]\n -3.65579963: [the]\n -3.65619993: [a]\n```", "```py\nnext step candidates:\n -15.39409454: [\"the quick brown fox jumps over the lazy dog\" is an English-language pangram. the phrase is commonly used for touch-typing practice, typing typewriters and keyboards. it is used in the annual Zaner-Bloser National Handwriting Competition.]\n -16.06867695: [\"the quick brown fox jumps over the lazy dog\" is an English-language pangram. the phrase is commonly used for touch-typing practice, testing typewriters and keyboards. it is used in the annual Zaner-Bloser National Handwriting Competition.]\n -16.10376084: [\"the quick brown fox jumps over the lazy dog\" is an English-language pangram. the phrase is commonly used for touch-typing practice, typing typewriters and keyboards. it is used in the annual Zaner-Bloser national handwriting competition.]\n```", "```py\n$ python3 src/main.py --beam 4 --temperature 4.0 --input ./wiki-fox.txt --prompt \"summarize the following document\"\n\n[lots of omitted output]\n\nbeam search (k=4, t=4.0) generation results:\n[\n \"the quick brown fox jumps over the lazy dog\" is an English-language pangram. \n it is commonly used for touch-typing practice, testing typewriters and \n computer keyboards. earliest known use of the phrase started with \"A\"\n]\n```", "```py\n$ python3 src/main.py --beam 4 --temperature 4.0 --input ./wiki-buffalo.txt --prompt \"summarize the linguistic construct in the following text\"\n\n[lots of omitted outputs]\n\nbeam search (k=4, t=4.0) generation results:\n[\n  \"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo \n  buffalo buffalo buffalo buffalo buffalo buffalo\n]\n```", "```py\nnext step candidates:\n  -361.66266489: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]\n  -362.13168168: [\"buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]\n  -362.22955942: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo.]\n  -362.60354519: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]\n  -363.03604889: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo,]\n  -363.07167459: [\"buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo]\n  -363.14155817: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo Buffalo]\n  -363.28574753: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo. the]\n  -363.35553551: [\"Buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo a]\n[more of the same]\n```", "```py\ntoken_counts = Counter(t.token_id for t in candidate)\n\n# For each of the top-k generated tokens, append to this candidate sequence,\n# update its score, and append to the list of next step candidates\nfor i in range(beam_width):\n  next_token_id = top_ids[:, i].item() # the new token ID\n  next_score = torch.log(top_probs[:, i]).item() # log-prob of the above token\n\n  # Optionally apply a token-specific score decay to repeated tokens\n  if decay_repeated and next_token_id in token_counts:\n    count = token_counts[next_token_id]\n    decay = 1 + math.log(count + 1)\n    next_score *= decay # inflate the score of the next sequence accordingly\n\n  new_seq = deepcopy(candidate)\n  new_seq.append(ScoredToken(next_token_id, next_score))\n```", "```py\n$ python3 src/main.py --decay --beam 4 --temperature 4.0 --input ./wiki-buffalo.txt --prompt \"summarize the linguistic construct in the following text\"\n\n[lots of omitted outputs]\n\nbeam search (k=4, t=4.0) generation results:\n[\n  \"Buffalo buffalo\" is grammatically correct sentence in English, often \n  presented as an example of how homophonies can be used to create complicated\n  language constructs through unpunctuated terms and sentences. it uses three \n  distinct meanings:An attributive noun (acting\n]\n```", "```py\nnext step candidates:\n -36.85023594: [\"Buffalo buffalo Buffalo]\n -37.23766947: [\"Buffalo buffalo\"]\n -37.31325269: [\"buffalo buffalo Buffalo]\n -37.45994210: [\"buffalo buffalo\"]\n -37.61866760: [\"Buffalo buffalo,\"]\n -37.73602080: [\"buffalo\" is]\n [omitted]\n\n-36.85023593902588: [\"Buffalo buffalo Buffalo], next token probabilities:\n p:  0.00728357: ▁buffalo\n p:  0.00166316: ▁Buffalo\n p:  0.00089072: \"\n p:  0.00066582: ,\"\n\n['▁buffalo'] count: 1 decay: 1.6931471805599454, score: -4.922133922576904, next: -8.33389717334955\n['▁Buffalo'] count: 1 decay: 1.6931471805599454, score: -6.399034023284912, next: -10.834506414832013\n-37.237669467926025: [\"Buffalo buffalo\"], next token probabilities:\n p:  0.00167652: ▁is\n p:  0.00076465: ▁was\n p:  0.00072227: ▁\n p:  0.00064367: ▁used\n\n-37.313252687454224: [\"buffalo buffalo Buffalo], next token probabilities:\n p:  0.00740433: ▁buffalo\n p:  0.00160758: ▁Buffalo\n p:  0.00091487: \"\n p:  0.00066765: ,\"\n\n['▁buffalo'] count: 1 decay: 1.6931471805599454, score: -4.905689716339111, next: -8.306054711921485\n['▁Buffalo'] count: 1 decay: 1.6931471805599454, score: -6.433023929595947, next: -10.892056328870039\n-37.45994210243225: [\"buffalo buffalo\"], next token probabilities:\n p:  0.00168198: ▁is\n p:  0.00077098: ▁was\n p:  0.00072504: ▁\n p:  0.00065945: ▁used\n\nnext step candidates:\n -43.62870741: [\"Buffalo buffalo\" is]\n -43.84772754: [\"buffalo buffalo\" is]\n -43.87371445: [\"Buffalo buffalo Buffalo\"]\n -44.16472149: [\"Buffalo buffalo Buffalo,\"]\n -44.30998302: [\"buffalo buffalo Buffalo\"]\n```"]