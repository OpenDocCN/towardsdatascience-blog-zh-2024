- en: 'Flash Attention (Fast and Memory-Efficient Exact Attention with IO-Awareness):
    A Deep Dive'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b?source=collection_archive---------3-----------------------#2024-05-29](https://towardsdatascience.com/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b?source=collection_archive---------3-----------------------#2024-05-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flash attention is power optimization transformer attention mechanism that provides
    15% efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@anishdubey?source=post_page---byline--724af489997b--------------------------------)[![Anish
    Dubey](../Images/f85f17fb79718c819b4bd1c9a16338a7.png)](https://medium.com/@anishdubey?source=post_page---byline--724af489997b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--724af489997b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--724af489997b--------------------------------)
    [Anish Dubey](https://medium.com/@anishdubey?source=post_page---byline--724af489997b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--724af489997b--------------------------------)
    ·7 min read·May 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcd08ecda3389e4ce550e4a823d5c333.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [sander traa](https://unsplash.com/@sandertraa?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/a-field-with-trees-and-mountains-in-the-background-KV2giR3tbX4?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Flash attention is a power optimization transformer attention mechanism which
    provides 15% efficiency in terms of wall-clock speed with no approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given transformer models are slow and memory hungry on long sequences (time
    and memory complexity is quadratic in nature), flash attention([paper](https://arxiv.org/pdf/2205.14135))
    provides a 15% end-to-end wall-clock speedup on BERT-large, 3x speed on GPT-2.
  prefs: []
  type: TYPE_NORMAL
- en: Considering, enormous amount of energy consumed in training these large models,
    Flash attention with software and hardware optimization is able to provide 15%
    efficiency which is a huge win in terms of improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Below, discussion helps to explain some of the basic concepts behind flash attention
    and how it is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Basic concepts around compute & memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we dive deeper into compute and memory, let’s revisit them:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Compute?
  prefs: []
  type: TYPE_NORMAL
- en: Time spent on your GPU computing actual floating point operations (FLOPS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Memory?
  prefs: []
  type: TYPE_NORMAL
- en: Time spent transferring tensors within a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, we want our gCPU to be performing matrix multiplication all the time
    and not restricted by memory. But in reality, compute have made more progress
    as compared to memory and we are in a world where gCPU sits idle waiting for data
    to be loaded. This is usually called **memory bound** operation. Refer below on
    illustrative diagram depicting this. Matrix multiplication is considered compute
    and memory is storing the data (considering it as a warehouse). Compute need data
    to process and memory bandwidth has to support that operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcc27ebfc81919bdb9d016a9290180ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from [https://horace.io/brrr_intro.html](https://horace.io/brrr_intro.html)
  prefs: []
  type: TYPE_NORMAL
- en: What is Memory hierarchy ?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The A100 GPU has **40–80GB** of high bandwidth memory with a bandwidth of **1.5–2.0
    TB/s** and **192KB** of on-chip SRAM with each 108 streaming multiprocessors with
    bandwidth estimated around **19TB/s.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b566bc7147d9ac2cc580a884d4259fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem with self attention architecture ?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the above context in mind, self attention architecture is **memory-bound.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad69afb8ec7fde9260e39d68b245318e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Looking at attention math, it is a softmax operation which causes the memory-bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative evidence: As you can see below, operations like softmax, dropout,
    masking are taking majority of the time as compared to Matrix multiplication (Matmul)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/383bf678304f14ad24715ef6a2c2d324.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
  prefs: []
  type: TYPE_NORMAL
- en: Why does softmax become a memory bound operation ?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scale at which it operates is our biggest bottleneck. In the below diagram
  prefs: []
  type: TYPE_NORMAL
- en: N -> number of tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: d -> number of embedding dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When Query and Key’ are multiplied, the attention matrix explodes to N * N
    which takes a lot of memory. For reference (d ~128; N ~128k tokens; google gemini:
    ~1 million tokens)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/14c37c8a0960f8b4efef2858021b8a24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo from [FlashAttention — Tri Dao | Stanford MLSys #67](https://www.youtube.com/watch?v=gMOAud7hZg4&ab_channel=StanfordMLSysSeminars)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Algorithm] How is self attention implemented ?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below is the algorithm of implementing self attention mechanism
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8a4ac1211ce636bae0007eb2f0462a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the above section, transferring information to HBM (write S to HBM)
    and then loading back from HBM to gCPU to compute softmax and then writing back
    to HBM is a lot of information traveling making it **memory-bound operation.**
  prefs: []
  type: TYPE_NORMAL
- en: '[Matrix multiplication] How is self attention implemented ?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Along with the diagram, below steps help explain how self attention is computed
    through matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have simplified this. In practice, each token is added with positional encoding
    to generate embeddings to feed into a linear layer to generate <key, query and
    value>. For illustration I used a dimension of 3 (generally it ranges from 64–128).
    This is standard transformer architecture input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Key -> Key’ (transpose) is computed, and multiplied with Query to give QK’ which
    is N*N. This contains the attention of each token with the rest of the tokens.
    Below diagram shows the relationship as well. Since these are tokens and we need
    to compute the importance of each token with respect to each other, softmax operation
    is applied row-wise to normalize it from 0 -1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This step **requires movement to HBM and is the most expensive operation** as
    we discussed. Entire flash attention paper is how to optimize this process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Step 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Softmax(QK’) * V is computed as the final output matrix. Dimension here is same
    as input embeddings of Key, query and value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final row in the output matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1*5 means, the embedding of “this” should be changed to incorporate relations
    with other tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2*5 means, the embedding of “is” should be changed to incorporate relations
    with other tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same as above for rest of the other rows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/eaefc012968e0a6a8cfdc38a808b28e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo by the Author: Illustrative diagram of how self attention mechanism works'
  prefs: []
  type: TYPE_NORMAL
- en: Basic idea behind the flash attention paper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Basic idea is explained through the below diagram where blocks of key, query
    and value are propagated from HBM to SRAM and through some mathematical tricks
    (explained below), the computation done here is not an approximate but actual
    correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: With this implementation, paper is able to reduce the wall-speed time by accessing
    information in blocks without sacrificing correctness.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d12c7f05ad017b1df7db1f66667f067.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo from [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
  prefs: []
  type: TYPE_NORMAL
- en: 'Algorithm behind the paper: How is Flash attention implemented ?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the most complex part of the paper. Let’s break this problem into sub-aspects
    and dive deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Below diagram breaks the matrix into blocks and how each block is used to compute
    partial softmax and then correct softmax.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initial input: Token: This is flash attention paper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Key: 4 (tokens) X 3(dimensions), Query: 4 (tokens) X 3(dimensions) and Value:
    4 (tokens) X 3(dimensions)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/bb787277f6cd09c9f7741daa795bc448.png)'
  prefs: []
  type: TYPE_IMG
- en: Image modified by author. Original image from [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)
  prefs: []
  type: TYPE_NORMAL
- en: Step 0
  prefs: []
  type: TYPE_NORMAL
- en: Assume memory is 24 bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SRAM will be divided into 4 blocks (Query, Key, Value and output matrix)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query, Key, Value, Output will get = 6 bytes each to store their info (12 bytes/4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each dimension is 3 since each embedding can not be broken, so
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Query: 6 bytes/ 3 (dimension) = 2\. Same for value, key and output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, [M/4d] gives the size of each block. In this case, the block size is
    2\. It means 2 rows can be fetched into SRAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general sense, Block size is [M/4d] and # of blocks is [N*4D/M]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1 & 2: Adding a table below which illustrates steps 1 and 2 on how flash
    attention works and compare memory and computation aspect of it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f3283d51b3061c764359ef361e8e74a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo by the Author: Step by step break-down of memory & computation usage
    in Flash attention'
  prefs: []
  type: TYPE_NORMAL
- en: Below diagram helps visualize matrix multiplication (block by block) used in
    flash attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e22bba7a834d0f89407473bbae1181ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo by the Author: Illustrative diagram of how flash attention mechanism
    works'
  prefs: []
  type: TYPE_NORMAL
- en: What is the mathematical aspect of softmax ?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most critical aspects of the paper on how breaking down matrices
    still results in computing softmax accuracy. Leaving the mathematical example
    below on how to show two different matrices can be clubbed to compute softmax
    again.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs: []
  type: TYPE_NORMAL
- en: This is the beautiful property of exponents which is leveraged here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each softmax is computed individually but along with this maximum value of the
    row is stored along with the summed exponent value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When merging with another matrix , we need to check how much max differs with
    the global max of 2 matrices. And because of the exponent, both numerator and
    denominator are adjusted with e^(current_max — global_max) to incorporate this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logic is quite complex and hence leaving an example below to go through. Once
    familiarized with an example, the above intuition will make a lot of sense.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa60177ed659c487bcb14b653e1919c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Photo by the Author: Example to demonstrate how breaking matrix into sub-components
    and eventually combining them to compute softmax'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at complexity analysis to get a sense of how things changed
  prefs: []
  type: TYPE_NORMAL
- en: Self attention
  prefs: []
  type: TYPE_NORMAL
- en: While computing S = QK’ it becomes a N*N matrix which needs to be propagated
    back to HRAM and then pulled back from HRAM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence O(N*N + N*N) = O(N*N) is HBM access
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flash attention
  prefs: []
  type: TYPE_NORMAL
- en: 'Outer loop: Key and Query will be accessed O(Nd) times'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inner loop: Only O(Nd/M) will be needed to load from HBM since operating on
    blocks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Overall: O(N*N*d*d/M)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practically, d is much smaller than M. d ranges from (64–128) while M ranges
    from 100 KB and hence HBM access is optimized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started with the objective of optimizing HBM access and with this complexity
    analysis, we see the paper has optimized the **HBM access by (d*d/M) factor with
    no approximation.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such a complex paper with huge improvement in efficiency. I hope the above explanation
    gives some intuition on how flash attention optimizes and improves the performance.
    I haven’t covered block sparse flash attention, how does this compare with other
    optimization techniques, forwards pass optimization etc. Hopefully to cover it
    in a future post.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Paper: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tri Dao’s talk: [FlashAttention — Tri Dao | Stanford MLSys #67](https://www.youtube.com/watch?v=gMOAud7hZg4&ab_channel=StanfordMLSysSeminars)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Medium article: [https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
