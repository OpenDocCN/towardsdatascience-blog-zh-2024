<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Evaluation-Driven Development for agentic applications using PydanticAI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Evaluation-Driven Development for agentic applications using PydanticAI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluation-driven-development-for-agentic-applications-using-pydanticai-d9293ac81d91?source=collection_archive---------0-----------------------#2024-12-21">https://towardsdatascience.com/evaluation-driven-development-for-agentic-applications-using-pydanticai-d9293ac81d91?source=collection_archive---------0-----------------------#2024-12-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="03af" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An open-source, model-agnostic agentic framework that supports dependency injection</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://lakshmanok.medium.com/?source=post_page---byline--d9293ac81d91--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lak Lakshmanan" class="l ep by dd de cx" src="../Images/9faaaf72d600f592cbaf3e9089cbb913.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*TveVoapl-TEk-jBTrbis8w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d9293ac81d91--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://lakshmanok.medium.com/?source=post_page---byline--d9293ac81d91--------------------------------" rel="noopener follow">Lak Lakshmanan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d9293ac81d91--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5f66" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Ideally, you can evaluate agentic applications even as you are developing them, instead of evaluation being an afterthought. For this to work, though, you need to be able to mock both internal and external dependencies of the agent you are developing. I am extremely excited by PydanticAI because it supports dependency injection from the ground up. It is the first framework that has allowed me to build agentic applications in an evaluation-driven manner.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng nh"><img src="../Images/6786f9e4942182ffc3b8b875a045bfd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*CFnOgxKkqvh-y6YzIt6kiw.png"/></div><figcaption class="np nq nr nf ng ns nt bf b bg z dx">Image of Krakow Cloth Hall, generated using Google Imagen by the author. This building was built in phases over the centuries, with improvements based on where the current building was falling short. Evaluation-driven development, in other words.</figcaption></figure><p id="82ce" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I’ll talk about the core challenges and demonstrate developing a simple agent in an evaluation-driven way using PydanticAI.</p><h2 id="334c" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">Challenges when developing GenAI applications</h2><p id="6015" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">Like many GenAI developers, I’ve been waiting for an agentic framework that supports the full development lifecycle. Each time a new framework comes along, I try it out hoping that this will be the One — see, for example, my articles about <a class="af ou" rel="noopener" target="_blank" href="/building-an-ai-assistant-with-dspy-2e1e749a1a95">DSPy</a>, <a class="af ou" rel="noopener" target="_blank" href="/four-approaches-to-build-on-top-of-generative-ai-foundational-models-43c1a64cffd5">Langchain</a>, <a class="af ou" href="https://www.linkedin.com/pulse/how-implement-genai-agent-using-autogen-langgraph-lakshmanan-cwx4c/" rel="noopener ugc nofollow" target="_blank">LangGraph, and Autogen</a>.</p><p id="ca68" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I find that there are core challenges that a software developer faces when developing an LLM-based application. These challenges are typically not blockers if you are building a simple PoC with GenAI, but they will come to bite you if you are building LLM-powered applications in production.</p><p id="1505" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What challenges?</p><p id="d665" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(1) <strong class="ml fr">Non-determinism</strong>: Unlike most software APIs, calls to an LLM with the exact same input could return different outputs each time. How do you even begin to test such an application?</p><p id="62db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(2) <strong class="ml fr">LLM limitations</strong>: Foundational models like GPT-4, Claude, and Gemini are limited by their training data (e.g., no access to enterprise confidential information), capability (e.g., you can not invoke enterprise APIs and databases), and can not plan/reason.</p><p id="5334" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(3) <strong class="ml fr">LLM flexibility</strong>: Even if you decide to stick to LLMs from a single provider such as Anthropic, you may find that you need a different LLM for each step — perhaps one step of your workflow needs a low-latency small language model (Haiku), another requires great code-generation capability (Sonnet), and a third step requires excellent contextual awareness (Opus).</p><p id="1b8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(4) <strong class="ml fr">Rate of Change:</strong> GenAI technologies are moving fast. Recently, many of the improvements have come about in foundational model capabilities. No longer are the foundational models just generating text based on user prompts. They are now multimodal, can generate structured outputs, and can have memory. Yet, if you try to build in an LLM-agnostic way, you often lose the low-level API access that will turn on these features.</p><p id="c31b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To help address the first problem, of non-determinism, your software testing needs to incorporate an evaluation framework. You will never have software that works 100%; instead, you will need to be able to design around software that is x% correct, build guardrails and human oversight to catch the exceptions, and monitor the system in real-time to catch regressions. Key to this capability is <strong class="ml fr">evaluation-driven development</strong> (my term), an extension of test-driven development in software.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="nf ng ov"><img src="../Images/9db9ba18e95ae10febbe505241b7d65a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUsAyLLP11Zj0iNQm71uwg.png"/></div></div><figcaption class="np nq nr nf ng ns nt bf b bg z dx">Evaluation-driven development. sketch by author.</figcaption></figure><p id="7804" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The current workaround for all the LLM limitations in Challenge #2 is to use <strong class="ml fr">agentic architectures</strong> like RAG, provide the LLM access to tools, and employ patterns like Reflection, ReACT and Chain of Thought. So, your framework will need to have the ability to orchestrate agents. However, evaluating agents that can call external tools is hard. You need to be able to <strong class="ml fr">inject proxies for these external dependencies</strong> so that you can test them individually, and evaluate as you build.</p><p id="4738" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To handle challenge #3, an agent needs to be able to invoke the capabilities of different types of foundational models. Your agent framework needs to be <strong class="ml fr">LLM-agnostic</strong> at the granularity of a single step of an agentic workflow. To address the rate of change consideration (challenge #4), you want to retain the ability to make <strong class="ml fr">low-level access </strong>to the foundational model APIs and to strip out sections of your codebase that are no longer necessary.</p><p id="aca1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Is there a framework that meets all these criteria? For the longest time, the answer was no. The closest I could get was to use Langchain, pytest’s dependency injection, and deepeval with something like this (full example is <a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/eval_weather_agent.py" rel="noopener ugc nofollow" target="_blank">here</a>):</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="16b2" class="pe nv fq pb b bg pf pg l ph pi">from unittest.mock import patch, Mock<br/>from deepeval.metrics import GEval<br/><br/>llm_as_judge = GEval(<br/>    name="Correctness",<br/>    criteria="Determine whether the actual output is factually correct based on the expected output.",<br/>    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],<br/>    model='gpt-3.5-turbo'<br/>)<br/><br/><br/>@patch('lg_weather_agent.retrieve_weather_data', Mock(return_value=chicago_weather))<br/>def eval_query_rain_today():<br/>    input_query = "Is it raining in Chicago?"<br/>    expected_output = "No, it is not raining in Chicago right now."<br/>    result = lg_weather_agent.run_query(app, input_query)<br/>    actual_output = result[-1]<br/>    <br/>    print(f"Actual: {actual_output}   Expected: {expected_output}")<br/>    test_case = LLMTestCase(<br/>        input=input_query,<br/>        actual_output=actual_output,<br/>        expected_output=expected_output<br/>    )<br/><br/>    llm_as_judge.measure(test_case)<br/>    print(llm_as_judge.score)</span></pre><p id="ed29" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Essentially, I’d construct a Mock object (chicago_weather in the above example) for every LLM call and patch the call to the LLM (retrieve_weather_data in the above example) with the hardcoded object whenever I needed to mock that part of the agentic workflow. The dependency injection is all over the place, you need a bunch of hardcoded objects, and the calling workflow becomes extremely hard to follow. Note that if you don’t have dependency injection, there is no way to test a function like this: obviously, the external service will return the current weather and there is no way to determine what the correct answer is for a question such as whether or not it’s raining right now.</p><p id="f783" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pj">So … is there an agent framework that supports dependency injection, is Pythonic, provides low-level access to LLMs, is model-agnostic, supports building it one eval-at-a-time, and is easy to use and follow?</em></p><p id="2ffc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Almost. <a class="af ou" href="https://ai.pydantic.dev/" rel="noopener ugc nofollow" target="_blank">PydanticAI</a> meets the first 3 requirements; the fourth (low-level LLM access) is not possible, but the design does not preclude it. In the rest of this article, I’ll show you how to use it to develop an agentic application in an evaluation-driven way.</p><h2 id="e30c" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">1. Your first PydanticAI Application</h2><p id="e23a" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">Let’s start out by building a simple PydanticAI application. This will use an LLM to answer questions about mountains:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="826f" class="pe nv fq pb b bg pf pg l ph pi">    agent = llm_utils.agent()<br/>    question = "What is the tallest mountain in British Columbia?"<br/>    print("&gt;&gt; ", question)<br/>    answer = agent.run_sync(question)<br/>    print(answer.data)<br/></span></pre><p id="f96c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the code above, I’m creating an agent (I’ll show you how, shortly) and then calling run_sync passing in the user prompt, and getting back the LLM’s response. run_sync is a way to have the agent invoke the LLM and wait for the response. Other ways are to run the query asynchronously, or to stream its response. (<a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/pydantic_ai_mountains/1_zero_shot.py" rel="noopener ugc nofollow" target="_blank">Full code</a> is here if you want to follow along).</p><p id="51ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Run the code above, and you will get something like:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="1169" class="pe nv fq pb b bg pf pg l ph pi">&gt;&gt;  What is the tallest mountain in British Columbia?<br/>The tallest mountain in British Columbia is **Mount Robson**, at 3,954 metres (12,972 feet).</span></pre><p id="6cf0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To create the agent, create a model and then tell the agent to use that Model for all its steps.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="1629" class="pe nv fq pb b bg pf pg l ph pi">import pydantic_ai<br/>from pydantic_ai.models.gemini import GeminiModel<br/><br/>def default_model() -&gt; pydantic_ai.models.Model:<br/>    model = GeminiModel('gemini-1.5-flash', api_key=os.getenv('GOOGLE_API_KEY'))<br/>    return model<br/><br/>def agent() -&gt; pydantic_ai.Agent:<br/>    return pydantic_ai.Agent(default_model())</span></pre><p id="a6ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The idea behind default_model() is to use a relatively inexpensive but fast model like Gemini Flash as the default. You can then change the model used in specific steps as necessary by passing in a different model to run_sync()</p><p id="25d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">PydanticAI model support <a class="af ou" href="https://ai.pydantic.dev/api/models/base/#pydantic_ai.models" rel="noopener ugc nofollow" target="_blank">looks sparse</a>, but the most commonly used models — the current frontier ones from OpenAI, Groq, Gemini, Mistral, Ollama, and Anthropic — are all supported. Through Ollama, you can get access to Llama3, Starcoder2, Gemma2, and Phi3. Nothing significant seems to be missing.</p><h2 id="1934" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">2. Pydantic with structured outputs</h2><p id="8a87" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">The example in the previous section returned free-form text. In most agentic workflows, you’ll want the LLM to return structured data so that you can use it directly in programs.</p><p id="ec22" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Considering that this API is from Pydantic, returning structured output is quite straightforward. Just define the desired output as a dataclass (full code is <a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/pydantic_ai_mountains/2_zero_shot_structured.py" rel="noopener ugc nofollow" target="_blank">here</a>):</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="16d1" class="pe nv fq pb b bg pf pg l ph pi">from dataclasses import dataclass<br/><br/>@dataclass<br/>class Mountain:<br/>    name: str<br/>    location: str<br/>    height: float</span></pre><p id="66fb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When you create the Agent, tell it the desired output type:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="72ec" class="pe nv fq pb b bg pf pg l ph pi">agent = Agent(llm_utils.default_model(),<br/>                  result_type=Mountain,<br/>                  system_prompt=(<br/>                      "You are a mountaineering guide, who provides accurate information to the general public.",<br/>                      "Provide all distances and heights in meters",<br/>                      "Provide location as distance and direction from nearest big city",<br/>                  ))</span></pre><p id="6d97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note also the use of the system prompt to specify units etc.</p><p id="ca2a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Running this on three questions, we get:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="b697" class="pe nv fq pb b bg pf pg l ph pi">&gt;&gt;  Tell me about the tallest mountain in British Columbia?<br/>Mountain(name='Mount Robson', location='130km North of Vancouver', height=3999.0)<br/>&gt;&gt;  Is Mt. Hood easy to climb?<br/>Mountain(name='Mt. Hood', location='60 km east of Portland', height=3429.0)<br/>&gt;&gt;  What's the tallest peak in the Enchantments?<br/>Mountain(name='Mount Stuart', location='100 km east of Seattle', height=3000.0)</span></pre><p id="72bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But how good is this agent? Is the height of Mt. Robson correct? Is Mt. Stuart really the tallest peak in the Enchantments? All of this information could have been hallucinated!</p><p id="e969" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There is no way for you to know how good an agentic application is unless you evaluate the agent against reference answers. You can not just “eyeball it”. Unfortunately, this is where a lot of LLM frameworks fall short — they make it really hard to evaluate as you develop the LLM application.</p><h2 id="a64b" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">3. Evaluate against reference answers</h2><p id="2d88" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">It is when you start to evaluate against reference answers that PydanticAI starts to show its strengths. Everything is quite Pythonic, so you can build custom evaluation metrics quite simply.</p><p id="45d7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, this is how we will evaluate a returned Mountain object on three criteria and create a composite score (<a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/pydantic_ai_mountains/3_eval_against_reference.py" rel="noopener ugc nofollow" target="_blank">full code </a>is here):</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="4407" class="pe nv fq pb b bg pf pg l ph pi">def evaluate(answer: Mountain, reference_answer: Mountain) -&gt; Tuple[float, str]:<br/>    score = 0<br/>    reason = []<br/>    if reference_answer.name in answer.name:<br/>        score += 0.5<br/>        reason.append("Correct mountain identified")<br/>        if reference_answer.location in answer.location:<br/>            score += 0.25<br/>            reason.append("Correct city identified")<br/>        height_error = abs(reference_answer.height - answer.height)<br/>        if height_error &lt; 10:<br/>            score += 0.25 * (10 - height_error)/10.0<br/>        reason.append(f"Height was {height_error}m off. Correct answer is {reference_answer.height}")<br/>    else:<br/>        reason.append(f"Wrong mountain identified. Correct answer is {reference_answer.name}")<br/><br/>    return score, ';'.join(reason)</span></pre><p id="4a04" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we can run this on a dataset of questions and reference answers:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="32ac" class="pe nv fq pb b bg pf pg l ph pi">    questions = [<br/>        "Tell me about the tallest mountain in British Columbia?",<br/>        "Is Mt. Hood easy to climb?",<br/>        "What's the tallest peak in the Enchantments?"<br/>    ]<br/><br/>    reference_answers = [<br/>        Mountain("Robson", "Vancouver", 3954),<br/>        Mountain("Hood", "Portland", 3429),<br/>        Mountain("Dragontail", "Seattle", 2690)<br/>    ]<br/><br/>    total_score = 0<br/>    for l_question, l_reference_answer in zip(questions, reference_answers):<br/>        print("&gt;&gt; ", l_question)<br/>        l_answer = agent.run_sync(l_question)<br/>        print(l_answer.data)<br/>        l_score, l_reason = evaluate(l_answer.data, l_reference_answer)<br/>        print(l_score, ":", l_reason)<br/>        total_score += l_score<br/><br/>    avg_score = total_score / len(questions)</span></pre><p id="00a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Running this, we get:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="7731" class="pe nv fq pb b bg pf pg l ph pi">&gt;&gt;  Tell me about the tallest mountain in British Columbia?<br/>Mountain(name='Mount Robson', location='130 km North-East of Vancouver', height=3999.0)<br/>0.75 : Correct mountain identified;Correct city identified;Height was 45.0m off. Correct answer is 3954<br/>&gt;&gt;  Is Mt. Hood easy to climb?<br/>Mountain(name='Mt. Hood', location='60 km east of Portland, OR', height=3429.0)<br/>1.0 : Correct mountain identified;Correct city identified;Height was 0.0m off. Correct answer is 3429<br/>&gt;&gt;  What's the tallest peak in the Enchantments?<br/>Mountain(name='Dragontail Peak', location='14 km east of Leavenworth, WA', height=3008.0)<br/>0.5 : Correct mountain identified;Height was 318.0m off. Correct answer is 2690<br/>Average score: 0.75</span></pre><p id="a636" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mt. Robson’s height is 45m off; Dragontail peak’s height was 318m off. How would you fix this?</p><p id="7e9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That’s right. You’d use a RAG architecture or arm the agent with a tool that provides the correct height information. Let’s use the latter approach and see how to do it with Pydantic.</p><p id="5ea6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note how evaluation-driven development shows us the path forward to improve our agentic application.</p><h2 id="821e" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">4a. Using a tool</h2><p id="8439" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">PydanticAI supports several ways to provide tools to an agent. Here, I annotate a function to be called whenever it needs the height of a mountain (<a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/pydantic_ai_mountains/4_use_tool.py" rel="noopener ugc nofollow" target="_blank">full code here</a>):</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="f667" class="pe nv fq pb b bg pf pg l ph pi">   agent = Agent(llm_utils.default_model(),<br/>                  result_type=Mountain,<br/>                  system_prompt=(<br/>                      "You are a mountaineering guide, who provides accurate information to the general public.",<br/>                      "Use the provided tool to look up the elevation of many mountains."<br/>                      "Provide all distances and heights in meters",<br/>                      "Provide location as distance and direction from nearest big city",<br/>                  ))<br/>    @agent.tool<br/>    def get_height_of_mountain(ctx: RunContext[Tools], mountain_name: str) -&gt; str:<br/>        return ctx.deps.elev_wiki.snippet(mountain_name)</span></pre><p id="08ac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The function, though, does something strange. It pulls an object called elev_wiki out of the run-time context of the agent. This object is passed in when we call run_sync:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="40f3" class="pe nv fq pb b bg pf pg l ph pi">class Tools:<br/>    elev_wiki: wikipedia_tool.WikipediaContent<br/>    def __init__(self):<br/>        self.elev_wiki = OnlineWikipediaContent("List of mountains by elevation")<br/><br/>tools = Tools()  # Tools or FakeTools<br/><br/>l_answer = agent.run_sync(l_question, deps=tools) # note how we are able to inject</span></pre><p id="5335" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Because the Runtime context can be passed into every agent invocation or tool call , we can use it to do dependency injection in PydanticAI. You’ll see this in the next section.</p><p id="a87f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The wiki itself just queries Wikipedia online (<a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/pydantic_ai_mountains/wikipedia_tool.py" rel="noopener ugc nofollow" target="_blank">code here</a>) and extracts the contents of the page and passes the appropriate mountain information to the agent:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="75f2" class="pe nv fq pb b bg pf pg l ph pi">import wikipedia<br/><br/>class OnlineWikipediaContent(WikipediaContent):<br/>    def __init__(self, topic: str):<br/>        print(f"Will query online Wikipedia for information on {topic}")<br/>        self.page = wikipedia.page(topic)<br/><br/>    def url(self) -&gt; str:<br/>        return self.page.url<br/><br/>    def html(self) -&gt; str:<br/>        return self.page.html()</span></pre><p id="4cde" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Indeed, when we run it, we get correct heights now:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="73f1" class="pe nv fq pb b bg pf pg l ph pi">Will query online Wikipedia for information on List of mountains by elevation<br/>&gt;&gt;  Tell me about the tallest mountain in British Columbia?<br/>Mountain(name='Mount Robson', location='100 km west of Jasper', height=3954.0)<br/>0.75 : Correct mountain identified;Height was 0.0m off. Correct answer is 3954<br/>&gt;&gt;  Is Mt. Hood easy to climb?<br/>Mountain(name='Mt. Hood', location='50 km ESE of Portland, OR', height=3429.0)<br/>1.0 : Correct mountain identified;Correct city identified;Height was 0.0m off. Correct answer is 3429<br/>&gt;&gt;  What's the tallest peak in the Enchantments?<br/>Mountain(name='Mount Stuart', location='Cascades, Washington, US', height=2869.0)<br/>0 : Wrong mountain identified. Correct answer is Dragontail<br/>Average score: 0.58</span></pre><h2 id="1177" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">4b. Dependency injecting a mock service</h2><p id="ae68" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">Waiting for the API call to Wikipedia each time during development or testing is a bad idea. Instead, we will want to mock the Wikipedia response so that we can develop quickly and be guaranteed of the result we are going to get.</p><p id="c1e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Doing that is very simple. We create a Fake counterpart to the Wikipedia service:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="f732" class="pe nv fq pb b bg pf pg l ph pi">class FakeWikipediaContent(WikipediaContent):<br/>    def __init__(self, topic: str):<br/>        if topic == "List of mountains by elevation":<br/>            print(f"Will used cached Wikipedia information on {topic}")<br/>            self.url_ = "https://en.wikipedia.org/wiki/List_of_mountains_by_elevation"<br/>            with open("mountains.html", "rb") as ifp:<br/>                self.html_ = ifp.read().decode("utf-8")<br/><br/>    def url(self) -&gt; str:<br/>        return self.url_<br/><br/>    def html(self) -&gt; str:<br/>        return self.html_</span></pre><p id="0c1b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then, inject this fake object into the runtime context of the agent during development:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="395d" class="pe nv fq pb b bg pf pg l ph pi">class FakeTools:<br/>    elev_wiki: wikipedia_tool.WikipediaContent<br/>    def __init__(self):<br/>        self.elev_wiki = FakeWikipediaContent("List of mountains by elevation")<br/><br/>tools = FakeTools()  # Tools or FakeTools<br/><br/>l_answer = agent.run_sync(l_question, deps=tools) # note how we are able to inject</span></pre><p id="0021" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This time when we run, the evaluation uses the cached wikipedia content:</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="d5b3" class="pe nv fq pb b bg pf pg l ph pi">Will used cached Wikipedia information on List of mountains by elevation<br/>&gt;&gt;  Tell me about the tallest mountain in British Columbia?<br/>Mountain(name='Mount Robson', location='100 km west of Jasper', height=3954.0)<br/>0.75 : Correct mountain identified;Height was 0.0m off. Correct answer is 3954<br/>&gt;&gt;  Is Mt. Hood easy to climb?<br/>Mountain(name='Mt. Hood', location='50 km ESE of Portland, OR', height=3429.0)<br/>1.0 : Correct mountain identified;Correct city identified;Height was 0.0m off. Correct answer is 3429<br/>&gt;&gt;  What's the tallest peak in the Enchantments?<br/>Mountain(name='Mount Stuart', location='Cascades, Washington, US', height=2869.0)<br/>0 : Wrong mountain identified. Correct answer is Dragontail<br/>Average score: 0.58</span></pre><p id="64bc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Look carefully at the above output — there are different errors from the zero-shot example. In Section #2, the LLM picked Vancouver as the closest city to Mt. Robson and Dragontail as the tallest peak in the Enchantments. Those answers happened to be correct. Now, it picks Jasper and Mt. Stuart. We need to do more work to fix these errors — but evaluation-driven development at least gives us a direction of travel.</p><h2 id="f4d0" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">Current Limitations</h2><p id="ccb9" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">PydanticAI is very new. There are a couple of places where it could be improved:</p><ul class=""><li id="863c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pk pl pm bk">There is no low-level access to the model itself. For example, different foundational models support context caching, prompt caching, etc. The model abstraction in PydanticAI doesn’t provide a way to set these on the model. Ideally, we can figure out a kwargs way of doing such settings.</li><li id="6dd4" class="mj mk fq ml b go pn mn mo gr po mq mr ms pp mu mv mw pq my mz na pr nc nd ne pk pl pm bk">The need to create two versions of agent dependencies, one real and one fake, is quite common. It would be good if we were able to annoate a tool or provide a simple way to switch between the two types of services across the board.</li><li id="7d47" class="mj mk fq ml b go pn mn mo gr po mq mr ms pp mu mv mw pq my mz na pr nc nd ne pk pl pm bk">During development, you don’t need logging as much. But when you go to run the agent, you will usually want to log the prompts and responses. Sometimes, you will want to log the intermediate responses. The way to do this seems to be a commercial product called Logfire. An OSS, cloud-agnostic logging framework that integrates with the PydanticAI library would be ideal.</li></ul><p id="f91a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is possible that these already exist and I missed them, or perhaps they will have been implemented by the time you are reading this article. In either case, leave a comment for future readers.</p><p id="004a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Overall, I like PydanticAI — it offers a very clean and Pythonic way to build agentic applications in an evaluation-driven manner.</p><h2 id="24f8" class="nu nv fq bf nw nx ny nz oa ob oc od oe ms of og oh mw oi oj ok na ol om on oo bk">Suggested next steps:</h2><ol class=""><li id="680e" class="mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne ps pl pm bk">This is one of those blog posts where you will benefit from actually running the examples because it describes a process of development as well as a new library. This GitHub repo contains the PydanticAI example I walked through in this post: <a class="af ou" href="https://github.com/lakshmanok/lakblogs/tree/main/pydantic_ai_mountains" rel="noopener ugc nofollow" target="_blank">https://github.com/lakshmanok/lakblogs/tree/main/pydantic_ai_mountains</a> Follow the instructions in the README to try it out.</li><li id="8962" class="mj mk fq ml b go pn mn mo gr po mq mr ms pp mu mv mw pq my mz na pr nc nd ne ps pl pm bk">Pydantic AI documentation: <a class="af ou" href="https://ai.pydantic.dev/" rel="noopener ugc nofollow" target="_blank">https://ai.pydantic.dev/</a></li><li id="6509" class="mj mk fq ml b go pn mn mo gr po mq mr ms pp mu mv mw pq my mz na pr nc nd ne ps pl pm bk">Patching a Langchain workflow with Mock objects. My “before” solution: <a class="af ou" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/eval_weather_agent.py" rel="noopener ugc nofollow" target="_blank">https://github.com/lakshmanok/lakblogs/blob/main/genai_agents/eval_weather_agent.py</a></li></ol></div></div></div></div>    
</body>
</html>