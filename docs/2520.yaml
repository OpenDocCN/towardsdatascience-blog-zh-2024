- en: 'Continual Learning: A Primer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/continual-learning-a-primer-e328ed1d072f?source=collection_archive---------5-----------------------#2024-10-15](https://towardsdatascience.com/continual-learning-a-primer-e328ed1d072f?source=collection_archive---------5-----------------------#2024-10-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Plus paper recommendations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pascaljanetzky.medium.com/?source=post_page---byline--e328ed1d072f--------------------------------)[![Pascal
    Janetzky](../Images/43d68509b63c5f9b3fc9cef3cbfc1a88.png)](https://pascaljanetzky.medium.com/?source=post_page---byline--e328ed1d072f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e328ed1d072f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e328ed1d072f--------------------------------)
    [Pascal Janetzky](https://pascaljanetzky.medium.com/?source=post_page---byline--e328ed1d072f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e328ed1d072f--------------------------------)
    ·7 min read·Oct 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Training large language models currently costs somewhere between $4.3 Million
    (GPT3) and $191 Million (Gemini) [1]. As soon as new text data is available, for
    example through licensing agreements, re-training with this data can improve model
    performance. However, at these costs (and not just at these levels; which company
    has $1 Million to spare for just the *final* training, not speaking of preliminary
    experiments?), frequent re-training from scratch is prohibitively expensive.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95561cd7ccfb6f32109e8e2d865c3628.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dan Schiumarini](https://unsplash.com/@dan_schiumarini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where continual learning (CL) jumps in. In CL, data arrives incrementally
    over time, and cannot be (fully) stored. The machine learning model is trained
    solely on the new data; the challenge here is *catastrophic forgetting*: performance
    on old data drops. The reason for the performance drop is that the model adapts
    its weights to the current data only, as there is no incentive to retain information
    gained from previous data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To combat forgetting and retain old knowledge, many methods have been proposed.
    These methods can be grouped into three central categories*: **rehearsal-based**,
    **regularization-based**, and **architecture-based**. In the following sections,
    I will detail each category and introduce select papers to explore further. While
    I focus on classification problems, all covered ideas are *mostly* equally valid
    for, e.g…'
  prefs: []
  type: TYPE_NORMAL
