- en: 'Fixing Faulty Gradient Accumulation: Understanding the Issue and Its Resolution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fixing-faulty-gradient-accumulation-understanding-the-issue-and-its-resolution-394ee82a1c20?source=collection_archive---------3-----------------------#2024-10-23](https://towardsdatascience.com/fixing-faulty-gradient-accumulation-understanding-the-issue-and-its-resolution-394ee82a1c20?source=collection_archive---------3-----------------------#2024-10-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Years of suboptimal model training?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--394ee82a1c20--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--394ee82a1c20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--394ee82a1c20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--394ee82a1c20--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--394ee82a1c20--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--394ee82a1c20--------------------------------)
    ·10 min read·Oct 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f3d8e3d9c8058731aa4e2b5202de6ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: When fine-tuning large language models (LLMs) locally, using large batch sizes
    is often impractical due to their substantial GPU memory consumption. To overcome
    this limitation, a technique called *gradient accumulation* is commonly used to
    simulate larger batch sizes. Instead of updating the model weights after processing
    each batch, gradient accumulation involves summing the gradients over several
    smaller mini-batches. The model weights are updated only after a predetermined
    number of these mini-batches have been processed. This method effectively mimics
    training with a larger batch size without the memory overhead typically associated
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, setting a mini-batch size of 1 and accumulating gradients over
    32 mini-batches should be equivalent to training with a full batch size of 32\.
    However, I discovered that gradient accumulation often results in significantly
    degraded performance compared to training with larger actual batch sizes with
    popular deep-learning frameworks like Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: After sharing this issue on [X](https://x.com/bnjmn_marie/status/1842202652672671964)
    and [Reddit](https://www.reddit.com/r/LocalLLaMA/comments/1g0dy0k/finetuning_with_small_batch_sizes_and_gradient/),
    Daniel Han from [Unsloth AI](https://unsloth.ai/) replicated the problem. He found
    that it was affecting not only gradient accumulation but also multi-GPU setups.
    In such…
  prefs: []
  type: TYPE_NORMAL
