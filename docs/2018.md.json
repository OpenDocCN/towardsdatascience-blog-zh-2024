["```py\nreal_df['Real'] = True\nsynth_df = pd.DataFrame() \nfor col_name in real_df.columns:\n    mean = real_df[col_name].mean()\n    stddev = real_df[col_name].std()\n    synth_df[col_name] = np.random.normal(\n       loc=mean, scale=stddev, size=len(real_df))\nsynth_df['Real'] = False\ntrain_df = pd.concat([real_df, synth_df])\n```", "```py\nfor col_name in real_df.columns:    \n    vc = real_df[col_name].value_counts(normalize=True)\n    synth_df[col_name] = np.random.choice(a=vc.keys().tolist(), \n                                          size=len(real_df),\n                                          replace=True, \n                                          p=vc.values.tolist())\n```", "```py\nimport numpy as np\nimport pandas as pd\n\ndef create_simple_testdata():\n    np.random.seed(0)\n    a_data = np.random.normal(size=100)\n    b_data = np.random.normal(size=100)\n    df = pd.DataFrame({\"A\": a_data, \"B\": b_data})\n    return df\n```", "```py\ndef create_four_clusters_test_data():\n    np.random.seed(0)\n\n    a_data = np.random.normal(loc=25.0, scale=2.0, size=5) \n    b_data = np.random.normal(loc=4.0, scale=2.0, size=5)\n    df0 = pd.DataFrame({\"A\": a_data, \"B\": b_data})\n\n    a_data = np.random.normal(loc=1.0, scale=2.0, size=50) \n    b_data = np.random.normal(loc=19.0, scale=2.0, size=50)\n    df1 = pd.DataFrame({\"A\": a_data, \"B\": b_data})\n\n    a_data = np.random.normal(loc=1.0, scale=1.0, size=200) \n    b_data = np.random.normal(loc=1.0, scale=1.0, size=200)\n    df2 = pd.DataFrame({\"A\": a_data, \"B\": b_data})\n\n    a_data = np.random.normal(loc=20.0, scale=3.0, size=500) \n    b_data = np.random.normal(loc=13.0, scale=3.0, size=500) + a_data\n    df3 = pd.DataFrame({\"A\": a_data, \"B\": b_data})\n\n    outliers = [[5.0, 40], \n                [1.5, 8.0],\n                [11.0, 0.5]]\n    df4 = pd.DataFrame(outliers, columns=['A', 'B'])\n\n    df = pd.concat([df0, df1, df2, df3, df4])\n    df = df.reset_index(drop=True)\n    return df\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\nfrom collections import Counter\nfrom sklearn.preprocessing import RobustScaler\n\nclass DMLOutlierDetection:\n    def __init__(self):\n        pass\n\n    def fit_predict(self, df):\n        real_df = df.copy()\n        real_df['Real'] = True\n\n        # Generate synthetic data that is similar to the real data\n        # For simplicity, this covers just the numeric case.  \n        synth_df = pd.DataFrame() \n        for col_name in df.columns:\n            mean = df[col_name].mean()\n            stddev = df[col_name].std()\n            synth_df[col_name] = np.random.normal(loc=mean, \n               scale=stddev, size=len(df))\n        synth_df['Real'] = False\n\n        train_df = pd.concat([real_df, synth_df])\n\n        clf = RandomForestClassifier(max_depth=5)\n        clf.fit(train_df.drop(columns=['Real']), train_df['Real'])\n\n        # Get the leaf node each record ends in \n        r = clf.apply(df) \n\n        # Initialize the score for all records to 0\n        scores = [0]*len(df) \n\n        # Loop through each tree in the Random Forest\n        for tree_idx in range(len(r[0])): \n            # Get the count of each leaf node\n            c = Counter(r[:, tree_idx]) \n\n            # Loop through each record and update its score based \n            # on the frequency of the node it ends in\n            for record_idx in range(len(df)): \n                node_idx = r[record_idx, tree_idx]\n                node_count = c[node_idx]\n                scores[record_idx] += len(df) - node_count\n\n        return scores\n\ndf = create_four_clusters_test_data()\ndf = pd.DataFrame(RobustScaler().fit_transform(df), columns=df.columns)\nclf = DMLOutlierDetection()\ndf['Scores'] = clf.fit_predict(df)\n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.scatterplot(x=df[\"A\"], y=df['B'], hue=df['Scores'])\nplt.show()\n```"]