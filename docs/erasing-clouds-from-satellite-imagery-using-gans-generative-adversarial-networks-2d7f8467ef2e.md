# ä½¿ç”¨ GANï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰å»é™¤å«æ˜Ÿå›¾åƒä¸­çš„äº‘

> åŸæ–‡ï¼š[`towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15`](https://towardsdatascience.com/erasing-clouds-from-satellite-imagery-using-gans-generative-adversarial-networks-2d7f8467ef2e?source=collection_archive---------2-----------------------#2024-06-15)

## **ä»é›¶å¼€å§‹åœ¨ Python ä¸­æ„å»º GAN**

[](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)![Aleksei Rozanov](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------) [Aleksei Rozanov](https://medium.com/@alexroz?source=post_page---byline--2d7f8467ef2e--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2d7f8467ef2e--------------------------------) Â·12 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 6 æœˆ 15 æ—¥

--

![](img/a8ae5b6d4e446b5af4e38c5c19ed1129.png)

å›¾ç‰‡ç”± [Michael & Diane Weidner](https://unsplash.com/@michaelbweidner?utm_source=medium&utm_medium=referral) æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„æ¦‚å¿µç”± Goodfellow å’Œä»–çš„åŒäº‹ä»¬åœ¨ 2014 å¹´æå‡º [1]ï¼Œå¹¶ä¸”å¾ˆå¿«åœ¨è®¡ç®—æœºè§†è§‰å’Œå›¾åƒç”Ÿæˆé¢†åŸŸè·å¾—äº†æå¤§å…³æ³¨ã€‚å°½ç®¡åœ¨è¿‡å»çš„ 10 å¹´é‡Œï¼Œäººå·¥æ™ºèƒ½é¢†åŸŸç»å†äº†å¿«é€Ÿå‘å±•å¹¶å‡ºç°äº†è®¸å¤šæ–°ç®—æ³•ï¼Œä½†è¿™ä¸ªæ¦‚å¿µçš„ç®€å•æ€§å’Œ brillianceï¼ˆå·§å¦™ä¹‹å¤„ï¼‰ä¾ç„¶ä»¤äººå°è±¡æ·±åˆ»ã€‚æ‰€ä»¥ä»Šå¤©æˆ‘æƒ³é€šè¿‡å°è¯•ä»å«æ˜Ÿ RGBï¼ˆçº¢ã€ç»¿ã€è“ï¼‰å›¾åƒä¸­å»é™¤äº‘å±‚ï¼Œæ¥å±•ç¤ºè¿™äº›ç½‘ç»œçš„å¼ºå¤§èƒ½åŠ›ã€‚

å‡†å¤‡ä¸€ä¸ªé€‚å½“å¹³è¡¡ã€è¶³å¤Ÿå¤§ä¸”ç»è¿‡æ­£ç¡®é¢„å¤„ç†çš„è®¡ç®—æœºè§†è§‰æ•°æ®é›†éœ€è¦å¤§é‡æ—¶é—´ï¼Œå› æ­¤æˆ‘å†³å®šæ¢ç´¢ Kaggle æä¾›çš„èµ„æºã€‚æˆ‘å‘ç°æœ€é€‚åˆè¿™ä¸ªä»»åŠ¡çš„æ•°æ®é›†æ˜¯ EuroSat [2]ï¼Œå®ƒå…·æœ‰å¼€æ”¾è®¸å¯ã€‚è¯¥æ•°æ®é›†åŒ…å« **27000** å¼ æ ‡æ³¨çš„ RGB å›¾åƒï¼Œå°ºå¯¸ä¸º 64x64 åƒç´ ï¼Œæ¥è‡ª [Sentinel-2](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/Sentinel-2)ï¼Œå¹¶ç”¨äºè§£å†³å¤šç±»åˆ†ç±»é—®é¢˜ã€‚

[](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------) [## EuroSat æ•°æ®é›†

### æ•°æ®é›†åŒ…å«æ¥è‡ª Sentinel-2 çš„æ‰€æœ‰ RGB å’Œæ³¢æ®µå›¾åƒ

[www.kaggle.com](https://www.kaggle.com/datasets/apollo2506/eurosat-dataset/data?source=post_page-----2d7f8467ef2e--------------------------------) ![](img/78b9510aa3c9c7e3a95911929f732e88.png)

EuroSat æ•°æ®é›†çš„å›¾åƒç¤ºä¾‹ã€‚[è®¸å¯](https://github.com/phelber/eurosat)ã€‚

æˆ‘ä»¬å¯¹åˆ†ç±»æœ¬èº«å¹¶ä¸æ„Ÿå…´è¶£ï¼Œä½† EuroSat æ•°æ®é›†çš„ä¸€ä¸ªä¸»è¦ç‰¹ç‚¹æ˜¯ï¼Œæ‰€æœ‰å›¾åƒéƒ½æœ‰æ¸…æ™°çš„å¤©ç©ºã€‚è¿™æ­£æ˜¯æˆ‘ä»¬éœ€è¦çš„ã€‚å€Ÿç”¨[3]ä¸­çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†è¿™äº› Sentinel-2 å½±åƒä½œä¸ºç›®æ ‡ï¼Œé€šè¿‡å‘å®ƒä»¬æ·»åŠ å™ªå£°ï¼ˆäº‘æœµï¼‰æ¥åˆ›å»ºè¾“å…¥ã€‚

é‚£ä¹ˆåœ¨çœŸæ­£è®¨è®º GANs ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå‡†å¤‡ä¸€ä¸‹æ•°æ®ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•°æ®ï¼Œå¹¶å°†æ‰€æœ‰ç±»åˆ«åˆå¹¶åˆ°ä¸€ä¸ªç›®å½•ä¸­ã€‚

**ğŸå®Œæ•´çš„ Python ä»£ç ï¼š** [**GitHub**](https://github.com/alexxxroz/Medium/blob/main/GANs%26Clouds.ipynb)**.**

```py
import numpy as np
import pandas as pd
import random

from os import listdir, mkdir, rename
from os.path import join, exists
import shutil
import datetime

import matplotlib.pyplot as plt
from highlight_text import ax_text, fig_text
from PIL import Image

import warnings

warnings.filterwarnings('ignore')
```

```py
classes = listdir('./EuroSat')
path_target = './EuroSat/all_targets'
path_input = './EuroSat/all_inputs'

"""RUN IT ONLY ONCE TO RENAME THE FILES IN THE UNPACKED ARCHIVE"""
mkdir(path_input)
mkdir(path_target)
k = 1
for kind in classes:
  path = join('./EuroSat', str(kind))
  for i, f in enumerate(listdir(path)):
    shutil.copyfile(join(path, f),
                  join(path_target, f))
    rename(join(path_target, f), join(path_target, f'{k}.jpg'))
    k += 1
```

ç¬¬äºŒä¸ªé‡è¦æ­¥éª¤æ˜¯ç”Ÿæˆå™ªå£°ã€‚è™½ç„¶ä½ å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œä¾‹å¦‚éšæœºé®ç½©ä¸€äº›åƒç´ ã€æ·»åŠ ä¸€äº›é«˜æ–¯å™ªå£°ï¼Œä½†åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³å°è¯•ä¸€ä¸ªå¯¹æˆ‘æ¥è¯´æ–°é¢–çš„ä¸œè¥¿â€”â€”Perlin å™ªå£°ã€‚å®ƒæ˜¯ç”± Ken Perlin åœ¨ 80 å¹´ä»£å‘æ˜çš„[4]ï¼Œç”¨äºå¼€å‘ç”µå½±ä¸­çš„çƒŸé›¾æ•ˆæœã€‚è¿™ç§å™ªå£°ä¸æ™®é€šçš„éšæœºå™ªå£°ç›¸æ¯”ï¼Œå…·æœ‰æ›´è‡ªç„¶çš„å¤–è§‚ã€‚è®©æˆ‘æ¥è¯æ˜ä¸€ä¸‹ã€‚

```py
def generate_perlin_noise(width, height, scale, octaves, persistence, lacunarity):
    noise = np.zeros((height, width))
    for i in range(height):
        for j in range(width):
            noise[i][j] = pnoise2(i / scale,
                                  j / scale,
                                  octaves=octaves,
                                  persistence=persistence,
                                  lacunarity=lacunarity,
                                  repeatx=width,
                                  repeaty=height,
                                  base=0)
    return noise

def normalize_noise(noise):
    min_val = noise.min()
    max_val = noise.max()
    return (noise - min_val) / (max_val - min_val)

def generate_clouds(width, height, base_scale, octaves, persistence, lacunarity):
    clouds = np.zeros((height, width))
    for octave in range(1, octaves + 1):
        scale = base_scale / octave
        layer = generate_perlin_noise(width, height, scale, 1, persistence, lacunarity)
        clouds += layer * (persistence ** octave)

    clouds = normalize_noise(clouds)
    return clouds

def overlay_clouds(image, clouds, alpha=0.5):

    clouds_rgb = np.stack([clouds] * 3, axis=-1)

    image = image.astype(float) / 255.0
    clouds_rgb = clouds_rgb.astype(float)

    blended = image * (1 - alpha) + clouds_rgb * alpha

    blended = (blended * 255).astype(np.uint8)
    return blended
```

```py
width, height = 64, 64
octaves = 12 #number of noise layers combined
persistence = 0.5 #lower persistence reduces the amplitude of higher-frequency octaves
lacunarity = 2 #higher lacunarity increases the frequency of higher-frequency octaves
for i in range(len(listdir(path_target))):
  base_scale = random.uniform(5,120) #noise frequency
  alpha = random.uniform(0,1) #transparency

  clouds = generate_clouds(width, height, base_scale, octaves, persistence, lacunarity)

  img = np.asarray(Image.open(join(path_target, f'{i+1}.jpg')))
  image = Image.fromarray(overlay_clouds(img,clouds, alpha))
  image.save(join(path_input,f'{i+1}.jpg'))
  print(f'Processed {i+1}/{len(listdir(path_target))}')
```

```py
idx = np.random.randint(27000)
fig,ax = plt.subplots(1,2)
ax[0].imshow(np.asarray(Image.open(join(path_target, f'{idx}.jpg'))))
ax[1].imshow(np.asarray(Image.open(join(path_input, f'{idx}.jpg'))))
ax[0].set_title("Target")
ax[0].axis('off')
ax[1].set_title("Input")
ax[1].axis('off')
plt.show()
```

![](img/663c3610117fdf1d41c60edb0e5a06e6.png)

å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚

å¦‚ä¸Šæ‰€ç¤ºï¼Œå›¾åƒä¸­çš„äº‘æœµéå¸¸é€¼çœŸï¼Œå®ƒä»¬å…·æœ‰ä¸åŒçš„â€œå¯†åº¦â€å’Œç±»ä¼¼çœŸå®äº‘æœµçš„çº¹ç†ã€‚

å¦‚æœä½ åƒæˆ‘ä¸€æ ·å¯¹ Perlin å™ªå£°æ„Ÿå…´è¶£ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸é…·çš„è§†é¢‘ï¼Œå±•ç¤ºäº†è¿™ç§å™ªå£°å¦‚ä½•åº”ç”¨äºæ¸¸æˆå¼€å‘è¡Œä¸šï¼š

æ—¢ç„¶æˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªç°æˆå¯ç”¨çš„æ•°æ®é›†ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬æ¥è°ˆè°ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€‚

# ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰

ä¸ºäº†æ›´å¥½åœ°è¯´æ˜è¿™ä¸ªæ¦‚å¿µï¼Œå‡è®¾ä½ æ­£åœ¨ä¸œå—äºšæ—…è¡Œï¼Œçªç„¶éœ€è¦ä¸€ä»¶è¿å¸½è¡«ï¼Œå› ä¸ºå¤–é¢å¤ªå†·äº†ã€‚ä½ æ¥åˆ°æœ€è¿‘çš„è¡—å¤´å¸‚åœºï¼Œå‘ç°ä¸€å®¶å°åº—æœ‰ä¸€äº›å“ç‰Œæœè£…ã€‚å–å®¶æ‹¿æ¥ä¸€ä»¶ä¸é”™çš„è¿å¸½è¡«è®©ä½ è¯•ç©¿ï¼Œå¹¶è¯´å®ƒæ˜¯è‘—åå“ç‰Œ ExpensiveButNotWorthItã€‚ä½ ä»”ç»†ä¸€çœ‹ï¼Œå¾—å‡ºç»“è®ºï¼Œè¿™æ˜¾ç„¶æ˜¯å‡çš„ã€‚å–å®¶è¯´ï¼šâ€œç­‰ä¸€ä¸‹ï¼Œæˆ‘æœ‰çœŸçš„ã€‚â€ç„¶åä»–å¸¦ç€å¦ä¸€ä»¶è¿å¸½è¡«å›æ¥ï¼Œçœ‹èµ·æ¥æ›´åƒæ˜¯å“ç‰Œçš„ï¼Œä½†ä¾æ—§æ˜¯å‡è´§ã€‚ç»è¿‡å‡ è½®è¿™æ ·çš„å°è¯•åï¼Œå–å®¶å¸¦æ¥äº†ä¸€ä»¶æ— æ³•åˆ†è¾¨çš„ä¼ å¥‡å“ç‰Œ ExpensiveButNotWorthIt çš„å¤åˆ¶å“ï¼Œä½ ä¾¿é«˜å…´åœ°ä¹°ä¸‹äº†å®ƒã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰çš„å·¥ä½œåŸç†ï¼

åœ¨ GANs çš„æƒ…å†µä¸‹ï¼Œä½ è¢«ç§°ä¸ºåˆ¤åˆ«å™¨ï¼ˆDï¼‰ã€‚åˆ¤åˆ«å™¨çš„ç›®æ ‡æ˜¯åŒºåˆ†çœŸå®ç‰©ä½“å’Œè™šå‡ç‰©ä½“ï¼Œæˆ–è€…è§£å†³äºŒåˆ†ç±»ä»»åŠ¡ã€‚å–å®¶è¢«ç§°ä¸ºç”Ÿæˆå™¨ï¼ˆGï¼‰ï¼Œå› ä¸ºä»–åœ¨å°è¯•ç”Ÿæˆé«˜è´¨é‡çš„å‡è´§ã€‚åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨æ˜¯ç‹¬ç«‹è®­ç»ƒçš„ï¼Œç›®çš„æ˜¯äº’ç›¸è¶…è¶Šã€‚å› æ­¤ï¼Œæœ€ç»ˆæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªé«˜è´¨é‡çš„å‡è´§ã€‚

![](img/dd222e2e675a330b6217b13ea4acc8d4.png)

GANs æ¶æ„ã€‚[è®¸å¯](https://paperswithcode.com/method/gan)ã€‚

è®­ç»ƒè¿‡ç¨‹æœ€åˆçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š

1.  é‡‡æ ·è¾“å…¥å™ªå£°ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯æœ‰äº‘çš„å›¾åƒï¼‰ã€‚

1.  å°†å™ªå£°è¾“å…¥ G å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚

1.  é€šè¿‡è·å–ä¸¤ä¸ªé¢„æµ‹å€¼æ¥è®¡ç®— D çš„æŸå¤±ï¼Œä¸€ä¸ªæ˜¯ G çš„è¾“å‡ºï¼Œå¦ä¸€ä¸ªæ˜¯çœŸå®æ•°æ®çš„é¢„æµ‹å€¼ã€‚

1.  æ›´æ–° D çš„æƒé‡ã€‚

1.  å†æ¬¡é‡‡æ ·è¾“å…¥å™ªå£°ã€‚

1.  å°†å™ªå£°è¾“å…¥ G å¹¶æ”¶é›†é¢„æµ‹ç»“æœã€‚

1.  é€šè¿‡å°† G çš„é¢„æµ‹è¾“å…¥åˆ° D ä¸­æ¥è®¡ç®— G çš„æŸå¤±ã€‚

1.  æ›´æ–° G çš„æƒé‡ã€‚

![](img/788502677c2e805ddd67f00aa1010601.png)

GANs è®­ç»ƒå¾ªç¯ã€‚æ¥æºï¼š[1]ã€‚

æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå€¼å‡½æ•° V(G,D)ï¼š

![](img/ba0147e691508a1cee54eda81aec27ca.png)

æ¥æºï¼š[1]ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–é¡¹ **log(1-D(G(z)))** æ¥è®­ç»ƒ Gï¼Œå¹¶æœ€å¤§åŒ– **log D(x)** æ¥è®­ç»ƒ Dï¼ˆåœ¨æ­¤ç¬¦å·ä¸­ï¼Œx è¡¨ç¤ºçœŸå®æ•°æ®æ ·æœ¬ï¼Œz è¡¨ç¤ºå™ªå£°ï¼‰ã€‚

ç°åœ¨è®©æˆ‘ä»¬å°è¯•åœ¨ pytorch ä¸­å®ç°å®ƒï¼

åœ¨åŸå§‹è®ºæ–‡ä¸­ï¼Œä½œè€…æåˆ°ä½¿ç”¨å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ï¼›å®ƒé€šå¸¸ä¹Ÿç®€ç§°ä¸º ANNï¼Œä½†æˆ‘æƒ³å°è¯•ä¸€ä¸ªæ›´å¤æ‚çš„æ–¹æ³•â€”â€”æˆ‘æƒ³ä½¿ç”¨ UNet [5] æ¶æ„ä½œä¸ºç”Ÿæˆå™¨ï¼ŒResNet [6] ä½œä¸ºåˆ¤åˆ«å™¨ã€‚è¿™ä¸¤è€…éƒ½æ˜¯è‘—åçš„ CNN æ¶æ„ï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šåœ¨è¿™é‡Œè§£é‡Šå®ƒä»¬ï¼ˆå¦‚æœéœ€è¦æˆ‘å†™ä¸€ç¯‡å•ç‹¬çš„æ–‡ç« ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼‰ã€‚

è®©æˆ‘ä»¬æ¥æ„å»ºå®ƒä»¬ã€‚åˆ¤åˆ«å™¨ï¼š

```py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torch.utils.data import Subset
```

```py
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Sequential(
                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),
                        nn.BatchNorm2d(out_channels),
                        nn.ReLU())
        self.conv2 = nn.Sequential(
                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),
                        nn.BatchNorm2d(out_channels))
        self.downsample = downsample
        self.relu = nn.ReLU()
        self.out_channels = out_channels

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.conv2(out)
        if self.downsample:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out

class ResNet(nn.Module):
    def __init__(self, block=ResidualBlock, all_connections=[3,4,6,3]):
        super(ResNet, self).__init__()
        self.inputs = 16
        self.conv1 = nn.Sequential(
                        nn.Conv2d(3, 16, kernel_size = 3, stride = 1, padding = 1),
                        nn.BatchNorm2d(16),
                        nn.ReLU()) #16x64x64
        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2) #16x32x32

        self.layer0 = self.makeLayer(block, 16, all_connections[0], stride = 1) #connections = 3, shape: 16x32x32
        self.layer1 = self.makeLayer(block, 32, all_connections[1], stride = 2)#connections = 4, shape: 32x16x16
        self.layer2 = self.makeLayer(block, 128, all_connections[2], stride = 2)#connections = 6, shape: 1281x8x8
        self.layer3 = self.makeLayer(block, 256, all_connections[3], stride = 2)#connections = 3, shape: 256x4x4
        self.avgpool = nn.AvgPool2d(4, stride=1)
        self.fc = nn.Linear(256, 1)

    def makeLayer(self, block, outputs, connections, stride=1):
        downsample = None
        if stride != 1 or self.inputs != outputs:
            downsample = nn.Sequential(
                nn.Conv2d(self.inputs, outputs, kernel_size=1, stride=stride),
                nn.BatchNorm2d(outputs),
            )
        layers = []
        layers.append(block(self.inputs, outputs, stride, downsample))
        self.inputs = outputs
        for i in range(1, connections):
            layers.append(block(self.inputs, outputs))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.maxpool(x)
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.avgpool(x)
        x = x.view(-1, 256)
        x = self.fc(x).flatten()
        return F.sigmoid(x)
```

ç”Ÿæˆå™¨ï¼š

```py
 class DoubleConv(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(DoubleConv, self).__init__()
        self.double_conv = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.double_conv(x)

class UNet(nn.Module):
    def __init__(self):
      super().__init__()
      self.conv_1 = DoubleConv(3, 32) # 32x64x64
      self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2) # 32x32x32

      self.conv_2 = DoubleConv(32, 64)  #64x32x32
      self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) #64x16x16

      self.conv_3 = DoubleConv(64, 128)  #128x16x16
      self.pool_3 = nn.MaxPool2d(kernel_size=2, stride=2) #128x8x8

      self.conv_4 = DoubleConv(128, 256)  #256x8x8
      self.pool_4 = nn.MaxPool2d(kernel_size=2, stride=2) #256x4x4

      self.conv_5 = DoubleConv(256, 512)  #512x2x2

      #DECODER
      self.upconv_1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2) #256x4x4
      self.conv_6 = DoubleConv(512, 256) #256x4x4

      self.upconv_2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2) #128x8x8
      self.conv_7 = DoubleConv(256, 128)  #128x8x8

      self.upconv_3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2) #64x16x16
      self.conv_8 = DoubleConv(128, 64)  #64x16x16

      self.upconv_4 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2) #32x32x32
      self.conv_9 = DoubleConv(64, 32)  #32x32x32

      self.output = nn.Conv2d(32, 3, kernel_size = 3, stride = 1, padding = 1) #3x64x64

    def forward(self, batch):

      conv_1_out = self.conv_1(batch)
      conv_2_out = self.conv_2(self.pool_1(conv_1_out))
      conv_3_out = self.conv_3(self.pool_2(conv_2_out))
      conv_4_out = self.conv_4(self.pool_3(conv_3_out))
      conv_5_out = self.conv_5(self.pool_4(conv_4_out))

      conv_6_out = self.conv_6(torch.cat([self.upconv_1(conv_5_out), conv_4_out], dim=1))
      conv_7_out = self.conv_7(torch.cat([self.upconv_2(conv_6_out), conv_3_out], dim=1))
      conv_8_out = self.conv_8(torch.cat([self.upconv_3(conv_7_out), conv_2_out], dim=1))
      conv_9_out = self.conv_9(torch.cat([self.upconv_4(conv_8_out), conv_1_out], dim=1))

      output = self.output(conv_9_out)

      return F.sigmoid(output)
```

ç°åœ¨æˆ‘ä»¬éœ€è¦å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å°†å®ƒä»¬å°è£…ä¸º torch æ•°æ®é›†ï¼š

```py
class dataset(Dataset):
  def __init__(self, batch_size, images_paths, targets, img_size = 64):
    self.batch_size = batch_size
    self.img_size = img_size
    self.images_paths = images_paths
    self.targets = targets
    self.len = len(self.images_paths) // batch_size

    self.transform = transforms.Compose([
                transforms.ToTensor(),
                ])

    self.batch_im = [self.images_paths[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]
    self.batch_t = [self.targets[idx * self.batch_size:(idx + 1) * self.batch_size] for idx in range(self.len)]

  def __getitem__(self, idx):
      pred = torch.stack([
              self.transform(Image.open(join(path_input,file_name)))
              for file_name in self.batch_im[idx]
          ])
      target = torch.stack([
              self.transform(Image.open(join(path_target,file_name)))
              for file_name in self.batch_im[idx]
          ])
      return pred, target

  def __len__(self):
      return self.len
```

å®Œç¾ã€‚æ˜¯æ—¶å€™ç¼–å†™è®­ç»ƒå¾ªç¯äº†ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼š

```py
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

batch_size = 64
num_epochs = 15
learning_rate_D = 1e-5
learning_rate_G = 1e-4

discriminator = ResNet()
generator = UNet()

bce = nn.BCEWithLogitsLoss()
l1loss = nn.L1Loss()

optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_D)
optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_G)

scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.1)
scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.1)
```

å¦‚ä½ æ‰€è§ï¼Œè¿™äº›æŸå¤±ä¸ GAN ç®—æ³•ä¸­çš„å›¾ç‰‡æœ‰æ‰€ä¸åŒã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘åŠ å…¥äº† L1Lossã€‚å…¶æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯ä»å™ªå£°ä¸­ç”Ÿæˆä¸€å¼ éšæœºå›¾ç‰‡ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›ä¿ç•™è¾“å…¥ä¸­çš„å¤§éƒ¨åˆ†ä¿¡æ¯ï¼Œåªå»é™¤å™ªå£°ã€‚æ‰€ä»¥ G çš„æŸå¤±å°†æ˜¯ï¼š

**G_loss = log(1 âˆ’ D(G(z))) + ğ€ |G(z)-y|**

è€Œä¸æ˜¯ä»…ä»…

**G_loss = log(1 âˆ’ D(G(z)))**

ğ€ æ˜¯ä¸€ä¸ªä»»æ„ç³»æ•°ï¼Œç”¨äºå¹³è¡¡æŸå¤±å‡½æ•°çš„ä¸¤ä¸ªéƒ¨åˆ†ã€‚

æœ€åï¼Œè®©æˆ‘ä»¬åˆ’åˆ†æ•°æ®å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹ï¼š

```py
test_ratio, train_ratio = 0.3, 0.7
num_test = int(len(listdir(path_target))*test_ratio)
num_train = int((int(len(listdir(path_target)))-num_test))

img_size = (64, 64)

print("Number of train samples:", num_train)
print("Number of test samples:", num_test)

random.seed(231)
train_idxs = np.array(random.sample(range(num_test+num_train), num_train))
mask = np.ones(num_train+num_test, dtype=bool)
mask[train_idxs] = False

images = {}
features = random.sample(listdir(path_input),num_test+num_train)
targets = random.sample(listdir(path_target),num_test+num_train)

random.Random(231).shuffle(features)
random.Random(231).shuffle(targets)

train_input_img_paths = np.array(features)[train_idxs]
train_target_img_path = np.array(targets)[train_idxs]
test_input_img_paths = np.array(features)[mask]
test_target_img_path = np.array(targets)[mask]

train_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=train_input_img_paths, targets=train_target_img_path)
test_loader = dataset(batch_size=batch_size, img_size=img_size, images_paths=test_input_img_paths, targets=test_target_img_path)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿è¡Œæˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ï¼š

```py
train_loss_G, train_loss_D, val_loss_G, val_loss_D = [], [], [], []
all_loss_G, all_loss_D = [], []
best_generator_epoch_val_loss, best_discriminator_epoch_val_loss = -np.inf, -np.inf
for epoch in range(num_epochs):

    discriminator.train()
    generator.train()

    discriminator_epoch_loss, generator_epoch_loss = 0, 0

    for inputs, targets in train_loader:
        inputs, true = inputs, targets

        '''1\. Training the Discriminator (ResNet)'''
        optimizer_D.zero_grad()

        fake = generator(inputs).detach()

        pred_fake = discriminator(fake).to(device)
        loss_fake = bce(pred_fake, torch.zeros(batch_size, device=device))

        pred_real = discriminator(true).to(device)
        loss_real = bce(pred_real, torch.ones(batch_size, device=device))

        loss_D = (loss_fake+loss_real)/2

        loss_D.backward()
        optimizer_D.step()

        discriminator_epoch_loss += loss_D.item()
        all_loss_D.append(loss_D.item())

        '''2\. Training the Generator (UNet)'''
        optimizer_G.zero_grad()

        fake = generator(inputs)
        pred_fake = discriminator(fake).to(device)

        loss_G_bce = bce(pred_fake, torch.ones_like(pred_fake, device=device))
        loss_G_l1 = l1loss(fake, targets)*100
        loss_G = loss_G_bce + loss_G_l1
        loss_G.backward()
        optimizer_G.step()

        generator_epoch_loss += loss_G.item()
        all_loss_G.append(loss_G.item())

    discriminator_epoch_loss /= len(train_loader)
    generator_epoch_loss /= len(train_loader)
    train_loss_D.append(discriminator_epoch_loss)
    train_loss_G.append(generator_epoch_loss)

    discriminator.eval()
    generator.eval()

    discriminator_epoch_val_loss, generator_epoch_val_loss = 0, 0

    with torch.no_grad():
        for inputs, targets in test_loader:
            inputs, targets = inputs, targets

            fake = generator(inputs)
            pred = discriminator(fake).to(device)

            loss_G_bce = bce(fake, torch.ones_like(fake, device=device))
            loss_G_l1 = l1loss(fake, targets)*100
            loss_G = loss_G_bce + loss_G_l1
            loss_D = bce(pred.to(device), torch.zeros(batch_size, device=device))

            discriminator_epoch_val_loss += loss_D.item()
            generator_epoch_val_loss += loss_G.item()

    discriminator_epoch_val_loss /= len(test_loader)
    generator_epoch_val_loss /= len(test_loader)

    val_loss_D.append(discriminator_epoch_val_loss)
    val_loss_G.append(generator_epoch_val_loss)

    print(f"------Epoch [{epoch+1}/{num_epochs}]------\nTrain Loss D: {discriminator_epoch_loss:.4f}, Val Loss D: {discriminator_epoch_val_loss:.4f}")
    print(f'Train Loss G: {generator_epoch_loss:.4f}, Val Loss G: {generator_epoch_val_loss:.4f}')

    if discriminator_epoch_val_loss > best_discriminator_epoch_val_loss:
        discriminator_epoch_val_loss = best_discriminator_epoch_val_loss
        torch.save(discriminator.state_dict(), "discriminator.pth")
    if generator_epoch_val_loss > best_generator_epoch_val_loss:
        generator_epoch_val_loss = best_generator_epoch_val_loss
        torch.save(generator.state_dict(), "generator.pth")
    #scheduler_D.step()
    #scheduler_G.step()

    fig, ax = plt.subplots(1,3)
    ax[0].imshow(np.transpose(inputs.numpy()[7], (1,2,0)))
    ax[1].imshow(np.transpose(targets.numpy()[7], (1,2,0)))
    ax[2].imshow(np.transpose(fake.detach().numpy()[7], (1,2,0)))
    plt.show()
```

ä»£ç å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶æŸå¤±å›¾ã€‚æ­¤ä»£ç éƒ¨åˆ†æ¥æºäº[è¿™ä¸ªé…·ç½‘ç«™](https://python-graph-gallery.com/web-small-multiple-with-highlights/)ï¼š

```py
from matplotlib.font_manager import FontProperties

background_color = '#001219'
font = FontProperties(fname='LexendDeca-VariableFont_wght.ttf')
fig, ax = plt.subplots(1, 2, figsize=(16, 9))
fig.set_facecolor(background_color)
ax[0].set_facecolor(background_color)
ax[1].set_facecolor(background_color)

ax[0].plot(range(len(all_loss_G)), all_loss_G, color='#bc6c25', lw=0.5) 
ax[1].plot(range(len(all_loss_D)), all_loss_D, color='#00b4d8', lw=0.5)

ax[0].scatter(
      [np.array(all_loss_G).argmax(), np.array(all_loss_G).argmin()],
      [np.array(all_loss_G).max(), np.array(all_loss_G).min()],
      s=30, color='#bc6c25',
   )
ax[1].scatter(
      [np.array(all_loss_D).argmax(), np.array(all_loss_D).argmin()],
      [np.array(all_loss_D).max(), np.array(all_loss_D).min()],
      s=30, color='#00b4d8',
   )

ax_text(
      np.array(all_loss_G).argmax()+60, np.array(all_loss_G).max()+0.1,
      f'{round(np.array(all_loss_G).max(),1)}',
      fontsize=13, color='#bc6c25',
      font=font,
      ax=ax[0]
   )
ax_text(
      np.array(all_loss_G).argmin()+60, np.array(all_loss_G).min()-0.1,
      f'{round(np.array(all_loss_G).min(),1)}',
      fontsize=13, color='#bc6c25',
      font=font,
      ax=ax[0]
   )

ax_text(
      np.array(all_loss_D).argmax()+60, np.array(all_loss_D).max()+0.01,
      f'{round(np.array(all_loss_D).max(),1)}',
      fontsize=13, color='#00b4d8',
      font=font,
      ax=ax[1]
   )
ax_text(
      np.array(all_loss_D).argmin()+60, np.array(all_loss_D).min()-0.005,
      f'{round(np.array(all_loss_D).min(),1)}',
      fontsize=13, color='#00b4d8',
      font=font,
      ax=ax[1]
   )
for i in range(2):
    ax[i].tick_params(axis='x', colors='white')
    ax[i].tick_params(axis='y', colors='white')
    ax[i].spines['left'].set_color('white') 
    ax[i].spines['bottom'].set_color('white') 
    ax[i].set_xlabel('Epoch', color='white', fontproperties=font, fontsize=13)
    ax[i].set_ylabel('Loss', color='white', fontproperties=font, fontsize=13)

ax[0].set_title('Generator', color='white', fontproperties=font, fontsize=18)
ax[1].set_title('Discriminator', color='white', fontproperties=font, fontsize=18)
plt.savefig('Loss.jpg')
plt.show()
# ax[0].set_axis_off()
# ax[1].set_axis_off()
```

![](img/413ed7942d93aa8dfd0134cb587b28e8.png)

å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚

åŒæ—¶ä¹Ÿå¯ä»¥å¯è§†åŒ–æ¥è‡ªæµ‹è¯•æ•°æ®é›†çš„éšæœºæ ·æœ¬ï¼š

```py
random.Random(2).shuffle(test_target_img_path)
random.Random(2).shuffle(test_input_img_paths)
subset_loader = dataset(batch_size=5, img_size=img_size, images_paths=test_input_img_paths,
                        targets=test_target_img_path)
generator = UNet()
generator.load_state_dict(torch.load('generator.pth'))

generator.eval()
for X, y in subset_loader:
    fig, axes = plt.subplots(5, 3, figsize=(9, 9))

    for i in range(5):
        axes[i, 0].imshow(np.transpose(X.numpy()[i], (1, 2, 0)))
        axes[i, 0].set_title("Input")
        axes[i, 0].axis('off')

        axes[i, 1].imshow(np.transpose(y.numpy()[i], (1, 2, 0)))
        axes[i, 1].set_title("Target")
        axes[i, 1].axis('off')

        generated_image = generator(X[i].unsqueeze(0)).detach().numpy()[0]
        axes[i, 2].imshow(np.transpose(generated_image, (1, 2, 0)))
        axes[i, 2].set_title("Generated")
        axes[i, 2].axis('off')

    # Adjust layout
    plt.tight_layout()
    plt.savefig('Test.jpg')
    plt.show()
    break 
```

![](img/a16b1e301ecc94fe2928a33cc3216350.png)

å›¾ç‰‡æ¥æºï¼š[ä½œè€…](https://medium.com/@alexroz)ã€‚

å¦‚ä½ æ‰€è§ï¼Œç»“æœå¹¶ä¸å®Œç¾ï¼Œå¹¶ä¸”å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºåœ°è²Œç±»å‹ã€‚ç„¶è€Œï¼Œæ„å»ºçš„æ¨¡å‹è‚¯å®šèƒ½å¤Ÿå»é™¤å›¾åƒä¸­çš„äº‘å±‚ï¼Œå¹¶ä¸”é€šè¿‡å¢åŠ  G å’Œ D çš„æ·±åº¦å¯ä»¥æé«˜å…¶æ€§èƒ½ã€‚å¦ä¸€ä¸ªæœ‰å‰æ™¯çš„ç­–ç•¥æ˜¯ä¸ºä¸åŒçš„åœ°è²Œç±»å‹è®­ç»ƒç‹¬ç«‹çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå†œç”°å’Œæ°´åŸŸçš„ç©ºé—´ç‰¹å¾å·®å¼‚è¾ƒå¤§ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½ä¸ºä½ æä¾›ä¸€ç§åœ¨åœ°ç†ç©ºé—´é¢†åŸŸåº”ç”¨æ·±åº¦å­¦ä¹ ç®—æ³•çš„æ–°è§†è§’ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰æ˜¯æ•°æ®ç§‘å­¦å®¶å¯ä»¥åˆ©ç”¨çš„æœ€å¼ºå¤§å·¥å…·ä¹‹ä¸€ï¼Œæˆ‘å¸Œæœ›å®ƒä»¬ä¹Ÿèƒ½æˆä¸ºä½ å·¥å…·ç®±ä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼

===========================================

***å‚è€ƒæ–‡çŒ®ï¼š***

1\. Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville å’Œ Yoshua Bengioã€‚â€œç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚â€ *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•* 27ï¼ˆ2014 å¹´ï¼‰ã€‚[`proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf`](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)

2\. Helber, Patrick, Benjamin Bischke, Andreas Dengel å’Œ Damian Borthã€‚â€œEurosatï¼šä¸€ä¸ªç”¨äºåœŸåœ°åˆ©ç”¨å’ŒåœŸåœ°è¦†ç›–åˆ†ç±»çš„å…¨æ–°æ•°æ®é›†å’Œæ·±åº¦å­¦ä¹ åŸºå‡†ã€‚â€ *IEEE åº”ç”¨åœ°çƒè§‚æµ‹ä¸é¥æ„Ÿç²¾é€‰ä¸»é¢˜æœŸåˆŠ* 12 å·ï¼Œç¬¬ 7 æœŸï¼ˆ2019 å¹´ï¼‰ï¼š2217â€“2226ã€‚[`arxiv.org/pdf/1709.00029`](https://arxiv.org/pdf/1709.00029)

3\. Wen, Xue, Zongxu Pan, Yuxin Hu å’Œ Jiayin Liuã€‚â€œåŸºäº YUV é¢œè‰²ç©ºé—´çš„ç”Ÿæˆå¯¹æŠ—å­¦ä¹ ç”¨äºå«æ˜Ÿå›¾åƒä¸­çš„è–„äº‘å»é™¤ã€‚â€ *é¥æ„Ÿ* 13 å·ï¼Œç¬¬ 6 æœŸï¼ˆ2021 å¹´ï¼‰ï¼š1079ã€‚[`www.mdpi.com/2072-4292/13/6/1079`](https://www.mdpi.com/2072-4292/13/6/1079)

4\. Perlin, Kenã€‚â€œå›¾åƒåˆæˆå™¨ã€‚â€ *ACM Siggraph è®¡ç®—æœºå›¾å½¢å­¦* 19 å·ï¼Œç¬¬ 3 æœŸï¼ˆ1985 å¹´ï¼‰ï¼š287â€“296ã€‚[`dl.acm.org/doi/pdf/10.1145/325165.325247`](https://dl.acm.org/doi/pdf/10.1145/325165.325247)

5\. Ronneberger, Olaf, Philipp Fischer å’Œ Thomas Broxã€‚â€œU-netï¼šç”¨äºç”Ÿç‰©åŒ»å­¦å›¾åƒåˆ†å‰²çš„å·ç§¯ç½‘ç»œã€‚â€ è§ *åŒ»å­¦å›¾åƒè®¡ç®—ä¸è®¡ç®—æœºè¾…åŠ©å¹²é¢„â€“MICCAI 2015ï¼šç¬¬ 18 å±Šå›½é™…ä¼šè®®ï¼Œå¾·å›½æ…•å°¼é»‘ï¼Œ2015 å¹´ 10 æœˆ 5 æ—¥è‡³ 9 æ—¥ï¼Œä¼šè®®å½•ï¼Œç¬¬ä¸‰éƒ¨åˆ† 18*ï¼Œç¬¬ 234â€“241 é¡µã€‚æ–½æ™®æ—æ ¼å›½é™…å‡ºç‰ˆå…¬å¸ï¼Œ2015 å¹´ã€‚[`arxiv.org/pdf/1505.04597`](https://arxiv.org/pdf/1505.04597)

6\. He, Kaiming ç­‰äººã€‚â€œæ·±åº¦æ®‹å·®å­¦ä¹ ç”¨äºå›¾åƒè¯†åˆ«ã€‚â€ *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®è®ºæ–‡é›†*ã€‚2016ã€‚[`openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf`](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

===========================================

***æˆ‘åœ¨ Medium ä¸Šçš„æ‰€æœ‰å‡ºç‰ˆç‰©éƒ½æ˜¯å…è´¹çš„å¹¶ä¸”å¼€æ”¾è®¿é—®çš„ï¼Œå› æ­¤å¦‚æœä½ åœ¨è¿™é‡Œå…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸æ„Ÿæ¿€ï¼***

P.s. æˆ‘å¯¹ï¼ˆåœ°ç†ï¼‰æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ /äººå·¥æ™ºèƒ½å’Œæ°”å€™å˜åŒ–å……æ»¡çƒ­æƒ…ã€‚å¦‚æœä½ æƒ³åˆä½œè¿›è¡ŒæŸäº›é¡¹ç›®ï¼Œè¯·åœ¨[LinkedIn](https://www.linkedin.com/in/alexxxroz/)ä¸Šè”ç³»æˆ‘ã€‚

ğŸ›°ï¸å…³æ³¨ä»¥è·å–æ›´å¤šä¿¡æ¯ğŸ›°ï¸
