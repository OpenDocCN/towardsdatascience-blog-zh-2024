<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Crash Course of Planning for Perception Engineers in Autonomous Driving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Crash Course of Planning for Perception Engineers in Autonomous Driving</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30">https://towardsdatascience.com/a-crash-course-of-planning-for-perception-engineers-in-autonomous-driving-ede324d78717?source=collection_archive---------1-----------------------#2024-06-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f91e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The fundamentals of planning and decision-making</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Patrick Langechuan Liu" class="l ep by dd de cx" src="../Images/fecbf85146a9bde21e6b2251538ddd65.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*ZjmT1D-KR0ssWtHTkZN5nQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@patrickllgc?source=post_page---byline--ede324d78717--------------------------------" rel="noopener follow">Patrick Langechuan Liu</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ede324d78717--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">44 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5098127e65432ebcafff9731d7e5c451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWSM1_eDQHacKnxKTTLrZw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">AlphaGo, ChatGPT and FSD (image credit <a class="af nc" href="https://unsplash.com/@elenapopova?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Elena Popova</a>, <a class="af nc" href="https://unsplash.com/@karthik1324?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Karthik Sridasyam</a> and <a class="af nc" href="https://unsplash.com/@jupp?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Jonathan Kemper</a> on <a class="af nc" href="https://unsplash.com/photos/a-computer-screen-with-a-text-description-on-it-5yuRImxKOcU?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a>)</figcaption></figure><p id="0681" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A classical modular autonomous driving system typically consists of perception, prediction, planning, and control. Until around 2023, AI (artificial intelligence) or ML (machine learning) primarily enhanced <strong class="nf fr">perception</strong> in most mass-production autonomous driving systems, with its influence diminishing in downstream components. In stark contrast to the low integration of AI in the planning stack, end-to-end perception systems (such as the <a class="af nc" href="https://medium.com/towards-data-science/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944?sk=b7498a375cf92533e8242b1837f29af4" rel="noopener">BEV, or birds-eye-view perception pipeline</a>) have been deployed in <a class="af nc" href="https://medium.com/towards-data-science/bev-perception-in-mass-production-autonomous-driving-c6e3f1e46ae0?sk=8963783161435815fa1b0957fd325d39" rel="noopener">mass production vehicles</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/4f4a37398d69465caf9d19fc89e145ad.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3_lBKtnK3oy1b7NSPiQ9cw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Classical modular design of an autonomous driving stack, 2023 and prior (Chart created by author)</figcaption></figure><p id="f2f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are multiple reasons for this. A classical stack based on a human-crafted framework is more explainable and can be iterated faster to fix field test issues (within hours) compared to machine learning-driven features (which could take days or weeks). However, it does not make sense to let readily available human driving data sit idle. Moreover, increasing computing power is more scalable than expanding the engineering team.</p><p id="fe8c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fortunately, there has been a strong trend in both academia and industry to change this situation. First, downstream modules are becoming increasingly data-driven and may also be integrated via different interfaces, such as the one proposed in <a class="af nc" href="https://arxiv.org/abs/2212.10156" rel="noopener ugc nofollow" target="_blank">CVPR 2023’s best paper, UniAD</a>. Moreover, driven by the ever-growing wave of Generative AI, a single unified vision-language-action (VLA) model shows great potential for handling complex robotics tasks (<a class="af nc" href="https://robotics-transformer2.github.io/assets/rt2.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> in academia, TeslaBot and 1X in industry) and autonomous driving (<a class="af nc" href="https://arxiv.org/abs/2309.17080" rel="noopener ugc nofollow" target="_blank">GAIA-1</a>, <a class="af nc" href="https://arxiv.org/abs/2402.12289" rel="noopener ugc nofollow" target="_blank">DriveVLM</a> in academia, and Wayve AI driver, Tesla FSD in industry). This brings the toolsets of AI and data-driven development from the perception stack to the planning stack.</p><p id="a22a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This blog post aims to introduce the problem settings, existing methodologies, and challenges of the planning stack, in the form of a crash course for perception engineers. As a perception engineer, I finally had some time over the past couple of weeks to systematically learn the classical planning stack, and I would like to share what I learned. I will also share my thoughts on how AI can help from the perspective of an AI practitioner.</p><p id="3880" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The intended audience for this post is AI practitioners who work in the field of autonomous driving, in particular, perception engineers.</p><p id="72cb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The article is a bit long (11100 words), and the table of contents below will most likely help those who want to do quick ctrl+F searches with the keywords.</p><p id="fb30" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Chinese version is available on <a class="af nc" href="https://zhuanlan.zhihu.com/p/706193528" rel="noopener ugc nofollow" target="_blank">Zhihu知乎</a>.</p><pre class="mm mn mo mp mq oa ob oc bp od bb bk"><span id="70d0" class="oe of fq ob b bg og oh l oi oj"><strong class="ob fr">Table of Contents (ToC)</strong><br/><br/>Why learn planning?<br/>What is planning?<br/>  The problem formulation<br/>  The Glossary of Planning<br/>  Behavior Planning<br/>  Frenet vs Cartesian systems<br/>Classical tools-the troika of planning<br/>  Searching<br/>  Sampling<br/>  Optimization<br/>Industry practices of planning<br/>  Path-speed decoupled planning<br/>  Joint spatiotemporal planning<br/>Decision making<br/>  What and why?<br/>  MDP and POMDP<br/>  Value iteration and Policy iteration<br/>  AlphaGo and MCTS-when nets meet trees<br/>  MPDM (and successors) in autonomous driving<br/>Industry practices of decision making<br/>  Trees<br/>  No trees<br/>Self-Reflections<br/>  Why NN in planning?<br/>  What about e2e NN planners?<br/>  Can we do without prediction?<br/>  Can we do with just nets but no trees?<br/>  Can we use LLMs to make decisions?<br/>The trend of evolution</span></pre><h1 id="ea41" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Why learn planning?</h1><p id="5da4" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">This brings us to an interesting question: why learn planning, especially the classical stack, in the era of AI?</p><p id="1df8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From a problem-solving perspective, understanding your customers’ challenges better will enable you, as a perception engineer, to serve your downstream customers more effectively, even if your main focus remains on perception work.</p><p id="2a7f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Machine learning is a tool, not a solution. The most efficient way to solve problems is to combine new tools with domain knowledge, especially those with solid mathematical formulations. Domain knowledge-inspired learning methods are likely to be more data-efficient. As planning transitions from rule-based to ML-based systems, even with early prototypes and products of end-to-end systems hitting the road, there is a need for engineers who can deeply understand both the fundamentals of planning and machine learning. Despite these changes, classical and learning methods will likely continue to coexist for a considerable period, perhaps shifting from an 8:2 to a 2:8 ratio. It is almost essential for engineers working in this field to understand both worlds.</p><p id="06a3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From a value-driven development perspective, understanding the limitations of classical methods is crucial. This insight allows you to effectively utilize new ML tools to design a system that addresses current issues and delivers immediate impact.</p><p id="e45f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally, planning is a critical part of all autonomous agents, not just in autonomous driving. Understanding what planning is and how it works will enable more ML talents to work on this exciting topic and contribute to the development of truly autonomous agents, whether they are cars or other forms of automation.</p><h1 id="9160" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">What is planning?</h1><h2 id="4a5f" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">The problem formulation</h2><p id="fbf1" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">As the “brain” of autonomous vehicles, the planning system is crucial for the safe and efficient driving of vehicles. The goal of the planner is to generate trajectories that are safe, comfortable, and efficiently progressing towards the goal. In other words, safety, comfort, and efficiency are the three key objectives for planning.</p><p id="9df1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As input to the planning systems, all perception outputs are required, including static road structures, dynamic road agents, free space generated by occupancy networks, and traffic wait conditions. The planning system must also ensure vehicle comfort by monitoring acceleration and jerk for smooth trajectories, while considering interaction and traffic courtesy.</p><p id="6ee3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The planning systems generate trajectories in the format of a sequence of waypoints for the ego vehicle’s low-level controller to track. Specifically, these waypoints represent the future positions of the ego vehicle at a series of fixed time stamps. For example, each point might be 0.4 seconds apart, covering an 8-second planning horizon, resulting in a total of 20 waypoints.</p><p id="4465" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A classical planning stack roughly consists of global route planning, local behavior planning, and local trajectory planning. Global route planning provides a road-level path from the start point to the end point on a global map. Local behavior planning decides on a semantic driving action type (e.g., car following, nudging, side passing, yielding, and overtaking) for the next several seconds. Based on the decided behavior type from the behavior planning module, local trajectory planning generates a short-term trajectory. The global route planning is typically provided by a map service once navigation is set and is beyond the scope of this post. We will focus on behavior planning and trajectory planning from now on.</p><p id="876c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Behavior planning and trajectory generation can work explicitly in tandem or be combined into a single process. In explicit methods, behavior planning and trajectory generation are distinct processes operating within a hierarchical framework, working at different frequencies, with behavior planning at 1–5 Hz and trajectory planning at 10–20 Hz. Despite being highly efficient most of the time, adapting to different scenarios may require significant modifications and fine-tuning. More advanced planning systems combine the two into a single optimization problem. This approach ensures feasibility and optimality without any compromise.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/84bae81967ad3e62c3fd2330dc51aa15.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fDiwxk5zMr0nKsn5bChWhQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Classification of planning design approaches (source: <a class="af nc" href="https://arxiv.org/abs/2406.05708" rel="noopener ugc nofollow" target="_blank">Fluid Dynamics Planner</a>)</figcaption></figure><h2 id="c126" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">The Glossary of Planning</h2><p id="420d" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">You might have noticed that the terminology used in the above section and the image do not completely match. There is no standard terminology that everyone uses. Across both academia and industry, it is not uncommon for engineers to use different names to refer to the same concept and the same name to refer to different concepts. This indicates that planning in autonomous driving is still under active development and has not fully converged.</p><p id="7d60" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here, I list the notation used in this post and briefly explain other notions present in the literature.</p><ul class=""><li id="bc52" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qb qc qd bk">Planning: A top-level concept, parallel to control, that generates trajectory waypoints. Together, planning and control are jointly referred to as PnC (planning and control).</li><li id="f9e1" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Control: A top-level concept that takes in trajectory waypoints and generates high-frequency steering, throttle, and brake commands for actuators to execute. Control is relatively well-established compared to other areas and is beyond the scope of this post, despite the common notion of PnC.</li><li id="e33f" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Prediction: A top-level concept that predicts the future trajectories of traffic agents other than the ego vehicle. Prediction can be considered a lightweight planner for other agents and is also called motion prediction.</li><li id="62b2" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Behavior Planning: A module that produces high-level semantic actions (e.g., lane change, overtake) and typically generates a coarse trajectory. It is also known as task planning or decision making, particularly in the context of interactions.</li><li id="ffac" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Motion Planning: A module that takes in semantic actions and produces smooth, feasible trajectory waypoints for the duration of the planning horizon for control to execute. It is also referred to as trajectory planning.</li><li id="92ed" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Trajectory Planning: Another term for motion planning.</li><li id="5654" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Decision Making: Behavior planning with a focus on interactions. Without ego-agent interaction, it is simply referred to as behavior planning. It is also known as tactical decision making.</li><li id="613f" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Route Planning: Finds the preferred route over road networks, also known as mission planning.</li><li id="5345" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Model-Based Approach: In planning, this refers to manually crafted frameworks used in the classical planning stack, as opposed to neural network models. Model-based methods contrast with learning-based methods.</li><li id="da49" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Multimodality: In the context of planning, this typically refers to multiple intentions. This contrasts with multimodality in the context of multimodal sensor inputs to perception or multimodal large language models (such as VLM or VLA).</li><li id="6ff3" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Reference Line: A local (several hundred meters) and coarse path based on global routing information and the current state of the ego vehicle.</li><li id="6acf" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Frenet Coordinates: A coordinate system based on a reference line. Frenet simplifies a curvy path in Cartesian coordinates to a straight tunnel model. See below for a more detailed introduction.</li><li id="1bff" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Trajectory: A 3D spatiotemporal curve, in the form of (x, y, t) in Cartesian coordinates or (s, l, t) in Frenet coordinates. A trajectory is composed of both path and speed.</li><li id="cb5c" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Path: A 2D spatial curve, in the form of (x, y) in Cartesian coordinates or (s, l) in Frenet coordinates.</li><li id="d733" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Semantic Action: A high-level abstraction of action (e.g., car following, nudge, side pass, yield, overtake) with clear human intention. Also referred to as intention, policy, maneuver, or primitive motion.</li><li id="923e" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Action: A term with no fixed meaning. It can refer to the output of control (high-frequency steering, throttle, and brake commands for actuators to execute) or the output of planning (trajectory waypoints). Semantic action refers to the output of behavior prediction.</li></ul><p id="57f7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Different literature may use various notations and concepts. Here are some examples:</p><ul class=""><li id="4a1e" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qb qc qd bk">Decision Making System: Sometimes includes planning and control as well. (source: <a class="af nc" href="https://arxiv.org/pdf/1604.07446" rel="noopener ugc nofollow" target="_blank">A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles</a>, and <a class="af nc" href="https://arxiv.org/abs/2310.10357" rel="noopener ugc nofollow" target="_blank">BEVGPT</a>)</li><li id="5e63" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Motion Planning: Sometimes is the top-level planning concept and includes behavior planning and trajectory planning. (source: <a class="af nc" href="https://arxiv.org/abs/2406.05708" rel="noopener ugc nofollow" target="_blank">Towards A General-Purpose Motion Planning for Autonomous Vehicles Using Fluid Dynamics</a>).</li><li id="7d41" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Planning: Sometimes includes behavior planning, motion planning, and also route planning.</li></ul><p id="267d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These variations illustrate the diversity in terminology and the evolving nature of the field.</p><h2 id="24d1" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Behavior Planning</h2><p id="18ac" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">As a machine learning engineer, you may notice that the behavior planning module is a heavily manually crafted intermediate module. There is no consensus on the exact form and content of its output. Concretely, the output of behavior planning can be a reference path or object labeling on ego maneuvers (e.g., pass from the left or right-hand side, pass or yield). The term “semantic action” has no strict definition and no fixed methods.</p><p id="638f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The decoupling of behavior planning and motion planning increases efficiency in solving the extremely high-dimensional action space of autonomous vehicles. The actions of an autonomous vehicle need to be reasoned at typically 10 Hz or more (time resolution in waypoints), and most of these actions are relatively straightforward, like going straight. After decoupling, the behavior planning layer only needs to reason about future scenarios at a relatively coarse resolution, while the motion planning layer operates in the local solution space based on the decision made by behavior planning. Another benefit of behavior planning is converting non-convex optimization to convex optimization, which we will discuss further below.</p><h2 id="1bed" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Frenet vs Cartesian systems</h2><p id="1f20" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">The Frenet coordinate system is a widely adopted system that merits its own introduction section. The Frenet frame simplifies trajectory planning by independently managing lateral and longitudinal movements relative to a reference path. The sss coordinate represents longitudinal displacement (distance along the road), while the lll (or ddd) coordinate represents lateral displacement (side position relative to the reference path).</p><p id="3a6b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Frenet simplifies a curvy path in Cartesian coordinates to a straight tunnel model. This transformation converts non-linear road boundary constraints on curvy roads into linear ones, significantly simplifying the subsequent optimization problems. Additionally, humans perceive longitudinal and lateral movements differently, and the Frenet frame allows for separate and more flexible optimization of these movements.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/557bbcb9cf482212a57762342fd2058c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uL_mkKWOEJZJMqkT3rd0kw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Schematics on the conversion from Cartesian frame to Frenet frame (source: <a class="af nc" href="https://ieeexplore.ieee.org/document/9703250" rel="noopener ugc nofollow" target="_blank">Cartesian Planner</a>)</figcaption></figure><p id="a4e4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Frenet coordinate system requires a clean, structured road graph with low curvature lanes. In practice, it is preferred for structured roads with small curvature, such as highways or city expressways. However, the issues with the Frenet coordinate system are amplified with increasing reference line curvature, so it should be used cautiously on structured roads with high curvature, like city intersections with guide lines.</p><p id="80d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For unstructured roads, such as ports, mining areas, parking lots, or intersections without guidelines, the more flexible Cartesian coordinate system is recommended. The Cartesian system is better suited for these environments because it can handle higher curvature and less structured scenarios more effectively.</p><h1 id="32c2" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Classical tools — the troika of planning</h1><p id="64ff" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Planning in autonomous driving involves computing a trajectory from an initial high-dimensional state (including position, time, velocity, acceleration, and jerk) to a target subspace, ensuring all constraints are satisfied. Searching, sampling, and optimization are the three most widely used tools for planning.</p><h2 id="717c" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Searching</h2><p id="62b9" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Classical graph-search methods are popular in planning and are used in route/mission planning on structured roads or directly in motion planning to find the best path in unstructured environments (such as parking or urban intersections, especially mapless scenarios). There is a clear evolution path, from Dijkstra’s algorithm to A* (A-star), and further to hybrid A*.</p><p id="ae34" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Dijkstra’s algorithm explores all possible paths to find the shortest one, making it a blind (uninformed) search algorithm. It is a systematic method that guarantees the optimal path, but it is inefficient to deploy. As shown in the chart below, it explores almost all directions. Essentially, Dijkstra’s algorithm is a breadth-first search (BFS) weighted by movement costs. To improve efficiency, we can use information about the location of the target to trim down the search space.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/90d0ec4849efafff5dd561d12527be42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QcNWlOj_fT4Afzmh-klyw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualization of Dijkstra’s algorithm and A-star search (Source: <a class="af nc" href="https://qiao.github.io/PathFinding.js/visual/" rel="noopener ugc nofollow" target="_blank">PathFinding.js</a>, example inspired by <a class="af nc" href="https://www.redblobgames.com/pathfinding/a-star/introduction.html" rel="noopener ugc nofollow" target="_blank">RedBlobGames</a>)</figcaption></figure><p id="a7f8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The A* algorithm uses heuristics to prioritize paths that appear to be leading closer to the goal, making it more efficient. It combines the cost so far (Dijkstra) with the cost to go (heuristics, essentially greedy best-first). A* only guarantees the shortest path if the heuristic is admissible and consistent. If the heuristic is poor, A* can perform worse than the Dijkstra baseline and may degenerate into a greedy best-first search.</p><p id="dc80" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the specific application of autonomous driving, the hybrid A* algorithm further improves A* by considering vehicle kinematics. A* may not satisfy kinematic constraints and cannot be tracked accurately (e.g., the steering angle is typically within 40 degrees). While A* operates in grid space for both state and action, hybrid A* separates them, maintaining the state in the grid but allowing continuous action according to kinematics.</p><p id="aec5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Analytical expansion (shot to goal) is another key innovation proposed by hybrid A*. A natural enhancement to A* is to connect the most recently explored nodes to the goal using a non-colliding straight line. If this is possible, we have found the solution. In hybrid A*, this straight line is replaced by Dubins and Reeds-Shepp (RS) curves, which comply with vehicle kinematics. This early stopping method strikes a balance between optimality and feasibility by focusing more on feasibility for the further side.</p><p id="8e98" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Hybrid A* is used heavily in parking scenarios and mapless urban intersections. Here is a very nice video showcasing how it works in a parking scenario.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/3f58234e1f2851acba82d22fabb261e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YuP96Jq__dEBOxs0RMZzEA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Hybrid A-star algorithm with analytical expansion (source: the <a class="af nc" href="https://www.semanticscholar.org/paper/Path-Planning-for-Autonomous-Vehicles-in-Unknown-Dolgov-Thrun/0e8c927d9c2c46b87816a0f8b7b8b17ed1263e9c" rel="noopener ugc nofollow" target="_blank">2010 IJRR Hybrid A-star paper</a> and <a class="af nc" href="https://www.youtube.com/watch?v=qXZt-B7iUyw&amp;ab_channel=Udacity" rel="noopener ugc nofollow" target="_blank">2012 Udacity class</a> )</figcaption></figure><h2 id="beed" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Sampling</h2><p id="f001" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Another popular method of planning is sampling. The well-known Monte Carlo method is a random sampling method. In essence, sampling involves selecting many candidates randomly or according to a prior, and then selecting the best one according to a defined cost. For sampling-based methods, the fast evaluation of many options is critical, as it directly impacts the real-time performance of the autonomous driving system.</p><blockquote class="qm qn qo"><p id="6f65" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Large Language Models (LLMs) essentially provide samples, and there needs to be an evaluator with a defined cost that aligns with human preferences. This evaluation process ensures that the selected output meets the desired criteria and quality standards. This process is basically the so-called alignment.</p></blockquote><p id="b240" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sampling can occur in a parameterized solution space if we already know the analytical solution to a given problem or subproblem. For example, typically we want to minimize the time integral of the square of jerk (the third derivative of position p(t)), indicated by the triple dots over p, where one dot represents one order derivative with respect to time), among other criteria.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/e328957af9c4e4da4e980f475582b866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6qpmKLAey75YPa_ENh-7rA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Minimizing squared jerk for driving comfort (source: <a class="af nc" href="https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760" rel="noopener ugc nofollow" target="_blank">Werling et al</a>, ICRA 2010)</figcaption></figure><p id="8b06" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It can be mathematically proven that quintic (5th order) polynomials provide the jerk-optimal connection between two states in a position-velocity-acceleration space, even when additional cost terms are considered. By sampling in this parameter space of quintic polynomials, we can find the one with the minimum cost to get the approximate solution. The cost takes into account factors such as speed, acceleration, jerk limit, and collision checks. This approach essentially solves the optimization problem through sampling.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/22c50c86d7b5b1fd2a459d17fafece5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K3QIk1AEx0yfJk9psGYA9A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Sampling of lateral movement time profiles (source: <a class="af nc" href="https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760" rel="noopener ugc nofollow" target="_blank">Werling et al</a>, ICRA 2010)</figcaption></figure><p id="65b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sampling-based methods have inspired numerous ML papers, including CoverNet, Lift-Splat-Shoot, NMP, and MP3. These methods replace mathematically sound quintic polynomials with human driving behavior, utilizing a large database. The evaluation of trajectories can be easily parallelized, which further supports the use of sampling-based methods. This approach effectively leverages a vast amount of expert demonstrations to mimic human-like driving behavior, while avoiding random sampling of acceleration and steering profiles.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/98b82507bc9c5bf06c32603d8a6d7823.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9LdWQ6KSAxURdAlJTc_kNw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Sampling from human-driving data for data-driven planning methods (source: <a class="af nc" href="http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf" rel="noopener ugc nofollow" target="_blank">NMP</a>, <a class="af nc" href="https://arxiv.org/abs/1911.10298" rel="noopener ugc nofollow" target="_blank">CoverNet</a> and <a class="af nc" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">Lift-splat-shoot</a>)</figcaption></figure><h2 id="6329" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Optimization</h2><p id="ea10" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Optimization finds the best solution to a problem by maximizing or minimizing a specific objective function under given constraints. In neural network training, a similar principle is followed using gradient descent and backpropagation to adjust the network’s weights. However, in optimization tasks outside of neural networks, models are usually less complex, and more effective methods than gradient descent are often employed. For example, while gradient descent can be applied to Quadratic Programming, it is generally not the most efficient method.</p><p id="84ad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In autonomous driving, the planning cost to optimize typically considers dynamic objects for obstacle avoidance, static road structures for following lanes, navigation information to ensure the correct route, and ego status to evaluate smoothness.</p><p id="d948" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Optimization can be categorized into convex and non-convex types. The key distinction is that in a convex optimization scenario, there is only one global optimum, which is also the local optimum. This characteristic makes it unaffected by the initial solution to the optimization problems. For non-convex optimization, the initial solution matters a lot, as illustrated in the chart below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/697da6b0533847e3cebf23f289d27301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DcMVxF5gxTcQWiimsytBA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Convex vs non-convex optimization (source: <a class="af nc" href="https://stanford.edu/~pilanci/papers/TALK_Sketching.pdf" rel="noopener ugc nofollow" target="_blank">Stanford course materials</a>)</figcaption></figure><p id="b05c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since planning involves highly non-convex optimization with many local optima, it heavily depends on the initial solution. Additionally, convex optimization typically runs much faster and is therefore preferred for onboard real-time applications such as autonomous driving. A typical approach is to use convex optimization in conjunction with other methods to outline a convex solution space first. This is the mathematical foundation behind separating behavior planning and motion planning, where finding a good initial solution is the role of behavior planning.</p><p id="ba40" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take obstacle avoidance as a concrete example, which typically introduces non-convex problems. If we know the nudging direction, then it becomes a convex optimization problem, with the obstacle position acting as a lower or upper bound constraint for the optimization problem. If we don’t know the nudging direction, we need to decide first which direction to nudge, making the problem a convex one for motion planning to solve. This nudging direction decision falls under behavior planning.</p><blockquote class="qm qn qo"><p id="0280" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Of course, we can do direct optimization of non-convex optimization problems with tools such as projected gradient descent, alternating minimization, particle swarm optimization (PSO), and genetic algorithms. However, this is beyond the scope of this post.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/6a71f4bf5ba9b067048f77546c92d93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEi1uAqPMUWPByf5cpBszg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A convex path planning problem vs a non-convex one (chart made by author)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/5aec290676543117fa55a96f20ec808c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-WK2SYtsBRXue8kzq11iw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The solution process of the convex vs non-convex path planning problem (chart made by author)</figcaption></figure><p id="d62b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">How do we make such decisions? We can use the aforementioned search or sampling methods to address non-convex problems. Sampling-based methods scatter many options across the parameter space, effectively handling non-convex issues similarly to searching.</p><p id="635c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You may also question why deciding which direction to nudge from is enough to guarantee the problem space is convex. To explain this, we need to discuss topology. In path space, similar feasible paths can transform continuously into each other without obstacle interference. These similar paths, grouped as “homotopy classes” in the formal language of topology, can all be explored using a single initial solution homotopic to them. All these paths form a driving corridor, illustrated as the red or green shaded area in the image above. For a 3D spatiotemporal case, please refer to the <a class="af nc" href="https://zhuanlan.zhihu.com/p/551381336" rel="noopener ugc nofollow" target="_blank">QCraft tech blog</a>.</p><blockquote class="qm qn qo"><p id="daa9" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can utilize the <a class="af nc" href="https://zhuanlan.zhihu.com/p/551381336" rel="noopener ugc nofollow" target="_blank"><em class="fq">Generalized Voronoi diagram</em></a> to enumerate all homotopy classes, which roughly corresponds to the different decision paths available to us. However, this topic delves into advanced mathematical concepts that are beyond the scope of this blog post.</p></blockquote><p id="c9ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The key to solving optimization problems efficiently lies in the capabilities of the optimization solver. Typically, a solver requires approximately 10 milliseconds to plan a trajectory. If we can boost this efficiency by tenfold, it can significantly impact algorithm design. This exact improvement was highlighted during Tesla AI Day 2022. A similar enhancement has occurred in perception systems, transitioning from 2D perception to Bird’s Eye View (BEV) as available computing power scaled up tenfold. With a more efficient optimizer, more options can be calculated and evaluated, thereby reducing the importance of the decision-making process. However, engineering an efficient optimization solver demands substantial engineering resources.</p><blockquote class="qm qn qo"><p id="3d49" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Every time compute scales up by 10x, algorithm will evolve to next generation. <br/>— — The unverified law of algorithm evolution</p></blockquote><h1 id="da75" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Industry Practices of Planning</h1><p id="2563" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">A key differentiator in various planning systems is whether they are spatiotemporally decoupled. Concretely, spatiotemporally decoupled methods plan in spatial dimensions first to generate a path, and then plan the speed profile along this path. This approach is also known as path-speed decoupling.</p><p id="4d33" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Path-speed decoupling is often referred to as lateral-longitudinal (lat-long) decoupling, where lateral (lat) planning corresponds to path planning and longitudinal (long) planning corresponds to speed planning. This terminology seems to originate from the Frenet coordinate system explained earlier.</p><p id="0ed1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Decoupled solutions are easier to implement and can solve about 95% of issues. In contrast, coupled solutions have a higher theoretical performance ceiling but are more challenging to implement. They involve more parameters to tune and require a more principled approach to parameter tuning.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4f7f92a26aabf6071ff72ba10662c0c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gqDr-tlWaqRTrX9mmncuw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The comparison of decoupled and joint planning (source: made by the author, inspired by <a class="af nc" href="https://www.xchuxing.com/article/63767" rel="noopener ugc nofollow" target="_blank">Qcraft</a>)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/80caac84205c7a3876e6207a9d7e841f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kWxc3ZMn2uBUtf3Yv3CZA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Pros and cons of decoupled vs joint spatiotemporal planning (chart made by author)</figcaption></figure><h2 id="dfc2" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Path-speed decoupled planning</h2><p id="2c39" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">We can take <a class="af nc" href="https://arxiv.org/abs/1807.08048" rel="noopener ugc nofollow" target="_blank">Baidu Apollo EM planner</a> as an example of a system that uses path-speed decoupled planning.</p><p id="0fdd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The EM planner significantly reduces computational complexity by transforming a three-dimensional station-lateral-speed problem into two two-dimensional problems: station-lateral and station-speed. At the core of Apollo’s EM planner is an iterative Expectation-Maximization (EM) step, consisting of path optimization and speed optimization. Each step is divided into an E-step (projection and formulation in a 2D state space) and an M-step (optimization in the 2D state space). The E-step involves projecting the 3D problem into either a Frenet SL frame or an ST speed tracking frame.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/dc607d36c1a73faf84d32ee4fd982b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XeqS4MPNs6IIDHBanmcM4Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The EM iteration in Apollo EM planner (source: <a class="af nc" href="https://arxiv.org/abs/1807.08048" rel="noopener ugc nofollow" target="_blank">Baidu Apollo EM planner</a> )</figcaption></figure><p id="15f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The M-step (maximization step) in both path and speed optimization involves solving non-convex optimization problems. For path optimization, this means deciding whether to nudge an object on the left or right side, while for speed optimization, it involves deciding whether to overtake or yield to a dynamic object crossing the path. The Apollo EM planner addresses these non-convex optimization challenges using a two-step process: Dynamic Programming (DP) followed by Quadratic Programming (QP).</p><p id="56a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">DP uses a sampling or searching algorithm to generate a rough initial solution, effectively pruning the non-convex space into a convex space. QP then takes the coarse DP results as input and optimizes them within the convex space provided by DP. In essence, DP focuses on feasibility, and QP refines the solution to achieve optimality within the convex constraints.</p><p id="1d5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our defined terminology, Path DP corresponds to lateral BP, Path QP to lateral MP, Speed DP to longitudinal BP, and Speed QP to longitudinal MP. Thus, the process involves conducting BP (Basic Planning) followed by MP (Master Planning) in both the path and speed steps.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/f32f0b7181bbaae16ff39a41c4366104.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UBfD_OBlts1rAX_55lobww.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A full autonomous driving stack with path-speed decoupled planning (chart made by author)</figcaption></figure><h2 id="f19c" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Joint spatiotemporal planning</h2><p id="c352" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Although decoupled planning can resolve 95% of cases in autonomous driving, the remaining 5% involve challenging dynamic interactions where a decoupled solution often results in suboptimal trajectories. In these complex scenarios, demonstrating intelligence is crucial, making it a very hot topic in the field.</p><p id="b96d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For example, in narrow-space passing, the optimal behavior might be to either decelerate to yield or accelerate to pass. Such behaviors are not achievable within the decoupled solution space and require joint optimization. Joint optimization allows for a more integrated approach, considering both path and speed simultaneously to handle intricate dynamic interactions effectively.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/d32a376deca659417fe15fe58533a45e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hqhJyBdq61ZMIENawTjBOQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A full autonomous driving stack with joint spatiotemporal planning (chart made by author)</figcaption></figure><p id="f62d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, there are significant challenges in joint spatiotemporal planning. Firstly, solving the non-convex problem directly in a higher-dimensional state space is more challenging and time-consuming than using a decoupled solution. Secondly, considering interactions in spatiotemporal joint planning is even more complex. We will cover this topic in more detail later when we discuss decision-making.</p><p id="5329" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here we introduce two solving methods: brute force search and constructing a spatiotemporal corridor for optimization.</p><p id="f787" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Brute force search occurs directly in 3D spatiotemporal space (2D in space and 1D in time), and can be performed in either XYT (Cartesian) or SLT (Frenet) coordinates. We will take SLT as an example. SLT space is long and flat, similar to an energy bar. It is elongated in the L dimension and flat in the ST face. For brute force search, we can use hybrid A-star, with the cost being a combination of progress cost and cost to go. During optimization, we must conform to search constraints that prevent reversing in both the s and t dimensions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/f2dd701764070281b4288f1bd789052f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPD3Qtape0JdKH4qG6LeZw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Overtake by lane change in spatiotemporal lattice (source: <a class="af nc" href="https://www.qichegongcheng.com/CN/abstract/abstract1500.shtml" rel="noopener ugc nofollow" target="_blank">Spatiotemporal optimization with A*</a>)</figcaption></figure><p id="9b5d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another method is constructing a spatiotemporal corridor, essentially a curve with the footprint of a car winding through a 3D spatiotemporal state space (SLT, for example). The <a class="af nc" href="https://arxiv.org/abs/1906.09788" rel="noopener ugc nofollow" target="_blank">SSC (spatiotemporal semantic corridor, RAL 2019)</a>, encodes requirements given by semantic elements into a semantic corridor, generating a safe trajectory accordingly. The semantic corridor consists of a series of mutually connected collision-free cubes with dynamical constraints posed by the semantic elements in the spatiotemporal domain. Within each cube, it becomes a convex optimization problem that can be solved using Quadratic Programming (QP).</p><p id="e2b5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">SSC still requires a BP (Behavior Planning) module to provide a coarse driving trajectory. Complex semantic elements of the environment are projected into the spatiotemporal domain concerning the reference lane. <a class="af nc" href="https://arxiv.org/abs/2108.07993" rel="noopener ugc nofollow" target="_blank">EPSILON (TRO 2021)</a>, showcases a system where SSC serves as the motion planner working in tandem with a behavior planner. In the next section, we will discuss behavior planning, especially focusing on interaction. In this context, behavior planning is usually referred to as decision making.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/abad96e9b684fca4f58bce59ddb353e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmnKHvQyuMFUD3mGD5Mttg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">An illustration of the spatiotemporal corridor (source: <a class="af nc" href="https://arxiv.org/abs/1906.09788" rel="noopener ugc nofollow" target="_blank">SSC</a>)</figcaption></figure><h1 id="3007" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Decision making</h1><h2 id="4557" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">What and why?</h2><p id="4730" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Decision making in autonomous driving is essentially behavior planning, but with a focus on interaction with other traffic agents. The assumption is that other agents are mostly rational and will respond to our behavior in a predictable manner, which we can describe as “noisily rational.”</p><p id="99b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">People may question the necessity of decision making when advanced planning tools are available. However, two key aspects — uncertainty and interaction — introduce a probabilistic nature to the environment, primarily due to the presence of dynamic objects. Interaction is the most challenging part of autonomous driving, distinguishing it from general robotics. Autonomous vehicles must not only navigate but also anticipate and react to the behavior of other agents, making robust decision-making essential for safety and efficiency.</p><p id="f09f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In a deterministic (purely geometric) world without interaction, decision making would be unnecessary, and planning through searching, sampling, and optimization would suffice. Brute force searching in the 3D XYT space could serve as a general solution.</p><p id="44e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In most classical autonomous driving stacks, a prediction-then-plan approach is adopted, assuming zero-order interaction between the ego vehicle and other vehicles. This approach treats prediction outputs as deterministic, requiring the ego vehicle to react accordingly. This leads to overly conservative behavior, exemplified by the “freezing robot” problem. In such cases, prediction fills the entire spatiotemporal space, preventing actions like lane changes in crowded conditions — something humans manage more effectively.</p><p id="902e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To handle stochastic strategies, Markov Decision Processes (MDP) or Partially Observable Markov Decision Processes (POMDP) frameworks are essential. These approaches shift the focus from geometry to probability, addressing chaotic uncertainty. By assuming that traffic agents behave rationally or at least noisily rationally, decision making can help create a safe driving corridor in the otherwise chaotic spatiotemporal space.</p><p id="41ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Among the three overarching goals of planning — safety, comfort, and efficiency — decision making primarily enhances efficiency. Conservative actions can maximize safety and comfort, but effective negotiation with other road agents, achievable through decision making, is essential for optimal efficiency. Effective decision making also displays intelligence.</p><h2 id="c56f" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">MDP and POMDP</h2><p id="c116" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">We will first introduce Markov Decision Processes (MDP) and Partially Observable Markov Decision Processes (POMDP), followed by their systematic solutions, such as value iteration and policy iteration.</p><p id="6b61" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A Markov Process (MP) is a type of stochastic process that deals with dynamic random phenomena, unlike static probability. In a Markov Process, the future state depends only on the current state, making it sufficient for prediction. For autonomous driving, the relevant state may only include the last second of data, and we can expand the state space to allow for a shorter history window (for example, instead of a longer history of positions, we can use a shorter history window of position, velocity and acceleration, etc).</p><p id="23a3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A Markov Decision Process (MDP) extends a Markov Process to include decision-making by introducing action. MDPs model decision-making where outcomes are partly random and partly controlled by the decision maker or agent. An MDP can be modeled with five factors:</p><ol class=""><li id="1745" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ra qc qd bk">State (S): The state of the environment.</li><li id="7709" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk">Action (A): The actions the agent can take to affect the environment.</li><li id="85a8" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk">Reward (R): The reward the environment provides to the agent as a result of the action.</li><li id="20e6" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk">Transition Probability (P): The probability of transitioning from the old state to a new state upon the agent’s action.</li><li id="680c" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk">Gamma (γ): A discount factor for future rewards.</li></ol><p id="768f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is also the common framework used by reinforcement learning (RL), which is essentially an MDP. The goal of MDP or RL is to maximize the cumulative reward received in the long run. This requires the agent to make good decisions given a state from the environment, according to a policy.</p><p id="f485" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A policy, π, is a mapping from each state, s ∈ S, and action, a ∈ A(s), to the probability π(a|s) of taking action a when in state s. MDP or RL studies the problem of how to derive the optimal policy.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/498d6c7687a59b14b330d3801aead9cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzvQP0PUn6MveiYrZYKCFg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The agent-environment interface in MDP and RL (source: <a class="af nc" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">Reinforcement Learning: An Introduction</a>)</figcaption></figure><p id="9fa7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A Partially Observable Markov Decision Process (POMDP) adds an extra layer of complexity by recognizing that states cannot be directly observed but rather inferred through observations. In a POMDP, the agent maintains a belief — a probability distribution over possible states — to estimate the state of the environment. Autonomous driving scenarios are better represented by POMDPs due to their inherent uncertainties and the partial observability of the environment. An MDP can be considered a special case of a POMDP where the observation perfectly reveals the state.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/826bad80b9b9275cb7ed42022798660f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ygZgKIzYC5WKVmHC8Ktgmg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">MDP vs POMDP (source: <a class="af nc" href="https://www.researchgate.net/figure/MDP-and-POMDP-visualization_fig1_374986767" rel="noopener ugc nofollow" target="_blank">POMDPs as stochastic contingent planning</a>)</figcaption></figure><p id="bce5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">POMDPs can actively collect information, leading to actions that gather necessary data, demonstrating the intelligent behavior of these models. This capability is particularly valuable in scenarios like waiting at intersections, where gathering information about other vehicles’ intentions and the state of the traffic light is crucial for making safe and efficient decisions.</p><h1 id="27b5" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Value iteration and Policy iteration</h1><p id="c12c" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk"><strong class="nf fr">Value iteration</strong> and <strong class="nf fr">policy iteration</strong> are systematic methods for solving MDP or POMDP problems. While these methods are not commonly used in real-world applications due to their complexity, understanding them provides insight into exact solutions and how they can be simplified in practice, such as using MCTS in AlphaGo or MPDM in autonomous driving.</p><p id="f338" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To find the best policy in an MDP, we must assess the potential or expected reward from a state, or more specifically, from an action taken in that state. This expected reward includes not just the immediate reward but also all future rewards, formally known as the return or cumulative discounted reward. (For a deeper understanding, refer to “<a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener">Reinforcement Learning: An Introduction</a>,” often considered the definitive guide on the subject.)</p><p id="7d5b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The value function (V) characterizes the quality of states by summing the expected returns. The action-value function (Q) assesses the quality of actions for a given state. Both functions are defined according to a given policy. The Bellman Optimality Equation states that an <em class="qp">optimal</em> policy will choose the action that <em class="qp">maximizes</em> the immediate reward plus the expected future rewards from the resulting new states. In simple terms, the Bellman Optimality Equation advises considering both the immediate reward and the future consequences of an action. For example, when switching jobs, consider not only the immediate pay raise (R) but also the future value (S’) the new position offers.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/b78b1653df30f808436b6e3d5b3f3137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMUQxGLWR6E2M_91oX0RpQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Bellman’s equation of optimality (chart made by author)</figcaption></figure><p id="0868" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is relatively straightforward to extract the optimal policy from the Bellman Optimality Equation once the optimal value function is available. But how do we find this optimal value function? This is where value iteration comes to the rescue.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/e68bbaeaec158ca39554728869f81b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fi1RwZcQlLgFNmrFaEmsSg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Extract best policy from optimal values (chart made by author)</figcaption></figure><p id="8ea1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Value iteration finds the best policy by repeatedly updating the value of each state until it stabilizes. This process is derived by turning the Bellman Optimality Equation into an update rule. Essentially, we use the optimal future picture to guide the iteration toward it. In plain language, “fake it until you make it!”</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/288f8fed1d74fe07a5ff0a778685e236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8qmCFlqBf3SgP3RRx7nMQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Update value functions under the guidance of Bellman’s Equation (chart made by author)</figcaption></figure><p id="27dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Value iteration is guaranteed to converge for finite state spaces, regardless of the initial values assigned to the states (for a detailed proof, please refer to the <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//medium.com/p/ede324d78717/edit%23%3A~%3Atext%3Dhttps%253A//web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener">Bible of RL</a>). If the discount factor gamma is set to 0, meaning we only consider immediate rewards, the value iteration will converge after just one iteration. A smaller gamma leads to faster convergence because the horizon of consideration is shorter, though it may not always be the best option for solving concrete problems. Balancing the discount factor is a key aspect of engineering practice.</p><p id="77d2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One might ask how this works if all states are initialized to zero. The immediate reward in the Bellman Equation is crucial for bringing in additional information and breaking the initial symmetry. Think about the states that immediately lead to the goal state; their value propagates through the state space like a virus. In plain language, it’s about making small wins, frequently.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/3e3a80128bd6e391639059888b443e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUBEZOAHvMhMahgMDRYkUQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Value and policy functions interact until they converge to optimum together (source: <a class="af nc" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">Reinforcement Learning: An Introduction</a>)</figcaption></figure><p id="4d95" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, value iteration also suffers from inefficiency. It requires taking the optimal action at each iteration by considering all possible actions, similar to Dijkstra’s algorithm. While it demonstrates feasibility as a basic approach, it is typically not practical for real-world applications.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rh"><img src="../Images/341299e3acf45f2fcba47891bf78fc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UDyxq1umG0h3T0s7hf7m9A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The contrast of Bellman Equation and Bellman Optimality Equation (chart made by author)</figcaption></figure><p id="77c3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Policy iteration improves on this by taking actions according to the current policy and updating it based on the Bellman Equation (not the Bellman Optimality Equation). Policy iteration decouples policy evaluation from policy improvement, making it a much faster solution. Each step is taken based on a given policy instead of exploring all possible actions to find the one that maximizes the objective. Although each iteration of policy iteration can be more computationally intensive due to the policy evaluation step, it generally results in a faster convergence overall.</p><p id="eb9a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In simple terms, if you can only fully evaluate the consequence of one action, it’s better to use your own judgment and do your best with the current information available.</p><h2 id="6377" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">AlphaGo and MCTS — when nets meet trees</h2><p id="704c" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">We have all heard the unbelievable story of AlphaGo beating the best human player in 2016. AlphaGo formulates the gameplay of Go as an MDP and solves it with Monte Carlo Tree Search (MCTS). But why not use value iteration or policy iteration?</p><p id="c91e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Value iteration and policy iteration are systematic, iterative methods that solve MDP problems. However, even with improved policy iteration, it still requires performing time-consuming operations to update the value of every state. A standard 19x19 Go board has roughly <a class="af nc" href="https://senseis.xmp.net/?NumberOfPossibleGoGames=" rel="noopener ugc nofollow" target="_blank">2e170 possible states</a>. This vast number of states makes it intractable to solve with traditional value iteration or policy iteration techniques.</p><p id="e4ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">AlphaGo and its successors use a <a class="af nc" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search" rel="noopener ugc nofollow" target="_blank">Monte Carlo tree search</a> (MCTS) algorithm to find their moves, guided by a value network and a policy network, trained on both human and computer play. Let’s take a look at vanilla MCTS first.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/2103789656021072b20fd0ceb6e0d8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kCcCYVG1yDHtgE-AN985A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The four steps of MCTS by AlphaGo, combining both value network and policy network (source: <a class="af nc" href="https://www.nature.com/articles/nature16961" rel="noopener ugc nofollow" target="_blank">AlphaGo</a>, Nature 2016)</figcaption></figure><p id="b9d8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Monte Carlo Tree Search (MCTS)</strong> is a method for policy estimation that focuses on decision-making from the current state. One iteration involves a four-step process: selection, expansion, simulation (or evaluation), and backup.</p><ol class=""><li id="d51f" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ra qc qd bk"><strong class="nf fr">Selection</strong>: The algorithm follows the most promising path based on previous simulations until it reaches a leaf node, a position not yet fully explored.</li><li id="97dc" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk"><strong class="nf fr">Expansion</strong>: One or more child nodes are added to represent possible next moves from the leaf node.</li><li id="73a8" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk"><strong class="nf fr">Simulation (Evaluation)</strong>: The algorithm plays out a random game from the new node until the end, known as a “rollout.” This assesses the potential outcome from the expanded node by simulating random moves until a terminal state is reached.</li><li id="9540" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk"><strong class="nf fr">Backup</strong>: The algorithm updates the values of the nodes on the path taken based on the game’s result. If the outcome is a win, the value of the nodes increases; if it is a loss, the value decreases. This process propagates the result of the rollout back up the tree, refining the policy based on simulated outcomes.</li></ol><p id="0c2b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After a given number of iterations, MCTS provides the percentage frequency with which immediate actions were selected from the root during simulations. During inference, the action with the most visits is selected. Here is <a class="af nc" href="https://vgarciasc.github.io/mcts-viz/" rel="noopener ugc nofollow" target="_blank">an interactive illustration of MTCS</a> with the game of tic-tac-toe for simplicity.</p><p id="ca6e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">MCTS in AlphaGo is enhanced by two neural networks. <strong class="nf fr">Value Network </strong>evaluates the winning rate from a given state (board configuration). <strong class="nf fr">Policy Network </strong>evaluates the action distribution for all possible moves. These neural networks improve MCTS by reducing the effective depth and breadth of the search tree. The policy network helps in sampling actions, focusing the search on promising moves, while the value network provides a more accurate evaluation of positions, reducing the need for extensive rollouts. This combination allows AlphaGo to perform efficient and effective searches in the vast state space of Go.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/f006e297e1acba4bad8d39b92f318b69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4qGTQHihyYyW3l-V8veSnQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The policy network and value network of AlphaGo (source: <a class="af nc" href="https://www.nature.com/articles/nature16961" rel="noopener ugc nofollow" target="_blank">AlphaGo</a>, Nature 2016)</figcaption></figure><p id="00a0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the <strong class="nf fr">expansion</strong> step, the policy network samples the most likely positions, effectively pruning the breadth of the search space. In the <strong class="nf fr">evaluation</strong> step, the value network provides an instinctive scoring of the position, while a faster, lightweight policy network performs rollouts until the game ends to collect rewards. MCTS then uses a weighted sum of the evaluations from both networks to make the final assessment.</p><blockquote class="qm qn qo"><p id="c76e" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that a single evaluation of the value network approaches the accuracy of Monte Carlo rollouts using the RL policy network but with 15,000 times less computation. This mirrors the fast-slow system design, akin to intuition versus reasoning, or <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//en.wikipedia.org/wiki/Thinking%2C_Fast_and_Slow" rel="noopener">System 1 versus System 2</a> as described by Nobel laureate Daniel Kahneman. Similar designs can be observed in more recent works, such as <a class="af nc" href="https://arxiv.org/abs/2402.12289" rel="noopener ugc nofollow" target="_blank">DriveVLM</a>.</p><p id="9289" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To be exact, AlphaGo incorporates two slow-fast systems at different levels. On the macro level, the policy network selects moves while the faster rollout policy network evaluates these moves. On the micro level, the faster rollout policy network can be approximated by a value network that directly predicts the winning rate of board positions.</p></blockquote><p id="27af" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">What can we learn from AlphaGo for autonomous driving? AlphaGo demonstrates the importance of extracting an excellent policy using a robust world model (simulation). Similarly, autonomous driving requires a highly accurate simulation to effectively leverage algorithms similar to those used by AlphaGo. This approach underscores the value of combining strong policy networks with detailed, precise simulations to enhance decision-making and optimize performance in complex, dynamic environments.</p><h1 id="aed9" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">MPDM (and successors) in autonomous driving</h1><p id="d0f3" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">In the game of Go, all states are immediately available to both players, making it a perfect information game where observation equals state. This allows the game to be characterized by an MDP process. In contrast, autonomous driving is a POMDP process, as the states can only be estimated through observation.</p><p id="dade" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">POMDPs connect perception and planning in a principled way. The typical solution for a POMDP is similar to that for an MDP, with a limited lookahead. However, the main challenges lie in the curse of dimensionality (explosion in state space) and the complex interactions with other agents. To make real-time progress tractable, domain-specific assumptions are typically made to simplify the POMDP problem.</p><p id="9d3d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://ieeexplore.ieee.org/document/7139412" rel="noopener ugc nofollow" target="_blank">MPDM</a> (and the <a class="af nc" href="https://www.roboticsproceedings.org/rss11/p43.pdf" rel="noopener ugc nofollow" target="_blank">two</a> <a class="af nc" href="https://link.springer.com/article/10.1007/s10514-017-9619-z" rel="noopener ugc nofollow" target="_blank">follow-ups</a>, and <a class="af nc" href="https://maymobility.com/resources/autonomy-at-scale-white-paper/" rel="noopener ugc nofollow" target="_blank">the white paper</a>) is one pioneering study in this direction. MPDM reduces the POMDP to a closed-loop forward simulation of a finite, discrete set of semantic-level policies, rather than evaluating every possible control input for every vehicle. This approach addresses the <a class="af nc" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">curse of dimensionality</a> by focusing on a manageable number of meaningful policies, allowing for effective real-time decision-making in autonomous driving scenarios.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rj"><img src="../Images/1e479cccd693fbe81acaa387068e7102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4apeih1scVuW2to5ZSSUjA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Semantic actions help control the curse of dimensionality (source: <a class="af nc" href="https://arxiv.org/abs/2108.07993" rel="noopener ugc nofollow" target="_blank">EPSILON</a>)</figcaption></figure><p id="412f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The assumptions of MPDM are twofold. First, much of the decision-making by human drivers involves discrete high-level semantic actions (e.g., slowing, accelerating, lane-changing, stopping). These actions are referred to as policies in this context. The second implicit assumption concerns other agents: other vehicles will make reasonably safe decisions. Once a vehicle’s policy is decided, its action (trajectory) is determined.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rk"><img src="../Images/0a6e9de32f012cfd6c27c556e6912cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJDY99whYYJ97qZlhhaogQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The framework of MPDM (chart created by author)</figcaption></figure><p id="2cfe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">MPDM first selects one policy for the ego vehicle from many options (hence the “multi-policy” in its name) and selects one policy for each nearby agent based on their respective predictions. It then performs forward simulation (similar to a fast rollout in MCTS). The best interaction scenario after evaluation is then passed on to motion planning, such as the Spatiotemporal Semantic Corridor (SCC) mentioned in the joint spatiotemporal planning session.</p><p id="7c72" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">MPDM enables intelligent and human-like behavior, such as actively cutting into dense traffic flow even when there is no sufficient gap present. This is not possible with a predict-then-plan pipeline, which does not explicitly consider interactions. The prediction module in MPDM is tightly integrated with the behavior planning model through forward simulation.</p><p id="2d42" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">MPDM assumes a single policy throughout the decision horizon (10 seconds). Essentially, MPDM adopts an MCTS approach with one layer deep and super wide, considering all possible agent predictions. This leaves room for improvement, inspiring many follow-up works such as EUDM, EPSILON, and MARC. For example, EUDM considers more flexible ego policies and assigns a policy tree with a depth of four, with each policy covering a time duration of 2 seconds over an 8-second decision horizon. To compensate for the extra computation induced by the increased tree depth, EUDM performs more efficient width pruning by guided branching, identifying critical scenarios and key vehicles. This approach explores a more balanced policy tree.</p><p id="0715" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The forward simulation in MPDM and EUDM uses very simplistic driver models (Intelligent driver model or IDM for longitudinal simulation, and Pure Pursuit or PP for lateral simulation). MPDM points out that high fidelity realism matters less than the closed-loop nature itself, as long as policy-level decisions are not affected by low-level action execution inaccuracies.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/7cfe5b2544f6f640ad9958c221f248b7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*bDd0arvG_Ndd11fwF9FkCg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The conceptual diagram of decision making, where prediction, BP and MP integrates tightly (chart created by author)</figcaption></figure><p id="377f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Contingency planning in the context of autonomous driving involves generating multiple potential trajectories to account for various possible future scenarios. A key motivating example is that experienced drivers anticipate multiple future scenarios and always plan for a safe backup plan. This anticipatory approach leads to a smoother driving experience, even when cars perform sudden cut-ins into the ego lane.</p><p id="0f49" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A critical aspect of contingency planning is deferring the decision bifurcation point. This means delaying the point at which different potential trajectories diverge, allowing the ego vehicle more time to gather information and respond to different outcomes. By doing so, the vehicle can make more informed decisions, resulting in smoother and more confident driving behaviors, similar to those of an experienced driver.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/6bed42bd618a07b194d8e27fc1327ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJvMXxtZRnOd3y65P5X46A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Risk-aware contingency planning (source: <a class="af nc" href="https://arxiv.org/abs/2308.12021" rel="noopener ugc nofollow" target="_blank">MARC</a>, RAL 2023)</figcaption></figure><blockquote class="qm qn qo"><p id="a2d4" class="nd ne qp nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">MARC also combines behavior planning and motion planning together. This extends the notion and utility of forward simulation. In other words, MPDM and EUDM still uses policy tree for high level behavior planning and rely on other motion planning pipelines such as <a class="af nc" href="https://arxiv.org/abs/1906.09788" rel="noopener ugc nofollow" target="_blank">semantic spatiotemporal corridors (SSC)</a>, due to the fact that ego motion in the policy tree is still characterized by heavily quantized behavior bucket. MARC extends this by keeping the quantized behavior for agents other than ego but uses more refined motion planning directly in the forward rollout. In a way it is a hybrid approach, where hybrid carries a similar meaning to that in hybrid A*, a mix of discrete and continuous.</p></blockquote><p id="6715" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One possible drawback of MPDM and all its follow-up works is their reliance on simple policies designed for highway-like structured environments, such as lane keeping and lane changing. This reliance may limit the capability of forward simulation to handle complex interactions. To address this, following the example of MPDM, the key to making POMDPs more effective is to simplify the action and state space through the growth of a high-level policy tree. It might be possible to create a more flexible policy tree, for example, by enumerating spatiotemporal relative position tags to all relative objects and then performing guided branching.</p><h1 id="92f2" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Industry practices of decision making</h1><p id="631b" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Decision-making remains a hot topic in current research. Even classical optimization methods have not been fully explored yet. Machine learning methods could shine and have a disruptive impact, especially with the advent of Large Language Models (LLMs), empowered by techniques like Chain of Thought (CoT) or Monte Carlo Tree Search (MCTS).</p><h2 id="af81" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Trees</h2><p id="060d" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Trees are systematic ways to perform decision-making. Tesla AI Day 2021 and 2022 showcased their decision-making capabilities, heavily influenced by AlphaGo and the subsequent MuZero, to address highly complex interactions.</p><p id="cb8e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At a high level, Tesla’s approach follows behavior planning (decision making) followed by motion planning. It searches for a convex corridor first and then feeds it into continuous optimization, using spatiotemporal joint planning. This approach effectively addresses scenarios such as narrow passing, a typical bottleneck for path-speed decoupled planning.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/ee5c3ba674a8e3a6622efd7ad84d89f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxLGT2w8Q8HA-uwOcMsuBQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Neural network heuristics guided MCTS (source: <a class="af nc" href="https://youtu.be/j0z4FweCy4M?t=4514" rel="noopener ugc nofollow" target="_blank">Tesla AI Day 2021</a>)</figcaption></figure><p id="5f10" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Tesla also adopts a hybrid system that combines data-driven and physics-based checks. Starting with defined goals, Tesla’s system generates seed trajectories and evaluates key scenarios. It then branches out to create more scenario variants, such as asserting or yielding to a traffic agent. Such an interaction search over the policy tree is showcased in the presentations of the years <a class="af nc" href="https://pic1.zhimg.com/80/v2-ede609df14524ce4a9c23689367f791c_1440w.webp" rel="noopener ugc nofollow" target="_blank">2021</a> and <a class="af nc" href="https://pic2.zhimg.com/v2-48022fc4ee278be6f730d3b2ee587b19_r.png" rel="noopener ugc nofollow" target="_blank">2022</a>.</p><p id="cc2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One highlight of Tesla’s use of machine learning is the acceleration of tree search via trajectory optimization. For each node, Tesla uses physics-based optimization and a neural planner, achieving a 10 ms vs. 100 µs time frame — resulting in a 10x to 100x improvement. The neural network is trained with expert demonstrations and offline optimizers.</p><p id="831d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Trajectory scoring is performed by combining classical physics-based checks (such as collision checks and comfort analysis) with neural network evaluators that predict intervention likelihood and rate human-likeness. This scoring helps prune the search space, focusing computation on the most promising outcomes.</p><p id="fe5f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While many argue that machine learning should be applied to high-level decision-making, Tesla uses ML fundamentally to accelerate optimization and, consequently, tree search.</p><p id="1f6b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Monte Carlo Tree Search (MCTS) method appears to be an ultimate tool for decision-making. Interestingly, those studying Large Language Models (LLMs) are trying to incorporate MCTS into LLMs, while those working on autonomous driving are attempting to replace MCTS with LLMs.</p><p id="dda2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As of roughly two years ago, Tesla’s technology followed this approach. However, since March 2024, Tesla’s Full Self-Driving (FSD) has switched to a more end-to-end approach, significantly different from their earlier methods.</p><h1 id="e251" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">No trees</h1><p id="c896" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">We can still consider interactions without implicitly growing trees. Ad-hoc logics can be implemented to perform one-order interaction between prediction and planning. Even one-order interaction can already generate good behavior, as demonstrated by TuSimple. MPDM, in its original form, is essentially one-order interaction, but executed in a more principled and extendable way.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/dd068780f3173683cf8bc15e030b0665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJJ-QIFVrbnVOManCKqYFA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Multi-order interaction between prediction and planning (source: <a class="af nc" href="https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7" rel="noopener ugc nofollow" target="_blank">TuSImple AI day</a>, in Chinese, translated by author)</figcaption></figure><p id="fcc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">TuSimple has also demonstrated the capability to perform contingency planning, similar to the approach proposed in <a class="af nc" href="https://arxiv.org/abs/2308.12021" rel="noopener ugc nofollow" target="_blank">MARC</a> (though MARC can also accommodate a customized risk preference).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rn"><img src="../Images/a06c89c942531828ed5dda56ef58dc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KGJHkreNmO5fqx0Wyf5urg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Contingency planning (source: <a class="af nc" href="https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7" rel="noopener ugc nofollow" target="_blank">TuSImple AI day</a>, in Chinese, translated by author)</figcaption></figure><h1 id="d536" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Self-Reflections</h1><p id="c470" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">After learning the basic building blocks of classical planning systems, including behavior planning, motion planning, and the principled way to handle interaction through decision-making, I have been reflecting on potential bottlenecks in the system and how machine learning (ML) and neural networks (NN) may help. I am documenting my thought process here for future reference and for others who may have similar questions. Note that the information in this section may contain personal biases and speculations.</p><h1 id="32e4" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Why NN in planning?</h1><p id="0fe9" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Let’s look at the problem from three different perspectives: in the existing modular pipeline, as an end-to-end (e2e) NN planner, or as an e2e autonomous driving system.</p><p id="f651" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Going back to the drawing board, let’s review the problem formulation of a planning system in autonomous driving. The goal is to obtain a trajectory that ensures safety, comfort, and efficiency in a highly uncertain and interactive environment, all while adhering to real-time engineering constraints onboard the vehicle. These factors are summarized as goals, environments, and constraints in the chart below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ro"><img src="../Images/fcaa289038ec10ef1f6767f19c7e5de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*la_9PSHoes1-6e02X0CDPQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The potentials of NN in planning (chart made by author)</figcaption></figure><p id="48f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Uncertainty in autonomous driving can refer to uncertainty in perception (observation) and predicting long-term agent behaviors into the future. Planning systems must also handle the uncertainty in future trajectory predictions of other agents. As discussed earlier, a principled decision-making system is an effective way to manage this.</p><p id="dcb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally, a typically overlooked aspect is that planning must tolerate uncertain, imperfect, and sometimes incomplete perception results, especially in the current age of vision-centric and HD map-less driving. Having a Standard Definition (SD) map onboard as a prior helps alleviate this uncertainty, but it still poses significant challenges to a heavily handcrafted planner system. This perception uncertainty was considered a solved problem by Level 4 (L4) autonomous driving companies through the heavy use of Lidar and HD maps. However, it has resurfaced as the industry moves toward mass production autonomous driving solutions without these two crutches. An NN planner is more robust and can handle largely imperfect and incomplete perception results, which is key to mass production vision-centric and HD-mapless Advanced Driver Assistance Systems (ADAS).</p><p id="9eb5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Interaction should be treated with a principled decision-making system such as Monte Carlo Tree Search (MCTS) or a simplified version of MPDM. The main challenge is dealing with the curse of dimensionality (combinatorial explosion) by growing a balanced policy tree with smart pruning through domain knowledge of autonomous driving. MPDM and its variants, in both academia and industry (e.g., Tesla), provide good examples of how to grow this tree in a balanced way.</p><p id="cd3d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">NNs can also enhance the real-time performance of planners by speeding up motion planning optimization. This can shift the compute load from CPU to GPU, achieving orders of magnitude speedup. A tenfold increase in optimization speed can fundamentally impact high-level algorithm design, such as MCTS.</p><p id="c3fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Trajectories also need to be more human-like. Human likeness and takeover predictors can be trained with the vast amount of human driving data available. It is more scalable to increase the compute pool rather than maintain a growing army of engineering talents.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/20831d3ba51f5c5083e4292b758d01d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZgnydlexQzbuqOykFrpIQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The NN-based planning stack can leverage human-driving data more effectively (Chart created by author)</figcaption></figure><h1 id="c07b" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">What about e2e NN planners?</h1><p id="f990" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">An end-to-end (e2e) neural network (NN) planner still constitutes a modular autonomous driving (AD) design, accepting structured perception results (and potentially latent features) as its input. This approach combines prediction, decision, and planning into a single network. Companies such as <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//www.xchuxing.com/article/124221" rel="noopener">DeepRoute (2022) and Huawei (2024)</a> claim to utilize this method. Note that relevant raw sensor inputs, such as navigation and ego vehicle information, are omitted here.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/44644c42a483b21510a189f25742521b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*bkU7xzqKttz0HDaLK7owJg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A full autonomous driving stack with an e2e planner (chart made by author)</figcaption></figure><p id="893e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This e2e planner can be further developed into an end-to-end autonomous driving system that combines both perception and planning. This is what <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//wayve.ai/thinking/lingo%2D2%2Ddriving%2Dwith%2Dlanguage/" rel="noopener">Wayve’s LINGO-2 (2024)</a> and Tesla’s FSDv12 (2024) claim to achieve.</p><p id="097d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The benefits of this approach are twofold. First, it addresses perception issues. There are many aspects of driving that we cannot easily model explicitly with commonly used perception interfaces. For example, it is quite challenging to handcraft a driving system to <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1760841783708418094" rel="noopener">nudge around a puddle of water</a> or <a class="af nc" href="https://medium.com/p/ede324d78717/edit#:~:text=https%3A//x.com/AIDRIVR/status/1759843256513564997" rel="noopener">slow down for dips or potholes</a>. While passing intermediate perception features might help, it may not fundamentally resolve the issue.</p><p id="5aa3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Additionally, emergent behavior will likely help resolve corner cases more systematically. The intelligent handling of edge cases, such as the examples above, may result from the emergent behavior of large models.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="ab cn cb nz"><img src="../Images/7a33be1918eb98c773763a5fc0ee3b0c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*syl50VkFylmVUF4m9nonTw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A full autonomous driving stack with a one-model e2e driver (chart made by author)</figcaption></figure><p id="1825" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">My speculation is that, in its ultimate form, the end-to-end (e2e) driver would be a large vision and action-native multimodal model enhanced by Monte Carlo Tree Search (MCTS), assuming no computational constraints.</p><p id="0f59" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A world model in autonomous driving, as of 2024 consensus, is typically a multimodal model covering at least vision and action modes (or a VA model). While language can be beneficial for accelerating training, adding controllability, and providing explainability, it is not essential. In its fully developed form, a world model would be a VLA (vision-language-action) model.</p><p id="f363" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are at least two approaches to developing a world model:</p><ol class=""><li id="f228" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ra qc qd bk"><strong class="nf fr">Video-Native Model</strong>: Train a model to predict future video frames, conditioned on or outputting accompanying actions, as demonstrated by models like GAIA-1.</li><li id="e3a2" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny ra qc qd bk"><strong class="nf fr">Multimodality Adaptors</strong>: Start with a pretrained Large Language Model (LLM) and add multimodality adaptors, as seen in models like Lingo-2, RT2, or ApolloFM. These multimodal LLMs are not native to vision or action but require significantly less training resources.</li></ol><p id="3936" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A world model can produce a policy itself through the action output, allowing it to drive the vehicle directly. Alternatively, MCTS can query the world model and use its policy outputs to guide the search. This World Model-MCTS approach, while much more computationally intensive, could have a higher ceiling in handling corner cases due to its explicit reasoning logic.</p><h2 id="4731" class="pk of fq bf ol pl pm pn oo po pp pq or nm pr ps pt nq pu pv pw nu px py pz qa bk">Can we do without prediction?</h2><p id="6203" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Most current motion prediction modules represent the future trajectories of agents other than the ego vehicle as one or multiple discrete trajectories. It remains a question whether this prediction-planning interface is sufficient or necessary.</p><p id="ccf7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In a classical modular pipeline, prediction is still needed. However, a predict-then-plan pipeline definitely caps the upper limit of autonomous driving systems, as discussed in the decision-making session. A more critical question is how to integrate this prediction module more effectively into the overall autonomous driving stack. Prediction should aid decision-making, and a queryable prediction module within an overall decision-making framework, such as MPDM and its variants, is preferred. There are no severe issues with concrete trajectory predictions as long as they are integrated correctly, such as through policy tree rollouts.</p><p id="1e49" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another issue with prediction is that open-loop Key Performance Indicators (KPIs), such as Average Displacement Error (ADE) and Final Displacement Error (FDE), are not effective metrics as they fail to reflect the impact on planning. Instead, metrics like recall and precision at the intent level should be considered.</p><p id="b3e3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In an end-to-end system, an explicit prediction module may not be necessary, but implicit supervision — along with other domain knowledge from a classical stack — can definitely help or at least boost the data efficiency of the learning system. Evaluating the prediction behavior, whether explicit or implicit, will also be helpful in debugging such an e2e system.</p><h1 id="2ec7" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Can we do with just nets but no trees?</h1><p id="74e5" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Conclusions First. For an assistant, neural networks (nets) can achieve very high, even superhuman performance. For agents, I believe that using a tree structure is still beneficial (though not necessarily a must).</p><p id="cf5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First of all, trees can boost nets. Trees enhance the performance of a given network, whether it’s NN-based or not. In AlphaGo, even with a policy network trained via supervised learning and reinforcement learning, the overall performance was still inferior to the MCTS-based AlphaGo, which integrates the policy network as one component.</p><p id="934e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Second, nets can distill trees. In AlphaGo, MCTS used both a value network and the reward from a fast rollout policy network to evaluate a node (state or board position) in the tree. The AlphaGo paper also mentioned that while a value function alone could be used, combining the results of the two yielded the best results. The value network essentially distilled the knowledge from the policy rollout by directly learning the state-value pair. This is akin to how humans distill the logical thinking of the slow System 2 into the fast, intuitive responses of System 1. Daniel Kahneman, in his book “<a class="af nc" href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="noopener ugc nofollow" target="_blank">Thinking, Fast and Slow</a>,” describes how a chess master can quickly recognize patterns and make rapid decisions after years of practice, whereas a novice would require significant effort to achieve similar results. Similarly, the value network in AlphaGo was trained to provide a fast evaluation of a given board position.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/94f5b0839feca2e4bcd4310d65598774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjs5CtBirrpK3sdTNJ-Mxw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Grandmaster-Level Chess Without Search (source: <a class="af nc" href="https://arxiv.org/abs/2402.04494" rel="noopener ugc nofollow" target="_blank">DeepMind, 2024</a>)</figcaption></figure><p id="358c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Recent papers explore the upper limits of this fast system with neural networks. The “<a class="af nc" href="https://arxiv.org/abs/2402.04494" rel="noopener ugc nofollow" target="_blank">chess without search</a>” paper demonstrates that with sufficient data (prepared through tree search using a conventional algorithm), it is possible to achieve grandmaster-level proficiency. There is a clear “scaling law” related to data size and model size, indicating that as the amount of data and the complexity of the model increase, so does the proficiency of the system.</p><p id="9177" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So here we are with a power duo: trees boost nets, and nets distill trees. This positive feedback loop is essentially what AlphaZero uses to bootstrap itself to reach superhuman performance in multiple games.</p><p id="bff6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The same principles apply to the development of large language models (LLMs). For games, since we have clearly defined rewards as wins or losses, we can use forward rollout to determine the value of a certain action or state. For LLMs, the rewards are not as clear-cut as in the game of Go, so we rely on human preferences to rate the models via reinforcement learning with human feedback (RLHF). However, with models like ChatGPT already trained, we can use supervised fine-tuning (SFT), which is essentially imitation learning, to distill smaller yet still powerful models without RLHF.</p><p id="bb4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Returning to the original question, nets can achieve extremely high performance with large quantities of high-quality data. This could be good enough for an assistant, depending on the tolerance for errors, but it may not be sufficient for an autonomous agent. For systems targeting driving assistance (ADAS), nets via imitation learning may be adequate.</p><p id="6b41" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Trees can significantly boost the performance of nets with an explicit reasoning loop, making them perhaps more suitable for fully autonomous agents. The extent of the tree or reasoning loop depends on the return on investment of engineering resources. For example, even one order of interaction can provide substantial benefits, as demonstrated in <a class="af nc" href="https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7" rel="noopener ugc nofollow" target="_blank">TuSimple AI Day</a>.</p><h1 id="0939" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Can we use LLMs to make decisions?</h1><p id="7451" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">From the summary below of the hottest representatives of AI systems, we can see that LLMs are not designed to perform decision-making. In essence, LLMs are trained to complete documents, and even SFT-aligned LLM assistants treat dialogues as a special type of document (completing a dialogue record).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rr"><img src="../Images/e0e938cc3c27089bf9c0be33a0a5ef29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoZMfXoYM4xgBhidxM9Khw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Representative AI products as of 2024 (chart made by author)</figcaption></figure><p id="8fea" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I do not fully agree with recent claims that LLMs are slow systems (System 2). They are unnecessarily slow in inference due to hardware constraints, but in their vanilla form, LLMs are fast systems as they cannot perform counterfactual checks. Prompting techniques such as Chain of Thought (CoT) or Tree of Thoughts (ToT) are actually simplified forms of MCTS, making LLMs function more like slower systems.</p><p id="1339" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There is extensive research trying to integrate full-blown MCTS with LLMs. Specifically, <a class="af nc" href="https://arxiv.org/abs/2305.14078" rel="noopener ugc nofollow" target="_blank">LLM-MCTS</a> (NeurIPS 2023) treats the LLM as a commonsense “world model” and uses LLM-induced policy actions as a heuristic to guide the search. LLM-MCTS outperforms both MCTS alone and policies induced by LLMs by a wide margin for complex, novel tasks. The <a class="af nc" href="https://www.lesswrong.com/posts/JnM3EHegiBePeKkLc/possible-openai-s-q-breakthrough-and-deepmind-s-alphago-type" rel="noopener ugc nofollow" target="_blank">highly speculated Q-star from OpenAI</a> seems to follow the same approach of boosting LLMs with MCTS, as the name suggests.</p><h1 id="c0b4" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">The trend of evolution</h1><p id="5555" class="pw-post-body-paragraph nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny fj bk">Below is a rough evolution of the planning stack in autonomous driving. It is rough as the listed solutions are not necessarily more advanced than the ones above, and their debut may not follow the exact chronological order. Nonetheless, we can observe general trends. Note that the listed representative solutions from the industry are based on my interpretation of various press releases and could be subject to error.</p><p id="0546" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One trend is the movement towards a more end-to-end design with more modules consolidated into one. We see the stack evolve from path-speed decoupled planning to joint spatiotemporal planning, and from a predict-then-plan system to a joint prediction and planning system. Another trend is the increasing incorporation of machine learning-based components, especially in the last three stages. These two trends converge towards an end-to-end NN planner (without perception) or even an end-to-end NN driver (with perception).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rs"><img src="../Images/3e753016647b4fc5cd09a94aa0263393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8_SjaahLNP9HgFq3MJ1KA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A rough history of evolution of planning (Chart made by author)</figcaption></figure><h1 id="2c53" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Takeaways</h1><ul class=""><li id="099c" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny qb qc qd bk"><strong class="nf fr">ML as a Tool</strong>: Machine learning is a tool, not a standalone solution. It can assist with planning even in current modular designs.</li><li id="e30d" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><strong class="nf fr">Full Formulation</strong>: Start with a full problem formulation, then make reasonable assumptions to balance performance and resources. This helps create a clear direction for a future-proof system design and allows for improvements as resources increase. Recall the transition from POMDP’s formulation to engineering solutions like AlphaGo’s MCTS and MPDM.</li><li id="8280" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><strong class="nf fr">Adapting Algorithms</strong>: Theoretically beautiful algorithms (e.g., Dijkstra and Value Iteration) are great for understanding concepts but need adaptation for practical engineering (Value Iteration to MCTS as Dijkstra’s algorithm to Hybrid A-star).</li><li id="f064" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><strong class="nf fr">Deterministic vs. Stochastic</strong>: Planning excels in resolving deterministic (not necessarily static) scenes. Decision-making in stochastic scenes is the most challenging task toward full autonomy.</li><li id="d34f" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><strong class="nf fr">Contingency Planning</strong>: This can help merge multiple futures into a common action. It’s beneficial to be aggressive to the degree that you can always resort to a backup plan.</li><li id="b092" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><strong class="nf fr">End-to-end Models</strong>: Whether an end-to-end model can solve full autonomy remains unclear. It may still need classical methods like MCTS. Neural networks can handle assistants, while trees can manage agents.</li></ul><h1 id="5455" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Acknowledgments</h1><ul class=""><li id="2727" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny qb qc qd bk">This blog post is heavily inspired by <a class="af nc" href="https://wenchaoding.github.io/personal/index.html" rel="noopener ugc nofollow" target="_blank">Dr. Wenchao Ding</a>’s course on planning on <a class="af nc" href="https://www.shenlanxueyuan.com/course/671" rel="noopener ugc nofollow" target="_blank">Shenlan Xueyuan (深蓝学院)</a>.</li><li id="68a1" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk">Thanks for the heavy and inspiring discussion with <a class="af nc" href="https://winsty.net/" rel="noopener ugc nofollow" target="_blank">Naiyan Wang</a> and Jingwei Zhao. Thanks to <a class="af nc" href="https://www.zhihu.com/people/george-reagan" rel="noopener ugc nofollow" target="_blank">论文推土机</a>, <a class="af nc" href="https://www.zhihu.com/people/REX-X-96-9" rel="noopener ugc nofollow" target="_blank">Invictus</a>, <a class="af nc" href="https://zhuanlan.zhihu.com/p/704847315" rel="noopener ugc nofollow" target="_blank">XXF</a>, PYZ and JCL for giving critical feedback to the initial draft. Thanks for the insightful discussion with <a class="af nc" href="https://zhanwei.site/" rel="noopener ugc nofollow" target="_blank">Professor Wei Zhan</a> from Berkeley on trends in academia.</li></ul><h1 id="2898" class="ok of fq bf ol om on gq oo op oq gt or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Reference</h1><ul class=""><li id="dc76" class="nd ne fq nf b go pf nh ni gr pg nk nl nm ph no np nq pi ns nt nu pj nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2212.10156" rel="noopener ugc nofollow" target="_blank">UniAD: Planning-oriented Autonomous Driving</a>, CVPR 2023 best paper</li><li id="ee9b" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2401.08658" rel="noopener ugc nofollow" target="_blank">End-To-End Planning of Autonomous Driving in Industry and Academia: 2022–2023</a>, Arxiv 2024</li><li id="578b" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2310.10357" rel="noopener ugc nofollow" target="_blank">BEVGPT: Generative Pre-trained Large Model for Autonomous Driving Prediction, Decision-Making, and Planning</a>, AAAI 2024</li><li id="0722" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2406.05708" rel="noopener ugc nofollow" target="_blank">Towards A General-Purpose Motion Planning for Autonomous Vehicles Using Fluid Dynamics</a>, Arxiv 2024</li><li id="0f60" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.bilibili.com/video/BV1Em4y1u7P7/?vd_source=73e03d3e246aa3ec284f028c7dcf0fa7" rel="noopener ugc nofollow" target="_blank">Tusimple AI day</a>, in Chinese with English subtitle on Bilibili, 2023/07</li><li id="ad69" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://zhuanlan.zhihu.com/p/551381336" rel="noopener ugc nofollow" target="_blank">Tech blog on joint spatiotemporal planning by Qcraft</a>, in Chinese on Zhihu, 2022/08</li><li id="1f7c" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://zhuanlan.zhihu.com/p/53495492" rel="noopener ugc nofollow" target="_blank">A review of the entire autonomous driving stack</a>, in Chinese on Zhihu, 2018/12</li><li id="24cd" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://zhuanlan.zhihu.com/p/572950979" rel="noopener ugc nofollow" target="_blank">Tesla AI Day Planning</a>, in Chinese on Zhihu, 2022/10</li><li id="8ff9" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://mp.weixin.qq.com/s/8d1qXTm5v4H94HxAibp1dA" rel="noopener ugc nofollow" target="_blank">Technical blog on ApolloFM</a>, in Chinese by Tsinghua AIR, 2024</li><li id="ca42" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.semanticscholar.org/paper/Optimal-trajectory-generation-for-dynamic-street-in-Werling-Ziegler/6bda8fc13bda8cffb3bb426a73ce5c12cc0a1760" rel="noopener ugc nofollow" target="_blank">Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame</a>, ICRA 2010</li><li id="4429" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2101.06806" rel="noopener ugc nofollow" target="_blank">MP3: A Unified Model to Map, Perceive, Predict and Plan</a>, CVPR 2021</li><li id="e107" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="http://www.cs.toronto.edu/~wenjie/papers/cvpr19/nmp.pdf" rel="noopener ugc nofollow" target="_blank">NMP: End-to-end Interpretable Neural Motion Planner</a>, CVPR 2019 oral</li><li id="bcba" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">Lift, Splat, Shoot: Encoding Images From Arbitrary Camera Rigs by Implicitly Unprojecting to 3D</a>, ECCV 2020</li><li id="c779" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/1911.10298" rel="noopener ugc nofollow" target="_blank">CoverNet: Multimodal Behavior Prediction using Trajectory Sets</a>, CVPR 2020</li><li id="090b" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/1807.08048" rel="noopener ugc nofollow" target="_blank">Baidu Apollo EM Motion Planner</a>, Baidu, 2018</li><li id="e13b" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.nature.com/articles/nature16961" rel="noopener ugc nofollow" target="_blank">AlphaGo: Mastering the game of Go with deep neural networks and tree search</a>, Nature 2016</li><li id="9b44" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.science.org/doi/full/10.1126/science.aar6404" rel="noopener ugc nofollow" target="_blank">AlphaZero: A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</a>, Science 2017</li><li id="6782" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.nature.com/articles/s41586-020-03051-4" rel="noopener ugc nofollow" target="_blank">MuZero: Mastering Atari, Go, chess and shogi by planning with a learned model</a>, Nature 2020</li><li id="843a" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2305.10601" rel="noopener ugc nofollow" target="_blank">ToT: Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a>, NeurIPS 2023 Oral</li><li id="db50" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2201.11903" rel="noopener ugc nofollow" target="_blank">CoT: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a>, NeurIPS 2022</li><li id="1f13" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2305.14078" rel="noopener ugc nofollow" target="_blank">LLM-MCTS: Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</a>, NeurIPS 2023</li><li id="9d78" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://ieeexplore.ieee.org/document/7139412" rel="noopener ugc nofollow" target="_blank">MPDM: Multipolicy decision-making in dynamic, uncertain environments for autonomous driving</a>, ICRA 2015</li><li id="aac8" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://www.roboticsproceedings.org/rss11/p43.pdf" rel="noopener ugc nofollow" target="_blank">MPDM2: Multipolicy Decision-Making for Autonomous Driving via Changepoint-based Behavior Prediction</a>, RSS 2015</li><li id="78bd" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://link.springer.com/article/10.1007/s10514-017-9619-z" rel="noopener ugc nofollow" target="_blank">MPDM3: Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction: Theory and experiment</a>, RSS 2017</li><li id="9d00" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2003.02746" rel="noopener ugc nofollow" target="_blank">EUDM: Efficient Uncertainty-aware Decision-making for Automated Driving Using Guided Branching</a>, ICRA 2020</li><li id="0cda" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2308.12021" rel="noopener ugc nofollow" target="_blank">MARC: Multipolicy and Risk-aware Contingency Planning for Autonomous Driving</a>, RAL 2023</li><li id="b215" class="nd ne fq nf b go qe nh ni gr qf nk nl nm qg no np nq qh ns nt nu qi nw nx ny qb qc qd bk"><a class="af nc" href="https://arxiv.org/abs/2108.07993" rel="noopener ugc nofollow" target="_blank">EPSILON: An Efficient Planning System for Automated Vehicles in Highly Interactive Environments</a>, TRO 2021</li></ul></div></div></div></div>    
</body>
</html>