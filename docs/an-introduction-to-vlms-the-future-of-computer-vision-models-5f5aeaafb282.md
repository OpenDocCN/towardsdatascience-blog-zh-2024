# VLM简介：计算机视觉模型的未来

> 原文：[https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06](https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06)

## 使用VLM构建一个准确度提高28%的多模态图像搜索引擎。

[](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[![Ro Isachenko](../Images/c2f1e41a389378cec8801e6eb8d8060c.png)](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------) [Ro Isachenko](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------) ·12分钟阅读·2024年11月6日

--

直到最近，AI模型的范围较窄，只能理解语言或特定图像，但很少同时理解两者。

在这方面，像GPT这样的通用语言模型是一次巨大的飞跃，因为我们从专业化的模型转向了通用但更强大的模型。

但即使语言模型有所进步，它们仍然与计算机视觉领域分离，每个领域在孤立的状态下发展，未能弥合这一鸿沟。试想一下，如果你只能听而不能看，或者只能看而不能听，会发生什么。

我的名字是Roman Isachenko，我是Yandex计算机视觉团队的一员。

在本文中，我将讨论视觉语言模型（VLM），我相信它们是复合AI系统的未来。

我将解释开发图像搜索多模态神经网络的基础知识和训练过程，并探讨使这一切成为可能的设计原则、挑战和架构。

到最后，我还将展示我们如何使用AI驱动的搜索产品来处理图像和文本，以及引入VLM后发生了哪些变化。

让我们开始吧！

## 什么是VLM？

拥有数十亿甚至数百亿参数的LLM已经不再是新鲜事物。

我们到处都能看到它们！

LLM研究的下一个重点是更多地倾向于开发多模态模型（全能模型）——能够理解和处理多种数据类型的模型。

![](../Images/e484761b3bcc1d96fbd1fbc82463d8a1.png)

多模态模型（图像由作者提供）

正如其名称所示，这些模型不仅能处理文本，还能分析图像、视频和音频。

那我们为什么要这么做呢？

> *万能工匠，什么都能做，什么都不精，往往比某一领域的专家更有优势。*

近年来，我们看到一个趋势，即通用方法正在主导特定的方法。

想一想。

今天的基于语言的机器学习模型已经相对先进且具有通用性。一个模型可以进行翻译、摘要、识别语音标签，等等。

![](../Images/7b35063c0d265ee30383b449d2567afa.png)

通用NLP模型（图片由作者提供）

但早些时候，这些模型往往是特定任务的（现在我们也有这种模型，但比以前少了）。

+   一个专门用于翻译的模型。

+   一个专门用于摘要的模型，等等。

换句话说，今天的自然语言处理模型（特别是大型语言模型）可以承担多种任务，这些任务以前需要开发高度特定的解决方案。

第二，这种方法让我们能够以指数级的速度扩展可用于模型训练的数据，这对于有限的文本数据量至关重要。然而，早期的做法是需要任务特定的数据：

+   一个专门的翻译标注数据集。

+   一个专门的摘要数据集，等等。

第三，我们相信训练一个多模态模型能够提升每种数据类型的表现，就像它对人类的表现一样。

对于这篇文章，我们将“黑盒”概念简化为一种情景：模型接收一张图像和一些文本（我们称之为“指令”）作为输入，并仅输出文本（响应）。

结果是，我们得到了一个更简化的过程，如下所示：

![](../Images/ab10af2ac1a9743ce6deba3b47a7279a.png)

一个简化的多模态模型（图片由作者提供）

我们将讨论图像辨识模型，它们能够分析和解释图像所描绘的内容。

在深入技术细节之前，请考虑这些模型能够解决的问题。

以下是一些示例：

![](../Images/6fa1a099ca302aa03dd704fbda095c2f.png)

任务示例（图片由作者提供）

+   左上角图片：我们让模型描述这张图像。这个要求是用文本指定的。

+   中上部图片：我们让模型解释这张图像。

+   右上角图片：我们让模型解释这张图像，并告诉我们如果我们遵循这个标志，会发生什么。

+   底部图片：这是最复杂的例子。我们给模型一些数学问题。从这些例子中，你可以看到任务范围是如此广泛和多样化。

VLMs是计算机视觉领域的一个新前沿，它们能够在零样本和单样本模式下解决各种[基本的计算机视觉任务](https://arxiv.org/abs/2405.18415)（分类、检测、描述）。

虽然VLMs可能尚未在每个标准任务中表现得特别出色，但它们正在迅速进步。

现在，让我们了解它们是如何工作的。

## VLM架构

这些模型通常有三个主要组件：

![](../Images/b96ccc97b7f95acfec256feef7397cbc.png)

VLM的简化表示（图片由作者提供）

1.  LLM — 一种文本模型（在我们的例子中是YandexGPT），它不理解图像。

1.  图像编码器 — 一种图像模型（CNN或视觉变换器），它不理解文本。

1.  适配器 — 一个充当中介的模型，确保LLM和图像编码器能够良好配合。

该流程相当直接：

+   将图像输入图像编码器。

+   将图像编码器的输出转换为某种表示，供适配器使用。

+   将适配器的输出整合到 LLM 中（下面将详细介绍）。

+   当图像被处理时，将文本指令转换为一系列标记并输入 LLM。

## 关于适配器的更多信息

适配器是模型中最令人兴奋和最重要的部分，因为它准确地促进了 LLM 和图像编码器之间的通信/交互。

有两种类型的适配器：

+   基于提示的适配器

+   基于交叉注意力的适配器

基于提示的适配器最早在 [BLIP-2](https://arxiv.org/abs/2301.12597) 和 [LLaVa](https://arxiv.org/abs/2304.08485) 模型中提出。

这个想法很简单且直观，从名字本身就可以看出。

我们将图像编码器的输出（一个向量、一系列向量或一个张量——取决于架构）转换为一系列向量（标记），然后将其输入到 LLM 中。你可以采用一个简单的 MLP 模型，带有几层，并将其作为适配器，结果可能会非常好。

基于交叉注意力的适配器在这方面要复杂一些。

它们已被应用于最近关于 [Llama 3.2](https://arxiv.org/abs/2407.21783) 和 [NVLM](https://arxiv.org/abs/2409.11402) 的论文中。

这些适配器旨在将图像编码器的输出转换为可在 LLM 的交叉注意力块中用作键/值矩阵的形式。此类适配器的示例包括像 [感知器重采样器](https://arxiv.org/abs/2204.14198) 或 [Q‑former](https://arxiv.org/abs/2301.12597) 这样的变换器架构。

![](../Images/0f4c28f4f6442ec22375f6141c9c62c1.png)

基于提示的适配器（左）和基于交叉注意力的适配器（右）（图像来源：作者）

基于提示的适配器（左）和基于交叉注意力的适配器（右）

两种方法各有优缺点。

目前，基于提示的适配器 [提供更好的结果](https://arxiv.org/abs/2409.11402)，但它们会削弱 LLM 的大量输入上下文，而这很重要，因为 LLM 的上下文长度有限（目前如此）。

基于交叉注意力的适配器不会削弱 LLM 的上下文，但需要大量的参数才能达到良好的质量。

## VLM 训练

架构确定后，让我们深入探讨训练过程。

首先，请注意，VLM 不是从头开始训练的（尽管我们认为这只是时间问题），而是基于预训练的 LLM 和图像编码器构建的。

使用这些预训练模型，我们在多模态文本和图像数据上对 VLM 进行微调。

这个过程包括两个步骤：

+   预训练

+   对齐：SFT + RL（可选）

![](../Images/7ea2b804900e8b6219c18579ac3fc28a.png)

VLM 训练过程（图像来源：作者）

注意这些阶段如何类似于 LLM 的训练？

这是因为这两个过程在概念上是相似的。让我们简要回顾一下这些阶段。

## VLM 预训练

这是我们在这个阶段希望实现的目标：

+   将文本和图像模态连接起来（记住，我们的模型包含一个我们之前没有训练过的适配器）。

+   将世界知识加载到我们的模型中（这些图像包含许多细节，例如 OCR 技能）。

预训练 VLM 时使用了三种类型的数据：

+   **交替预训练**：这与 LLM 的预训练阶段相似，在这个阶段，我们通过输入网络文档来训练模型执行下一个 token 预测任务。对于 VLM 的预训练，我们选择带有图像的网络文档，训练模型预测文本。这里的关键区别在于，VLM 会同时考虑页面上的文本和图像。这样的数据很容易获得，因此这种类型的预训练不难扩展。然而，数据质量并不高，提升其质量证明是一项艰巨的任务。

![](../Images/a9640b861d6f47a20c1846f5e1cabe28.png)

交替预训练数据集（图片由作者提供）

**图像-文本配对预训练**：我们训练模型执行一个特定任务：为图像生成标题。你需要一个包含相关描述的大量图像数据集来完成这项任务。这种方法更为流行，因为许多此类数据集被用于训练其他模型（如文本生成图像、图像到文本检索）。

![](../Images/eda08149f9e808bb81b4be9dac331113.png)

图像-文本配对预训练数据集（图片由作者提供）

**基于指令的预训练**：在推理过程中，我们将图像和文本输入模型。为什么不从一开始就用这种方式训练模型呢？这正是基于指令的预训练所做的：它在一个庞大的图像-指令-答案三元组数据集上训练模型，即使数据并不总是完美的。

![](../Images/c5885ad9b34e987c993aabbdfe139b97.png)

基于指令的预训练数据集（图片由作者提供）

要正确训练一个 VLM 模型需要多少数据是一个复杂的问题。在这一阶段，所需的数据集大小可以从几百万到几十亿（幸运的是，不是上万亿！）个样本不等。

我们团队使用了基于指令的预训练，样本量为几百万。然而，我们相信交替预训练具有巨大的潜力，我们正在积极朝着这个方向努力。

## VLM 对齐

一旦预训练完成，就可以开始对齐阶段了。

它包括 SFT 训练和可选的 RL 阶段。由于我们只有 SFT 阶段，我将重点介绍这个。

尽管如此，最近的论文（例如 [这篇](https://arxiv.org/abs/2405.17220) 和 [这篇](https://arxiv.org/abs/2407.21783)）通常在 VLM 上加入 RL 阶段，使用与 LLM 相同的方法（DPO 和方法名称首字母的各种修改）。

无论如何，回到 SFT。

严格来说，这一阶段与基于指令的预训练相似。

区别在于我们注重高质量的数据，具备适当的响应结构、格式化和强大的推理能力。

这意味着模型必须能够理解图像并对其进行推断。理想情况下，它应对没有图像的文本指令做出同样好的响应，因此我们还将添加高质量的纯文本数据。

最终，这个阶段的数据通常在数十万到几百万个样本之间。在我们的案例中，这个数字大约在六位数。

## 质量评估

让我们讨论一下评估VLM质量的方法。我们使用两种方法：

+   计算开源基准上的指标。

+   通过并排（SBS）评估来比较模型，在这种评估中，评估者比较两个模型的响应并选择更好的一个。

第一种方法允许我们在特定的数据子集上测量代理指标（例如分类任务中的准确性）。

然而，由于大多数基准测试都是英语的，因此它们不能用于比较其他语言（如德语、法语、俄语等）训练的模型。

虽然可以使用翻译，但翻译模型引入的错误使得结果不可靠。

第二种方法可以更深入地分析模型，但需要细致（且昂贵）的手动数据标注。

我们的模型是双语的，能够用英语和俄语进行响应。因此，我们可以使用英语开源基准并进行并排比较。

我们信任这种方法并在其中投入了大量资金。以下是我们要求评估者评估的内容：

+   语法

+   可读性

+   完整性

+   与指令的相关性

+   错误（逻辑错误和事实错误）

+   幻觉

我们力求评估模型技能的一个完整且多样的子集。

以下饼图展示了我们SbS评估任务的分配情况。

![](../Images/608427413661c3c7cc8a8694a629a64b.png)

质量评估任务分配（图片来源：作者）

这总结了VLM基础知识的概述，以及如何训练一个模型并评估其质量。

## 流水线架构

今年春天，我们为Neuro（一个由AI驱动的搜索产品）增加了多模态功能，允许用户通过文本和图像提问。

直到最近，它的底层技术还不是真正的多模态。

这里展示的是这个流水线之前的样子。

![](../Images/5d139ed87d58cc9543ac873ac974c51e.png)

流水线架构（图片来源：作者）

这个图看起来很复杂，但一旦将其分解成步骤后，实际上是直观的。

以下是这个过程之前的样子

1.  用户提交一张图像和一个文本查询。

1.  我们将图像发送到我们的视觉搜索引擎，后者会返回关于该图像的大量信息（标签、识别的文本、信息卡片）。

1.  我们使用改写器（一个微调的LLM）结合这些信息和原始查询来构造文本查询。

1.  使用改写后的文本查询，我们使用Yandex搜索来检索相关文档（或摘录，我们称之为信息上下文）。

1.  最后，利用所有这些信息（原始查询、视觉搜索信息、重述的文本查询和信息上下文），我们通过生成器模型（另一个经过微调的LLM）生成最终响应。

完成！

如你所见，我们过去依赖两个单模态LLM和我们的视觉搜索引擎。这个方案在少量查询样本中表现良好，但也有其局限性。

下面是一个例子（虽然有些夸张），说明事情可能如何出错。

![](../Images/2d65efa3148887ebeeb8f1a47fac7994.png)

两个单模态LLM的问题（图像来源：作者）

在这里，重述器接收视觉搜索服务的输出，但根本无法理解用户的原始意图。

反过来，LLM模型对图像一无所知，生成了一个错误的搜索查询，同时获取了关于哈巴狗和苹果的标签。

为了提高我们的多模态响应质量并允许用户提出更复杂的问题，我们将VLM引入了我们的架构中。

更具体地说，我们进行了两项主要修改：

1.  我们用VLM重述器替代了LLM重述器。从本质上讲，我们开始将原始图像与视觉搜索引擎的文本一起输入到重述器中。

1.  我们在流程中添加了一个独立的VLM字幕生成器。这个模型提供了图像描述，我们将其作为最终生成器的信息上下文。

你可能会问

> 为什么不让生成器本身基于VLM呢？

这是个好主意！

但有一个问题。

我们的生成器训练继承自Neuro的文本模型，该模型经常更新。

为了更快、更方便地更新流程，我们引入了一个独立的VLM模块，这样做要容易得多。

此外，这个设置同样有效，下面展示了结果：

![](../Images/a122d37d30387dcc9720cbaae7ffc16d.png)

在AI驱动的搜索中使用VLM（图像来源：作者）

训练VLM重述器和VLM字幕生成器是两个独立的任务。

为此，我们使用前面提到的VLM，并对其进行了针对这些特定任务的微调。

微调这些模型需要收集成千上万的独立训练数据集。

我们还必须对基础设施进行重大改进，以提高流程的计算效率。

## 评估质量

现在，进入重要的问题：

> 引入VLM到这个相对复杂的流程中，真的有改善吗？

简而言之，是的，确实如此！

我们进行了并行测试，以衡量新流程的性能，并将我们之前的LLM框架与新的VLM框架进行了比较。

这个评估与之前讨论的核心技术评估类似。然而，在这种情况下，我们使用了一组不同的图像和查询，这些查询更贴近用户可能提出的需求。

下面是这个桶中聚类的大致分布。

![](../Images/29abfb15a93da3ac9445af9b1a49ada7.png)

聚类分布（图像来源：作者）

我们的离线并行评估显示，我们大大提高了最终响应的质量。

VLM管道显著提高了响应质量，涵盖了更多用户场景。

![](../Images/9a70b5dbf5bb31451cad96af55767ae4.png)

VLM与LLM在神经网络中的准确性对比（图源：作者）

我们还希望在真实观众中测试这些结果，看看我们的用户是否会注意到我们认为能够改善产品体验的技术变化。

因此，我们进行了一个在线拆分测试，将我们的LLM管道与新的VLM管道进行了对比。初步结果显示了以下变化：

+   包含图片的指令数量增加了17%。

+   会话数量（用户连续输入多个查询）增长了4.5%。

重申一下之前的观点，我们坚信VLMs是计算机视觉模型的未来。

VLMs已经能够解决许多开箱即用的问题。经过一点微调，它们完全可以提供最先进的质量。

感谢阅读！
