- en: Take a Look Under the Hood
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 看一看引擎盖下的情况
- en: 原文：[https://towardsdatascience.com/take-a-look-under-the-hood-24e40281c900?source=collection_archive---------7-----------------------#2024-06-13](https://towardsdatascience.com/take-a-look-under-the-hood-24e40281c900?source=collection_archive---------7-----------------------#2024-06-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/take-a-look-under-the-hood-24e40281c900?source=collection_archive---------7-----------------------#2024-06-13](https://towardsdatascience.com/take-a-look-under-the-hood-24e40281c900?source=collection_archive---------7-----------------------#2024-06-13)
- en: Using Monosemanticity to understand the concepts a Large Language Model learned
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单义性理解大型语言模型所学到的概念
- en: '[](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)[![Dorian
    Drost](../Images/1795395ad0586eafd83d3e2f7b975ca8.png)](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)
    [Dorian Drost](https://medium.com/@doriandrost?source=post_page---byline--24e40281c900--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)
    ·10 min read·Jun 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--24e40281c900--------------------------------)
    ·阅读时间10分钟·2024年6月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2276f286b884bf6fec521e0d14acb17f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2276f286b884bf6fec521e0d14acb17f.png)'
- en: Just like with the brain, it is quite hard to understand, what is really happening
    inside an LLM. Photo by [Robina Weermeijer](https://unsplash.com/@averey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 就像大脑一样，理解LLM内部到底发生了什么是相当困难的。照片由[Robina Weermeijer](https://unsplash.com/@averey?utm_source=medium&utm_medium=referral)提供，来源：[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: With the increasing use of Large Language Models (LLMs), the need for understanding
    their reasoning and behavior increases as well. In this article, I want to present
    to you an approach that sheds some light on the concepts an LLM represents internally.
    In this approach, a representation is extracted that allows one to understand
    a model's activation in terms of discrete concepts being used for a given input.
    This is called [Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html),
    indicating that these concepts have just a single (mono) meaning (semantic).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）使用的增加，我们对它们的推理和行为的理解需求也在增加。在本文中，我想向你介绍一种方法，它揭示了LLM内部表示的概念。在这种方法中，提取了一种表示方法，使人们能够理解模型的激活，并通过离散的概念来解释给定输入。这被称为[单义性](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)，意味着这些概念只有一个（mono）含义（semantic）。
- en: In this article, I will first describe the main idea behind Monosemanticity.
    For that, I will explain sparse autoencoders, which are a core mechanism within
    the approach, and show how they are used to structure an LLM’s activation in an
    interpretable way. Then I will retrace some demonstrations the authors of the
    Monosemanticity approach proposed to explain the insights of their approach, which
    closely follows [their original publication](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将首先描述单义性（Monosemanticity）背后的主要思想。为此，我将解释稀疏自编码器，这是该方法中的核心机制，并展示它们是如何以可解释的方式构建大型语言模型（LLM）的激活的。接着，我将回顾一些单义性方法的作者提出的示范，以解释他们方法的洞见，这些洞见紧密跟随了[他们的原始出版物](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)。
- en: Sparse autoencoders
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: '![](../Images/57186dd272b00fbd5f6be5bcb887a1ec.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57186dd272b00fbd5f6be5bcb887a1ec.png)'
- en: Just like an hourglass, an autoencoder has a bottleneck the data must pass through.
    Photo by [Alexandar Todov](https://unsplash.com/@alexandar_todov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像沙漏一样，自编码器有一个数据必须通过的瓶颈。照片由[Alexandar Todov](https://unsplash.com/@alexandar_todov?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: We have to start by taking a look at sparse autoencoders. First of all, an autoencoder
    is a neural net that is trained to reproduce a given input, i.e. it is supposed
    to produce exactly the vector it was given. Now you wonder, what’s the point?
    The important detail is, that the autoencoder has intermediate layers that are
    smaller than the input and output. Passing information through these layers necessarily
    leads to a loss of information and hence the model is not able to just learn the
    element by heart and reproduce it fully. It has to pass the information through
    a bottleneck and hence needs to come up with a dense representation of the input
    that still allows it to reproduce it as well as possible. The first half of the
    model we call the *encoder* (from input to bottleneck) and the second half we
    call the *decoder* (from bottleneck to output). After having trained the model,
    you may throw away the decoder. The encoder now transforms a given input into
    a representation that keeps important information but has a different structure
    than the input and potentially removes unneeded parts of the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先了解稀疏自编码器。首先，自编码器是一个神经网络，它的训练目标是重现给定的输入，也就是说，它应该产生与输入完全相同的向量。现在你可能会问，这样做的意义何在？关键的细节在于，自编码器有一些中间层，它们的大小比输入和输出都要小。通过这些层传递信息必然会导致信息丢失，因此模型不能仅仅通过记住元素来完全重现它。它必须通过一个瓶颈传递信息，因此需要提出一种密集的输入表示，它仍然能够尽可能地重现输入。我们将模型的前半部分称为*编码器*（从输入到瓶颈），后半部分称为*解码器*（从瓶颈到输出）。在训练完模型之后，你可以丢弃解码器。编码器现在将给定的输入转换为一种表示，保留了重要信息，但具有与输入不同的结构，并且可能去除了不需要的数据部分。
- en: To make an autoencoder *sparse*, its objective is extended. Besides reconstructing
    the input as well as possible, the model is also encouraged to activate as few
    neurons as possible. Instead of using all the neurons a little, it should focus
    on using just a few of them but with a high activation. This also allows to have
    more neurons in total, making the bottleneck disappear in the model’s architecture.
    However, the fact that activating too many neurons is punished still keeps the
    idea of compressing the data as much as possible. The neurons that are activated
    are then expected to represent important concepts that describe the data in a
    meaningful way. We call them *features* from now on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使自编码器变得*稀疏*，它的目标得到了扩展。除了尽可能地重建输入外，模型还被鼓励尽可能少地激活神经元。与其让所有神经元都稍微激活一下，不如集中精力激活其中的少数神经元，并且激活值要高。这也使得在模型中可以有更多的神经元，从而让瓶颈在模型架构中消失。然而，激活过多神经元会受到惩罚，这仍然保持了尽可能压缩数据的思想。被激活的神经元将被期望表示描述数据的有意义的关键概念。我们从现在开始称它们为*特征*。
- en: 'In the original Monosemanticity publication, such a sparse autoencoder is trained
    on an intermediate layer in the middle of the [Claude 3 Sonnet](https://www.anthropic.com/news/claude-3-family)
    model (an LLM published by [Anthropic](https://www.anthropic.com/) that can be
    said to play in the same league as the GPT models from OpenAI). That is, you can
    take some tokens (i.e. text snippets), forward them to the first half of the Claude
    3 Sonnett model, and forward that activation to the sparse autoencoder. You will
    then get an activation of the features that represent the input. However, we don’t
    really know what these features mean so far. To find out, let’s imagine we feed
    the following texts to the model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的《单义性》出版物中，这种稀疏自编码器是在[Claude 3 Sonnet](https://www.anthropic.com/news/claude-3-family)模型的中间层上训练的（Claude
    3是由[Anthropic](https://www.anthropic.com/)发布的一个大型语言模型，可以说与OpenAI的GPT模型处于同一水平）。也就是说，你可以将一些标记（即文本片段）传递给Claude
    3 Sonnet模型的前半部分，并将该激活传递给稀疏自编码器。然后，你将得到一个表示输入的特征激活。然而，到目前为止，我们并不真正知道这些特征是什么意思。为了找出答案，我们假设将以下文本输入模型：
- en: The cat is chasing the dog.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫在追逐狗。
- en: My cat is lying on the couch all day long.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我的猫整天躺在沙发上。
- en: I don’t have a cat.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我没有猫。
- en: If there is one feature that activates for all three of the sentences, you may
    guess that this feature represents the idea of a *cat*. There may be other features
    though, that just activate for single sentences but not for the others. For sentence
    one, you would expect the feature for *dog* to be activated, and to represent
    the meaning of sentence three, you would expect a feature that represents some
    form of *negation* or “*not having something*”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个特征在所有三句话中都被激活，你可以猜测这个特征代表的是*猫*的概念。然而，也可能存在一些只在某一句话中激活而不在其他句子中激活的特征。对于第一句，你可以预期*狗*的特征会被激活；而为了表达第三句的意思，你可以预期会有一个代表某种*否定*或“*没有某物*”的特征。
- en: Different features
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的特征
- en: '![](../Images/89d3a1d1783a558a8342f1cd531ecf61.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89d3a1d1783a558a8342f1cd531ecf61.png)'
- en: Features can describe quite different things, from apples and bananas to the
    notion of being edible and tasting sweet. Photo by [Jonas Kakaroto](https://unsplash.com/@jkakaroto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以描述非常不同的事物，从苹果和香蕉到可以食用和味道甜美的概念。照片由[Jonas Kakaroto](https://unsplash.com/@jkakaroto?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: From the aforementioned example, we saw that features can describe quite different
    things. There may be features that represent concrete objects or entities (such
    as *cats*, the *Eiffel Tower,* or *Benedict Cumberbatch*), but there may also
    be features dedicated to more abstract concepts like *sadness*, *gender*, *revolution*,
    *lying*, *things that can melt* or the *german letter ß* (yes, we indeed have
    an additional letter just for ourselves). As the model also saw programming code
    during its training, it also includes many features that are related to programming
    languages, representing contexts such as *code errors* or *computational functions*.
    You can explore the features of the Claude 3 model [here](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_781220).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面提到的例子中，我们看到特征可以描述非常不同的事物。有些特征代表具体的物体或实体（例如*猫*、*埃菲尔铁塔*或*班尼迪克特·康伯巴奇*），但也有一些特征专门表示更抽象的概念，如*悲伤*、*性别*、*革命*、*撒谎*、*能融化的东西*或*德语字母ß*（是的，确实有一个只属于我们的额外字母）。由于模型在训练过程中也接触过编程代码，因此它还包含了许多与编程语言相关的特征，表示诸如*代码错误*或*计算函数*之类的上下文。你可以在[这里](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_781220)探索Claude
    3模型的特征。
- en: If the model is capable of speaking multiple languages, the features are found
    to be multilingual. That means, a feature that corresponds to, say, the concept
    of *sorrow*, would be activated in relevant sentences in each language. In a likewise
    fashion, the features are also multimodal, if the model is able to work with different
    input modalities. The Benedict Cumberbatch feature would then activate for the
    name, but also for pictures or verbal mentions of Benedict Cumberbatch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型能够使用多种语言进行表达，那么这些特征就是多语言的。这意味着，与*悲伤*相关的特征会在每种语言的相关句子中被激活。同样，如果模型能够处理不同的输入方式，那么这些特征也是多模态的。以班尼迪克特·康伯巴奇为例，该特征会在提到这个名字时被激活，也会在涉及到图片或口头提及班尼迪克特·康伯巴奇时被激活。
- en: Influence on behavior
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对行为的影响
- en: '![](../Images/7959beedc7cfd09f544567d96dcd39c4.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7959beedc7cfd09f544567d96dcd39c4.png)'
- en: Features can influence behavior, just like a steering wheel influences the way
    you take. Photo by [Niklas Garnholz](https://unsplash.com/@niklasgarnholz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以影响行为，就像方向盘影响你行驶的方式一样。照片由[Niklas Garnholz](https://unsplash.com/@niklasgarnholz?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: So far we have seen that certain features are activated when the model produces
    a certain output. From a model’s perspective, the direction of causality is the
    other way round though. If the feature for the *Golden Gate Bridge* is activated,
    this causes the model to produce an answer that is related to this feature’s concept.
    In the following, this is demonstrated by artificially increasing the activation
    of a feature within the model’s inference.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，当模型生成特定输出时，某些特征会被激活。然而，从模型的角度来看，因果关系的方向是相反的。如果*金门大桥*的特征被激活，它会促使模型生成与该特征概念相关的答案。接下来，我们将通过在模型推理过程中人为增加某个特征的激活来演示这一点。
- en: '![](../Images/9ad46a1d3f77a8cb239afab01a76049b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ad46a1d3f77a8cb239afab01a76049b.png)'
- en: Answers of the model being influenced by a high activation of a certain feature.
    Image taken from the [original publication](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的答案受某个特征高度激活的影响。图片来源于[原始出版物](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)。
- en: On the left, we see the answers to two questions in the normal setup, and on
    the right we see, how these answers change if the activation of the features *Golden
    Gate Bridge* (first row) and *brain sciences* (second row) are increased. It is
    quite intuitive, that activating these features makes the model produce texts
    that include the concepts of the *Golden Gate Bridge* and *brain sciences*. In
    the usual case, the features are activated from the model’s input and its prompt,
    but with the approach we saw here, one can also activate some features in a more
    deliberate and explicit way. You could think of always activating the *politeness*
    feature to steer the model’s answers in the desired way. Without the notion of
    features, you would do that by adding instructions to the prompt such as “*always
    be polite in your answers”,* but with the feature concept, this could be done
    more explicitly. On the other hand, you can also think of deactivating features
    explicitly to avoid the model telling you how to build an atomic bomb or conduct
    tax fraud.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧，我们看到正常设置下两个问题的答案，而右侧则显示了如果*金门大桥*（第一行）和*大脑科学*（第二行）特征的激活程度增加时，这些答案如何变化。很直观，激活这些特征会让模型生成包含*金门大桥*和*大脑科学*概念的文本。在通常情况下，这些特征由模型的输入和提示激活，但在我们看到的方法中，也可以通过更有意图和明确的方式激活某些特征。你可以想象，始终激活*礼貌*特征来引导模型的回答朝着期望的方向发展。如果没有特征的概念，你可能需要通过在提示中添加“*在回答中始终保持礼貌*”这样的指示来实现，但通过特征概念，这可以更明确地完成。另一方面，你也可以考虑显式地去激活某些特征，以避免模型告诉你如何制造原子弹或进行逃税。
- en: 'Taking a deeper look: Specificity, Sensitivity and Completeness'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨：特异性、敏感性和完整性
- en: '![](../Images/4b6d406b56f3b48205cf43932f5ff416.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b6d406b56f3b48205cf43932f5ff416.png)'
- en: Let’s observe the features in more detail. Photo by [K8](https://unsplash.com/@_k8_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地观察这些特征。图片来自[K8](https://unsplash.com/@_k8_?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上。
- en: Now that we have understood how the features are extracted, we can follow some
    of the author’s experiments that show us which features and concepts the model
    actually learned.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了特征是如何提取的，我们可以参考一些作者的实验，展示模型实际学到的是哪些特征和概念。
- en: First, we want to know how *specific* the features are, i.e. how well they stick
    to their exact concept. We may ask, does the feature that represents Benedict
    Cumberbatch indeed activate **only** for Benedict Cumberbatch and not for other
    actors? To shed some light on this question, the authors used an LLM to rate texts
    regarding their relevance to a given concept. In the following example, it was
    assessed how much a text relates to the concept of brain science on a scale from
    0 (completely irrelevant) to 3 (very relevant). In the next figure, we see these
    ratings as the colors (blue for 0, red for 3) and we see the activation level
    on the x-axis. The more we go to the right, the more the feature is activated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想知道特征的*特异性*，即它们与具体概念的契合度。我们可以问，代表本尼迪克特·康伯巴奇的特征是否**仅仅**对本尼迪克特·康伯巴奇起作用，而不会对其他演员产生作用？为了回答这个问题，作者们使用了一个LLM对文本进行评分，评估它们与给定概念的相关性。在以下示例中，评估了文本与“大脑科学”概念的相关性，评分范围从0（完全无关）到3（非常相关）。在下图中，我们看到这些评分以颜色表示（蓝色表示0，红色表示3），而在横轴上是激活水平。越往右走，特征的激活程度越高。
- en: '![](../Images/d38cb47ea6b34fb67388e5597e1985fe.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d38cb47ea6b34fb67388e5597e1985fe.png)'
- en: The activation of the feature for brain science together with relevance scores
    of the inputs. Image taken from the [original publication](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑科学特征的激活与输入的相关性得分。图片来源于[原始出版物](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)。
- en: We see a clear correlation between the activation (x-axis) and the relevance
    (color). The higher the activation, the more often the text is considered highly
    relevant to the topic of brain sciences. The other way round, for texts that are
    of little or no relevance to the topic of brain sciences, the feature only activates
    marginally (if at all). That means, that the feature is quite specific for the
    topic of brain science and does not activate that much for related topics such
    as psychology or medicine.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到激活（x轴）与相关性（颜色）之间有明确的相关性。激活越高，文本越常被认为与脑科学的主题高度相关。反过来，对于与脑科学主题关联较小或完全无关的文本，该特征仅在边缘上激活（如果有的话）。这意味着，该特征在脑科学主题上非常特定，并且对于相关主题（如心理学或医学）并不那么激活。
- en: Sensitivity
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵敏度
- en: The other side of the coin to *specificity* is *sensitivity*. We just saw an
    example, of how a feature activates only for its topic and not for related topics
    (at least not so much), which is the specificity. Sensitivity now asks the question
    “but does it activate for **every** mention of the topic?” In general, you can
    easily have the one without the other. A feature may only activate for the topic
    of brain science (high specificity), but it may miss the topic in many sentences
    (low sensitivity).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*特异性*的另一面是*灵敏度*。我们刚才看到的例子展示了一个特征仅在其主题上激活，而不是相关主题（至少不是那么多），这就是特异性。灵敏度现在提出的问题是：“它是否在**每个**提到该主题的地方都激活？”通常，灵敏性和特异性可以独立存在。一个特征可能只在脑科学的主题上激活（高特异性），但它可能在许多句子中都无法激活该主题（低灵敏度）。'
- en: 'The authors spend less effort on the investigation of sensitivity. However,
    there is a demonstration that is quite easy to understand: The feature for the
    *Golden Gate Bridge* activates for sentences on that topic in many different languages,
    even without the explicit mention of the English term “Golden Gate Bridge”. More
    fine-grained analyses are quite difficult here because it is not always clear
    what a feature is supposed to represent in detail. Say you have a feature that
    you think represents *Benedict Cumberbatch*. Now you find out, that it is very
    specific (reacting to Benedict Cumberbatch only), but only reacts to some — not
    all — pictures. How can you know, if the feature is just insensitive, or if it
    is rather a feature for a more fine-grained subconcept such as *Sherlock from
    the BBC series* (played by Benedict Cumberbatch)?'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在灵敏度的研究上花费的精力较少。然而，有一个易于理解的示范：*金门大桥*的特征会在许多不同语言中激活，尽管并未明确提到英文术语“Golden Gate
    Bridge”。更精细的分析在这里是相当困难的，因为并不总是清楚一个特征应该在细节上表示什么。例如，假设你有一个特征，你认为它表示*本尼迪克特·康伯巴奇*。现在你发现，它非常具体（仅对本尼迪克特·康伯巴奇有反应），但只对某些——而不是所有——图片做出反应。那么你如何知道，这个特征只是缺乏灵敏度，还是它代表的是一个更细化的子概念，例如*BBC系列中的福尔摩斯*（由本尼迪克特·康伯巴奇饰演）？
- en: Completeness
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整性
- en: In addition to the features’ activation for their concepts (specificity and
    sensitivity), you may wonder if the model has features for all important concepts.
    It is quite difficult to decide which concepts it should have though. Do you really
    *need* a feature for Benedict Cumberbatch? Are “*sadness*” and “*feeling sad*”
    two different features? Is “*misbehaving*” a feature on its own, or can it be
    represented by the combination of the features for “*behaving*” and “*negation*”?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征对其概念的激活（特异性和灵敏度）之外，你可能会想知道模型是否涵盖了所有重要的概念。然而，决定应该包含哪些概念是相当困难的。你真的*需要*为本尼迪克特·康伯巴奇（Benedict
    Cumberbatch）创建一个特征吗？“*悲伤*”与“*感到悲伤*”是两个不同的特征吗？“*行为不端*”是一个独立的特征，还是可以通过“*行为*”和“*否定*”这两个特征的组合来表示？
- en: To catch a glance at the feature completeness, the authors selected some categories
    of concepts that have a limited number such as the elements in the periodic table.
    In the following figure, we see all the elements on the x-axis and we see whether
    a corresponding feature has been found for three different sizes of the autoencoder
    model (from 1 million to 34 million parameters).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速了解特征的完整性，作者选择了一些类别的概念，这些概念的数量有限，例如周期表中的元素。在下图中，我们可以看到所有元素位于x轴上，并且可以看到是否已经为三种不同大小的自编码器模型（从100万到3400万个参数）找到对应的特征。
- en: '![](../Images/3be6b6670fd4b10c506861c402e8e777.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3be6b6670fd4b10c506861c402e8e777.png)'
- en: Elements of the periodic table having a feature in the autoencoders of different
    sizes. Image taken from [original publication](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 各种尺寸的自编码器中具有周期表元素特征的情况。图片来自[原始出版物](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)。
- en: It is not surprising, that the biggest autoencoder has features for more different
    elements of the periodic table than the smaller ones. However, it also doesn’t
    catch all of them. We don’t know though, if this really means, that the model
    does not have a clear concept of, say, Bohrium, or if it just did not survive
    within the autoencoder.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，最大的自编码器具有比较小的自编码器更多的周期表元素特征。然而，它也并没有捕捉到所有的元素。不过，我们并不清楚这是否意味着模型没有清晰的概念，比如*博尔赫里乌姆*，还是它仅仅没有在自编码器中存活下来。
- en: Limitations
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: While we saw some demonstrations of the features representing the concepts the
    model learned, we have to emphasize that these were in fact qualitative demonstrations
    and not quantitative evaluations. All the examples were great to get an idea of
    what the model actually learned and to demonstrate the usefulness of the Monosemanticity
    approach. However, a formal evaluation that assesses all the features in a systematic
    way is needed, to really backen the insights gained from such investigations.
    That is easy to say and hard to conduct, as it is not clear, how such an evaluation
    could look like. Future research is needed to find ways to underpin such demonstrations
    with quantitative and systematic data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们看到了一些展示模型学习的概念特征的示范，但我们必须强调，这些实际上是定性的示范，而不是定量评估。所有的示例都很好地帮助我们了解模型实际上学到了什么，并展示了单义性方法的有用性。然而，需要一种正式的评估方式，系统地评估所有特征，才能真正支持从这些研究中获得的洞见。虽然这说起来容易，但实施起来很难，因为目前尚不清楚这种评估应该是什么样子。未来的研究需要找到方法，用定量和系统的数据来支撑这些示范。
- en: Summary
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '![](../Images/0bdd7ec1db2de88e15c8aa73416d04f7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bdd7ec1db2de88e15c8aa73416d04f7.png)'
- en: Monosemanticity is an interesting path, but we don’t yet know where it will
    lead us. Photo by [Ksenia Kudelkina](https://unsplash.com/@kseny?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 单义性是一个有趣的路径，但我们还不知道它将把我们引向何方。照片由[Ksenia Kudelkina](https://unsplash.com/@kseny?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: We just saw an approach that allows to gain some insights into the concepts
    a Large Language Model may leverage to arrive at its answers. A number of demonstrations
    showed how the features extracted with a sparse autoencoder can be interpreted
    in a quite intuitive way. This promises a new way to understand Large Language
    Models. If you know that the model has a feature for the concept of *lying*, you
    could expect it do to so, and having a concept of *politeness* (vs. not having
    it) can influence its answers quite a lot. For a given input, the features can
    also be used to understand the model’s thought traces. When asking a model to
    tell a story, the activation of the feature *happy end* may explain how it comes
    to a certain ending, and when the model does your tax declaration, you may want
    to know if the concept of *fraud* is activated or not.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了一种方法，可以帮助我们深入了解大型语言模型在得出答案时可能利用的概念。一些示范展示了如何以相当直观的方式解释通过稀疏自编码器提取的特征。这预示着一种新的理解大型语言模型的方法。如果你知道模型有一个*撒谎*的概念特征，你就能预期它会这样做，拥有*礼貌*的概念（与没有礼貌相比）会大大影响它的回答。对于给定的输入，这些特征还可以用来理解模型的思维轨迹。当请求模型讲一个故事时，*幸福结局*特征的激活可能解释了它如何得出某个结局，而当模型做你的税务申报时，你可能想知道*欺诈*的概念是否被激活。
- en: As we see, there is quite some potential to understand LLMs in more detail.
    A more formal and systematical evaluation of the features is needed though, to
    back the promises this format of analysis introduces.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，理解LLM的潜力相当巨大。不过，仍然需要更加正式和系统化的特征评估，以支持这种分析格式所带来的承诺。
- en: Sources
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: 'This article is based on this publication, where the Monosemanticity approach
    is applied to an LLM:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文基于这篇出版物，其中将单义性方法应用于大型语言模型（LLM）：
- en: '[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)'
- en: 'There is also a previous work that introduces the core ideas in a more basic
    model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一项前期工作介绍了更基础模型中的核心思想：
- en: '[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[走向单义性：通过词典学习解构语言模型](https://transformer-circuits.pub/2023/monosemantic-features/index.html)'
- en: 'For the Claude 3 model that has been analyzed, see here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于已分析的Claude 3模型，请参见此处：
- en: '[https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)'
- en: 'The features can be explored here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性可以在这里探索：
- en: '[https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_354889](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_354889)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_354889](https://transformer-circuits.pub/2024/scaling-monosemanticity/features/index.html?featureId=1M_354889)'
- en: '*Like this article?* [*Follow me*](/@doriandrost) *to be notified of my future
    posts.*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢这篇文章吗？* [*关注我*](/@doriandrost) *以便收到我的未来更新。*'
