- en: Do Machine Learning Models Store Protected Content?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœºå™¨å­¦ä¹ æ¨¡å‹æ˜¯å¦å­˜å‚¨å—ä¿æŠ¤çš„å†…å®¹ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06](https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06](https://towardsdatascience.com/do-machine-learning-models-store-protected-content-abec357c6b70?source=collection_archive---------6-----------------------#2024-05-06)
- en: ~A proof of concept~
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ~æ¦‚å¿µéªŒè¯~
- en: '[](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[![Nathan
    Reitinger](../Images/a4f92fd800035099e00b92ea9006181d.png)](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    [Nathan Reitinger](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[![Nathan
    Reitinger](../Images/a4f92fd800035099e00b92ea9006181d.png)](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    [Nathan Reitinger](https://medium.com/@nathanReitinger?source=post_page---byline--abec357c6b70--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    Â·5 min readÂ·May 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abec357c6b70--------------------------------)
    Â·5åˆ†é’Ÿé˜…è¯»Â·2024å¹´5æœˆ6æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aa8bacd956c21071a77783c79542970b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa8bacd956c21071a77783c79542970b.png)'
- en: 'From chatGPT to Stable Diffusion, Artificial Intelligence (AI) is having a
    summer the likes of which rival only the AI heydays of the [1970s](https://clivethompson.medium.com/the-risk-of-a-new-ai-winter-332ffb4767f0).
    This jubilation, however, has not been met without resistance. From [Hollywood](https://www.newscientist.com/article/2402251-hollywood-strike-ends-but-actors-battle-against-ai-may-not-be-over/#:~:text=The%20use%20of%20AI%20to,companies%20use%20performers''%20digital%20twins.)
    to the [Louvre](https://nftevening.com/claire-silver-brings-artificial-intelligence-nft-art-to-the-louvre/),
    AI seems to have awoken a sleeping giant â€” a giant keen to protect a world that
    once seemed exclusively human: creativity.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä»chatGPTåˆ°Stable Diffusionï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£ç»å†ä¸€ä¸ªç±»ä¼¼äº[1970å¹´ä»£](https://clivethompson.medium.com/the-risk-of-a-new-ai-winter-332ffb4767f0)çš„å¤å¤©ï¼ŒAIçš„ç››å†µå¯ä¸é‚£ä¸ªæ—¶ä»£çš„è¾‰ç…Œç›¸æå¹¶è®ºã€‚ç„¶è€Œï¼Œè¿™ä¸€æ¬¢åº†å¹¶éæ²¡æœ‰é­é‡åå¯¹ã€‚ä»[å¥½è±å](https://www.newscientist.com/article/2402251-hollywood-strike-ends-but-actors-battle-against-ai-may-not-be-over/#:~:text=The%20use%20of%20AI%20to,companies%20use%20performers'%20digital%20twins.)åˆ°[å¢æµ®å®«](https://nftevening.com/claire-silver-brings-artificial-intelligence-nft-art-to-the-louvre/)ï¼Œäººå·¥æ™ºèƒ½ä¼¼ä¹å”¤é†’äº†ä¸€ä¸ªæ²‰ç¡çš„å·¨äººâ€”â€”ä¸€ä¸ªæ¸´æœ›ä¿æŠ¤æ›¾ç»çœ‹ä¼¼ä¸“å±äºäººç±»çš„ä¸–ç•Œï¼šåˆ›é€ åŠ›ã€‚
- en: 'For those desiring to protect creativity, AI appears to have an Achilles heel:
    training data. Indeed, all of the [best models today](https://arxiv.org/pdf/2310.19909)
    necessitate a high-quality, world-encompassing data diet â€” but what does that
    mean?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›æ¸´æœ›ä¿æŠ¤åˆ›é€ åŠ›çš„äººæ¥è¯´ï¼Œäººå·¥æ™ºèƒ½ä¼¼ä¹æœ‰ä¸€ä¸ªè‡´å‘½å¼±ç‚¹ï¼šè®­ç»ƒæ•°æ®ã€‚äº‹å®ä¸Šï¼Œæ‰€æœ‰[æœ€ä½³æ¨¡å‹](https://arxiv.org/pdf/2310.19909)éƒ½éœ€è¦ä¸€ä¸ªé«˜è´¨é‡ã€æ¶µç›–å…¨çƒçš„æ•°æ®æºâ€”â€”ä½†è¿™æ„å‘³ç€ä»€ä¹ˆå‘¢ï¼Ÿ
- en: '*First*, high-quality means human created. Although [not-human-created](https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf)
    data has made many strides since the idea of a computer playing itself was popularized
    by [War Games](https://www.youtube.com/watch?v=YIh41wZEd5c), computer science
    literature has shown that model quality degrades over time if humanness is completely
    taken out of the loop (i.e., model rot or [model collapse](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240207712D/doi:10.48550/arXiv.2402.07712)).
    In simple terms: human data is the lifeblood of these models.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*é¦–å…ˆ*ï¼Œé«˜è´¨é‡æ„å‘³ç€äººä¸ºåˆ›é€ çš„ã€‚å°½ç®¡[éäººå·¥åˆ›é€ çš„](https://law.stanford.edu/wp-content/uploads/2019/01/Bellovin_20190129.pdf)æ•°æ®è‡ªä»è®¡ç®—æœºè‡ªæˆ‘å¯¹å¼ˆçš„æ¦‚å¿µè¢«[æˆ˜äº‰æ¸¸æˆ](https://www.youtube.com/watch?v=YIh41wZEd5c)æ¨å¹¿ä»¥æ¥å–å¾—äº†è®¸å¤šè¿›å±•ï¼Œè®¡ç®—æœºç§‘å­¦æ–‡çŒ®å´è¡¨æ˜ï¼Œå¦‚æœå®Œå…¨å»é™¤äººçš„å› ç´ ï¼ˆå³æ¨¡å‹è…åŒ–æˆ–[æ¨¡å‹å´©å¡Œ](https://ui.adsabs.harvard.edu/link_gateway/2024arXiv240207712D/doi:10.48550/arXiv.2402.07712)ï¼‰ï¼Œæ¨¡å‹è´¨é‡éšç€æ—¶é—´çš„æ¨ç§»ä¼šä¸‹é™ã€‚ç®€å•æ¥è¯´ï¼šäººç±»æ•°æ®æ˜¯è¿™äº›æ¨¡å‹çš„å‘½è„‰ã€‚'
- en: '*Second*, world-encompassing means world-encompassing. If you put it online,
    you should assume the model has used it in training: that Myspace post you were
    hoping only you and Tom remembered (ingested), that [picture-encased-memory](https://www.cnn.com/2022/05/24/tech/cher-scarlett-facial-recognition-trauma/index.html)
    you gladly forgot about until PimEyes forced you to remember it (ingested), and
    those late-night Reddit tirades you hoped were just a dream (ingested).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬äºŒ*ï¼Œå…¨çƒæ€§æ„å‘³ç€å…¨çƒæ€§ã€‚å¦‚æœä½ æŠŠå®ƒæ”¾åˆ°ç½‘ä¸Šï¼Œä½ åº”è¯¥å‡è®¾æ¨¡å‹å·²ç»åœ¨è®­ç»ƒä¸­ä½¿ç”¨äº†å®ƒï¼šé‚£ä¸ªä½ åŸæœ¬å¸Œæœ›åªæœ‰ä½ å’ŒTomè®°å¾—çš„Myspaceå¸–å­ï¼ˆå·²è¢«å¸æ”¶ï¼‰ï¼Œé‚£ä¸ªä½ é«˜å…´åœ°å¿˜è®°çš„[å›¾ç‰‡å°å­˜è®°å¿†](https://www.cnn.com/2022/05/24/tech/cher-scarlett-facial-recognition-trauma/index.html)ï¼Œç›´åˆ°PimEyesè¿«ä½¿ä½ é‡æ–°è®°èµ·å®ƒï¼ˆå·²è¢«å¸æ”¶ï¼‰ï¼Œä»¥åŠé‚£äº›ä½ å¸Œæœ›åªæ˜¯æ¢¦å¢ƒçš„æ·±å¤œRedditäº‰è®ºï¼ˆå·²è¢«å¸æ”¶ï¼‰ã€‚'
- en: Models like LLaMa, BERT, Stable Diffusion, Claude, and chatGPT were all trained
    on massive amounts of human-created data. And whatâ€™s unique about some, many,
    or most human-created expressions â€” especially those that happen to be fixed in
    a tangible medium a computer can access and learn from â€” is that they qualify
    for copyright protection.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åƒLLaMaã€BERTã€Stable Diffusionã€Claudeå’ŒchatGPTè¿™æ ·çš„æ¨¡å‹éƒ½æ˜¯åœ¨å¤§é‡ç”±äººç±»åˆ›ä½œçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒçš„ã€‚è€Œä¸€äº›ã€è®¸å¤šæˆ–å¤§å¤šæ•°äººç±»åˆ›ä½œçš„è¡¨è¾¾æ–¹å¼â€”â€”å°¤å…¶æ˜¯é‚£äº›æ°å¥½å›ºå®šåœ¨è®¡ç®—æœºå¯ä»¥è®¿é—®å¹¶å­¦ä¹ çš„æœ‰å½¢ä»‹è´¨ä¸Šçš„è¡¨è¾¾â€”â€”å…·æœ‰ç‰ˆæƒä¿æŠ¤çš„èµ„æ ¼ã€‚
- en: '![](../Images/4d7d5938bdf54b319faf9fd7ff8b4290.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d7d5938bdf54b319faf9fd7ff8b4290.png)'
- en: Anderson v. Stability AI; Concord Music Group, Inc. v. Anthropic PBC; Doe v.
    GitHub, Inc.; Getty Images v. Stability AI; {Tremblay, Silverman, Chabon} v. OpenAI;
    New York Times v. Microsoft
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson v. Stability AIï¼›Concord Music Group, Inc. v. Anthropic PBCï¼›Doe v. GitHub,
    Inc.ï¼›Getty Images v. Stability AIï¼›{Tremblay, Silverman, Chabon} v. OpenAIï¼›çº½çº¦æ—¶æŠ¥è¯‰å¾®è½¯
- en: Fortuitous as it may be, the data these models cannot survive without is the
    same data most protected by copyright. And this gives rise to the titanic copyright
    battles we are seeing today.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å¯èƒ½æ˜¯å¶ç„¶çš„ï¼Œè¿™äº›æ¨¡å‹æ— æ³•ç”Ÿå­˜çš„æ•°æ®æ­£æ˜¯å¤§å¤šæ•°å—åˆ°ç‰ˆæƒä¿æŠ¤çš„æ•°æ®ã€‚è¿™ä¹Ÿå‚¬ç”Ÿäº†æˆ‘ä»¬ä»Šå¤©çœ‹åˆ°çš„å·¨å¤§çš„ç‰ˆæƒæ–—äº‰ã€‚
- en: 'Of the many questions arising in these lawsuits, one of the most pressing is
    whether models themselves store protected content. This question seems rather
    obvious, because how can we say that models â€” merely collections of numbers (i.e.,
    weights) with an architecture â€” â€œstoreâ€ anything? As Professor Murray states:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›è¯‰è®¼ä¸­äº§ç”Ÿçš„è®¸å¤šé—®é¢˜ä¸­ï¼Œæœ€ç´§è¿«çš„ä¸€ä¸ªé—®é¢˜æ˜¯æ¨¡å‹æœ¬èº«æ˜¯å¦å­˜å‚¨å—ä¿æŠ¤çš„å†…å®¹ã€‚è¿™ä¸ªé—®é¢˜ä¼¼ä¹ç›¸å½“æ˜æ˜¾ï¼Œå› ä¸ºæˆ‘ä»¬æ€ä¹ˆèƒ½è¯´æ¨¡å‹â€”â€”ä»…ä»…æ˜¯ç”±æ•°å­—ï¼ˆå³æƒé‡ï¼‰å’Œæ¶æ„ç»„æˆçš„é›†åˆâ€”â€”â€œå­˜å‚¨â€äº†ä»€ä¹ˆï¼Ÿæ­£å¦‚Murrayæ•™æˆæ‰€è¯´ï¼š
- en: Many of the participants in the current debate on visual generative AI systems
    have latched onto the idea that generative AI systems have been trained on datasets
    and foundation models that contained actual copyrighted image files, .jpgs, .gifs,
    .png files and the like, scraped from the internet, that somehow the dataset or
    foundation model must have made and stored copies of these works, and somehow
    the generative AI system further selected and copied individual images out of
    that dataset, and somehow the system copied and incorporated significant copyrightable
    parts of individual images into the final generated images that are offered to
    the end-user. This is magical thinking.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“å‰å…³äºè§†è§‰ç”Ÿæˆå‹AIç³»ç»Ÿçš„è¾©è®ºä¸­çš„è®¸å¤šå‚ä¸è€…æŠ“ä½äº†è¿™æ ·ä¸€ä¸ªè§‚ç‚¹ï¼šç”Ÿæˆå‹AIç³»ç»Ÿå·²åœ¨åŒ…å«å®é™…ç‰ˆæƒä¿æŠ¤çš„å›¾åƒæ–‡ä»¶ï¼ˆå¦‚.jpgã€.gifã€.pngç­‰ï¼‰æ•°æ®é›†å’ŒåŸºç¡€æ¨¡å‹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ–‡ä»¶æ˜¯ä»äº’è”ç½‘ä¸ŠæŠ“å–çš„ï¼Œæ•°æ®é›†æˆ–åŸºç¡€æ¨¡å‹ä¸€å®šå·²ç»åˆ¶ä½œå¹¶å­˜å‚¨äº†è¿™äº›ä½œå“çš„å‰¯æœ¬ï¼Œå¹¶ä¸”ç”Ÿæˆå‹AIç³»ç»Ÿä»¥æŸç§æ–¹å¼è¿›ä¸€æ­¥é€‰æ‹©å¹¶å¤åˆ¶äº†è¿™äº›æ•°æ®é›†ä¸­çš„ä¸ªåˆ«å›¾åƒï¼Œå¹¶ä»¥æŸç§æ–¹å¼å°†è¿™äº›å›¾åƒçš„é‡å¤§å¯ç‰ˆæƒéƒ¨åˆ†å¤åˆ¶å¹¶çº³å…¥åˆ°æœ€ç»ˆç”Ÿæˆçš„å›¾åƒä¸­ï¼Œä¾›æœ€ç»ˆç”¨æˆ·ä½¿ç”¨ã€‚è¿™æ˜¯ä¸€ç§é­”æ³•èˆ¬çš„æ€ç»´ã€‚
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Michael D. Murray, 26 SMU Science and Technology Law Review 259, 281 (2023)
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Michael D. Murray, 26 SMU ç§‘æŠ€ä¸æ³•å¾‹è¯„è®º 259, 281 (2023)
- en: And yet, models themselves do seem, in some circumstances, [to memorize training
    data](https://arxiv.org/pdf/2301.13188).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ¨¡å‹æœ¬èº«ä¼¼ä¹åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œ[ç¡®å®ä¼šè®°ä½è®­ç»ƒæ•°æ®](https://arxiv.org/pdf/2301.13188)ã€‚
- en: The following toy example is from a [Gradio Space on HuggingFace](https://huggingface.co/spaces/nathanReitinger/modelProblems)
    which allows users to pick a model, see an output, and check â€” from that modelâ€™s
    training data â€” how similar the generated image is to any image in its training
    data. MNIST digits were used to generate because they are easy for the machine
    to parse, easy for humans to interpret in terms of similarity, and have the nice
    property of being easily classified â€” allowing a hunt of similarity to only consider
    images that are of the same number (efficiency gains).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç¤ºä¾‹æ¥è‡ª[HuggingFaceä¸Šçš„Gradio Space](https://huggingface.co/spaces/nathanReitinger/modelProblems)ï¼Œè¯¥å¹³å°å…è®¸ç”¨æˆ·é€‰æ‹©ä¸€ä¸ªæ¨¡å‹ï¼ŒæŸ¥çœ‹è¾“å‡ºï¼Œå¹¶ä»è¯¥æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸­æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒä¸å…¶è®­ç»ƒæ•°æ®ä¸­ä»»ä½•å›¾åƒçš„ç›¸ä¼¼åº¦ã€‚ç”±äºMNISTæ•°å­—æ˜“äºæœºå™¨è§£æã€æ˜“äºäººç±»ä»ç›¸ä¼¼æ€§è§’åº¦ç†è§£ï¼Œå¹¶ä¸”å…·æœ‰æ˜“äºåˆ†ç±»çš„ä¼˜ç‚¹â€”â€”è¿™ä½¿å¾—ç›¸ä¼¼æ€§æœç´¢åªè€ƒè™‘ç›¸åŒæ•°å­—çš„å›¾åƒï¼ˆæé«˜æ•ˆç‡ï¼‰ï¼Œå› æ­¤ä½¿ç”¨äº†MNISTæ•°å­—ã€‚
- en: Letâ€™s see how it works!
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼
- en: The following image has a similarity score of .00039\. RMSE stands for Root
    Mean Squared Error and is a way of assessing the similarity between two images.
    True enough, many other methods for similarity assessment exist, but RMSE gives
    you a pretty good idea of whether an image is a duplicate or not (i.e., we are
    not hunting for a legal definition of similarity here). As an example, an RMSE
    of <.006 gets you into the nearly â€œcopyâ€ range, and an RMSE of <.0009 is entering
    perfect copy territory (indistinguishable to the naked eye).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾åƒçš„ç›¸ä¼¼åº¦å¾—åˆ†ä¸º0.00039ã€‚RMSEä»£è¡¨å‡æ–¹æ ¹è¯¯å·®ï¼Œæ˜¯è¯„ä¼°ä¸¤å¼ å›¾åƒç›¸ä¼¼åº¦çš„ä¸€ç§æ–¹å¼ã€‚äº‹å®ä¸Šï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–ç›¸ä¼¼æ€§è¯„ä¼°æ–¹æ³•ï¼Œä½†RMSEèƒ½å¾ˆå¥½åœ°åˆ¤æ–­ä¸€å¼ å›¾åƒæ˜¯å¦ä¸ºå‰¯æœ¬ï¼ˆå³ï¼Œæˆ‘ä»¬è¿™é‡Œå¹¶ä¸æ˜¯åœ¨å¯»æ‰¾æ³•å¾‹å®šä¹‰çš„ç›¸ä¼¼æ€§ï¼‰ã€‚ä¸¾ä¸ªä¾‹å­ï¼ŒRMSEå€¼å°äº0.006æ—¶ï¼Œå›¾åƒå·²æ¥è¿‘â€œå¤åˆ¶â€èŒƒå›´ï¼Œè€ŒRMSEå€¼å°äº0.0009æ—¶ï¼Œåˆ™è¿›å…¥å®Œç¾å¤åˆ¶çš„é¢†åŸŸï¼ˆè‚‰çœ¼æ— æ³•åˆ†è¾¨ï¼‰ã€‚
- en: '![](../Images/826e5055d8ed033dbdf5653fa61840b8.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/826e5055d8ed033dbdf5653fa61840b8.png)'
- en: '[ğŸ¤—](https://huggingface.co/spaces/nathanReitinger/modelProblems) A model that
    generates a nearly exact copy of training data (RMSE at .0003) [ğŸ¤—](https://huggingface.co/spaces/nathanReitinger/modelProblems)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[ğŸ¤—](https://huggingface.co/spaces/nathanReitinger/modelProblems) ä¸€ä¸ªç”Ÿæˆå‡ ä¹å®Œå…¨ç›¸åŒè®­ç»ƒæ•°æ®å‰¯æœ¬çš„æ¨¡å‹ï¼ˆRMSEä¸º0.0003ï¼‰[ğŸ¤—](https://huggingface.co/spaces/nathanReitinger/modelProblems)'
- en: 'To use the [Gradio space](https://huggingface.co/spaces/nathanReitinger/modelProblems),
    follow these three steps (optionally build the space if itâ€™s sleeping):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨[Gradioç©ºé—´](https://huggingface.co/spaces/nathanReitinger/modelProblems)ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤æ“ä½œï¼ˆå¦‚æœç©ºé—´å¤„äºä¼‘çœ çŠ¶æ€ï¼Œå¯ä»¥é€‰æ‹©æ„å»ºè¯¥ç©ºé—´ï¼‰ï¼š
- en: '**STEP 1**: Select the type of pre-trained model to use'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤1**ï¼šé€‰æ‹©è¦ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç±»å‹'
- en: '**STEP 2**: Hit â€œsubmitâ€ and the model will generate an image for you (a 28x28
    grayscale image)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤2**ï¼šç‚¹å‡»â€œæäº¤â€æŒ‰é’®ï¼Œæ¨¡å‹å°†ä¸ºæ‚¨ç”Ÿæˆä¸€å¼ å›¾åƒï¼ˆ28x28çš„ç°åº¦å›¾åƒï¼‰'
- en: '**STEP 3**: The Gradio app searches through that modelâ€™s training data to identify
    the most similar image to the generated image (out of 60K examples)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤3**ï¼šGradioåº”ç”¨ç¨‹åºä¼šåœ¨è¯¥æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸­æœç´¢ï¼Œè¯†åˆ«ä¸ç”Ÿæˆå›¾åƒæœ€ç›¸ä¼¼çš„å›¾åƒï¼ˆä»60Kä¸ªç¤ºä¾‹ä¸­ç­›é€‰ï¼‰'
- en: As is plain to see, the image generated on the left (AI creation) is nearly
    an exact copy of the training data on the right when the â€œFASHION-diffusion-oneImageâ€
    model is used. And this makes sense. This model was trained on *only* a single
    image from the [FASHION dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist).
    The same is true for the â€œMNIST-diffusion-oneImageâ€ model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸Šæ‰€ç¤ºï¼Œå·¦ä¾§ç”Ÿæˆçš„å›¾åƒï¼ˆAIåˆ›ä½œï¼‰å‡ ä¹ä¸å³ä¾§çš„è®­ç»ƒæ•°æ®å®Œå…¨ç›¸åŒï¼Œå½“ä½¿ç”¨â€œFASHION-diffusion-oneImageâ€æ¨¡å‹æ—¶ï¼Œç»“æœæ­£æ˜¯å¦‚æ­¤ã€‚è¿™æ˜¯æœ‰é“ç†çš„ã€‚è¯¥æ¨¡å‹ä»…å¯¹[FASHIONæ•°æ®é›†](https://www.tensorflow.org/datasets/catalog/fashion_mnist)ä¸­çš„ä¸€å¼ å›¾åƒè¿›è¡Œäº†è®­ç»ƒã€‚åŒæ ·çš„æƒ…å†µä¹Ÿé€‚ç”¨äºâ€œMNIST-diffusion-oneImageâ€æ¨¡å‹ã€‚
- en: 'That said, even models trained on more images (e.g., 300, 3K, or 60K images)
    can produce eerily similar output. This example comes from a Generative Adversarial
    Network (GAN) trained on the full 60K image dataset (training only) of [MNIST
    hand-drawn digits](https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe).
    As background, GANs are known to produce [less-memorized generations](https://arxiv.org/abs/2301.13188)
    than diffusion models:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¦‚æ­¤ï¼Œå³ä½¿æ˜¯è®­ç»ƒäº†æ›´å¤šå›¾åƒï¼ˆä¾‹å¦‚300å¼ ã€3000å¼ æˆ–60000å¼ å›¾åƒï¼‰çš„æ¨¡å‹ï¼Œä¹Ÿèƒ½äº§ç”Ÿéå¸¸ç›¸ä¼¼çš„è¾“å‡ºã€‚è¿™ä¸ªç¤ºä¾‹æ¥è‡ªä¸€ä¸ªç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼Œå®ƒåœ¨å®Œæ•´çš„60Kå›¾åƒæ•°æ®é›†ï¼ˆä»…é™è®­ç»ƒï¼‰ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼Œæ•°æ®é›†åŒ…æ‹¬[MNISTæ‰‹å†™æ•°å­—](https://etzold.medium.com/mnist-dataset-of-handwritten-digits-f8cf28edafe)ã€‚ä½œä¸ºèƒŒæ™¯ï¼Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰é€šå¸¸ç”Ÿæˆçš„å›¾åƒ[æ¯”æ‰©æ•£æ¨¡å‹](https://arxiv.org/abs/2301.13188)è®°å¿†æ€§å·®ï¼š
- en: '![](../Images/d11c66b90b7f39f29515de89eaa6a1b8.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d11c66b90b7f39f29515de89eaa6a1b8.png)'
- en: RMSE at .008
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RMSEä¸º0.008
- en: 'Hereâ€™s another with a*diffusion model* trained on the 60K MNIST dataset (i.e.,
    the type of model powering Stable Diffusion):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦ä¸€ä¸ªä½¿ç”¨*æ‰©æ•£æ¨¡å‹*ï¼Œå¹¶åœ¨60K MNISTæ•°æ®é›†ä¸Šè®­ç»ƒçš„å›¾åƒï¼ˆå³ï¼Œæ”¯æŒç¨³å®šæ‰©æ•£çš„æ¨¡å‹ç±»å‹ï¼‰ï¼š
- en: '![](../Images/f05a6c47cea9b8483157508ec39a87d1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f05a6c47cea9b8483157508ec39a87d1.png)'
- en: RMSE at .004
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RMSEä¸º0.004
- en: Feel free to play around with the [Gradio space yourself](https://huggingface.co/spaces/nathanReitinger/modelProblems),
    investigate the models, or reach out to me with questions!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ—¶å¯ä»¥è‡ªå·±å°è¯•ä½¿ç”¨[Gradioç©ºé—´](https://huggingface.co/spaces/nathanReitinger/modelProblems)ï¼Œæ¢ç´¢æ¨¡å‹ï¼Œæˆ–è€…å¦‚æœæœ‰é—®é¢˜å¯ä»¥è”ç³»æˆ‘ï¼
- en: '**Summary:** The point of this small, toy example is that there is nothing
    mystical or absolute-copyright-nullifying about machine-learning models. Machine
    learning models can and do produce images that are copies of their training data
    â€” in other words, models can and do *store* protected content, and may therefore
    run into copyright problems. True enough, there are many counterarguments to be
    made here (my work in progress!); this demo should only be taken as anecdotal
    evidence of storage, and possibly a canary for developers working in this space.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ç»“ï¼š** è¿™ä¸ªå°å‹ç¤ºä¾‹çš„é‡ç‚¹æ˜¯ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹æ²¡æœ‰ä»€ä¹ˆç¥ç§˜æˆ–ç»å¯¹çš„ç‰ˆæƒè±å…ã€‚æœºå™¨å­¦ä¹ æ¨¡å‹ç¡®å®å¯ä»¥å¹¶ä¸”ä¼šç”Ÿæˆä¸å…¶è®­ç»ƒæ•°æ®ç›¸åŒçš„å›¾åƒâ€”â€”æ¢å¥è¯è¯´ï¼Œæ¨¡å‹ç¡®å®ä¼š*å­˜å‚¨*å—ä¿æŠ¤çš„å†…å®¹ï¼Œå› æ­¤å¯èƒ½ä¼šé‡åˆ°ç‰ˆæƒé—®é¢˜ã€‚å½“ç„¶ï¼Œä¹Ÿæœ‰è®¸å¤šåé©³çš„è®ºç‚¹ï¼ˆæˆ‘æ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼ï¼‰ï¼›è¿™ä¸ªæ¼”ç¤ºåº”è¯¥ä»…ä½œä¸ºå­˜å‚¨çš„è½¶äº‹æ€§è¯æ®ï¼Œå¯èƒ½æ˜¯å¼€å‘è€…åœ¨è¿™ä¸€é¢†åŸŸå·¥ä½œçš„â€œé‡‘ä¸é›€â€ã€‚'
- en: What goes into a model is just as important as what comes out, and this is especially
    true for certain models performing certain tasks. We need to be careful and mindful
    of our â€œback boxesâ€ because this analogy often turns out not to be true. That
    you cannot interpret for yourself the set of weights held by a model does not
    mean you escape all forms of liability or scrutiny.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å†…å®¹å’Œä»æ¨¡å‹ä¸­å¾—åˆ°çš„ç»“æœåŒæ ·é‡è¦ï¼Œå°¤å…¶å¯¹äºæŸäº›æ‰§è¡Œç‰¹å®šä»»åŠ¡çš„æ¨¡å‹æ¥è¯´æ›´æ˜¯å¦‚æ­¤ã€‚æˆ‘ä»¬éœ€è¦å°å¿ƒå¹¶å…³æ³¨æˆ‘ä»¬çš„â€œé»‘ç®±â€ï¼Œå› ä¸ºè¿™ä¸ªç±»æ¯”å¾€å¾€å¹¶ä¸æˆç«‹ã€‚ä½ æ— æ³•è‡ªå·±è§£è¯»æ¨¡å‹æ‰€æŒæœ‰çš„æƒé‡é›†åˆï¼Œå¹¶ä¸æ„å‘³ç€ä½ å¯ä»¥æ‘†è„±æ‰€æœ‰å½¢å¼çš„è´£ä»»æˆ–å®¡æŸ¥ã€‚
- en: '*â€”* [*@nathanReitinge*](https://nathanreitinger.umiacs.io)*r stay tuned for
    further work in this space!*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€”* [*@nathanReitinge*](https://nathanreitinger.umiacs.io)*rï¼Œæ•¬è¯·å…³æ³¨è¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥å·¥ä½œï¼*'
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
