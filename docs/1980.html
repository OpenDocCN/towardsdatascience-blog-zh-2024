<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Talk to a PDF File Without Using Proprietary Models: CLI + Streamlit + Ollama</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Talk to a PDF File Without Using Proprietary Models: CLI + Streamlit + Ollama</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14">https://towardsdatascience.com/how-to-talk-to-a-pdf-file-without-using-proprietary-models-cli-streamlit-ollama-6c22437ed932?source=collection_archive---------4-----------------------#2024-08-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/5bbf5cec507aee72cad3963558d8f2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMbSpkta64YCd8PvJIJW8Q.gif"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Talk to a PDF file <em class="gi">(Gif by author)</em></figcaption></figure><div/><div><h2 id="4068" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">A contribution to the creation of a locally executed, free PDF chat app with Streamlit and Meta AI’s LLaMA model, without API limitations</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Stefan Pietrusky" class="l ep by dd de cx" src="../Images/f5abf75db277f3aec8d8e56877daafe4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*_XYa6lOVGzIsZ67doGI91w.png"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@stefanpietrusky?source=post_page---byline--6c22437ed932--------------------------------" rel="noopener follow">Stefan Pietrusky</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c22437ed932--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lx ly ab q ee lz ma" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lw"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lv lw">4</span></p></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="mb k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mc an ao ap iy md me mf" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mg cn"><div class="l ae"><div class="ab cb"><div class="mh mi mj mk ml gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="fc31" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I have already read various articles on the internet about how the open source framework Streamlit can be used in combination with machine learning to quickly and easily create interesting interactive web applications. This is very useful for developing experimental applications without extensive front-end development. One article showed how to create a conversation chain using an OpenAI language model and then execute it. An instance of the chat model <em class="nz">“gpt-3.5-turbo”</em> was created, the parameter <em class="nz">“temperature”</em> was defined with a value of 0 so that the model responds deterministically and finally a placeholder for the API key was implemented. The latter is required to authenticate the model when it is used.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="bf82" class="oj ok gl og b bg ol om l on oo">llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0, api_key="")</span></pre><p id="dd7f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the comments, I often read the question of how to deal with a particular error message or how it can be solved.</p><p id="e8cb" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">RateLimitError: Error code: 429 — {‘error’: {‘message’: ‘You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: </em><a class="af op" href="https://platform.openai.com/docs/guides/error-codes/api-errors.'," rel="noopener ugc nofollow" target="_blank"><em class="nz">https://platform.openai.com/docs/guides/error-codes/api-errors.',</em></a><em class="nz"> ‘type’: ‘insufficient_quota’, ‘param’: None, ‘code’: ‘insufficient_quota’}}</em></p><p id="528c" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Error 429 indicates that the request sent to the OpenAI API has exceeded the current usage quota. The available API calls in a certain period or the general usage limit of the subscription have been reached. The error is easy to solve. You take out a paid subscription with the respective provider and thus increase your usage limit. This gave me the idea of why you can’t simply use open source models that are operated locally and thus circumvent the limitation without having to pay anything.</p><p id="0433" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, I will show how to use Streamlit to create an application to which PDF files can be uploaded in order to ask questions based on their content, which are answered by integrating an LLM. There are no limits or costs incurred when using the app. The response time (input-output) will take a little longer, depending on the system, but remains within reasonable limits. First, we take care of the LLM that we will use.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4203" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">A KINGDOM FOR A LLAMA</h2><p id="2a82" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">We will use the open source language model Llama from Meta AI. As part of the recent development in the field of large language models, it will be used within the app to understand and generate natural language (NLP). In order to use the LLM locally, we first need to install Ollama on our system. To do this, we go to the following <a class="af op" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank"><strong class="nf gm">official site</strong></a> and download the open source platform. The system may need to be restarted afterward.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp px"><img src="../Images/b2e80402088c8e5141c11a64a52bea18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zuJQuwxZWAsbaBHwj0fqxw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Download Ollama (<a class="af op" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank">Public Domain</a>)</figcaption></figure><p id="dee9" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once Ollama has been installed, we click on<em class="nz"> “Models” </em>and select the <em class="nz">“llama3.1”</em> model in the overview that opens. Llama is based on the Transformer architecture, has been trained on large and diverse data sets, is available in different sizes and is ideally suited for the development of practical applications due to its openness and accessibility. In this article, the smallest size <em class="nz">“8B”</em> is used to ensure that the app also works on less powerful systems. Once the correct model has been selected, copy the command shown and execute it in the terminal.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp py"><img src="../Images/6bbb1a1ecbe418de4246df8470fddfcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUMG2kSHz4Io9UK0habNcA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">LLama3.1 overview on Ollama platform (<a class="af op" href="https://ollama.com/library/llama3.1" rel="noopener ugc nofollow" target="_blank">Public Domain</a>)</figcaption></figure><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="c445" class="oj ok gl og b bg ol om l on oo">ollama run llama3.1</span></pre><p id="2445" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once the model has been downloaded, you can communicate with it via the terminal. Next, let’s move on to setting up the app. In a nutshell, the process is as follows. The PDF file is uploaded and the text it contains is extracted. The extracted text is divided into smaller chunks that are stored in a vector store. The user enters a question. The question, i.e. the input, is prepared for the model by combining the question and the context. The LLM is queried and generates the answer.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pz"><img src="../Images/f6c3884b08f96c6d4738e89ed1795425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONaF6-tfN-odSsj1lD5PWQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">App process (<em class="gi">Image by author)</em></figcaption></figure></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b528" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [REQUIRED LIBRARIES]</h2><p id="e51c" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">Various libraries are required for the application to function correctly, which are briefly described below. The execution of system commands in Python and communication with them is made possible by <em class="nz">“subprocess”</em>. We need <em class="nz">“streamlit” </em>to create the web application. <em class="nz">“PyPDF2”</em> is used to read PFD documents. The splitting of texts into smaller sections is done by <em class="nz">“langchain.text_splitter.RecursiveCharacterTextSplitter”</em>. The library <em class="nz">“langchain_community.embeddings.SpacyEmbeddings” </em>is used to generate text embeddings with the Spacy model. The vector store <em class="nz">“langchain_community.vectorstores.FAISS” </em>enables the efficient saving and retrieval of embeddings. For the definition of prompt templates for chat interactions, “<em class="nz">langchain_core.prompts.ChatPromptTemplate”</em> is used. Access to operating system functions is obtained via <em class="nz">“os” and “re”</em> is used to recognize patterns in character strings. Python should also be installed on the system. Depending on the operating system, the required execution file can be downloaded from the <a class="af op" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank"><strong class="nf gm">official site</strong></a>. After installation, you can check whether the installation was successful using the terminal with the following command.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="0af6" class="oj ok gl og b bg ol om l on oo">python --version</span></pre><p id="7346" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The required libraries are installed via the terminal with the following command:</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="e70b" class="oj ok gl og b bg ol om l on oo">pip install streamlit PyPDF2 langchain langchain-community spacy faiss-cpu</span></pre><p id="d3a5" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">“subprocess”, “os” and “re” </em>are built-in Python libraries and do not need to be installed separately. The Spacy language model, however, must be downloaded separately with the following command.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="3598" class="oj ok gl og b bg ol om l on oo">python -m spacy download en_core_web_sm</span></pre><p id="3b1f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The list of dependencies for the app is as follows.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="9877" class="oj ok gl og b bg ol om l on oo">import subprocess<br/>import streamlit as st<br/>from PyPDF2 import PdfReader<br/>from langchain.text_splitter import RecursiveCharacterTextSplitter<br/>from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings<br/>from langchain_community.vectorstores import FAISS<br/>from langchain.tools.retriever import create_retriever_tool<br/>from langchain_core.prompts import ChatPromptTemplate<br/>import os<br/>import re<br/>import psutil</span></pre><p id="12f5" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that everything you need is in place, let’s move on to setting up the app. The various components of the script are described below.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9361" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [ENVIRONMENT CONFIGURATION]</h2><p id="4dd9" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">To avoid problems when loading libraries, especially during parallel processing, it is necessary to set the environment variable <em class="nz">“KMP_DUPLICATE_LIB_OK” </em>to <em class="nz">“TRUE”</em>. In the context of this article, this configuration is due to the use of FAISS [Facebook AI Similarity Search], which uses parallel computing operations when searching data sets.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="7aa8" class="oj ok gl og b bg ol om l on oo">os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9e1e" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [PDF READING FUNCTION]</h2><p id="9a4b" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The <em class="nz">“pdf_read()”</em> function reads the entire text from a PDF file. Specifically, <em class="nz">“PyPDF2” </em>is used to extract the text. The text is then combined into a single character string <em class="nz">“text”</em>, which is returned. The function is important in order to make the content of the PDF file available for further processing steps.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="381a" class="oj ok gl og b bg ol om l on oo">def pdf_read(pdf_doc):<br/>    """Read the text from PDF document."""<br/>    text = ""<br/>    for pdf in pdf_doc:<br/>        pdf_reader = PdfReader(pdf)<br/>        for page in pdf_reader.pages:<br/>            page_text = page.extract_text()<br/>            if page_text:<br/>                text += page_text<br/>    return text</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6fff" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In principle, several PDF files can be uploaded at the same time, whereby these together form the context. If you want to analyze individual files, you should only upload one file at a time, then remove it and upload a new file. The upload of multiple files, whereby these are viewed as individual contexts, is implemented in a customized version of the app.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="eb74" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [TEXT-CHUNKS FUNCTION]</h2><p id="bf4c" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The combined character string from the previous function is split into smaller text chunks in the next step using the <em class="nz">“create_text_chunks()” </em>function. The maximum number of characters per chunk <em class="nz">“chunk_size”</em> is 1000 and the number of characters that can overlap in adjacent chunks <em class="nz">“chunk_overlap”</em> is 200. This implementation enables the app to query and process larger amounts of text more efficiently. Exceeding the input size of the model is prevented. The search is optimized by the split, as smaller, contextualized sections (granularity) can be queried more accurately (more detailed vectors), improving the provision of information overall. The accuracy and processing speed of the model are also increased.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="d37d" class="oj ok gl og b bg ol om l on oo">def create_text_chunks(text, chunk_size=1000, chunk_overlap=200):<br/>    """Create text chunks from a large text block."""<br/>    text_splitter = RecursiveCharacterTextSplitter(<br/>        chunk_size=chunk_size,<br/>        chunk_overlap=chunk_overlap<br/>    )<br/>    text_chunks = text_splitter.split_text(text)<br/>    return text_chunks</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b10e" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [TEXT-EMBEDDING]</h2><p id="83a1" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">An object for text embeddings, numerical representation of text, using the Spacy model must be created to capture the meaning of the text uploaded as a PDF file. The embeddings are then used to vectorize the text in order to store them in a vector store so that they can be used for semantic search.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="d54f" class="oj ok gl og b bg ol om l on oo">embeddings = SpacyEmbeddings(model_name="en_core_web_sm")</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ce8b" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [VECTOR-STORE-FUNCTION]</h2><p id="fa1c" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The <em class="nz">“vector_store()”</em> function uses the aforementioned FAISS to store the embeddings of the text chunks. The vector store enables faster retrieval and searching of texts based on the existing embeddings. The vector store is saved locally so that it can be accessed later.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qa"><img src="../Images/fa02e6bddbeb2ed4097a99f593e4d89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6fKHGHQnBNuPeDr49JBPw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">From PDF to vector store <em class="gi">(Image by author)</em></figcaption></figure><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="beec" class="oj ok gl og b bg ol om l on oo">def vector_store(text_chunks):<br/>    """Create a vector store for the text chunks."""<br/>    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)<br/>    vector_store.save_local("faiss_db")</span></pre><p id="f650" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Vectors capture the meaning and context of text by translating sentences and words into a mathematically interpretable space. The text “<em class="nz">The weather is nice today”</em> is converted into an example vector [0.25, -0.47, 0.19, …]. This makes it easier to carry out similarity searches.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c953" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [CLI BASED LLAMA REQUEST]</h2><p id="aacf" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The function <em class="nz">“query_llama_via_cli()” </em>enables communication with an external LLaMA model process via the command line. Input data is sent, the response is received, processed and any errors that occur are handled errors=’ignore’. This function allows the LLM to be implemented in the application workflow, although it runs in a separate environment that is controlled via CLI (Command Line Interface). The advantage of CLIs as a command line interface is that they are platform-independent, which makes the app available on almost any operating system.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="332c" class="oj ok gl og b bg ol om l on oo">def query_llama_via_cli(input_text):<br/>    """Query the Llama model via the CLI."""<br/>    try:<br/>        # Start the interactive process<br/>        process = subprocess.Popen(<br/>            ["ollama", "run", "llama3.1"],<br/>            stdin=subprocess.PIPE,<br/>            stdout=subprocess.PIPE,<br/>            stderr=subprocess.PIPE,<br/>            text=True,  # Ensure that communication takes place as text (UTF-8)<br/>            encoding='utf-8',  # Set UTF-8 encoding explicitly<br/>            errors='ignore',  # Ignore incorrect characters<br/>            bufsize=1<br/>        )<br/>        <br/>        # Send the input to the process<br/>        stdout, stderr = process.communicate(input=f"{input_text}\n", timeout=30)<br/><br/>        # Check error output<br/>        if process.returncode != 0:<br/>            return f"Error in the model request: {stderr.strip()}"<br/><br/>        # Filter response and remove control characters<br/>        response = re.sub(r'\x1b\[.*?m', '', stdout)  # Remove ANSI codes<br/><br/>        # Extract the relevant answer<br/>        return extract_relevant_answer(response)<br/><br/>    except subprocess.TimeoutExpired:<br/>        process.kill()<br/>        return "Timeout for the model request"<br/>    except Exception as e:<br/>        return f"An unexpected error has occurred: {str(e)}"</span></pre><p id="e1ef" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A more detailed explanation of the function follows. The process is started by <em class="nz">“subprocess.Popen()”</em>. The command to start the LLM is executed [“ollama”, “run”, “llama3.1”]. Access to the input and output streams of the process (sending data and receiving results) is made possible by the parameters <em class="nz">“stdin”</em>, <em class="nz">“stdout”</em> and <em class="nz">“stderr”</em>. Communication takes place as UTF-8 encoded text <em class="nz">encoding=’utf-8'</em>. To improve interactivity, the buffer size for the I/O operations is set to line buffering <em class="nz">“bufsize=1”</em>.</p><p id="263f" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The input <em class="nz">“input_text”</em> is transmitted to the process, specifically to the LLM, whereupon a response is generated <em class="nz">“stdout”</em>. The maximum time (seconds) that is waited until the process has to deliver a response is 30 seconds “timeout=30”. If it takes longer, a timeout error is triggered <em class="nz">“stderr”</em>. The return code checks whether the process was successful <em class="nz">“returncode == 0”</em>. If this is not the case, an error message is returned. The app does not take as long to return a response. Finally, the response “stdout” is processed. Unwanted characters are removed and ANSI color and formatting codes are removed from the output <em class="nz">“response = re.sub(r’\x1b\[.*?m’, ‘’, stdout)”</em>. To extract and format the relevant response from the complete module response, <em class="nz">“extract_relevant_answer” i</em>s called. The process is terminated with <em class="nz">“process.kill()”</em> if the timeout of 30 seconds is exceeded. Errors that occur during communication are intercepted and returned as a general error message.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="abad" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [EXTRACTION OF RELEVANT ANSWERS]</h2><p id="56d4" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The relevant response is extracted from the entire model response using the <em class="nz">“extract_relevant_answer()”</em> function. At the same time, the function also removes simple formatting problems, specifically spaces at the beginning and end of the composite character string that forms the response <em class="nz">“strip()”</em>. Depending on the specific requirements of the app, the function can be extended to return certain keywords or sentences (markers). The integration of additional rules for cleansing and formatting is also possible.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="4a69" class="oj ok gl og b bg ol om l on oo">def extract_relevant_answer(full_response):<br/>    """Extract the relevant response from the full model response."""<br/>    response_lines = full_response.splitlines()<br/><br/>    # Search for the relevant answer; if there is a marker, it can be used here<br/>    if response_lines:<br/>        # Assume that the answer comes as a complete return to be filtered<br/>        return "\n".join(response_lines).strip()<br/><br/>    return "No answer received"</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3bd8" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [THE CONVERSATION CHAIN]</h2><p id="2fdb" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The conversation chain is created by the function <em class="nz">“get_conversational_chain()”</em>. The function prepares the input for the LLM by combining a specific prompt and the context together with the user’s question. The model should be provided with a clear and structured input in order to deliver the best possible answer. A multi-level prompt schema (system message, human message <em class="nz">“{input}”</em> and a placeholder) is defined by <em class="nz">“ChatPromptTemplate.from_message()”</em>. The role of the model is defined by the system message “system”. The human message then contains the user’s question. The prompt (behavior of the model), the context (content of the PDF file) and the question (user of the app) are combined in <em class="nz">“input_text”</em>. The prepared input is sent to the LLM via CLI using the function <em class="nz">“query_llama_via_cli(input_text)”</em>. The output is saved as <em class="nz">“response” </em>and displayed in the Streamlit app with <em class="nz">“st.write(“PDF: “, response)”</em>.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="3b71" class="oj ok gl og b bg ol om l on oo">def get_conversational_chain(context, ques):<br/>    """Create the input for the model based on the prompt and context."""<br/>    # Define the prompt behavior<br/>    prompt = ChatPromptTemplate.from_messages(<br/>        [<br/>            (<br/>                "system",<br/>                """You are an intelligent and helpful assistant. Your goal is to provide the most accurate and detailed answers <br/>                possible to any question you receive. Use all available context to enhance your answers, and explain complex <br/>                concepts in a simple manner. If additional information might help, suggest further areas for exploration. If the <br/>                answer is not available in the provided context, state this clearly and offer related insights when possible.""",<br/>            ),<br/>            ("human", "{input}"),<br/>            ("placeholder", "{agent_scratchpad}"),<br/>        ]<br/>    )<br/><br/>    # Combine the context and the question<br/>    input_text = f"Prompt: {prompt.format(input=ques)}\nContext: {context}\nQuestion: {ques}"<br/>    <br/>    # Request to the model<br/>    response = query_llama_via_cli(input_text)<br/>    st.write("PDF: ", response)  # The answer is displayed here</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="42b2" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [USER INPUT PROCESSING]</h2><p id="531b" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The user input is processed and forwarded to the LLM using the <em class="nz">“user_input()”</em> function. Specifically, the entire text of the uploaded PDF file is used as the context. The conversation chain function <em class="nz">“get_conversational_chain”</em> is called to finally answer the user’s question <em class="nz">“user_question”</em> using the context <em class="nz">“context”</em> . In other words, the interaction between user and model is enabled by this function.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="4ec6" class="oj ok gl og b bg ol om l on oo">def user_input(user_question, pdf_text):<br/>    """Processes the user input and calls up the model."""<br/>    # Use the entire text of the PDF as context<br/>    context = pdf_text<br/><br/>    # Configure and request<br/>    get_conversational_chain(context, user_question)</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4e48" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PDF CHAT APP [MAIN SCRIPT]</h2><p id="487b" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">The main logic of the Streamlit web application is defined by the <em class="nz">“main()”</em> function. Specifically, the Streamlit page is set up and the layout is defined. Currently, the layout only consists of the option to upload a PDF file <em class="nz">“st.file_uploader”</em> and an input field <em class="nz">“st.text_input”</em>. The user’s questions are entered in the latter. The user interface enables interaction with the model. If a PDF file is uploaded, it is read <em class="nz">“pdf_text = pdf_read(pdf_doc)”</em>. If there is still a question and the input has been confirmed, the request is processed <em class="nz">“user_input(user_question, pdf_text)”</em>.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="f6ec" class="oj ok gl og b bg ol om l on oo">def main():<br/>    """Main function of the Streamlit application."""<br/>    st.set_page_config(page_title="CHAT WITH YOUR PDF")<br/>    st.header("PDF CHAT APP")<br/><br/>    pdf_text = ""<br/>    pdf_doc = st.file_uploader("Upload your PDF Files and confirm your question", accept_multiple_files=True)<br/><br/>    if pdf_doc:<br/>        pdf_text = pdf_read(pdf_doc)  # Read the entire PDF text<br/><br/>    user_question = st.text_input("Ask a Question from the PDF Files")<br/><br/>    if user_question and pdf_text:<br/>        user_input(user_question, pdf_text)<br/><br/>    # Monitor RAM consumption<br/>    process = psutil.Process(os.getpid())<br/>    memory_usage = process.memory_info().rss / (1024 ** 2)  # Conversion to megabytes<br/>    st.sidebar.write(f"Memory usage: {memory_usage:.2f} MB")<br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="db01" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The app described is started with the following command and looks like this.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="e85e" class="oj ok gl og b bg ol om l on oo">streamlit run pca1.py</span></pre><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qb"><img src="../Images/f88464f892b955054d9be41ba9dffbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NtMMtNFQQ_c-Iff7qfzsvQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Streamlit PDF chat app 1 <em class="gi">Image by author</em></figcaption></figure></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="172e" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">INDIVIDUAL CUSTOMIZATION OPTIONS</h2><p id="d485" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">There are individual customization options in various areas. To improve the quality and relevance of the LLM’s responses, the system prompting or the system message, see Conversation chain, can be adapted. The behavior of the model can be directly controlled by specific instructions and context. You can experiment with different prompts to test how the model’s responses change. The current system message still offers a lot of potential for customization.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="2d18" class="oj ok gl og b bg ol om l on oo">    prompt = ChatPromptTemplate.from_messages(<br/>        [<br/>            (<br/>                "system",<br/>                """You are an intelligent and helpful assistant. Your goal is to provide the most accurate and detailed answers <br/>                possible to any question you receive. Use all available context to enhance your answers, and explain complex <br/>                concepts in a simple manner. If additional information might help, suggest further areas for exploration. If the <br/>                answer is not available in the provided context, state this clearly and offer related insights when possible.""",<br/>            ),</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a21b" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another customization option is to replace the currently used LLM Llama3.1. Various models (e.g. gemma2, mistral, phi3, qwen2 etc.) in different sizes (2B, 8B, 70B etc.) are available on OLLAMA. To be able to use a different model, it must first be downloaded and then the function <em class="nz">“query_llama_via_cli()” </em>in the Python script must be adapted. Specifically, the command <em class="nz">[“ollama”, “run”, “llama3.1”]</em>, which starts the LLM, must be changed.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qc"><img src="../Images/2f06154b207a96a2797fd38a6ce31ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wikq0ZuMkc2rUecjYL5Ayw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Some Ollama models <em class="gi">(Image by author)</em></figcaption></figure><p id="f7f4" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When using other models, it must be ensured that the available computing power is sufficient for local use. You also need to consider whether the model will be executed on GPUs or CPUs. The following example can be used as a rule of thumb to assess whether a model will work on your own computer. A model with 1 billion parameters (1B) requires around 2 to 3 GB RAM (2–3 bytes per parameter). The Task Manager (Windows) can be used to check to what extent the execution of the application affects the performance of the system.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qd"><img src="../Images/409ebb2896dcb3a0f74598467322cb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4bZjEElUS4AzsBIMEBNyw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Windows Task Manager (<em class="gi">Image by author)</em></figcaption></figure><p id="35af" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Alternatively, you can also integrate the application’s memory usage directly into Streamlit. This is done using the <em class="nz">“psutil”</em> library, which must first be installed and then implemented in the Python script.</p><pre class="oa ob oc od oe of og oh bp oi bb bk"><span id="abb2" class="oj ok gl og b bg ol om l on oo">    # Monitor RAM consumption<br/>    process = psutil.Process(os.getpid())<br/>    memory_usage = process.memory_info().rss / (1024 ** 2)  # Conversion to megabytes<br/>    st.sidebar.write(f"Memory usage: {memory_usage:.2f} MB")</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ebab" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is also possible to customize the layout and functionality of the application. For example, the main logic, specifically the <em class="nz">“main()”</em> function, can be extended to allow the simultaneous upload of multiple PDF files <em class="nz">“accept_multiple_files=True”</em>. The files can then be displayed in a list and selected by the user <em class="nz">“selected_pdf_file”</em>. Processing then takes place as usual. Depending on the selected file, the extracted context is forwarded to the LLM together with the user’s question. The customized code can be found in the file <em class="nz">“pca2.py”</em>.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qb"><img src="../Images/b7d2661ffafe3abed3580dd73f52935c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sq4hyH98nALoRk9YtPZEsA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Streamlit PDF chat app 2 <em class="gi">(Image by author)</em></figcaption></figure></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e7e7" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">PCA PYTHON SCRIPT [DOWNLOAD]</h2><p id="9ebb" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">Click on the folder to download the zip file with the two apps!</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><a href="https://drive.google.com/file/d/15x3i1ov7GkOGWbml3LY_Os-DVR8kN98C/view?usp=sharing"><div class="fo fp qe"><img src="../Images/495251495ae1862c340639d9f89f25d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NIVXNC5JWVG1ZJmvAvUPUg.png"/></div></a><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Load the Python scripts by clicking on the folder (<em class="gi">Image by author)</em></figcaption></figure><h2 id="8491" class="oy ok gl bf oz pa pb pc pd pe pf pg ph nm pi pj pk nq pl pm pn nu po pp pq pr bk">CONCLUSION</h2><p id="c643" class="pw-post-body-paragraph nd ne gl nf b hj ps nh ni hm pt nk nl nm pu no np nq pv ns nt nu pw nw nx ny fj bk">This article showed how Python in combination with tools such as Streamlit, FAISS, Spacy, CLI, OLLAMA and the LLM Llama3.1 can be used to create a web application that allows users to extract text from PDF files locally, save it in the form of embeddings and ask questions about the content of the file using an AI model. By further optimizing the scripts (e.g. prompt), using other models and adapting the layout to include additional functionalities, the app can offer added value in everyday life without incurring additional costs. Have fun customizing and using the app.</p><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qf"><img src="../Images/6986252b10d7723e67c0529d265a19a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j6DnTo7-FNuMgfQOD6HrGA.gif"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">You can clap up to 50 times!</figcaption></figure><figure class="oa ob oc od oe fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qg"><img src="../Images/127ee9bba2f9b657ec9f4cb2b3c1f6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8MOBqUQNhqh7WFrxh9Dl-w.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Just for the thumbnail (Image by author)</figcaption></figure></div></div></div></div>    
</body>
</html>