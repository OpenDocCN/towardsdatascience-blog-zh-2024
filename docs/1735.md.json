["```py\n# Install the package\npip install datasetsforecast\n# Load Data\ndf, *_ = M4.load('./data', group='Weekly')\n# Randomly select three items\ndf = df[df['unique_id'].isin(['W96', 'W100', 'W99'])]\n# Define the start date (for example, \"1970-01-04\")\nstart_date = pd.to_datetime(\"1970-01-04\")\n# Convert 'ds' to actual week dates\ndf['ds'] = start_date + pd.to_timedelta(df['ds'] - 1, unit='W')\n# Display the DataFrame\ndf.head()\n```", "```py\n# Install the package\npip install tensorflow\n\n# Load the package\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, concatenate, Layer\n\n# Define Global Parameters\ninput_shape = (104, 1)\nquantiles = [0.1, 0.9]\noutput_steps = 12\ncut_off_date = '2013-10-13'\ntf.random.set_seed(20240710)\n```", "```py\n# Preprocess The Data\ndef preprocess_data(df, window_size = 104, forecast_horizon = 12):\n    # Ensure the dataframe is sorted by item and date\n\n    df = df.sort_values(by=['unique_id', 'ds'])\n    # List to hold processed data for each item\n    X, y, unique_id, ds = [], [], [], []\n    # Normalizer\n    scaler = StandardScaler()\n    # Iterate through each item\n    for key, group in df.groupby('unique_id'):\n        demand = group['y'].values.reshape(-1, 1)\n        scaled_demand = scaler.fit_transform(demand)\n        dates = group['ds'].values\n        # Create sequences (sliding window approach)    \n        for i in range(len(scaled_demand) - window_size - forecast_horizon + 1):\n            X.append(scaled_demand[i:i+window_size])\n            y.append(scaled_demand[i+window_size:i+window_size+forecast_horizon].flatten())\n            unique_id.append(key)\n            ds.append(dates[i+window_size:i+window_size+forecast_horizon])\n    X = np.array(X)\n    y = np.array(y)\n    return X, y, unique_id, ds, scaler\n```", "```py\n# Split Data\ndef split_data(X, y, unique_id, ds, cut_off_date):\n    cut_off_date = pd.to_datetime(cut_off_date)\n    val_start_date = cut_off_date - pd.Timedelta(weeks=12)\n    train_idx = [i for i, date in enumerate(ds) if date[0] < val_start_date]\n    val_idx = [i for i, date in enumerate(ds) if val_start_date <= date[0] < cut_off_date]\n    test_idx = [i for i, date in enumerate(ds) if date[0] >= cut_off_date]\n\n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n    X_test, y_test = X[test_idx], y[test_idx]\n\n    train_unique_id = [unique_id[i] for i in train_idx]\n    train_ds = [ds[i] for i in train_idx]\n    val_unique_id = [unique_id[i] for i in val_idx]\n    val_ds = [ds[i] for i in val_idx]\n    test_unique_id = [unique_id[i] for i in test_idx]\n    test_ds = [ds[i] for i in test_idx]\n\n    return X_train, y_train, X_val, y_val, X_test, y_test, train_unique_id, train_ds, val_unique_id, val_ds, test_unique_id, test_ds\n```", "```py\n# Attention Layer\nclass Attention(Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = Dense(units)\n        self.W2 = Dense(units)\n        self.V = Dense(1)\n    def call(self, query, values):\n        hidden_with_time_axis = tf.expand_dims(query, 1)\n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n        attention_weights = tf.nn.softmax(score, axis=1)\n        context_vector = attention_weights * values\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        return context_vector, attention_weights\n\n# Quantile Loss Function\ndef quantile_loss(q, y_true, y_pred):\n    e = y_true - y_pred\n    return tf.reduce_mean(tf.maximum(q*e, (q-1)*e))\n\ndef combined_quantile_loss(quantiles, y_true, y_pred, output_steps):\n    losses = [quantile_loss(q, y_true, y_pred[:, i*output_steps:(i+1)*output_steps]) for i, q in enumerate(quantiles)]\n    return tf.reduce_mean(losses)\n\n# Model architecture\ndef create_model(input_shape, quantiles, output_steps):\n    inputs = Input(shape=input_shape)\n    lstm1 = LSTM(256, return_sequences=True)(inputs)\n    lstm_out, state_h, state_c = LSTM(256, return_sequences=True, return_state=True)(lstm1)\n    context_vector, attention_weights = Attention(256)(state_h, lstm_out)\n    global_context = Dense(100, activation = 'relu')(context_vector)\n    forecasts = []\n    for q in quantiles:\n        local_context = concatenate([global_context, context_vector])\n        forecast = Dense(output_steps, activation = 'linear')(local_context)\n        forecasts.append(forecast)\n    outputs = concatenate(forecasts, axis=1)\n    model = Model(inputs, outputs)\n    model.compile(optimizer='adam', loss=lambda y, f: combined_quantile_loss(quantiles, y, f, output_steps))\n    return model\n```", "```py\n# Install the package\npip install neuralforecast\n\n# Load the package\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import PatchTST\nfrom neuralforecast.losses.pytorch import HuberMQLoss, MQLoss\n\n# Define Parameters for PatchTST\nPARAMS = {'input_size': 104, \n          'h': output_steps, \n          'max_steps': 6000, \n          'encoder_layers': 4,\n          'start_padding_enabled': False,\n          'learning_rate': 1e-4,\n          'patch_len': 52,  # Length of each patch\n          'hidden_size': 256,  # Size of the hidden layers\n          'n_heads': 4,  # Number of attention heads\n          'res_attention': True,\n          'dropout': 0.1,  # Dropout rate\n          'activation': 'gelu',  # Activation function\n          'dropout': 0.1,\n          'attn_dropout': 0.1,\n          'fc_dropout': 0.1,\n          'random_seed': 20240710,\n          'loss': HuberMQLoss(quantiles=[0.1, 0.5, 0.9]),\n          'scaler_type': 'standard',\n          'early_stop_patience_steps': 10}\n\n# Get Training Data\ntrain_df = df[df.ds<cut_off_date] \n\n# Fit and predict with PatchTST\nmodels = [PatchTST(**PARAMS)]\nnf = NeuralForecast(models=models, freq='W') \nnf.fit(df=train_df, val_size=12)\nY_hat_df = nf.predict().reset_index()\n```", "```py\n# Get Training Data and Transform\ntrain_df = df[df.ds<cut_off_date] \ntrain_df_chronos = TimeSeriesDataFrame(train_df.rename(columns={'ds': 'timestamp', 'unique_id': 'item_id', 'y': 'target'}))\n\n# Zero-shot forecast with Chronos\npredictor = TimeSeriesPredictor(prediction_length=output_steps, freq='W', quantile_levels = [0.1, 0.9]).fit(\n            train_df_chronos, presets=\"chronos_base\", \n            random_seed = 20240710\n        )\nY_hat_df_chronos = predictor.predict(train_df_chronos).reset_index().rename(columns={'mean': 'Chronos', \n                                                                                     '0.1': 'P10', \n                                                                                     '0.9': 'P90',\n                                                                                     'timestamp': 'ds',\n                                                                                     'item_id': 'unique_id'})\n```"]