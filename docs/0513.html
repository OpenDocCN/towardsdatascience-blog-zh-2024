<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Designing and Deploying a Machine Learning Python Application (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Designing and Deploying a Machine Learning Python Application (Part 2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24">https://towardsdatascience.com/designing-and-deploying-a-machine-learning-python-application-part-2-99eb37787b2b?source=collection_archive---------4-----------------------#2024-02-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4c66" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">You don’t have to be Atlas to get your model into the cloud</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Noah Haglund" class="l ep by dd de cx" src="../Images/edfcc90677444ebced16549a1524d7fe.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*t5ATfQk3UNrxNvS_hf0IUg.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@noahhaglund?source=post_page---byline--99eb37787b2b--------------------------------" rel="noopener follow">Noah Haglund</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--99eb37787b2b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ea381d092591530df711ecba890d8217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtHeRFfKCK1__XKR-7MKQw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Midjourney</figcaption></figure><p id="0ebe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we have our trained Detectron2 model (<a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">see Part 1</a>), let’s deploy it as a part of an application to provide its inferencing abilities to others.</p><p id="8a8d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even though Part 1 and 2 of this series use Detectron2 for Object Detection, no matter the machine learning library you are using (<em class="nz">Detectron, Yolo, PyTorch, Tensorflow, etc</em>) and no matter your use case (<em class="nz">Computer Vision, Natural Language Processing, Deep Learning, etc</em>), various topics discussed here concerning model deployment will be useful for all those developing ML processes.</p><p id="70f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although the fields of Data Science and Computer Science overlap in many ways, training and deploying an ML model combines the two, as those concerned with developing an efficient and accurate model are not typically the ones trying to deploy it and vice versa. On the other hand, someone more CS oriented may not have the understanding of ML or its associated libraries to determine whether application bottlenecks could be fixed with configurations to the ML process or rather the backend and hosting service/s.</p><p id="49bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to aid you in your quest to deploy an application that utilizes ML, this article will begin by discussing: (1) high level CS design concepts that can help DS folks makes decisions in order to balance load and mitigate bottlenecks and (2) low level design by walking through deploying a Detectron2 inferencing process using the Python web framework Django, an API using Django Rest Framework, the distributed task queue Celery, Docker, Heroku, and AWS S3.</p><p id="7cc4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For following along with this article, it will be helpful to have in advance:</p><ul class=""><li id="c267" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Strong Python Knowledge</li><li id="8c22" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Understanding of Django, Django Rest Framework, Docker, Celery, and AWS</li><li id="637e" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Familiarity with Heroku</li></ul></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6194" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">High Level Design</h1><p id="d309" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">In order to dig into the high level design, let’s discuss a couple key problems and potential solutions.</p><h2 id="d507" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">Problem 1: Memory</h2><p id="262c" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">The saved ML model from <a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">Part 1</a>, titled model_final.pth, will start off at ~325MB. Additionally, an application based on (1) a Python runtime, (2) Detectron2, (3) large dependencies such as Torch, and (4) a Django web framework will utilize ~150MB of memory on deployment.</p><blockquote class="qi qj qk"><p id="3e83" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So at minimum, we are looking at ~475MB of memory utilized right off the bat.</p></blockquote><p id="0f45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We could load the Detectron2 model only when the ML process needs to run, but this would still mean that our application would eat up ~475MB eventually. If you have a tight budget and are unable to vertically scale your application, memory now becomes a substantial limitation on many hosting platforms. For example, Heroku offers containers to run applications, termed “dynos”, that started with 512MB RAM for base payment plans, will begin writing to disk beyond the 512MB threshold, and will crash and restart the dyno at 250% utilization (1280MB).</p><p id="5798" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">On the topic of memory, Detectron2 inferencing will cause spikes in memory usage depending on the amount of objects detected in an image, so it is important to ensure memory is available during this process.</p><p id="e0d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For those of you trying to speed up inferencing, but are cautious of memory constraints, batch inferencing will be of no help here either. <a class="af ny" href="https://github.com/facebookresearch/detectron2/issues/1539" rel="noopener ugc nofollow" target="_blank">As noted by one of the contributors to the Detectron2 repo</a>, with batch inferencing:</p><blockquote class="qi qj qk"><p id="2523" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">N images use N times more memory than 1 image…You can predict on N images one by one in a loop instead.</p></blockquote><p id="8759" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Overall, this summarizes <strong class="ne fr">problem #1</strong>:</p><blockquote class="qi qj qk"><p id="6f67" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">running a long ML processes as a part of an application will most likely be memory intensive, due to the size of the model, ML dependencies, and inferencing process.</p></blockquote><h2 id="9704" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">Problem 2: Time</h2><p id="f6cb" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">A deployed application that incorporates ML will likely need to be designed to manage a long-running process.</p><p id="8e6f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using the example of an application that uses Detectron2, the model would be sent an image as input and output inference coordinates. With one image, inference may only take a few seconds, but say for instance we are processing a long PDF document with one image per page (as per the training data in <a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">Part 1</a>), this could take a while.</p><p id="057d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During this process, Detectron2 inferencing would be either CPU or GPU bound, depending on your configurations. See the below Python code block to change this (CPU is entirely fine for inferencing, however, GPU/Cuda is necessary for training as mentioned in <a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">Part 1</a>):</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="1d32" class="qp or fq qm b bg qq qr l qs qt">from detectron2.config import get_cfg<br/>cfg = get_cfg()<br/>cfg.MODEL.DEVICE = "cpu" #or "cuda"</span></pre><p id="e1dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, saving images after inferencing, say to AWS S3 for example, would introduce I/O bound processes. Altogether, this could serve to clog up the backend, which introduces <strong class="ne fr">problem #2</strong>:</p><blockquote class="qi qj qk"><p id="5601" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">single-threaded Python applications will not process additional HTTP requests, concurrently or otherwise, while running a process.</p></blockquote><h2 id="20a0" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">Problem 3: Scale</h2><p id="8df2" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">When considering the horizontal scalability of a Python application, it is important to note that Python (assuming it is compiled/interpreted by CPython) suffers from the limitations of the Global Interpreter Lock (GIL), which <a class="af ny" href="https://realpython.com/python-gil/" rel="noopener ugc nofollow" target="_blank">allows only one thread to hold the control of the Python interpreter</a>. Thus, the paradigm of multithreading doesn’t correctly apply to Python, as applications can still implement multithreading, using web servers such as Gunicorn, but will do so concurrently, meaning that the threads aren’t running in parallel.</p><p id="3956" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I know all of this sounds fairly abstract, perhaps especially for the Data Science folks, so let me provide an example to illustrate this problem.</p><p id="382f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You are your application and right now your hardware, brain, is processing two requests, cleaning the counter and texting on your phone. With two arms to do this, you are now a multithreaded Python application, doing both simultaneously. But you’re not actually thinking about both at the same <em class="nz">exact</em> time, you start your hand in a cleaning motion, then switch your attention to your phone to look at what you are typing, then look back at the counter to make sure you didn’t miss a spot.</p><blockquote class="qi qj qk"><p id="b8a3" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In actuality, you are processing these tasks concurrently.</p></blockquote><p id="b26e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The GIL functions in the same way, processing one thread at a time but switching between them for concurrency. This means that multithreading a Python application is still useful for running background or I/O bound-oriented tasks, such as downloading a file, while the main execution’s thread is still running. To take the analogy this far, your background task of cleaning the counter (i.e. downloading a file) continues to happen while you are thinking about texting, but you still need to change your focus back to your cleaning hand in order to process the next step.</p><p id="ec9e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This “change in focus” may not seem like a big deal when concurrently processing multiple requests, but when you need to handle hundreds of requests simultaneously, suddenly this becomes a limiting factor for large scale applications that need to be adequately responsive to end users.</p><p id="994d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thus, we have problem #3:</p><blockquote class="qi qj qk"><p id="029b" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">the GIL prevents multithreading from being a good scalability solution for Python applications.</p></blockquote><h2 id="6076" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">Solutions</h2><p id="b732" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">Now that we have identified key problems, let’s discuss a few potential solutions.</p><p id="b17d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The aforementioned problems are ordered in terms of importance, as we need to manage memory first and foremost (problem #1) to ensure the application doesn’t crash, then leave room for the app to process more than one request at a time (problem #2) while still ensuring our means of simultaneous request handling is effective at scale (problem #3).</p><p id="a6a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, let’s jump right into addressing problem #1.</p><p id="40c1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Depending on the hosting platform, we will need to be fully aware of the configurations available in order to scale. As we will be using Heroku, feel free to check out the guidance on <a class="af ny" href="https://devcenter.heroku.com/articles/scaling" rel="noopener ugc nofollow" target="_blank">dyno scaling</a>. Without having to vertically scale up your dyno, <em class="nz">we can scale out by adding another </em><a class="af ny" href="https://devcenter.heroku.com/articles/process-model" rel="noopener ugc nofollow" target="_blank"><em class="nz">process</em></a>. For instance, with the <a class="af ny" href="https://devcenter.heroku.com/articles/dyno-types" rel="noopener ugc nofollow" target="_blank">Basic dyno type</a>, a developer is able to deploy both a web process and a worker process on the same dyno. A few reasons this is useful:</p><ul class=""><li id="f4ce" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">This enables a means of multiprocessing.</li><li id="4611" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">The dyno resources are now duplicated, meaning each process has a 512MB RAM threshold.</li><li id="5ce8" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">Cost wise, we are looking at $7 per month per process (so $14 a month with both a web and worker process). Much cheaper than vertically scaling the dyno to get more RAM, with $50 a month per dyno if you want to increase the 512MB allocation to 1024MB.</li></ul><p id="f885" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hopping back to the previous analogy of cleaning the counter and texting on your phone, instead of threading yourself further by adding additional arms to your body, we can now have two people (multiprocessing in parallel) to perform the separate tasks. We are scaling out by increasing workload diversity as opposed to scaling up, in turn saving us money.</p><p id="006c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Okay, but with two separate processes, what’s the difference?</p><p id="6277" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using Django, our web process will be initialized with:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="9a9a" class="qp or fq qm b bg qq qr l qs qt">python manage.py runserver</span></pre><p id="e705" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And using a distributed task queue, such as Celery, the worker will be initialized with:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="b624" class="qp or fq qm b bg qq qr l qs qt">celery -A &lt;DJANGO_APP_NAME_HERE&gt; worker</span></pre><p id="bb4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://devcenter.heroku.com/articles/process-model#mapping-the-unix-process-model-to-web-apps" rel="noopener ugc nofollow" target="_blank">As intended by Heroku</a>, the web process is the server for our core web framework and the worker process is intended for queuing libraries, cron jobs, or other work performed in the background. Both represent an instance of the deployed application, so will be running at ~150MB given the core dependencies and runtime. However, we can ensure that the worker is the only process that runs the ML tasks, saving the web process from using ~325MB+ in RAM. This has multiple benefits:</p><ul class=""><li id="0f25" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oa ob oc bk">Memory usage, although still high for the worker, will be distributed to a node outside of the system, ensuring any problems encountered during the execution of an ML task can be handled and monitored separately from the web process. This helps to mitigate problem #1.</li><li id="d8b8" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">The newly found means of parallelism ensures that the web process can still respond to requests during a long-running ML task, helping to address problem #2.</li><li id="42db" class="nc nd fq ne b go od ng nh gr oe nj nk nl of nn no np og nr ns nt oh nv nw nx oa ob oc bk">We are preparing for scale by implementing a means of multiprocessing, helping to address problem #3.</li></ul></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2486" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we haven’t quite solved the key problems, let’s dig in just a bit further before getting into the low-level nitty-gritty. <a class="af ny" href="https://devcenter.heroku.com/articles/python-gunicorn" rel="noopener ugc nofollow" target="_blank">As stated by Heroku</a>:</p><blockquote class="qi qj qk"><p id="ae98" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Web applications that process incoming HTTP requests concurrently make much more efficient use of dyno resources than web applications that only process one request at a time. Because of this, we recommend using web servers that support concurrent request processing whenever developing and running production services.</p><p id="22cd" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The Django and Flask web frameworks feature convenient built-in web servers, but these blocking servers only process a single request at a time. If you deploy with one of these servers on Heroku, your dyno resources will be underutilized and your application will feel unresponsive.</p></blockquote><p id="7610" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are already ahead of the game by utilizing worker multiprocessing for the ML task, but can take this a step further by using Gunicorn:</p><blockquote class="qi qj qk"><p id="c340" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://gunicorn.org/" rel="noopener ugc nofollow" target="_blank">Gunicorn</a> is a pure-Python HTTP server for WSGI applications. It allows you to run any Python application concurrently by running multiple Python processes within a single dyno. It provides a perfect balance of performance, flexibility, and configuration simplicity.</p></blockquote><p id="9514" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Okay, awesome, now we can utilize even more processes, but there’s a catch: each new worker Gunicorn worker process will represent a copy of the application, meaning that they too will utilize the base ~150MB RAM <em class="nz">in addition </em>to the Heroku process. So, say we pip install gunicorn and now initialize the Heroku web process with the following command:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="baf6" class="qp or fq qm b bg qq qr l qs qt">gunicorn &lt;DJANGO_APP_NAME_HERE&gt;.wsgi:application --workers=2 --bind=0.0.0.0:$PORT</span></pre><p id="7020" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The base ~150MB RAM in the web process turns into ~300MB RAM (base memory usage multipled by # gunicorn workers).</p><p id="b35f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While being cautious of the limitations to multithreading a Python application, we can add threads to workers as well using:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="a96f" class="qp or fq qm b bg qq qr l qs qt">gunicorn &lt;DJANGO_APP_NAME_HERE&gt;.wsgi:application --threads=2 --worker-class=gthread --bind=0.0.0.0:$PORT</span></pre><p id="b5e9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even with problem #3, we can still find a use for threads, as we want to ensure our web process is capable of processing more than one request at a time while being careful of the application’s memory footprint. Here, our threads could process miniscule requests while ensuring the ML task is distributed elsewhere.</p><p id="722a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Either way, by utilizing gunicorn workers, threads, or both, we are setting our Python application up to process more than one request at a time. We’ve more or less solved problem #2 by incorporating various ways to implement concurrency and/or parallel task handling while ensuring our application’s critical ML task doesn’t rely on potential pitfalls, such as multithreading, setting us up for scale and getting to the root of problem #3.</p><p id="65e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Okay so what about that tricky problem #1. At the end of the day, ML processes will typically end up taxing the hardware in one way or another, whether that would be memory, CPU, and/or GPU. <em class="nz">However</em>, by using a distributed system, our ML task is integrally linked to the main web process yet handled in parallel via a Celery worker. We can track the start and end of the ML task via the chosen Celery <a class="af ny" href="https://docs.celeryq.dev/en/3.1/getting-started/brokers/index.html" rel="noopener ugc nofollow" target="_blank">broker</a>, as well as review metrics in a more isolated manner. Here, curtailing Celery and Heroku worker process configurations are up to you, but it is an excellent starting point for integrating a long-running, memory-intensive ML process into your application.</p><h1 id="a27b" class="oq or fq bf os ot qu gq ov ow qv gt oy oz qw pb pc pd qx pf pg ph qy pj pk pl bk">Low Level Design and Setup</h1><p id="ce15" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">Now that we’ve had a chance to really dig in and get a high level picture of the system we are building, let’s put it together and focus on the specifics.</p><p id="a656" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For your convenience, <a class="af ny" href="https://github.com/nzh2534/mltutorial/tree/main" rel="noopener ugc nofollow" target="_blank">here is the repo</a> I will be mentioning in this section.</p><p id="93c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First we will begin by setting up Django and Django Rest Framework, with installation guides <a class="af ny" href="https://docs.djangoproject.com/en/5.0/intro/install/" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af ny" href="https://www.django-rest-framework.org/#installation" rel="noopener ugc nofollow" target="_blank">here</a> respectively. All requirements for this app can be found in the repo’s requirements.txt file (and Detectron2 and Torch will be built from Python wheels specified in the Dockerfile, in order to keep the Docker image size small).</p><p id="9734" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next part will be setting up the Django app, configuring the backend to save to AWS S3, and exposing an endpoint using DRF, so if you are already comfortable doing this, feel free to skip ahead and go straight to the <em class="nz">ML Task Setup and Deployment </em>section.</p><h2 id="c3ac" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">Django Setup</h2><p id="6a91" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">Go ahead and create a folder for the Django project and cd into it. Activate the virtual/conda env you are using, ensure Detectron2 is installed as per the installation instructions in <a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">Part 1</a>, and install the requirements as well.</p><p id="eb6c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Issue the following command in a terminal:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="ea1a" class="qp or fq qm b bg qq qr l qs qt">django-admin startproject mltutorial</span></pre><p id="cb36" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This will create a Django project root directory titled “mltutorial”. Go ahead and cd into it to find a manage.py file and a mltutorial sub directory (which is the actual Python package for your project).</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="0125" class="qp or fq qm b bg qq qr l qs qt">mltutorial/<br/>    manage.py<br/>    mltutorial/<br/>        __init__.py<br/>        settings.py<br/>        urls.py<br/>        asgi.py<br/>        wsgi.py</span></pre><p id="c5b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Open settings.py and add ‘rest_framework’, ‘celery’, and ‘storages’ (needed for boto3/AWS) in the INSTALLED_APPS list to register those packages with the Django project.</p><p id="c929" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the root dir, let’s create an app which will house the core functionality of our backend. Issue another terminal command:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="65af" class="qp or fq qm b bg qq qr l qs qt">python manage.py startapp docreader</span></pre><p id="3fc7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This will create an app in the root dir called docreader.</p><p id="d781" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s also create a file in docreader titled mltask.py. In it, define a simple function for testing our setup that takes in a variable, file_path, and prints it out:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="02a7" class="qp or fq qm b bg qq qr l qs qt">def mltask(file_path):<br/>  return print(file_path)</span></pre><p id="a6db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now getting to structure, Django apps use the <a class="af ny" href="https://www.geeksforgeeks.org/mvc-design-pattern/" rel="noopener ugc nofollow" target="_blank">Model View Controller</a> (MVC) design pattern, defining the Model in <a class="af ny" href="https://docs.djangoproject.com/en/5.0/topics/db/models/" rel="noopener ugc nofollow" target="_blank">models.py</a>, View in <a class="af ny" href="https://docs.djangoproject.com/en/5.0/topics/http/views/" rel="noopener ugc nofollow" target="_blank">views.py</a>, and Controller in Django <a class="af ny" href="https://docs.djangoproject.com/en/5.0/topics/templates/" rel="noopener ugc nofollow" target="_blank">Templates</a> and <a class="af ny" href="https://docs.djangoproject.com/en/5.0/topics/http/urls/" rel="noopener ugc nofollow" target="_blank">urls.py</a>. Using Django Rest Framework, we will include serialization in this pipeline, which provide a way of serializing and deserializing native Python data structures into representations such as json. Thus, the application logic for exposing an endpoint is as follows:</p><blockquote class="qi qj qk"><p id="2746" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Database ← → models.py ← → serializers.py ← → views.py ← → urls.py</p></blockquote><p id="38f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In docreader/models.py, write the following:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="336a" class="qp or fq qm b bg qq qr l qs qt">from django.db import models<br/>from django.dispatch import receiver<br/>from .mltask import mltask<br/>from django.db.models.signals import(<br/>    post_save<br/>)<br/><br/>class Document(models.Model):<br/>    title = models.CharField(max_length=200)<br/>    file = models.FileField(blank=False, null=False)<br/><br/>@receiver(post_save, sender=Document)<br/>def user_created_handler(sender, instance, *args, **kwargs):<br/>    mltask(str(instance.file.file))</span></pre><p id="566b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This sets up a model Document that will require a title and file for each entry saved in the database. Once saved, the @receiver decorator listens for a post save signal, meaning that the specified model, Document, was saved in the database. Once saved, user_created_handler() takes the saved instance’s file field and passes it to, what will become, our Machine Learning function.</p><p id="124c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Anytime changes are made to models.py, you will need to run the following two commands:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="5cd9" class="qp or fq qm b bg qq qr l qs qt">python manage.py makemigrations<br/>python manage.py migrate</span></pre><p id="c192" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Moving forward, create a serializers.py file in docreader, allowing for the serialization and deserialization of the Document’s title and file fields. Write in it:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="a2bd" class="qp or fq qm b bg qq qr l qs qt">from rest_framework import serializers<br/>from .models import Document<br/><br/>class DocumentSerializer(serializers.ModelSerializer):<br/>    class Meta:<br/>        model = Document<br/>        fields = [<br/>            'title',<br/>            'file'<br/>        ]</span></pre><p id="a7c0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next in views.py, where we can define our CRUD operations, let’s define the ability to create, as well as list, Document entries using <a class="af ny" href="https://www.django-rest-framework.org/api-guide/generic-views/" rel="noopener ugc nofollow" target="_blank">generic views</a> (which essentially allows you to quickly write views using an abstraction of common view patterns):</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="251b" class="qp or fq qm b bg qq qr l qs qt">from django.shortcuts import render<br/>from rest_framework import generics<br/>from .models import Document<br/>from .serializers import DocumentSerializer<br/><br/>class DocumentListCreateAPIView(<br/>    generics.ListCreateAPIView):<br/><br/>    queryset = Document.objects.all()<br/>    serializer_class = DocumentSerializer</span></pre><p id="ecc0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, update urls.py in mltutorial:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="0faa" class="qp or fq qm b bg qq qr l qs qt">from django.contrib import admin<br/>from django.urls import path, include<br/><br/>urlpatterns = [<br/>    path("admin/", admin.site.urls),<br/>    path('api/', include('docreader.urls')),<br/>]</span></pre><p id="8237" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And create urls.py in docreader app dir and write:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="c0c8" class="qp or fq qm b bg qq qr l qs qt">from django.urls import path<br/><br/>from . import views<br/><br/>urlpatterns = [<br/>    path('create/', views.DocumentListCreateAPIView.as_view(), name='document-list'),<br/>]</span></pre><p id="ccd5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we are all setup to save a Document entry, with title and field fields, at the /api/create/ endpoint, which will call mltask() post save! So, let’s test this out.</p><p id="26cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To help visualize testing, let’s register our Document model with the Django <a class="af ny" href="https://docs.djangoproject.com/en/5.0/ref/contrib/admin/" rel="noopener ugc nofollow" target="_blank">admin interface</a>, so we can see when a new entry has been created.</p><p id="e623" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In docreader/admin.py write:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="af66" class="qp or fq qm b bg qq qr l qs qt">from django.contrib import admin<br/>from .models import Document<br/><br/>admin.site.register(Document)</span></pre><p id="8af9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Create a user that can login to the Django admin interface using:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="730a" class="qp or fq qm b bg qq qr l qs qt">python manage.py createsuperuser</span></pre><p id="869b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s test the endpoint we exposed.</p><p id="4217" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To do this without a frontend, run the Django server and go to Postman. Send the following POST request with a PDF file attached:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/b9a6e26e977b4778e759baa5aca0f5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yAHBJbu1JGFLVuoara1WfQ.png"/></div></div></figure><p id="088e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we check our Django logs, we should see the file path printed out, as specified in the post save mltask() function call.</p><h2 id="6423" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">AWS Setup</h2><p id="7394" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">You will notice that the PDF was saved to the project’s root dir. Let’s ensure any media is instead saved to AWS S3, getting our app ready for deployment.</p><p id="7d01" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Go to the <a class="af ny" href="https://s3.console.aws.amazon.com/" rel="noopener ugc nofollow" target="_blank">S3 console</a> (and create an account and get our your account’s <a class="af ny" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html" rel="noopener ugc nofollow" target="_blank">Access and Secret keys</a> if you haven’t already). Create a new bucket, here we will be titling it ‘djangomltest’. Update the permissions to ensure the bucket is public for testing (and revert back, as needed, for production).</p><p id="e440" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s configure Django to work with AWS.</p><p id="b1df" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Add your model_final.pth, trained in <a class="af ny" href="https://medium.com/towards-data-science/training-and-deploying-a-custom-detectron2-model-for-object-detection-using-pdf-documents-part-1-c724f61d8b4b" rel="noopener">Part 1</a>, into the docreader dir. Create a .env file in the root dir and write the following:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="d538" class="qp or fq qm b bg qq qr l qs qt">AWS_ACCESS_KEY_ID = &lt;Add your Access Key Here&gt;<br/>AWS_SECRET_ACCESS_KEY = &lt;Add your Secret Key Here&gt;<br/>AWS_STORAGE_BUCKET_NAME = 'djangomltest'<br/><br/>MODEL_PATH = './docreader/model_final.pth'</span></pre><p id="0aeb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Update settings.py to include AWS configurations:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="e6c2" class="qp or fq qm b bg qq qr l qs qt">import os<br/>from dotenv import load_dotenv, find_dotenv<br/>load_dotenv(find_dotenv())<br/><br/># AWS<br/>AWS_ACCESS_KEY_ID = os.environ['AWS_ACCESS_KEY_ID']<br/>AWS_SECRET_ACCESS_KEY = os.environ['AWS_SECRET_ACCESS_KEY']<br/>AWS_STORAGE_BUCKET_NAME = os.environ['AWS_STORAGE_BUCKET_NAME']<br/><br/>#AWS Config<br/>AWS_DEFAULT_ACL = 'public-read'<br/>AWS_S3_CUSTOM_DOMAIN = f'{AWS_STORAGE_BUCKET_NAME}.s3.amazonaws.com'<br/>AWS_S3_OBJECT_PARAMETERS = {'CacheControl': 'max-age=86400'}<br/><br/>#Boto3<br/>STATICFILES_STORAGE = 'mltutorial.storage_backends.StaticStorage'<br/>DEFAULT_FILE_STORAGE = 'mltutorial.storage_backends.PublicMediaStorage'<br/><br/>#AWS URLs<br/>STATIC_URL = f'https://{AWS_S3_CUSTOM_DOMAIN}/static/'<br/>MEDIA_URL = f'https://{AWS_S3_CUSTOM_DOMAIN}/media/'</span></pre><p id="ba69" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Optionally, with AWS serving our static and media files, you will want to run the following command in order to serve static assets to the admin interface using S3:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="46aa" class="qp or fq qm b bg qq qr l qs qt">python manage.py collectstatic</span></pre><p id="576c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we run the server again, our admin should appear the same as how it would with our static files served locally.</p><p id="d867" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once again, let’s run the Django server and test the endpoint to make sure the file is now saved to S3.</p><h2 id="4e98" class="pr or fq bf os ps pt pu ov pv pw px oy nl py pz qa np qb qc qd nt qe qf qg qh bk">ML Task Setup and Deployment</h2><p id="6363" class="pw-post-body-paragraph nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx fj bk">With Django and AWS properly configured, let’s set up our ML process in mltask.py. As the file is long, see the repo <a class="af ny" href="https://github.com/nzh2534/mltutorial/blob/main/docreader/mltask.py" rel="noopener ugc nofollow" target="_blank">here</a> for reference (with comments added in to help with understanding the various code blocks).</p><p id="33a8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What’s important to see is that Detectron2 is imported and the model is loaded only when the function is called. Here, we will call the function only through a Celery task, ensuring the memory used during inferencing will be isolated to the Heroku worker process.</p><p id="ddbb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So finally, let’s setup Celery and then deploy to Heroku.</p><p id="be1b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In mltutorial/_init__.py write:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="8c86" class="qp or fq qm b bg qq qr l qs qt">from .celery import app as celery_app<br/>__all__ = ('celery_app',)</span></pre><p id="2e13" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Create celery.py in the mltutorial dir and write:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="e87d" class="qp or fq qm b bg qq qr l qs qt">import os<br/><br/>from celery import Celery<br/><br/># Set the default Django settings module for the 'celery' program.<br/>os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'mltutorial.settings')<br/><br/># We will specify Broker_URL on Heroku<br/>app = Celery('mltutorial', broker=os.environ['CLOUDAMQP_URL'])<br/><br/># Using a string here means the worker doesn't have to serialize<br/># the configuration object to child processes.<br/># - namespace='CELERY' means all celery-related configuration keys<br/>#   should have a `CELERY_` prefix.<br/>app.config_from_object('django.conf:settings', namespace='CELERY')<br/><br/># Load task modules from all registered Django apps.<br/>app.autodiscover_tasks()<br/><br/>@app.task(bind=True, ignore_result=True)<br/>def debug_task(self):<br/>    print(f'Request: {self.request!r}')</span></pre><p id="8830" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Lastly, make a tasks.py in docreader and write:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="88fc" class="qp or fq qm b bg qq qr l qs qt">from celery import shared_task<br/>from .mltask import mltask<br/><br/>@shared_task<br/>def ml_celery_task(file_path):<br/>    mltask(file_path)<br/>    return "DONE"</span></pre><p id="d2a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This Celery task, ml_celery_task(), should now be imported into models.py and used with the post save signal instead of the mltask function pulled directly from mltask.py. Update the post_save signal block to the following:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="8ea5" class="qp or fq qm b bg qq qr l qs qt">@receiver(post_save, sender=Document)<br/>def user_created_handler(sender, instance, *args, **kwargs):<br/>    ml_celery_task.delay(str(instance.file.file))</span></pre><p id="e9a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And to test Celery, let’s deploy!</p><p id="8eae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the root project dir, include a Dockerfile and heroku.yml file, both specified in the <a class="af ny" href="https://github.com/nzh2534/mltutorial/tree/main" rel="noopener ugc nofollow" target="_blank">repo</a>. Most importantly, editing the heroku.yml <em class="nz">commands</em> will allow you to configure the gunicorn web process and the Celery worker process, which can aid in further mitigating potential problems.</p><p id="2b31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Make a Heroku account and create a new app called “mlapp” and gitignore the .env file. Then initialize git in the projects root dir and change the Heroku app’s stack to container (in order to deploy using Docker):</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="64ed" class="qp or fq qm b bg qq qr l qs qt">$ heroku login<br/>$ git init<br/>$ heroku git:remote -a mlapp<br/>$ git add .<br/>$ git commit -m "initial heroku commit"<br/>$ heroku stack:set container<br/>$ git push heroku master</span></pre><p id="f9f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once pushed, we just need to add our env variables into the Heroku app.</p><p id="250e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Go to settings in the online interface, scroll down to Config Vars, click Reveal Config Vars, and add each line listed in the .env file.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ra"><img src="../Images/0cb721c6c2ac8c4e9fbadf33d1aa882f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*uFIEXLba7aGujCBYMQNZmA.png"/></div></figure><p id="8977" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You may have noticed there was a CLOUDAMQP_URL variable specified in celery.py. We need to provision a Celery Broker on Heroku, for which there are a variety of options. I will be using <a class="af ny" href="https://elements.heroku.com/addons/cloudamqp" rel="noopener ugc nofollow" target="_blank">CloudAMQP</a> which has a free tier. Go ahead and add this to your app. Once added, the CLOUDAMQP_URL environment variable will be included automatically in the Config Vars.</p><p id="53cc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, let’s test the final product.</p><p id="d766" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To monitor requests, run:</p><pre class="mm mn mo mp mq ql qm qn bp qo bb bk"><span id="f4af" class="qp or fq qm b bg qq qr l qs qt">$ heroku logs --tail</span></pre><p id="89e6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Issue another Postman POST request to the Heroku app’s url at the /api/create/ endpoint. You will see the POST request come through, Celery receive the task, load the model, and start running pages:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/96ab69c4cda3ae0833d35f08f528b544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKGJEXuBFAYMDtLRQK80aQ.png"/></div></div></figure><p id="b1c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will continue to see the “Running for page…” until the end of the process and you can check the AWS S3 bucket as it runs.</p><p id="7c00" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Congrats! You’ve now deployed and ran a Python backend using Machine Learning as a part of a distributed task queue running in parallel to the main web process!</p><p id="1d65" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned, you will want to adjust the heroku.yml <em class="nz">commands </em>to incorporate gunicorn threads and/or worker processes and fine tune celery. For further learning, here’s a <a class="af ny" href="https://medium.com/building-the-system/gunicorn-3-means-of-concurrency-efbb547674b7" rel="noopener">great article</a> on configuring gunicorn to meet your app’s needs, one for digging into <a class="af ny" href="https://progressstory.com/tech/python/production-ready-celery-configuration/" rel="noopener ugc nofollow" target="_blank">Celery for production</a>, and another for exploring Celery <a class="af ny" href="https://celery.school/celery-worker-pools" rel="noopener ugc nofollow" target="_blank">worker pools</a>, in order to help with properly managing your resources.</p><p id="b9aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Happy coding!</p><p id="6424" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="nz">Unless otherwise noted, all images used in this article are by the author</em></p></div></div></div></div>    
</body>
</html>