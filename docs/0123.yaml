- en: Exploring the Superhero Role of 2D Batch Normalization in Deep Learning Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12](https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Internal working and intuitions are explained through simple examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    ·10 min read·Jan 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e13dca13cb4875088d4c47bef20965f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by Author
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning (DL) has been a game-changer in the evolution of Convolutional
    Neural Networks (CNN) and Generative Artificial Intelligence (Gen AI). Such DL
    models can extract complex patterns and features from multidimensional spatial
    data, such as images, and make predictions. The more intricate the patterns in
    the input data are, the more complex can the model architecture be. There are
    many ways to accelerate the model training convergence and enhance the model inference
    performance, but Batch Normalization 2D (BN2D) has emerged as a superhero in this
    area. This write-up aims to showcase how integrating BN2D in a DL architecture
    can lead to faster convergence and better inference.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BN2D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BN2D is a normalization technique applied in batches to multidimensional spatial
    inputs such as images to normalize their dimensional (channel) values so that
    dimensions across such batches have a mean of 0 and a variance of 1.
  prefs: []
  type: TYPE_NORMAL
- en: The primary purpose of incorporating BN2D components is to prevent internal
    covariate shifts across dimensions or channels in input data from previous layers
    within a network. Internal covariate shifts across dimensions occur when the distributions
    of dimensional data change due to updates made to network parameters during training
    epochs. For instance, N filters in a convolutional layer produce N-dimensional
    activations as output. This layer maintains weight and bias parameters for its
    filters that get updated incrementally with each training epoch.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of these updates, activations from one filter can have a markedly
    different distribution than activations from another of the same convolutional
    layer. Such differences in distribution indicate that activations from one filter
    are on a vastly different scale than activations from another filter. When inputting
    such dimensional data with vastly different scales to the next layer in the network,
    the learnability of that layer is hindered because the weights of dimensions with
    larger scales require larger updates during gradient descent than those with smaller
    scales.
  prefs: []
  type: TYPE_NORMAL
- en: The other possible consequence is that gradients of weights with smaller scales
    can vanish, while gradients of weights with larger scales can explode. When the
    network experiences such learning obstacles, gradient descent will oscillate across
    the larger-scale dimensions, severely hindering learning convergence and training
    stability. BN2D effectively mitigates this phenomenon by normalizing the dimensional
    data to a standard scale with a mean of 0 and standard deviation of 1 and facilitates
    faster convergence during training, reducing the number of epochs required to
    achieve optimal performance. As such, by easing the network’s training phase,
    the technique ensures that the network can focus on learning more complex and
    abstract features, allowing the extraction of richer representations from the
    input data.
  prefs: []
  type: TYPE_NORMAL
- en: In standard practice, BN2D instances are inserted post-convolution, but pre-activation
    layers, such as ReLU, as shown in a sample DL network in Figure 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfa49a7a52a210f3ec43e17f7ad888cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A Sample Deep CNN (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Inner Workings of BN2D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An example batch of simple multidimensional spatial data, such as 3-channel
    images, is shown in Figure 2 to illustrate the internal workings of the BN2D technique.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9409d2f31f38493938b0732c1cc2d97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Inner Workings of BN2D (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in Figure 2, BN2D functions by processing a batch at every dimension
    or channel. If an input batch has N dimensions or channels, the BN2D instance
    will have N BN2D layers. The separate processing of red, green, and blue channels
    in the example case implies that the corresponding BN2D instance has 3 BN2D layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71f2e87fc6285cac9df0f5a7a429c310.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Formulae Used by BN2D (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: During training, BN2D computes mean and variance for each batch dimension and
    normalizes values as illustrated in Figure 2 using the training-time formula shown
    in Figure 3\. Preset epsilon (ε) is a constant in the denominator to avoid division
    by zero. BN2D instance maintains scale (γ) and shift (β) learnable parameters
    per each dimension or BN2D layer, which are updated during training optimization.
    BN2D instance also maintains moving average and variance per BN2D layer, as illustrated
    in Figure 2, which get updated during training using the formula shown in Figure
    3\. Preset momentum (α) is used as the exponential average factor.
  prefs: []
  type: TYPE_NORMAL
- en: During inference, using the inference-time formula as shown in Figure 3, a BN2D
    instance normalizes values for each dimension using dimension-specific moving
    average, moving variance, and learned scale (γ) and shift (β) parameters. Example
    training-time batch normalization computations are shown in Figure 2 for each
    dimension in the batch input. The example in Figure 2 also illustrates the output
    from a BN2D instance containing the entire batch normalized independently across
    the dimensions or channels. The PyTorch Jupyter Notebook used to work through
    the example illustrated in Figure 2 is available at the following GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb](https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: BN2D in Action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To inspect the expected performance improvements of incorporating BN2D instances
    in a DL network architecture, a simple (toy-like) image dataset is used to build
    relatively simpler DL networks with and without BN2D to predict classes. The following
    are the crucial DL model performance improvements expected with BN2D:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Improved Generalization: The normalizations introduced by BN2D are expected
    to improve the generalization of a DL model. In the example, improved inference-time
    classification accuracy is expected when BN2D layers are introduced in the network.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Faster Convergence: Introducing BN2D layers is expected to facilitate faster
    convergence during training, reducing the number of epochs required to achieve
    optimal performance. In the example, lowered training losses are expected starting
    at early epochs after introducing BN2D layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Smoother Gradient Descent: Since BN2D normalizes the dimensional data to a
    standard scale with a mean of 0 and standard deviation of 1, the possibility of
    oscillations of gradient descent across the larger-scale dimensions is expected
    to be minimized, and the gradient descent is expected to progress smoothly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hindi language hand-written digits (0–9) data published by Kaggle at [https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data)
    (GNU license) is used for training and testing a convolutional DL model with and
    without BN2D incorporated. Refer to this article’s banner image at the top to
    see how Hindi digits are written. The DL model network was built using PyTorch
    DL modules. The choice of hand-written Hindi digits over their English counterparts
    was based on their complexity compared to the latter. Edge detection in Hindi
    digits is more challenging than in English due to more curves than straight lines
    in Hindi digits. Moreover, there could be more variations for the same digit based
    on one’s writing style.
  prefs: []
  type: TYPE_NORMAL
- en: A utility Python function is developed to make the access to the digits data
    more PyTorch dataset/dataloader compliant, as shown in the following code snippet.
    The training dataset had 17000 samples, while the testing dataset had 3000\. Note
    that the PyTorch Grayscale transformer is applied while loading the images as
    PyTorch Tensors. A utility module, ‘ml_utils.py,’ is specifically developed to
    package functions for running epochs, training and testing deep learning models
    using PyTorch Tensor-based operations. The train and test functions also capture
    model metrics to help evaluate the model’s performance. Python notebooks and utility
    modules can be accessed at the author’s public GitHub repository, whose link is
    provided below.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kbmurali/hindi_hw_digits](https://github.com/kbmurali/hindi_hw_digits)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Example DL Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first DL model will comprise three convolutional layers with 16 filters,
    each with a kernel size of 3 and padding 1, resulting in the ‘Same’ convolution.
    The activation function for each convolution is the Rectified Linear Unit (ReLU).
    The max pooling layer with a pool size 2 is placed before a fully connected layer,
    leading to a softmax layer producing 10 class outputs. The model’s network architecture
    is shown in Figure 4\. The corresponding PyTorch model definition is shown in
    the following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79421bd55a1a4a869202dca57b2c0c89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Convolutional Network Without BN2D (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The second DL model shares a similar structure to the first one but introduces
    BN2D instances after convolution and before activation. The model’s network architecture
    is shown in Figure 5\. The corresponding PyTorch model definition is shown in
    the following code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c7125aa3003cee916a07be5148fd8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Convolutional Network With BN2D (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The two DL models are trained on the example Hindi digits dataset using the
    utility function shown in the following code snippet. Note that two sample weights
    from two dimensions/channels of a filter in the last convolutional layer are captured
    to visualize the training loss’s gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding 1: Improved Test Accuracy'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The testing accuracy of the DL model was better with BN2D instances, as shown
    in Figure 6\. The testing accuracy improved gradually with training epochs for
    the model with BN2D, while it oscillated with training epochs for the model without
    BN2D. At the end of epoch 30, the test accuracy for the model with BN2D was 99.1%,
    while 92.4% for the model without BN2D. These results suggest that incorporating
    BN2D instances positively affected the model’s performance, significantly increasing
    the testing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/50762ac4b67a6cc01a89757113068d2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Test Accuracy Over Training Epochs (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding 2: Faster Convergence'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training loss of the DL model was much lower with BN2D instances, as shown
    in Figure 7\. By around training epoch 3 itself, the model with BN2D manifested
    lower training losses than without BN2D. The lower training losses suggest that
    BN2D facilitates faster convergence during training, perhaps reducing the number
    of training epochs for reasonable convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6f6fc394b8216181c525f7b720591c9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Training Loss Over Training Epochs (Image created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding 3: Smoother Gradient Descent'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss function over the two sample weights taken from the last convolution
    of the model with BN2D manifested smoother gradient descent than without BN2D,
    as shown in Figure 8\. The loss function of the model without BN2D followed a
    rather zig-zag gradient descent. The smoother gradient descent with BN2D suggests
    that normalizing the dimensional data to a standard scale with a mean of 0 and
    standard deviation of 1 enables weights of different dimensions possibly to be
    on a similar scale, reducing the possible oscillations of the gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9094c7646a7ecb72dd92a28bb0925bc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Loss Function Gradient Descent Over Sample Weights (Image created
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the benefits of BN2D are clear, its implementation requires careful consideration.
    Proper initialization of weights, suitable learning rates, and the placement of
    BN2D layers within the DL network are crucial factors to maximize its effectiveness.
    While BN2D often prevents over-fitting, there can be cases where it may even contribute
    to over-fitting under certain circumstances. For example, if BN2D is used along
    with another technique called Dropout, the combination might have different effects
    on over-fitting depending on the specific configuration and the dataset. Likewise,
    in the case of small batch sizes, the batch mean and variance may not closely
    represent the overall dataset statistics, potentially resulting in noisy normalization,
    which may not be as effective in preventing over-fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The write-up intended to showcase the intuitions behind using BN2D in deep learning
    networks. The example convolutional models using toy-like image data were solely
    to showcase expected performance improvements incorporating BN2D instances in
    a DL network architecture. The BN2D normalization across spatial and channel dimensions
    brings about training stability, faster convergence, and enhanced generalization,
    ultimately contributing to the success of deep learning models. Hopefully, the
    write-up gives a good understanding of how BN2D works and the intuition behind
    it. Such understanding and intuition come in handy while developing more complex
    DL models.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)
    [## Hindi Character Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: Solve the problem of classifying Devanagari script.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.kaggle.com](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)  [##
    BatchNorm2d - PyTorch 2.1 documentation
  prefs: []
  type: TYPE_NORMAL
- en: Join the PyTorch developer community to contribute, learn, and get your questions
    answered.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?source=post_page-----b4eb869e8b60--------------------------------)
    [](https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------)
    [## Why 2D batch normalization is used in features and 1D in classifiers?
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between BatchNorm2d and BatchNorm1d? Why a BatchNorm2d
    is used in features and BatchNorm1d is…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'discuss.pytorch.org](https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------)
    [](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)
    [## Keras documentation: BatchNormalization layer'
  prefs: []
  type: TYPE_NORMAL
- en: Keras documentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ': BatchNormalization layer Keras documentationkeras.io](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
