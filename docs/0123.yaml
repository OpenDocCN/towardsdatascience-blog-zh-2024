- en: Exploring the Superhero Role of 2D Batch Normalization in Deep Learning Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索二维批量归一化在深度学习架构中的超级英雄角色
- en: 原文：[https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12](https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12](https://towardsdatascience.com/exploring-the-superhero-role-of-2d-batch-normalization-in-deep-learning-architectures-b4eb869e8b60?source=collection_archive---------9-----------------------#2024-01-12)
- en: Internal working and intuitions are explained through simple examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过简单的例子来解释内部工作原理和直觉
- en: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Murali
    Kashaboina](../Images/ff1118f3c317dab87fe4b625a614fb93.png)](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    [Murali Kashaboina](https://murali-kashaboina.medium.com/?source=post_page---byline--b4eb869e8b60--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    ·10 min read·Jan 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b4eb869e8b60--------------------------------)
    ·10分钟阅读·2024年1月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4e13dca13cb4875088d4c47bef20965f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e13dca13cb4875088d4c47bef20965f.png)'
- en: Image created by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者创建
- en: Deep Learning (DL) has been a game-changer in the evolution of Convolutional
    Neural Networks (CNN) and Generative Artificial Intelligence (Gen AI). Such DL
    models can extract complex patterns and features from multidimensional spatial
    data, such as images, and make predictions. The more intricate the patterns in
    the input data are, the more complex can the model architecture be. There are
    many ways to accelerate the model training convergence and enhance the model inference
    performance, but Batch Normalization 2D (BN2D) has emerged as a superhero in this
    area. This write-up aims to showcase how integrating BN2D in a DL architecture
    can lead to faster convergence and better inference.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）在卷积神经网络（CNN）和生成式人工智能（Gen AI）的发展中起到了革命性的作用。这样的深度学习模型能够从多维空间数据（如图像）中提取复杂的模式和特征，并进行预测。输入数据中的模式越复杂，模型架构也可以越复杂。有很多方法可以加速模型训练的收敛速度并提升模型推理性能，但二维批量归一化（BN2D）已经成为该领域的“超级英雄”。本文旨在展示将BN2D集成到深度学习架构中如何带来更快的收敛速度和更好的推理效果。
- en: Understanding BN2D
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解BN2D
- en: BN2D is a normalization technique applied in batches to multidimensional spatial
    inputs such as images to normalize their dimensional (channel) values so that
    dimensions across such batches have a mean of 0 and a variance of 1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BN2D是一种归一化技术，应用于批量处理的多维空间输入（如图像），通过归一化它们的维度（通道）值，使得这些批次中的各维度具有均值为0，方差为1的分布。
- en: The primary purpose of incorporating BN2D components is to prevent internal
    covariate shifts across dimensions or channels in input data from previous layers
    within a network. Internal covariate shifts across dimensions occur when the distributions
    of dimensional data change due to updates made to network parameters during training
    epochs. For instance, N filters in a convolutional layer produce N-dimensional
    activations as output. This layer maintains weight and bias parameters for its
    filters that get updated incrementally with each training epoch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 引入BN2D组件的主要目的是防止输入数据在网络中前一层的维度或通道之间出现内部协变量偏移。维度之间的内部协变量偏移发生在训练周期中，由于网络参数的更新，维度数据的分布发生变化。例如，卷积层中的N个滤波器会产生N维的激活作为输出。该层为其滤波器维护权重和偏差参数，这些参数会在每个训练周期中逐步更新。
- en: As a result of these updates, activations from one filter can have a markedly
    different distribution than activations from another of the same convolutional
    layer. Such differences in distribution indicate that activations from one filter
    are on a vastly different scale than activations from another filter. When inputting
    such dimensional data with vastly different scales to the next layer in the network,
    the learnability of that layer is hindered because the weights of dimensions with
    larger scales require larger updates during gradient descent than those with smaller
    scales.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些更新，来自一个滤波器的激活可能与来自同一卷积层的另一个滤波器的激活有明显不同的分布。这种分布差异表明，一个滤波器的激活与另一个滤波器的激活处于完全不同的尺度。当这样的维度数据以相差悬殊的尺度输入到网络的下一个层时，网络该层的可学习性会受到影响，因为尺度较大的维度在梯度下降时需要较大的更新，而尺度较小的维度则需要较小的更新。
- en: The other possible consequence is that gradients of weights with smaller scales
    can vanish, while gradients of weights with larger scales can explode. When the
    network experiences such learning obstacles, gradient descent will oscillate across
    the larger-scale dimensions, severely hindering learning convergence and training
    stability. BN2D effectively mitigates this phenomenon by normalizing the dimensional
    data to a standard scale with a mean of 0 and standard deviation of 1 and facilitates
    faster convergence during training, reducing the number of epochs required to
    achieve optimal performance. As such, by easing the network’s training phase,
    the technique ensures that the network can focus on learning more complex and
    abstract features, allowing the extraction of richer representations from the
    input data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能的后果是，尺度较小的权重梯度可能会消失，而尺度较大的权重梯度可能会爆炸。当网络遇到这种学习障碍时，梯度下降将在较大尺度的维度之间震荡，严重阻碍学习收敛和训练稳定性。BN2D通过将维度数据标准化为均值为0、标准差为1的标准尺度，有效缓解了这种现象，并促进了训练期间的更快收敛，减少了达到最佳性能所需的训练周期数。因此，通过简化网络的训练过程，该技术确保网络可以集中精力学习更复杂、更抽象的特征，从输入数据中提取更丰富的表示。
- en: In standard practice, BN2D instances are inserted post-convolution, but pre-activation
    layers, such as ReLU, as shown in a sample DL network in Figure 1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准实践中，BN2D实例被插入在卷积层后，但在激活层之前，如图1所示的示例深度学习网络中。
- en: '![](../Images/cfa49a7a52a210f3ec43e17f7ad888cd.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfa49a7a52a210f3ec43e17f7ad888cd.png)'
- en: 'Figure 1: A Sample Deep CNN (Image created by Author)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个示例深度卷积神经网络（图片由作者创建）
- en: Inner Workings of BN2D
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BN2D的内部工作原理
- en: An example batch of simple multidimensional spatial data, such as 3-channel
    images, is shown in Figure 2 to illustrate the internal workings of the BN2D technique.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2展示了一个简单的多维空间数据批次（如3通道图像），用来说明BN2D技术的内部工作原理。
- en: '![](../Images/e9409d2f31f38493938b0732c1cc2d97.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9409d2f31f38493938b0732c1cc2d97.png)'
- en: 'Figure 2: Inner Workings of BN2D (Image created by Author)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：BN2D的内部工作原理（图片由作者创建）
- en: As depicted in Figure 2, BN2D functions by processing a batch at every dimension
    or channel. If an input batch has N dimensions or channels, the BN2D instance
    will have N BN2D layers. The separate processing of red, green, and blue channels
    in the example case implies that the corresponding BN2D instance has 3 BN2D layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2所示，BN2D通过在每个维度或通道上处理一个批次来工作。如果输入批次有N个维度或通道，则该BN2D实例将具有N个BN2D层。示例中对红色、绿色和蓝色通道的独立处理意味着相应的BN2D实例具有3个BN2D层。
- en: '![](../Images/71f2e87fc6285cac9df0f5a7a429c310.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71f2e87fc6285cac9df0f5a7a429c310.png)'
- en: 'Figure 3: Formulae Used by BN2D (Image created by Author)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：BN2D使用的公式（图片由作者创建）
- en: During training, BN2D computes mean and variance for each batch dimension and
    normalizes values as illustrated in Figure 2 using the training-time formula shown
    in Figure 3\. Preset epsilon (ε) is a constant in the denominator to avoid division
    by zero. BN2D instance maintains scale (γ) and shift (β) learnable parameters
    per each dimension or BN2D layer, which are updated during training optimization.
    BN2D instance also maintains moving average and variance per BN2D layer, as illustrated
    in Figure 2, which get updated during training using the formula shown in Figure
    3\. Preset momentum (α) is used as the exponential average factor.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，BN2D 为每个批次维度计算均值和方差，并按照图 2 所示的方式进行归一化，使用图 3 所示的训练时公式。预设的 epsilon（ε）是分母中的常数，用于避免除以零的情况。BN2D
    实例为每个维度或 BN2D 层维护可学习的缩放（γ）和平移（β）参数，这些参数在训练优化过程中进行更新。BN2D 实例还维护每个 BN2D 层的移动平均和方差，如图
    2 所示，这些值在训练过程中根据图 3 所示的公式进行更新。预设的动量（α）作为指数平均因子使用。
- en: During inference, using the inference-time formula as shown in Figure 3, a BN2D
    instance normalizes values for each dimension using dimension-specific moving
    average, moving variance, and learned scale (γ) and shift (β) parameters. Example
    training-time batch normalization computations are shown in Figure 2 for each
    dimension in the batch input. The example in Figure 2 also illustrates the output
    from a BN2D instance containing the entire batch normalized independently across
    the dimensions or channels. The PyTorch Jupyter Notebook used to work through
    the example illustrated in Figure 2 is available at the following GitHub repository.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，使用图 3 所示的推理时公式，BN2D 实例会为每个维度使用维度特定的移动平均、移动方差以及学习到的缩放（γ）和平移（β）参数来归一化数值。图
    2 显示了每个批次输入维度的示例训练时批量归一化计算。图 2 中的示例还展示了 BN2D 实例的输出，包含整个批次的独立维度或通道归一化。用于演示图 2 中示例的
    PyTorch Jupyter Notebook 可通过以下 GitHub 仓库访问。
- en: '[https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb](https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb](https://github.com/kbmurali/hindi_hw_digits/blob/main/how_batch_norm2d_works.ipynb)'
- en: BN2D in Action
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BN2D 的应用
- en: 'To inspect the expected performance improvements of incorporating BN2D instances
    in a DL network architecture, a simple (toy-like) image dataset is used to build
    relatively simpler DL networks with and without BN2D to predict classes. The following
    are the crucial DL model performance improvements expected with BN2D:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查在深度学习网络架构中引入 BN2D 实例后的预期性能提升，使用了一个简单的（类似玩具的）图像数据集来构建相对简单的深度学习网络，分别使用和不使用
    BN2D 来预测类别。以下是期望通过 BN2D 实现的深度学习模型性能提升：
- en: 'Improved Generalization: The normalizations introduced by BN2D are expected
    to improve the generalization of a DL model. In the example, improved inference-time
    classification accuracy is expected when BN2D layers are introduced in the network.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改进的泛化能力：BN2D 引入的归一化预计能够改善深度学习模型的泛化能力。在示例中，当在网络中引入 BN2D 层时，推理时分类准确率预计会得到提升。
- en: 'Faster Convergence: Introducing BN2D layers is expected to facilitate faster
    convergence during training, reducing the number of epochs required to achieve
    optimal performance. In the example, lowered training losses are expected starting
    at early epochs after introducing BN2D layers.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更快的收敛：引入 BN2D 层预计会促进训练过程中的更快收敛，减少达到最佳性能所需的训练轮次。在示例中，预计在引入 BN2D 层后的早期轮次，训练损失将会降低。
- en: 'Smoother Gradient Descent: Since BN2D normalizes the dimensional data to a
    standard scale with a mean of 0 and standard deviation of 1, the possibility of
    oscillations of gradient descent across the larger-scale dimensions is expected
    to be minimized, and the gradient descent is expected to progress smoothly.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更平滑的梯度下降：由于 BN2D 将数据标准化到均值为 0，标准差为 1 的标准尺度，预计可以最小化梯度下降在较大尺度维度上的震荡，梯度下降预计会平稳进行。
- en: Example Dataset
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例数据集
- en: Hindi language hand-written digits (0–9) data published by Kaggle at [https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data)
    (GNU license) is used for training and testing a convolutional DL model with and
    without BN2D incorporated. Refer to this article’s banner image at the top to
    see how Hindi digits are written. The DL model network was built using PyTorch
    DL modules. The choice of hand-written Hindi digits over their English counterparts
    was based on their complexity compared to the latter. Edge detection in Hindi
    digits is more challenging than in English due to more curves than straight lines
    in Hindi digits. Moreover, there could be more variations for the same digit based
    on one’s writing style.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 印地语手写数字（0–9）数据由 Kaggle 发布，数据链接为 [https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data)（GNU
    许可证），用于训练和测试包含和不包含 BN2D 的卷积深度学习模型。请参考本文顶部的横幅图片，查看印地语数字的书写方式。深度学习模型的网络使用 PyTorch
    深度学习模块构建。选择印地语手写数字而非英语手写数字，是因为前者的复杂度高于后者。由于印地语数字中的曲线比直线更多，因此在印地语数字中的边缘检测比在英语数字中更具挑战性。此外，由于书写风格的不同，同一个数字的书写方式可能会有更多变化。
- en: A utility Python function is developed to make the access to the digits data
    more PyTorch dataset/dataloader compliant, as shown in the following code snippet.
    The training dataset had 17000 samples, while the testing dataset had 3000\. Note
    that the PyTorch Grayscale transformer is applied while loading the images as
    PyTorch Tensors. A utility module, ‘ml_utils.py,’ is specifically developed to
    package functions for running epochs, training and testing deep learning models
    using PyTorch Tensor-based operations. The train and test functions also capture
    model metrics to help evaluate the model’s performance. Python notebooks and utility
    modules can be accessed at the author’s public GitHub repository, whose link is
    provided below.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 开发了一个实用的 Python 函数，使得访问数字数据更加符合 PyTorch 数据集/数据加载器的标准，如下代码片段所示。训练数据集包含 17000
    个样本，测试数据集包含 3000 个样本。请注意，在加载图像为 PyTorch 张量时，应用了 PyTorch 灰度图像转换器。一个名为‘ml_utils.py’的实用模块专门用于封装函数，以使用
    PyTorch 张量操作运行迭代周期，训练和测试深度学习模型。训练和测试函数还会捕获模型的指标，以帮助评估模型的表现。Python 笔记本和实用模块可以在作者的公开
    GitHub 仓库中找到，链接如下所示。
- en: '[https://github.com/kbmurali/hindi_hw_digits](https://github.com/kbmurali/hindi_hw_digits)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kbmurali/hindi_hw_digits](https://github.com/kbmurali/hindi_hw_digits)'
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example DL Models
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例深度学习模型
- en: The first DL model will comprise three convolutional layers with 16 filters,
    each with a kernel size of 3 and padding 1, resulting in the ‘Same’ convolution.
    The activation function for each convolution is the Rectified Linear Unit (ReLU).
    The max pooling layer with a pool size 2 is placed before a fully connected layer,
    leading to a softmax layer producing 10 class outputs. The model’s network architecture
    is shown in Figure 4\. The corresponding PyTorch model definition is shown in
    the following code snippet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个深度学习（DL）模型包含三个卷积层，每个卷积层有 16 个滤波器，卷积核大小为 3，填充为 1，从而实现‘Same’卷积。每个卷积层的激活函数是修正线性单元（ReLU）。最大池化层的池大小为
    2，位于全连接层之前，最终通过 softmax 层输出 10 类结果。该模型的网络架构如图 4 所示。相应的 PyTorch 模型定义如下代码片段所示。
- en: '![](../Images/79421bd55a1a4a869202dca57b2c0c89.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79421bd55a1a4a869202dca57b2c0c89.png)'
- en: 'Figure 4: Convolutional Network Without BN2D (Image created by Author)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：没有 BN2D 的卷积网络（图片由作者创建）
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The second DL model shares a similar structure to the first one but introduces
    BN2D instances after convolution and before activation. The model’s network architecture
    is shown in Figure 5\. The corresponding PyTorch model definition is shown in
    the following code snippet.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个深度学习（DL）模型与第一个模型结构类似，但在卷积后和激活前引入了 BN2D 实例。该模型的网络架构如图 5 所示。相应的 PyTorch 模型定义如下代码片段所示。
- en: '![](../Images/4c7125aa3003cee916a07be5148fd8e8.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c7125aa3003cee916a07be5148fd8e8.png)'
- en: 'Figure 5: Convolutional Network With BN2D (Image created by Author)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：带 BN2D 的卷积网络（图片由作者创建）
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The two DL models are trained on the example Hindi digits dataset using the
    utility function shown in the following code snippet. Note that two sample weights
    from two dimensions/channels of a filter in the last convolutional layer are captured
    to visualize the training loss’s gradient descent.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个深度学习（DL）模型在示例印地语数字数据集上进行了训练，使用了如下代码片段所示的实用函数。请注意，捕获了最后一个卷积层中两个维度/通道的滤波器的两个样本权重，用于可视化训练损失的梯度下降过程。
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finding 1: Improved Test Accuracy'
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现1：更高的测试准确度
- en: The testing accuracy of the DL model was better with BN2D instances, as shown
    in Figure 6\. The testing accuracy improved gradually with training epochs for
    the model with BN2D, while it oscillated with training epochs for the model without
    BN2D. At the end of epoch 30, the test accuracy for the model with BN2D was 99.1%,
    while 92.4% for the model without BN2D. These results suggest that incorporating
    BN2D instances positively affected the model’s performance, significantly increasing
    the testing accuracy.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BN2D实例时，DL模型的测试准确度更高，如图6所示。使用BN2D的模型在训练轮次中测试准确度逐渐提高，而没有BN2D的模型则在训练轮次中出现波动。在第30个训练轮次结束时，使用BN2D的模型测试准确度为99.1%，而没有BN2D的模型为92.4%。这些结果表明，加入BN2D实例对模型的表现有积极影响，显著提高了测试准确度。
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/50762ac4b67a6cc01a89757113068d2e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50762ac4b67a6cc01a89757113068d2e.png)'
- en: 'Figure 6: Test Accuracy Over Training Epochs (Image created by Author)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：测试准确度与训练轮次的关系（作者制作的图像）
- en: 'Finding 2: Faster Convergence'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现2：更快的收敛
- en: The training loss of the DL model was much lower with BN2D instances, as shown
    in Figure 7\. By around training epoch 3 itself, the model with BN2D manifested
    lower training losses than without BN2D. The lower training losses suggest that
    BN2D facilitates faster convergence during training, perhaps reducing the number
    of training epochs for reasonable convergence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7所示，使用BN2D实例时，DL模型的训练损失显著较低。在大约第3个训练轮次时，使用BN2D的模型比没有BN2D的模型表现出较低的训练损失。较低的训练损失表明，BN2D有助于训练过程中更快的收敛，可能减少了合理收敛所需的训练轮次。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/6f6fc394b8216181c525f7b720591c9f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f6fc394b8216181c525f7b720591c9f.png)'
- en: 'Figure 7: Training Loss Over Training Epochs (Image created by Author)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：训练损失与训练轮次的关系（作者制作的图像）
- en: 'Finding 3: Smoother Gradient Descent'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现3：更平滑的梯度下降
- en: The loss function over the two sample weights taken from the last convolution
    of the model with BN2D manifested smoother gradient descent than without BN2D,
    as shown in Figure 8\. The loss function of the model without BN2D followed a
    rather zig-zag gradient descent. The smoother gradient descent with BN2D suggests
    that normalizing the dimensional data to a standard scale with a mean of 0 and
    standard deviation of 1 enables weights of different dimensions possibly to be
    on a similar scale, reducing the possible oscillations of the gradient descent.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8所示，使用BN2D的模型在最后一个卷积层的两个样本权重上的损失函数呈现出比没有BN2D时更平滑的梯度下降。没有BN2D的模型的损失函数则表现为较为锯齿状的梯度下降。BN2D带来的更平滑的梯度下降表明，将维度数据归一化为均值为0、标准差为1的标准尺度，使得不同维度的权重可能处于类似的尺度，从而减少梯度下降过程中的波动。
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/9094c7646a7ecb72dd92a28bb0925bc3.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9094c7646a7ecb72dd92a28bb0925bc3.png)'
- en: 'Figure 8: Loss Function Gradient Descent Over Sample Weights (Image created
    by Author)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：样本权重上的损失函数梯度下降（作者制作的图像）
- en: Practical Considerations
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际考虑
- en: While the benefits of BN2D are clear, its implementation requires careful consideration.
    Proper initialization of weights, suitable learning rates, and the placement of
    BN2D layers within the DL network are crucial factors to maximize its effectiveness.
    While BN2D often prevents over-fitting, there can be cases where it may even contribute
    to over-fitting under certain circumstances. For example, if BN2D is used along
    with another technique called Dropout, the combination might have different effects
    on over-fitting depending on the specific configuration and the dataset. Likewise,
    in the case of small batch sizes, the batch mean and variance may not closely
    represent the overall dataset statistics, potentially resulting in noisy normalization,
    which may not be as effective in preventing over-fitting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BN2D的好处显而易见，但其实现需要谨慎考虑。权重的正确初始化、合适的学习率以及BN2D层在DL网络中的位置都是最大化其效果的关键因素。尽管BN2D通常能防止过拟合，但在某些情况下，它可能反而促成过拟合。例如，当BN2D与另一种叫做Dropout的技术一起使用时，根据具体配置和数据集的不同，二者的组合可能对过拟合产生不同的影响。同样，在小批量数据的情况下，批量的均值和方差可能无法准确代表整个数据集的统计特性，这可能导致噪声归一化，从而无法有效防止过拟合。
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The write-up intended to showcase the intuitions behind using BN2D in deep learning
    networks. The example convolutional models using toy-like image data were solely
    to showcase expected performance improvements incorporating BN2D instances in
    a DL network architecture. The BN2D normalization across spatial and channel dimensions
    brings about training stability, faster convergence, and enhanced generalization,
    ultimately contributing to the success of deep learning models. Hopefully, the
    write-up gives a good understanding of how BN2D works and the intuition behind
    it. Such understanding and intuition come in handy while developing more complex
    DL models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在展示在深度学习网络中使用 BN2D 的直觉。使用玩具图像数据的卷积模型示例仅用于展示将 BN2D 实例融入深度学习网络架构中的预期性能提升。BN2D
    在空间和通道维度上的归一化带来了训练稳定性、更快的收敛性和更强的泛化能力，最终有助于深度学习模型的成功。希望本文能够让读者很好地理解 BN2D 的工作原理及其背后的直觉。这种理解和直觉在开发更复杂的深度学习模型时非常有用。
- en: 'References:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)
    [## Hindi Character Recognition'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 印地语字符识别](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)'
- en: Solve the problem of classifying Devanagari script.
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决分类天城文字符号（Devanagari script）的问题。
- en: www.kaggle.com](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)  [##
    BatchNorm2d - PyTorch 2.1 documentation
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.kaggle.com](https://www.kaggle.com/datasets/suvooo/hindi-character-recognition/data?source=post_page-----b4eb869e8b60--------------------------------)
    [## BatchNorm2d - PyTorch 2.1 文档](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?source=post_page-----b4eb869e8b60--------------------------------)'
- en: Join the PyTorch developer community to contribute, learn, and get your questions
    answered.
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加入 PyTorch 开发者社区，贡献、学习并解答你的问题。
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?source=post_page-----b4eb869e8b60--------------------------------)
    [](https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------)
    [## Why 2D batch normalization is used in features and 1D in classifiers?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?source=post_page-----b4eb869e8b60--------------------------------)
    [## 为什么在特征中使用 2D 批归一化，在分类器中使用 1D 批归一化？](https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------)'
- en: What is the difference between BatchNorm2d and BatchNorm1d? Why a BatchNorm2d
    is used in features and BatchNorm1d is…
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BatchNorm2d 和 BatchNorm1d 有什么区别？为什么在特征中使用 BatchNorm2d，在分类器中使用 BatchNorm1d？
- en: 'discuss.pytorch.org](https://discuss.pytorch.org/t/why-2d-batch-normalisation-is-used-in-features-and-1d-in-classifiers/88360/3?source=post_page-----b4eb869e8b60--------------------------------)
    [](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)
    [## Keras documentation: BatchNormalization layer'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[Keras 文档：BatchNormalization 层](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)'
- en: Keras documentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Keras 文档
- en: ': BatchNormalization layer Keras documentationkeras.io](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ': BatchNormalization 层 Keras 文档 [keras.io](https://keras.io/api/layers/normalization_layers/batch_normalization/?source=post_page-----b4eb869e8b60--------------------------------)'
