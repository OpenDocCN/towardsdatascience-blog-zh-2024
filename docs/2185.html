<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Reducing the Size of AI Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Reducing the Size of AI Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07">https://towardsdatascience.com/reducing-the-size-of-ai-models-4ab4cfe5887a?source=collection_archive---------3-----------------------#2024-09-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="874a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Running large AI models on edge devices</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Arun Nanda" class="l ep by dd de cx" src="../Images/48836e7e13dbe0821bed6902209f2d25.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kp3EETBZ43AvPhT2YETFQw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@arunnanda?source=post_page---byline--4ab4cfe5887a--------------------------------" rel="noopener follow">Arun Nanda</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4ab4cfe5887a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/a1084f4a571f30f8cc548dcd6d831b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*_vh00uIPaoH_UKwhNmYJmA.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Image created using Pixlr</figcaption></figure><p id="78e3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">AI models, particularly Large Language Models (LLMs), need large amounts of GPU memory. For example, in the case of the <a class="af nu" href="https://ai.meta.com/blog/meta-llama-3-1/" rel="noopener ugc nofollow" target="_blank">LLaMA 3.1 model, released in July 2024</a>, the <a class="af nu" href="https://huggingface.co/blog/llama31#how-much-memory-does-llama-31-need" rel="noopener ugc nofollow" target="_blank">memory requirements</a> are:</p><ul class=""><li id="8b4a" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">The 8 billion parameter model needs 16 GB memory in 16-bit floating point weights</li><li id="9298" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">The larger 405 billion parameter model needs 810 GB using 16-bit floats</li></ul><p id="175e" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In a full-sized machine learning model, the weights are represented as 32-bit floating point numbers. Modern models have hundreds of millions to tens (or even hundreds) of billions of weights. Training and running such large models is very resource-intensive:</p><ul class=""><li id="4dea" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">It takes lots of compute (processing power).</li><li id="b08e" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">It requires large amounts of GPU memory.</li><li id="5794" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">It consumes large amounts of energy, In particular, the biggest contributors to this energy consumption are:<br/>- Performing a large number of computations (matrix multiplications) using 32-bit floats<br/>- Data transfer — copying the model data from memory to the processing units.</li></ul><p id="3587" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Being highly resource-intensive has two main drawbacks:</p><ul class=""><li id="841f" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk"><strong class="na fr">Training</strong>: Models with large GPU requirements are expensive and slow to train. This limits new research and development to groups with big budgets.</li><li id="69e2" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><strong class="na fr">Inference</strong>: Large models need specialized (and expensive) hardware (dedicated GPU servers) to run. They cannot be run on consumer devices like regular laptops and mobile phones.</li></ul><p id="3f28" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Thus, end-users and personal devices must necessarily access AI models via a paid API service. This leads to a suboptimal user experience for both consumer apps and their developers:</p><ul class=""><li id="bd30" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">It introduces latency due to network access and server load.</li><li id="a0ec" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">It also introduces budget constraints on developers building AI-based software. Being able to run AI models locally — on consumer devices, would mitigate these problems.</li></ul><p id="1cdb" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Reducing the size of AI models is therefore an active area of research and development. This is the first of a series of articles discussing ways of reducing model size, in particular by a method called quantization. These articles are based on studying the original research papers. Throughout the series, you will find links to the PDFs of the reference papers.</p><ul class=""><li id="8edb" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">The current introductory article gives an overview of different approaches to reducing model size. It introduces quantization as the most promising method and as a subject of current research.</li><li id="35d3" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194" rel="noopener"><em class="od">Quantizing the Weights of AI Models</em></a> illustrates the arithmetics of quantization using numerical examples.</li><li id="6fa9" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3" rel="noopener"><em class="od">Quantizing Neural Network Models</em></a> discusses the architecture and process of applying quantization to neural network models, including the basic mathematical principles. In particular, it focuses on how to train models to perform well during inference with quantized weights.</li><li id="e099" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/different-approaches-to-quantization-e3fac905bd5a" rel="noopener"><em class="od">Different Approaches to Quantization</em></a> explains different types of quantization, such as quantizing to different precisions, the granularity of quantization, deterministic and stochastic quantization, and different quantization methods used during training models.</li><li id="9b86" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener"><em class="od">Extreme Quantization: 1-bit AI Models</em></a> is about binary quantization, which involves reducing the model weights from 32-bit floats to binary numbers. It shows the mathematical principles of binary quantization and summarizes the approach adopted by the first researchers who implemented binary quantization of transformer-based models (BERT).</li><li id="0da0" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener"><em class="od">Understanding 1-bit Large Language Models</em></a> presents recent work on quantizing large language models (LLMs) to use 1-bit (binary) weights, i.e. {-1, 1}. In particular, the focus is on BitNet, which was the first successful attempt to redesign the transformer architecture to use 1-bit weights.</li><li id="596b" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk"><a class="af nu" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener"><em class="od">Understanding 1.58-bit Language Models</em></a> discusses the quantization of neural network models, in particular LLMs, to use ternary weights ({-1, 0, +1}). This is also referred to as 1.58-bit quantization and it has proved to deliver very promising results. This topic has attracted much attention in the tech press in the first half of 2024. The background explained in the previous articles helps to get a deeper understanding of how and why LLMs are quantized to 1.58 bits.</li></ul><h1 id="4247" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Approaches to Reducing Model Size</h1><p id="27cb" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Not relying on expensive hardware would make AI applications more accessible and accelerate the development and adoption of new models. Various methods have been proposed and attempted to tackle this challenge of building high-performing yet small-sized models.</p><h2 id="ec2c" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Low-rank decomposition</h2><p id="791f" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Neural networks express their weights in the form of high-dimensional tensors. It is mathematically possible to decompose a high-ranked tensor into a set of smaller-dimensional tensors. This makes the computations more efficient. This is known as <a class="af nu" href="https://en.wikipedia.org/wiki/Tensor_rank_decomposition" rel="noopener ugc nofollow" target="_blank">Tensor rank decomposition</a>. For example, in Computer Vision models, weights are typically 4D tensors.</p><p id="2897" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Lebedev et al, in their 2014 paper titled <a class="af nu" href="https://arxiv.org/pdf/1412.6553" rel="noopener ugc nofollow" target="_blank"><em class="od">Speeding-Up Convolutional Neural Networks Using Fine-Tuned Cp-Decomposition</em></a> demonstrate that using a common decomposition technique, Canonical Polyadic Decomposition (CP Decomposition), convolutions with 4D weight tensors (which are common in computer vision models) can be reduced to a series of four convolutions with smaller 2D tensors. <a class="af nu" href="https://arxiv.org/abs/2106.09685" rel="noopener ugc nofollow" target="_blank">Low Rank Adaptation</a> (LoRA) is a modern (proposed in 2021) technique based on a similar approach applied to Large Language Models.</p><h2 id="7326" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Pruning</h2><p id="6b2f" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Another way to reduce network size and complexity is by eliminating connections from a network. In a 1989 paper titled <a class="af nu" href="https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html" rel="noopener ugc nofollow" target="_blank">Optimal Brain Damage</a>, Le Cun et al propose deleting connections with small magnitudes and retraining the model. Applied iteratively, this approach reduces half or more of the weights of a neural network. The <a class="af nu" href="http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf" rel="noopener ugc nofollow" target="_blank">full paper is available on the website of Le Cun</a>, who (as of 2024) is the chief AI scientist at Meta (Facebook).</p><p id="4620" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In the context of large language models, pruning is especially challenging. SparseGPT, first shared by Frantar et al in a 2023 paper titled <a class="af nu" href="https://arxiv.org/pdf/2301.00774" rel="noopener ugc nofollow" target="_blank"><em class="od">SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot</em></a>, is a well-known pruning method that successfully reduces by half the size of LLMs without losing much accuracy. Pruning LLMs to a fraction of their original size has not yet been feasible. The article, <a class="af nu" href="https://leimao.github.io/article/Neural-Networks-Pruning/" rel="noopener ugc nofollow" target="_blank">Pruning for Neural Networks</a>, by Lei Mao, gives an introduction to this technique.</p><h2 id="68f7" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Knowledge Distillation</h2><p id="0ed9" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Knowledge transfer is a way of training a smaller (student) neural network to replicate the behavior of a larger and more complex (teacher) neural network. In many cases, the student is trained based on the final prediction layer of the teacher network. In other approaches, the student is also trained based on the intermediate hidden layers of the teacher. Knowledge Distillation has been used successfully in some cases, but in general, the student networks are unable to generalize to new unseen data. They tend to be overfitted to replicate the teacher’s behavior within the training dataset.</p><h2 id="9f15" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Quantization</h2><p id="bd11" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">In a nutshell, quantization involves starting with a model with 32-bit or 16-bit floating point weights and applying various techniques to reduce the precision of the weights, to 8-bit integers or even binary (1-bit), without sacrificing model accuracy. Lower precision weights have lower memory and computational needs.</p><p id="2415" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The rest of this article, from the next section onwards, and the rest of this series give an in-depth understanding of quantization.</p><h2 id="2ef1" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Hybrid</h2><p id="f193" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">It is also possible to apply different compression techniques in sequence. Han et al, in their 2016 paper titled <a class="af nu" href="https://arxiv.org/pdf/1510.00149" rel="noopener ugc nofollow" target="_blank"><em class="od">Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding</em></a><em class="od">,</em> apply pruning followed by quantization followed by Huffman coding to compress the AlexNet model by a factor of 35X, to reduce the model size from 240 MB to 6.9 MB without significant loss of accuracy. As of July 2024, such approaches have yet to be tried on low-bit LLMs.</p><h1 id="9450" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Quantization 101</h1><p id="b1c6" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">The “size” of a model is mainly determined by two factors:</p><ul class=""><li id="56a2" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">The number of weights (or parameters)</li><li id="956d" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">The size (length in bits) of each parameter.</li></ul><p id="ceb1" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">It is well-established that the number of parameters in a model is crucial to its performance — hence, reducing the number of parameters is not a viable approach. Thus, attempting to reduce the length of each weight is a more promising angle to explore.</p><p id="ac98" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Traditionally, LLMs are trained with 32-bit weights. Models with 32-bit weights are often referred to as full-sized models. Reducing the length (or precision) of model parameters is called quantization. 16-bit and 8-bit quantization are common approaches. More radical approaches involve quantizing to 4 bits, 2 bits, and even 1 bit. To understand how higher precision numbers are quantized to lower precision numbers, refer to <a class="af nu" href="https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194" rel="noopener"><em class="od">Quantizing the Weights of AI Models</em></a>, with examples of quantizing model weights.</p><p id="4754" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Quantization helps with reducing the memory requirements and reducing the computational cost of running the model. Typically, model weights are quantized. It is also common to quantize the activations (in addition to quantizing the weights). The function that maps the floating point weights to their lower precision integer versions is called the quantizer, or quantization function.</p><h2 id="4497" class="pf of fq bf og pg ph pi oj pj pk pl om nh pm pn po nl pp pq pr np ps pt pu pv bk">Quantization in Neural Networks</h2><p id="42d5" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Simplistically, the linear and non-linear transformation applied by a neural network layer can be expressed as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/2cb3cb4f2a97e8b82ff63dc9256d958e.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*srN-RvDvQXhWkaXtWfqBJg.png"/></div></figure><p id="eeac" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In the above expression:</p><ul class=""><li id="5a33" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nv nw nx bk">z denotes the output of the non-linear function. It is also referred to as the activation.</li><li id="d0b2" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">Sigma is the non-linear activation function. It is often the sigmoid function or the tanh function.</li><li id="0cb1" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">W is the weight matrix of that layer</li><li id="5a43" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">a is the input vector</li><li id="82fc" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">B is the bias vector</li><li id="06e1" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">The matrix multiplication of the weight and the input is referred to as convolution. Adding the bias to the product matrix is called accumulation.</li><li id="ad14" class="my mz fq na b go ny nc nd gr nz nf ng nh oa nj nk nl ob nn no np oc nr ns nt nv nw nx bk">The term passed to the sigma (activation) function is called a Multiply-Accumulate (MAC) operation.</li></ul><p id="65ea" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Most of the computational workload in running neural networks comes from the convolution operation — which involves the multiplication of many floating point numbers. Large models with many weights have a very large number of convolution operations.</p><p id="55fc" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This computational cost could potentially be reduced by doing the multiplication in lower-precision integers instead of floating-point numbers. In an extreme case, as discussed in <a class="af nu" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener"><em class="od">Understanding 1.58-bit Language Models</em></a>, the 32-bit weights could potentially be represented by ternary numbers {-1, 0, +1} and the multiplication operations would be replaced by much simpler addition and subtraction operations. This is the intuition behind quantization.</p><p id="f972" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The computational cost of digital arithmetic is quadratically related to the number of bits. As studied by <a class="af nu" href="https://arxiv.org/pdf/2201.08442" rel="noopener ugc nofollow" target="_blank">Siddegowda et al in their paper on Neural Network Quantization</a> (Section 2.1), using 8-bit integers instead of 32-bit floats leads to 16x higher performance, in terms of energy consumption. When there are billions of weights, the cost savings are very significant.</p><p id="4ee8" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The quantizer function maps the high-precision (typically 32-bit floating point weights) to lower-precision integer weights.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/c3816b4d4012514ef1e090adb7df803e.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*6c1WB9vUxUzue8PnkPqslA.png"/></div></figure><p id="7efe" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The “knowledge” the model has acquired via training is represented by the value of its weights. When these weights are quantized to lower precision, a portion of their information is also lost. The challenge of quantization is to reduce the precision of the weights while maintaining the accuracy of the model.</p><p id="d96e" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">One of the main reasons some quantization techniques are effective is that the relative values of the weights and the statistical properties of the weights are more important than their actual values. This is especially true for large models with millions or billions of weights. Later articles on quantized <a class="af nu" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener">BERT models — BinaryBERT and BiBERT</a>, on <a class="af nu" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">BitNet — which is a transformer LLM quantized down to binary weights</a>, and on <a class="af nu" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener">BitNet b1.58 — which quantizes transformers to use ternary weights</a>, illustrate the use of successful quantization techniques. <a class="af nu" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization" rel="noopener ugc nofollow" target="_blank"><em class="od">A Visual Guide to Quantization</em></a>, by Maarten Grootendoorst, has many illustrations and graphic depictions of quantization.</p><h1 id="8ea3" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Quantized Inference</h1><p id="ae26" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">Inference means using an AI model to generate predictions, such as the classification of an image, or the completion of a text string. When using a full-precision model, the entire data flow through the model is in 32-bit floating point numbers. When running inference through a quantized model, many parts — but not all, of the data flow are in lower precision.</p><p id="f63e" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The bias is typically not quantized because the number of bias terms is much less than the number of weights in a model. So, the cost savings is not enough to justify the overhead of quantization. The accumulator’s output is in high precision. The output of the activation is also in higher precision.</p><h1 id="0d2e" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Conclusion</h1><p id="4723" class="pw-post-body-paragraph my mz fq na b go pa nc nd gr pb nf ng nh pc nj nk nl pd nn no np pe nr ns nt fj bk">This article discussed the need to reduce the size of AI models and gave a high-level overview of ways to achieve reduced model sizes. It then introduced the basics of quantization, a method that is currently the most successful in reducing model sizes while managing to maintain an acceptable level of accuracy.</p><p id="3f45" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The goal of this series is to give you enough background to appreciate the extreme quantization of language models, starting from <a class="af nu" href="https://medium.com/@arunnanda/extreme-quantization-1-bit-ai-models-07169ee29d96" rel="noopener">simpler models like BERT</a> before finally <a class="af nu" href="https://medium.com/@arunnanda/understanding-1-bit-large-language-models-a33cc6acabb3" rel="noopener">discussing 1-bit LLMs</a> and the <a class="af nu" href="https://medium.com/@arunnanda/understanding-1-58-bit-large-language-models-88373010974a" rel="noopener">recent work on 1.58-bit LLMs</a>. To this end, the next few articles in this series present a semi-technical deep dive into the different subtopics like the <a class="af nu" href="https://medium.com/@arunnanda/quantizing-the-weights-of-ai-models-39f489455194" rel="noopener">mathematical operations behind quantization</a> and the <a class="af nu" href="https://medium.com/@arunnanda/quantizing-neural-network-models-8ce49332f1d3" rel="noopener">process of training quantized models</a>. It is important to understand that because this is an active area of research and development, there are few standard procedures and different workers adopt innovative methods to achieve better results.</p></div></div></div></div>    
</body>
</html>