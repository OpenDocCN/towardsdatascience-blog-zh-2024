<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Comprehensive Guide to Training and Running YOLOv8 Models on Custom Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Comprehensive Guide to Training and Running YOLOv8 Models on Custom Datasets</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02">https://towardsdatascience.com/the-comprehensive-guide-to-training-and-running-yolov8-models-on-custom-datasets-22946da259c3?source=collection_archive---------2-----------------------#2024-10-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f58a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">It’s now easier than ever to train your own computer vision models on custom datasets using Python, the command line, or Google Colab.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Oliver Ma" class="l ep by dd de cx" src="../Images/02280890ed87239c75cbcbfa7c5d686c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*dmbNkD5D-u45r44go_cf0g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@oliverma.california?source=post_page---byline--22946da259c3--------------------------------" rel="noopener follow">Oliver Ma</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--22946da259c3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/a6b9123219a8ae3b5339064876987f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*2GgQIPk-FanDUTzLQ1YFpg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image created by author using ChatGPT Auto.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cc53" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Ultralytics’ cutting-edge <strong class="nl fr">YOLOv8</strong> model is one of the best ways to tackle computer vision while minimizing hassle. It is the 8th and latest iteration of the <strong class="nl fr">YOLO (You Only Look Once)</strong> series of models from Ultralytics, and like the other iterations uses a <strong class="nl fr">convolutional neural network (CNN)</strong> to predict object classes and their bounding boxes. The YOLO series of object detectors has become well known for being accurate and quick, and provides a platform built on top of <strong class="nl fr">PyTorch</strong> that simplifies much of the process of creating models from scratch.</p><p id="79d6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Importantly, YOLOv8 is also a very flexible model. That is, it can be trained on a variety of platforms, using any dataset of your choice, and the prediction model can be ran from many sources. This guide will act as a comprehensive tutorial covering the many different ways to train and run YOLOv8 models, as well as the strengths and limitations of each method that will be most relevant in helping you choose the most appropriate procedure depending on your hardware and dataset.</p><p id="6c98" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><em class="of">Note: all images that were used in the creation of this example dataset were taken by the author.</em></p><h1 id="7e1e" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Environment</h1><p id="ab99" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">To get started with training our YOLOv8 model, the first step is to decide what kind of <strong class="nl fr">environment</strong> we want to train our model in (keep in mind that <strong class="nl fr">training</strong> and <strong class="nl fr">running</strong> the model are separate tasks).</p><p id="721d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The environments that are available for us to choose can largely be broken down into two categories: <strong class="nl fr">local-based</strong> and <strong class="nl fr">cloud-based.</strong></p><p id="c0a3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With local-based training, we are essentially running the process of training directly on our system, using the physical hardware of the device. Within local-based training, YOLOv8 provides us with two options: the <strong class="nl fr">Python API</strong> and the <strong class="nl fr">CLI.</strong> There is no real difference in the results or speed of these two options, because the same process is being run under the hood; the only difference is in how the training is setup and run.</p><p id="977c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">On the other hand, cloud-based training allows you to take advantage of the hardware of cloud servers. By using the Internet, you can connect to cloud runtimes and execute code just as you would on your local machine, except now it runs on the cloud hardware.</p><p id="be35" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">By far, the most popular cloud platform for machine learning has been <strong class="nl fr">Google Colab.</strong> It uses a Jupyter notebook format, which allows users to create <strong class="nl fr">“cells”</strong> in which code snippets can be written and run, and offers robust integrations with Google Drive and Github.</p><p id="69ab" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Which environment you decide to use will largely depend on the hardware that is available to you. If you have a powerful system with a high-end NVIDIA GPU, local-based training will likely work well for you. If your local machine’s hardware isn’t up to spec for machine learning, or if you just want more computation power than you have locally, Google Colab may be the way to go.</p><p id="764e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One of the greatest benefits of Google Colab is that it offers some computing resources for free, but also has a simple upgrade path that allows you to leverage faster computing hardware. Even if you already have a powerful system, you could consider using Google Colab if the faster GPUs offered in their higher tier plans represent a significant performance improvement over your existing hardware. With the free plan, you are limited to the NVIDIA T4, which performs roughly equivalent to an RTX 2070. With higher tier plans, the L4 (about the performance of a 4090) and A100 (about the performance of 2 4090s) are available. Keep in mind when comparing GPUs that the amount of <strong class="nl fr">VRAM</strong> is the primary determinant of machine learning performance.</p><h1 id="0f52" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Dataset</h1><p id="e428" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">In order to start training a model, you need lots of data to train it on. Object detection <strong class="nl fr">datasets</strong> normally consist of a collection of images of various objects, in addition to a <strong class="nl fr">“bounding box”</strong> around the object that indicates its location within the image.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ph"><img src="../Images/be9bea36e381b5db08f5ee1263ebf970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ucy-K4PAVsCZyaoPfjneNQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example of a bounding box around a detected object. Image by author.</figcaption></figure><p id="7d04" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">YOLOv8-compatible datasets have a specific structure. They are primarily divided into <strong class="nl fr">valid</strong>, <strong class="nl fr">train</strong>, and <strong class="nl fr">test</strong> folders, which are used for <strong class="nl fr">validation</strong>, <strong class="nl fr">training</strong>, and <strong class="nl fr">testing</strong> of the model respectively (the difference between <em class="of">validation</em> and <em class="of">testing</em> is that during validation, the results are used to tune the model to increase its accuracy, whereas during testing, the results are only used to provide a measure of the model’s real-world accuracy).</p><p id="ca1f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Within each of these folders the dataset is further divided into two folders: the <code class="cx pi pj pk pl b">images</code> and <code class="cx pi pj pk pl b">labels</code> folders. The content of these two folders are closely linked with each other.</p><p id="f567" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The <code class="cx pi pj pk pl b">images</code> folder, as its name suggests, contains all of the object images of the dataset. These images usually have a square aspect ratio, a low resolution, and a small file size.</p><p id="24bb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The <code class="cx pi pj pk pl b">labels</code> folder contains the data of the bounding box’s position and size within each image as well as the type (or class) of object represented by each image. For example:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="9448" class="pp oh fq pl b bg pq pr l ps pt">5 0.8762019230769231 0.09615384615384616 0.24519230769230768 0.18990384615384615<br/>11 0.8846153846153846 0.2800480769230769 0.057692307692307696 0.019230769230769232<br/>11 0.796875 0.2668269230769231 0.04807692307692308 0.02403846153846154<br/>17 0.5649038461538461 0.29927884615384615 0.07211538461538461 0.026442307692307692<br/>8 0.48197115384615385 0.39663461538461536 0.06490384615384616 0.019230769230769232<br/>11 0.47716346153846156 0.7884615384615384 0.07932692307692307 0.10576923076923077<br/>11 0.3425480769230769 0.5745192307692307 0.11057692307692307 0.038461538461538464<br/>6 0.43509615384615385 0.5216346153846154 0.019230769230769232 0.004807692307692308<br/>17 0.4855769230769231 0.5264423076923077 0.019230769230769232 0.004807692307692308<br/>2 0.26322115384615385 0.3713942307692308 0.02403846153846154 0.007211538461538462</span></pre><p id="927a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Each line represents an individual object that is present in the image. Within each line, the first number<strong class="nl fr"> </strong>represents the object’s <strong class="nl fr">class,</strong> the second and third numbers represent the <strong class="nl fr">x- and y-coordinates of the center of the bounding box,</strong> and the fourth and fifth numbers represent the <strong class="nl fr">width and height of the bounding box.</strong></p><p id="4d17" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The data within the <code class="cx pi pj pk pl b">images</code> and <code class="cx pi pj pk pl b">labels</code> folders are linked together by file names. Every image in the <code class="cx pi pj pk pl b">images</code> folder will have a corresponding file in the <code class="cx pi pj pk pl b">labels</code> folder with the same file name, and vice versa. Within the dataset, there will always be matching pairs of files within the <code class="cx pi pj pk pl b">images</code> and <code class="cx pi pj pk pl b">labels</code> folders <strong class="nl fr">with the same file name, but with different file extensions;</strong> <code class="cx pi pj pk pl b">.jpg</code> is used for the images whereas <code class="cx pi pj pk pl b">.txt</code> is used for the labels. The data for the bounding box(es) for each object in a <code class="cx pi pj pk pl b">.jpg</code> picture is contained in the corresponding <code class="cx pi pj pk pl b">.txt</code> file.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pu"><img src="../Images/6c9f69516362ab97198fbaa292a2d689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WBvleDwMNMqULzB9"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Typical file structure of a YOLOv8-compatible dataset. Source: Ultralytics YOLO Docs (<a class="af pv" href="https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format</a>)</figcaption></figure><p id="8394" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There are several ways to obtain a YOLOv8-compatible dataset to begin training a model. You can <strong class="nl fr">create your own dataset</strong> or <strong class="nl fr">use a pre-configured one from the Internet.</strong> For the purposes of this tutorial, we will use <a class="af pv" href="https://www.cvat.ai" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">CVAT</strong></a> to create our own dataset and <a class="af pv" href="https://www.kaggle.com" rel="noopener ugc nofollow" target="_blank"><strong class="nl fr">Kaggle</strong></a> to find a pre-configured one.</p><h2 id="e018" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">CVAT</h2><p id="18fe" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">CVAT (<a class="af pv" href="https://www.cvat.ai" rel="noopener ugc nofollow" target="_blank">cvat.ai</a>) is a annotation tool that lets you create your own datasets by manually adding labels to images and videos.</p><p id="5764" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After creating an account and logging in, the process to start annotating is simple. Just create a <strong class="nl fr">project</strong>, give it a suitable name, and add the labels for as many types/classes of objects as you want.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/66545aacff5ffc39ba7ee3ef5fdfdd10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qoQ_2hOZPQXB4PR1.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Creating a new project and label on cvat.ai. Video by author.</figcaption></figure><p id="79cc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Create a new task and upload all the images you want to be part of your dataset. Click “Submit &amp; Open”, and a new <strong class="nl fr">task</strong> should be created under the project, with one <strong class="nl fr">job.</strong></p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/88f64ea615d0116d65026515abf1c224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*styY5qXixBfddLjE.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Creating a new task and job on cvat.ai. Video by author.</figcaption></figure><p id="c0af" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Opening this job will allow you to start the annotation process. Use the <strong class="nl fr">rectangle tool</strong> to create bounding boxes and labels for each of the images in your dataset.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/8429aa65679bfb6eab9da9f7685eda75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*23jtJVFYmA6kJVUy.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Using the rectangle tool on cvat.ai to create bounding boxes. Video by author.</figcaption></figure><p id="eda1" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After annotating all your images, go back to the task and select Actions → Export task dataset, and choose <strong class="nl fr">YOLOv8 Detection 1.0</strong> as the Export format. After downloading the task dataset, you will find that it only contains the <em class="of">labels</em> folder and not the <em class="of">images</em> folder (unless you selected the “Save images” option while exporting). You will have to manually create the <em class="of">images</em> folder and move your images there (you may want to first compress your images to a lower resolution e.g. 640x640). Remember to not change the file names as they must match the file names of the .txt files in the <em class="of">labels</em> folder. You will also need to decide how to allocate the images between <code class="cx pi pj pk pl b">valid</code>, <code class="cx pi pj pk pl b">train</code>, and <code class="cx pi pj pk pl b">test</code> (<code class="cx pi pj pk pl b">train</code> is the most important out of these).</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qo"><img src="../Images/3c462e59f50cbba681575cae8e7cbf29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EgN99t7n6yq88XGwQ-X-6Q.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example dataset exported from cvat.ai. Image by author.</figcaption></figure><p id="45fa" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Your dataset is completed and ready to use!</p><h2 id="0e66" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">Kaggle</h2><p id="6489" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Kaggle (<a class="af pv" href="https://kaggle.com/" rel="noopener ugc nofollow" target="_blank">kaggle.com</a>) is one of the largest online data science communities and one of the best websites to explore datasets. You can try finding a dataset you need by simply searching their website, and unless you are looking for something very specific, chances are you will find it. However, many datasets on Kaggle are not in a YOLOv8-compatible format and/or are unrelated to computer vision, so you may want to include “YOLOv8” in your query to refine your search.</p><p id="c620" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can tell if a dataset is YOLOv8-compatible by the file structure in the dataset’s <strong class="nl fr">Data Explorer</strong> (on the right side of the page).</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq qp"><img src="../Images/26abb66176e709df39df24c94e9a8db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*GKIA9r2aQPTSV4hTBN1lNA.png"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example of a YOLOv8-compatible dataset on Kaggle. Image by author.</figcaption></figure><p id="b3ce" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If the dataset is relatively small (a few MB) and/or you are training locally, you can download the dataset directly from Kaggle. However, if you are planning on training with a large dataset on Google Colab, it is better to retrieve the dataset from the notebook itself (more info below).</p><h1 id="cc6b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Training</h1><p id="2038" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">The training process will differ depending on if you are training locally or on the cloud.</p><h2 id="4fc7" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">Local</h2><p id="29a2" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Create a project folder for all the training files. For this tutorial we will call it <code class="cx pi pj pk pl b">yolov8-project</code>. Move/copy the dataset to this folder.</p><p id="5f27" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Set up a Python virtual environment with required YOLOv8 dependencies:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="bc39" class="pp oh fq pl b bg pq pr l ps pt">python3 -m venv venv<br/>source venv/bin/activate<br/>pip3 install ultralytics</span></pre><p id="9f05" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Create a file named <code class="cx pi pj pk pl b">config.yaml</code>. This is where important dataset information for training will be specified:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="fd3d" class="pp oh fq pl b bg pq pr l ps pt">path: /Users/oliverma/yolov8-project/dataset/ # absolute path to dataset<br/>test: test/images # relative path to test images<br/>train: train/images # relative path to training images<br/>val: val/images # relative path to validation images<br/><br/># classes<br/>names:<br/>  0: bottle</span></pre><p id="5213" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In <code class="cx pi pj pk pl b">path</code> put the <strong class="nl fr">absolute</strong> file path to the dataset’s root directory. You can also use a relative file path, but that will depend on the relative location of <code class="cx pi pj pk pl b">config.yaml</code>.</p><p id="776a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In <code class="cx pi pj pk pl b">test</code>, <code class="cx pi pj pk pl b">train</code>, and <code class="cx pi pj pk pl b">val</code>, put the locations of the images for testing, training, and validation (if you only have <code class="cx pi pj pk pl b">train</code> images, just use <code class="cx pi pj pk pl b">train/images</code> for all 3).</p><p id="e4a7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Under <code class="cx pi pj pk pl b">names</code>, specify the name of each class. This information can usually be found in the <code class="cx pi pj pk pl b">data.yaml</code> file of any YOLOv8 dataset.</p><p id="9163" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As previously mentioned, both the <strong class="nl fr">Python API</strong> or the <strong class="nl fr">CLI</strong> can be used for local training.</p><p id="2ecb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Python API</strong></p><p id="bc25" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Create another file named <code class="cx pi pj pk pl b">main.py</code>. This is where the actual training will begin:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="ff60" class="pp oh fq pl b bg pq pr l ps pt">from ultralytics import YOLO<br/><br/>model = YOLO("yolov8n.yaml")<br/><br/>model.train(data="config.yaml", epochs=100)</span></pre><p id="d9fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">By initializing our model as <code class="cx pi pj pk pl b">YOLO("yolov8n.yaml")</code> we are essentially creating a new model from scratch. We are using <code class="cx pi pj pk pl b">yolov8n</code> because it is the fastest model, but you may also use other models depending on your use case.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qq"><img src="../Images/52d48d4b1cb5bbaf7e1c47d9f4a23f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pr8AKMojEob_0BWbNZFBFA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Performance metrics for YOLOv8 variants. Source: Ultralytics YOLO Docs (<a class="af pv" href="https://docs.ultralytics.com/models/yolov8/#performance-metrics" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/models/yolov8/#performance-metrics</a>)</figcaption></figure><p id="55d3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Finally, we train the model and pass in the config file and the number of <strong class="nl fr">epochs,</strong> or rounds of training. A good baseline is 300 epochs, but you may want to tweak this number depending on the size of your dataset and the speed of your hardware.</p><p id="cb58" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There are a few more helpful settings that you may want to include:</p><ul class=""><li id="19f6" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">imgsz</code>: resizes all images to the specified amount. For example, <code class="cx pi pj pk pl b">imgsz=640</code> would resize all images to 640x640. This is useful if you created your own dataset and did not resize the images.</li><li id="8728" class="nj nk fq nl b go qu nn no gr qv nq nr ns qw nu nv nw qx ny nz oa qy oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">device</code>: specifies which device to train on. By default, YOLOv8 tries to train on GPU and uses CPU training as a fallback, but if you are training on an M-series Mac, you will have to use <code class="cx pi pj pk pl b">device="mps"</code> to train with Apple’s <strong class="nl fr">Metal Performance Shaders (MPS)</strong> backend for GPU acceleration.</li></ul><p id="7542" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For more information on all the training arguments, visit <a class="af pv" href="https://docs.ultralytics.com/modes/train/#train-settings" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/modes/train/#train-settings</a>.</p><p id="b5f2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Your project directory should now look similar to this:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qz"><img src="../Images/381dd501e838530b79c2f06e6f3eb6a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3QYD39XvSFxKClbpDh5kA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example file structure of the project directory. Image by author.</figcaption></figure><p id="d59a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We are finally ready to start training our model. Open a terminal in the project directory and run:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="8c58" class="pp oh fq pl b bg pq pr l ps pt">python3 main.py</span></pre><p id="6e75" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The terminal will display information about the training progress for each epoch as the training progresses.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/ef2b4c1a094f37cf27f77f9c1ea3b9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moSCHmXm8SjeGvHjGtiXqw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Training progress for each epoch displayed in the terminal. Image by author.</figcaption></figure><p id="8404" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The training results will be saved in <code class="cx pi pj pk pl b">runs/detect/train</code> (or <code class="cx pi pj pk pl b">train2</code>, <code class="cx pi pj pk pl b">train3</code>, etc.). This includes the <strong class="nl fr">weights</strong> (with a <code class="cx pi pj pk pl b">.pt</code> file extension), which will be important for running the model later, as well as <code class="cx pi pj pk pl b">results.png</code> which shows many graphs containing relevant training statistics.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/96cd51726353cefbcc09089e6ed6bfa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CvHv2LQm56elkR1C"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Example graphs from results.png. Image by Author.</figcaption></figure><p id="2254" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">CLI</strong></p><p id="bef8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Open a new terminal in the project directory and run this command:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="71ee" class="pp oh fq pl b bg pq pr l ps pt">yolo detect train data=config.yaml model=yolov8n.yaml epochs=100</span></pre><p id="96b8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This command can be modified with the same arguments as listed above for the Python API. For example:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="1271" class="pp oh fq pl b bg pq pr l ps pt">yolo detect train data=config.yaml model=yolov8n.yaml epochs=300 imgsz=640 device=mps</span></pre><p id="9b0f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Training will begin, and progress will be displayed in the terminal. The rest of the training process is the same as with the Python CLI.</p><h2 id="75e9" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">Google Colab</h2><p id="4916" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Go to <a class="af pv" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/</a> and create a new notebook for training.</p><p id="7364" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Before training, make sure you are connected to a GPU runtime by selecting <strong class="nl fr">Change runtime type</strong> in the upper-right corner. Training will be extremely slow on a CPU runtime.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/3e4126352e3072779e670e5c6d788f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cgSOrjuBAuw_wW7_.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Changing the notebook runtime from CPU to T4 GPU. Video by author.</figcaption></figure><p id="b9d8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Before we can begin any training on Google Colab, we first need to import our dataset into the notebook. Intuitively, the simplest way would be to upload the dataset to Google Drive and import it from there into our notebook. However, it takes an exceedingly long amount of time to upload any dataset that is larger than a few MB. The workaround to this is to upload the dataset onto a remote file hosting service (like Amazon S3 or even Kaggle), and pull the dataset directly from there into our Colab notebook.</p><p id="2278" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Import from Kaggle</strong></p><p id="5e1f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here are instructions on how to import a Kaggle dataset directly into a Colab notebook:</p><p id="3527" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In Kaggle account settings, scroll down to <strong class="nl fr">API</strong> and select <strong class="nl fr">Create New Token.</strong> This will download a file named <code class="cx pi pj pk pl b">kaggle.json</code>.</p><p id="4ae5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Run the following in a notebook cell:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="502b" class="pp oh fq pl b bg pq pr l ps pt">!pip install kaggle<br/>from google.colab import files<br/>files.upload()</span></pre><p id="16bf" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Upload the <code class="cx pi pj pk pl b">kaggle.json</code> file that was just downloaded, then run the following:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="1352" class="pp oh fq pl b bg pq pr l ps pt">!mkdir ~/.kaggle<br/>!cp kaggle.json ~/.kaggle/<br/>!chmod 600 ~/.kaggle/kaggle.json<br/>!kaggle datasets download -d [DATASET] # replace [DATASET] with the desired dataset ref</span></pre><p id="0d66" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The dataset will download as a zip archive. Use the <code class="cx pi pj pk pl b">unzip</code> command to extract the contents:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="b8bc" class="pp oh fq pl b bg pq pr l ps pt">!unzip dataset.zip -d dataset</span></pre><p id="8527" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk"><strong class="nl fr">Start Training</strong></p><p id="71f2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Create a new <code class="cx pi pj pk pl b">config.yaml</code> file in the notebook’s file explorer and configure it as previously described. The default working directory in a Colab notebook is <code class="cx pi pj pk pl b">/content/</code>, so the absolute path to the dataset will be <code class="cx pi pj pk pl b">/content/[dataset folder]</code>. For example:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="99b0" class="pp oh fq pl b bg pq pr l ps pt">path: /content/dataset/ # absolute path to dataset<br/>test: test/images # relative path to test images<br/>train: train/images # relative path to training images<br/>val: val/images # relative path to validation images<br/><br/># classes<br/>names:<br/>  0: bottle</span></pre><p id="f545" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Make sure to check the file structure of your dataset to make sure the paths specified in <code class="cx pi pj pk pl b">config.yaml</code> are accurate. Sometimes datasets will be nestled within multiple levels of folders.</p><p id="2af3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Run the following as cells:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="0a37" class="pp oh fq pl b bg pq pr l ps pt">!pip install ultralytics</span></pre><pre class="rc pm pl pn bp po bb bk"><span id="aecb" class="pp oh fq pl b bg pq pr l ps pt">import os<br/><br/>from ultralytics import YOLOmodel = YOLO("yolov8n.yaml")<br/><br/>results = model.train(data="config.yaml", epochs=100)</span></pre><p id="674e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The previously mentioned arguments used to modify local training settings also apply here.</p><p id="728b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similar to local training, results, weights, and graphs will be saved in <code class="cx pi pj pk pl b">runs/detect/train</code>.</p><h1 id="c0ad" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Running</h1><p id="d76e" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Regardless of whether you trained locally or on the cloud, <strong class="nl fr">predictions</strong> must be run locally.</p><p id="c4c9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">After a model has completed training, there will be two weights located in <code class="cx pi pj pk pl b">runs/detect/train/weights</code>, named <code class="cx pi pj pk pl b">best.pt</code> and <code class="cx pi pj pk pl b">last.pt</code>, which are the weights for the best epoch and the latest epoch, respectively. For this tutorial, we will use <code class="cx pi pj pk pl b">best.pt</code> to run the model.</p><p id="e3e5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If you trained locally, move <code class="cx pi pj pk pl b">best.pt</code> to a convenient location (e.g. our project folder <code class="cx pi pj pk pl b">yolov8-project</code>) for running predictions. If you trained on the cloud, download <code class="cx pi pj pk pl b">best.pt</code> to your device. On Google Colab, right-click on the file in the notebook’s file explorer and select <strong class="nl fr">Download.</strong></p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/8e49ce892006ea4c4bb92290abf0513b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H2ivBCI-qEWE8Ica.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Downloading weights on Google Colab. Video by author.</figcaption></figure><p id="f9e9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similar to local training, predictions can be run either through the <strong class="nl fr">Python API</strong> or the <strong class="nl fr">CLI.</strong></p><h2 id="0f16" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">Python API</h2><p id="3045" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">In the same location as <code class="cx pi pj pk pl b">best.pt</code>, create a new file named <code class="cx pi pj pk pl b">predict.py</code>:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="e6e1" class="pp oh fq pl b bg pq pr l ps pt">from ultralytics import YOLO<br/><br/>model = YOLO("best.pt")<br/><br/>results = model(source=0, show=True, conf=0.25, save=True)</span></pre><p id="b641" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Similar to training, there are many useful arguments that will modify the prediction settings:</p><ul class=""><li id="f667" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">source</code>: controls the input source for the predictions. <code class="cx pi pj pk pl b">source=0</code> sets the webcam as the input source. More info below.</li><li id="71b2" class="nj nk fq nl b go qu nn no gr qv nq nr ns qw nu nv nw qx ny nz oa qy oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">show</code>: if <code class="cx pi pj pk pl b">True</code> , displays the predictions, bounding boxes, and confidences on-screen.</li><li id="efc7" class="nj nk fq nl b go qu nn no gr qv nq nr ns qw nu nv nw qx ny nz oa qy oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">conf</code>: the minimum confidence score threshold for a prediction to be considered.</li><li id="89a8" class="nj nk fq nl b go qu nn no gr qv nq nr ns qw nu nv nw qx ny nz oa qy oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">save</code>: if <code class="cx pi pj pk pl b">True</code> , saves prediction results to <code class="cx pi pj pk pl b">runs/detect/predict</code> (or <code class="cx pi pj pk pl b">predict2</code>, <code class="cx pi pj pk pl b">predict3</code>, etc.).</li><li id="3608" class="nj nk fq nl b go qu nn no gr qv nq nr ns qw nu nv nw qx ny nz oa qy oc od oe qr qs qt bk"><code class="cx pi pj pk pl b">device</code>: as previously stated, use <code class="cx pi pj pk pl b">device="mps"</code> on an M-series Mac.</li></ul><p id="8067" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For the full list of prediction arguments, visit <a class="af pv" href="https://docs.ultralytics.com/modes/predict/#inference-arguments" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/modes/predict/#inference-arguments</a>.</p><h2 id="2857" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">CLI</h2><p id="3423" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Run the following command to start the model:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="172e" class="pp oh fq pl b bg pq pr l ps pt">python3 predict.py</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qn"><img src="../Images/74b8a60dea9072833c12d524f9918642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hDQFk_FEjjj9_kZO.gif"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Running YOLOv8 model predictions through live webcam feed. Video by author.</figcaption></figure><h2 id="fd8a" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">CLI</h2><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="0af7" class="pp oh fq pl b bg pq pr l ps pt">yolo detect predict model=best.pt source=0 show=True conf=0.25 save=True</span></pre><p id="5155" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The arguments are the same as with the Python API.</p><h2 id="fb21" class="pw oh fq bf oi px py pz ol qa qb qc oo ns qd qe qf nw qg qh qi oa qj qk ql qm bk">Implementation</h2><p id="6a1a" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We have now been able to successfully run our model on a live webcam feed, but so what? How can we actually use this model and integrate it into a project?</p><p id="1dd2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s think about it in terms of <strong class="nl fr">input</strong> and <strong class="nl fr">output</strong>. In order for this model to be of any use for us in an external application, it must be able to accept useful inputs and produce useful outputs. Thankfully, the flexibility of the YOLOv8 model makes it possible to integrate a model into a variety of use cases.</p><p id="ac23" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We used <code class="cx pi pj pk pl b">source=0</code> to set the webcam as the input source for our predictions. However, YOLOv8 models can utilize many more input sources than just this. Below are several examples:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="d9b1" class="pp oh fq pl b bg pq pr l ps pt">results = model(source="path/to/image.jpg", show=True, conf=0.25, save=True) # static image<br/>results = model(source="screen", show=True, conf=0.25, save=True) # screenshot of current screen<br/>results = model(source="https://ultralytics.com/images/bus.jpg", show=True, conf=0.25, save=True) # image or video URL<br/>results = model(source="path/to/file.csv", show=True, conf=0.25, save=True) # CSV file<br/>results = model(source="path/to/video.mp4", show=True, conf=0.25, save=True) # video file<br/>results = model(source="path/to/dir", show=True, conf=0.25, save=True) # all images and videos within directory<br/>results = model(source="path/to/dir/**/*.jpg", show=True, conf=0.25, save=True) # glob expression<br/>results = model(source="https://www.youtube.com/watch?v=dQw4w9WgXcQ", show=True, conf=0.25, save=True) # YouTube video URL</span></pre><p id="3d49" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For the full list of prediction sources and input options, visit <a class="af pv" href="https://docs.ultralytics.com/modes/predict/#inference-sources" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/modes/predict/#inference-sources</a>.</p><p id="e4ae" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Whenever we run a prediction, YOLOv8 returns huge amounts of valuable data in the form of a list of <code class="cx pi pj pk pl b">Results</code> objects, which includes information about the <strong class="nl fr">bounding boxes, segmentation masks, keypoints, class probabilities, and oriented bounding boxes (OBBs)</strong> of a prediction.</p><p id="7d85" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since we assigned the results of the prediction to the <code class="cx pi pj pk pl b">results</code> variable in our code, we can use it to retrieve information about the prediction:</p><pre class="ms mt mu mv mw pm pl pn bp po bb bk"><span id="89df" class="pp oh fq pl b bg pq pr l ps pt">from ultralytics import YOLO<br/><br/>model = YOLO("best.pt")<br/><br/>results = model(source="bottles.jpg", show=True, conf=0.25, save=True)<br/><br/>print("Bounding boxes of all detected objects in xyxy format:")<br/>for r in results:<br/>  print(r.boxes.xyxy)<br/><br/>print("Confidence values of all detected objects:")<br/>for r in results:<br/>  print(r.boxes.conf)<br/><br/>print("Class values of all detected objects:")<br/>for r in results:<br/>  print(r.boxes.cls)</span></pre><p id="9a74" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">There are far too many types of output results to include in this tutorial, but you can learn more by visiting <a class="af pv" href="https://docs.ultralytics.com/modes/predict/#working-with-results" rel="noopener ugc nofollow" target="_blank">https://docs.ultralytics.com/modes/predict/#working-with-results</a>.</p><p id="972d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This was only a very basic example of what you can do with the outputs of a YOLOv8 model, and there are countless ways you could potentially apply a model to a project of your own.</p><h1 id="fe22" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="843c" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Congratulations for making it all the way to the end!</p><p id="61da" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, we were able to start from scratch and make our own YOLOv8-compatible dataset, import datasets from Kaggle, train a model using multiple environments including Python API, CLI, and Google Colab, run our model locally, and discover many input/output methods that enable us to leverage YOLOv8 models in our own projects.</p><p id="0e13" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Please keep in mind that the objective of this tutorial is to act as a starting point or introduction to YOLOv8 or computer vision. We have barely scratched the surface of the intricacies of the YOLOv8 model, and as you become more experienced with YOLOv8 and computer vision in general, it is definitely wise to take a deeper dive into the topic. There are plenty of articles on the Internet and here on Medium that work great for this very purpose.</p><p id="932e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That being said, if you have followed along with this tutorial and made it to the end, that is nevertheless a great accomplishment. I hope that this article has helped you to gain a basic understanding of machine learning, computer vision, and the YOLOv8 model. Perhaps you have even found a passion for the subject, and will continue to learn more as you progress to more advanced topics in the future.</p><p id="0194" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thanks for reading, and have a great day!</p></div></div></div></div>    
</body>
</html>