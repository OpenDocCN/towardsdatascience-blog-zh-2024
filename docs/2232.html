<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Transformer? Diffusion? Transfusion!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Transformer? Diffusion? Transfusion!</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12">https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8aa7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A gentle introduction to the latest multi-modal transfusion model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d13b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Recently, Meta and Waymo released their latest paper — <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank"><em class="ng">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</em></a><em class="ng">, </em>which integrates the popular transformer model with the diffusion model for multi-modal training and prediction purposes.</p><p id="9e69" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Like Meta’s <a class="af nf" href="https://arxiv.org/pdf/2405.09818" rel="noopener ugc nofollow" target="_blank">previous work</a>, the Transfusion model is based on the <a class="af nf" href="https://arxiv.org/abs/2302.13971" rel="noopener ugc nofollow" target="_blank">Llama architecture</a> with early fusion, which takes both the text token sequence and the image token sequence and uses a single transformer model to generate the prediction. But different from previous art, the Transfusion model addresses the image tokens differently:</p><ul class=""><li id="ec5f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nh ni nj bk">The image token sequence is generated by a pre-trained Variational Auto-Encoder part.</li><li id="cbb0" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">The transformer attention for the image sequence is bi-directional rather than causal.</li></ul><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq nr"><img src="../Images/56e8aa62586af6fcaf8b4b136e95f299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ic8yP4MVEKLjznKReT6hEw.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Transfusion model architecture with pre-training tasks. The text pretraining is the next word prediction task. The image is pretraining is a denoising diffusion task. Image source: <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">https://www.arxiv.org/pdf/2408.11039</a></figcaption></figure><p id="eb97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s discuss the following in detail. We’ll first review the basics, like auto-regressive and diffusion models, then dive into the Transfusion architecture.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="40d1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Auto-regressive Models</strong></p><p id="f893" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Nowadays, large language models (LLMs) are primarily based on transformer architectures, which were proposed in the <a class="af nf" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a> paper in 2017. The transformer architecture contains two parts: the encoder and the decoder.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div class="np nq oq"><img src="../Images/f406a6310d469c83c6397b89d15ec541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*NBf6H0qeLnJG94xYdO_6Sw.png"/></div><figcaption class="od oe of np nq og oh bf b bg z dx">Transformer architecture. Left — Encoder; Right — Decoder. Image source: <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.03762</a></figcaption></figure><p id="2e9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Masked Language Models like BERT use the encoder part pre-trained with randomly bidirectional masked token prediction tasks (and next sentence prediction). For auto-regressive models like the latest LLMs, the decoder part is usually trained on the next token prediction task, where the LM loss is minimized:</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div class="np nq or"><img src="../Images/1c05b3937ed1c7f83c91ecda23c4f01e.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*3I06ruxpCdlZWts9ai35ug.png"/></div><figcaption class="od oe of np nq og oh bf b bg z dx">Equation source: <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">https://www.arxiv.org/pdf/2408.11039</a></figcaption></figure><p id="d93f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the equation above, \theta is the model parameter set, and y_i is the token at index i in a sequence of length n. y&lt;i are all the tokens before y_i.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="00cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Diffusion Models</strong></p><p id="8cfe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What is the diffusion model? It is a series of deep learning models commonly used in computer vision (especially for medical image analysis) for image generation/denoising and other purposes. One of the most well-known diffusion models is the DDPM, which is from the <a class="af nf" href="https://arxiv.org/pdf/2006.11239" rel="noopener ugc nofollow" target="_blank">Denoising diffusion probabilistic models</a> paper published in 2020. The model is a parameterized Markov chain containing a backward and forward transition, as shown below.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq os"><img src="../Images/b22249fb84cdffa38178102bb5a7d39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kiCb4z2pEaMC1EOq3qD9jQ.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">The diffusion model is a bi-directional Markov chain. Image source: <a class="af nf" href="https://arxiv.org/pdf/2006.11239" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2006.11239</a></figcaption></figure><p id="ff86" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What is a Markov chain? It’s a statistical process in which the current step only relies on the previous step, and the reverse is vice versa. By assuming a Markov process, the model can start with a clean image by iteratively adding Gaussian noise in the forward process (right -&gt; left in the figure above) and iteratively “learn” the noise by using a Unet-based architecture in the reverse process (left -&gt; right in the figure above). That’s why we can sometimes see the diffusion model as a generative model (when used from left to right) and sometimes as a denoising model (when used from right to left). The DDPM loss is given below, where the theta is the model parameter set, \epsilon is the known noise, and the \epsilon_theta is the noise estimated by a deep learning model (usually a UNet):</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div class="np nq ot"><img src="../Images/7de90ea23694edffed94c90c0409641f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*jtVGfMukX2kT058NAC6hHw.png"/></div><figcaption class="od oe of np nq og oh bf b bg z dx">Equation source: <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">https://www.arxiv.org/pdf/2408.11039</a></figcaption></figure></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4cca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Diffusion Model in the Latent Space</strong></p><p id="1e1a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The idea of diffusion was further extended to the latent space in the <a class="af nf" href="https://arxiv.org/pdf/2112.10752" rel="noopener ugc nofollow" target="_blank">CVPR’22 paper</a>, where the images are first “compressed” onto the latent space by using the encoder part of a pre-trained <a class="af nf" href="https://arxiv.org/pdf/1312.6114" rel="noopener ugc nofollow" target="_blank">Variational Auto Encoder</a> (VAE). Then, the diffusion and reverse processes are performed on the latent space and mapped back to pixel space using the decoder part of the VAE. This could largely improve the learning speed and efficiency, as most calculations are performed in a lower dimensional space.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq ou"><img src="../Images/a9fad25cf2eddb54eeae02027cedab2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fz0wzQ2OxxmOfEzD6is-Yg.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Latent diffusion model architecture. The \Epsilon and D are encoders and decoders individually. Image source: <a class="af nf" href="https://arxiv.org/pdf/2112.10752" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2112.10752</a></figcaption></figure></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fbee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">VAE-based Image Transfusion</strong></p><p id="802b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The core part of the Transfusion model is the fusion between the diffusion and the transformer for input images. First, an image is divided into a sequence of 8*8 patches; each patch is passed into a pre-trained VAE encoder to “compress” into an 8-element latent vector representation. Then, noise is added to the latent representation and further processed by a linear layer/U-Net encoder to generate the “noisy” x_t. Third, the transformer model processes the sequence of noisy latent representations. Last, the outputs are reversely processed by another linear/U-Net decoder before using a VAE decoder to generate the “true” x_0 image.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div class="np nq ov"><img src="../Images/56fad8e93f71ed2b84dcbf2d20796ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*g81KwATHBXGZcED0jwIoGA.png"/></div><figcaption class="od oe of np nq og oh bf b bg z dx">Diffusion module part for image input. The noise is added to VAE-encoded embedding. Image souce: <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">https://www.arxiv.org/pdf/2408.11039</a></figcaption></figure><p id="e78a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the actual implementation, the beginning of the image (BOI) token and the end of the image (EOI) token are padded to both sides of the image representation sequence before concatenating the text tokens. Self-attention for image training is bi-directional attention, while self-attention for text tokens is causal. At the training stage, the loss for the image sequence is DDPM loss, while the rest of the text tokens use the LM loss.</p><p id="2ec2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So why bother? Why do we need such a complicated procedure for processing image patch tokens? The paper explains that the token space for text and images is different. <strong class="ml fr">While the text tokens are discrete, the image tokens/patches are naturally continuous</strong>. In the previous art, image tokens need to be “discretized” before fusing into the transformer model, while integrating the diffusion model directly could resolve this issue.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9587" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Compare with state-of-the-art</strong></p><p id="4cfb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The primary multi-modal model the paper compares to is the <a class="af nf" href="https://arxiv.org/abs/2405.09818" rel="noopener ugc nofollow" target="_blank">Chameleon model</a>, which Meta proposed earlier this year. Here, we compare the difference between architecture and training set size between the Chameleon-7B and Transfusion-7B.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq ow"><img src="../Images/64436cc8da1edbde103d95a46a0605f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lyh9BaxVFMVG76RSpIlYQw.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Arechitecture and training difference between Chameleon 7B and Transfusion 7B. Image by author.</figcaption></figure><p id="f32a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The paper lists the performance comparison over the Llama2 pre-training suite accuracy, COCO zero-shot Frechet Inception Distance (FID) and GenEval benchmark. We can see that the Transfusion performs much better than Chameleon on the image-related benchmarks (COCO and Gen) while losing very little margin compared to Chameleon, with the same amount of parameters.</p><figure class="ns nt nu nv nw nx np nq paragraph-image"><div role="button" tabindex="0" class="ny nz ed oa bh ob"><div class="np nq ox"><img src="../Images/e202a86699418ec3765bc3b34f706bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zWUYIo3Oie7olyWUqw_cug.png"/></div></div><figcaption class="od oe of np nq og oh bf b bg z dx">Image source: <a class="af nf" href="https://www.arxiv.org/pdf/2408.11039" rel="noopener ugc nofollow" target="_blank">https://www.arxiv.org/pdf/2408.11039</a></figcaption></figure></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4cc1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Further Comments.</strong></p><p id="aab8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although the idea of the paper is super interesting, the “Diffusion” part of the Transfusion is hardly an actual Diffusion, as there are only two timestamps in the Markov process. Besides, the pre-trained VAE makes the model no longer strictly end-to-end. Also, the VAE + Linear/UNet + Transformer Encoder + Linear/UNet + VAE design looks so complicated, which makes the audience can’t help but ask, is there a more elegant way to implement this idea? Besides, I previously wrote about the <a class="af nf" href="https://medium.com/towards-data-science/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1" rel="noopener">latest publication from Apple</a> on the generalization benefits of using autoregressive modelling on images, so it might be interesting to give a second thought to the “MIM + autoregressive” approach.<br/>If you find this post interesting and would like to discuss it, you’re welcome to leave a comment, and I’m happy to further the discussion there :)</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4507" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">References</strong></p><ul class=""><li id="dbd6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nh ni nj bk">Zhou et al., Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model. arXiv 2024.</li><li id="a971" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Team C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint 2024.</li><li id="e777" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Touvron et al., Llama: Open and efficient foundation language models. arXiv 2023.</li><li id="bb6d" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Rombach et al., High-resolution image synthesis with latent diffusion models. CVPR 2022.</li><li id="b097" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Ho et al., Denoising diffusion probabilistic models. NeurIPS 2020.</li><li id="1c22" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Vaswani, Attention is all you need. NeurIPS 2017.</li><li id="f09b" class="mj mk fq ml b go nk mn mo gr nl mq mr ms nm mu mv mw nn my mz na no nc nd ne nh ni nj bk">Kingma, Auto-encoding variational bayes. arXiv preprint 2013.</li></ul></div></div></div></div>    
</body>
</html>