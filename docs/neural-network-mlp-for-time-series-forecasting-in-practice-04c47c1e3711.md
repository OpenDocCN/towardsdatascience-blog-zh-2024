# ç¥ç»ç½‘ç»œï¼ˆMLPï¼‰åœ¨æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„å®è·µåº”ç”¨

> åŸæ–‡ï¼š[`towardsdatascience.com/neural-network-mlp-for-time-series-forecasting-in-practice-04c47c1e3711?source=collection_archive---------0-----------------------#2024-07-08`](https://towardsdatascience.com/neural-network-mlp-for-time-series-forecasting-in-practice-04c47c1e3711?source=collection_archive---------0-----------------------#2024-07-08)

## ç‰¹å¾å·¥ç¨‹å’Œæ„å»º MLP æ¨¡å‹çš„å®ç”¨ç¤ºä¾‹

[](https://tothjd.medium.com/?source=post_page---byline--04c47c1e3711--------------------------------)![Daniel J. TOTH](https://tothjd.medium.com/?source=post_page---byline--04c47c1e3711--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--04c47c1e3711--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04c47c1e3711--------------------------------) [Daniel J. TOTH](https://tothjd.medium.com/?source=post_page---byline--04c47c1e3711--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--04c47c1e3711--------------------------------) Â·é˜…è¯»æ—¶é•¿ 16 åˆ†é’ŸÂ·2024 å¹´ 7 æœˆ 8 æ—¥

--

# **ä»‹ç»**

æ—¶é—´åºåˆ—ï¼Œå°¤å…¶æ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ï¼Œæ˜¯æ•°æ®ç§‘å­¦é¢†åŸŸä¸€ä¸ªéå¸¸è‘—åçš„é—®é¢˜ï¼Œå—ä¸“ä¸šäººå£«å’Œå•†ä¸šç”¨æˆ·çš„å¹¿æ³›å…³æ³¨ã€‚

å­˜åœ¨å¤šç§é¢„æµ‹æ–¹æ³•ï¼Œå¯ä»¥å°†å®ƒä»¬å½’ç±»ä¸ºç»Ÿè®¡æ–¹æ³•æˆ–æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä¾¿äºç†è§£å’Œæ¦‚è§ˆï¼Œä½†å®é™…ä¸Šï¼Œé¢„æµ‹éœ€æ±‚å¦‚æ­¤ä¹‹é«˜ï¼Œç°æœ‰çš„é€‰é¡¹ç§ç±»ç¹å¤šã€‚

æœºå™¨å­¦ä¹ æ–¹æ³•è¢«è®¤ä¸ºæ˜¯æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œå¹¶ä¸”å› èƒ½å¤Ÿæ•æ‰æ•°æ®ä¸­çš„å¤æ‚éçº¿æ€§å…³ç³»è€Œè¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œé€šå¸¸èƒ½æä¾›æ›´é«˜çš„é¢„æµ‹å‡†ç¡®æ€§[1]ã€‚å…¶ä¸­ï¼Œç¥ç»ç½‘ç»œé¢†åŸŸæ˜¯ä¸€ä¸ªå¹¿å—å…³æ³¨çš„æœºå™¨å­¦ä¹ åˆ†æ”¯ã€‚ç‰¹åˆ«æ˜¯åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œå·²ç»è¢«å¼€å‘å¹¶åº”ç”¨äºè§£å†³é¢„æµ‹é—®é¢˜[2]ã€‚

æ•°æ®ç§‘å­¦çˆ±å¥½è€…å¯èƒ½ä¼šè§‰å¾—è¿™äº›æ¨¡å‹èƒŒåçš„å¤æ‚æ€§ä»¤äººæœ›è€Œç”Ÿç•ï¼Œä½œä¸ºå…¶ä¸­çš„ä¸€å‘˜ï¼Œæˆ‘å¯ä»¥å‘Šè¯‰ä½ ï¼Œæˆ‘ä¹Ÿæœ‰åŒæ ·çš„æ„Ÿè§‰ã€‚ç„¶è€Œï¼Œæœ¬æ–‡æ—¨åœ¨å±•ç¤º

> å°½ç®¡æœºå™¨å­¦ä¹ æ–¹æ³•çš„æœ€æ–°è¿›å±•éå¸¸æ˜¾è‘—ï¼Œä½†åœ¨å¯»æ±‚ç‰¹å®šé—®é¢˜çš„è§£å†³æ–¹æ¡ˆæ—¶ï¼Œå¹¶ä¸ä¸€å®šéœ€è¦è¿½æ±‚æœ€å¤æ‚çš„åº”ç”¨ã€‚ç»è¿‡å¼ºåŒ–çš„æˆç†Ÿæ–¹æ³•ä¸å¼ºå¤§çš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ç»“åˆï¼Œä¾ç„¶èƒ½å¤Ÿæä¾›ä»¤äººæ»¡æ„çš„ç»“æœã€‚

æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘åº”ç”¨äº†å¤šå±‚æ„ŸçŸ¥å™¨æ¨¡å‹ï¼Œå¹¶åˆ†äº«äº†ä»£ç å’Œç»“æœï¼Œè®©ä½ èƒ½å¤Ÿäº²è‡ªä½“éªŒå¦‚ä½•æœ‰æ•ˆåœ°è¿›è¡Œæ—¶é—´åºåˆ—ç‰¹å¾å·¥ç¨‹å’Œé¢„æµ‹ã€‚

# **æœ¬æ–‡ç›®æ ‡**

æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘æƒ³ä¸ºè‡ªå­¦çš„ä¸“ä¸šäººå£«æä¾›çš„å†…å®¹å¯ä»¥æ€»ç»“ä¸ºä»¥ä¸‹å‡ ç‚¹ï¼š

1.  åŸºäºå®é™…é—®é¢˜/æ•°æ®è¿›è¡Œé¢„æµ‹

1.  å¦‚ä½•ä¸ºæ•æ‰æ—¶é—´æ¨¡å¼å·¥ç¨‹åŒ–æ—¶é—´åºåˆ—ç‰¹å¾

1.  æ„å»ºä¸€ä¸ªèƒ½å¤Ÿåˆ©ç”¨æ··åˆå˜é‡ï¼ˆæµ®åŠ¨å’Œæ•´æ•°ï¼Œé€šè¿‡åµŒå…¥å¤„ç†ä¸ºç±»åˆ«å˜é‡ï¼‰çš„ MLP æ¨¡å‹

1.  ä½¿ç”¨ MLP è¿›è¡Œç‚¹é¢„æµ‹

1.  ä½¿ç”¨ MLP è¿›è¡Œå¤šæ­¥é¢„æµ‹

1.  ä½¿ç”¨ç½®æ¢ç‰¹å¾é‡è¦æ€§æ–¹æ³•è¯„ä¼°ç‰¹å¾é‡è¦æ€§

1.  é’ˆå¯¹ä¸€ç»„åˆ†ç»„ç‰¹å¾ï¼ˆå¤šä¸ªç»„ï¼Œåˆ†åˆ«é’ˆå¯¹æ¯ä¸ªç»„è¿›è¡Œè®­ç»ƒï¼‰é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œä»¥ç»†åŒ–åˆ†ç»„ç‰¹å¾çš„é‡è¦æ€§

1.  é€šè¿‡ä¸`UnobservedComponents`æ¨¡å‹è¿›è¡Œæ¯”è¾ƒæ¥è¯„ä¼°æ¨¡å‹

# **å…³é”®æŠ€æœ¯æœ¯è¯­**

è¯·æ³¨æ„ï¼Œæœ¬æ–‡å‡å®šè¯»è€…å·²ç»å…·å¤‡ä¸€äº›å…³é”®æŠ€æœ¯æœ¯è¯­çš„åŸºç¡€çŸ¥è¯†ï¼Œå¹¶ä¸æ‰“ç®—è¯¦ç»†è§£é‡Šè¿™äº›æœ¯è¯­ã€‚ä»¥ä¸‹åˆ—å‡ºäº†è¿™äº›å…³é”®æœ¯è¯­ï¼Œå¹¶æä¾›äº†å‚è€ƒï¼Œè¯»è€…å¯æŸ¥é˜…ä»¥ä¾¿ç†è§£ï¼š

1.  **æ—¶é—´åºåˆ—** [3]

1.  **é¢„æµ‹** [4] â€” åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ç”¨äºåŒºåˆ†è®­ç»ƒæœŸé—´çš„æ¨¡å‹è¾“å‡º

1.  **é¢„æµ‹** [4] â€” åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†ç”¨äºåŒºåˆ†æµ‹è¯•æœŸé—´çš„æ¨¡å‹è¾“å‡º

1.  **ç‰¹å¾å·¥ç¨‹** [5]

1.  **è‡ªç›¸å…³** [6]

1.  **åè‡ªç›¸å…³** [6]

1.  **MLPï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨ï¼‰** [7]

1.  **è¾“å…¥å±‚** [7]

1.  **éšè—å±‚** [7]

1.  **è¾“å‡ºå±‚** [7]

1.  **åµŒå…¥** [8]

1.  **çŠ¶æ€ç©ºé—´æ¨¡å‹** [9]

1.  **æœªè§‚å¯Ÿåˆ°çš„ç»„ä»¶æ¨¡å‹** [9]

1.  **RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰** [10]

1.  **ç‰¹å¾é‡è¦æ€§** [11]

1.  **ç½®æ¢ç‰¹å¾é‡è¦æ€§** [11]

# **æ•°æ®æ¢ç´¢**

åœ¨åˆ†æè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ ¸å¿ƒåŒ…åŒ…æ‹¬ï¼šç”¨äºæ•°æ®å¤„ç†çš„`numpy`å’Œ`pandas`ï¼Œç”¨äºäº¤äº’å¼å›¾è¡¨çš„`plotly`ï¼Œç”¨äºç»Ÿè®¡å’ŒçŠ¶æ€ç©ºé—´å»ºæ¨¡çš„`statsmodels`ï¼Œä»¥åŠç”¨äº MLP æ¶æ„çš„`tensorflow`ã€‚

*æ³¨æ„ï¼šç”±äºæŠ€æœ¯é™åˆ¶ï¼Œæˆ‘å°†æä¾›äº¤äº’å¼ç»˜å›¾çš„ä»£ç ç‰‡æ®µï¼Œä½†æ­¤å¤„å±•ç¤ºçš„å›¾è¡¨å°†æ˜¯é™æ€çš„ã€‚*

```py
import opendatasets as od
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import tensorflow as tf

from sklearn.preprocessing import StandardScaler
from sklearn.inspection import permutation_importance
import statsmodels.api as sm
from statsmodels.tsa.stattools import acf, pacf
import datetime

import warnings
warnings.filterwarnings('ignore')
```

æ•°æ®é€šè¿‡`opendatasets`è‡ªåŠ¨åŠ è½½ã€‚

```py
dataset_url = "https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/"
od.download(dataset_url)
df = pd.read_csv(".\hourly-energy-consumption" + "\AEP_hourly.csv", index_col=0)
df.sort_index(inplace = True)
```

è¯·è®°ä½ï¼Œæ•°æ®æ¸…ç†æ˜¯åˆ†æçš„å…³é”®ç¬¬ä¸€æ­¥ã€‚å¦‚æœä½ å¯¹ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œç‰¹åˆ«æ˜¯çŠ¶æ€ç©ºé—´å»ºæ¨¡ï¼Œè¯·å‚è€ƒæˆ‘ä¹‹å‰çš„æ–‡ç« [è¿™é‡Œ](https://medium.com/analytics-vidhya/multi-seasonal-time-series-analysis-decomposition-and-forecasting-with-python-609409570007)ã€‚â˜šğŸ“° ç®€è€Œè¨€ä¹‹ï¼Œè¿›è¡Œäº†ä»¥ä¸‹æ­¥éª¤ï¼š

1.  è¯†åˆ«ç¼ºå¤±çš„æ—¶é—´æˆ³ï¼ˆä»…è¯†åˆ«äº†å•æ­¥ç¼ºå¤±ï¼‰

1.  æ‰§è¡Œæ’è¡¥ï¼ˆä½¿ç”¨å‰åè®°å½•çš„å‡å€¼ï¼‰

1.  è¯†åˆ«å¹¶åˆ é™¤é‡å¤é¡¹

1.  å°†æ—¶é—´æˆ³åˆ—è®¾ç½®ä¸ºæ•°æ®æ¡†çš„ç´¢å¼•

1.  å°†æ•°æ®æ¡†çš„ç´¢å¼•é¢‘ç‡è®¾ç½®ä¸ºæ¯å°æ—¶ï¼Œå› ä¸ºè¿™æ˜¯è¿›ä¸€æ­¥å¤„ç†çš„è¦æ±‚

åœ¨å‡†å¤‡å¥½æ•°æ®åï¼Œæˆ‘ä»¬é€šè¿‡ç»˜åˆ¶ 5 ä¸ªéšæœºæ—¶é—´æˆ³æ ·æœ¬æ¥æ¢ç´¢æ•°æ®ï¼Œå¹¶æ¯”è¾ƒä¸åŒå°ºåº¦ä¸‹çš„æ—¶é—´åºåˆ—ã€‚

```py
fig = make_subplots(rows=5, cols=4, shared_yaxes=True, horizontal_spacing=0.01, vertical_spacing=0.04)

#  drawing a random sample of 5 indices without repetition
sample = sorted([x for x in np.random.choice(range(0, len(df), 1), 5, replace=False)])

# zoom x scales for plotting
periods = [9000, 3000, 720, 240]

colors = ["#E56399", "#F0B67F", "#DE6E4B", "#7FD1B9", "#7A6563"]

# s for sample datetime start
for si, s in enumerate(sample):

    # p for period length
    for pi, p in enumerate(periods):
        cdf = df.iloc[s:(s+p+1),:].copy()
        fig.add_trace(go.Scatter(x=cdf.index,
                                 y=cdf.AEP_MW.values,
                                 marker=dict(color=colors[si])),
                        row=si+1, col=pi+1)

fig.update_layout(
    font=dict(family="Arial"),
    margin=dict(b=8, l=8, r=8, t=8),
    showlegend=False,
    height=1000,
    paper_bgcolor="#FFFFFF",
    plot_bgcolor="#FFFFFF")
fig.update_xaxes(griddash="dot", gridcolor="#808080")
fig.update_yaxes(griddash="dot", gridcolor="#808080")
```

![](img/0a11e7108055273a11e5804af3946d3c.png)

æ•°æ®é›†çš„éšæœºæŠ½æ ·å’Œä¸åŒæ—¶é—´å°ºåº¦çš„å¯è§†åŒ–ã€‚æ¥æºï¼šä½œè€…

# çŠ¶æ€ç©ºé—´å»ºæ¨¡

é€šè¿‡ä»”ç»†åˆ†æè¿™ä¸ªç®€å•ä½†æœ‰æ•ˆçš„å›¾è¡¨ï¼Œæˆ‘å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œåˆ†æåº”è¯¥è€ƒè™‘å‡ ä¸ªå­£èŠ‚æ€§æ•ˆåº”ï¼š

1.  èƒ½æºæ¶ˆè€—â€”â€”é€šå¸¸â€”â€”åœ¨å¤å­£å’Œå†¬å­£çš„ä¸­æœŸè¾¾åˆ°å³°å€¼ï¼Œæ— è®ºé€‰æ‹©å“ªä¸ªå¹´ä»½

1.  åœ¨å‘¨ä¸€ä¼¼ä¹ä¼šå‡ºç°æ¯å‘¨æœ€å°å€¼æ¨¡å¼

1.  åœ¨å¤œé—´æœ‰ä¸€ä¸ªæ¯æ—¥æœ€ä½å€¼ï¼Œç™½å¤©æœ‰ä¸€ä¸ªæ¯æ—¥æœ€é«˜å€¼ã€‚

è¿›ä¸€æ­¥åˆ†æä¼šæ­ç¤ºï¼Œæ•°æ®é›†çš„å¹´åº¦æ¨¡å¼æœ‰ 2 ä¸ªè°æ³¢ï¼Œå› ä¸ºå†¬å­£å’Œå¤å­£çš„å³°å€¼æ°´å¹³ä¸åŒã€‚å› æ­¤ï¼Œè€ƒè™‘äº†ä»¥ä¸‹çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œå…¶ä¸­å‘¨æœŸä»¥å°æ—¶ä¸ºå•ä½ï¼ˆè§ä¸‹æ–‡æ¨¡å‹æ€»ç»“ï¼‰ï¼š

```py
# splitting time series to train and test subsets
y_train = df.iloc[:-8766, :].copy()
y_test = df.iloc[-8766:, :].copy()

# Unobserved Components model definition
model = sm.tsa.UnobservedComponents(y_train,
                                    level='dtrend',
                                    irregular=True,
                                    stochastic_level = False,
                                    stochastic_trend = False,
                                    stochastic_freq_seasonal = [False, False, False],
                                    freq_seasonal=[{'period': 24, 'harmonics': 1},
                                                    {'period': 168, 'harmonics': 1},
                                                    {'period': 8766, 'harmonics': 2}])
# fitting model to train data
model_results = model.fit()

# printing statsmodels summary for model
print(model_results.summary())
```

```py
Value of `irregular` may be overridden when the trend component is specified using a model string.

                           Unobserved Components Results                            
====================================================================================
Dep. Variable:                       AEP_MW   No. Observations:               112530
Model:                  deterministic trend   Log Likelihood            -1002257.017
                     + freq_seasonal(24(1))   AIC                        2004516.033
                    + freq_seasonal(168(1))   BIC                        2004525.664
                   + freq_seasonal(8766(2))   HQIC                       2004518.941
Date:                      Tue, 25 Jun 2024                                         
Time:                              08:13:35                                         
Sample:                          10-01-2004                                         
                               - 08-02-2017                                         
Covariance Type:                        opg                                         
====================================================================================
                       coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------
sigma2.irregular  3.168e+06    1.3e+04    244.095      0.000    3.14e+06    3.19e+06
===================================================================================
Ljung-Box (L1) (Q):              104573.71   Jarque-Bera (JB):              2731.37
Prob(Q):                              0.00   Prob(JB):                         0.00
Heteroskedasticity (H):               1.04   Skew:                             0.35
Prob(H) (two-sided):                  0.00   Kurtosis:                         3.30
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).
```

åœ¨ä¸æå‰è¿‡å¤šå±•å¼€çš„æƒ…å†µä¸‹ï¼Œæˆ‘æƒ³æŒ‡å‡ºï¼Œæ¨¡å‹è¿‘ä¼¼äº†è¿‡å» 365 å¤©çš„æ€»èƒ½æºæ¶ˆè€—ï¼Œè¯¯å·®çº¦ä¸º~2%ï¼Œä»å•†ä¸šè§’åº¦æ¥çœ‹ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ç›¸å½“å‡†ç¡®çš„ã€‚ä¸‹é¢æ„å»ºçš„ MLP æ¨¡å‹å°†é€šè¿‡ä¸ä¸Šè¿°çŠ¶æ€ç©ºé—´æ¨¡å‹çš„æ¯”è¾ƒæ¥è¯„ä¼°ã€‚

# ç‰¹å¾å·¥ç¨‹

åœ¨æ„å»º MLP æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬åº”ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°ç‹¬ç‰¹çš„è¶‹åŠ¿å’Œå­£èŠ‚æ€§æ•ˆåº”ã€‚è¿™å¯ä»¥é€šè¿‡å‘æ•°æ®é›†æ·»åŠ æ–°ç‰¹å¾æ¥å®ç°ï¼Œè¿™äº›ç‰¹å¾æ˜¯ä»åŸå§‹çš„ 1D æ—¶é—´åºåˆ—æ•°æ®æ´¾ç”Ÿè€Œæ¥çš„ã€‚ä¸ºæ•æ‰å·²ç»è¯†åˆ«æˆ–æœªè¯†åˆ«çš„æ¨¡å¼ï¼Œæ´¾ç”Ÿç‰¹å¾åŒ…æ‹¬ï¼š

1.  æ»å

1.  å·®å¼‚

1.  æ»šåŠ¨å‡å€¼

1.  æ»šåŠ¨æ ‡å‡†å·®

1.  ä¸€å¤©ä¸­çš„å°æ—¶

1.  ä¸€å‘¨ä¸­çš„å¤©æ•°

1.  æ ‡è®°å‘¨æœ«

è¿™äº›æ´¾ç”Ÿçš„â€”â€”ä»¥åŠæ•°å€¼å‹â€”â€”ç‰¹å¾å¯ä»¥åœ¨å¤šä¸ªæ—¶é—´é—´éš”ä¸­è¿›è¡Œè€ƒè™‘ã€‚ä¸ºäº†ç¡®å®šæ¨¡å‹åœ¨å“ªäº›æ—¶é—´é—´éš”ä¸­èƒ½å¤Ÿè·ç›Šï¼Œå¼ºçƒˆå»ºè®®æ£€æŸ¥æ•°æ®é›†çš„è‡ªç›¸å…³ç‰¹æ€§ã€‚

```py
dff = df.copy()
acorr = acf(dff.AEP_MW.values, nlags=2*366)     # autocorrelation
pacorr = pacf(dff.AEP_MW.values, nlags=2*366)   # partial autocorrelation

fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0)
fig.add_trace(go.Scatter(
    x=np.linspace(0, len(acorr), len(acorr)+1),
    y=acorr,
    name="Autocorrelation",
    marker=dict(color="rgb(180, 120, 80)")
), row=1, col=1)
fig.add_trace(go.Scatter(
    x=np.linspace(0, len(pacorr), len(pacorr)+1),
    y=pacorr,
    name="Partial Autocorrelation",
    marker=dict(color="rgb(80, 180, 120)")
), row=2, col=1)
fig.update_layout(
    font=dict(family="Arial"),
    margin=dict(b=4, l=4, r=4, t=4),
    showlegend=False,
    height=500,
    paper_bgcolor="#FFFFFF",
    plot_bgcolor="#FFFFFF")
fig.update_xaxes(griddash="dot", gridcolor="#808080", row=1, col=1)
fig.update_xaxes(griddash="dot", gridcolor="#808080", title_text="No. of lags", row=2, col=1)
fig.update_yaxes(griddash="dot", gridcolor="#808080", title_text="Autocorrelation", row=1, col=1)
fig.update_yaxes(griddash="dot", gridcolor="#808080", title_text="Partial Autocorrelation", row=2, col=1)
```

![](img/2db5e7d8a24a253430c45eadd4fb3031.png)

æ—¶é—´åºåˆ—çš„è‡ªç›¸å…³å’Œéƒ¨åˆ†è‡ªç›¸å…³å›¾ã€‚æ¥æºï¼šä½œè€…

æ•°æ®é›†å…·æœ‰å¾ˆé«˜çš„è‡ªç›¸å…³æ€§ï¼Œè¿™å¾ˆåˆç†ï¼Œå› ä¸ºå€¼å¤§å¤šåœ¨ 10K MW åˆ° 20K MW ä¹‹é—´æ³¢åŠ¨ï¼Œä¸”ä»ä¸€ä¸ªå°æ—¶åˆ°ä¸‹ä¸€ä¸ªå°æ—¶çš„è¿‡æ¸¡å¹³æ»‘ã€‚ç„¶è€Œï¼Œä¸“æ³¨äºä¸‹å›¾æ‰€ç¤ºçš„éƒ¨åˆ†è‡ªç›¸å…³æ€§ï¼Œä¼¼ä¹åœ¨ 24 å°æ—¶çš„å€æ•°ä»¥åŠæœ€åå‡ ä¸ªå°æ—¶ä¸­å­˜åœ¨æ˜¾è‘—çš„ç›¸å…³æ€§ã€‚å› æ­¤ï¼Œæ´¾ç”Ÿç‰¹å¾ä¸»è¦å¯ä»¥åˆ†ç±»ä¸ºï¼š

1.  æ¯æ—¥ï¼ˆ24 å°æ—¶çš„å€æ•°ï¼‰ï¼Œ

1.  æ¯å°æ—¶ï¼ˆä¸“æ³¨äºæœ€åå‡ ä¸ªå°æ—¶ï¼‰å’Œ

1.  åˆ†ç±»ç‰¹å¾

```py
dff = df.reset_index(drop=False)
dff["Datetime"] = pd.to_datetime(dff.Datetime.values)

# lags and difference of multiple days for capturing seasonal effects
for i in np.linspace(24, 15*24, 15, dtype=int):
    dff[f"lag_{i}"] = dff.AEP_MW.shift(i)
    dff[f"difference_{i}"] = dff.AEP_MW.diff(periods=i)

# rolling mean and standard deviation up to 3 days for capturing seasonal effects better
for i in np.linspace(24, 72, 3, dtype=int):
    dff[f"rolling_mean_{i}"] = dff.AEP_MW.rolling(window=i).mean()
    dff[f"rolling_std_{i}"] = dff.AEP_MW.rolling(window=i).std()

# lag, rolling mean, rolling standard deviation and difference up to 4 hours for capturing immediate effects
for i in range(2, 5, 1):
    dff[f"lag_{i}"] = dff.AEP_MW.shift(i)
    dff[f"rolling_mean_{i}"] = dff.AEP_MW.rolling(window=i).mean()
    dff[f"rolling_std_{i}"] = dff.AEP_MW.rolling(window=i).std()
    dff[f"difference_{i}"] = dff.AEP_MW.diff(periods=i)

# categorical features
dff["hour_of_day"] = dff.Datetime.dt.hour
dff["day_of_week"] = dff.Datetime.dt.day_of_week
dff["is_weekend"] = dff["day_of_week"].isin([5, 6]).astype(int)

# grouping derived features for later use in feature importance analysis
daily_lags = [col for col in dff.columns if all(["lag_" in col, len(col)>5])]
hourly_lags = [col for col in dff.columns if all(["lag_" in col, len(col)<=5])]
daily_differences = [col for col in dff.columns if all(["difference_" in col, len(col)>12])]
hourly_differences = [col for col in dff.columns if all(["difference_" in col, len(col)<=12])]
daily_rolling_means = [col for col in dff.columns if all(["rolling_mean_" in col, len(col)>14])]
hourly_rolling_means = [col for col in dff.columns if all(["rolling_mean_" in col, len(col)<=14])]
daily_rolling_stds = [col for col in dff.columns if all(["rolling_std_" in col, len(col)>13])]
hourly_rolling_stds = [col for col in dff.columns if all(["rolling_std_" in col, len(col)<=13])]
categoricals = ["hour_of_day", "day_of_week", "is_weekend"]
```

# **æ„å»º MLP æ¨¡å‹**

ç”Ÿæˆä¸Šè¿°è¯¦ç»†ç‰¹å¾åï¼Œè¾“å…¥å½¢çŠ¶å·²çŸ¥ï¼Œå¯ä»¥æ„å»º MLP æ¨¡å‹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¤„ç†çš„æ˜¯æ··åˆæ•°æ®ç±»å‹ï¼šæµ®åŠ¨å‹å’Œæ•´æ•°å‹ã€‚è¿˜è¯·æ³¨æ„ï¼Œå°½ç®¡æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œæ•´æ•°å‹è¾“å…¥æœ¬è´¨ä¸Šæ˜¯åˆ†ç±»ç‰¹å¾ï¼Œåº”å½“è§†ä¸ºåˆ†ç±»ç‰¹å¾æ¥å¤„ç†ã€‚

æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥ä½¿ç”¨ä¾‹å¦‚ç‹¬çƒ­ç¼–ç æŠ€æœ¯å¯¹ç±»åˆ«è¿›è¡Œç¼–ç ï¼Œä½†è¿™ä¼šæ˜¾è‘—å¢åŠ ç‰¹å¾çš„æ•°é‡ï¼Œå› ä¸ºæ¯ä¸ªç±»åˆ«åˆ—éƒ½åº”è¯¥æ‰©å±•ä¸ºä¸ç±»åˆ«æ•°ç›¸ç­‰çš„åˆ—æ•°ï¼ˆå‡å»ä¸€ä¸ªï¼‰[12]ã€‚æˆ‘æ•…æ„é€‰æ‹©äº†åµŒå…¥æ–¹æ³•ï¼Œä»¥é™åˆ¶ç‰¹å¾æ•°é‡ï¼Œè™½ç„¶è¿™æ ·åšä¼šä½¿å¾—æ¨¡å‹çš„è¾“å…¥å±‚æ›´åŠ å¤æ‚ï¼Œå› ä¸ºç±»åˆ«æ•°æ®é¦–å…ˆé€šè¿‡åµŒå…¥è½¬æ¢ä¸ºå‘é‡ï¼Œå†ä¸æµ®åŠ¨è¾“å…¥ç»“åˆã€‚

è¯·æŸ¥çœ‹ä»£ç éƒ¨åˆ†åçš„å›¾è¡¨ä»¥è·å¾—æ›´æ¸…æ™°çš„ç†è§£ã€‚è¯¥æ¶æ„æ˜¯ä½¿ç”¨ç»éªŒæ³•åˆ™æ„å»ºçš„ï¼Œå› ä¸ºè¶…å‚æ•°è°ƒä¼˜ä¸åœ¨æœ¬æ–‡èŒƒå›´å†…ã€‚ç„¶è€Œï¼Œå¦‚æœä½ å¯¹å¦‚ä½•è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜çš„é€šç”¨æ¡†æ¶æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹ğŸ“°â˜› [æˆ‘ä¹‹å‰çš„æ–‡ç« ](https://medium.com/towards-data-science/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d)ï¼ˆåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä½¿ç”¨ Optuna ä½œä¸ºè´å¶æ–¯æœç´¢å·¥å…·è°ƒä¼˜äº† XGBoost æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ï¼‰ã€‚

```py
# segmenting last year as test data
inputs = dff.dropna().iloc[:, 2:].columns
xs_train = dff.dropna().iloc[:-8766, 2:]
xs_test = dff.dropna().iloc[-8766:, 2:]
ys_train = dff.dropna().iloc[:-8766, 1]
ys_test = dff.dropna().iloc[-8766:, 1]
embedding_dim = 4       # potential hyperparameter

# defining baseline NN model
float_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name="float_inputs")           # floats can be directly used in model fitting
integer_inputs = tf.keras.layers.Input(shape=(3,), dtype="int32", name="integer_inputs")    # integers should be treated as categoricals ang get them embedded
embedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)          # embedding will be performed during model fitting
embedded_integer_inputs = embedding_layer(integer_inputs)
flattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)                   
preprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])    # float and embedded inputs are combined
hidden_layers = tf.keras.layers.Dense(units=64, activation="relu")(preprocessing_layers)    # No. of hidden layers, No. of units, activation function are potential hyperparameters
hidden_layers = tf.keras.layers.Dense(units=32, activation="relu")(hidden_layers)
output = tf.keras.layers.Dense(units=1, activation="linear")(hidden_layers)                 # single unit for one step ahead, multiple units for multiple step prediction
model_NN_baseline = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)

# compiling baseline NN model
model_NN_baseline.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError(),
    jit_compile=True)

# fitting baseline NN model
model_NN_baseline.fit(
    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],
    y=ys_train,
    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],
    epochs=128,
    batch_size=64,
    verbose=1
)
```

![](img/d1e93289f78adc2ad27fe54af42aa5d8.png)

ä½¿ç”¨ Tensorflow/Keras åˆ›å»ºçš„ MLP æ¶æ„ã€‚æ¥æºï¼šä½œè€…

å°±ç‚¹é¢„æµ‹è€Œè¨€ï¼Œç»“æœéå¸¸å‡†ç¡®ã€‚è¿™æ˜¯ä¸€ä¸ªå¥½å…†å¤´ï¼Œè¯´æ˜æ‰€åº”ç”¨çš„ç‰¹å¾å·¥ç¨‹åŸåˆ™æ­£ç¡®åœ°æ•æ‰äº†æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿå°†å…¶æ³›åŒ–ã€‚

![](img/181dd0c122d63276258e6f6f5576e070.png)

åŸºå‡† MLP æ¨¡å‹çš„ç‚¹é¢„æµ‹ä¸æµ‹è¯•æ•°æ®å¯¹æ¯”ã€‚æ¥æºï¼šä½œè€…

ç‚¹é¢„æµ‹ä¸æµ‹è¯•é›†é‡å ï¼Œä¸”ä¸¤ä¸ªå›¾å½¢è½¨è¿¹å‡ ä¹æ— æ³•åŒºåˆ†ã€‚æ›´ç²¾ç¡®åœ°è¯´ï¼Œé¢„æµ‹ï¼ˆè®­ç»ƒé›†ï¼‰å’Œé¢„æµ‹å€¼ï¼ˆæµ‹è¯•é›†ï¼‰çš„ RMSE åˆ†åˆ«çº¦ä¸º 19.3 å’Œ 18.9ï¼ˆç›¸å¯¹è¯¯å·®çº¦ä¸º 0.1%ï¼‰ã€‚

# **ç‰¹å¾é‡è¦æ€§**

æ˜¯ä»€ä¹ˆä½¿å¾—æ¨¡å‹å‡†ç¡®ï¼Ÿæ‰€æœ‰æ´¾ç”Ÿç‰¹å¾æ˜¯å¦åŒæ ·é‡è¦ï¼Œè¿˜æ˜¯æœ‰ä¸€ä¸ªå­é›†åœ¨å†³å®šç»“æœæ—¶å…·æœ‰æ›´å¤§çš„æƒé‡ï¼Ÿè¿™ä¸¤ä¸ªé—®é¢˜æœ‰å…¶æœ‰æ•ˆæ€§ï¼ŒåŸå› æœ‰äºŒï¼š

1.  åœ¨å®é™…åœºæ™¯ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ•°æ®æƒ…å†µä¸‹ï¼Œè®­ç»ƒæ¨¡å‹çš„èµ„æºæœ‰é™ï¼Œæ‰€ä½¿ç”¨çš„æ•°æ®é‡å¯èƒ½å¯¹æ˜¯å¦èƒ½å¤Ÿè®­ç»ƒæ¨¡å‹äº§ç”Ÿé‡å¤§å½±å“ã€‚

1.  å¦‚æœæ²¡æœ‰ä»»ä½•è§£é‡Šï¼Œæ¨¡å‹å°±åƒä¸€ä¸ªé»‘ç®±ï¼Œè¿™ä¼šå¸¦æ¥å…³äºå…¶æ€§èƒ½çš„ä¸ç¡®å®šæ€§ã€‚ç¥ç»ç½‘ç»œå°¤å…¶å®¹æ˜“æˆä¸ºé»‘ç®±æ¨¡å‹ï¼Œè§£é‡Šå®ƒä»¬æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¢†åŸŸ[11]ã€‚

ç›®å‰æœ‰å¤§é‡çš„æ¨¡å‹è§£é‡ŠæŠ€æœ¯ï¼Œæ¯ç§æ–¹æ³•éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚æˆ‘é€‰æ‹©äº†æ’åˆ—ç‰¹å¾é‡è¦æ€§æ–¹æ³•ï¼Œä»¥ä¾¿ä¸ºæ¨¡å‹è§£é‡Šæä¾›ä¸€äº›è§è§£ã€‚ç„¶è€Œï¼Œæˆ‘åˆ†æä¸­çš„ä¸€ä¸ªå…³é”®ç»“è®ºæ˜¯ï¼Œ

> æ¨¡å‹è§£é‡ŠæŠ€æœ¯ä»…ä»…æ˜¯åœ¨ç‰¹å®šèŒƒå›´å†…è§£é‡Šæ¨¡å‹ï¼Œè€Œä¸ä¸€å®šæ˜¯è§£é‡Šå…¶èƒŒåçš„è¿‡ç¨‹ã€‚ç°å®å¯èƒ½ä¸ç‰¹å¾é‡è¦æ€§åˆ†æå¤§ç›¸å¾„åº­ï¼Œå› æ­¤ä¸åº”å°†å…¶è§†ä¸ºè‡ªå˜é‡ä¸ç›®æ ‡å˜é‡ä¹‹é—´å› æœå…³ç³»çš„æœ€ç»ˆçœŸç›¸ã€‚

è®©æˆ‘ç”¨æˆ‘çš„åˆ†æç»“æœæ¥è§£é‡Šè¿™ä¸€ç‚¹ã€‚é€ä¸€ç½®æ¢ç‰¹å¾ï¼Œé‡æ–°è®¡ç®— RMSE å¾—åˆ†å¹¶è®°å½•ç›¸å¯¹äºä½¿ç”¨åŸå§‹æ•°æ®çš„é¢„æµ‹ RMSE çš„ç›¸å¯¹å˜åŒ–ï¼Œå°†ç»™å‡ºç‰¹å¾çš„ç›¸å¯¹é‡è¦æ€§[13]ã€‚

```py
# permutation feature importance
features = xs_test.columns
permutation_importance_results = {}
rmse = tf.keras.metrics.RootMeanSquaredError()
rmse_permuted = tf.keras.metrics.RootMeanSquaredError()
rmse.update_state(ys_test.values, model_NN_baseline.predict([xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], verbose=0).flatten())

for feature in features:

    xs_test_permuted = xs_test.copy()
    xs_test_permuted.loc[:, feature] = xs_test.loc[:, feature].sample(frac=1, axis=0, replace=False, random_state=42).values

    rmse_permuted.reset_state()
    rmse_permuted.update_state(ys_test.values, model_NN_baseline.predict([xs_test_permuted.iloc[:, :-3], xs_test_permuted.iloc[:, -3:]], verbose=0).flatten())

    permutation_importance_results[feature] = rmse_permuted.result().numpy() / rmse.result().numpy()

pi_results_sorted_keys = sorted(permutation_importance_results, key=permutation_importance_results.get, reverse=True)

fig3 = make_subplots()
fig3.add_trace(go.Bar(
    x=pi_results_sorted_keys,
    y=[permutation_importance_results[key] for key in pi_results_sorted_keys]))
fig3.update_layout(
    title="<b>Permutation Feature Importance</b>",
    font=dict(family="Arial"),
    margin=dict(b=4, l=4, r=4, t=36),
    showlegend=False,
    height=500,
    paper_bgcolor="#FFFFFF",
    plot_bgcolor="#FFFFFF"
)
fig3.update_xaxes(griddash="dot", gridcolor="#808080", row=1, col=1)
fig3.update_yaxes(griddash="dot", gridcolor="#808080", row=1, col=1)
```

![](img/2564e8b61f4fb48c7d19a4b671007da9.png)

ç½®æ¢ç‰¹å¾é‡è¦æ€§ç›´æ–¹å›¾ã€‚æ¥æºï¼šä½œè€…

æ¯å°æ—¶å’Œæ¯æ—¥æ»åä»¥åŠå·®å¼‚ä¼¼ä¹å¾ˆé‡è¦ï¼Œä¹Ÿè®¸æ¯å°æ—¶çš„æ»šåŠ¨å‡å€¼ä¹Ÿå¾ˆé‡è¦ã€‚ç„¶è€Œï¼Œæ¯æ—¥å’Œæ¯å°æ—¶æ»šåŠ¨æ ‡å‡†ä»¥åŠåˆ†ç±»ç‰¹å¾ä¼¼ä¹ç›¸å¯¹è¾ƒå°ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ï¼Œä¸ä¸Šè¿°ç‰¹å¾ç›¸æ¯”ã€‚ç½®æ¢ç‰¹å¾é‡è¦æ€§çš„ä¸€é¡¹è­¦å‘Šæ˜¯ï¼Œå®ƒæ²¡æœ‰è€ƒè™‘å¤šé‡å…±çº¿æ€§ï¼Œå› æ­¤å¯èƒ½ä¼šç»™å‡ºä¸å‡†ç¡®çš„ç»“æœã€‚è¯·è®°ä½ï¼Œè¿™äº›ç‰¹å¾æ˜¯ä»å…·æœ‰é«˜è‡ªç›¸å…³çš„æ•°æ®é›†ä¸­æ¨å¯¼å‡ºæ¥çš„ã€‚

å¤„ç†è¿™ç§æƒ…å†µçš„ä¸€ç§å¯èƒ½æ–¹å¼æ˜¯éµå¾ª`scikit learn`çš„æŒ‡å¯¼ï¼š

> å¯¹ Spearman ç­‰çº§é¡ºåºç›¸å…³æ€§æ‰§è¡Œå±‚æ¬¡èšç±»ï¼Œé€‰æ‹©ä¸€ä¸ªé˜ˆå€¼ï¼Œå¹¶ä»æ¯ä¸ªç°‡ä¸­ä¿ç•™ä¸€ä¸ªç‰¹å¾ã€‚[13]

ç„¶è€Œï¼Œæˆ‘æƒ³ä¸“æ³¨äºçªå‡ºä¸å‡†ç¡®ä¹‹å¤„ï¼Œå¹¶é€šè¿‡é€ä¸€è®­ç»ƒæ›¿ä»£æ¨¡å‹ä»¥åˆ†ç»„ç‰¹å¾æ¥ä¸ºæ•°æ®é›†æ·»åŠ æ›´å¤šæ´è§ã€‚ä¸ºæ­¤ä½¿ç”¨äº†ç›¸åŒçš„ MLP æ¶æ„ï¼Œä»…å¯¹è¾“å…¥å±‚è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥é€‚åº”æ•°æ®çš„å­é›†ã€‚ä»¥ä¸‹ç»„åœ¨ç‰¹å¾å·¥ç¨‹éƒ¨åˆ†åˆ›å»ºå¹¶åœ¨æ­¤æµ‹è¯•ï¼ˆè®­ç»ƒ/æµ‹è¯•æ•°æ®é›†çš„ RMSE ç»“æœä¹Ÿåˆ†åˆ«æŠ¥å‘Šï¼‰ï¼š

1.  æ¯æ—¥æ»åï¼ˆ942 å’Œ 994ï¼‰

1.  æ¯æ—¥å·®å¼‚ï¼ˆ1792 å’Œ 1952ï¼‰

1.  æ¯å°æ—¶æ»åï¼ˆ686 å’Œ 611ï¼‰

1.  æ—¥å¸¸æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆ1710 å’Œ 1663ï¼‰

1.  æ¯å°æ—¶æ»šåŠ¨å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆ84.4 å’Œ 75.5ï¼‰

æ˜¾ç„¶ï¼Œæ›¿ä»£æ¨¡å‹æ˜¾ç¤ºçš„ç»“æœä¸ç®€å•çš„ç½®æ¢ç‰¹å¾é‡è¦æ€§åˆ†æé¢„æœŸä¸åŒï¼Œä¸”æœªå¤„ç†å¤šé‡å…±çº¿æ€§ï¼šä¾‹å¦‚ï¼Œæ¯æ—¥æ»šåŠ¨ç‰¹å¾çš„å¾—åˆ†ä¼˜äºæ¯æ—¥å·®å¼‚ï¼Œä¸”è®­ç»ƒäºæ¯å°æ—¶æ»šåŠ¨ç‰¹å¾çš„æ¨¡å‹åœ¨æ‰€æœ‰æ›¿ä»£æ¨¡å‹ä¸­è¡¨ç°æœ€ä½³ï¼Œæ¥è¿‘åŸºçº¿æ¨¡å‹ï¼ˆRMSE åˆ†åˆ«ä¸ºç™¾åˆ†æ¯”~0.5%å’Œ~0.1%ï¼‰ã€‚

# æ•°æ®ä¸­çš„ç‰¹å®šå¼‚å¸¸è¯´æ˜

æˆ‘æƒ³å¼ºè°ƒ 2008 å¹´ 10 æœˆ 20 æ—¥ 14:00 è§‚å¯Ÿåˆ°çš„ä¸€ä¸ªéå¸¸ç‰¹æ®Šçš„å¼‚å¸¸æƒ…å†µã€‚ è¿™æ˜¯æœ‰å²ä»¥æ¥è®°å½•çš„æœ€é«˜å€¼ï¼Œä¸”æ²¡æœ‰æ˜æ˜¾çš„åŸå› ï¼Œæ•°æ®é›†ä¸­ä¹‹å‰å’Œä¹‹åæ²¡æœ‰ç±»ä¼¼çš„æ•°æ®ç‚¹ã€‚

> ç„¶è€Œï¼Œç”±ç‰¹å¾å·¥ç¨‹é©±åŠ¨çš„åŸºçº¿æ¨¡å‹èƒ½å¤Ÿé¢„æµ‹è¯¥æ•°æ®ç‚¹ï¼Œå¹¶ä¸”ä¸è¢«è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼ï¼

![](img/fe5663c1e874d9c4bcbf0d8dd541ab3c.png)

åŸºçº¿ MLP æ¨¡å‹çš„ç‚¹é¢„æµ‹å’Œè§‚å¯Ÿåˆ°çš„æ½œåœ¨å¼‚å¸¸ã€‚æ¥æºï¼šä½œè€…

æ¨¡å‹æ˜¯å¦‚ä½•é¢„æµ‹è¯¥æ•°æ®ç‚¹çš„å‘¢ï¼Ÿè®©æˆ‘ä»¬ä½¿ç”¨æ›¿ä»£æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚æœ€ä½³çš„æ›¿ä»£æ¨¡å‹ï¼ˆæ¯å°æ—¶æ»šåŠ¨ç‰¹å¾ï¼‰åœ¨è¯¥ç‚¹é™„è¿‘ä¼¼ä¹éå¸¸å‡†ç¡®ï¼Œä½†åªèƒ½éƒ¨åˆ†è§£é‡Šè¿™ä¸€ç°è±¡ï¼š

![](img/1d891089cfb4b6494f8ac7e5afc5959e.png)

æ›¿ä»£çš„ MLP æ¨¡å‹ï¼ˆåˆ©ç”¨æ¯å°æ—¶æ»šåŠ¨ç‰¹å¾ï¼‰ç‚¹é¢„æµ‹å’Œè§‚å¯Ÿåˆ°çš„æ½œåœ¨å¼‚å¸¸ã€‚æ¥æºï¼šä½œè€…

ç¬¬äºŒå¥½çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯åˆ©ç”¨æ¯å°æ—¶æ»åçš„æ¨¡å‹ï¼Œä½†å®ƒå®Œå…¨æ²¡æœ‰è§£é‡Šä¸ºä½•ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µï¼š

![](img/4d699078a9afb2b07b740af85194b30d.png)

æ›¿ä»£çš„ MLP æ¨¡å‹ï¼ˆåˆ©ç”¨æ¯å°æ—¶æ»åç‰¹å¾ï¼‰ç‚¹é¢„æµ‹å’Œè§‚å¯Ÿåˆ°çš„æ½œåœ¨å¼‚å¸¸ã€‚æ¥æºï¼šä½œè€…

ç®€è€Œè¨€ä¹‹ï¼Œæ¯æ—¥å·®å¼‚å¯èƒ½åŒ…å«æœ‰å…³æ½œåœ¨æ¨¡å¼çš„é‡è¦ä¿¡æ¯ã€‚å°½ç®¡å•ç‹¬ä½¿ç”¨æ¯æ—¥å·®å¼‚ç»„ä¼šç»™å‡ºæ›´é«˜çš„é¢„æµ‹å€¼ï¼Œä½†åŸºå‡†æ¨¡å‹ä¼¼ä¹æ‰¾åˆ°äº†ç‰¹å¾æƒé‡çš„è‰¯å¥½å¹³è¡¡ã€‚

![](img/c0f627ebf8eb29789a55dda1e01f9faf.png)

æ›¿ä»£çš„ MLP æ¨¡å‹ï¼ˆåˆ©ç”¨æ¯æ—¥å·®å¼‚ç‰¹å¾ï¼‰ç‚¹é¢„æµ‹å’Œè§‚å¯Ÿåˆ°çš„æ½œåœ¨å¼‚å¸¸ã€‚æ¥æºï¼šä½œè€…

# å¤šæ­¥é¢„æµ‹æ¨¡å‹

æœ€åï¼Œæ¨¡å‹æ¶æ„å·²è¢«ä¿®æ”¹ï¼Œä»¥ç”Ÿæˆå¤šæ­¥é¢„æµ‹ã€‚é¢„æµ‹æœŸä¸ºä¸€å¹´ï¼Œå¦‚æ•°æ®é›†å‘å¸ƒè€…æ‰€å»ºè®®çš„[14]ã€‚è€ƒè™‘åˆ°è¿™ç§è¿‡ç¨‹ä¸­çš„æ‰€æœ‰ä¸ç¡®å®šæ€§ï¼Œç‰¹åˆ«æ˜¯å¤©æ°”æ¡ä»¶æ–¹é¢ï¼Œè€ƒè™‘è¿™ä¹ˆé•¿çš„é¢„æµ‹æœŸå¯èƒ½æ²¡æœ‰æ„ä¹‰ã€‚ç„¶è€Œï¼Œè¿™å¯¹äºè¯„ä¼°å¤šæ­¥æ¨¡å‹ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹çš„è¡¨ç°æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç»ƒä¹ ï¼Œåè€…æ˜ç¡®å»ºæ¨¡äº†è·¨å¹´è§‚å¯Ÿåˆ°çš„è¶‹åŠ¿å’Œå­£èŠ‚æ€§æ•ˆåº”ï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰ã€‚

å®ç°å¤šæ­¥æ¨¡å‹çš„å…³é”®ç‚¹å¦‚ä¸‹ï¼š

1.  ç›®æ ‡æ˜¯ä¸€ä¸ªå‘é‡åºåˆ—ï¼ˆä¸ºæ¯ä¸ªæ­¥éª¤å®šä¹‰çš„æ¥ä¸‹æ¥çš„ 8766 å°æ—¶ï¼‰

1.  ç»“æœæ˜¯ï¼Œé¢„æµ‹æˆ–é¢„æŠ¥æ˜¯æ¥ä¸‹æ¥ 8766 å°æ—¶ï¼ˆå¤§çº¦ä¸€å¹´ï¼‰çš„æœ€åä¸€è¡Œè¾“å…¥æ•°æ®

1.  ç”±äºèµ„æºé™åˆ¶ï¼Œæˆ‘ä¸å¾—ä¸é™åˆ¶å‰ä¸€è®­ç»ƒæ•°æ®é›†çš„æœ€åä¸€å¹´çš„è®­ç»ƒæ•°æ®ã€‚

1.  è¾“å‡ºå±‚å·²ç›¸åº”ä¿®æ”¹ï¼Œä»¥ç»™å‡ºæ‰€éœ€çš„å‘é‡è¾“å‡º

```py
first_index = -8766*5
last_index = -8766*2
final_index = -8766
inputs = dff.dropna().iloc[:, 2:].columns
xs_train = dff.dropna().iloc[first_index:last_index, 2:]
xs_train.iloc[:, :-3] = xs_train.iloc[:, :-3].astype(np.float32)
xs_test = dff.dropna().iloc[last_index:final_index, 2:]
xs_test.iloc[:, :-3] = xs_test.iloc[:, :-3].astype(np.float32)
ys_train = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(first_index, last_index, 1)])
ys_test = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(last_index, final_index, 1)])
embedding_dim = 4

# defining, compiling and training NN model for MULTIPLE STEP PREDICTIONS. Model architecture is the same, except output layer
float_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name="float_inputs")
integer_inputs = tf.keras.layers.Input(shape=(3,), dtype="int32", name="integer_inputs")
embedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)
embedded_integer_inputs = embedding_layer(integer_inputs)
flattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)
preprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])
hidden_layers = tf.keras.layers.Dense(units=64, activation="relu")(preprocessing_layers)
hidden_layers = tf.keras.layers.Dense(units=32, activation="relu")(hidden_layers)
output = tf.keras.layers.Dense(units=np.abs(final_index)-1, activation="linear")(hidden_layers)

model_NN_multistep = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)
model_NN_multistep.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.MeanSquaredError(),
    jit_compile=True)
model_NN_multistep.fit(
    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],
    y=ys_train,
    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],
    epochs=128,
    batch_size=64,
    verbose=1
)
```

å¯¹äºå¯è§†åŒ–è¯„ä¼°ï¼Œå¯ä»¥çœ‹å‡ºæ¨¡å‹è¯•å›¾å¯¹æ¨¡å¼è¿›è¡Œæ³›åŒ–ï¼š

![](img/2c6c7c1d334527dfc742b07d89445916.png)

å¤šæ­¥ MLP æ¨¡å‹çš„é¢„æµ‹ä¸åŸå§‹æ•°æ®çš„å¯¹æ¯”ã€‚æ¥æºï¼šä½œè€…

# MLP ä¸çŠ¶æ€ç©ºé—´æ¨¡å‹

ç”±äºæ•°æ®çš„æ³›åŒ–ï¼ŒRMSE å¾—åˆ†æ˜¾è‘—å¢åŠ ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å¾—åˆ†åˆ†åˆ«ä¸º 1982 å’Œ 2017ã€‚ç„¶è€Œï¼Œä¸ºäº†æ­£ç¡®è¯„ä¼°å¤šæ­¥ MLPï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚æ­£å¦‚æˆ‘åœ¨å‰ä¸€èŠ‚ä¸­æåˆ°çš„ï¼ŒçŠ¶æ€ç©ºé—´æ¨¡å‹æä¾›äº†å¯¹è·¨å¹´è§‚å¯Ÿåˆ°çš„è¶‹åŠ¿å’Œå­£èŠ‚æ€§æ•ˆåº”çš„ç›¸å½“å¯ç†è§£çš„è¿‘ä¼¼ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿å¾—å®ƒä»¬ç›¸å¯¹å®¹æ˜“è§£é‡Šï¼Œä¸åƒç¥ç»ç½‘ç»œã€‚ä¸»è¦åŸå› æ˜¯éšè—å±‚æœ‰å¾ˆå¤šè¿æ¥ï¼Œç†è§£å®ƒä»¬æ˜¯å¦‚ä½•è¢«æ¿€æ´»çš„å¹¶ä¸æ˜¯ä¸€ä¸ªç›´æ¥çš„è¿‡ç¨‹ã€‚[11]

åœ¨[æˆ‘ä¹‹å‰çš„æ–‡ç« ](https://medium.com/analytics-vidhya/multi-seasonal-time-series-analysis-decomposition-and-forecasting-with-python-609409570007)ä¸­ï¼Œâ˜šğŸ“°æˆ‘ä½¿ç”¨äº†ä¸€ç§ç®€åŒ–ä½†æœ‰æ„ä¹‰çš„è¯„ä¼°æ–¹æ³•ï¼šæ¯”è¾ƒè¿‡å»ä¸€å¹´å†…çš„æ€»èƒ½è€—ã€‚å®é™…ä¸Šï¼Œè¿™æ˜¯èƒ½è€—æ—¶é—´åºåˆ—ä¸‹çš„æ›²çº¿é¢ç§¯ã€‚å¯ä»¥ç›´æ¥æ¯”è¾ƒåŸå§‹æ•°æ®å’Œæ¨¡å‹é¢„æµ‹çš„å€¼ã€‚å¯¹äº`UnobservedComponents`æ¨¡å‹ï¼š

```py
y_train = df.iloc[:-8766, 0].values
y_test = df.iloc[-8766:, 0].values
observed_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]
forecast = model_results.forecast(steps=8766)
UC_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]

# calculating absolute and percentage error of forecast integral compared to observed integral
fcast_integral_abserror = UC_integral - observed_integral
fcast_integral_perror4 = (UC_integral - observed_integral) * 100 / observed_integral

print(f"Observed yearly energy demand: {'%.3e' % observed_integral} MWh")
print(f"Forecast yearly energy demand: {'%.3e' % UC_integral} MWh")
print(f"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %")
```

```py
Observed yearly energy demand: 1.312e+08 MWh
Forecast yearly energy demand: 1.283e+08 MWh
Forecast error of yearly energy demand: -2.832e+06 MWh or -2.159 %
```

å¯¹äº MLP æ¨¡å‹ï¼š

```py
y_test = dff.dropna().iloc[-8766:-1, 1].values
observed_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]
forecast = model_NN_multistep.predict([xs_test.iloc[-1:, :-3], xs_test.iloc[-1:, -3:]], verbose=0).flatten()
model_NN_multistep_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]

# calculating absolute and percentage error of forecast integral compared to observed integral
fcast_integral_abserror = model_NN_multistep_integral - observed_integral
fcast_integral_perror4 = (model_NN_multistep_integral - observed_integral) * 100 / observed_integral

print(f"Observed yearly energy demand: {'%.3e' % observed_integral} MWh")
print(f"Forecast yearly energy demand: {'%.3e' % model_NN_multistep_integral} MWh")
print(f"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %")
```

```py
Observed yearly energy demand: 1.312e+08 MWh
Forecast yearly energy demand: 1.286e+08 MWh
Forecast error of yearly energy demand: -2.508e+06 MWh or -1.912 %
```

ç®€è€Œè¨€ä¹‹ï¼šå®ƒæ˜¯-1.912% vs. -2.159%ï¼Œåå‘äº MLP æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯é€šè¿‡ä½¿ç”¨ MLP æ¶æ„å¹¶ç»“åˆä¸€äº›ç®€å•çš„ç»éªŒæ³•åˆ™å®ç°çš„ï¼Œç”šè‡³æ²¡æœ‰è€ƒè™‘è¶…å‚æ•°è°ƒä¼˜æˆ–æŸäº›æœ‰æ•ˆçš„æ¨¡å‹è®­ç»ƒç‰¹å¾ï¼Œä¾‹å¦‚åœ¨è¯„ä¼°æŒ‡æ ‡è¾¾åˆ°å¹³å°æœŸæ—¶å‡å°‘å­¦ä¹ ç‡æˆ–æå‰åœæ­¢ã€‚

ç»“æœåº”è¯¥æ˜¯ç›¸å½“ä»¤äººä¿¡æœçš„ï¼Œç¡®å®ï¼Œé€šè¿‡åˆ©ç”¨ç›¸å¯¹ç®€å•çš„ç¥ç»ç½‘ç»œæ¶æ„ç»“åˆå¼ºå¤§çš„ç‰¹å¾å·¥ç¨‹æŠ€æœ¯ï¼Œå‡†ç¡®çš„é¢„æµ‹å·¥å…·å·²ç»åœ¨æ•°æ®ç§‘å­¦å®¶çš„åˆçº§é˜¶æ®µè§¦æ‰‹å¯å¾—ã€‚

# èµ„æº

æ•°æ®æ¥æºï¼š

[`www.kaggle.com/datasets/robikscube/hourly-energy-consumption/`](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/) (CC0)

ç¬”è®°æœ¬ï¼ˆä»…ä»£ç ï¼Œä¸åŒ…å«è¾“å‡ºï¼‰ï¼š[`gist.github.com/danielandthelions/2e6f0edd30902113ad10fd9f20bda215`](https://gist.github.com/danielandthelions/2e6f0edd30902113ad10fd9f20bda215)

# å‚è€ƒæ–‡çŒ®

[1] [`preset.io/blog/time-series-forecasting-a-complete-guide/`](https://preset.io/blog/time-series-forecasting-a-complete-guide/)

[2] [`www.ibm.com/topics/recurrent-neural-networks`](https://www.ibm.com/topics/recurrent-neural-networks)

[3] [`www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it/`](https://www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it/)

[4] [`plat.ai/blog/difference-between-prediction-and-forecast/`](https://plat.ai/blog/difference-between-prediction-and-forecast/)

[5] [`dotdata.com/blog/practical-guide-for-feature-engineering-of-time-series-data/`](https://dotdata.com/blog/practical-guide-for-feature-engineering-of-time-series-data/)

[6] [`statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/`](https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/)

[7] [`www.sciencedirect.com/topics/computer-science/multilayer-perceptron`](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron)

[8] [`jina.ai/news/embeddings-in-depth/`](https://jina.ai/news/embeddings-in-depth/)

[9] Hyndman, R.J., & Athanasopoulos, G. (2021) ã€ŠForecasting: principles and practiceã€‹ï¼Œç¬¬ä¸‰ç‰ˆï¼ŒOTextsï¼šæ¾³å¤§åˆ©äºšå¢¨å°”æœ¬ã€‚OTexts.com/fpp3ã€‚è®¿é—®æ—¶é—´ï¼š2024 å¹´ 7 æœˆ 7 æ—¥

[10] [`statisticsbyjim.com/regression/root-mean-square-error-rmse/`](https://statisticsbyjim.com/regression/root-mean-square-error-rmse/)

[11] [`christophm.github.io/interpretable-ml-book/`](https://christophm.github.io/interpretable-ml-book/)

[12] [`scikit-learn.org/stable/modules/preprocessing.html`](https://scikit-learn.org/stable/modules/preprocessing.html)

[13] [`scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance`](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance)

[14] [`www.kaggle.com/datasets/robikscube/hourly-energy-consumption/`](https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/)
