<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Needle In a Haystack Test</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Needle In a Haystack Test</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38?source=collection_archive---------1-----------------------#2024-02-15">https://towardsdatascience.com/the-needle-in-a-haystack-test-a94974c1ad38?source=collection_archive---------1-----------------------#2024-02-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/5d3dd87eaedfaa9810c9a5f571ce002f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYDN7bgP6r3q_2EsNdQPNA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image created by author using Dall-E 3</figcaption></figure><div/><div><h2 id="cef5" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Evaluating the performance of RAG systems</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://aparnadhinak.medium.com/?source=post_page---byline--a94974c1ad38--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Aparna Dhinakaran" class="l ep by dd de cx" src="../Images/e431ee69563ecb27c86f3428ba53574c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbKXdndNnweCZQQa2TohWw.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a94974c1ad38--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://aparnadhinak.medium.com/?source=post_page---byline--a94974c1ad38--------------------------------" rel="noopener follow">Aparna Dhinakaran</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a94974c1ad38--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lu lv ab q ee lw lx" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ly"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="lz k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ma an ao ap ix mb mc md" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep me cn"><div class="l ae"><div class="ab cb"><div class="mf mg mh mi mj gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ma an ao ap ix mk ml lx mm mn mo mp mq s mr ms mt mu mv mw mx u my mz na"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1b65" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">My thanks to </em><a class="af ny" href="https://twitter.com/GregKamradt" rel="noopener ugc nofollow" target="_blank"><em class="nx">Greg Kamradt</em></a><em class="nx"> and </em><a class="af ny" href="https://www.linkedin.com/in/evanjolley/" rel="noopener ugc nofollow" target="_blank"><em class="nx">Evan Jolley</em></a><em class="nx"> for their contributions to this piece</em></p><p id="e08b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Retrieval-augmented generation (RAG) underpins many of the LLM applications in the real world today, from companies generating headlines to solo developers solving problems for small businesses.</p><p id="bea6" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ny" href="https://arize.com/blog-course/rag-evaluation/" rel="noopener ugc nofollow" target="_blank">RAG evaluation</a>, therefore, has become a critical part in the development and deployment of these systems. One new innovative approach to this challenge is the “Needle in a Haystack’’ test, first outlined by <a class="af ny" href="https://twitter.com/GregKamradt" rel="noopener ugc nofollow" target="_blank">Greg Kamradt</a> in <a class="af ny" href="https://twitter.com/GregKamradt/status/1722386725635580292?lang=en" rel="noopener ugc nofollow" target="_blank">this X post</a> and discussed in detail on his YouTube <a class="af ny" href="https://www.youtube.com/watch?v=KwRRuiCCdmc" rel="noopener ugc nofollow" target="_blank">here</a>. This test is designed to evaluate the performance of RAG systems across different sizes of context. It works by embedding specific, targeted information (the “needle”) within a larger, more complex body of text (the “haystack”). The goal is to assess an LLM’s ability to identify and utilize this specific piece of information amidst a vast amount of data.</p><p id="f961" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Often in RAG systems, the context window is absolutely overflowing with information. Large pieces of context returned from a vector database are cluttered together with instructions for the language model, templating, and anything else that might exist in the prompt. The Needle in a Haystack evaluation tests the capabilities of an LLM to pinpoint specifics in amongst this mess. Your RAG system might do a stellar job of retrieving the most relevant context, but what use is this if the granular specifics within are overlooked?</p><p id="e03b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We ran this test multiple times across several major language models. Let’s take a closer look at the process and overall results.</p><h1 id="63c8" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Takeaways</h1><ul class=""><li id="39c5" class="nb nc gk nd b hi ov nf ng hl ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pa pb pc bk">Not all LLMs are the same. Models are trained with different objectives and requirements in mind. For example, Anthropic’s Claude is known for being a slightly wordier model, which often stems from its objective to not make unsubstantiated claims.</li><li id="8f5c" class="nb nc gk nd b hi pd nf ng hl pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk"><strong class="nd gl">Minute differences in prompts can lead to drastically different outcomes </strong>across models due to this fact. Some LLMs need more tailored prompting to perform well at specific tasks.</li><li id="4072" class="nb nc gk nd b hi pd nf ng hl pe ni nj nk pf nm nn no pg nq nr ns ph nu nv nw pa pb pc bk">When building on top of LLMs — especially when those models are connected to private data — it is necessary to <strong class="nd gl">evaluate retrieval and model performance throughout development and deployment</strong>. Seemingly insignificant differences can lead to incredibly large differences in performance.</li></ul><h1 id="2524" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Understanding the Needle In a Haystack Test</h1><p id="b705" class="pw-post-body-paragraph nb nc gk nd b hi ov nf ng hl ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The Needle in a Haystack test was first used to evaluate the recall of two popular LLMs, OpenAI’s ChatGPT-4 and Anthropic’s Claude 2.1. An out of place statement, “The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day,” was placed at varying depths within snippets of varying lengths taken from <a class="af ny" href="https://paulgraham.com/articles.html" rel="noopener ugc nofollow" target="_blank">essays</a> by Paul Graham, similar to this:</p><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pi"><img src="../Images/fda17ae9412103a6f11920bd500ff0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C2z-ZzomXVkNpUl_"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx"><em class="po">Figure 1: About 120 tokens and 50% depth</em> | Image by <a class="af ny" href="https://twitter.com/GregKamradt" rel="noopener ugc nofollow" target="_blank">Greg Kamradt</a> on <a class="af ny" href="https://twitter.com/GregKamradt/status/1722386725635580292" rel="noopener ugc nofollow" target="_blank">X</a>, used here with author’s permission</figcaption></figure><p id="7bb2" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The models were then prompted to answer what the best thing to do in San Francisco was, only using the provided context. This was then repeated for different depths between 0% (top of document) and 100% (bottom of document) and different context lengths between 1K tokens and the token limit of each model (128k for GPT-4 and 200k for Claude 2.1). The below graphs document the performance of these two models:</p><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/6016bcb61c2d84a0d250f1bd99bc4857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xn9RZT28LvcXPkbx"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 2: ChatGPT-4’s performance | Image by <a class="af ny" href="https://twitter.com/GregKamradt" rel="noopener ugc nofollow" target="_blank">Greg Kamradt</a> on <a class="af ny" href="https://twitter.com/GregKamradt/status/1722386725635580292" rel="noopener ugc nofollow" target="_blank">X</a>, used here with author’s permission</figcaption></figure><p id="22bf" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As you can see, ChatGPT’s performance begins to decline at &lt;64k tokens and sharply falls at 100k and over. Interestingly, if the “needle” is positioned towards the beginning of the context, the model tends to overlook or “forget” it — whereas if it’s placed towards the end or as the very first sentence, the model’s performance remains solid.</p><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/32ac9bfcaf7e2b20842a4d4b61596967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AzJEEgsEA_PraVCz"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 3: Claude 2.1’s performance | | Image by <a class="af ny" href="https://twitter.com/GregKamradt" rel="noopener ugc nofollow" target="_blank">Greg Kamradt</a> on <a class="af ny" href="https://twitter.com/GregKamradt/status/1722386725635580292" rel="noopener ugc nofollow" target="_blank">X</a>, used here with author’s permission</figcaption></figure><p id="1062" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For Claude, initial testing did not go as smoothly, finishing with an overall score of 27% retrieval accuracy. A similar phenomenon was observed with performance declining as context length increased, performance generally increasing as the needle was hidden closer to the bottom of the document, and 100% accuracy retrieval if the needle was the first sentence of the context.</p><h2 id="4b7e" class="pq oa gk bf ob pr ps pt oe pu pv pw oh nk px py pz no qa qb qc ns qd qe qf qg bk">Anthropic’s Response</h2><p id="0043" class="pw-post-body-paragraph nb nc gk nd b hi ov nf ng hl ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In response to these findings, Anthropic published an <a class="af ny" href="https://www.anthropic.com/news/claude-2-1-prompting" rel="noopener ugc nofollow" target="_blank">article</a> detailing their re-run of this test with a few key changes.</p><p id="1ad9" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First, they changed the needle to more closely mirror the topic of the haystack. Claude 2.1 was trained to “not [answer] a question based on a document if it doesn’t contain enough information to justify that answer.” Thus, Claude may well have correctly identified eating a sandwich in Dolores Park as the best thing to do in San Francisco. However, along with an essay about doing great work, this small piece of information may have appeared unsubstantiated. This could have led to a verbose response explaining that Claude cannot confirm that eating a sandwich is the best thing to do in San Francisco or an omission of the detail entirely. When re-running the experiments, researchers at Anthropic found that changing the needle to a small detail originally mentioned in the essay led to significantly increased outcomes.</p><p id="f0a4" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Second, a small edit was made to the prompt template used to query the model. A single line — <em class="nx">here is the most relevant sentence in the context</em> — was added to the end of the template, directing the model to simply return the most relevant sentence provided in the context. Similar to the first, this change allows us to circumvent the model’s propensity to avoid unsubstantiated claims by directing it to simply return a sentence rather than make an assertion.</p><pre class="pj pk pl pm pn qh qi qj bp qk bb bk"><span id="78a5" class="ql oa gk qi b bg qm qn l qo qp">PROMPT = """<br/><br/>HUMAN: &lt;context&gt;<br/>{context}<br/>&lt;/context&gt;<br/><br/>What is the most fun thing to do in San Francisco based on the context? Don't give information outside the document or repeat our findings<br/><br/>Assistant: here is the most relevant sentence in the context:"""</span></pre><p id="6da3" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">These changes led to a significant jump in Claude’s overall retrieval accuracy: from 27% to 98%! Finding this initial research fascinating, we decided to run our own set of experiments using the Needle in a Haystack test.</p><h1 id="1bb3" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Further Experiments</h1><p id="c936" class="pw-post-body-paragraph nb nc gk nd b hi ov nf ng hl ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In conducting a new series of tests, we implemented several modifications to the original experiments. The needle we used was a random number that changed each iteration, eliminating the possibility of caching. Additionally, we used our open source Phoenix evals <a class="af ny" href="https://docs.arize.com/phoenix/llm-evals/running-pre-tested-evals" rel="noopener ugc nofollow" target="_blank">library</a> (full disclosure: I lead the team that built Phoenix) to reduce the testing time and use rails to search directly for the random number in the output, cutting through wordiness that would decrease a retrieval score. Finally, we considered the negative case where the system fails to retrieve the results, marking it as unanswerable. We ran a separate test for this negative case to assess how well the system recognizes when it can’t retrieve the data. These modifications allowed us to conduct a more rigorous and comprehensive evaluation.</p><p id="11cf" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The updated tests were run across several different configurations using four different large language models: ChatGPT-4, Claude 2.1 (with and without the aforementioned change to the prompt that Anthropic suggested), and Mistral AI’s <a class="af ny" href="https://arize.com/blog/mistral-ai" rel="noopener ugc nofollow" target="_blank">Mixtral-8X7B</a>-v0.1 and 7B Instruct. Given that small nuances in prompting can lead to vastly different results across models, we used several prompt templates in the attempt to compare these models performing at their best. The simple template we used for ChatGPT and Mixtral was as follows:</p><pre class="pj pk pl pm pn qh qi qj bp qk bb bk"><span id="8396" class="ql oa gk qi b bg qm qn l qo qp">SIMPLE_TEMPLATE = ''' <br/>   You are a helpful AI bot that answers questions for a user. Keep your responses short and direct. <br/>   The following is a set of context and a question that will relate to the context.<br/>   #CONTEXT<br/>   {context}<br/>   #ENDCONTEXT<br/><br/>   #QUESTION<br/>   {question} Don’t give information outside the document or repeat your findings. If the information is not available in the context respond UNANSWERABLE</span></pre><p id="de9b" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For Claude, we tested both previously discussed templates.</p><pre class="pj pk pl pm pn qh qi qj bp qk bb bk"><span id="09d7" class="ql oa gk qi b bg qm qn l qo qp">ANTHROPIC_TEMPLATE_ORIGINAL = ''' Human: You are a close-reading bot with a great memory who answers questions for users. I’m going to give you the text of some essays. Amidst the essays (“the haystack”) I’ve inserted a sentence (“the needle”) that contains an answer to the user’s question. <br/>Here's the question:<br/>   &lt;question&gt;{question}&lt;/question&gt;<br/>   Here’s the text of the essays. The answer appears in it somewhere.<br/>   &lt;haystack&gt;<br/>   {context}<br/>   &lt;/haystack&gt;<br/>   Now that you’ve read the context, please answer the user's question, repeated one more time for reference:<br/>   &lt;question&gt;{question}&lt;/question&gt;<br/><br/>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside &lt;most_relevant_sentence&gt; XML tags. Then, put your answer in &lt;answer&gt; tags. Base your answer strictly on the context, without reference to outside information. Thank you. <br/>   If you can’t find the answer return the single word UNANSWERABLE<br/>   Assistant: '''</span></pre><pre class="qq qh qi qj bp qk bb bk"><span id="bae4" class="ql oa gk qi b bg qm qn l qo qp">ANTHROPIC_TEMPLATE_REV2 = ''' Human: You are a close-reading bot with a great memory who answers questions for users. I'm going to give you the text of some essays. Amidst the essays ("the haystack") I've inserted a sentence ("the needle") that contains an answer to the user's question. <br/>Here's the question:<br/>   &lt;question&gt;{question}&lt;/question&gt;<br/>   Here's the text of the essays. The answer appears in it somewhere.<br/>   &lt;haystack&gt;<br/>   {context}<br/>   &lt;/haystack&gt;<br/>   Now that you've read the context, please answer the user's question, repeated one more time for reference:<br/>   &lt;question&gt;{question}&lt;/question&gt;<br/><br/>   To do so, first find the sentence from the haystack that contains the answer (there is such a sentence, I promise!) and put it inside &lt;most_relevant_sentence&gt; XML tags. Then, put your answer in &lt;answer&gt; tags. Base your answer strictly on the context, without reference to outside information. Thank you. <br/>   If you can't find the answer return the single word UNANSWERABLE<br/>   Assistant: Here is the most relevant sentence in the context:'''</span></pre><p id="d7e4" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">All code run to complete these tests can be found in <a class="af ny" href="https://github.com/Arize-ai/LLMTest_NeedleInAHaystack" rel="noopener ugc nofollow" target="_blank">this GitHub repository</a>.</p><h2 id="b669" class="pq oa gk bf ob pr ps pt oe pu pv pw oh nk px py pz no qa qb qc ns qd qe qf qg bk">Results</h2><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/3eddc04a8685a9a9df55ec5fed5f02c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Av5fFxZ6KiFzM3xK"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 7: Comparison of GPT-4 results between the initial research (Run #1) and our testing (Run #2) | Image by author</figcaption></figure><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/ed7e0c7a1308960520963cbb485fa689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T6m6aDNWAlDnV217"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 8: Comparison of Claude 2.1 (without prompting guidance) results between Run #1 and Run #2 | Image by author</figcaption></figure><p id="60e3" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Our results for ChatGPT and Claude (without prompting guidance) did not stray far from Mr. Kamradt’s findings, and the generated graphs appear relatively similar: the upper right (long context, needle near the beginning of the context) is where LLM information retrieval sufferers.</p><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/1c1a96ba84a13180a3a67282298894c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iY_aC0anWdRITA2N"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 9: Comparison of Claude 2.1 results with and without prompting guidance</figcaption></figure><p id="cf97" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Although we were not able to replicate Anthropic’s results of 98% retrieval accuracy for Claude 2.1 with prompting guidance, we did see a significant decrease in total misses when the prompt was updated (from 165 to 74). This jump was achieved by simply adding a 10 word instruction to the end of the existing prompt, highlighting that small differences in prompts can have drastically different outcomes for LLMs.</p><figure class="pj pk pl pm pn fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/5871e33962023634315d861adec2a6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNnt6OqlmZ91YgukZVvXyA.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 10: Mixtral results | Image by author</figcaption></figure><p id="034a" class="pw-post-body-paragraph nb nc gk nd b hi ne nf ng hl nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Last but certainly not least, it is interesting to see just how well Mixtral performed at this task despite these being by far the smallest models tested. The Mixture of Experts (MOEs) model was far better than 7B-Instruct, and we are finding that MOEs do much better for retrieval evaluations.</p><h1 id="5be5" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="a397" class="pw-post-body-paragraph nb nc gk nd b hi ov nf ng hl ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The Needle in a Haystack test is a clever way to quantify an LLM’s ability to parse context to find needed information. <a class="af ny" href="https://arize.com/blog-course/the-needle-in-a-haystack-test-evaluating-the-performance-of-llm-rag-systems/" rel="noopener ugc nofollow" target="_blank">Our research</a> concluded with a few main takeaways. First, ChatGPT-4 is the industry’s current leader in this arena along with many other evaluations that we and others have carried out. Second, at first Claude 2.1 seemed to underperform this test, but with tweaks to the prompt structure the model showed significant improvement. Claude is a bit wordier than some other models, and taking extra care to direct it can go a long way in terms of results. Finally, Mixtral MOE greatly outperformed our expectations, and we are excited to see Mixtral models continually overperform expectations.</p></div></div></div></div>    
</body>
</html>