<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Business Guide to Tailoring Language AI Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Business Guide to Tailoring Language AI Part 2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19">https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fbe1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Prompting ChatGPT and other chat-based language AI — and why you should (not) care about it</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Georg Ruile, Ph.D." class="l ep by dd de cx" src="../Images/83b04db23852ba2df2818fe62250ca22.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Fxv4PQe6wU8TSUorsryxzA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------" rel="noopener follow">Georg Ruile, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d38c05848cc0a25ec1ad62765d9fe3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tip7QDFNIlYzsIZ21Wdfuw.png"/></div></div></figure><h2 id="f58d" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Foreword</h2><p id="07b3" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">This article sheds some light on the question of how to “talk” to Large Language Models (LLM) that are designed to interact in conversational ways, like ChatGPT, Claude and others, so that the answers you get from them are as useful as possible for the task at hand. This particular communication from human to the language chatbot is what’s typically referred to as prompting. With this article, I mean to give people with no computer science background a compact overview of the topic so that everyone can understand. It can also help businesses to contextualize of what (not) to expect from their LLM adaption endeavors.</p><p id="c794" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Prompting is the first of four steps you can take when climbing the ladder of adapting language models for your businesses’ custom use. I have introduced the overall <strong class="nx fr">4 Step Framework </strong>of unlocking custom LLM in <a class="af ot" href="https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838" rel="noopener">my previous post</a>. If you haven’t already it might be helpful to read it first so that you can fit the ideas presented here into a larger context.</p><div class="ou ov ow ox oy oz"><a rel="noopener follow" target="_blank" href="/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------"><div class="pa ab ig"><div class="pb ab co cb pc pd"><h2 class="bf fr hw z io pe iq ir pf it iv fp bk">The Business Guide to Tailoring Language AI</h2><div class="pg l"><h3 class="bf b hw z io pe iq ir pf it iv dx">A Framework for Unlocking Custom LLM Solutions You’ll Understand</h3></div><div class="ph l"><p class="bf b dy z io pe iq ir pf it iv dx">towardsdatascience.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn lr oz"/></div></div></a></div><h1 id="992c" class="po my fq bf mz pp pq gq nd pr ps gt nh pt pu pv pw px py pz qa qb qc qd qe qf bk">Introduction</h1><p id="f462" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Shortly after the mass market introduction of ChatGPT, a new, hot profession has entered the AI scene: <strong class="nx fr">Prompt engineering</strong>. These AI “whisperers”, i.e. people that have certain skills to “prompt”, that is, talk to language AI so that it responds in useful ways, have become highly sought-after (<a class="af ot" href="https://www.forbes.com/sites/jackkelly/2024/03/06/the-hot-new-high-paying-career-is-an-ai-prompt-engineer/" rel="noopener ugc nofollow" target="_blank">and generously paid</a>) new roles. Considering that a main building block of proper prompting is simply (or not so simply) giving precise instructions (see below), I must confess that I was surprised by this development (regardless of the fact that prompt <em class="qg">engineering</em> certainly involves more than just “whispering”): Isn’t communicating in a precise and concise manner a basic professional skill that we all should possess? But then again, I was reflecting on how important it is to have well-crafted requirements in software development, and “requirement engineering” roles have been an important ingredient of successful software development projects for a while now.</p><p id="3b0b" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">I observe a level of uncertainty and “best guessing” and even contradictions in the topic of LLM and prompting that I have not yet experienced in any IT-related subject. This has to do with the type and size of AI models and their <strong class="nx fr">stochastic characteristics</strong>, which is beyond the scope of this article. Considering the <a class="af ot" href="https://en.wikipedia.org/wiki/GPT-4" rel="noopener ugc nofollow" target="_blank">1.76 trillion parameters</a> of models like GPT-4, the number of possible combinations and paths from input (your “prompt”) to output (the model response) is virtually indefinite and non-deterministic. Hence, applications treat these models mainly as black boxes, and related research focuses on empirical approaches such as benchmarking their performance.</p><p id="ed79" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">The sad news is that I cannot present you a perfect one-size-fits-all prompting solution that will forever solve your LLM requirements. Add to this that <strong class="nx fr">different models behave differently</strong>, and you may understand my dilemma. There’s some good news, though: On the one hand, you can, and should, always consider some <strong class="nx fr">basic principles and concepts </strong>that will help you optimize your interactions with the machines. Well-crafted prompts gets you farther than poor ones, and this is why it is well worthwhile to dig a bit deeper into the topic. On the other hand, <strong class="nx fr">it may not even be necessary to worry too much about prompting at all</strong>, which saves you valuable computing time (literally, CPU/GPU and figuratively, in your own brain).</p></div></div></div><div class="ab cb qh qi qj qk" role="separator"><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e2cc" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Start with Why</h2><p id="6738" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Here I am not referring to Simon Sinek’s classic TEDx business advice. Instead, I encourage you to curiously wonder <strong class="nx fr">why technology does what it does</strong>. I strongly believe in the notion that if you understand at least a bit of the inner workings of software, it will tremendously help you in its application.</p><p id="51d4" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">So how, in principle, is the input (the prompt) related to the output (the response), and why is it that proper prompts result in better suited responses? To figure this out, we need to have at least a superficial look at the model architecture and its training and fine-tuning, without needing to understand the details of the impressive underlying concepts like the infamous Transformer Architecture and Attention Mechanisms which ultimately caused the breakthrough of ChatGPT-like Generative AI as we know it today.</p><p id="e964" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">For our purposes, we can look at it from two angles:</p><p id="ebc3" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><em class="qg">How does the model </em><strong class="nx fr"><em class="qg">retrieve knowledge and generate its response?</em></strong><em class="qg"><br/></em>and closely related<br/><em class="qg">How has the model been </em><strong class="nx fr"><em class="qg">trained and fine-tuned?</em></strong></p><p id="40e4" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">It is important to understand that an LLM is in essence a <strong class="nx fr">Deep Neural Network </strong>and as such, it works based on <strong class="nx fr">statistics and probabilities</strong>. Put very simplistically, the model generates output which reflects the closest match to the context, based on its knowledge it has learned from vast amounts of training data. One of the building blocks here are so-called <strong class="nx fr">Embeddings</strong>, where similar word meanings are (mathematically) close to each other, even though the model does not actually “understand” those meanings. If this sounds fancy, it kinda is, but at same time, it is “only” mathematics, so don’t be afraid.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/bdb1b66d57538d9d96891eb1aea06b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUBt6_iTX9Z14h0LyazsbQ.png"/></div></div><figcaption class="qp qq qr mj mk qs qt bf b bg z dx">A simple illustration of word vector embeddings — similar word “meanings” are close to each other</figcaption></figure><p id="c7ae" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">When looking at training, it helps considering the training data and process a language model has gone through. Not only has the model seen vast amounts of text data. It has also learned what makes out a high rated response to a specific question, for instance on sites like StackOverflow, and on high-quality Q&amp;A assistant documents written for model training and tuning. In addition, in its fine-tuning stage, it learned and iteratively adapted its optimal responses <strong class="nx fr">based on human feedback</strong>. Without all this intense training and tuning efforts, the model might answer a question like “what is your first name” simply with “what is your last name”, because it has seen this frequently on internet forms [1].</p><p id="840e" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Where I am trying to get at is this: When interacting with natural language AI, always keep in mind what and how the model has learned and how it gets to its output, given your input. Even though no one really knows this exactly, it is useful to consider probable correlations: Where and in what context could the model have seen input similar to yours before? What data has been available during the pre-training stage, and in which quality and quantity? For instance: Ever wondered why LLMs can solve mathematical equations (not reliably, however, sometimes still surprisingly), without inherent calculation capabilities? LLMs don’t calculate, they match patterns!</p></div></div></div><div class="ab cb qh qi qj qk" role="separator"><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b3bd" class="po my fq bf mz pp qu gq nd pr qv gt nh pt qw pv pw px qx pz qa qb qy qd qe qf bk">Prompting 101</h1><p id="f0dd" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">There is a plethora of Prompting techniques, and plenty of scientific literature that benchmarks their effectiveness. Here, I just want to introduce a few well-known concepts. I believe that once you get the general idea, you will be able to expand your prompting repertoire and even develop and test new techniques yourself.</p><h2 id="b945" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Ask and it will be given to you</h2><p id="4883" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Before going into specific prompting concepts, I would like to stress a general idea that, in my opinion, cannot be stressed enough:</p><p id="adfc" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><strong class="nx fr"><em class="qg">The quality of your prompt highly determines the response of the model.</em></strong></p><p id="ae54" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">And by quality I don’t necessarily mean a sophisticated prompt construction. I mean the basic idea of asking a precise question or giving well-structured instructions and providing necessary context. I have touched on this already when we met Sam, the piano player, in <a class="af ot" href="https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838" rel="noopener">my previous article</a>. If you ask a bar piano player to play some random Jazz tune, chances are that he might not play what you had in mind. Instead, if you ask exactly what it is you want to hear, your satisfaction with the result is likely to increase.</p><p id="3789" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Similarly, if you ever had the chance of, say, hire someone to do something around your house and your contract specification only says, say, “bathroom renovation”, you might be surprised that in the end your bathroom does not look like what you had in mind. The contractor, just like the model, will only refer to what he has learned about renovations and bathroom tastes and will take the learned route to deliver.</p><p id="dc76" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">So here are some <strong class="nx fr">general guidelines for prompting</strong>:</p><p id="f89d" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">· Be clear and specific.</p><p id="d14b" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">· Be complete.</p><p id="10e1" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">· Provide context.</p><p id="f2be" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">· Specify the desired output style, length, etc.</p><p id="227d" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">This way, the model has sufficient and matching reference data in your prompt that it can relate to when generating its response.</p><h2 id="af2c" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Roleplay prompting — simple, but overrated</h2><p id="f4d6" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">In the early days of ChatGPT, the idea of roleplay prompting was all around: Instead of asking the assistant to give you an immediate answer (i.e. a simple query), you first assign it a specific role, such as “teacher” or “consultant” etc. Such a prompt could look like [2]:</p><p id="d88d" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><strong class="nx fr"><em class="qg">From now on, you are an excellent math teacher and always teach your students math problems correctly. And I am one of your students.</em></strong></p><p id="e8be" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">It has been shown that this concept yields superior results. One <a class="af ot" href="https://arxiv.org/abs/2308.07702" rel="noopener ugc nofollow" target="_blank">paper </a>reports that by this role play, the model implicitly triggers a step by step reasoning process, which is what you want it to do when applying the CoT- technique, see below. However, this approach has also been shown to <strong class="nx fr">sometimes perform sub-optimal</strong> and needs to be well designed.</p><p id="8ac3" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">In my experience, simply assigning a role doesn’t do the trick. I have experimented with the example task from the paper referred to above. Unlike in this research, GPT3.5 (which is as of today the free version of OpenAI’s ChatGPT, so you can try it yourself) has given the correct result, using a simple query:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/89a418b3d6837d5d59787565537487e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvpmUjTnL4rkQymzCXbKeg.png"/></div></div><figcaption class="qp qq qr mj mk qs qt bf b bg z dx">An example using a simple query instead of the roleplay prompt suggested by [2], still yielding the correct response</figcaption></figure><p id="a453" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">I have also experimented with different logical challenges with both simple queries and roleplay, using a similar prompt like the one above. In my experiments two things happen:</p><p id="7357" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><em class="qg">either </em><strong class="nx fr"><em class="qg">simple queries provides the correct answer on the first attempt</em></strong><em class="qg">, or</em></p><p id="2e37" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><em class="qg">both </em><strong class="nx fr"><em class="qg">simple queries and roleplay come up with false, however different answers</em></strong></p><p id="e083" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><strong class="nx fr">Roleplay did not outperform </strong>the queries in any of my simple (not scientifically sound) experiments. Hence, I conclude that the models must have improved recently and the <strong class="nx fr">impact of roleplay prompting is diminishing</strong>.</p><p id="23b3" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Looking at different research, and without extensive further own experimenting, I believe that in order to outperform simple queries, roleplay prompts need to be embedded into a <strong class="nx fr">sound and thoughtful design </strong>to outperform the most basic approaches — or are not valuable at all.</p><p id="2ef1" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">I am happy to read your experiences on this in the comments below.</p><h2 id="bd01" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">Few-Shot aka in-context learning</h2><p id="b479" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Another intuitive and relatively simple concept is what’s referred to as Few-Shot prompting, also referred to as in-context learning. Unlike in a Zero-Shot Prompt, we not only ask the model to perform a task and expect it to deliver, we additionally <strong class="nx fr">provide (“few”) examples of the solutions</strong>. Even though you may find this obvious that providing examples leads to better performance, this is quite a remarkable ability: These LLMs are able to in-context learn, i.e. perform new tasks via inference alone by conditioning on a few input-label pairs and making predictions for new inputs [3].</p><p id="3939" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Setting up a few-shot prompt involves</p><p id="0148" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><em class="qg">(1) </em><strong class="nx fr"><em class="qg">collecting examples of the desired responses</em></strong><em class="qg">, and <br/>(2) writing your prompt with</em><strong class="nx fr"><em class="qg"> instructions on what to do with these examples</em></strong><em class="qg">.</em></p><p id="1c78" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Let’s look at a typical classification example. Here the model is given several examples of statements that are either positive, neutral or negative judgements. The model’s task is to rate the final statement:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/964b159019f7b64cc3c4b8f5ea1fa594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UQtkysxZ_yqSnWLcLb6sNA.png"/></div></div><figcaption class="qp qq qr mj mk qs qt bf b bg z dx">A typical classification example of a Few-Shot prompt. The model is required to classify statements into the given categories (positive / negative)</figcaption></figure><p id="99b4" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Again, even though this is a simple and intuitive approach, I am sceptical about its value in state-of-the-art language models. In my (again, not scientifically sound) experiments, <strong class="nx fr">Few-Shot prompts have not outperformed Zero-Shot in any case</strong>. (The model knew already that a drummer who doesn’t keep the time, is a negative experience, without me teaching it…). My finding seems to be consistent with recent research, where even the opposite effect (<strong class="nx fr">Zero-Shot outperforming Few-Shot</strong>) has been shown [4].</p><p id="5218" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">In my opinion and on this empirical background it is worth considering if the cost of design as well as computational, API and latency cost of this approach are a worthwhile investment.</p><h2 id="f391" class="mx my fq bf mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu bk">CoT-Prompting or “Let’s think step-by-step’’</h2><p id="7842" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">Chain of Thought (CoT) Prompting aims to make our models better at solving complex, multi-step reasoning problems. It can be as simple as adding the CoT instruction “Let’s think step-by-step’’ to the input query, to improve accuracy significantly [5][6].</p><p id="759c" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Instead of just providing the final query or add one or few examples within your prompt like in the Few-Shot approach, you prompt the model to <strong class="nx fr">break down its reasoning process into a series of intermediate steps</strong>. This is akin to how a human would (ideally) approach a challenging problem.</p><p id="a2fe" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Remember your math exams in school? Often, at more advanced classes, you were asked to not only solve a mathematical equation, but also write down the logical steps how you arrived at the final solution. And even if it was incorrect, you might have gotten some credits for mathematically sound solution steps. Just like your teacher in school, you expect the model to break the task down into sub-tasks, perform intermediate reasoning, and arrive at the final answer.</p><p id="3a2b" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Again, I have experimented with CoT myself quite a bit. And again, most of the time, simply adding <strong class="nx fr">“Let’s think step-by-step” didn’t improve the quality of the response</strong>. In fact, it seems that the <strong class="nx fr">CoT approach has become an implicit standard</strong> of the recent fine-tuned chat-based LLM like ChatGPT, and the response is frequently broken down into chunks of reasoning without the explicit command to do so.</p><p id="573e" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">However, I came across one instance where the explicit CoT <strong class="nx fr">command did in fact improve the answer significantly</strong>. I used a CoT example from <a class="af ot" href="https://medium.com/@thomasczerny/chain-of-thought-cot-prompting-9ee4967e927c" rel="noopener">this article</a>, however, altered it into a trick question. Here you can see how ChatGPT fell into my trap, when not explicitly asked for a CoT approach (even though the response shows a step wise reasoning):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d66d445d5c947e3d7919d1c8553ca5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcwuorktrWQjafZP1kGlDA.png"/></div></div><figcaption class="qp qq qr mj mk qs qt bf b bg z dx">A trick question with a simple query instead of a CoT prompt. Even though the response is broken down “step by step”, it is not quite correct.</figcaption></figure><p id="5726" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">When I added “Let’s think step-by-step” to the same prompt, it solved the trick question correctly (well, it is unsolvable, which ChatGPT rightfully pointed out):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/286f905eaa9ca7c70dc194b2da1c2fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B98TQRassWCcRi-T6mE3Ug.png"/></div></div><figcaption class="qp qq qr mj mk qs qt bf b bg z dx">The same trick question with an explicit CoT prompt, delivering a correct response</figcaption></figure><p id="1069" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">To summarize, Chain of Thought prompting aims at building up reasoning skills that are otherwise difficult for language models to acquire implicitly. It encourages models to articulate and refine their reasoning process rather than attempting to jump directly from question to answer.</p><p id="06d5" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Again, my experiments have revealed <strong class="nx fr">only limited benefits of the simple CoT approach</strong> (adding “Let’s think step-by-step“). <strong class="nx fr">CoT did outperform a simple query on one occasion</strong>, and at the same time the extra effort of adding the CoT command is minimal. This<strong class="nx fr"> cost-benefit ratio </strong>is one of the reasons why this approach is one of my favorites. Another reason why I personally like this approach is, it not only helps the model, but also can <strong class="nx fr">help us humans to reflect </strong>and maybe even iteratively consider necessary reasoning steps while crafting the prompt.</p><p id="1f22" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">As before, we will likely see diminishing benefits of this simple CoT approach when models become more and more fine-tuned and accustomed to this reasoning process.</p><h1 id="3f31" class="po my fq bf mz pp pq gq nd pr ps gt nh pt pu pv pw px py pz qa qb qc qd qe qf bk">Conclusion</h1><p id="7e75" class="pw-post-body-paragraph nv nw fq nx b go ny nz oa gr ob oc od ni oe of og nm oh oi oj nq ok ol om on fj bk">In this article, we have taken a journey into the world of prompting chat-based Large Language Models. Rather than just giving you the most popular prompting techniques, I have encouraged you to begin the journey with the question of Why prompting matters at all. During this journey we have discovered that the importance of prompting is diminishing thanks to the evolution of the models. Instead of requiring users to invest in continuously improving their prompting skills, currently evolving model architectures will likely further reduce their relevance. An <strong class="nx fr">agent-based framework</strong>, where different “routes” are taken while processing specific queries and tasks, is one of those.</p><p id="7b8a" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">This does not mean, however, that <strong class="nx fr">being clear and specific and providing the necessary context within your prompts</strong> isn’t worth the effort. On the contrary, I am a strong advocate of this, since it not only helps the model but also yourself to figure out what exactly it is you’re trying to achieve.</p><p id="7028" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">Just like in human communication, multiple factors determine the appropriate approach for reaching a desired outcome. Often, it is a mix and iteration of different approaches that yield optimal results for the given context. Try, test, iterate!</p><p id="b2f2" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">And finally, unlike in human interactions, you can test virtually limitlessly into your personal trial-and-error prompting journey. Enjoy the ride!</p></div></div></div><div class="ab cb qh qi qj qk" role="separator"><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn qo"/><span class="ql by bm qm qn"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b1ad" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><em class="qg">Notes and references<br/>All drawings are hand crafted with pride by the author :)</em></p><p id="0063" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[1]: How Large Language Models work: From zero to ChatGPT</p><p id="625f" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><a class="af ot" href="https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f" rel="noopener">https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f</a></p><p id="a2ea" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[2]: Better Zero-Shot Reasoning with Role-Play Prompting</p><p id="cd7c" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><a class="af ot" href="https://arxiv.org/abs/2308.07702" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2308.07702</a></p><p id="bae1" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[3]: Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?<br/><a class="af ot" href="https://arxiv.org/abs/2202.12837" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2202.12837</a></p><p id="03b8" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[4]: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</p><p id="de16" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><a class="af ot" href="https://dl.acm.org/doi/abs/10.1145/3411763.3451760" rel="noopener ugc nofollow" target="_blank">https://dl.acm.org/doi/abs/10.1145/3411763.3451760</a>.</p><p id="bbe9" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[5]: When do you need Chain-of-Thought Prompting for ChatGPT?</p><p id="caf0" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk"><a class="af ot" href="https://arxiv.org/abs/2304.03262" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2304.03262</a></p><p id="eb68" class="pw-post-body-paragraph nv nw fq nx b go oo nz oa gr op oc od ni oq of og nm or oi oj nq os ol om on fj bk">[6]: Large Language Models are Zero-Shot Reasoners<br/><a class="af ot" href="https://arxiv.org/abs/2205.11916" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.11916</a></p></div></div></div></div>    
</body>
</html>