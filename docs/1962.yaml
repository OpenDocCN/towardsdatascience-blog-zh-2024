- en: Building a User Insights-Gathering Tool for Product Managers from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-a-user-insights-gathering-tool-for-product-managers-from-scratch-a6459dc1c3f6?source=collection_archive---------8-----------------------#2024-08-12](https://towardsdatascience.com/building-a-user-insights-gathering-tool-for-product-managers-from-scratch-a6459dc1c3f6?source=collection_archive---------8-----------------------#2024-08-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Say goodbye to paid customer insights hubs! Learn how to combine five open-source
    AI models to automate insights gathering from your user interviews.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@zaninihugo?source=post_page---byline--a6459dc1c3f6--------------------------------)[![Hugo
    Zanini](../Images/cbda793f1cf82f34b29c6e136556361d.png)](https://medium.com/@zaninihugo?source=post_page---byline--a6459dc1c3f6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a6459dc1c3f6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a6459dc1c3f6--------------------------------)
    [Hugo Zanini](https://medium.com/@zaninihugo?source=post_page---byline--a6459dc1c3f6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a6459dc1c3f6--------------------------------)
    ·11 min read·Aug 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6094d50facdc92f35218923622bab796.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Daria Nepriakhina](https://unsplash.com/@epicantus?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/printed-sticky-notes-glued-on-board-zoCDWPuiRuA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: As a technical product manager for a data platform, I frequently run user interviews
    to identify challenges associated with data development processes.
  prefs: []
  type: TYPE_NORMAL
- en: However, when exploring a new problem area with users, I can easily become overwhelmed
    by the numerous conversations I have with various individuals across the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, I have adopted a systematic approach to address this challenge. I
    focus on taking comprehensive notes during each interview and then revisit them.
    This allows me to consolidate my understanding and identify user discussion patterns.
  prefs: []
  type: TYPE_NORMAL
- en: However, dividing my attention between note-taking and active listening often
    compromised the quality of my conversations. I noticed that when someone else
    took notes for me, my interviews significantly improved. This allowed me to fully
    engage with the interviewees, concentrate solely on what they were saying, and
    have more meaningful and productive interactions.
  prefs: []
  type: TYPE_NORMAL
- en: To be more efficient, I transitioned from taking notes during meetings to recording
    and transcribing them whenever the functionality was available. This significantly
    reduced the number of interviews I needed to conduct, as I could gain more insights
    from fewer conversations. However, this change required me to invest time reviewing
    transcriptions and watching videos. The figure below shows what became a simplified
    flow of the process I follow for mapping a new product development opportunity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98af683776e7bdb22a3114f9e803ed9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Due to the size of the meeting transcriptions and difficulties in categorizing
    user insights, consolidation and analysis became challenging.**'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the meeting transcription tools available to me were limited to
    English, while most of my conversations were in Portuguese. As a result, I decided
    to look to the market for a solution that could help me with those challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The solutions I found that solved most of my pain points were [Dovetail](https://dovetail.com/),
    [Marvin](https://heymarvin.com/), [Condens](https://condens.io/), and [Reduct](https://reduct.video/).
    They position themselves as customer insights hubs, and their main product is
    generally Customer Interview transcriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, you can upload an interview video there, and you are going to receive
    a transcription indicating who is speaking with hyperlinks to the original video
    on every phrase. Over the text, you can add highlights, tags, and comments and
    ask for a summary of the conversation. **These features would solve my problem;
    however, these tools are expensive, especially considering that I live in Brazil
    and they charge in dollars.**
  prefs: []
  type: TYPE_NORMAL
- en: '**The tools offer nothing revolutionary, so I decided to implement an open-source
    alternative that would run for free on Colab notebooks.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a good PM, the first thing I did was identify the must-haves for my product
    based on the user''s (my) needs. Here are the high-level requirements I mapped:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost and Accessibility**'
  prefs: []
  type: TYPE_NORMAL
- en: Free;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No coding experience is required to use;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Privacy and Security**'
  prefs: []
  type: TYPE_NORMAL
- en: Keep data private — no connection with external services;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance**'
  prefs: []
  type: TYPE_NORMAL
- en: Execution must be faster than the video duration;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-precision transcription in different languages;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functionality**'
  prefs: []
  type: TYPE_NORMAL
- en: Speakers identification;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to search over the transcriptions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to highlight the transcriptions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to create repositories of research being done;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**'
  prefs: []
  type: TYPE_NORMAL
- en: Integrated with my company's existing tools (Google Workspace);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM model integrated to receive prompts for tasks on top of the transcriptions;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the requirements, I designed what would be the features of my solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92381377517963db8d4c326de1666e2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, I projected what would be the expected inputs and the user interfaces
    to define them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2cd00c7a1d251e1fb7d75824b24ef412.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Users will upload their interview to YouTube as an unlisted video and create
    a Google Drive folder to store the transcription. They will then access a Google
    Colab notebook to provide basic information about the interview, paste the video
    URL, and optionally define tasks for an LLM model. The output will be Google Docs,
    where they can consolidate insights.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the product architecture. The solution required combining five different
    ML models and some Python libraries. The next sections provide overviews of each
    of the building blocks; however, if you are more interested in trying the product,
    please go to the "I got it" section.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96168c1df8b7d3ba902a9a547c92e61f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Interview Setup and Video Upload
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create a user-friendly interface for setting up interviews and providing
    video links, I used [Google Colab’s forms functionality](https://colab.research.google.com/notebooks/forms.ipynb?ref=dataschool.io#scrollTo=ig8PIYeLtM8g).
    This allows for the creation of text fields, sliders, dropdowns, and more. The
    code is hidden behind the form, making it very accessible for non-technical users.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89dea50e1e495b9369553a667507f74a.png)'
  prefs: []
  type: TYPE_IMG
- en: Interview selection form — Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Audio download and conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used the yt-dlp lib to download only the audio from a YouTube video and convert
    it to the mp3 format. It is very straightforward to use, and you can [check its
    documentation here](https://github.com/yt-dlp/yt-dlp).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c47572c2a50d6190d0dac82e9bbbae7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [yt-dlp](https://github.com/yt-dlp/yt-dlp)
  prefs: []
  type: TYPE_NORMAL
- en: Audio transcription
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To transcribe the meeting, I used [Whisper from Open AI](https://github.com/openai/whisper).
    It is an open-source model for speech recognition trained on more than 680K hours
    of multilingual data.
  prefs: []
  type: TYPE_NORMAL
- en: The model runs incredibly fast; a one-hour audio clip takes around 6 minutes
    to be transcribed on a 16GB T4 GPU (offered by free on Google Colab), and it supports
    [99 different languages](https://github.com/openai/whisper/blob/ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab/whisper/tokenizer.py#L11-L110).
  prefs: []
  type: TYPE_NORMAL
- en: Since privacy is a requirement for the solution, the model weights are downloaded,
    and all the inference occurs inside the colab instance. I also added a Model Selection
    form in the notebook so the user can choose different models based on the precision
    they are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12597ca463c3d2bcaf8f4ae25e6d8dfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Speakers Identification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Speaker identification is done through a technique called Speakers Diarization.
    The idea is to identify and segment audio into distinct speech segments, where
    each segment corresponds to a particular speaker. With that, we can identify who
    spoke and when.
  prefs: []
  type: TYPE_NORMAL
- en: Since the videos uploaded from YouTube don't have metadata identifying who is
    speaking, the speakers will be divided into Speaker 1, Speaker 2, etc.… Later,
    the user can find and replace those names in Google Docs to add the speakers’
    identification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8b66de2615683dd7210b7c4d1be7a3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: For the diarization, we will use a model called the [Multi-Scale Diarization
    Decoder (MSDD)](https://arxiv.org/pdf/2203.15974), which was developed by Nvidia
    researchers. It is a sophisticated approach to speaker diarization that leverages
    multi-scale analysis and dynamic weighting to achieve high accuracy and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: The model is known for being quite good at identifying and properly categorizing
    moments where multiple speakers are talking—a thing that occurs frequently during
    interviews.
  prefs: []
  type: TYPE_NORMAL
- en: The model can be used through the [NVIDIA NeMo framework](https://github.com/NVIDIA/NeMo).
    It allowed me to get MSDD checkpoints and run the diarization directly in the
    colab notebook with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Looking into the Diarization results from MSDD, I noticed that punctuation was
    pretty bad, with long phrases, and some interruptions such as "*hmm*" and "*yeah*"
    were taken into account as a speaker interruption — making the text difficult
    to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, I decided to add a punctuation model to the pipeline to improve the readability
    of the transcribed text and facilitate human analysis. So I got the [punctuate-all
    model from Hugging Face](https://huggingface.co/kredor/punctuate-all), which is
    a very precise and fast solution and supports the following languages: English,
    German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese,
    Slovak, and Slovenian.'
  prefs: []
  type: TYPE_NORMAL
- en: Video Synchronization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the industry solutions I benchmarked, a strong requirement was that every
    phrase should be linked to the moment in the interview the speaker was talking.
  prefs: []
  type: TYPE_NORMAL
- en: The Whisper transcriptions have metadata indicating the timestamps when the
    phrases were said; however, this metadata is not very precise.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I used a model called [Wav2Vec2](https://arxiv.org/abs/2006.11477)
    to do this match in a more accurate way. Basically, the solution is a neural network
    designed to learn representations of audio and perform speech recognition alignment.
    The process involves finding the exact timestamps in the audio signal where each
    segment was spoken and aligning the text accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: With the transcription <> timestamp match properly done, through simple Python
    code, I created hyperlinks pointing to the moment in the video where the phrases
    start to be said.
  prefs: []
  type: TYPE_NORMAL
- en: The LLM Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This step of the pipeline has a large language model ready to run locally and
    analyze the text, providing insights about the interview. By default, I added
    a Gemma Model 1.1b with a prompt to summarize the text. If the users choose to
    have the summarization, it will be in a bullet list at the top of the document.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1175b705bc66896a25ab9333288fe76a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Also, by clicking on *Show code*, the users can change the prompt and ask the
    model to perform a different task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f782e28c6df86a0db83ca2f6bb0210bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Document generation for tagging, highlights, and comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last task performed by the solution is to generate Google Docs with the
    transcriptions and hyperlinks to the interviews. This was done through the [Google
    API Python client library](https://googleapis.github.io/google-api-python-client/).
  prefs: []
  type: TYPE_NORMAL
- en: I got it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the product has become incredibly useful in my day-to-day work, I decided
    to give it a name for easier reference. I called it the **I**nsights **G**athering
    **O**pen-source **T**ool, or iGot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d4274e8d7769313b98c24f070f4ab62.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image generated by DALL·E-3](https://openai.com/policies/terms-of-use/)'
  prefs: []
  type: TYPE_NORMAL
- en: When using the solution for the first time, some initial setup is required.
    Let me guide you through a real-world example to help you get started.
  prefs: []
  type: TYPE_NORMAL
- en: Open the iGot notebook and install the required libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Click on [this link](https://colab.research.google.com/drive/1eMN4-b4kAH4uNTzzgJ-X4J90BYozGl6o?usp=sharing)
    to open the notebook and run the first cell to install the required libraries.
    It is going to take around 5 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd64fbb094759505bde82f7e4b22e499.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If you get a prompt asking you to restart the notebook, just cancel it. There
    is no need.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/933bc06cfe3b64c5b15ec643472bc6e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If everything runs as expected, you are going to get the message "All libraries
    installed!".
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bfd13f827456fe8cd37187bf397eddd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Getting the Hugging User Access Token and model access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*(This step is required just the first time you are executing the notebook)*'
  prefs: []
  type: TYPE_NORMAL
- en: For running the Gemma and punctuate-all models, we will download weights from
    hugging face. To do so, you must request a user token and model access.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, you need to create a hugging face account and follow these steps to
    get a token with reading permissions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc3cd784c45aa25cbac192ff56881600.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the token, copy it and return to the lab notebook. Go to the secrets
    tab and click on "Add new secret."
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/805b3887ad6143ac349b4a479e64bc27.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Name your token as *HF_TOKEN* and past the key you got from Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d133e00fce9e77c3aebe876f80e2051.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Next, click [this link](https://medium.com/r?url=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fgemma-1.1-2b-it)
    to open the Gemma model on Hugging Face. Then, click on “Acknowledge license”
    to get access the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a1a389a054a697d3c52a2aee0a80a2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Sending the interview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To send an interview to iGot, you need to upload it as an unlisted video on
    YouTube previously. For the purpose of this tutorial, I got a piece of the Andrej
    Karpathy interview with Lex Fridman and uploaded it to my account. It is part
    of the conversation where Andrej gave some advice for Machine Learning Beginners.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you need to get the video URL, paste in the *video_url* field of the *Interview
    Selection* notebook cell, define a name for it, and indicate the language spoken
    in the video.
  prefs: []
  type: TYPE_NORMAL
- en: Once you run the cell, you are going to receive a message indicating that an
    audio file was generated.t into
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7cf4b3fb57c466921b073bc7a0f4af38.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Model selection and execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next cell, you can select the size of the Whisper model you want to use
    for the transcription. The bigger the model, the higher the transcription precision.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the largest model is selected. Make your choice and run the cell.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48cd47e34184546ea5fb73b849b06a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Then, run the models execution cell to run the pipeline of models showed in
    the previous section. If everything goes as expected, you should receive the message
    "Punctuation done!" by the end.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85223fc8f974c1c2401713da9585adf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If you get prompted with a message asking for access to the hugging face token,
    grant access to it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6405dfa35e01114692e8732e8ce80f3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the transcript output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step is to save the transcription to a Google Docs file. To accomplish
    this, you need to specify the file path, provide the interview name, and indicate
    whether you want Gemma to summarize the meeting.
  prefs: []
  type: TYPE_NORMAL
- en: When executing the cell for the first time, you can get prompted with a message
    asking for access to your Google Drive. Click in allow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b8a9c6633be1f95a04f9f472ca7d678.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Then, give Colab full access to your Google Drive workspace.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b18e48f3be24d56a6af91e644cd05110.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If everything runs as expected, you are going to see a link to the google docs
    file at the end. Just click on it, and you are going to have access to your interview
    transcription.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fb83c207c6df706d2958faf05fdba3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Gathering insights from the generated document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final document will have the transcriptions, with each phrase linked to
    the corresponding moment in the video where it begins. Since YouTube does not
    provide speaker metadata, I recommend using Google Docs’ find and replace tool
    to substitute “Speaker 0,” “Speaker 1,” and so on with the actual names of the
    speakers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/931431f03c01af159f04973515555704.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, you can work on highlights, notes, reactions, etc. As envisioned
    in the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08bd85c364d8aa7ff383f3237f763ad0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tool is just in its first version, and I plan to evolve it into a more user-friendly
    solution. Maybe hosting a website so users don’t need to interact directly with
    the notebook, or creating a plugin for using it in Google Meets and Zoom.
  prefs: []
  type: TYPE_NORMAL
- en: My main goal with this project was to create a high-quality meeting transcription
    tool that can be beneficial to others while demonstrating how available open-source
    tools can match the capabilities of commercial solutions.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find it useful! Feel free to [reach out to me on LinkedIn](https://www.linkedin.com/in/hugozanini/)
    if you have any feedback or are interested in collaborating on the evolution of
    iGot :)
  prefs: []
  type: TYPE_NORMAL
