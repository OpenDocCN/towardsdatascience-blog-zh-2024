["```py\nimport openai\nimport json\n\n# Set your OpenAI API key here\nopenai.api_key = 'your-openai-api-key'\ndef evaluate_sql_query(question, query, results):\n    # Define the prompt with placeholders for question, query, and results\n    prompt = f\"\"\"\n    As an external observer, evaluate the SQL query and results against the client's question. Provide an assessment from three perspectives:\n    1\\. End User\n    2\\. Data Scientist\n    3\\. Business Leader\n\n    For each role, provide:\n    1\\. **Overall Score** (0-10)\n    2\\. **Criteria Scores** (0-10):\n       - Accuracy: How well does it meet the question?\n       - Relevance: Is all needed data included, and is irrelevant data excluded?\n       - Logic: Does the query make sense?\n       - Efficiency: Is it concise and free of unnecessary complexity?\n    3\\. **Issue Tags** (2D array: ['tag', 'details']):\n       - Examples: Wrong Granularity, Excessive Columns, Missing Data, Incorrect Values, Wrong Filters, Performance Issues.\n    4\\. **Other Observations** (2D array: ['tag', 'details'])\n\n    Client Question:\n    {question}\n\n    SQL Query:\n    {query}\n\n    SQL Results:\n    {results}\n\n    Respond ONLY in this format:\n    ```", "```py\n    \"\"\"\n    # Call the OpenAI API with the prompt\n    response = openai.Completion.create(\n        engine=\"gpt-4\",  # or whichever model you're using\n        prompt=prompt,\n        max_tokens=500,  # Adjust token size based on expected response length\n        temperature=0  # Set temperature to 0 for more deterministic results\n    )\n    # Parse and return the result\n    return json.loads(response['choices'][0]['text'])\n# Example usage\nquestion = \"How many Pro Plan users converted from trial?\"\nquery = \"SELECT COUNT(*) FROM users WHERE plan = 'Pro' AND status = 'Converted' AND source = 'Trial';\"\nresults = \"250\"\nevaluation = evaluate_sql_query(question, query, results)\nprint(json.dumps(evaluation, indent=4))\n```", "```py\nfor question, query, results in test_cases:\n    # Call the OpenAI API to evaluate the SQL query and results\n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",  # Replace with GPT-4 or relevant engine\n        prompt=prompt.format(question=question, query=query, results=results),\n        max_tokens=1000\n    )\n\n    # Process and store the response\n    process_response(response)\n\ndef store_results_in_db(test_run_id, question, role, scores, issue_tags, observations):\n    # SQL insert query to store evaluation results in the issue catalog\n    insert_query = \"\"\"\n    INSERT INTO issue_catalog \n    (test_run_id, question, role, overall_score, accuracy_score, relevance_score, logic_score, efficiency_score, issue_tags, other_observations)\n    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n    \"\"\"\n    db_cursor.execute(insert_query, (\n        test_run_id, question, role, scores['overall'], scores['accuracy'], scores['relevance'], \n        scores['logic'], scores['efficiency'], json.dumps(issue_tags), json.dumps(observations)\n    ))\n    db_conn.commit()\n```", "```py\nCREATE TABLE issue_catalog (\n    id SERIAL PRIMARY KEY,\n    test_run_id INT NOT NULL,\n    question TEXT NOT NULL,\n    role TEXT NOT NULL,  -- e.g., endUser, dataScientist, businessLeader\n    overall_score INT NOT NULL,\n    accuracy_score INT NOT NULL,\n    relevance_score INT NOT NULL,\n    logic_score INT NOT NULL,\n    efficiency_score INT NOT NULL,\n    issue_tags JSONB,  -- Storing issue tags as JSON for flexibility\n    other_observations JSONB,  -- Storing other observations as JSON\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n```"]