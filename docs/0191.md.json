["```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport yfinance as yf\nimport datetime as dt\nimport math\n\n#### Data Processing\nstart_date = dt.datetime(2020,4,1)\nend_date = dt.datetime(2023,4,1)\n\n#loading from yahoo finance\ndata = yf.download(\"GOOGL\",start_date, end_date)\n\npd.set_option('display.max_rows', 4)\npd.set_option('display.max_columns',5)\ndisplay(data)\n\n# #Splitting the dataset\ntraining_data_len = math.ceil(len(data) * .8)\ntrain_data = data[:training_data_len].iloc[:,:1]\ntest_data = data[training_data_len:].iloc[:,:1]\n\ndataset_train = train_data.Open.values\n# Reshaping 1D to 2D array\ndataset_train = np.reshape(dataset_train, (-1,1))\ndataset_train.shape\nscaler = MinMaxScaler(feature_range=(0,1))\n# scaling dataset\nscaled_train = scaler.fit_transform(dataset_train)\n\ndataset_test = test_data.Open.values\ndataset_test = np.reshape(dataset_test, (-1,1))\nscaled_test = scaler.fit_transform(dataset_test)\n\nX_train = []\ny_train = []\nfor i in range(50, len(scaled_train)):\n    X_train.append(scaled_train[i-50:i, 0])\n    y_train.append(scaled_train[i, 0])\n\nX_test = []\ny_test = []\nfor i in range(50, len(scaled_test)):\n    X_test.append(scaled_test[i-50:i, 0])\n    y_test.append(scaled_test[i, 0])\n\n# The data is converted to Numpy array\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n#Reshaping\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\ny_train = np.reshape(y_train, (y_train.shape[0],1))\nprint(\"X_train :\",X_train.shape,\"y_train :\",y_train.shape)\n\n# The data is converted to numpy array\nX_test, y_test = np.array(X_test), np.array(y_test)\n\n#Reshaping\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\ny_test = np.reshape(y_test, (y_test.shape[0],1))\n```", "```py\nclass SimpleRNN:\n    def __init__(self,input_dim,output_dim, hidden_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.Waa = np.random.randn(hidden_dim, hidden_dim) * 0.01 # we initialise as non-zero to help with training later\n        self.Wax = np.random.randn(hidden_dim, input_dim) * 0.01\n        self.Way = np.random.randn(output_dim, hidden_dim) * 0.01\n        self.ba = np.zeros((hidden_dim, 1))\n        self.by = 0 # a single value shared over all outputs #np.zeros((hidden_dim, 1))\n\n    def FeedForward(self, x):\n        # let's calculate the hidden states\n        a = [np.zeros((self.hidden_dim,1))]\n        y = []\n        for ii in range(len(x)):\n\n            a_next = np.tanh(np.dot(self.Waa, a[ii])+np.dot(self.Wax,x[ii].reshape(-1,1))+self.ba)\n            a.append(a_next)\n            y_local = np.dot(self.Way,a_next)+self.by\n            y.append(np.dot(self.Way,a_next)+self.by)\n\n        # remove the first a and y values used for initialisation\n        #a = a[1:]\n        return y, a\n\n    def ComputeLossFunction(self, y_pred, y_actual):\n        # for a normal many to many model:\n        #loss = np.sum((y_pred - y_actual) ** 2)\n        # in our case, we are only using the last value so we expect scalar values here rather than a vector\n        loss = (y_pred[-1] - y_actual) ** 2\n        return loss\n\n    def ComputeGradients(self, a, x, y_pred, y_actual):\n        # Backpropagation through time\n        dLdy = []\n        dLdby = np.zeros((self.output_dim, 1))\n        dLdWay = np.random.randn(self.output_dim, self.hidden_dim)/5.0\n        dLdWax = np.random.randn(self.hidden_dim, self.input_dim)/5.0\n        dLdWaa = np.zeros((self.hidden_dim, self.hidden_dim))\n        dLda = np.zeros_like(a)\n        dLdba = np.zeros((self.hidden_dim, 1))\n\n        for t in range(self.hidden_dim-1, 0, -1):\n            if t == self.hidden_dim-1:\n                dldy = 2*(y_pred[t] - y_actual)\n            else:\n                dldy = 0\n            dLdy.append(dldy)\n            #dLdby.append(dldy)\n            dLdby += dldy\n            #print(dldy.shape)\n            dLdWay += np.dot(np.array(dldy).reshape(-1,1), a[t].T)\n\n            # Calculate gradient of loss with respect to a[t]\n            if t == self.hidden_dim-1:\n                dlda_t= np.dot(self.Way.T, np.array(dldy).reshape(-1,1))\n\n            else:\n                dlda_t = np.dot(self.Way.T, np.array(dldy).reshape(-1,1)) + np.dot(self.Waa, dLda[t+1]) * (1 - a[t]**2)\n            dLda[t] = dlda_t\n            #print(dlda_t.shape)\n\n            rec_term = (1-a[t]*a[t])\n\n            dLdWax += np.dot(dlda_t, x[t].reshape(-1,1))*rec_term\n            dLdWaa += np.dot(dlda_t, a[t-1].T)*rec_term\n            dLdba += dlda_t*rec_term\n\n        return dLdy[::-1], dLdby[::-1], dLdWay, dLdWax, dLdWaa, dLdba\n\n    def UpdateParameters(self,dLdby, dLdWay, dLdWax, dLdWaa, dLdba,learning_rate):\n        self.Waa -= learning_rate * dLdWaa\n        self.Wax -= learning_rate * dLdWax\n        self.Way -= learning_rate * dLdWay\n        self.ba -= learning_rate * dLdba\n        self.by -= learning_rate * dLdby    \n\n    def predict(self, x, n, a_training):\n        # let's calculate the hidden states\n        a_future = a_training\n        y_predict = []\n\n        # Predict the next n terms\n        for ii in range(n):\n            a_next = np.tanh(np.dot(self.Waa, a_future[-1]) + np.dot(self.Wax, x[ii]) + self.ba)\n            a.append(a_next)\n            y_predict.append(np.dot(self.Way, a_next) + self.by)\n\n        return y_predict\n```", "```py\ninput_dim = 1\noutput_dim = 1\nhidden_dim = 50\n\nlearning_rate = 1e-3\n\n# Initialize The RNN model\nrnn_model = SimpleRNN(input_dim, output_dim, hidden_dim)\n\n# train the model for 200 epochs\n\nfor epoch in range(200):\n    for ii in range(len(X_train)):\n        y_pred, a = rnn_model.FeedForward(X_train[ii])\n        loss = rnn_model.ComputeLossFunction(y_pred, y_train[ii])\n        dLdy, dLdby, dLdWay, dLdWax, dLdWaa, dLdba = rnn_model.ComputeGradients(a, X_train[ii], y_pred, y_train[ii])\n        rnn_model.UpdateParameters(dLdby, dLdWay, dLdWax, dLdWaa, dLdba, learning_rate)\n        print(f'Loss: {loss}')\n\ny_test_predicted = []\nfor jj in range(len(X_test)):\n    forecasted_values, _ = rnn_model.FeedForward(X_test[jj])\n    y_test_predicted.append(forecasted_values[-1])\n\ny_test_predicted_flat = np.array([val[0, 0] for val in y_test_predicted])\ntrace1 = go.Scatter(y = y_test.ravel(), mode =\"lines\", name = \"original data\")\ntrace2 = go.Scatter(y=y_test_predicted_flat, mode = \"lines\", name = \"RNN output\")\nlayout = go.Layout(title='Testing data Fit', xaxis=dict(title='X-Axis'), yaxis=dict(title='Dependent Variable'))\nfigure = go.Figure(data = [trace1,trace2], layout = layout)\n\niplot(figure)\n```"]