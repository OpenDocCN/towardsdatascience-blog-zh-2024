- en: 'BlazeFace: How to Run Real-time Object Detection in the Browser'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BlazeFace：如何在浏览器中运行实时目标检测
- en: 原文：[https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17](https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17](https://towardsdatascience.com/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=collection_archive---------2-----------------------#2024-07-17)
- en: A step-by-step guide to training a BlazeFace model, from the Python training
    pipeline to the JavaScript demo through model conversion.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一份逐步指南，介绍如何训练BlazeFace模型，从Python训练管道到通过模型转换实现JavaScript演示。
- en: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--66c2ac9acd75--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)
    ·11 min read·Jul 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--66c2ac9acd75--------------------------------)
    ·11分钟阅读·2024年7月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/05b105d691b1a727b04e20b7e15caa57.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05b105d691b1a727b04e20b7e15caa57.png)'
- en: Freely adapted from a photo by [visuals](https://unsplash.com/@visuals?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/man-in-black-suit-jacket-smiling-Y4qzW3AsvqI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 参考自[visuals](https://unsplash.com/@visuals?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)在[Unsplash](https://unsplash.com/photos/man-in-black-suit-jacket-smiling-Y4qzW3AsvqI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)上的一张照片。
- en: Thanks to libraries such as [YOLO by Ultralytics](https://docs.ultralytics.com/),
    it is fairly easy today to make robust object detection models with as little
    as a few lines of code. Unfortunately, those solutions are not yet fast enough
    to work in a web browser on a real-time video stream at 30 frames per second (which
    is usually considered the real-time limit for video applications) on any device.
    More often than not, it will run at less than 10 fps on an average mobile device.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于[YOLO by Ultralytics](https://docs.ultralytics.com/)等库，今天我们可以通过几行代码轻松地创建稳健的目标检测模型。遗憾的是，这些解决方案在30帧每秒的视频实时流中，在任何设备上都不够快（通常认为30帧每秒是视频应用的实时限制）。在大多数情况下，它们在普通的移动设备上运行时，帧率通常低于10帧每秒。
- en: The most famous real-time object detection solution on web browser is [Google’s
    MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/object_detector).
    This is a really convenient and versatile solution, as it can work on many devices
    and platforms easily. But what if you want to make your own solution?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在浏览器中最著名的实时目标检测解决方案是[Google的MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/vision/object_detector)。这是一个非常方便且多功能的解决方案，可以轻松地在许多设备和平台上运行。但如果你想制作自己的解决方案呢？
- en: In this post, we propose to build our own lightweight, fast and robust object
    detection model, that runs at more than 30 fps on almost any devices, based on
    the BlazeFace model. All the code used for this is available on my [GitHub](https://github.com/vincent-vdb/medium_posts/tree/main/blazeface),
    in the *blazeface* folder.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提出构建一个自己的轻量级、快速且稳健的目标检测模型，基于BlazeFace模型，该模型在几乎所有设备上以超过30帧每秒的速度运行。所有用于此的代码都可以在我的[GitHub](https://github.com/vincent-vdb/medium_posts/tree/main/blazeface)上的*blazeface*文件夹中找到。
- en: The [BlazeFace](https://arxiv.org/abs/1907.05047) model, proposed by Google
    and originally used in MediaPipe for face detection, is really small and fast,
    while being robust enough for easy object detection tasks such as face detection.
    Unfortunately, to my knowledge, no training pipeline of this model is available
    online on GitHub; all I could find is [this inference-only model architecture](https://github.com/hollance/BlazeFace-PyTorch).
    Through this post, we will train our own BlazeFace model with a fully working
    pipeline and use it on browser with a working JavaScript code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[BlazeFace](https://arxiv.org/abs/1907.05047)模型由 Google 提出，最初用于 MediaPipe 中的人脸检测，体积小且速度快，同时对人脸检测等简单物体检测任务具有足够的鲁棒性。不幸的是，据我所知，GitHub
    上没有该模型的训练流程；我所能找到的只有[这个仅用于推理的模型架构](https://github.com/hollance/BlazeFace-PyTorch)。通过本文，我们将训练我们自己的
    BlazeFace 模型，构建一个完整的工作流程，并使用能够运行 JavaScript 代码的浏览器。'
- en: 'More specifically, we will go through the following steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们将经过以下步骤：
- en: Training the model using [PyTorch](https://pytorch.org/)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 [PyTorch](https://pytorch.org/) 训练模型
- en: Converting the PyTorch model into a TFLite model
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 PyTorch 模型转换为 TFLite 模型
- en: Running the object detection in the browser thanks to JavaScript and TensorFlow.js
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在浏览器中运行物体检测，得益于 JavaScript 和 TensorFlow.js
- en: Let’s get started with the model training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始模型训练吧。
- en: Training the PyTorch Model
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 PyTorch 模型
- en: 'As usual when training a model, there are a few typical steps in a training
    pipeline:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，在训练模型时，训练流程中有几个典型的步骤：
- en: 'Preprocessing the data: we will use a freely available Kaggle dataset for simplicity,
    but any dataset with the right format of labels would work'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理：为了简便，我们将使用一个公开可用的 Kaggle 数据集，但任何格式正确的标签数据集都可以使用
- en: 'Building the model: we will reuse the proposed architecture in the original
    paper and the inference-only GitHub code'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建模型：我们将复用原论文中提出的架构和仅用于推理的 GitHub 代码
- en: 'Training and evaluating the model: we will use a simple Multibox loss as the
    cost function to minimize'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型：我们将使用一个简单的 Multibox 损失作为最小化的代价函数
- en: Let’s go through those steps together.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起走过这些步骤。
- en: Data Preprocessing
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: We are going to use a subset of the [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)
    V7, proposed by Google. This dataset is made of about 9 million images with many
    annotations (including bounding boxes, segmentation masks, and many others). The
    dataset itself is quite large and contains many types of images.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google 提出的 [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)
    V7 数据集的一个子集。该数据集包含大约 900 万张图像，附有许多注释（包括边界框、分割掩膜等）。该数据集本身相当庞大，包含多种类型的图像。
- en: 'For our specific use case, I decided to select images in the validation set
    fulfilling two specific conditions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的具体用例，我决定选择验证集中的图像，并满足两个特定条件：
- en: Containing labels of human face bounding box
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含人脸边界框标签
- en: Having a permissive license for such a use case, more specifically the [CC BY
    2.0](https://creativecommons.org/licenses/by/2.0/) license
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有适用于此类用例的宽松许可证，具体来说是[CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)许可证
- en: The script to download and build the dataset under those strict conditions is
    provided in the GitHub, so that anyone can reproduce it. The downloaded dataset
    with this script contains labels in the YOLO format (meaning box center, width
    and height). In the end, the downloaded dataset is made of about 3k images and
    8k faces, that I have separated into train and validation set with a 80%-20% split
    ratio.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在这些严格条件下下载和构建数据集的脚本已提供在 GitHub 上，任何人都可以复现。通过这个脚本下载的数据集包含 YOLO 格式的标签（即框的中心、宽度和高度）。最终，下载的数据集由大约
    3000 张图像和 8000 个面孔组成，我已将其分为训练集和验证集，比例为 80%-20%。
- en: 'From this dataset, typical preprocessing it required before being able to train
    a model. The data preprocessing code I used is the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，在能够训练模型之前，通常需要进行以下预处理。以下是我使用的数据预处理代码：
- en: 'Data preprocessing class for model training with PyTorch. Some code has been
    omitted for clarity: full code is available on GitHub.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 PyTorch 模型训练的数据预处理类。为了简洁，部分代码已被省略：完整代码可在 GitHub 上找到。
- en: 'As we can see, the preprocessing is made of the following steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据预处理包括以下几个步骤：
- en: It loads images and labels
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它加载图像和标签
- en: It converts labels from YOLO format (center position, width, height) to box
    corner format (top-left corner position, bottom-right corner position)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将标签从 YOLO 格式（中心位置、宽度、高度）转换为框角格式（左上角位置、右下角位置）
- en: It resizes images to the target size (e.g. 128 pixels), and adds padding if
    necessary to keep the original image aspect ratio and avoid image deformation.
    Finally, it normalizes the images.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将图像调整为目标尺寸（例如 128 像素），如果需要，添加填充以保持原始图像的纵横比，并避免图像变形。最后，它会对图像进行归一化处理。
- en: 'Optionally, this code allows for data augmentation using [Albumentations](https://albumentations.ai/).
    For the training, I used the following data augmentations:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，这段代码允许使用 [Albumentations](https://albumentations.ai/) 进行数据增强。在训练过程中，我使用了以下数据增强技术：
- en: Horizontal flip
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平翻转
- en: Random brightness contrast
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机亮度对比度
- en: Random crop from borders
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从边界随机裁剪
- en: Affine transformation
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仿射变换
- en: 'Those augmentations will allow us to have a more robust, regularized model.
    After all those transformations and augmentations, the input data may look like
    the following sample:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增强操作将使我们拥有一个更强健、更具正则化的模型。经过这些转换和增强处理后，输入数据可能会呈现出以下样本：
- en: '![](../Images/f180793b42b7b04702cec440680ad18f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f180793b42b7b04702cec440680ad18f.png)'
- en: Preprocessed images, with data augmentation, used to train the model. Image
    by author, made of images from the [Open Images Dataset](https://storage.googleapis.com/openimages/web/download_v7.html).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 经过数据增强的预处理图像，用于训练模型。图像由作者制作，素材来自 [Open Images 数据集](https://storage.googleapis.com/openimages/web/download_v7.html)。
- en: As we can see, the preprocessed images have grey borders because of augmentation
    (with rotation or translation) or padding (because the original image did not
    have a square aspect ratio). They all contain faces, although the context might
    be really different depending on the image.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，经过预处理的图像由于增强（如旋转或平移）或填充（因为原始图像不是方形纵横比）而具有灰色边框。所有这些图像都包含面部，尽管背景可能因图像而异。
- en: '***Important Note:***'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '***重要提示:***'
- en: '*Face detection is a highly sensitive task with significant ethical and safety
    considerations. Bias in the dataset, such as underrepresentation or overrepresentation
    of certain facial characteristics, can lead to false negatives or false positives,
    potentially causing harm or offense. See below a dedicated section about ethical
    considerations.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*人脸检测是一个高度敏感的任务，涉及重大的伦理和安全问题。数据集中的偏差，如某些面部特征的代表性不足或过度代表，可能导致假阴性或假阳性，从而可能造成伤害或冒犯。请参阅下文有关伦理考虑的专门章节。*'
- en: 'Now that our data can be loaded and preprocessed, let’s go to the next step:
    building the model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经可以加载和预处理了，接下来我们进入下一步：构建模型。
- en: Model Building
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型构建
- en: In this section, we will build the model architecture of the original BlazeFace
    model, based on the original article and adapted from the [BlazeFace repository](https://github.com/hollance/BlazeFace-PyTorch)
    containing inference code only.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将根据原始文章并从 [BlazeFace 仓库](https://github.com/hollance/BlazeFace-PyTorch)（仅包含推理代码）改编，构建原始
    BlazeFace 模型的架构。
- en: The whole BlazeFace architecture is rather simple and is mostly made of what
    the paper’s author call a BlazeBlock, with various parameters.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 整个 BlazeFace 架构相当简单，主要由论文作者称之为 BlazeBlock 的结构组成，并且包含不同的参数。
- en: 'The BlazeBlock can be defined with PyTorch as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: BlazeBlock 可以通过 PyTorch 这样定义：
- en: Implementation of the BlazeBlock, of which the BlazeFace is made of. Full code
    available on GitHub.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: BlazeBlock 的实现，BlazeFace 由其构成。完整代码可在 GitHub 上获取。
- en: 'As we can see from this code, a BlazeBlock is simply made of the following
    layers:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码中我们可以看到，BlazeBlock 由以下几个层组成：
- en: One depthwise 2D convolution layer
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个深度卷积 2D 层
- en: One batch norm 2D layer
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批归一化 2D 层
- en: One 2D convolution layer
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 2D 卷积层
- en: One batch norm 2D layer
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批归一化 2D 层
- en: '*N.B.: You can read the PyTorch documentation for more about these layers:*
    [*Conv2D layer*](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)
    *and* [*BatchNorm2D layer*](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)*.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：你可以阅读 PyTorch 文档了解更多关于这些层的信息：* [*Conv2D 层*](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)
    *和* [*BatchNorm2D 层*](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)*.*'
- en: This block is repeated many times with different input parameters, to go from
    a 128-pixel image up to a typical object detection prediction using tensor reshaping
    in the final stages. Feel free to have a look at the full code in the GitHub repository
    for more about the implementation of this architecture.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块会被多次重复，使用不同的输入参数，将 128 像素的图像处理到最终阶段的典型物体检测预测中，最终通过张量重塑来完成。欢迎查看 GitHub 仓库中的完整代码，了解该架构的实现。
- en: 'Before moving to the next section about training the model, note that there
    are actually two architectures:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入关于训练模型的下一部分之前，请注意，实际上有两种架构：
- en: A 128-pixel input image architecture
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 128像素输入图像架构
- en: A 256-pixel input image architecture
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 256像素输入图像架构
- en: As you can imagine, the 256-pixel architecture is slightly larger, but still
    lightweight and sometimes more robust. This architecture is also implemented in
    the provided code, so that you can use it if you want.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想，256像素的架构略大，但仍然轻量且有时更具鲁棒性。该架构也已在提供的代码中实现，因此如果需要，你可以使用它。
- en: '*N.B.: The original BlazeFace model not only predicts a bounding box, but also
    six approximate face landmarks. Since I did not have such labels, I simplified
    the model architecture to predict only the bounding boxes.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：原始BlazeFace模型不仅预测边界框，还预测六个大致的人脸关键点。由于我没有此类标签，我简化了模型架构，仅预测边界框。*'
- en: 'Now that we can build a model, let’s move on to the next step: training the
    model.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建一个模型了，接下来让我们进入下一步：训练模型。
- en: Model Training
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'For anyone familiar with PyTorch, training models such as this one is usually
    quite simple and straightforward, as shown in this code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉PyTorch的人来说，训练这样的模型通常非常简单直接，如这段代码所示：
- en: Code used to train the BlazeFace model. Full code available on GitHub.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练BlazeFace模型的代码。完整代码可在GitHub上获取。
- en: 'As we can see, the idea is to loop over your data for a given number of epochs,
    one batch at a time, and do the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，关键是对数据进行多次循环，每次一个批次，重复进行以下操作：
- en: Get the processed data and corresponding labels
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取处理后的数据和相应的标签。
- en: Make the forward inference
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行前向推理
- en: Compute the loss of the inference against the label
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算推理结果与标签之间的损失
- en: Update the weights
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新权重
- en: I am not getting into all the details for clarity in this post, but feel free
    to navigate through the code to get a better sense of the training part if needed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持文章的清晰度，我不会进入所有的细节，但如果需要，你可以通过浏览代码更好地了解训练部分。
- en: 'After training on 100 epochs, I had the following results on the validation
    set:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过100个epoch的训练后，我在验证集上得到了以下结果：
- en: '![](../Images/47ff2c73041c624853d570fdc4a90a0e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47ff2c73041c624853d570fdc4a90a0e.png)'
- en: Results of the model on the validation set after 50 epochs. Green boxes are
    ground truth labels, red boxes are model predictions. Image by author, made of
    images from the [Open Images Dataset](https://storage.googleapis.com/openimages/web/download_v7.html).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在验证集上经过50个epoch后的结果。绿色框是实际标签，红色框是模型预测。图像由作者提供，图片来源于[开放图像数据集](https://storage.googleapis.com/openimages/web/download_v7.html)。
- en: As we can see on those results, even if the object detection is not perfect,
    it works pretty well for most cases (probably the IoU threshold was not optimal,
    leading sometimes to overlapping boxes). Keep in mind it’s a very light model;
    it can’t exhibit the same performances as a YOLOv8, for example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这些结果中看到的，即使物体检测不是完美的，它在大多数情况下表现得相当不错（可能是IoU阈值不理想，导致有时出现重叠框）。请记住，这是一个非常轻量的模型，它不能像YOLOv8那样展示相同的性能。
- en: Before going to the next step about converting the model, let’s have a short
    discussion about ethical and safety considerations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入关于转换模型的下一步之前，让我们简短地讨论一下伦理和安全的考量。
- en: Ethical and Safety Considerations
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理与安全考虑
- en: 'Let’s go over a few points about ethics and safety, since face detection can
    be a very sensitive topic:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下有关伦理和安全的几个要点，因为人脸检测可能是一个非常敏感的话题：
- en: '**Dataset importance and selection:** This dataset is used to demonstrate face
    detection techniques for educational purposes. It was chosen for its relevance
    to the topic, but it may not fully represent the diversity needed for unbiased
    results.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集的重要性与选择：** 该数据集用于展示人脸检测技术，旨在教育目的。它因与主题的相关性而被选择，但可能并未充分代表无偏结果所需的多样性。'
- en: '**Bias awareness:** The dataset is not claimed to be bias-free, and potential
    biases have not been fully mitigated. Please be aware of potential biases that
    can affect the accuracy and fairness of face detection models.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差意识：** 数据集并未声称没有偏差，潜在的偏差尚未完全消除。请注意，潜在的偏差可能会影响人脸检测模型的准确性和公平性。'
- en: '**Risks:** The trained face detection model may reflect these biases, raising
    potential ethical concerns. Users should critically assess the outcomes and consider
    the broader implications.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险：** 训练后的人脸检测模型可能会反映这些偏差，从而引发潜在的伦理问题。用户应批判性地评估结果，并考虑更广泛的影响。'
- en: 'To address these concerns, anyone willing to build a product on such topic
    should focus on:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些问题，任何希望在这一领域构建产品的人都应该关注：
- en: Collecting diverse and representative images
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集多样化和具有代表性的图像
- en: Verifying the data is bias-free and any category is equally represented
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保数据没有偏差，并且每个类别都得到平等代表
- en: Continuously evaluating the ethical implications of face detection technologies
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续评估人脸检测技术的伦理影响
- en: '*N.B.: A useful approach to address these concerns is to examine what Google
    did for their own* [*face detection*](https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf)
    *and* [*face landmarks*](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20MediaPipe%20Face%20Mesh%20V2.pdf)
    *models*.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：一个有用的方式是查看Google在其自己的* [*人脸检测*](https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf)
    *和* [*人脸关键点*](https://storage.googleapis.com/mediapipe-assets/Model%20Card%20MediaPipe%20Face%20Mesh%20V2.pdf)
    *模型上所做的工作。*'
- en: Again, the used dataset is intended solely for educational purposes. Anyone
    willing to use it should exercise caution and be mindful of its limitations when
    interpreting results. Let’s now move to the next step with the model conversion.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，使用的数据集仅用于教育目的。任何希望使用该数据集的人都应谨慎，并在解释结果时考虑其局限性。现在我们可以进入下一步，即模型转换。
- en: Converting the Model
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型转换
- en: Remember that our goal is to make our object detection model work in a web browser.
    Unfortunately, once we have a trained PyTorch model, we can not directly use it
    in a web browser. We first need to convert it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的目标是使我们的目标检测模型在Web浏览器中工作。不幸的是，一旦我们训练好了PyTorch模型，就无法直接在浏览器中使用它。我们需要首先将其转换。
- en: Currently, to my knowledge, the most reliable way to run a deep learning model
    in a web browser is by using a [TFLite](https://www.tensorflow.org/lite) model
    with T[ensorFlow.js](https://www.tensorflow.org/js). In other words, we need to
    convert our PyTorch model into a TFLite model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，据我所知，在Web浏览器中运行深度学习模型的最可靠方法是使用[TFLite](https://www.tensorflow.org/lite)模型与[TensorFlow.js](https://www.tensorflow.org/js)。换句话说，我们需要将PyTorch模型转换为TFLite模型。
- en: '*N.B.: Some alternative ways are emerging, such as* [*ExecuTorch*](https://pytorch.org/executorch-overview)*,
    but they do not seem to be mature enough yet for web use.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：一些替代方案正在出现，例如* [*ExecuTorch*](https://pytorch.org/executorch-overview)*，但它们似乎还不够成熟，无法用于Web。*'
- en: As far as I know, there is no robust, reliable way to do so directly. But there
    are side ways, by going through ONNX. [ONNX](https://onnx.ai/) (which stands for
    Open Neural Network Exchange) is a standard for storing and running (using [ONNX
    Runtime](https://onnxruntime.ai/)) machine learning models. Conveniently, there
    are available libraries for conversion from torch to ONNX, as well as from ONNX
    to TensorFlow models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 据我所知，目前没有直接可靠的方式来实现这一点。但有一些间接的方法，可以通过ONNX来实现。[ONNX](https://onnx.ai/)（即开放神经网络交换）是一个用于存储和运行（使用[ONNX
    Runtime](https://onnxruntime.ai/)）机器学习模型的标准。方便的是，已经有了从Torch到ONNX的转换库，以及从ONNX到TensorFlow模型的转换库。
- en: 'To summarize, the conversion workflow is made of the three following steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，转换工作流程由以下三个步骤组成：
- en: Convert from PyTorch to ONNX
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从PyTorch转换为ONNX
- en: Convert from ONNX to TensorFlow
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从ONNX转换为TensorFlow
- en: Convert from TensorFlow to TFLite
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从TensorFlow转换为TFLite
- en: 'This is exactly what the following code does:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是以下代码所做的事情：
- en: Model conversion from PyTorch format to TFLite format, through ONNX. Full code
    available on GitHub.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将PyTorch格式的模型转换为TFLite格式，通过ONNX。完整代码可在GitHub上获取。
- en: This code can be slightly more cryptic than the previous ones, as there are
    some specific optimizations and parameters used to make it work properly. One
    can also try to go one step further and quantize the TFLite model to make it even
    smaller. If you are interested in doing so, you can have a look at the [official
    documentation](https://www.tensorflow.org/lite/performance/post_training_quantization).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可能比之前的代码稍显复杂，因为有一些特定的优化和参数用于确保其正常运行。你也可以尝试更进一步，对TFLite模型进行量化，使其更小。如果你有兴趣，可以查看[官方文档](https://www.tensorflow.org/lite/performance/post_training_quantization)。
- en: '*N.B.: The conversion code is highly sensitive of the versions of the libraries.
    To ensure a smooth conversion, I would strongly recommend using the specified
    versions in the requirements.txt file on GitHub.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：转换代码对库的版本非常敏感。为了确保顺利转换，我强烈建议使用GitHub上的requirements.txt文件中指定的版本。*'
- en: On my side, after TFLite conversion, I finally have a TFLite model of only about
    400kB, which is lightweight and quite acceptable for web usage. Next step is to
    actually test it out in a web browser, and to make sure it works as expected.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我这边，经过TFLite转换后，我终于得到了一个仅约400kB的TFLite模型，体积小巧，非常适合网页使用。下一步是实际在网页浏览器中进行测试，并确保它按预期工作。
- en: 'On a side note, be aware that another solution is currently being developed
    by Google for PyTorch model conversion to TFLite format: [AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch).
    Unfortunately, this is quite new and I couldn’t make it work for my use case.
    However, any feedback about this library is very welcome.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，Google目前正在开发另一种解决方案，用于将PyTorch模型转换为TFLite格式：[AI Edge Torch](https://github.com/google-ai-edge/ai-edge-torch)。不幸的是，这个解决方案相当新，我没能使其在我的用例中工作。不过，任何关于这个库的反馈都非常欢迎。
- en: Running the Model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行模型
- en: Now that we finally have a TFLite model, we are able to run it in a web browser
    using TensorFlow.js. If you are not familiar with JavaScript (since this is not
    usually a language used by data scientists and machine learning engineers) do
    not worry; all the code is provided and is rather easy to understand.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于得到了一个TFLite模型，能够在网页浏览器中使用TensorFlow.js运行它。如果你不熟悉JavaScript（因为这通常不是数据科学家和机器学习工程师常用的语言），不用担心；所有代码都已提供，并且相当容易理解。
- en: 'I won’t comment all the code here, just the most relevant parts. If you look
    at the code on [GitHub](https://github.com/vincent-vdb/medium_posts), you will
    see the following in the *javascript* folder:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会在这里对所有代码进行注释，只注释最相关的部分。如果你查看[GitHub](https://github.com/vincent-vdb/medium_posts)上的代码，你会看到在*javascript*文件夹中有以下内容：
- en: '*index.html*: contains the home page running the whole demo'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*index.html*：包含运行整个演示的主页'
- en: '*assets*: the folder containing the TFLite model that we just converted'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*assets*：包含我们刚刚转换的TFLite模型的文件夹'
- en: '*js*: the folder containing the JavaScript codes'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*js*：包含JavaScript代码的文件夹'
- en: 'If we take a step back, all we need to do in the JavaScript code is to loop
    over the frames of the camera feed (either a webcam on a computer or the front-facing
    camera on a mobile phone) and do the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退后一步来看，在JavaScript代码中我们需要做的就是遍历摄像头视频流的每一帧（无论是计算机上的网络摄像头还是手机上的前置摄像头），然后执行以下操作：
- en: 'Preprocess the image: resize it as a 128-pixel image, with padding and normalization'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行预处理：将其调整为128像素的图像，进行填充和归一化
- en: Compute the inference on the preprocessed image
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对预处理后的图像进行推理计算
- en: 'Postprocess the model output: apply thresholding and non max suppression to
    the detections'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型输出进行后处理：应用阈值化和非极大抑制来处理检测结果
- en: 'We won’t comment the image preprocessing since this would be redundant with
    the Python preprocessing, but feel free to have a look at the code. When it comes
    to making an inference with a TFLite model in JavaScript, it’s fairly easy:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会评论图像预处理部分，因为这与Python预处理部分是重复的，但你可以随时查看代码。至于在JavaScript中进行TFLite模型推理，其实非常简单：
- en: Simplistic example of code to instantiate a TFLite model and compute an inference,
    assuming an image of the right shape. Full working code on GitHub.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的代码示例，用于实例化TFLite模型并计算推理，假设图像形状正确。完整的可工作代码在GitHub上。
- en: 'The tricky part is actually the postprocessing. As you may know, the output
    of a SSD object detection model is not directly usable: this is not the bounding
    boxes locations. Here is the postprocessing code that I used:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 棘手的部分实际上是后处理。正如你所知，SSD目标检测模型的输出是不能直接使用的：这并不是边界框的位置。以下是我使用的后处理代码：
- en: Postprocessing the BlazeFace model output in JavaScript. Full code on GitHub.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在JavaScript中对BlazeFace模型输出进行后处理。完整代码在GitHub上。
- en: 'In the code above, the model output is postprocessed with the following steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，模型输出经过以下步骤的后处理：
- en: The boxes locations are corrected with the anchors
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用锚点纠正框的位置
- en: The box format is converted to get the top-left and the bottom-right corners
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将框格式转换为获取左上角和右下角坐标
- en: Non-max suppression is applied to the boxes with the detection score, allowing
    the removal of all boxes below a given threshold and overlapping other already-existing
    boxes
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对带有检测分数的框应用非极大抑制，移除所有低于给定阈值的框，并去除与其他已存在框重叠的框
- en: This is exactly what has been done in Python too to display the resulting bounding
    boxes, if it may help you get a better understanding of that part.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是Python中所做的操作，用于显示生成的边界框，如果这能帮助你更好地理解这一部分内容的话。
- en: 'Finally, below is a screenshot of the resulting web browser demo:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下是生成的网页浏览器演示的截图：
- en: '![](../Images/7531f025b2456d12dcd9b3b675f298ca.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7531f025b2456d12dcd9b3b675f298ca.png)'
- en: Screenshot of the running demo in the web browser, with picture-in-picture by
    [Vitaly Gariev](https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw)
    on Unsplash
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 网页浏览器中运行演示的截图，画中画由[Vitaly Gariev](https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw)提供，图片来自Unsplash
- en: As you can see, it properly detects the face in the image. I decided to use
    a [static image from Unsplash](https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw),
    but the code on GitHub allows you to run it on your webcam, so feel free to test
    it yourself.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它正确地检测到了图像中的人脸。我决定使用一张来自[Unsplash的静态图片](https://unsplash.com/photos/a-man-sitting-at-a-table-in-front-of-a-laptop-wAUjHURVbDw)，但GitHub上的代码允许你在你的网络摄像头上运行它，因此请随时自己测试。
- en: Before concluding, note that if you run this code on your own computer or smartphone,
    depending on your device you may not reach 30 fps (on my personal laptop having
    a rather old 2017 [Intel® Core™ i5–8250U](https://www.intel.fr/content/www/fr/fr/products/sku/124967/intel-core-i58250u-processor-6m-cache-up-to-3-40-ghz/specifications.html),
    it runs at 36fps). If that’s the case, a few tricks may help you get there. The
    easiest one is to run the model inference only once every N frames (N to be fine
    tuned depending on your application, of course). Indeed, in most cases, from one
    frame to the next, there are not many changes, and the boxes can remain almost
    unchanged.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结之前，请注意，如果你在自己的电脑或智能手机上运行此代码，具体取决于你的设备，你可能无法达到30帧每秒（在我个人的笔记本电脑上，搭载的是一款较旧的2017年[Intel®
    Core™ i5–8250U](https://www.intel.fr/content/www/fr/fr/products/sku/124967/intel-core-i58250u-processor-6m-cache-up-to-3-40-ghz/specifications.html)，它的运行速度为36fps）。如果是这种情况，一些技巧可能会帮助你达到目标。最简单的办法是每N帧运行一次模型推理（N的值可以根据你的应用场景进行微调）。实际上，在大多数情况下，从一帧到下一帧变化不大，框的位置几乎可以保持不变。
- en: Conclusion
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: I hope you enjoyed reading this post and thanks if you got this far. Even though
    doing object detection is fairly easy nowadays, doing it with limited resources
    can be quite challenging. Learning about BlazeFace and converting models for web
    browser gives some insights into how MediaPipe was built, and opens the way to
    other interesting applications such as blurring backgrounds in video call (like
    Google Meets or Microsoft Teams) in real time in the browser.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢阅读这篇文章，如果你看到这里，非常感谢。尽管现在做物体检测相对简单，但在资源有限的情况下做物体检测仍然非常具有挑战性。了解BlazeFace并将模型转换为网页浏览器应用，可以深入了解MediaPipe是如何构建的，并为其他有趣的应用打开了大门，例如在视频通话中实时模糊背景（如Google
    Meet或Microsoft Teams）。
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: The [Open Images Dataset publication](https://arxiv.org/abs/1811.00982)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Open Images数据集的发布](https://arxiv.org/abs/1811.00982)'
- en: The [GitHub repo](https://github.com/vincent-vdb/medium_posts) containing all
    the working code in the folder blazeface
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub仓库](https://github.com/vincent-vdb/medium_posts)，其中包含在blazeface文件夹中的所有工作代码'
- en: The [original GitHub](https://github.com/hollance/BlazeFace-PyTorch/blob/master/blazeface.py)
    containing inference code for BlazeFace
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[包含BlazeFace推理代码的原始GitHub](https://github.com/hollance/BlazeFace-PyTorch/blob/master/blazeface.py)'
- en: The [BlazeFace paper](https://arxiv.org/abs/1907.05047)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[BlazeFace论文](https://arxiv.org/abs/1907.05047)'
- en: '[MediaPipe’s face detection](https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector)
    and [BlazeFace model card](https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MediaPipe的人脸检测](https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector)
    和 [BlazeFace模型卡片](https://storage.googleapis.com/mediapipe-assets/MediaPipe%20BlazeFace%20Model%20Card%20(Short%20Range).pdf)'
- en: '[ONNX](https://onnx.ai/) and [ONNX Runtime](https://onnxruntime.ai/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ONNX](https://onnx.ai/) 和 [ONNX Runtime](https://onnxruntime.ai/)'
- en: '[TFLite quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TFLite量化](https://www.tensorflow.org/lite/performance/post_training_quantization)'
