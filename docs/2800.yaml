- en: Attention (Is Not) All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19](https://towardsdatascience.com/attention-is-not-all-you-need-ef7b6956d425?source=collection_archive---------4-----------------------#2024-11-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative approach to the transformer model for text generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[![Josh
    Taylor](../Images/e3c9cb25df3e0b870d28b5844cd3ddff.png)](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    [Josh Taylor](https://medium.com/@thejoshtaylor?source=post_page---byline--ef7b6956d425--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ef7b6956d425--------------------------------)
    ·7 min read·Nov 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8840625d116b00a261505a81da53e8a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Can fractal patterns help us to create a more efficient text generation model?
    Photo by [Giulia May](https://unsplash.com/@giuliamay?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Since the release of ChatGPT at the end of November 2022, LLMs (Large Language
    Models) have, almost, become a household name.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2dd39a5162081d74bf681e8a8b5c6908.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Worldwide search interest for ‘LLM’. Source: [Google Trends](https://trends.google.com/trends/explore?date=today+5-y&q=llm&hl=en-GB)'
  prefs: []
  type: TYPE_NORMAL
- en: There is good reason for this; their success lies in their architecture, particularly
    the **attention mechanism.** It allows the model to compare every word they process
    to *every other* word.
  prefs: []
  type: TYPE_NORMAL
- en: This gives LLMs the extraordinary capabilities in understanding and generating
    human-like text that we are all familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: However, these models are not without flaws. They demand immense computational
    resources to train. For example, Meta’s Llama 3 model took 7.7 million GPU hours
    of training[1]. Moreover, their reliance on enormous datasets — spanning trillions
    of tokens — raises questions about scalability, accessibility, and environmental
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these challenges, ever since the paper ‘Attention is all you need’ in
    mid 2017, much of the recent progress in AI has focused on scaling attention mechanisms
    further, rather than exploring fundamentally new architectures.
  prefs: []
  type: TYPE_NORMAL
