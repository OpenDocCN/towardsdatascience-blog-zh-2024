- en: Is Less More? Do Deep Learning Forecasting Models Need Feature Reduction?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°‘å³æ˜¯å¤šï¼Ÿæ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹éœ€è¦ç‰¹å¾å‡å°‘å—ï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30](https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30](https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30)
- en: To curate, or not to curate, that is the question
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜¯å¦éœ€è¦ç­›é€‰ç‰¹å¾ï¼Œè¿™æ˜¯ä¸€ä¸ªé—®é¢˜
- en: '[](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Philippe
    Ostiguy, M. Sc.](../Images/8b292bc1baa848a0c5de821dc9576534.png)](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    [Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Philippe
    Ostiguy, M. Sc.](../Images/8b292bc1baa848a0c5de821dc9576534.png)](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    [Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    Â·12 min readÂ·Sep 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)
    Â·12 åˆ†é’Ÿé˜…è¯»Â·2024å¹´9æœˆ30æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d06437fd932d3128b235a26000f217c7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d06437fd932d3128b235a26000f217c7.png)'
- en: AI image created on MidJourney V6.1 by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI å›¾åƒç”±ä½œè€…åœ¨ MidJourney V6.1 ä¸Šåˆ›å»ºã€‚
- en: 'Time series forecasting is a powerful tool in data science, offering insights
    into future trends based on historical patterns. In our previous article, we explored
    [how making your time series stationary automatically](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)
    can significantly enhance model performance. But stationarity is just one piece
    of the puzzle. As we continue to refine our forecasting models, another crucial
    question arises: how do we handle the multitude of features our data may present?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯æ•°æ®ç§‘å­¦ä¸­çš„ä¸€é¡¹å¼ºå¤§å·¥å…·ï¼Œé€šè¿‡å†å²æ¨¡å¼æä¾›æœªæ¥è¶‹åŠ¿çš„æ´å¯Ÿã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº† [å¦‚ä½•ä½¿ä½ çš„æ—¶é—´åºåˆ—è‡ªåŠ¨å¹³ç¨³åŒ–](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)ï¼Œè¿™èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä½†å¹³ç¨³æ€§åªæ˜¯è§£å†³æ–¹æ¡ˆçš„ä¸€éƒ¨åˆ†ã€‚éšç€æˆ‘ä»¬ç»§ç»­ä¼˜åŒ–é¢„æµ‹æ¨¡å‹ï¼Œå¦ä¸€ä¸ªå…³é”®é—®é¢˜å‡ºç°äº†ï¼šå¦‚ä½•å¤„ç†æ•°æ®å¯èƒ½å‘ˆç°çš„ä¼—å¤šç‰¹å¾ï¼Ÿ
- en: As you work with time series data, youâ€™ll often find yourself with many potential
    features to include in your model. While itâ€™s tempting to use all available data,
    adding more features isnâ€™t always better. It can make your model more complex
    and slower to train, without necessarily improving its performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ—¶ï¼Œä½ ç»å¸¸ä¼šå‘ç°è‡ªå·±é¢ä¸´ç€è®¸å¤šå¯èƒ½çš„ç‰¹å¾ï¼Œéœ€è¦å°†å…¶åŒ…å«åœ¨æ¨¡å‹ä¸­ã€‚è™½ç„¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®å¾ˆæœ‰è¯±æƒ‘åŠ›ï¼Œä½†æ·»åŠ æ›´å¤šç‰¹å¾å¹¶ä¸æ€»æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚è¿™å¯èƒ½ä¼šä½¿ä½ çš„æ¨¡å‹å˜å¾—æ›´åŠ å¤æ‚ï¼Œè®­ç»ƒé€Ÿåº¦å˜æ…¢ï¼Œè€Œä¸ä¸€å®šä¼šæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: 'You might be wondering: is it important to simplify the feature set and what
    are the techniques available out there? Thatâ€™s exactly what weâ€™ll discuss in this
    article.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½åœ¨æƒ³ï¼šç®€åŒ–ç‰¹å¾é›†æ˜¯å¦é‡è¦ï¼Œç°æœ‰çš„æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿè¿™æ­£æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­è®¨è®ºçš„å†…å®¹ã€‚
- en: 'Hereâ€™s a quick summary of what will be covered:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æˆ‘ä»¬å°†è¦è®¨è®ºçš„å†…å®¹çš„ç®€è¦æ€»ç»“ï¼š
- en: '**Feature reduction in time series** â€” *Weâ€™ll explain the concept of feature
    reduction in time series analysis and why it matters.*'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ—¶é—´åºåˆ—ä¸­çš„ç‰¹å¾å‡å°‘** â€” *æˆ‘ä»¬å°†è§£é‡Šæ—¶é—´åºåˆ—åˆ†æä¸­ç‰¹å¾å‡å°‘çš„æ¦‚å¿µä»¥åŠå®ƒçš„é‡è¦æ€§ã€‚*'
- en: '**Practical implementation guide** â€” *Using Python, weâ€™ll walk through evaluating
    and selecting features for your time series model, providing hands-on tools to
    optimize your approach. Weâ€™ll also assess whether trimming down features is necessary
    for our forecasting models.*'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®é™…å®æ–½æŒ‡å—** â€” *é€šè¿‡ä½¿ç”¨Pythonï¼Œæˆ‘ä»¬å°†é€æ­¥è®²è§£å¦‚ä½•è¯„ä¼°å’Œé€‰æ‹©æ—¶é—´åºåˆ—æ¨¡å‹çš„ç‰¹å¾ï¼Œæä¾›ä¼˜åŒ–ä½ æ–¹æ³•çš„å®ç”¨å·¥å…·ã€‚æˆ‘ä»¬è¿˜å°†è¯„ä¼°æ˜¯å¦éœ€è¦å‰Šå‡ç‰¹å¾ä»¥ä¼˜åŒ–æˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹ã€‚*'
- en: Once youâ€™re familiar with techniques like stationarity and feature reduction
    as explained in this article, and youâ€™re looking to elevate your models even further?
    Check out this article on [using a custom validation loss](https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80)
    in your deep learning model to get better stock forecasts â€” itâ€™s a great next
    step!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ ç†Ÿæ‚‰äº†åƒå¹³ç¨³æ€§å’Œç‰¹å¾å‡å°‘è¿™æ ·çš„æŠ€æœ¯ï¼Œå¹¶ä¸”æƒ³è¿›ä¸€æ­¥æå‡ä½ çš„æ¨¡å‹ï¼Ÿå¯ä»¥æŸ¥çœ‹è¿™ç¯‡å…³äº[åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨è‡ªå®šä¹‰éªŒè¯æŸå¤±](https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80)çš„æ–‡ç« ï¼Œä»¥è·å¾—æ›´å¥½çš„è‚¡ç¥¨é¢„æµ‹â€”â€”è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸‹ä¸€æ­¥ï¼
- en: 'Feature reduction in time series : a simple explanation'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—ä¸­çš„ç‰¹å¾å‡å°‘ï¼šä¸€ä¸ªç®€å•çš„è§£é‡Š
- en: Feature reduction is like cleaning up your workspace to make it easier to find
    what you need. In time series analysis, it means cutting down the number of input
    variables (features) that your model uses to make predictions. The goal is to
    simplify the model while retaining its predictive power. This is important because
    too many and correlated features can make your model complicated, slow, and less
    accurate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å‡å°‘å°±åƒæ˜¯æ•´ç†å·¥ä½œåŒºï¼Œä½¿å¾—ä½ æ›´å®¹æ˜“æ‰¾åˆ°æ‰€éœ€çš„ä¸œè¥¿ã€‚åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œè¿™æ„å‘³ç€å‡å°‘æ¨¡å‹ç”¨äºé¢„æµ‹çš„è¾“å…¥å˜é‡ï¼ˆç‰¹å¾ï¼‰çš„æ•°é‡ã€‚ç›®æ ‡æ˜¯ç®€åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶é¢„æµ‹èƒ½åŠ›ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºè¿‡å¤šçš„ç‰¹å¾å’Œç›¸å…³ç‰¹å¾ä¼šä½¿æ¨¡å‹å˜å¾—å¤æ‚ã€ç¼“æ…¢å¹¶é™ä½å‡†ç¡®æ€§ã€‚
- en: 'Specifically, simplifying the feature set can :'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œç®€åŒ–ç‰¹å¾é›†å¯ä»¥ï¼š
- en: '**Reduced complexity:** Fewer features means a simpler model, which is often
    faster to train and use.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é™ä½å¤æ‚åº¦**ï¼šç‰¹å¾è¾ƒå°‘æ„å‘³ç€æ¨¡å‹æ›´ç®€å•ï¼Œè¿™é€šå¸¸ä½¿å¾—è®­ç»ƒå’Œä½¿ç”¨æ›´å¿«ã€‚'
- en: '**Improved generalization**: By removing noise, eliminating correlated features
    and focusing on key information, it helps the model to learn the true underlying
    patterns rather than memorizing redundant information. This enhances the modelâ€™s
    ability to generalize its predictions to different datasets.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡å»é™¤å™ªå£°ã€æ¶ˆé™¤ç›¸å…³ç‰¹å¾å¹¶èšç„¦äºå…³é”®ä¿¡æ¯ï¼Œå®ƒå¸®åŠ©æ¨¡å‹å­¦ä¹ çœŸå®çš„æ½œåœ¨æ¨¡å¼ï¼Œè€Œä¸æ˜¯è®°ä½å†—ä½™ä¿¡æ¯ã€‚è¿™å¢å¼ºäº†æ¨¡å‹å°†é¢„æµ‹æ¨å¹¿åˆ°ä¸åŒæ•°æ®é›†çš„èƒ½åŠ›ã€‚'
- en: '**Easier interpretation**: A model with fewer features is often easier for
    humans to understand and explain.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ›´æ˜“è§£é‡Š**ï¼šç‰¹å¾è¾ƒå°‘çš„æ¨¡å‹é€šå¸¸æ›´å®¹æ˜“è®©äººç±»ç†è§£å’Œè§£é‡Šã€‚'
- en: '**Computational efficiency**: Fewer features requires less memory and processing
    power, which can be crucial for large datasets or real-time applications.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®¡ç®—æ•ˆç‡**ï¼šè¾ƒå°‘çš„ç‰¹å¾éœ€è¦æ›´å°‘çš„å†…å­˜å’Œå¤„ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºå¤§æ•°æ®é›†æˆ–å®æ—¶åº”ç”¨è‡³å…³é‡è¦ã€‚'
- en: Itâ€™s also important to note that most time series packages in Python for forecasting
    donâ€™t perform feature reduction automatically. This is a step you typically need
    to handle on your own before using these packages.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„PythonåŒ…å¹¶ä¸ä¼šè‡ªåŠ¨è¿›è¡Œç‰¹å¾å‡å°‘ã€‚è¿™æ˜¯ä½ é€šå¸¸éœ€è¦åœ¨ä½¿ç”¨è¿™äº›åŒ…ä¹‹å‰è‡ªè¡Œå¤„ç†çš„ä¸€ä¸ªæ­¥éª¤ã€‚
- en: 'To better understand these concepts, letâ€™s walk through a practical example
    using real-world daily data from the Federal Reserve Economic Data (FRED) database.
    Weâ€™ll skip the data retrieval process here, as weâ€™ve already covered how to get
    free and reliable data from the FRED API in a [previous article](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea).
    You can get the data weâ€™ll be using [this script](https://github.com/philippe-ostiguy/free-fin-data).
    Once youâ€™ve fetched the data :'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ¦‚å¿µï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä¾‹å­æ¥ä½¿ç”¨æ¥è‡ªç¾å›½è”é‚¦å‚¨å¤‡ç»æµæ•°æ®ï¼ˆFREDï¼‰æ•°æ®åº“çš„çœŸå®ä¸–ç•Œæ—¥æ•°æ®ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œè·³è¿‡æ•°æ®è·å–çš„è¿‡ç¨‹ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)ä¸­è¯¦ç»†ä»‹ç»äº†å¦‚ä½•é€šè¿‡FRED
    APIè·å–å…è´¹çš„å¯é æ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡[è¿™ä¸ªè„šæœ¬](https://github.com/philippe-ostiguy/free-fin-data)è·å–æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®ã€‚ä¸€æ—¦ä½ è·å–äº†æ•°æ®ï¼š
- en: Create a`data` directory in your current directory
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª`data`ç›®å½•
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Copy the data in your directory
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ•°æ®å¤åˆ¶åˆ°ä½ çš„ç›®å½•ä¸­
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have our data, letâ€™s dive into our feature reduction example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ç‰¹å¾å‡å°‘çš„ç¤ºä¾‹ã€‚
- en: Weâ€™ve previously demonstrated how to clean the daily data fetched from the FRED
    API in [another article](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea),
    so weâ€™ll skip that process here and use the `processed_dataframes`(list of dataframes)
    that resulted from the steps outlined in that article.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰åœ¨[å¦ä¸€ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)ä¸­å±•ç¤ºäº†å¦‚ä½•æ¸…ç†ä»FRED
    APIè·å–çš„æ¯æ—¥æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œè·³è¿‡è¯¥è¿‡ç¨‹ï¼Œç›´æ¥ä½¿ç”¨æ–‡ç« ä¸­æ­¥éª¤å¾—åˆ°çš„`processed_dataframes`ï¼ˆæ•°æ®æ¡†åˆ—è¡¨ï¼‰ã€‚
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You might be wondering why weâ€™ve divided our data into training and testing
    sets? The reason is to ensure that there is no data leakage before applying any
    transformation or reduction technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šé—®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ŸåŸå› æ˜¯ä¸ºäº†ç¡®ä¿åœ¨åº”ç”¨ä»»ä½•è½¬æ¢æˆ–é™ç»´æŠ€æœ¯ä¹‹å‰ä¸ä¼šå‘ç”Ÿæ•°æ®æ³„æ¼ã€‚
- en: '`initial_model_data` contains the S&P 500 daily prices (stored originally `processed_dataframes`),
    which will be the data weâ€™ll be trying to forecast.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`initial_model_data`åŒ…å«æ ‡å‡†æ™®å°”500çš„æ¯æ—¥ä»·æ ¼ï¼ˆæœ€åˆå­˜å‚¨åœ¨`processed_dataframes`ä¸­ï¼‰ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬å°è¯•é¢„æµ‹çš„æ•°æ®ã€‚'
- en: Then, we need to ensure our data is stationary. For a detailed explanation on
    how to automatically make your data stationary and improve your model by 20%,
    refer to [this article](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„æ•°æ®æ˜¯å¹³ç¨³çš„ã€‚å…³äºå¦‚ä½•è‡ªåŠ¨ä½¿æ•°æ®å¹³ç¨³å¹¶æé«˜20%æ¨¡å‹è¡¨ç°çš„è¯¦ç»†è§£é‡Šï¼Œè¯·å‚é˜…[è¿™ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)ã€‚
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we will count the number of variables that have at least a 95% correlation
    coefficient with another variable.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†ç»Ÿè®¡è‡³å°‘ä¸å¦ä¸€ä¸ªå˜é‡æœ‰95%ç›¸å…³ç³»æ•°çš„å˜é‡æ•°é‡ã€‚
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/3e88d562d5f155546e490822fecc2ca8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e88d562d5f155546e490822fecc2ca8.png)'
- en: Number of variables highly correlated. Image by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜åº¦ç›¸å…³çš„å˜é‡æ•°é‡ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'Having 260 out of 438 variables that have a correlation of 95% or more with
    at least another variable can be an issue. It indicates significant multicollinearity
    in the dataset. This redundancy can lead to several issues:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ438ä¸ªå˜é‡ä¸­æœ‰260ä¸ªå˜é‡ä¸è‡³å°‘å¦ä¸€ä¸ªå˜é‡çš„ç›¸å…³æ€§è¾¾åˆ°95%æˆ–æ›´é«˜ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è¿™è¡¨æ˜æ•°æ®é›†ä¸­å­˜åœ¨æ˜¾è‘—çš„å¤šé‡å…±çº¿æ€§ã€‚è¿™ç§å†—ä½™å¯èƒ½ä¼šå¯¼è‡´ä»¥ä¸‹å‡ ä¸ªé—®é¢˜ï¼š
- en: It complicates the model without adding substantial new information
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿æ¨¡å‹å˜å¾—å¤æ‚ï¼Œä½†å¹¶æœªå¢åŠ å®è´¨æ€§çš„æ–°ä¿¡æ¯
- en: Potentially causes instability in coefficient estimates
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯èƒ½å¯¼è‡´å›å½’ç³»æ•°ä¼°è®¡çš„ä¸ç¨³å®š
- en: Increases the risk of overfitting
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¢åŠ è¿‡æ‹Ÿåˆçš„é£é™©
- en: Makes interpretation of individual variable impacts challenging
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿å¾—è§£é‡Šå•ä¸ªå˜é‡çš„å½±å“å˜å¾—å›°éš¾
- en: Feature evaluation and selection
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰¹å¾è¯„ä¼°ä¸é€‰æ‹©
- en: We understand that feature reduction can be important, but how do we perform
    it? Which techniques should we use? These are the questions weâ€™ll explore now.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç†è§£ç‰¹å¾é™ç»´å¯èƒ½å¾ˆé‡è¦ï¼Œä½†æˆ‘ä»¬è¯¥å¦‚ä½•è¿›è¡Œå‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªäº›æŠ€æœ¯ï¼Ÿè¿™äº›æ˜¯æˆ‘ä»¬ç°åœ¨è¦æ¢è®¨çš„é—®é¢˜ã€‚
- en: The first technique weâ€™ll examine is Principal Component Analysis (PCA). Itâ€™s
    a common and an effective dimensionality reduction technique. PCA identifies linear
    relationships between features and retains the principal components that explain
    a predetermined percentage of the variance in the original dataset. In our use
    case, weâ€™ve set the `EXPLAINED_VARIANCE` threshold to 90%.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦æ¢è®¨çš„ç¬¬ä¸€ç§æŠ€æœ¯æ˜¯ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€‚è¿™æ˜¯ä¸€ç§å¸¸è§ä¸”æœ‰æ•ˆçš„é™ç»´æŠ€æœ¯ã€‚PCAè¯†åˆ«ç‰¹å¾ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼Œå¹¶ä¿ç•™è§£é‡ŠåŸå§‹æ•°æ®é›†ä¸­é¢„å®šç™¾åˆ†æ¯”æ–¹å·®çš„ä¸»æˆåˆ†ã€‚åœ¨æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†`EXPLAINED_VARIANCE`é˜ˆå€¼è®¾ç½®ä¸º90%ã€‚
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/5544416f6efe78886454deea283a782d.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5544416f6efe78886454deea283a782d.png)'
- en: Feature reduction using Principal Component Analysis. Image by the author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸»æˆåˆ†åˆ†æè¿›è¡Œç‰¹å¾é™ç»´ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'Itâ€™s an impressive: only 76 components out of 438 features remaining after
    the reduction while keeping 90% of the variance explained! Now letâ€™s move to a
    non-linear reduction technique.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼šåœ¨é™ç»´åï¼Œ438ä¸ªç‰¹å¾ä¸­åªå‰©ä¸‹76ä¸ªæˆåˆ†ï¼ŒåŒæ—¶ä¿æŒäº†90%çš„æ–¹å·®è§£é‡Šï¼æ¥ä¸‹æ¥æˆ‘ä»¬å°†è½¬å‘ä¸€ç§éçº¿æ€§é™ç»´æŠ€æœ¯ã€‚
- en: The [Temporal Fusion Transformers (TFT)](https://arxiv.org/pdf/1912.09363.pdf)
    is an advanced model for time series forecasting. It includes the Variable Selection
    Network (VSN), which is a key component of the model. Itâ€™s specifically designed
    to automatically identify and focus on the most relevant features within a dataset.
    It achieves this by assigning learned weights to each input variable, effectively
    highlighting which features contribute most to the predictive task.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ—¶é—´åºåˆ—èåˆå˜æ¢å™¨ï¼ˆTFTï¼‰](https://arxiv.org/pdf/1912.09363.pdf)æ˜¯ä¸€ç§ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„å…ˆè¿›æ¨¡å‹ã€‚å®ƒåŒ…å«å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ï¼Œè¿™æ˜¯æ¨¡å‹çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ã€‚å®ƒä¸“é—¨è®¾è®¡ç”¨äºè‡ªåŠ¨è¯†åˆ«å’Œå…³æ³¨æ•°æ®é›†ä¸­çš„æœ€ç›¸å…³ç‰¹å¾ã€‚é€šè¿‡ä¸ºæ¯ä¸ªè¾“å…¥å˜é‡åˆ†é…å­¦ä¹ åˆ°çš„æƒé‡ï¼Œå®ƒæœ‰æ•ˆåœ°çªå‡ºå“ªäº›ç‰¹å¾å¯¹é¢„æµ‹ä»»åŠ¡çš„è´¡çŒ®æœ€å¤§ã€‚'
- en: This VSN-based approach will be our second reduction technique. Weâ€™ll implement
    it using [PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/),
    which allows us to leverage the Variable Selection Network from the TFT model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºVSNçš„æ–¹æ³•å°†æ˜¯æˆ‘ä»¬ç¬¬äºŒç§é™ç»´æŠ€æœ¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/)æ¥å®ç°ï¼Œå®ƒå…è®¸æˆ‘ä»¬åˆ©ç”¨TFTæ¨¡å‹ä¸­çš„å˜é‡é€‰æ‹©ç½‘ç»œã€‚
- en: Weâ€™ll use a basic configuration. Our goal isnâ€™t to create the highest-performing
    model possible, but rather to identify the most relevant features while using
    minimal resources.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåŸºç¡€é…ç½®ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯åˆ›å»ºæ€§èƒ½æœ€å¼ºçš„æ¨¡å‹ï¼Œè€Œæ˜¯è¯†åˆ«æœ€ç›¸å…³çš„ç‰¹å¾ï¼ŒåŒæ—¶å°½é‡å‡å°‘èµ„æºä½¿ç”¨ã€‚
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `VARIABLES_IMPORTANCE` threshold is set to 0.8, which means we'll retain
    features in the top 80th percentile of importance as determined by the Variable
    Selection Network (VSN). For more information about the Temporal Fusion Transformers
    (TFT) and its parameters, please refer to the [documentation](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`VARIABLES_IMPORTANCE`é˜ˆå€¼è®¾ç½®ä¸º0.8ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å°†ä¿ç•™ç”±å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ç¡®å®šçš„æ’åå‰80%çš„é‡è¦ç‰¹å¾ã€‚æœ‰å…³æ—¶é—´åºåˆ—èåˆå˜æ¢å™¨ï¼ˆTFTï¼‰åŠå…¶å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer)ã€‚'
- en: Next, weâ€™ll train the TFT model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®­ç»ƒTFTæ¨¡å‹ã€‚
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We intentionally set `max_epochs=20` so the model doesnâ€™t train too long. Additionally,
    we implemented an `early_stop_callback` that halts training if the model shows
    no improvement for 5 consecutive epochs (`patience=5`).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ•…æ„è®¾ç½®äº†`max_epochs=20`ï¼Œä»¥é¿å…æ¨¡å‹è®­ç»ƒæ—¶é—´è¿‡é•¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†`early_stop_callback`ï¼Œå¦‚æœæ¨¡å‹åœ¨è¿ç»­5ä¸ªepochä¸­æ²¡æœ‰æ”¹å–„ï¼Œå°†åœæ­¢è®­ç»ƒï¼ˆ`patience=5`ï¼‰ã€‚
- en: Finally, using the best model obtained, we select the 80th percentile of the
    most important features as determined by the VSN.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä½¿ç”¨è·å¾—çš„æœ€ä½³æ¨¡å‹ï¼Œæˆ‘ä»¬é€‰æ‹©ç”±VSNç¡®å®šçš„æœ€é‡è¦ç‰¹å¾çš„80ç™¾åˆ†ä½ã€‚
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/752a7a4f43f1726c1de6b2236a68e5c7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/752a7a4f43f1726c1de6b2236a68e5c7.png)'
- en: Feature reduction using Variable Selection Network. Image by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å˜é‡é€‰æ‹©ç½‘ç»œè¿›è¡Œç‰¹å¾é™ç»´ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'The original dataset contained 438 features, which were then reduced to 1 feature
    only after applying the VSN method! This drastic reduction suggests several possibilities:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹æ•°æ®é›†åŒ…å«438ä¸ªç‰¹å¾ï¼Œåœ¨åº”ç”¨VSNæ–¹æ³•åï¼Œä»…å‰©ä¸‹1ä¸ªç‰¹å¾ï¼è¿™ç§å‰§çƒˆçš„é™ç»´æš—ç¤ºäº†å‡ ç§å¯èƒ½æ€§ï¼š
- en: Many of the original features may have been redundant.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¸å¤šåŸå§‹ç‰¹å¾å¯èƒ½æ˜¯å†—ä½™çš„ã€‚
- en: The feature selection process may have oversimplified the data.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¯èƒ½å·²è¿‡åº¦ç®€åŒ–æ•°æ®ã€‚
- en: Using only the target variableâ€™s historical values (autoregressive approach)
    might perform as well as, or possibly better than, models incorporating exogenous
    variables.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»…ä½¿ç”¨ç›®æ ‡å˜é‡çš„å†å²å€¼ï¼ˆè‡ªå›å½’æ–¹æ³•ï¼‰å¯èƒ½ä¸åŒ…å«å¤–ç”Ÿå˜é‡çš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œç”šè‡³å¯èƒ½æ›´å¥½ã€‚
- en: Evaluating feature reduction techniques
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°ç‰¹å¾é™ç»´æŠ€æœ¯
- en: In this final section, we compare out reduction techniques applied to our model.
    Each method is tested while maintaining identical model configurations, varying
    only the features subjected to reduction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒåº”ç”¨äºæ¨¡å‹çš„é™ç»´æŠ€æœ¯ã€‚æ¯ç§æ–¹æ³•åœ¨ä¿æŒç›¸åŒæ¨¡å‹é…ç½®çš„åŒæ—¶è¿›è¡Œæµ‹è¯•ï¼Œå”¯ä¸€åŒºåˆ«æ˜¯æ‰€é€‰ç‰¹å¾çš„é™ç»´å¤„ç†ã€‚
- en: Weâ€™ll use [TiDE](https://arxiv.org/pdf/2304.08424.pdf), a small state-of-the-art
    Transformer-based model. Weâ€™ll use the implementation provided by [NeuralForecast](https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html).
    Any model from NeuralForecast [here](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)
    would work as long as it allows exogenous historical variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[TiDE](https://arxiv.org/pdf/2304.08424.pdf)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºTransformerçš„å°å‹æœ€å…ˆè¿›æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[NeuralForecast](https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html)æä¾›çš„å®ç°ã€‚åªè¦å®ƒå…è®¸å¤–ç”Ÿå†å²å˜é‡ï¼ŒNeuralForecastçš„ä»»ä½•æ¨¡å‹[è¿™é‡Œ](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)éƒ½å¯ä»¥ä½¿ç”¨ã€‚
- en: 'Weâ€™ll train and test two models using daily SPY (S&P 500 ETF) data. Both models
    will have the same:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æ¯æ—¥SPYï¼ˆæ ‡æ™®500ETFï¼‰æ•°æ®è®­ç»ƒå’Œæµ‹è¯•ä¸¤ä¸ªæ¨¡å‹ã€‚ä¸¤ä¸ªæ¨¡å‹å°†å…·æœ‰ç›¸åŒçš„ï¼š
- en: Train-test split ratio
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®­ç»ƒ-æµ‹è¯•åˆ†å‰²æ¯”ä¾‹
- en: Hyperparameters
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°
- en: Single time series (SPY)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å•ä¸€æ—¶é—´åºåˆ—ï¼ˆSPYï¼‰
- en: Forecasting horizon of 1 step ahead
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢„æµ‹èŒƒå›´ä¸ºä¸€æ­¥ ahead
- en: The only difference between the models will be the feature reduction technique.
    Thatâ€™s it!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´å”¯ä¸€çš„åŒºåˆ«æ˜¯ç‰¹å¾å‡å°‘æŠ€æœ¯ã€‚å°±è¿™æ ·ï¼
- en: 'First model: Original features (no feature reduction)'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼šåŸå§‹ç‰¹å¾ï¼ˆæ²¡æœ‰ç‰¹å¾å‡å°‘ï¼‰
- en: 'Second model: Feature reduction using PCA'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ¨¡å‹ï¼šä½¿ç”¨PCAè¿›è¡Œç‰¹å¾å‡å°‘
- en: 'Third model: Feature reduction using VSN'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªæ¨¡å‹ï¼šä½¿ç”¨VSNè¿›è¡Œç‰¹å¾å‡å°‘
- en: This setup allows us to isolate the impact of each feature reduction technique
    on model performance.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è®¾ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿéš”ç¦»æ¯ç§ç‰¹å¾å‡å°‘æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
- en: First we train the 3 models with the same configuration except for the features.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é…ç½®è®­ç»ƒä¸‰ä¸ªæ¨¡å‹ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ç‰¹å¾ã€‚
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, we make the predictions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œé¢„æµ‹ã€‚
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We forecast the daily returns using the model, then convert these back to prices.
    This approach allows us to calculate prediction errors using prices and compare
    the actual prices to the forecasted prices in a plot.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ¯æ—¥å›æŠ¥ï¼Œç„¶åå°†è¿™äº›å›æŠ¥è½¬æ¢ä¸ºä»·æ ¼ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ä»·æ ¼è®¡ç®—é¢„æµ‹è¯¯å·®ï¼Œå¹¶å°†å®é™…ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼è¿›è¡Œæ¯”è¾ƒï¼Œå‘ˆç°åœ¨å›¾è¡¨ä¸­ã€‚
- en: '![](../Images/269abd534c5abdfe8194df39136fd090.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269abd534c5abdfe8194df39136fd090.png)'
- en: Comparison of prediction errors with different feature reduction techniques.
    Image by the author.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸åŒç‰¹å¾å‡å°‘æŠ€æœ¯çš„é¢„æµ‹è¯¯å·®æ¯”è¾ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'The similar performance of the TiDE model across both original and reduced
    feature sets reveals a crucial insight: feature reduction did not lead to improved
    predictions as one might expect. This suggests potential key issues:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TiDEæ¨¡å‹åœ¨åŸå§‹å’Œå‡å°‘ç‰¹å¾é›†ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œæ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„è§è§£ï¼šç‰¹å¾å‡å°‘å¹¶æ²¡æœ‰åƒé¢„æœŸé‚£æ ·æé«˜é¢„æµ‹ç²¾åº¦ã€‚è¿™è¡¨æ˜å¯èƒ½å­˜åœ¨ä¸€äº›å…³é”®é—®é¢˜ï¼š
- en: 'Information loss: despite aiming to preserve essential data, dimensionality
    reduction techniques discarded information relevant to the prediction task, explaining
    the lack of improvement with fewer features.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¿¡æ¯æŸå¤±ï¼šå°½ç®¡ç›®çš„æ˜¯ä¿ç•™é‡è¦æ•°æ®ï¼Œä½†é™ç»´æŠ€æœ¯ä¸¢å¼ƒäº†ä¸é¢„æµ‹ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ï¼Œè¿™è§£é‡Šäº†åœ¨å‡å°‘ç‰¹å¾æ—¶é¢„æµ‹æ²¡æœ‰æ”¹è¿›ã€‚
- en: 'Generalization struggles: consistent performance across feature sets indicates
    the modelâ€™s difficulty in capturing underlying patterns, regardless of feature
    count.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³›åŒ–å›°éš¾ï¼šä¸åŒç‰¹å¾é›†ä¹‹é—´çš„ä¸€è‡´è¡¨ç°è¡¨æ˜æ¨¡å‹åœ¨æ•æ‰æ½œåœ¨æ¨¡å¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— è®ºç‰¹å¾æ•°é‡å¤šå°‘ã€‚
- en: 'Complexity overkill: similar results with fewer features suggest TiDEâ€™s sophisticated
    architecture may be unnecessarily complex. A simpler model, like ARIMA, could
    potentially perform just as well.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤æ‚æ€§è¿‡åº¦ï¼šä½¿ç”¨è¾ƒå°‘ç‰¹å¾è·å¾—ç±»ä¼¼ç»“æœè¡¨æ˜ï¼ŒTiDEçš„å¤æ‚æ¶æ„å¯èƒ½è¿‡äºå¤æ‚ã€‚ä¸€ç§æ›´ç®€å•çš„æ¨¡å‹ï¼Œä¾‹å¦‚ARIMAï¼Œå¯èƒ½è¡¨ç°å¾—åŒæ ·å¥½ã€‚
- en: Then, letâ€™s examine the chart to see if we can observe any significant differences
    among the three forecasting methods and the actual prices.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹å›¾è¡¨ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½è§‚å¯Ÿåˆ°ä¸‰ç§é¢„æµ‹æ–¹æ³•ä¸å®é™…ä»·æ ¼ä¹‹é—´æœ‰ä»»ä½•æ˜¾è‘—å·®å¼‚ã€‚
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/198959fd7771ea78751d923086b0820d.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/198959fd7771ea78751d923086b0820d.png)'
- en: SPY price forecast using all original features. Image created by the author.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ‰€æœ‰åŸå§‹ç‰¹å¾çš„SPYä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/12c100de5f3352051765fbd2db5be6e0.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12c100de5f3352051765fbd2db5be6e0.png)'
- en: SPY price forecast using PCA. Image created by the author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PCAçš„SPYä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/ba26bb6bbb1727805ee38cd8788db4cd.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba26bb6bbb1727805ee38cd8788db4cd.png)'
- en: SPY price forecast using VSN. Image created by the author.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨VSNçš„SPYä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The difference between true and predicted prices appears consistent across all
    three models, with no noticeable variation in performance between them.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸå®ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼ä¹‹é—´çš„å·®å¼‚åœ¨ä¸‰ä¸ªæ¨¡å‹ä¸­çœ‹èµ·æ¥ä¸€è‡´ï¼Œæ€§èƒ½ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„å·®å¼‚ã€‚
- en: Conclusion
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'We did it! We explored the importance of feature reduction in time series analysis
    and provided a practical implementation guide:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åšåˆ°äº†ï¼æˆ‘ä»¬æ¢è®¨äº†ç‰¹å¾å‡å°‘åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å®ç°æŒ‡å—ï¼š
- en: Feature reduction aims to simplify models while maintaining predictive power.
    Benefits include reduced complexity, improved generalization, easier interpretation,
    and computational efficiency.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾é™ç»´çš„ç›®æ ‡æ˜¯ç®€åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé¢„æµ‹èƒ½åŠ›ã€‚å…¶å¥½å¤„åŒ…æ‹¬å‡å°‘å¤æ‚æ€§ã€æé«˜æ³›åŒ–èƒ½åŠ›ã€ç®€åŒ–è§£é‡Šå’Œè®¡ç®—æ•ˆç‡ã€‚
- en: 'We demonstrated two reduction techniques using FRED data:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨FREDæ•°æ®æ¼”ç¤ºäº†ä¸¤ç§é™ç»´æŠ€æœ¯ï¼š
- en: Principal Component Analysis (PCA), a linear dimensionality reduction method,
    reduced features from 438 to 76 while retaining 90% of explained variance.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œä¸€ç§çº¿æ€§é™ç»´æ–¹æ³•ï¼Œå°†ç‰¹å¾ä»438ä¸ªå‡å°‘åˆ°76ä¸ªï¼ŒåŒæ—¶ä¿ç•™äº†90%çš„è§£é‡Šæ–¹å·®ã€‚
- en: Variable Selection Network (VSN) from the Temporal Fusion Transformers, a non-linear
    approach, drastically reduced features to just 1 using an 80th percentile importance
    threshold.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¥è‡ªæ—¶åºèåˆå˜æ¢å™¨ï¼ˆTemporal Fusion Transformersï¼‰çš„å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ï¼Œä¸€ç§éçº¿æ€§æ–¹æ³•ï¼Œé€šè¿‡è®¾ç½®80ç™¾åˆ†ä½é‡è¦æ€§é˜ˆå€¼ï¼Œå°†ç‰¹å¾å¤§å¹…å‡å°‘åˆ°ä»…å‰©1ä¸ªã€‚
- en: Evaluation using TiDE models showed similar performance across original and
    reduced feature sets, suggesting feature reduction may not always improve forecasting
    performance. This could be due to information loss during reduction, the modelâ€™s
    difficulty in capturing underlying patterns, or the possibility that a simpler
    model might be equally effective for this particular forecasting task.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TiDEæ¨¡å‹è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŸå§‹ç‰¹å¾é›†å’Œé™ç»´åçš„ç‰¹å¾é›†åœ¨æ€§èƒ½ä¸Šç›¸ä¼¼ï¼Œè¿™è¡¨æ˜ç‰¹å¾é™ç»´å¹¶ä¸æ€»æ˜¯èƒ½æå‡é¢„æµ‹æ€§èƒ½ã€‚è¿™å¯èƒ½æ˜¯ç”±äºé™ç»´è¿‡ç¨‹ä¸­ä¿¡æ¯ä¸¢å¤±ã€æ¨¡å‹éš¾ä»¥æ•æ‰æ½œåœ¨æ¨¡å¼ï¼Œæˆ–å¯èƒ½æ˜¯å¯¹äºè¿™ä¸ªç‰¹å®šçš„é¢„æµ‹ä»»åŠ¡ï¼Œæ›´ç®€å•çš„æ¨¡å‹åŒæ ·æœ‰æ•ˆã€‚
- en: On a final note, we didnâ€™t explore all feature reduction techniques, such as
    SHAP (SHapley Additive exPlanations), which provides a unified measure of feature
    importance across various model types. Even if we didnâ€™t improve our model, itâ€™s
    still better to perform feature curation and compare performance across different
    reduction methods. This approach helps ensure youâ€™re not discarding valuable information
    while optimizing your modelâ€™s efficiency and interpretability.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ç‚¹ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰æ¢ç´¢æ‰€æœ‰çš„ç‰¹å¾é™ç»´æŠ€æœ¯ï¼Œä¾‹å¦‚SHAPï¼ˆSHapley Additive exPlanationsï¼‰ï¼Œå®ƒæä¾›äº†è·¨å¤šç§æ¨¡å‹ç±»å‹çš„ç»Ÿä¸€ç‰¹å¾é‡è¦æ€§åº¦é‡ã€‚å³ä½¿æˆ‘ä»¬æ²¡æœ‰æ”¹è¿›æ¨¡å‹ï¼Œè¿›è¡Œç‰¹å¾ç­›é€‰å¹¶æ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•çš„æ€§èƒ½ä»ç„¶æ˜¯å€¼å¾—çš„ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºç¡®ä¿åœ¨ä¼˜åŒ–æ¨¡å‹æ•ˆç‡å’Œå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œä¸ä¸¢å¤±æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚
- en: In future articles, weâ€™ll apply these feature reduction techniques to more complex
    models, comparing their impact on performance and interpretability. Stay tuned!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›ç‰¹å¾é™ç»´æŠ€æœ¯åº”ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹ï¼Œæ¯”è¾ƒå®ƒä»¬å¯¹æ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„å½±å“ã€‚æ•¬è¯·å…³æ³¨ï¼
- en: Ready to put these concepts into action? You can find the complete code implementation
    [here](https://github.com/philippe-ostiguy/feature-reduction-ts).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡å°†è¿™äº›æ¦‚å¿µä»˜è¯¸å®è·µäº†å—ï¼Ÿä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/philippe-ostiguy/feature-reduction-ts)æ‰¾åˆ°å®Œæ•´çš„ä»£ç å®ç°ã€‚
- en: Liked this article? Show your support!
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿè¡¨ç¤ºä½ çš„æ”¯æŒï¼
- en: ğŸ‘ Clap it up to 50 times
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘ é¼“æŒ50æ¬¡
- en: ğŸ¤ Send me a [LinkedIn](https://www.linkedin.com/in/philippe-ostiguy/) connection
    request to stay in touch
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¤ é€šè¿‡[LinkedIn](https://www.linkedin.com/in/philippe-ostiguy/)å‘æˆ‘å‘é€è¿æ¥è¯·æ±‚ï¼Œä¿æŒè”ç³»
- en: '*Your support means everything!* ğŸ™'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä½ çš„æ”¯æŒæ„ä¹‰é‡å¤§ï¼* ğŸ™'
