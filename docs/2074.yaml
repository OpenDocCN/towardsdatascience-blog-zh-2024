- en: Introducing Markov Decision Processes, Setting up Gymnasium Environments and
    Solving them via Dynamic Programming Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍马尔可夫决策过程，设置Gymnasium环境并通过动态规划方法求解
- en: 原文：[https://towardsdatascience.com/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f?source=collection_archive---------4-----------------------#2024-08-26](https://towardsdatascience.com/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f?source=collection_archive---------4-----------------------#2024-08-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f?source=collection_archive---------4-----------------------#2024-08-26](https://towardsdatascience.com/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f?source=collection_archive---------4-----------------------#2024-08-26)
- en: Dissecting “Reinforcement Learning” by Richard S. Sutton with custom Python
    implementations, Episode II
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过理查德·S·萨顿的《强化学习》一书和自定义的Python实现，解析“强化学习”，第二集
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--e806c36dc04f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)
    ·12 min read·Aug 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e806c36dc04f--------------------------------)
    ·阅读时间：12分钟·2024年8月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In a [previous post](https://medium.com/towards-data-science/introduction-to-reinforcement-learning-and-solving-the-multi-armed-bandit-problem-e4ae74904e77)
    we started our series about Reinforcement Learning (RL) following Sutton’s great
    book [1]. In that post we introduced RL in general, and discussed Multi-armed
    Bandits as a *nonassociative* toy problem.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [上一篇文章](https://medium.com/towards-data-science/introduction-to-reinforcement-learning-and-solving-the-multi-armed-bandit-problem-e4ae74904e77)
    中，我们开始了关于强化学习（RL）系列的第一篇文章，依据萨顿的经典著作[1]。在那篇文章中，我们介绍了强化学习的一般概念，并讨论了多臂赌博机问题，这是一个*非关联*的玩具问题。
- en: Here, we will build on this — but go significantly beyond. In particular, we
    will introduce our first *associative* problem, which might feel much more like
    “real” RL to many readers — and introduce a simple but general solution technique.
    Furthermore, we will introduce Gymnasium [2], a powerful library providing a multitude
    of environments (e.g. Atari or MuJoCo games) and allowing us to quickly experiment
    with solving them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将在此基础上进一步扩展——但会有显著的提升。特别是，我们将引入我们的第一个*关联*问题，这对许多读者来说可能更像是“真实”的强化学习（RL）——并介绍一种简单但通用的解决技巧。此外，我们还将介绍Gymnasium
    [2]，这是一个强大的库，提供了多种环境（例如，Atari或MuJoCo游戏），并允许我们快速实验解决这些问题。
- en: '![](../Images/7ea806371c177ee506c0b0a5ecfb843b.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ea806371c177ee506c0b0a5ecfb843b.png)'
- en: Photo by [Adarsh Kummur](https://unsplash.com/@akummur?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/wilted-tree-during-daytime-zThTy8rPPsY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Adarsh Kummur](https://unsplash.com/@akummur?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/photos/wilted-tree-during-daytime-zThTy8rPPsY?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: 'The previously mentioned associative setting is the “standard” in RL: as opposed
    to the previously introduced nonassociative setting where there is only a single
    state, and we only have to decide on what action to take, here we have multiple
    states — and for every state we might decide for a different best action.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的关联设置是强化学习（RL）中的“标准”：与之前介绍的非关联设置不同，后者只有一个状态，我们只需决定采取什么行动，而在这里，我们有多个状态——对于每个状态，我们可能会选择不同的最佳行动。
