- en: Understanding Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Transformer
- en: 原文：[https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27](https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27](https://towardsdatascience.com/understanding-transformers-3344d16c8c36?source=collection_archive---------0-----------------------#2024-06-27)
- en: A straightforward breakdown of “Attention is All You Need”¹
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 《Attention is All You Need》¹的简明分解
- en: '[](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)[![Aveek
    Goswami](../Images/605b68f373d08d4d82223f9478417177.png)](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)
    [Aveek Goswami](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)[![Aveek
    Goswami](../Images/605b68f373d08d4d82223f9478417177.png)](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)
    [Aveek Goswami](https://medium.com/@aveekgoswami?source=post_page---byline--3344d16c8c36--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)
    ·10 min read·Jun 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3344d16c8c36--------------------------------)
    ·阅读时间：10 分钟·2024 年 6 月 27 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The transformer came out in 2017\. There have been many, many articles explaining
    how it works, but I often find them either going too deep into the math or too
    shallow on the details. I end up spending as much time googling (or chatGPT-ing)
    as I do reading, which isn’t the best approach to understanding a topic. That
    brought me to writing this article, where I attempt to explain the most revolutionary
    aspects of the transformer while keeping it succinct and simple for anyone to
    read.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 于 2017 年发布。尽管已有许多文章解释其工作原理，但我经常发现它们要么深入探讨数学内容，要么对细节的讲解过于浅显。我往往需要花费与阅读同样多的时间去谷歌搜索（或使用
    ChatGPT），这显然不是理解一个主题的最佳方法。因此，我写了这篇文章，试图简洁明了地解释 Transformer 最具革命性的方面，让任何人都能轻松阅读。
- en: This article assumes a general understanding of machine learning principles.
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文假设读者对机器学习原理有一般性的理解。
- en: '![](../Images/7e1f9078391c08b14a89cb789a4dcdd8.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e1f9078391c08b14a89cb789a4dcdd8.png)'
- en: 'Tranformers, transforming. Image source: DALL-E (we’re learning about gen ai
    anyway)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer，正在转变。图片来源：DALL-E（反正我们也在学习生成式人工智能）
- en: The ideas behind the Transformer led us to the era of Generative AI
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 背后的理念将我们带入了生成式人工智能的时代。
- en: Transformers represented a new architecture of **sequence transduction models.**
    A sequence model is a type of model that transforms an input sequence to an output
    sequence. This input sequence can be of various data types, such as characters,
    words, tokens, bytes, numbers, phonemes (speech recognition), and may also be
    multimodal¹.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 代表了一种新的**序列转导模型架构**。序列模型是一种将输入序列转化为输出序列的模型。这些输入序列可以是各种数据类型，例如字符、单词、符号、字节、数字、音素（语音识别），也可以是多模态的¹。
- en: Before transformers, sequence models were largely based on recurrent neural
    networks (RNNs), long short-term memory (LSTM), gated recurrent units (GRUs) and
    convolutional neural networks (CNNs). They often contained some form of an attention
    mechanism to account for the context provided by items in various positions of
    a sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 出现之前，序列模型主要基于递归神经网络（RNN）、长短期记忆网络（LSTM）、门控递归单元（GRU）和卷积神经网络（CNN）。这些模型通常包含某种形式的注意力机制，用于考虑序列中各个位置的元素所提供的上下文。
- en: The downsides of previous models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以前模型的缺点
- en: '![](../Images/89222b340d0651d0fe49a577cfff4f47.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89222b340d0651d0fe49a577cfff4f47.png)'
- en: 'RNN Illustration. Image source: [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 插图。图片来源：[Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: '**RNNs**: The model tackles the data **sequentially**, so anything learned
    from the previous computation is accounted for in the next computation². However,
    its sequential nature causes a few problems: the model struggles to account for
    long-term dependencies for longer sequences (known as **vanishing or exploding
    gradients**), and **prevents parallel processing** of the input sequence as you
    cannot train on different chunks of the input at the same time (batching) because
    you will lose context of the previous chunks. This makes it more computationally
    expensive to train.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNNs**：该模型按顺序处理数据，因此从前一次计算中学到的任何内容都会在下一次计算中得到考虑²。然而，它的顺序性质带来了一些问题：模型在处理较长序列时难以考虑长期依赖关系（即**梯度消失或爆炸**问题），并且**阻止了输入序列的并行处理**，因为你无法在同一时间对输入的不同部分进行训练（批处理），否则会失去前部分的上下文。这使得训练变得更加计算密集。'
- en: '![](../Images/b831a80d471f6266880cfbe8635facde.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b831a80d471f6266880cfbe8635facde.png)'
- en: 'LSTM and GRU overview. Image source: [Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM和GRU概述。图片来源：[Christopher Olah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: '**LSTM and GRUs**: Made use of **gating mechanisms** to preserve long-term
    dependencies³. The model has a *cell state* which contains the relevant information
    from the whole sequence. The cell state changes through gates such as the **forget,
    input, output gates (LSTM)***,* and**update, reset gates****(GRU)***.* These gates
    decide, at each sequential iteration, how much information from the previous state
    should be kept, how much information from the new update should be added, and
    then which part of the new cell state should be kept overall. While this improves
    the vanishing gradient issue, the models still **work sequentially** and hence
    **train slowly** due to limited parallelisation, especially when sequences get
    longer.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTM和GRUs**：利用**门控机制**来保持长期依赖关系³。该模型具有一个*单元状态*，包含整个序列的相关信息。单元状态通过门控（如**遗忘门、输入门、输出门（LSTM）**、**更新门、重置门（GRU）**）进行变化。这些门控决定了在每一次顺序迭代中，应该保留前一状态的多少信息，应该添加多少来自新更新的信息，然后在最终保留新单元状态的哪个部分。虽然这解决了梯度消失问题，但模型仍然是**顺序工作的**，因此由于并行化有限，尤其是当序列变长时，**训练较慢**。'
- en: '**CNNs**: Process data in a more parallel fashion, but still technically operates
    sequentially. They are **effective in capturing local patterns** but **struggle
    with long-term dependencies** due to the way in which convolution works. The number
    of operations to capture relationships between two input positions **increases
    with distance** between the positions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CNNs**：以更并行的方式处理数据，但从技术上讲仍然是顺序操作。它们**擅长捕捉局部模式**，但由于卷积工作的方式，**难以处理长期依赖关系**。捕捉两个输入位置之间关系所需的操作数量**随着位置之间的距离增加**。'
- en: Hence, introducing the **Transformer**, which relies **entirely on the attention
    mechanism** and does away with the recurrence and convolutions. Attention is what
    the model uses to focus on different parts of the input sequence at each step
    of generating an output. The Transformer was the first model to use attention
    without sequential processing, **allowing for parallelisation** and hence **faster
    training without losing long-term dependencies**. It also performs a **constant
    number of operations** between input positions, regardless of how far apart they
    are.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，引入了**Transformer**，该模型完全依赖**注意力机制**，并且不再使用循环和卷积。注意力机制是模型在每一步生成输出时，用来集中关注输入序列不同部分的机制。Transformer是第一个不依赖顺序处理而使用注意力机制的模型，**允许并行化**，因此**在不丧失长期依赖关系的情况下加速训练**。它还在输入位置之间执行**恒定数量的操作**，无论这些位置相距多远。
- en: '**Walking through the Transformer model architecture**'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**深入了解Transformer模型架构**'
- en: '![](../Images/05d6f25e062e4c69745db402de2c99cf.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05d6f25e062e4c69745db402de2c99cf.png)'
- en: 'Transformer architecture. Image source: [Attention is All You Need](https://arxiv.org/pdf/1706.03762)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构。图片来源：[Attention is All You Need](https://arxiv.org/pdf/1706.03762)
- en: 'The important features of the transformer are: **tokenisation**, the **embedding
    layer,** the **attention mechanism,** the **encoder** and the **decoder.** Let’s
    imagine an input sequence in french: “*Je suis etudiant”*and a target output sequence
    in English “*I am a student”* (I am blatantly copying from this [link](http://jalammar.github.io/illustrated-transformer/),
    which explains the process very descriptively)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的关键特性有：**分词**，**嵌入层**，**注意力机制**，**编码器**和**解码器**。假设有一个法语输入序列：“*Je
    suis etudiant*”和一个目标输出序列为英语“*I am a student*”（我在这里直接复制了这个[链接](http://jalammar.github.io/illustrated-transformer/)，它非常详细地解释了这个过程）
- en: '**Tokenisation**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**分词**'
- en: The input sequence of words is converted into tokens of 3–4 characters long
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的单词序列会转换成长度为3到4个字符的标记
- en: '**Embeddings**'
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**嵌入**'
- en: The input and output sequence are mapped to a sequence of continuous representations,
    **z**, which represents the **input and output embeddings.** Each token will be
    represented by an embedding to capture some kind of **meaning**, which helps in
    **computing its relationship** to other tokens; this embedding will be represented
    as a vector. To create these embeddings, we use the **vocabulary** of the training
    dataset, which contains every unique output token that is being used to train
    the model. We then determine an appropriate embedding dimension, which corresponds
    to the size of the vector representation for each token; higher embedding dimensions
    will better capture more complex / diverse / intricate meanings and relationships.
    The dimensions of the embedding matrix, for vocabulary size V and embedding dimension
    D, hence becomes V x D, making it a **high-dimensional vector**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出序列会映射到一个连续表示的序列**z**，该序列代表了**输入和输出的嵌入**。每个标记会通过嵌入表示，从而捕获某种**含义**，这有助于**计算它与其他标记的关系**；该嵌入将作为向量表示。为了创建这些嵌入，我们使用训练数据集的**词汇表**，它包含了用于训练模型的每个唯一输出标记。接着，我们确定一个合适的嵌入维度，这对应于每个标记的向量表示的大小；较高的嵌入维度将更好地捕获更复杂/多样/精细的含义和关系。因此，嵌入矩阵的维度，对于词汇表大小V和嵌入维度D，变成V
    x D，从而使其成为一个**高维向量**。
- en: At initialisation, these embeddings can be initialised randomly and more accurate
    embeddings are **learned during the training process**. The embedding matrix is
    then updated during training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化时，这些嵌入可以随机初始化，并且更精确的嵌入会在**训练过程中学习**。嵌入矩阵随后会在训练过程中更新。
- en: '**Positional encodings** are added to these embeddings because the transformer
    does not have a built-in sense of the order of tokens.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码**会添加到这些嵌入中，因为Transformer没有内置的标记顺序感知能力。'
- en: '![](../Images/4a8f5359678476367201129a60f79a75.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a8f5359678476367201129a60f79a75.png)'
- en: 'Computing attention scores for the token “it”. As you can see, the model is
    paying large attention to the tokens “The” and “Animal”. Image source: [Jay Alammar](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 计算标记“it”的注意力分数。如你所见，模型将大量注意力集中在标记“The”和“Animal”上。图片来源：[Jay Alammar](http://jalammar.github.io/illustrated-transformer/)
- en: '**Attention mechanism**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**注意力机制**'
- en: Self-attention is the mechanism where each token in a sequence **computes attention
    scores with every other token** in a sequence to **understand relationships**
    between all tokens regardless of distance from each other. I’m going to avoid
    too much math in this article, but you can read up [here](http://jalammar.github.io/illustrated-transformer/)
    about the different matrices formed to compute attention scores and hence capture
    relationships between each token and every other token.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制是每个标记在序列中**与每个其他标记计算注意力分数**，以**理解标记之间的关系**，不管它们彼此之间的距离如何。我会避免在本文中过多涉及数学内容，但你可以在[这里](http://jalammar.github.io/illustrated-transformer/)了解用于计算注意力分数的不同矩阵，从而捕获每个标记与其他标记之间的关系。
- en: These attention scores result in a **new set of representations⁴** for each
    token which is then used in the next layer of processing. During training, the
    **weight matrices are updated through back-propagation**, so the model can better
    account for relationships between tokens.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注意力分数将生成每个标记的**新一组表示⁴**，然后在下一层处理中使用。在训练过程中，**权重矩阵会通过反向传播进行更新**，以便模型能够更好地考虑标记之间的关系。
- en: Multi-head attention is just an extension of self-attention. Different attention
    scores are computed, the results are concatenated and transformed and the resulting
    representation enhances the model’s ability to **capture various complex relationships
    between tokens**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力是自注意力的扩展。计算不同的注意力得分，结果被拼接并转换，最终的表示增强了模型**捕捉词汇间各种复杂关系**的能力。
- en: '**Encoder**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**编码器**'
- en: 'Input embeddings (built from the input sequence) with positional encodings
    are fed into the encoder. The encoder is 6 layers, with each layer containing
    2 sub-layers: **multi-head attention** and **feed forward networks**. There is
    also a residual connection which leads to the output of each layer being LayerNorm(x+Sublayer(x))
    as shown. The output of the encoder is a sequence of vectors which are **contextualised
    representations** of the inputs after accounting for attention scores. These are
    then fed to the decoder.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 输入词嵌入（由输入序列生成）带有位置编码，被输入到编码器中。编码器有6层，每一层包含2个子层：**多头注意力**和**前馈网络**。此外，还有一个残差连接，导致每一层的输出为LayerNorm(x+Sublayer(x))，如图所示。编码器的输出是一系列向量，这些向量是**考虑注意力得分后的输入的上下文化表示**。这些向量随后被传送到解码器中。
- en: '**Decoder**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**解码器**'
- en: Output embeddings (generated from the target output sequence) with positional
    encodings are fed into the decoder. The decoder also contains 6 layers, and there
    are two differences from the encoder.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出词嵌入（由目标输出序列生成）带有位置编码，被输入到解码器中。解码器也包含6层，与编码器有两个不同之处。
- en: First, the output embeddings go through **masked** **multi-head attention**,
    which means that the embeddings from subsequent positions in the sequence are
    ignored when computing the attention scores. This is because when we generate
    the current token (in position i), we should **ignore all output tokens at positions
    after i**. Moreover, the output embeddings are offset to the right by one position,
    so that the predicted token at position i only depends on outputs at positions
    less than it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输出词嵌入通过**掩蔽的** **多头注意力**层，这意味着计算注意力得分时会忽略序列中后续位置的嵌入。这是因为在生成当前位置i的词时，我们应该**忽略位置i之后的所有输出词**。此外，输出词嵌入会向右偏移一个位置，使得位置i的预测词仅依赖于小于它的输出位置的词。
- en: For example, let’s say the input was “*je suis étudiant à l’école”* and target
    output is “*i am a student in school”.* When predicting the token for *student*,
    the encoder takes embeddings for *“je suis etudiant”* while the decoder conceals
    the tokens after “a” so that the prediction of *student* only considers the previous
    tokens in the sentence, namely “I am a”. This trains the model to predict tokens
    sequentially. Of course, the tokens “*in school”* provide added context for the
    model’s prediction, but we are training the model to capture this context from
    the **input token,“***etudiant***”** and **subsequent input tokens**, “*à l’école”.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设输入是“*je suis étudiant à l’école*”，目标输出是“*i am a student in school*”。在预测*student*这个词时，编码器会提取*“je
    suis etudiant”*的词嵌入，而解码器则会隐藏“a”之后的词汇，这样预测*student*时仅考虑句子中之前的词汇，即“I am a”。这训练模型按顺序预测词汇。当然，“*in
    school*”这些词为模型的预测提供了额外的上下文，但我们在训练模型时，是让它从**输入词汇“**etudiant**”**以及**后续输入词汇**“*à
    l’école*”中捕捉到这个上下文。
- en: 'How is the decoder getting this context? Well that brings us to the second
    difference: The second multi-head attention layer in the decoder takes in the
    **contextualised representations of the inputs before being passed into the feed-forward
    network**, to ensure that the output representations capture the full context
    of the input tokens and prior outputs. This gives us a sequence of vectors corresponding
    to each target token, which are **contextualised** **target representations.**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是如何获取上下文的呢？这就引出了第二个区别：解码器中的第二个多头注意力层接收**在输入传入前经过上下文化表示的输入**，以确保输出表示能够捕捉到输入词汇和之前输出的完整上下文。这为每个目标词生成一系列向量，即**上下文化的目标表示**。
- en: '![](../Images/159b57da25747e495fea64d731ba15e6.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/159b57da25747e495fea64d731ba15e6.png)'
- en: 'Image source: [Jay Alammar](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[Jay Alammar](http://jalammar.github.io/illustrated-transformer/)
- en: '**The prediction using the Linear and Softmax layers**'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用线性层和Softmax层进行预测**'
- en: Now, we want to use those contextualised target representations to figure out
    what the next token is. Using the contextualised target representations from the
    decoder, the linear layer projects the sequence of vectors into a much larger
    **logits vector** which is the same length as our model’s vocabulary, let’s say
    of length L. The linear layer contains a weight matrix which, when multiplied
    with the decoder outputs and added with a bias vector, produces a logits vector
    of size 1 x L. Each cell is the score of a unique token, and the softmax layer
    than normalises this vector so that the entire vector sums to one; each cell now
    **represents the probabilities of each token**. The highest probability token
    is chosen, and voila! we have our predicted token.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望使用这些上下文化的目标表示来确定下一个标记是什么。使用解码器中的上下文化目标表示，线性层将向量序列投影到一个更大的**logits向量**中，该向量的长度与我们的模型词汇表的长度相同，假设长度为L。线性层包含一个权重矩阵，乘以解码器的输出并加上一个偏置向量，生成一个大小为1
    x L的logits向量。每个单元格表示一个独特标记的分数，softmax层随后对该向量进行归一化，使整个向量的和为1；此时每个单元格**表示每个标记的概率**。选择概率最高的标记，
    voilà！我们得到了预测标记。
- en: '**Training the model**'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: Next, we compare the predicted token probabilities to the actual token probabilites
    (which will just be logits vector of 0 for every token except for the target token,
    which has probability 1.0). We calculate an appropriate **loss function** for
    each token prediction and average this loss over the entire target sequence. We
    then **back-propagate** this loss over all the model’s parameters to calculate
    appropriate gradients, and use an appropriate optimisation algorithm to **update
    the model parameters**. Hence, for the classic transformer architecture, this
    leads to updates of
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预测的标记概率与实际的标记概率进行比较（实际标记概率对于每个标记来说是一个全为0的logits向量，除了目标标记，其概率为1.0）。我们为每个标记预测计算合适的**损失函数**，并将整个目标序列上的损失进行平均。然后，我们**反向传播**这个损失，计算所有模型参数的合适梯度，并使用合适的优化算法来**更新模型参数**。因此，对于经典的变换器架构，这会导致以下更新：
- en: The embedding matrix
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入矩阵
- en: The different matrices used to compute attention scores
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于计算注意力分数的不同矩阵
- en: The matrices associated with the feed-forward neural networks
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与前馈神经网络相关的矩阵
- en: The linear matrix used to make the logits vector
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于生成logits向量的线性矩阵
- en: Matrices in 2–4 are weight matrices, and there are additional bias terms associated
    with each output which are also updated during training.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第2至第4行中的矩阵是权重矩阵，每个输出还附有额外的偏置项，这些偏置项在训练过程中也会更新。
- en: ''
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** The linear matrix and embedding matrix are often transposes of each
    other. This is the case for the Attention is All You Need paper; the technique
    is called “weight-tying”. The number of parameters to train are thus reduced.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 线性矩阵和嵌入矩阵通常是彼此的转置。这是《Attention is All You Need》论文中的情况；这一技术称为“权重绑定”。因此，需要训练的参数数量得以减少。'
- en: This represents **one epoch** of training. Training comprises multiple epochs,
    with the number depending on the size of the datasets, size of the models, and
    the model’s task.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了**一次训练周期**。训练包含多个周期，周期数取决于数据集的大小、模型的大小以及模型的任务。
- en: '**Going back to what makes Transformers so good**'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**回到为什么变换器如此优秀**'
- en: As we mentioned earlier, the problems with the RNNs, CNNs, LSTMs and more include
    the lack of parallel processing, their sequential architecture, and inadequate
    capturing of long-term dependencies. The transformer architecture above solves
    these problems as…
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RNN、CNN、LSTM等的缺点包括缺乏并行处理、顺序架构以及无法充分捕捉长期依赖关系。上述的变换器架构解决了这些问题，因为…
- en: The Attention mechanism allowsthe **entire sequence to be processed in parallel
    rather than sequentially**. With self-attention, each token in the input sequence
    attends to every other token in the input sequence (of that mini batch, explained
    next). This captures all relationships at the same time, rather than in a sequential
    manner.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力机制允许**整个序列并行处理而不是顺序处理**。通过自注意力机制，输入序列中的每个标记都会关注输入序列中的其他所有标记（在下文解释的该小批次中）。这会同时捕捉所有关系，而不是按顺序捕捉。
- en: Mini-batching of input within each epoch allows **parallel processing, faster
    training,** and **easier scalability of the model**. In a large text full of examples,
    mini-batches represent a smaller collection of these examples. The examples in
    the dataset are shuffled before being put into mini-batches, and reshuffled at
    the beginning of each epoch. Each mini-batch is passed into the model at the same
    time.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个 epoch 中进行输入的小批量处理允许**并行处理、加快训练**，并且**更容易扩展模型**。在一个充满示例的大文本中，小批量表示这些示例的较小集合。数据集中的示例在被放入小批量之前会被打乱，并在每个
    epoch 开始时重新洗牌。每个小批量会同时传入模型。
- en: By using positional encodings and batch processing, the order of tokens in a
    sequence is accounted for. Distances between tokens are also **accounted for equally
    regardless of how far they are**, and the mini-batch processing further ensures
    this.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用位置编码和批处理，考虑到序列中标记的顺序。标记之间的距离也会被**平等地考虑，无论它们之间的距离有多远**，而小批量处理进一步确保了这一点。
- en: As shown in the paper, the results were fantastic.
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正如论文中所示，结果非常出色。
- en: Welcome to the world of transformers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到变换器的世界。
- en: A quick bit on GPT Architecture
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于 GPT 架构的简要介绍
- en: The transformer architecture was introduced by the researcher Ashish Vaswani
    in 2017 while he was working at Google Brain. The Generative Pre-trained Transformer
    (GPT) was introduced by OpenAI in 2018\. The primary difference is that GPT’s
    do not contain an encoder stack in their architecture. The encoder-decoder makeup
    is useful when were directly converting one sequence into another sequence. The
    GPT was designed to focus on generative capabilities, and it did away with the
    encoder while keeping the rest of the components similar.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构由研究员 Ashish Vaswani 于 2017 年提出，当时他在 Google Brain 工作。生成式预训练变换器（GPT）由
    OpenAI 于 2018 年推出。两者的主要区别在于 GPT 的架构中没有编码器堆栈。编码器-解码器结构在直接将一个序列转换为另一个序列时非常有用。而 GPT
    的设计侧重于生成能力，它去除了编码器，同时保持了其他组件相似。
- en: '![](../Images/7207b4ada7af6ca14f8ad89dafe24fbf.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7207b4ada7af6ca14f8ad89dafe24fbf.png)'
- en: 'Image source: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[通过生成预训练提高语言理解](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- en: The GPT model is pre-trained on a large corpus of text, unsupervised, to learn
    relationships between all words and tokens⁵. After fine-tuning for various use
    cases (such as a general purpose chatbot), they have proven to be extremely effective
    in generative tasks.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型是在一个大型文本语料库上进行预训练的，采用无监督学习，以了解所有单词和标记之间的关系⁵。经过针对不同应用场景（如通用聊天机器人）的微调后，它们在生成任务中已被证明非常有效。
- en: Example
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: When you ask it a question, the steps for prediction are largely the same as
    a regular transformer. If you ask it the question “How does GPT predict responses”,
    these words are tokenised, embeddings generated, attention scores computed, probabilities
    of the next word are calculated, and a token is chosen to be the next predicted
    token. For example, the model might generate the response step by step, starting
    with “GPT predicts responses by…” and continuing based on probabilities until
    it forms a complete, coherent response. (*guess what, that last sentence was from
    chatGPT)*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当你问它一个问题时，预测的步骤与常规的变换器大致相同。如果你问它问题：“GPT 如何预测回应”，这些词会被标记化，生成嵌入，计算注意力分数，计算下一个词的概率，并选择一个标记作为下一个预测的标记。例如，模型可能会一步步生成回应，从“GPT
    通过…预测回应”开始，并根据概率继续，直到形成完整且连贯的回应。（*猜猜看，最后一句话来自 chatGPT*）。
- en: I hope all this was easy enough to understand. If it wasn’t, then maybe it’s
    somebody else’s turn to have a go at explaining transformers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这一切足够容易理解。如果没有，那也许是时候让别人来尝试解释一下变换器了。
- en: Feel free to share your thoughts and connect with me if this article was interesting
    to you!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这篇文章对你有启发，欢迎分享你的想法并与我联系！
- en: 'LinkedIn: [https://www.linkedin.com/in/aveekg00/](https://www.linkedin.com/in/aveekg00/)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LinkedIn：[https://www.linkedin.com/in/aveekg00/](https://www.linkedin.com/in/aveekg00/)
- en: 'Website: [aveek.info](http://aveek.info)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网站：[aveek.info](http://aveek.info)
- en: 'References:'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)'
- en: '[https://deeplearningmath.org/sequence-models](https://deeplearningmath.org/sequence-models)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://deeplearningmath.org/sequence-models](https://deeplearningmath.org/sequence-models)'
- en: '[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: '[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
- en: '[https://openai.com/index/language-unsupervised/](https://openai.com/index/language-unsupervised/)'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://openai.com/index/language-unsupervised/](https://openai.com/index/language-unsupervised/)'
- en: 'Other great articles to refer to:'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他值得参考的优秀文章：
- en: '[https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)'
- en: '[https://bastings.github.io/annotated_encoder_decoder/](https://bastings.github.io/annotated_encoder_decoder/)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://bastings.github.io/annotated_encoder_decoder/](https://bastings.github.io/annotated_encoder_decoder/)'
- en: '[https://nlp.seas.harvard.edu/annotated-transformer/#prelims](https://nlp.seas.harvard.edu/annotated-transformer/#prelims)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://nlp.seas.harvard.edu/annotated-transformer/#prelims](https://nlp.seas.harvard.edu/annotated-transformer/#prelims)'
