- en: A Critical Look at AI Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对AI图像生成的批判性审视
- en: 原文：[https://towardsdatascience.com/a-critical-look-at-ai-image-generation-45001f410147?source=collection_archive---------4-----------------------#2024-10-17](https://towardsdatascience.com/a-critical-look-at-ai-image-generation-45001f410147?source=collection_archive---------4-----------------------#2024-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-critical-look-at-ai-image-generation-45001f410147?source=collection_archive---------4-----------------------#2024-10-17](https://towardsdatascience.com/a-critical-look-at-ai-image-generation-45001f410147?source=collection_archive---------4-----------------------#2024-10-17)
- en: What does image generative AI really tell us about our world?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像生成型AI究竟能告诉我们关于这个世界什么？
- en: '[](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--45001f410147--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)
    ·9 min read·Oct 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--45001f410147--------------------------------)
    ·阅读时间9分钟·2024年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6acab230e5de0801ddad5c6b19f9ddfe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6acab230e5de0801ddad5c6b19f9ddfe.png)'
- en: Photo by [Math](https://unsplash.com/@builtbymath?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Math](https://unsplash.com/@builtbymath?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I recently had the opportunity to provide analysis on [an interesting project](https://bit.ly/genaiSK),
    and I had more to say than could be included in that single piece, so today I’m
    going to discuss some more of my thoughts about it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近有机会对[一个有趣的项目](https://bit.ly/genaiSK)进行分析，并且我有更多的想法，超出了那篇文章所能包含的内容，所以今天我将进一步讨论我对这个项目的看法。
- en: 'The approach the researchers took with this project involved providing a series
    of prompts to different generative AI image generation tools: Stable Diffusion,
    Midjourney, YandexART, and ERNIE-ViLG (by Baidu). The prompts were particularly
    framed around different generations — Baby Boomers, Gen X, Millennials, and Gen
    Z, and requested images of these groups in different contexts, such as “with family”,
    “on vacation”, or “at work”.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员在这个项目中的方法是向不同的生成型AI图像生成工具提供一系列提示词：Stable Diffusion、Midjourney、YandexART
    和 ERNIE-ViLG（由百度开发）。这些提示词特别围绕不同的代际群体——婴儿潮一代、X世代、千禧一代和Z世代，要求生成这些群体在不同场景下的图像，如“与家人在一起”，“度假”或“在工作中”。
- en: While the results were very interesting, and perhaps revealed some insights
    about visual representation, I think we should also take note of what this cannot
    tell us, or what the limitations are. I’m going to divide up my discussion into
    the aesthetics (what the pictures look like) and representation (what is actually
    shown in the images), with a few side tracks into how these images come to exist
    in the first place, because that’s really important to both topics.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些结果非常有趣，或许能揭示一些关于视觉表现的见解，但我认为我们也应该注意到这些图像无法告诉我们什么，或者它们的局限性在哪里。我将把我的讨论分为审美（图像的外观）和表现（图像中实际呈现的内容）两部分，并稍微偏离一下，谈谈这些图像是如何生成的，因为这一点对这两个话题都非常重要。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Before I start, though, a quick overview of these image generator models. They’re
    created by taking giant datasets of images (photographs, artwork, etc) paired
    with short text descriptions, and the goal is to get the model to learn the relationships
    between words and the appearance of the images, such that when given a word the
    model can create an image that matches, more or less. There’s a lot more detail
    under the hood, and the models (like other generative AI) have a built in degree
    of randomness that allows for variations and surprises.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不过在开始之前，先快速回顾一下这些图像生成模型。它们的创建方式是通过大量图像数据集（包括照片、艺术作品等）与简短的文本描述配对，目标是让模型学习词语和图像外观之间的关系，以便当给定一个词时，模型能够生成与之大致匹配的图像。模型的内部机制更为复杂，而且这些模型（像其他生成式
    AI）内建了随机性，这样可以产生变化和惊喜。
- en: When you use one of these hosted models, you give a text prompt and an image
    is returned. However, it’s important to note that your prompt is not the ONLY
    thing the model gets. There are also built in instructions, which I call pre-prompting
    instructions sometimes, and these can have an effect on what the output is. Examples
    might be telling the model to refuse to create certain kinds of offensive images,
    or to reject prompts using offensive language.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用这些托管模型时，你输入一个文本提示，然后模型会返回一张图像。然而，重要的是要注意，提示并不是模型接收到的唯一信息。模型还有一些内建的指令，有时我称之为预提示指令，这些指令会对输出结果产生影响。举例来说，可能会告诉模型拒绝创建某些类型的冒犯性图像，或者拒绝使用冒犯性语言的提示。
- en: Training Data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: An important framing point here is that the training data, those big sets of
    images that are paired with text blurbs, is what the model is trying to replicate.
    So, we should ask more questions about the training data, and where it comes from.
    To train models like these, the volume of image data required is extraordinary.
    Midjourney was trained on [https://laion.ai/](https://laion.ai/), whose larger
    dataset has 5 billion image-text pairs across multiple languages, and we can assume
    the other models had similar volumes of content. This means that engineers can’t
    be TOO picky about which images are used for training, because they basically
    need everything they can get their hands on.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个重要的框架是，训练数据——那些与文本描述配对的大型图像集——是模型试图复制的对象。所以，我们应该对训练数据及其来源提出更多问题。为了训练这些模型，所需的图像数据量是巨大的。Midjourney
    就是在 [https://laion.ai/](https://laion.ai/) 上进行训练的，该平台的更大数据集包含了跨多种语言的 50 亿对图像-文本配对，我们可以假设其他模型的内容量也相似。这意味着工程师不能对用于训练的图像挑剔太多，因为他们基本上需要尽可能多的内容。
- en: 'Ok, so where do we get images? How are they generated? Well, we create our
    own and post them on social media by the bucketload, so that’s necessarily going
    to be a chunk of it. (It’s also easy to get a hold of, from these platforms.)
    Media and advertising also create tons of images, from movies to commercials to
    magazines and beyond. Many other images are never going to be accessible to these
    models, like your grandma’s photo album that no one has digitized, but the ones
    that are available to train are largely from these two buckets: independent/individual
    creators and media/ads.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么我们从哪里获取图像呢？它们是如何生成的？嗯，我们自己制作图像并通过社交媒体大量发布，所以这无疑会占据其中一部分。（从这些平台上获取也很容易。）媒体和广告也创造了大量的图像，从电影到广告，再到杂志等等。许多其他图像永远无法被这些模型访问，比如你奶奶的照片专辑没人数字化过，但可以用于训练的图像主要来自这两个来源：独立/个人创作者和媒体/广告。
- en: So, what do you actually get when you use one of these models?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当你使用这些模型时，实际上会得到什么呢？
- en: Aesthetics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 美学
- en: One thing you’ll notice if you try out these different image generators is the
    stylistic distinctions between them, and the internal consistency of styles. I
    think this is really fascinating, because they feel like they almost have personalities!
    Midjourney is dark and moody, with shadowy elements, while Stable Diffusion is
    bright and hyper-saturated, with very high contrast. ERNIE-ViLG seems to lean
    towards a cartoonish style, also with very high contrast and textures appearing
    rubbery or highly filtered. YandexART has washed out coloring, with often featureless
    or very blurred backgrounds and the appearance of spotlighting (it reminds me
    of a family photo taken at a department store in some cases). A number of different
    elements may be responsible for each model’s trademark style.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试这些不同的图像生成器，你会注意到它们之间的风格区别，以及风格内部的一致性。我认为这非常有趣，因为它们几乎像是有自己的个性！Midjourney
    显得阴暗且情绪化，带有阴影元素，而 Stable Diffusion 则明亮且过度饱和，对比度极高。ERNIE-ViLG 似乎倾向于卡通风格，同样具有极高的对比度，且质感呈现出橡胶感或高度滤镜化的效果。YandexART
    的颜色偏褪色，背景通常没有特征或非常模糊，并且有聚光灯效果（在某些情况下让我想起了在百货商店拍的家庭照）。多种不同的因素可能导致每个模型的独特风格。
- en: As I’ve mentioned, pre-prompting instructions are applied in addition to whatever
    input the user gives. These may indicate specific aesthetic components that the
    outputs should always have, such as stylistic choices like the color tones, brightness,
    and contrast, or they may instruct the model not to follow objectionable instructions,
    among other things. This forms a way for the model provider to implement some
    limits and guardrails on the tool, preventing abuse, but can also create aesthetic
    continuity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，预先提示指令是在用户提供输入的基础上应用的。这些指令可能会指定输出应该始终具有的特定美学成分，例如色调、亮度和对比度等风格选择，或者指示模型避免遵循不当的指令等。这为模型提供者实现工具的某些限制和保护措施，防止滥用，也能创造出美学上的连贯性。
- en: The process of fine tuning with reinforcement learning may also affect style,
    where human observers are making judgments about the outputs that are provided
    back to the model for learning. The human observers will have been trained and
    given instructions about what kinds of features of the output images to approve
    of/accept and which kinds should be rejected or down-scored, and this may involve
    giving higher ratings to certain kinds of visuals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过强化学习进行微调的过程也可能影响风格，其中人工观察员对返回模型的输出做出判断。人工观察员会接受培训，并被指示哪些图像特征应该被批准/接受，哪些应该被拒绝或降分，这可能涉及对某些类型的视觉效果给予更高的评分。
- en: The type of training data also has an impact. We know some of the massive datasets
    that are employed for training the models, but there is probably more we don’t
    know, so we have to infer from what the models produce. If the model is producing
    high-contrast, brightly colored images, there’s a good chance the training data
    included a lot of images with those characteristics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的类型也会产生影响。我们知道一些用于训练模型的大型数据集，但可能还有我们不知道的内容，因此我们必须从模型输出中推断。如果模型生成的是高对比度、明亮的图像，那么训练数据中很可能包含了大量具有这些特征的图像。
- en: As we analyze the outputs of the different models, however, it’s important to
    keep in mind that these styles are probably a combination of pre-prompting instructions,
    the training data, and the human fine tuning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们分析不同模型的输出时，重要的是要记住，这些风格可能是预先提示指令、训练数据和人工微调的结合。
- en: Beyond the visual appeal/style of the images, what’s actually in them?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了图像的视觉吸引力/风格之外，图像中实际包含了什么内容？
- en: Representation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表现形式
- en: Limitations
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'What the models will have the capability to do is going to be limited by the
    reality of how they’re trained. These models are trained on images from the past
    — some the very recent past, but some much further back. For example, consider:
    as we move forward in time, younger generations will have images of their entire
    lives online, but for older groups, images from their youth or young adulthood
    are not available digitally in large quantities (or high quality) for training
    data, so we may never see them presented by these models as young people. It’s
    very visible in this project: For Gen Z and Millennials, in this data we see that
    the models struggle to “age” the subjects in the output appropriately to the actual
    age ranges of the generation today. Both groups seem to look more or less the
    same age in most cases, with Gen Z sometimes shown (in prompts related to schooling,
    for example) as actual children. In contrast, Boomers and Gen X are shown primarily
    in middle age or old age, because the training data that exists is unlikely to
    have scanned copies of photographs from their younger years, from the 1960s-1990s.
    This makes perfect sense if you think in the context of the training data.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的能力将受限于它们的训练现实。这些模型是基于过去的图像进行训练的——一些是最近的图像，但有些则是更久远的图像。例如，考虑到：随着时间的推移，年轻一代的整个生活都将在线上有图像，但对于年长的群体来说，他们年轻或青少年时期的图像并没有大量数字化（或者是高质量的）可用于训练数据，因此我们可能永远不会看到这些模型将他们展现为年轻人。在这个项目中，这一点非常明显：对于Z世代和千禧一代，在这些数据中我们看到模型在输出中很难适当地“衰老”这些人像，以符合当今这些群体的实际年龄范围。大多数情况下，这两代人看起来年龄相仿，Z世代有时在（比如和学校相关的提示）中被展示为孩子。相比之下，婴儿潮一代和X世代主要显示为中年或老年，因为现有的训练数据不太可能有他们年轻时的照片扫描副本，这些照片大多来自1960年代到1990年代。如果你从训练数据的背景来看，这完全是合理的。
- en: '[A]s we move forward in time, younger generations will have images of their
    entire lives online, but for older groups, images from their youth or young adulthood
    are not available digitally for training data, so we may never see them presented
    by these models as young people.'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[随着时间的推移，年轻一代的整个生活将在线上有图像，但对于年长的群体来说，他们年轻或青少年时期的图像并没有数字化用于训练数据，因此我们可能永远不会看到这些模型将他们展现为年轻人。]'
- en: Identity
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 身份
- en: With this in mind, I’d argue that what we can get from these images, if we investigate
    them, is some impression of A. how different age groups present themselves in
    imagery, particularly selfies for the younger sets, and B. how media representation
    looks for these groups. (It’s hard to break these apart sometimes, because media
    and youth culture are so dialectical.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有鉴于此，我认为如果我们调查这些图像，能够从中得到的印象是A.不同年龄群体如何在图像中展示自己，尤其是年轻人自拍照的表现，B.这些群体在媒体中的表现如何。（有时很难将这两者分开，因为媒体和青少年文化是相互对立的。）
- en: The training data didn’t come out of nowhere — human beings chose to create,
    share, label, and curate the images, so those people’s choices are coloring everything
    about them. The models are getting the image of these generations that someone
    has chosen to portray, and in all cases these portrayals have a reason and intention
    behind it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据并非凭空产生——人类选择了创建、分享、标注和策划这些图像，因此这些人的选择影响了图像的所有内容。模型获取的是这些世代中有人选择展示的图像，且在所有情况下，这些展示背后都有原因和意图。
- en: A teen or twentysomething taking a selfie and posting it online (so that it
    is accessible to become training data for these models) probably took ten, or
    twenty, or fifty before choosing which one to post to Instagram. At the same time,
    a professional photographer choosing a model to shoot for an ad campaign has many
    considerations in play, including the product, the audience, the brand identity,
    and more. Because professional advertising isn’t free of racism, sexism, ageism,
    or any of the other -isms, these images won’t be either, and as a result, the
    image output of these models comes with that same baggage. Looking at the images,
    you can see many more phenotypes resembling people of color among Millennial and
    Gen Z for certain models (Midjourney and Yandex in particular), but hardly any
    of those phenotypes among Gen X and Boomers in the same models. This may be at
    least partly because advertisers targeting certain groups choose representation
    of race and ethnicity (as well as age) among models that they believe will appeal
    to them and be relatable, and they’re presupposing that Boomers and Gen X are
    more likely to purchase if the models are older and white. These are the images
    that get created, and then end up in the training data, so that’s what the models
    learn to produce.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个青少年或二十多岁的人自拍并将其发布到网上（以便它能够成为这些模型的训练数据）可能在选择发布到 Instagram 之前，已经拍了十张、二十张或者五十张。与此同时，一个专业摄影师在为广告活动挑选模特时，考虑的因素很多，包括产品、受众、品牌形象等等。因为专业广告并不免于种族主义、性别歧视、年龄歧视或其他任何“主义”，所以这些图像也不会例外，结果是这些模型生成的图像也带有这些社会负担。从这些图像中，你可以看到在一些模型（尤其是
    Midjourney 和 Yandex）中，千禧一代和 Z 世代中更多的种族特征，而在同样的模型中，X 世代和婴儿潮一代几乎看不到这些种族特征。这可能至少部分是因为广告商在针对某些群体时，选择那些他们认为会吸引并让他们产生共鸣的种族、民族（以及年龄）代表，且他们预设认为，婴儿潮一代和
    X 世代如果看到较老且白人的模特，更可能产生购买欲望。这些图像被创建出来，随后进入训练数据，因此模型学会了生成这些图像。
- en: The point I want to make is that these are not free of influence from culture
    and society — whether that influence is good or bad. The training data came from
    human creations, so the model is bringing along all the social baggage that those
    humans had.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我想表达的观点是，这些数据并非不受文化和社会的影响——无论这种影响是好是坏。训练数据来源于人类创造，因此模型带着那些人类的社会负担。
- en: '*The point I want to make is that these are not free of influence from culture
    and society — whether that influence is good or bad. The training data came from
    human creations, so the model is bringing along all the social baggage that those
    humans had.*'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我想表达的观点是，这些数据并非不受文化和社会的影响——无论这种影响是好是坏。训练数据来源于人类创造，因此模型带着那些人类的社会负担。*'
- en: Because of this reality, I think that asking whether we can learn about generations
    from the images that models produce is kind of the wrong question, or at least
    a misguided premise. We might incidentally learn something about the people whose
    creations are in the training set, which may include selfies, but we’re much more
    likely to learn about the broader society, in the form of people taking pictures
    of others as well as themselves, the media, and commercialism. Some (or even a
    lot) of what we’re getting, especially for the older groups who don’t contribute
    as much self-generated visual media online, is at best perceptions of that group
    from advertising and media, which we know has inherent flaws.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这个现实，我认为问我们是否能从模型生成的图像中了解不同代际是一个错误的问题，或者至少是一个误导性的前提。我们可能偶然学到一些关于训练集中人类创造的东西，可能包括自拍，但我们更有可能学到关于更广泛社会的东西，形式上包括人们拍摄他人和自己的照片、媒体以及商业化。我们得到的一些（甚至是很多）内容，尤其是对于那些较少在线发布自创视觉媒体的老年群体，充其量是广告和媒体中的对该群体的刻板印象，而我们知道这些刻板印象本身就有固有缺陷。
- en: Is there *anything* to be gained about generational understanding from these
    images? Perhaps. I’d say that this project can potentially help us see how generational
    identities are being filtered through media, although I wonder if it is the most
    convenient or easy way to do that analysis. After all, we could go to the source
    — although the aggregation that these models conduct may be academically interesting.
    It also may be more useful for younger generations, because more of the training
    data is self-produced, but even then I still think we should remember that we
    imbue our own biases and agendas into the images we put out into the world about
    ourselves.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些图像中是否可以获得*关于代际理解的任何*东西？或许可以。我认为这个项目可能有助于我们看到代际身份是如何通过媒体进行过滤的，尽管我怀疑这是否是进行这种分析的最方便或最简单的方式。毕竟，我们可以去直接了解源头——尽管这些模型进行的聚合可能在学术上很有趣。它对于年轻一代来说可能更有用，因为更多的训练数据是自我生成的，但即便如此，我依然认为我们应该记住，我们在发布的关于自己的图像中也会注入自己的偏见和议程。
- en: As an aside, there is a knee-jerk impulse among some commentators to demand
    some sort of whitewashing of the things that models like this create— that’s how
    we get [models that will create images of Nazi soldiers of various racial and
    ethnic appearances](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical).
    [As I’ve written before](https://medium.com/towards-data-science/seeing-our-reflection-in-llms-7b9505e901fd),
    this is largely a way to avoid dealing with the realities about our society that
    models feed back to us. We don’t like the way the mirror looks, so we paint over
    the glass instead of considering our own face.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，一些评论员有一种本能的冲动，要求对像这样的模型所生成的内容进行某种程度的美化——这就是我们得到[会生成各种种族和民族外貌的纳粹士兵图像的模型](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical)的原因。[正如我之前写的](https://medium.com/towards-data-science/seeing-our-reflection-in-llms-7b9505e901fd)，这在很大程度上是为了避免面对这些模型反馈给我们的社会现实。我们不喜欢镜子中的样子，所以我们涂抹镜面，而不是去审视我们自己的面容。
- en: Of course, that’s not completely true either — *all* of our norms and culture
    are not going to be represented in the model’s output, only that which we commit
    to images and feed in to the training data. We’re seeing some slice of our society,
    but not the whole thing in a truly warts-and-all fashion. So, we must set our
    expectations realistically based on what these models are and how they are created.
    We are not getting a pristine picture of our lives in these models, because the
    photos we take (and the ones we don’t take, or don’t share), and the images media
    creates and disseminates, are not free of bias or objective. It’s the same reason
    we shouldn’t judge ourselves and our lives against the images our friends post
    on Instagram — that’s not a complete and accurate picture of their life either.
    Unless we implement a massive campaign of photography and image labeling that
    pursues accuracy and equal representation, for use in training data, we are not
    going to be able to change the way this system works.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这也并不完全正确——我们所有的规范和文化并不会在模型的输出中得到体现，只有那些我们付诸于图像并输入到训练数据中的内容才会被表示出来。我们只看到社会的一部分，而不是以完全真实、无所隐瞒的方式呈现整个社会。因此，我们必须根据这些模型的性质和它们的创建方式，现实地设定我们的期望。我们无法在这些模型中得到我们生活的纯粹图像，因为我们拍摄的照片（以及我们未拍摄的照片，或者未分享的照片），以及媒体创造和传播的图像，都不是没有偏见或客观性的。这就像我们不应该拿自己和生活与朋友们在
    Instagram 上发布的图像做比较——那也不是他们生活的完整和准确的写照。除非我们进行一场大规模的摄影和图像标注活动，追求准确性和平等的代表性，并将这些用于训练数据，否则我们无法改变这个系统的运作方式。
- en: Conclusion
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Getting to spend time with these ideas has been really interesting for me, and
    I hope the analysis is helpful for those of you who use these kinds of models
    regularly. There are lots of issues with using generative AI image generating
    models, from the [environmental](https://medium.com/towards-data-science/environmental-implications-of-the-ai-boom-279300a24184)
    to the [economic](https://medium.com/towards-data-science/the-coming-copyright-reckoning-for-generative-ai-b7fe0963c58f),
    but I think understanding what they are (and aren’t) and what they really do is
    critical if you choose to use the models in your day to day.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间思考这些想法对我来说非常有趣，我希望这个分析对那些经常使用这些类型模型的人有所帮助。使用生成型 AI 图像生成模型有许多问题，从[环境](https://medium.com/towards-data-science/environmental-implications-of-the-ai-boom-279300a24184)到[经济](https://medium.com/towards-data-science/the-coming-copyright-reckoning-for-generative-ai-b7fe0963c58f)，但我认为理解这些模型是什么（以及不是）以及它们实际做的事情，对于你选择是否在日常生活中使用这些模型是至关重要的。
- en: Read more from me at [www.stephaniekirmer.com](http://www.stephaniekirmer.com).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读更多我的文章，请访问 [www.stephaniekirmer.com](http://www.stephaniekirmer.com)。
- en: Further Reading
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: '[](/seeing-our-reflection-in-llms-7b9505e901fd?source=post_page-----45001f410147--------------------------------)
    [## Seeing Our Reflection in LLMs'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/seeing-our-reflection-in-llms-7b9505e901fd?source=post_page-----45001f410147--------------------------------)
    [## 在LLMs中看到我们的倒影'
- en: When LLMs give us outputs that reveal flaws in human society, can we choose
    to listen to what they tell us?
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 当LLMs给我们呈现出揭示人类社会缺陷的结果时，我们能否选择倾听它们所告诉我们的？
- en: towardsdatascience.com](/seeing-our-reflection-in-llms-7b9505e901fd?source=post_page-----45001f410147--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/seeing-our-reflection-in-llms-7b9505e901fd?source=post_page-----45001f410147--------------------------------)'
- en: '[https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical](https://www.theverge.com/2024/2/21/24079371/google-ai-gemini-generative-inaccurate-historical)'
- en: 'The project: [https://bit.ly/genaiSK](https://bit.ly/genaiSK)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 项目：[https://bit.ly/genaiSK](https://bit.ly/genaiSK)
