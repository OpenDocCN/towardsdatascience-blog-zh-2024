- en: Evaluating SQL Generation with LLM as a Judge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-sql-generation-with-llm-as-a-judge-1ff69a70e7cf?source=collection_archive---------4-----------------------#2024-07-31](https://towardsdatascience.com/evaluating-sql-generation-with-llm-as-a-judge-1ff69a70e7cf?source=collection_archive---------4-----------------------#2024-07-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/9476fd2135a3132b8b6ef277b53fe2a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: Results point to a promising approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1ff69a70e7cf--------------------------------)
    ·4 min read·Jul 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*A special shoutout to Manas Singh and Evan Jolley for collaborating with us
    on this research!*'
  prefs: []
  type: TYPE_NORMAL
- en: A potential application of LLMs that has attracted attention and investment
    is around their ability to generate SQL queries. Querying large databases with
    natural language unlocks several compelling use cases, from increasing data transparency
    to improving accessibility for non-technical users.
  prefs: []
  type: TYPE_NORMAL
- en: However, as with any AI-generated content, the question of evaluation is important.
    How can we determine if an LLM-generated SQL query is correct and produces the
    intended results? Our recent research dives into this question and explores the
    effectiveness of using [LLM as a judge](https://docs.arize.com/phoenix/evaluation/concepts-evals/llm-as-a-judge)
    to evaluate SQL generation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Findings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLM as a judge shows initial promise in evaluating SQL generation, with F1 scores
    between 0.70 and 0.76 using OpenAI’s GPT-4 Turbo in this experiment. Including
    relevant schema information in the evaluation prompt can significantly reduce
    false positives. While challenges remain — including false negatives due to incorrect
    schema interpretation or assumptions about data — LLM as a judge provides a solid
    proxy for AI SQL generation performance, especially as a quick check on results.
  prefs: []
  type: TYPE_NORMAL
- en: Methodology and Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This study builds upon previous work done by the Defog.ai team, who [developed
    an approach](https://github.com/defog-ai/sql-eval) to evaluate SQL queries using
    golden datasets and queries. The process involves using a golden dataset question
    for AI SQL generation, generating test results “x” from the AI-generated SQL,
    using a pre-existing golden query on the same dataset to produce results “y,”
    and then comparing results “x” and “y” for accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0321f9684b937ee0c804967416443634.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: For this comparison, we first explored traditional methods of SQL evaluation,
    such as exact data matching. This approach involves a direct comparison of the
    output data from the two queries. For instance, when evaluating a query about
    author citations, any differences in the number of authors or their citation counts
    would result in a mismatch and failure. While straightforward, this method does
    not handle edge cases, such as how to handle zero-count bins or slight variations
    in numeric outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/128cb4d97ec04ebbac643c34ee6044ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We then tried a more nuanced approach: using an LLM-as-a-judge. Our initial
    tests with this method, using OpenAI’s GPT-4 Turbo without including database
    schema information in the evaluation prompt, yielded promising results with F1
    scores between 0.70 and 0.76\. In this setup, the LLM judged the generated SQL
    by examining only the question and the resulting query.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40b5b95a1c7b9ce81ea8e9c3bdf0be65.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Results: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: In this test we noticed that there were quite a few false positives and negatives,
    many of them related to mistakes or assumptions about the database schema. In
    this false negative case, the LLM assumed that the response would be in a different
    unit than expected (semesters versus days).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e03dd586b3b2a1be3425f269d1f9caf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: These discrepancies led us to add the database schema into the evaluation prompt.
    Contrary to our expectations, this resulted in worse performance. However, when
    we refined our approach to include only the schema for tables referenced in the
    queries, we saw significant improvement in both the false positive and negative
    rates.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73f63923309c1d1977eb4a238041a0de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Results: Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and Future Directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the potential of using LLMs to evaluate SQL generation is clear, challenges
    remain. Often, LLMs make incorrect assumptions about data structures and relationships
    or incorrectly assume units of measurement or data formats. Finding the right
    amount and type of schema information to include in the evaluation prompt is important
    for optimizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Anyone exploring a SQL generation use case might explore several other areas
    like optimizing the inclusion of schema information, improving LLMs’ understanding
    of database concepts, and developing hybrid evaluation methods that combine LLM
    judgment with traditional techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the ability to catch nuanced errors, LLM as a judge shows promise as a
    quick and effective tool for assessing AI-generated SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Carefully selecting what information is provided to the LLM judge helps in getting
    the most out of this method; by including relevant schema details and continually
    refining the [LLM evaluation process](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/),
    we can improve the accuracy and reliability of SQL generation assessment.
  prefs: []
  type: TYPE_NORMAL
- en: As natural language interfaces to databases increase in popularity, the need
    for effective evaluation methods will only grow. The LLM as a judge approach,
    while not perfect, provides a more nuanced evaluation than simple data matching,
    capable of understanding context and intent in a way that traditional methods
    cannot.
  prefs: []
  type: TYPE_NORMAL
- en: Questions? Feel free to reach out here or on [*LinkedIn*](https://www.linkedin.com/in/aparnadhinakaran/)
    or [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg).
  prefs: []
  type: TYPE_NORMAL
