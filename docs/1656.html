<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding and Implementing Medprompt</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding and Implementing Medprompt</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-implementing-medprompt-77bbd2777c91?source=collection_archive---------0-----------------------#2024-07-06">https://towardsdatascience.com/understanding-and-implementing-medprompt-77bbd2777c91?source=collection_archive---------0-----------------------#2024-07-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c01c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Digging into the details behind the prompting framework</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anand.subu10?source=post_page---byline--77bbd2777c91--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anand Subramanian" class="l ep by dd de cx" src="../Images/096dc5504d6ada2493e0ac26959e60f0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IfxBBsal-XaXfAXh_c9g1A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--77bbd2777c91--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anand.subu10?source=post_page---byline--77bbd2777c91--------------------------------" rel="noopener follow">Anand Subramanian</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--77bbd2777c91--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/17dd8d46598db7fb9ab33ac9280bb740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wonLSrQLvFeiO4u8Svd28Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Illustration of the various components of the Medprompt Strategy (Image taken from Fig:6 from the Medprompt paper [1] (<a class="af nc" href="https://arxiv.org/abs/2311.16452" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2311.16452</a>)</figcaption></figure><p id="8fe5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In my <a class="af nc" href="https://medium.com/towards-data-science/an-introduction-to-prompting-for-llms-61d36aec2048" rel="noopener">first blog post</a>, I explored prompting and its significance in the context of Large Language Models (LLMs). Prompting is crucial for obtaining high-quality outputs from LLMs, as it guides the model’s responses and ensures they are relevant to the task at hand. Building on that foundation, two crucial questions often arise when trying to solve a use case using LLMs: how far can you push performance with prompting alone, and when do you bite the bullet and decide it might be more effective to fine-tune a model instead?</p><p id="6826" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When making design decisions about leveraging prompting, several considerations come into play. Techniques like few-shot prompting and Chain-of-Thought (CoT) [2] prompting can help in boosting the performance of LLMs for most tasks. Retrieval-Augmented Generation (RAG) pipelines can further enhances LLM performance by adapting to new domains without fine-tuning and providing controllability over grounding the generated outputs while reducing hallucinations. Overall, we have a suite of tools to push the needle in terms of LLM performance without explicitly resorting to fine-tuning.</p><p id="a5f7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fine-tuning comes with its own set of challenges and complications, in terms of labelled data requirements and the costs associated with training of LLMs and their deployment. Fine-tuning may also increase the hallucinations of the LLM in certain cases [3]. Putting this all together, we can see that there is significant value in trying to optimize LLM performance for our task through prompting before resorting to fine-tuning.</p><p id="d679" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, how do we go about this? In this article, we explore Medprompt [1], a sophisticated prompting strategy introduced by Microsoft. Medprompt ties together principles from few-shot prompting, CoT prompting and RAG to enhance the performance of GPT-4 in the healthcare domain without any domain-specific fine-tuning.</p><h2 id="c619" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Table of Contents:</h2><ol class=""><li id="200a" class="nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny oz pa pb bk"><a class="af nc" href="#90ba" rel="noopener ugc nofollow"><strong class="nf fr">MedPrompt Explained</strong></a></li><li id="a947" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk"><a class="af nc" href="#d584" rel="noopener ugc nofollow"><strong class="nf fr">Components of MedPrompt</strong></a></li><li id="0d5e" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk"><a class="af nc" href="#6d10" rel="noopener ugc nofollow"><strong class="nf fr">Implementing MedPrompt</strong></a></li><li id="17f1" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk"><a class="af nc" href="#b207" rel="noopener ugc nofollow"><strong class="nf fr">Evaluating Performance</strong></a></li><li id="d8aa" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk"><a class="af nc" href="#b79b" rel="noopener ugc nofollow"><strong class="nf fr">Conclusion</strong></a></li><li id="417c" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk"><a class="af nc" href="#2755" rel="noopener ugc nofollow"><strong class="nf fr">References</strong></a></li></ol><h1 id="90ba" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">MedPrompt Explained</h1><p id="bcdf" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">LLMs have demonstrated impressive capabilities across various sectors, particularly in healthcare. Last year, Google introduced MedPaLM [4] and MedPaLM-2 [5], LLMs that not only excel in Medical Multiple-Choice Question Answering (MCQA) datasets but also perform competitively and even outperform clinicians in open-ended medical question answering . These models have been tailored specifically for the healthcare domain through instruction fine-tuning and the use of clinician-written Chain-of-Thought templates, significantly enhancing their performance.</p><p id="acda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this context, the paper <strong class="nf fr">“Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine” </strong>[1]<strong class="nf fr"> </strong>from Microsoft raises a compelling question:</p><blockquote class="pz"><p id="ecad" class="qa qb fq bf qc qd qe qf qg qh qi ny dx">Can the performance of a generalist model like GPT-4 be improved for a specific domain without relying on domain-specific fine-tuning or expert-crafted resources?</p></blockquote><p id="f919" class="pw-post-body-paragraph nd ne fq nf b go qj nh ni gr qk nk nl nm ql no np nq qm ns nt nu qn nw nx ny fj bk">As part of this study, the paper introduces <strong class="nf fr">Medprompt</strong>, an innovative prompting strategy that not only improves the model’s performance but also surpasses specialized models such as MedPaLM-2.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/56bd75bc7bca6dbb22de6d88744d8ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I6PsfKnmiNl62g24rOwOvg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Comparison of various LLMs on medical knowledge benchmarks. GPT-4 with Medprompt outperforms Med-PaLM 2 across all these datasets. (Image of Table 1 from the Medprompt paper [1] (<a class="af nc" href="https://arxiv.org/abs/2311.16452" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2311.16452</a>))</figcaption></figure><p id="7663" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GPT-4 with Medprompt outperforms Med-PaLM 2 across all medical MCQA benchmarks without any <strong class="nf fr">domain-specific fine-tuning. </strong>Let’s explore the components in Medprompt.</p><h1 id="d584" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Components of Medprompt</h1><p id="78eb" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Medprompt ties together principles from few-shot prompting, CoT prompting and RAG. Specifically there are 3 components in this pipeline:</p><h2 id="5a69" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Dynamic Few-shot Selection`</h2><p id="5737" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Few-shot prompting refers to utilizing example input-output pairs as context for prompting the LLM. If these few-shot samples are static, the downside is that they may not be the most relevant examples for the new input. <strong class="nf fr">Dynamic Few-shot Selection, </strong>the first component in Medprompt, helps overcome this by selecting the few-shot examples based on each new task input. This method involves training a K-Nearest Neighbors (K-NN) algorithm on the training set, which then retrieves the most similar training set examples to the test input based on cosine similarity in an embedding space. This strategy efficiently utilizes the existing training dataset to retrieve relevant few-shot examples for prompting the LLM.</p><h2 id="1483" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Self-Generated CoT</h2><p id="5e15" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">As noted in the paper [1], CoT traditionally relies on manually crafted few-shot exemplars that include detailed reasoning steps, as used with MedPaLM-2, where such prompts were written by medical professionals. Medprompt introduces <strong class="nf fr">Self-Generated CoT</strong> as the second module, where the LLM is used to produce detailed, step-by-step explanations of its reasoning process, culminating in a final answer choice. By automatically generating CoT reasoning steps for each training datapoint, the need for manually crafted exemplars is bypassed. To ensure that only correct predictions with reasoning steps are retained and incorrect responses are filtered out, the answer generated by GPT-4 is cross-verified against the ground truth.</p><h2 id="8e65" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Choice Shuffling Ensemble</h2><p id="6c0b" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The Choice Shuffling Ensemble technique is the third technique introduced by Medprompt. It is designed to combat the inherent biases that may affect the model’s decision-making process, particularly position bias in multiple-choice settings. The ordering of the answer choices is shuffled, and this process is repeated k times to create k variants of the same question with shuffled answer choices. During inference, each variant is used to generate an answer, and a majority vote is performed over all variants to pick the final predicted option.</p><h2 id="be41" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">How are these components used in the preprocessing and inference stage?</h2><p id="450a" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Let’s now have a look at the preprocessing and inference stages in Medprompt.</p><h2 id="141f" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Preprocessing Stage</h2><p id="7fc6" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In the preprocessing pipeline, we begin by taking each question from the training dataset and incorporating detailed instructions within the prompt to guide the generation of both an answer and its associated reasoning steps. The LLM is prompted to generate the answer and reasoning steps. After obtaining the generated response, we verify its accuracy by comparing the predicted answer to the ground truth for that particular question.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qp ln qq lo qr cf qs cg qt ci bh"><figure class="mm mn mo mp mq mr qv qw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/c963d79a34de678026ba379c9833720a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Tu6fZt7CKWljG7yz8d5YPg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Medprompt Preprocessing Pipeline (Image by Author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ccaa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If the prediction is incorrect, we exclude this instance from our database of relevant questions. If the prediction is correct, we proceed by embedding the question using a text embedding model. We then store the question, question embedding, answer, and Chain of Thought (CoT) reasoning in a buffer. Once all questions have been processed, we utilize the embeddings for training a KNN model. This trained KNN model acts as our retriever in a RAG pipeline, enabling us to efficiently query and retrieve the top-k similar data points based on cosine similarity within the embedding space.</p><p id="09fe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Inference Pipeline</strong></p><p id="d9a0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">During the inference stage, each question from our test set is first embedded using the text embedding model. We then utilize the KNN model to identify the top-k most similar questions. For each retrieved data point, we have access to the self-generated Chain of Thought (CoT) reasoning and the predicted answer. We format these elements — question, CoT reasoning, and answer — into few-shot examples for our eventual prompt.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qp ln qq lo qr cf qs cg qt ci bh"><figure class="mm mn mo mp mq mr qv qw paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/71c38445d50f25f5fca81cb0d6352615.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*lgbxOy0t3Hk1OXXcqn5o-A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Medprompt Inference Pipline (Image by Author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="bd66" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now perform <strong class="nf fr">choice shuffling ensembling</strong> by shuffling the order of answer choices for each test question, creating multiple variants of the same question. The LLM is then prompted with these variants, along with the corresponding few-shot exemplars, to generate reasoning steps and an answer for each variant. Finally, we perform a majority vote over the predictions from all variants and select the final prediction.</p><h1 id="6d10" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Implementing Medprompt</h1><blockquote class="qy qz ra"><p id="ba07" class="nd ne rb nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The code related to this implementation can be found at this <a class="af nc" href="https://github.com/anand-subu/blog_resources/tree/main/medprompt" rel="noopener ugc nofollow" target="_blank">github repo link</a>.</p></blockquote><p id="30bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use the MedQA [6] dataset for implementing and evaluating Medprompt. We first define helper functions for parsing the jsonl files.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="61ac" class="rg oa fq rd b bg rh ri l rj rk">def write_jsonl_file(file_path, dict_list):<br/>    """<br/>    Write a list of dictionaries to a JSON Lines file.<br/><br/>    Args:<br/>    - file_path (str): The path to the file where the data will be written.<br/>    - dict_list (list): A list of dictionaries to write to the file.<br/>    """<br/>    with open(file_path, 'w') as file:<br/>        for dictionary in dict_list:<br/>            json_line = json.dumps(dictionary)<br/>            file.write(json_line + '\n')<br/><br/>def read_jsonl_file(file_path):<br/>    """<br/>    Parses a JSONL (JSON Lines) file and returns a list of dictionaries.<br/><br/>    Args:<br/>        file_path (str): The path to the JSONL file to be read.<br/><br/>    Returns:<br/>        list of dict: A list where each element is a dictionary representing<br/>            a JSON object from the file.<br/>    """<br/>    jsonl_lines = []<br/>    with open(file_path, 'r', encoding="utf-8") as file:<br/>        for line in file:<br/>            json_object = json.loads(line)<br/>            jsonl_lines.append(json_object)<br/>            <br/>    return jsonl_lines</span></pre><h2 id="d223" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Implementing Self-Generated CoT</h2><p id="19c0" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">For our implementation, we utilize the training set from MedQA. We implement a zero-shot CoT prompt and process all the training questions. We use <strong class="nf fr">GPT-4o</strong> in our implementation. For each question, we generate the CoT and the corresponding answer. We define a prompt which is based on the template provided in the Medprompt paper.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="4e44" class="rg oa fq rd b bg rh ri l rj rk">system_prompt = """You are an expert medical professional. You are provided with a medical question with multiple answer choices.<br/>Your goal is to think through the question carefully and explain your reasoning step by step before selecting the final answer.<br/>Respond only with the reasoning steps and answer as specified below.<br/>Below is the format for each question and answer:<br/><br/>Input:<br/>## Question: {{question}}<br/>{{answer_choices}}<br/><br/>Output:<br/>## Answer<br/>(model generated chain of thought explanation)<br/>Therefore, the answer is [final model answer (e.g. A,B,C,D)]"""</span></pre><pre class="rl rc rd re bp rf bb bk"><span id="d4f5" class="rg oa fq rd b bg rh ri l rj rk">def build_zero_shot_prompt(system_prompt, question):<br/>    """<br/>    Builds the zero-shot prompt.<br/><br/>    Args:<br/>        system_prompt (str): Task Instruction for the LLM<br/>        content (dict): The content for which to create a query, formatted as<br/>            required by `create_query`.<br/><br/>    Returns:<br/>        list of dict: A list of messages, including a system message defining<br/>            the task and a user message with the input question.<br/>    """<br/>    messages = [{"role": "system", "content": system_prompt},<br/>                {"role": "user", "content": create_query(question)}]<br/>    return messages<br/><br/><br/>def build_few_shot_prompt(system_prompt, question, examples, include_cot=True):<br/>    """<br/>    Builds the few-shot prompt.<br/><br/>    Args:<br/>        system_prompt (str): Task Instruction for the LLM<br/>        content (dict): The content for which to create a query, formatted as<br/>            required by `create_query`.<br/><br/>    Returns:<br/>        list of dict: A list of messages, including a system message defining<br/>            the task and a user message with the input question.<br/>    """<br/>    messages = [{"role": "system", "content": system_prompt}]<br/>    <br/>    for elem in examples:<br/>        messages.append({"role": "user", "content": create_query(elem)})<br/>        if include_cot:<br/>            messages.append({"role": "assistant", "content": format_answer(elem["cot"], elem["answer_idx"])})        <br/>        else:           <br/>            answer_string = f"""## Answer\nTherefore, the answer is {elem["answer_idx"]}"""<br/>            messages.append({"role": "assistant", "content": answer_string})<br/>            <br/>    messages.append({"role": "user", "content": create_query(question)})<br/>    return messages<br/><br/>def get_response(messages, model_name, temperature = 0.0, max_tokens = 10):<br/>    """<br/>    Obtains the responses/answers of the model through the chat-completions API.<br/><br/>    Args:<br/>        messages (list of dict): The built messages provided to the API.<br/>        model_name (str): Name of the model to access through the API<br/>        temperature (float): A value between 0 and 1 that controls the randomness of the output.<br/>        A temperature value of 0 ideally makes the model pick the most likely token, making the outputs deterministic.<br/>        max_tokens (int): Maximum number of tokens that the model should generate<br/><br/>    Returns:<br/>        str: The response message content from the model.<br/>    """<br/>    response = client.chat.completions.create(<br/>        model=model_name,<br/>        messages=messages,<br/>        temperature=temperature,<br/>        max_tokens=max_tokens<br/>    )<br/>    return response.choices[0].message.content</span></pre><p id="5ce1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We also define helper functions for parsing the reasoning and the final answer option from the LLM response.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="45af" class="rg oa fq rd b bg rh ri l rj rk">def matches_ans_option(s):<br/>    """<br/>    Checks if the string starts with the specific pattern 'Therefore, the answer is [A-Z]'.<br/>    <br/>    Args:<br/>    s (str): The string to be checked.<br/><br/>    Returns:<br/>    bool: True if the string matches the pattern, False otherwise.<br/>    """<br/>    return bool(re.match(r'^Therefore, the answer is [A-Z]', s))<br/><br/>def extract_ans_option(s):<br/>    """<br/>    Extracts the answer option (a single capital letter) from the start of the string.<br/>    <br/>    Args:<br/>    s (str): The string containing the answer pattern.<br/><br/>    Returns:<br/>    str or None: The captured answer option if the pattern is found, otherwise None.<br/>    """<br/>    match = re.search(r'^Therefore, the answer is ([A-Z])', s)<br/>    if match:<br/>        return match.group(1)  # Returns the captured alphabet<br/>    return None <br/><br/>def matches_answer_start(s):<br/>    """<br/>    Checks if the string starts with the markdown header '## Answer'.<br/>    <br/>    Args:<br/>    s (str): The string to be checked.<br/><br/>    Returns:<br/>    bool: True if the string starts with '## Answer', False otherwise.<br/>    """<br/>    return s.startswith("## Answer")<br/><br/>def validate_response(s):<br/>    """<br/>    Validates a multi-line string response that it starts with '## Answer' and ends with the answer pattern.<br/>    <br/>    Args:<br/>    s (str): The multi-line string response to be validated.<br/><br/>    Returns:<br/>    bool: True if the response is valid, False otherwise.<br/>    """<br/>    file_content = s.split("\n")<br/>    <br/>    return matches_ans_option(file_content[-1]) and matches_answer_start(s)<br/><br/>def parse_answer(response):<br/>    """<br/>    Parses a response that starts with '## Answer', extracting the reasoning and the answer choice.<br/>    <br/>    Args:<br/>    response (str): The multi-line string response containing the answer and reasoning.<br/><br/>    Returns:<br/>    tuple: A tuple containing the extracted CoT reasoning and the answer choice.<br/>    """<br/>    split_response = response.split("\n")<br/>    assert split_response[0] == "## Answer"<br/>    cot_reasoning = "\n".join(split_response[1:-1]).strip()<br/>    ans_choice = extract_ans_option(split_response[-1])<br/>    return cot_reasoning, ans_choice</span></pre><p id="a0bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now process the questions in the training set of MedQA. We obtain CoT responses and answers for all questions and store them to a folder.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="03f3" class="rg oa fq rd b bg rh ri l rj rk">train_data = read_jsonl_file("data/phrases_no_exclude_train.jsonl")<br/><br/>cot_responses = []<br/>os.mkdir("cot_responses")<br/><br/>for idx, item in enumerate(tqdm(train_data)):<br/>    prompt = build_zero_shot_prompt(system_prompt, item)<br/>    try:<br/>        response = get_response(prompt, model_name="gpt-4o", max_tokens=500)<br/>        cot_responses.append(response)<br/>        with open(os.path.join("cot_responses", str(idx) + ".txt"), "w", encoding="utf-8") as f:<br/>            f.write(response)           <br/>    except Exception as e :<br/>        print(str(e))<br/>        cot_responses.append("")</span></pre><p id="c42e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now iterate across all the generated responses to check if they are valid and adhere to the prediction format defined in the prompt. We discard responses that do not conform to the required format. After that, we check the predicted answers against the ground truth for each question and only retain questions for which the predicted answers match the ground truth.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="cf68" class="rg oa fq rd b bg rh ri l rj rk">questions_dict = []<br/>ctr = 0<br/>for idx, question in enumerate(tqdm(train_data)):<br/>    file = open(os.path.join("cot_responses/", str(idx) + ".txt"), encoding="utf-8").read()<br/>    if not validate_response(file):<br/>        continue<br/>    <br/>    cot, pred_ans = parse_answer(file)<br/>    <br/>    dict_elem = {}<br/>    dict_elem["idx"] = idx<br/>    dict_elem["question"] = question["question"]<br/>    dict_elem["answer"] = question["answer"]<br/>    dict_elem["options"] = question["options"]<br/>    dict_elem["cot"] = cot<br/>    dict_elem["pred_ans"] = pred_ans<br/>    questions_dict.append(dict_elem)        <br/><br/>filtered_questions_dict = []<br/>for item in tqdm(questions_dict):<br/>    pred_ans = item["options"][item["pred_ans"]]<br/>    if pred_ans == item["answer"]:<br/>        filtered_questions_dict.append(item)</span></pre><h2 id="2422" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Implementing the KNN model</h2><p id="1823" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Having processed the training set and obtained the CoT response for all these questions, we now embed all questions using the <strong class="nf fr">text-embedding-ada-002</strong> from OpenAI.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="4e76" class="rg oa fq rd b bg rh ri l rj rk">def get_embedding(text, model="text-embedding-ada-002"):<br/>    return client.embeddings.create(input = [text], model=model).data[0].embedding<br/><br/>for item in tqdm(filtered_questions_dict):<br/>    item["embedding"] = get_embedding(item["question"])<br/>    inv_options_map = {v:k for k,v in item["options"].items()}<br/>    item["answer_idx"] = inv_options_map[item["answer"]]    </span></pre><p id="ff63" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now train a KNN model using these question embeddings. This acts as a retriever at inference time, as it helps us to retrieve similar datapoints from the training set that are most similar to the question from the test set.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="6961" class="rg oa fq rd b bg rh ri l rj rk">import numpy as np<br/>from sklearn.neighbors import NearestNeighbors<br/><br/>embeddings = np.array([d["embedding"] for d in filtered_questions_dict])<br/>indices = list(range(len(filtered_questions_dict)))<br/><br/>knn = NearestNeighbors(n_neighbors=5, algorithm='auto', metric='cosine').fit(embeddings)</span></pre><h2 id="1df0" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Implementing the Dynamic Few-Shot and Choice Shuffling Ensemble Logic</h2><p id="77d6" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We can now run inference. We subsample 500 questions from the MedQA test set for our evaluation. For each question, we retrieve the 5 most similar questions from the train set using the KNN module, along with their respective CoT reasoning steps and predicted answers. We construct a few-shot prompt using these examples.</p><p id="33a2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For each question, we also shuffle the order of the options 5 times to create different variants. We then utilize the constructed few-shot prompt to get the predicted answer for each of the variants with shuffled options.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="7ed3" class="rg oa fq rd b bg rh ri l rj rk">def shuffle_option_labels(answer_options):<br/>    """<br/>    Shuffles the options of the question.<br/>    <br/>    Parameters:<br/>    answer_options (dict): A dictionary with the options.<br/><br/>    Returns:<br/>    dict: A new dictionary with the shuffled options.<br/>    """<br/>    options = list(answer_options.values())<br/>    random.shuffle(options)<br/>    labels = [chr(i) for i in range(ord('A'), ord('A') + len(options))]<br/>    shuffled_options_dict = {label: option for label, option in zip(labels, options)}<br/>    <br/>    return shuffled_options_dict</span></pre><pre class="rl rc rd re bp rf bb bk"><span id="88d0" class="rg oa fq rd b bg rh ri l rj rk">test_samples = read_jsonl_file("final_processed_test_set_responses_medprompt.jsonl")<br/><br/>for question in tqdm(test_samples, colour ="green"):<br/>    question_variants = []<br/>    prompt_variants = []<br/>    cot_responses = []<br/>    question_embedding = get_embedding(question["question"])<br/>    distances, top_k_indices = knn.kneighbors([question_embedding], n_neighbors=5)<br/>    top_k_dicts = [filtered_questions_dict[i] for i in top_k_indices[0]]<br/>    question["outputs"] = []<br/>    <br/>    for idx in range(5):<br/>        question_copy = question.copy()<br/>        shuffled_options = shuffle_option_labels(question["options"])<br/>        inv_map = {v:k for k,v in shuffled_options.items()}<br/>        <br/>        question_copy["options"] = shuffled_options<br/>        question_copy["answer_idx"] = inv_map[question_copy["answer"]]<br/>        question_variants.append(question_copy)<br/>        prompt = build_few_shot_prompt(system_prompt,  question_copy, top_k_dicts)<br/>        prompt_variants.append(prompt)<br/>    <br/>    for prompt in tqdm(prompt_variants):<br/>        response = get_response(prompt, model_name="gpt-4o", max_tokens=500)<br/>        cot_responses.append(response)<br/>    <br/>    for question_sample, answer in zip(question_variants, cot_responses):<br/>        if validate_response(answer):<br/>            cot, pred_ans = parse_answer(answer)<br/>            <br/>        else:<br/>            cot = ""<br/>            pred_ans = ""<br/>                <br/>        question["outputs"].append({"question": question_sample["question"], "options": question_sample["options"], "cot": cot, "pred_ans": question_sample["options"].get(pred_ans, "")})</span></pre><p id="493f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now evaluate the results of Medprompt over the test set. For each question, we have five predictions generated through the ensemble logic. We take the mode, or most frequently occurring prediction, for each question as the final prediction and evaluate the performance. Two edge cases are possible here:</p><ol class=""><li id="5114" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oz pa pb bk">Two different answer options are predicted two times each, with no clear winner.</li><li id="c904" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk">There is an error with the response generated, meaning that we don’t have a predicted answer option.</li></ol><p id="8828" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For both of these edge cases, we consider the question to be wrongly answered by the LLM.</p><pre class="mm mn mo mp mq rc rd re bp rf bb bk"><span id="9d65" class="rg oa fq rd b bg rh ri l rj rk">def find_mode_string_list(string_list):<br/>    """<br/>    Finds the most frequently occurring strings.<br/><br/>    Parameters:<br/>    string_list (list of str): A list of strings.<br/>    Returns:<br/>    list of str or None: A list containing the most frequent string(s) from the input list.<br/>                         Returns None if the input list is empty.<br/>    """    <br/>    if not string_list:<br/>        return None  <br/><br/>    string_counts = Counter(string_list)<br/>    max_freq = max(string_counts.values())<br/>    mode_strings = [string for string, count in string_counts.items() if count == max_freq]<br/>    return mode_strings<br/><br/>ctr = 0 <br/>for item in test_samples:<br/>    pred_ans = [x["pred_ans"] for x in item["outputs"]]<br/>    freq_ans = find_mode_string_list(pred_ans)<br/>    <br/>    if len(freq_ans) &gt; 1:<br/>        final_prediction = ""<br/>    else:<br/>        final_prediction = freq_ans[0]<br/>        <br/>    if final_prediction == item["answer"]:<br/>        ctr +=1<br/><br/>print(ctr / len(test_samples))</span></pre><h1 id="b207" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Evaluating Performance</h1><p id="465c" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We evaluate the performance of Medprompt with GPT-4o in terms of accuracy on the MedQA test subset. Additionally, we benchmark the performance of Zero-shot prompting, Random Few-Shot prompting, and Random Few-Shot with CoT prompting.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rm"><img src="../Images/5689439bdcd82036f6539f1e4fbd3b44.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*Aey3y_SmH4NbhFLIa0PpCg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Results of our evaluation (Image by Author)</figcaption></figure><p id="15d4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We observe that Medprompt and Random Few-Shot CoT prompting outperform the Zero and Few-Shot prompting baselines. However, surprisingly, we notice that Random Few-Shot CoT outperforms our Medprompt performance. This could be due to a couple of reasons:</p><ol class=""><li id="4ab0" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny oz pa pb bk">The original Medprompt paper benchmarked the performance of GPT-4. We observe that GPT-4o outperforms GPT-4T and GPT-4 on various text benchmarks significantly (<a class="af nc" href="https://openai.com/index/hello-gpt-4o/" rel="noopener ugc nofollow" target="_blank">https://openai.com/index/hello-gpt-4o/</a>), indicating that Medprompt could have a lesser effect on a stronger model like GPT-4o.</li><li id="b180" class="nd ne fq nf b go pc nh ni gr pd nk nl nm pe no np nq pf ns nt nu pg nw nx ny oz pa pb bk">We restrict our evaluation to 500 questions subsampled from MedQA. The Medprompt paper evaluates other Medical MCQA datasets and the full version of MedQA. Evaluating GPT-4o on the complete versions of the datasets could give a better picture of the overall performance.</li></ol><h1 id="b79b" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk">Conclusion</h1><p id="e4d5" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Medprompt is an interesting framework for creating sophisticated prompting pipelines, particularly for adapting a generalist LLM to a specific domain without the need for fine-tuning. It also highlights the considerations involved in deciding between prompting and fine-tuning for various use cases. Exploring how far prompting can be pushed to enhance LLM performance is important, as it offers a resource and cost-efficient alternative to fine-tuning.</p><h1 id="2755" class="ph oa fq bf ob pi pj gq of pk pl gt oj pm pn po pp pq pr ps pt pu pv pw px py bk"><strong class="al">References:</strong></h1><p id="d27f" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">[1] Nori, H., Lee, Y. T., Zhang, S., Carignan, D., Edgar, R., Fusi, N., … &amp; Horvitz, E. (2023). Can generalist foundation models outcompete special-purpose tuning? case study in medicine. <em class="rb">arXiv preprint arXiv:2311.16452</em>. (<a class="af nc" href="https://arxiv.org/abs/2311.16452" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2311.16452</a>)</p><p id="3e28" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., … &amp; Zhou, D. (2022). Chain-of-thought prompting elicits reasoning in large language models. <em class="rb">Advances in Neural Information Processing Systems</em>, <em class="rb">35</em>, 24824–24837. (<a class="af nc" href="https://openreview.net/pdf?id=_VjQlMeSB_J" rel="noopener ugc nofollow" target="_blank">https://openreview.net/pdf?id=_VjQlMeSB_J</a>)</p><p id="14cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Gekhman, Z., Yona, G., Aharoni, R., Eyal, M., Feder, A., Reichart, R., &amp; Herzig, J. (2024). Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?. <em class="rb">arXiv preprint arXiv:2405.05904</em>. (<a class="af nc" href="https://arxiv.org/abs/2405.05904" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2405.05904</a>)</p><p id="d9f2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., … &amp; Natarajan, V. (2023). Large language models encode clinical knowledge. <em class="rb">Nature</em>, <em class="rb">620</em>(7972), 172–180. (<a class="af nc" href="https://www.nature.com/articles/s41586-023-06291-2" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/s41586-023-06291-2</a>)</p><p id="78b3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., … &amp; Natarajan, V. (2023). Towards expert-level medical question answering with large language models. <em class="rb">arXiv preprint arXiv:2305.09617</em>. (<a class="af nc" href="https://arxiv.org/abs/2305.09617" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2305.09617</a>)</p><p id="6271" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[6] Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., &amp; Szolovits, P. (2021). What disease does this patient have? a large-scale open domain question answering dataset from medical exams. <em class="rb">Applied Sciences</em>, <em class="rb">11</em>(14), 6421. (<a class="af nc" href="https://arxiv.org/abs/2009.13081" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2009.13081</a>) (Original dataset is released under a MIT License)</p></div></div></div></div>    
</body>
</html>