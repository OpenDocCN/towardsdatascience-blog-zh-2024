<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Beyond Attention: How Advanced Positional Embedding Methods Improve upon the Original Approach in Transformer Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Beyond Attention: How Advanced Positional Embedding Methods Improve upon the Original Approach in Transformer Architecture</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29">https://towardsdatascience.com/beyond-attention-how-advanced-positional-embedding-methods-improve-upon-the-original-transformers-90380b74d324?source=collection_archive---------3-----------------------#2024-10-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7736" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">From Sinusoidal to RoPE and ALiBi: How advanced positional encodings overcome limitations in Transformers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Elahe Aghapour" class="l ep by dd de cx" src="../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DfA3l4L2kLpNaOAUK9Rb4g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--90380b74d324--------------------------------" rel="noopener follow">Elahe Aghapour</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--90380b74d324--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="85c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Authors: </strong><span class="ia"><span class="ia" aria-hidden="false"><a class="nf ib ng" href="https://medium.com/u/75214fb27311?source=post_page---user_mention--90380b74d324--------------------------------" rel="noopener" target="_blank"><strong class="ml fr">Elahe Aghapour</strong></a></span></span><strong class="ml fr">, </strong><span class="ia"><span class="ia" aria-hidden="false"><a class="nf ib ng" href="https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--90380b74d324--------------------------------" rel="noopener" target="_blank"><strong class="ml fr">Salar Rahili</strong></a></span></span></p><h1 id="9f51" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">Introduction:</h1><p id="9f2c" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">The exponential progress of models built in recent years is deeply connected with the advent of the Transformer architecture. Previously, AI scientists had to select architectures for each task at hand, and then optimize the hyper-parameters to get the best performance out of it. Another challenge limiting their potential was the difficulty in handling long-range dependencies of the data, surfacing the issues of vanishing gradients, loss of context over long sequences, and the inability to capture global context due to locality constraints. Additionally, the lack of scalability and parallelization in traditional models slowed training on large datasets, holding back the progress in the field.</p><p id="a720" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Transformer architecture revolutionized the field by addressing these issues through its self-attention mechanism. It enabled models to capture relationships over long sequences and efficiently understand global context, all while being highly parallelizable and adaptable across various modalities, such as text, images, and more. In the self-attention mechanism, for each token, its query is compared against the keys of all other tokens to compute similarity scores. These similarities are then used to weigh the value vectors, which ultimately decide where the current token should attend to. Self-attention treats all tokens as equally important regardless of their order, losing critical information about the sequence in which tokens appear, and in other words, it sees the input data as a set with no order. Now we need a mechanism to enforce some notion of order on the data, as natural language and many other types of data are inherently sequential and position-sensitive. This is where positional embeddings come into play. Positional embeddings encode the position of each token in the sequence, enabling the model to maintain awareness of the sequence’s structure. Various methods for encoding positional information have been explored, and we will cover them in this blog post.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj ok"><img src="../Images/998a5ddc31efe8bb315ebe9eab6c558c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrahfDXgHHpmX4rTF68Z5g.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Image generated by DALL-E</figcaption></figure><h1 id="97df" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">Attention Mechanism:</strong></h1><p id="0093" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Let <em class="pb">S = {wi}</em> for <em class="pb">i =1,…,N</em> be a sequence of <em class="pb">N</em> input tokens where <em class="pb">wi</em> represents the <em class="pb">i</em>-th token. Hence, the corresponding token embedding of <em class="pb">S</em> can be denoted as <em class="pb">E = {xi}</em> for <em class="pb">i =1,…,N</em> where <em class="pb">xi</em> is the <em class="pb">d</em>-dimensional token embedding vector for token <em class="pb">wi</em>. The self-attention mechanism incorporates position embedding into token embeddings and generates the query, key, and value representations as:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj pc"><img src="../Images/764b45fac9c35b4504c2338f65a6995b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pK6qO-9WFS5tsd6_"/></div></div></figure><p id="4e95" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then, the attention weights is computed based on the similarity between query and key vectors:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj pd"><img src="../Images/6d9f343c6adfcbfcddf0bfb8dc532620.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*Pk7eaZg9APVJQfw5vH_haA.png"/></div></figure><p id="55e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The attention weights determine how important token <em class="pb">n</em> is for token <em class="pb">m</em>. In the other words, how much attention token <em class="pb">m</em> should pay to token <em class="pb">n</em>. The output for token <em class="pb">m</em> is computed as a weighted sum of the value vectors:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj pe"><img src="../Images/5303090e8fe3e310ecf8ee7321a4e822.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*Z_v6PmYbYh9ccNz5H7_vhg.png"/></div></figure><p id="c5d8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Therefore, the attention mechanism token <em class="pb">m</em> to gather information from other tokens in the sequence.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj pf"><img src="../Images/5d1d99e883ab4f86c0d399614800f9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dr5tZ-q_WzrJLJO5"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Fig 1. Positional encoding in transformer architecture (image from <a class="af pg" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">paper</a>).</figcaption></figure><h1 id="9ac4" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">1. Absolute Position Embedding:</strong></h1><p id="ac32" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">A typical choice for the equation (1) is to have:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj ph"><img src="../Images/1009b706a42c879f4af02733f5d9c422.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*SQq_vubqGpf1Ifd5TzJd_g.png"/></div></figure><p id="f997" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where <em class="pb">pi</em> is a <em class="pb">d</em>-dimensional vector, representing the absolute position of token <em class="pb">xi</em>. Sinusoidal positional encoding and learned positional encoding are two alternatives to generate <em class="pb">pi</em>.</p><h2 id="bae2" class="pi ni fq bf nj pj pk pl nm pm pn po np ms pp pq pr mw ps pt pu na pv pw px py bk"><strong class="al">1.a Sinusoidal Positional Encoding</strong></h2><p id="88ae" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Sinusoidal positional encoding was introduced in the “<a class="af pg" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is all you need</a>” paper where transformer architecture was proposed. Sinusoidal Positional Encoding provides a unique position representation for each token in the input sequence. It is based on sine and cosine functions with different frequencies as:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj pz"><img src="../Images/108d3fd539207a2e549a52e501c3172e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*6FFZkHubVibB8gHxMkOcFg.png"/></div></figure><p id="654a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where <em class="pb">pos</em> is the position of the token in the sequence, <em class="pb">d</em> is the position embedding dimension, and i is the dimension index (<em class="pb">0&lt;=i&lt;d</em>).</p><p id="b596" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The use of sine and cosine functions in sinusoidal positional encoding has a deep relationship with the <strong class="ml fr">Fourier transform. </strong>By using a range of different frequencies to encode positions, the Transformer creates a representation similar to a Fourier transform where:</p><ul class=""><li id="b56a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qa qb qc bk"><strong class="ml fr">High-frequency components</strong> (lower <em class="pb">i</em>) enable the model to capture local positional information. This is useful for understanding relationships between neighbor tokens in a sequence, such as word pairs.</li><li id="136d" class="mj mk fq ml b go qd mn mo gr qe mq mr ms qf mu mv mw qg my mz na qh nc nd ne qa qb qc bk"><strong class="ml fr">Low-frequency components</strong> (higher <em class="pb">i</em>) capture more global patterns over the entire sequence. This helps the model to focus on broader relationships between tokens that may be far apart, such as dependencies between words in two different sentences.</li></ul><p id="ca6c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This helps the model understand the relative positions of tokens by comparing their positional encodings. Sinusoidal positional encoding needs no additional training parameters while generalizing to larger sequence lengths at inference time. However, its expressiveness is limited.</p><h2 id="c090" class="pi ni fq bf nj pj pk pl nm pm pn po np ms pp pq pr mw ps pt pu na pv pw px py bk"><strong class="al">1.b Learned Positional Encoding</strong></h2><p id="f290" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Learned positional encoding was introduced in the “<a class="af pg" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention is all you need</a>” paper and it was applied in the <a class="af pg" href="https://arxiv.org/pdf/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT</a> and <a class="af pg" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">GPT</a> models as an alternative to Sinusoidal positional encoding. In learned positional encoding, each position in the sequence (e.g. first token, second token, etc) is assigned an embedding vector. These position embeddings are learned along with other transformer parameters during training. For example, if the model has a context length of 512 with a token embedding of size 768 (i.e. <em class="pb">d</em>=768), a learnable tensor of size 512*768 will be added to the other trainable parameters. This means the model gradually learns the best way to encode positional information for the specific task, such as text classification or translation.</p><p id="aba0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Learned positional embedding is more expressive than sinusoidal one as the model can learn a position embedding, effective for its specific task. However, they introduce more trainable parameters which increases the model size and its computational cost.</p><h1 id="8c55" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">2. Relative Positional Embeddings</strong></h1><p id="f7e8" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Both sinusoidal and learned position encodings focused on the absolute position of the token. However, the attention mechanism works by computing how important other tokens are for each specific token in the sequence. Hence, this process depends on the relative position of the tokens (how far apart they are from each other), rather than the absolute position of the tokens. To address the limitations of absolute position embedding, relative position encoding was introduced.</p><p id="2b04" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pg" href="https://arxiv.org/pdf/1803.02155" rel="noopener ugc nofollow" target="_blank">RelativePosEmb</a> doesn’t add position information to token embeddings. Instead, it modifies the way key and value are computed at every layer as:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qi"><img src="../Images/72e5b379418a7f719d2e2f6566b6c4bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nZQ594cbZWsYKrgr"/></div></div></figure><p id="36f6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here, <em class="pb">r = clip(m-n, Rmin, Rmax)</em> represents the relative distance between position m and n. The maximum relative position is clipped, assuming that precise relative position is not useful beyond a certain distance. Clipping the maximum distance enables the model to extrapolate at inference time, i.e. to generalize to sequence length not seen during training. However, this approach may miss some useful information from the absolute position of the token (like the position of the first token).</p><p id="17ae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You may notice that <em class="pb">fq</em> lacks position embedding. That’s because we are encoding the relative position. In the attention formula, the query and key values are used to compute attention weights as equation (2) therefore we only need either the query or the key to include the relative position embedding.</p><p id="45b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This encoding has been used in many models as <a class="af pg" href="https://arxiv.org/pdf/1901.02860" rel="noopener ugc nofollow" target="_blank">Transformer-XL</a> and <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a>. There are different alternatives in applying relative positional encoding that you can find in papers <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">[7]</a> and <a class="af pg" href="https://arxiv.org/pdf/2006.03654" rel="noopener ugc nofollow" target="_blank">[8]</a> .</p><h1 id="162e" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">3. Rotary Positional Embedding (RoPE)</strong></h1><p id="f8f8" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Unlike previous methods, <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a> rotates the vectors in a multi-dimensional space based on the position of tokens. Instead of adding position information to token embeddings, it modifies the way attention weights are computed at every layer as:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qj"><img src="../Images/2c2c747a497c0ec921bc2dc550a8ac7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TXc6k5zkkt_y3Y_c"/></div></div></figure><p id="f03d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">They proposed a generalized rotation matrix to any even embedding dimensionality <em class="pb">d</em> as:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qk"><img src="../Images/a9b8dc8e3f08c5dbc3709a5c8eef9607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OoNLAqzHk8ObVVdG"/></div></div></figure><p id="82e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where <em class="pb">θi</em> is pre-defined:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj ql"><img src="../Images/e7d44151cb5fc03ca784f80c9665741b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*Bk9hFbx3xpG_jUHi9P6P7w.png"/></div></figure><p id="2d6b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Applying <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a> to attention weight yields to:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj qm"><img src="../Images/6d510a7dbb26896bd120fdcbb45ff781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*-ctz8NVoFIkEfn_HSr0bIw.png"/></div></figure><p id="b365" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a> formulation doesn’t add position information to the values in the attention module. The output of the attention module is a weighted sum of the value vector and since position information isn’t added to values, the outputs of each transformer layer don’t have explicit position details.</p><p id="629a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Popular models such as <a class="af pg" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> and <a class="af pg" href="https://arxiv.org/pdf/2204.06745#cite.su2021roformer" rel="noopener ugc nofollow" target="_blank">GPT-NeoX</a> are using <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a>.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qn"><img src="../Images/042b42f1c09ae586cd90a211c1723857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*09a8Mi2KbKKyyUH5"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Fig 2: <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> method visualization (image from <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">paper</a>).</figcaption></figure><h1 id="856d" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">4. Attention with Linear Biases (ALiBi)</strong></h1><p id="a589" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk"><a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> also does not add positional encodings to word embeddings; instead, it adds a penalty to attention weight scores that is proportional to the distance between tokens. Therefore, the attention score between two tokens i and j at every layer is calculated as:</p><p id="ceaf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pb">Attention score = query_i . key_j — m.(i-j)</em></p><p id="c088" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where <em class="pb">-m.(i-j)</em> is a penalty which is proportional to the distance between token <em class="pb">i </em>and <em class="pb">j</em>. The scalar <em class="pb">m</em> is a head-specific slope fixed before training and its values for different heads are chosen as a geometric sequence. For example, for 8 head, <em class="pb">m</em> might be:</p><figure class="ol om on oo op oq oi oj paragraph-image"><div class="oi oj qo"><img src="../Images/7a0c79a5fea18ba0aabfa8f9b196834a.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*9E6seZL08N5eEu--ZciuGA.png"/></div></figure><p id="bcaf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This means, the first head has a relatively large <em class="pb">m</em> so it penalizes far apart tokens more and focuses on recent tokens, while the 8th head has the smallest <em class="pb">m</em>, allowing it to attend to more distant tokens. Fig. 2 also offers visualization.</p><p id="659c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">ALiBi is used in <a class="af pg" href="https://arxiv.org/pdf/2303.17564" rel="noopener ugc nofollow" target="_blank">BloombergGPT</a> and <a class="af pg" href="https://inria.hal.science/hal-03850124v1/document" rel="noopener ugc nofollow" target="_blank">BLOOM</a>.</p><h1 id="f766" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk"><strong class="al">Transformer Extrapolation At Inference Time:</strong></h1><p id="d667" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">Transformer extrapolation at inference time is the model’s ability to perform well to input sequences that are longer than those it was trained on. The transformer mechanism is agnostic to input length which means at inference time, it can work with longer sequences. However, note that the computational cost grows quadratically with input length even though the transformer layers themselves are agnostic to it.</p><p id="2303" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The authors of <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> demonstrated that the bottleneck for transformer extrapolation is its position embedding method. As shown in Fig. 3, they compared the extrapolation capabilities of different position embedding methods. Since learned position embedding does not have a capability to encode positions greater than the training length, it has no extrapolation ability.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qp"><img src="../Images/463e1fb96f99aff8dbb3dd6fad4ab1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhXRjLXM8ivF5G2TF_7S_g.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Fig 3: Extrapolation: as the input sequence gets longer (x-axis), <a class="af pg" href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">sinusoidal</a>, <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a>, and <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a> position encodings show degraded perplexity (y-axis, lower is better), while <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> does not (image from <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">paper</a>).</figcaption></figure><p id="0de3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Fig. 3 shows that the sinusoidal position embedding in practice has very limited extrapolation capabilities. While <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a> outperforms the sinusoidal one, it still does not achieve satisfactory results. The <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a> bias method (a version of relative position embedding) leads to better extrapolation than both sinusoidal and <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a> embedding. Unfortunately, the <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a> bias is computationally expensive (Fig. 4). <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> outperforms all these position embeddings with negligible (0–0.7%) memory increase.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qq"><img src="../Images/5de14ba673b2dde6cc992ccadda9135d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gMm4ex075P_gojgX1GRuJQ.png"/></div></div><figcaption class="ow ox oy oi oj oz pa bf b bg z dx">Fig. 4: comparison of batched training, inference speed and memory use of <a class="af pg" href="https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">sinusoidal</a>, <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a>, <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a>, and <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a> position encodings (image from <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">paper</a>).</figcaption></figure><h1 id="9220" class="nh ni fq bf nj nk nl gq nm nn no gt np nq nr ns nt nu nv nw nx ny nz oa ob oc bk">Conclusion:</h1><p id="a73c" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">In summary, the way positional information is being encoded in Transformer architecture significantly affects its ability to understand sequential data, especially its extrapolation at inference time. While absolute positional embedding methods provide positional awareness, they often struggle with Transformer extrapolation. That’s why newer position embeddings are proposed. Relative position encoding, RoPE, and ALiBi have the capability to extrapolate at inference time. As transformers continue to be integrated in various applications, refining position encoding is crucial to push the boundaries of their performance.</p><blockquote class="qr qs qt"><p id="1e5c" class="mj mk pb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The opinions expressed in this blog post are solely our own and do not reflect those of our employer.</p></blockquote><h2 id="79c3" class="pi ni fq bf nj pj pk pl nm pm pn po np ms pp pq pr mw ps pt pu na pv pw px py bk"><strong class="al">References:</strong></h2><p id="6bee" class="pw-post-body-paragraph mj mk fq ml b go od mn mo gr oe mq mr ms of mu mv mw og my mz na oh nc nd ne fj bk">[1] Vaswani, A. “Attention is all you need.” (2017).<br/>[2] <a class="af pg" href="https://arxiv.org/pdf/1810.04805" rel="noopener ugc nofollow" target="_blank">BERT</a>: Devlin, Jacob. “Bert: Pre-training of deep bidirectional transformers for language understanding.” (2018).<br/>[3] <a class="af pg" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">GPT</a>: Radford, Alec, et al. “Language models are unsupervised multitask learners.” (2019).<br/>[4] <a class="af pg" href="https://arxiv.org/pdf/1803.02155" rel="noopener ugc nofollow" target="_blank">RelativePosEmb</a>: Shaw, Peter, et al. “Self-attention with relative position representations.” (2018).<br/>[5] <a class="af pg" href="https://arxiv.org/pdf/1901.02860" rel="noopener ugc nofollow" target="_blank">Transformer-XL</a> Dai, Zihang. “Transformer-xl: Attentive language models beyond a fixed-length context.” (2019).<br/>[6] <a class="af pg" href="https://www.jmlr.org/papers/volume21/20-074/20-074.pdf" rel="noopener ugc nofollow" target="_blank">T5</a>: Raffel, Colin, et al. “Exploring the limits of transfer learning with a unified text-to-text transformer.” (2020).<br/>[7] Raffel, Colin, et al. “Exploring the limits of transfer learning with a unified text-to-text transformer.” (2020)<br/>[8] He, Pengcheng, et al. “Deberta: Decoding-enhanced bert with disentangled attention.” (2020).<br/>[9] <a class="af pg" href="https://arxiv.org/pdf/2104.09864" rel="noopener ugc nofollow" target="_blank">RoPE</a>: Su, Jianlin, et al. “Roformer: Enhanced transformer with rotary position embedding.” (2024).<br/>[10] <a class="af pg" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a>: Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.” (2023).<br/>[11] <a class="af pg" href="https://arxiv.org/pdf/2204.06745#cite.su2021roformer" rel="noopener ugc nofollow" target="_blank">GPT-NeoX</a>: Black, Sid, et al. “Gpt-neox-20b: An open-source autoregressive language model.” (2022).<br/>[12] <a class="af pg" href="https://arxiv.org/pdf/2108.12409" rel="noopener ugc nofollow" target="_blank">ALiBi</a>: Press, Ofir, et al. “Train short, test long: Attention with linear biases enables input length extrapolation.” (2021).<br/>[13] <a class="af pg" href="https://arxiv.org/pdf/2303.17564" rel="noopener ugc nofollow" target="_blank">BloombergGPT</a>: Wu, Shijie, et al. “Bloomberggpt: A large language model for finance.” (2023).<br/>[14] <a class="af pg" href="https://inria.hal.science/hal-03850124v1/document" rel="noopener ugc nofollow" target="_blank">BLOOM</a>: Le Scao, Teven, et al. “Bloom: A 176b-parameter open-access multilingual language model.” (2023).</p></div></div></div></div>    
</body>
</html>