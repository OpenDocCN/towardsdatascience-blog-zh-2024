["```py\ndef parse_label_file(label_file_path):\n    \"\"\"\n    KITTI 3D Object Detection Label Fields:\n\n    Each line in the label file corresponds to one object in the scene and contains 15 fields:\n\n    1\\. Type (string):\n    - The type of object (e.g., Car, Van, Truck, Pedestrian, Cyclist, etc.).\n    - \"DontCare\" indicates regions to ignore during training.\n\n    2\\. Truncated (float):\n    - Value between 0 and 1 indicating how truncated the object is.\n    - 0: Fully visible, 1: Completely truncated (partially outside the image).\n\n    3\\. Occluded (integer):\n    - Level of occlusion:\n        0: Fully visible.\n        1: Partly occluded.\n        2: Largely occluded.\n        3: Fully occluded (annotated based on prior knowledge).\n\n    4\\. Alpha (float):\n    - Observation angle of the object in the image plane, ranging from [-π, π].\n    - Encodes the orientation of the object relative to the camera plane.\n\n    5\\. Bounding Box (4 floats):\n    - (xmin, ymin, xmax, ymax) in pixels.\n    - Defines the 2D bounding box in the image plane.\n\n    6\\. Dimensions (3 floats):\n    - (height, width, length) in meters.\n    - Dimensions of the object in the 3D world.\n\n    7\\. Location (3 floats):\n    - (x, y, z) in meters.\n    - 3D coordinates of the object center in the camera coordinate system:\n        - x: Right, y: Down, z: Forward.\n\n    8\\. Rotation_y (float):\n    - Rotation around the Y-axis in camera coordinates, ranging from [-π, π].\n    - Defines the orientation of the object in 3D space.\n\n    9\\. Score (float) [optional]:\n    - Confidence score for detections (used for results, not training).\n\n    Example Line:\n    Car 0.00 0 -1.82 587.00 156.40 615.00 189.50 1.48 1.60 3.69 1.84 1.47 8.41 -1.56\n\nNotes:\n    - \"DontCare\" objects: Regions ignored during training and evaluation. Their bounding boxes can overlap with actual objects.\n    - Camera coordinates: All 3D values are given relative to the camera coordinate system, with the camera at the origin.\n    \"\"\" \n```", "```py\ndef parse_calib_file(calib_file_path):\n    \"\"\"\n        Parses a calibration file to extract and organize key transformation matrices.\n\n        The calibration file contains the following data:\n        - P0, P1, P2, P3: 3x4 projection matrices for the respective cameras.\n        - R0: 3x3 rectification matrix for aligning data points across sensors.\n        - Tr_velo_to_cam: 3x4 transformation matrix from the LiDAR frame to the camera frame.\n        - Tr_imu_to_velo: 3x4 transformation matrix from the IMU frame to the LiDAR frame.\n\n        Parameters:\n        calib_file_path (str): Path to the calibration file.\n\n        Returns:\n        dict: A dictionary where each key corresponds to a calibration parameter \n            (e.g., 'P0', 'R0') and its value is the associated 3x4 NumPy matrix.\n\n        Process:\n        1\\. Reads the calibration file line by line.\n        2\\. Maps each line to its corresponding key ('P0', 'P1', etc.).\n        3\\. Extracts numerical elements, converts them to a NumPy 3x4 matrix, \n        and stores them in a dictionary.\n\n        Example:\n        Input file line for 'P0':\n        P0: 1.0 0.0 0.0 0.0  0.0 1.0 0.0 0.0  0.0 0.0 1.0 0.0\n        Output dictionary:\n        {\n            'P0': [[1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, 0.0]]\n        }\n    \"\"\"\n```", "```py\ndef read_velodyne_bin(file_path):\n    \"\"\"\n    Reads a KITTI Velodyne .bin file and returns the point cloud data as a numpy array.\n\n    :param file_path: Path to the .bin file\n    :return: Numpy array of shape (N, 4) where N is the number of points,\n             and each point has (x, y, z, reflectivity)\n\n    ### For KITTI's Velodyne LiDAR point cloud, the coordinate system used is forward-right-up (FRU).\n    KITTI Coordinate System (FRU):\n        X-axis (Forward): Points in the positive X direction move forward from the sensor.\n        Y-axis (Right): Points in the positive Y direction move to the right of the sensor.\n        Z-axis (Up): Points in the positive Z direction move upward from the sensor.\n\n    ### Units: All coordinates are in meters (m). A point (10, 5, 2) means:\n\n        It is 10 meters forward.\n        5 meters to the right.\n        2 meters above the sensor origin.\n        Reflectivity: The fourth value in KITTI’s .bin files represents the reflectivity or intensity of the LiDAR laser at that point. It is unrelated to the coordinate system but adds extra context for certain tasks like segmentation or object detection.\n\n        Velodyne Sensor Placement:\n\n        The LiDAR sensor is mounted on a vehicle at a specific height and offset relative to the car's reference frame.\n        The point cloud captures objects relative to the sensor’s position.\n\n    \"\"\"\n```", "```py\nprint(f\"Points before downsampling: {len(sample_point_cloud.points)} \")\nsample_point_cloud = sample_point_cloud.voxel_down_sample(voxel_size=0.2)\nprint(f\"Points after downsampling: {len(sample_point_cloud.points)}\")\n```", "```py\nsample_point_cloud = sample_point_cloud.voxel_down_sample(voxel_size=0.2)\n```", "```py\n# 3\\. RANSAC Segmentation to identify the largest plane\nplane_model, inliers = sample_point_cloud.segment_plane(distance_threshold=0.3, ransac_n=3, num_iterations=150)\n\n## Identify inlier points -> road\ninlier_cloud = sample_point_cloud.select_by_index(inliers)\ninlier_cloud.paint_uniform_color([0, 1, 1]) # R, G, B format\n\n## Identify outlier points -> objects on the road\noutlier_cloud = sample_point_cloud.select_by_index(inliers, invert=True)\noutlier_cloud.paint_uniform_color([1, 0, 0]) # R, G, B format\n```", "```py\n# 4\\. Clustering using DBSCAN -> To further segment objects on the road\nwith o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n    labels = np.array(outlier_cloud.cluster_dbscan(eps=0.45, min_points=10, print_progress=True))\n```", "```py\ndef compute_box_3d(obj, Tr_cam_to_velo):\n    \"\"\"\n    Compute the 8 corners of a 3D bounding box in Velodyne coordinates.\n    Args:\n        obj (dict): Object parameters (dimensions, location, rotation_y).\n        Tr_cam_to_velo (np.ndarray): Camera to Velodyne transformation matrix.\n    Returns:\n        np.ndarray: Array of shape (8, 3) with the 3D box corners.\n    \"\"\"\n```", "```py\ndef transform_points(points, transformation):\n    \"\"\"\n    Apply a transformation matrix to 3D points.\n    Args:\n        points (np.ndarray): Nx3 array of 3D points.\n        transformation (np.ndarray): 4x4 transformation matrix.\n    Returns:\n        np.ndarray: Transformed Nx3 points.\n    \"\"\"\n```", "```py\ndef inverse_rigid_trans(Tr):\n    \"\"\"\n    Inverse a rigid body transform matrix (3x4 as [R|t]) to [R'|-R't; 0|1].\n    Args:\n        Tr (np.ndarray): 4x4 transformation matrix.\n    Returns:\n        np.ndarray: Inverted 4x4 transformation matrix.\n    \"\"\"\n```", "```py\ndef compute_iou(box1, box2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) between two bounding boxes.\n    :param box1: open3d.cpu.pybind.geometry.AxisAlignedBoundingBox object for the first box\n    :param box2: open3d.cpu.pybind.geometry.AxisAlignedBoundingBox object for the second box\n    :return: IoU value (float)\n    \"\"\"\n```", "```py\n# Function to evaluate metrics (TP, FP, FN)\ndef evaluate_metrics(ground_truth_boxes, predicted_boxes, iou_threshold=0.5):\n    \"\"\"\n    Evaluate True Positives (TP), False Positives (FP), and False Negatives (FN).\n    :param ground_truth_boxes: List of AxisAlignedBoundingBox objects for ground truth\n    :param predicted_boxes: List of AxisAlignedBoundingBox objects for predictions\n    :param iou_threshold: IoU threshold for a match\n    :return: TP, FP, FN counts\n    \"\"\" \n```"]