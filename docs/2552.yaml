- en: Understanding LLMs from Scratch Using Middle School Math
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始使用中学数学理解 LLM
- en: 原文：[https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876?source=collection_archive---------0-----------------------#2024-10-19](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876?source=collection_archive---------0-----------------------#2024-10-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876?source=collection_archive---------0-----------------------#2024-10-19](https://towardsdatascience.com/understanding-llms-from-scratch-using-middle-school-math-e602d27ec876?source=collection_archive---------0-----------------------#2024-10-19)
- en: A self-contained, full explanation to inner workings of an LLM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一篇完整的、关于 LLM 内部工作原理的自包含解释
- en: '[](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)[![Rohit
    Patel](../Images/29700f417885837857c50ec9db2f7029.png)](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)
    [Rohit Patel](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)[![Rohit
    Patel](../Images/29700f417885837857c50ec9db2f7029.png)](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)
    [Rohit Patel](https://rohit-patel.medium.com/?source=post_page---byline--e602d27ec876--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)
    ·40 min read·Oct 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e602d27ec876--------------------------------)
    ·40 分钟阅读·2024年10月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'In this article, we talk about how Large Language Models (LLMs) work, from
    scratch — assuming only that you know how to add and multiply two numbers. The
    article is meant to be fully self-contained. We start by building a simple Generative
    AI on pen and paper, and then walk through everything we need to have a firm understanding
    of modern LLMs and the Transformer architecture. The article will strip out all
    the fancy language and jargon in ML and represent everything simply as they are:
    numbers. We will still call out what things are called to tether your thoughts
    when you read jargon-y content.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论大型语言模型（LLMs）是如何工作的，从零开始——假设你只知道如何加法和乘法两个数字。本文旨在完全自包含。我们从在纸上构建一个简单的生成式
    AI 开始，然后逐步讲解我们需要了解现代 LLM 和 Transformer 架构的所有内容。文章将去除所有机器学习中的复杂语言和术语，将一切简单呈现为它们本身：数字。我们仍然会指出每个概念的名称，以帮助你在阅读术语较多的内容时保持思路清晰。
- en: Going from addition/multiplication to the most advanced AI models today without
    assuming other knowledge or referring to other sources means we cover a LOT of
    ground. This is NOT a toy LLM explanation — a determined person can theoretically
    recreate a modern LLM from all the information here. I have cut out every word/line
    that was unnecessary and as such this article isn’t really meant to be browsed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从加法/乘法到今天最先进的 AI 模型，而不假设任何其他知识或参考其他来源，这意味着我们涵盖了大量的内容。这不是一个玩具版的 LLM 解释——一个有决心的人理论上可以从所有这些信息中重建一个现代的
    LLM。我已删除了所有不必要的词/行，因此本文并不适合随便浏览。
- en: What will we cover?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们将涵盖哪些内容？
- en: A simple neural network
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个简单的神经网络
- en: How are these models trained?
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些模型是如何训练的？
- en: How does all this generate language?
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一切是如何生成语言的？
- en: What makes LLMs work so well?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么使得 LLM 能够如此高效地工作？
- en: Embeddings
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入
- en: Sub-word tokenizers
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子词分词器
- en: Self-attention
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力
- en: Softmax
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax
- en: Residual connections
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Layer Normalization
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层归一化
- en: Dropout
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）
- en: Multi-head attention
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Positional embeddings
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: The GPT architecture
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT 架构
- en: The transformer architecture
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: Let’s dive in.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: The first thing to note is that neural networks can only take numbers as inputs
    and can only output numbers. No exceptions. The art is in figuring out how to
    feed your inputs as numbers, interpreting the output numbers in a way that achieves
    your goals. And finally, building neural nets that will take the inputs you provide
    and give you the outputs you want (given the interpretation you chose for these
    outputs). Let’s walk through how we get from adding and multiplying numbers to
    things like [Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，神经网络只能接受数字作为输入，并且只能输出数字。没有例外。关键在于如何将输入作为数字馈送到网络中，以一种实现目标的方式解释输出数字。最后，构建神经网络，使其接受您提供的输入并给出您想要的输出（考虑到您为这些输出选择的解释）。让我们看看如何从加法和乘法数字到像[Llama
    3.1](https://ai.meta.com/blog/meta-llama-3-1/)这样的事物。
- en: 'A simple neural network:'
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的神经网络：
- en: 'Let’s work through a simple neural network that can classify an object:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的可以对对象进行分类的神经网络：
- en: '**Object data available:** Dominant color (RGB) & Volume (in milli-liters)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用的对象数据：** 主要颜色（RGB）和体积（毫升）'
- en: '**Classify into**: Leaf OR Flower'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类为：** 叶子 或 花朵'
- en: 'Here’s what the data for a leaf and a sunflower can look like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是叶子和向日葵的数据示例：
- en: '![](../Images/89473398aab0b9444ce4fab8ae68fe69.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89473398aab0b9444ce4fab8ae68fe69.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Let’s now build a neural net that does this classification. We need to decide
    on input/output interpretations. Our inputs are already numbers, so we can feed
    them directly into the network. Our outputs are two objects, leaf and flower which
    the neural network cannot output. Let’s look at a couple of schemes we can use
    here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个可以进行这种分类的神经网络。我们需要决定输入/输出的解释。我们的输入已经是数字，所以我们可以直接将它们馈送到网络中。我们的输出是两个对象，叶子和花朵，神经网络无法直接输出。让我们看看我们可以在这里使用的几种方案：
- en: We can make the network output a single number. And if the number is positive
    we say it’s a leaf and if it is negative we say it’s a flower
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使网络输出一个数字。如果这个数字是正的，我们说它是叶子，如果是负的，我们说它是花朵。
- en: OR, we can make the network output two numbers. We interpret the first one as
    a number for leaf and second one as the number for flower and we will say that
    the selection is whichever number is larger
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，我们可以使网络输出两个数字。我们将第一个解释为叶子的数字，第二个解释为花朵的数字，我们将选择较大的数字作为选择。
- en: 'Both schemes allow the network to output number(s) that we can interpret as
    leaf or flower. Let’s pick the second scheme here because it generalizes well
    to other things we will look at later. And here’s a neural network that does the
    classification using this scheme. Let’s work through it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方案都允许网络输出我们可以解释为叶子或花朵的数字。让我们选择第二种方案，因为它对我们稍后将要查看的其他事物具有很好的泛化能力。这里是一个使用这种方案进行分类的神经网络。让我们逐步进行：
- en: '![](../Images/fe8ecca672520f0a10f6c1ec1eb0db94.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe8ecca672520f0a10f6c1ec1eb0db94.png)'
- en: Image by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Some jargon:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些行话：
- en: '***Neurons/nodes***: The numbers in the circles'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '***神经元/节点***：圆圈中的数字'
- en: '***Weights***: The colored numbers on the lines'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '***权重***：线上的有色数字'
- en: '***Layers***: A collection of neurons is called a layer. You could think of
    this network as having 3 layers: Input layer with 4 neurons, Middle layer with
    3 neurons, and the Output layer with 2 neurons.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '***层***：一组神经元称为一层。你可以将这个网络看作有3层：具有4个神经元的输入层，具有3个神经元的中间层，以及具有2个神经元的输出层。'
- en: To calculate the prediction/output from this network (called a “**forward pass**”),
    you start from the left. We have the data available for the neurons in the Input
    layer. To move “forward” to the next layer, you multiply the number in the circle
    with the weight for the corresponding neuron pairing and you add them all up.
    We demonstrate blue and orange circle math above. Running the whole network we
    see that the first number in the output layer comes out higher so we interpret
    it as “network classified these (RGB,Vol) values as leaf”. A well trained network
    can take various inputs for (RGB,Vol) and correctly classify the object.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算这个网络的预测/输出（称为“**前向传播**”），你从左边开始。我们有输入层神经元的数据可用。要向“前”移动到下一层，你将圆圈中的数字与相应神经元配对的权重相乘，然后将它们全部加起来。我们在上面展示了蓝色和橙色圆圈的数学运算。运行整个网络，我们看到输出层中的第一个数字较高，因此我们将其解释为“网络将这些（RGB，Vol）值分类为叶子”。一个训练良好的网络可以接受各种（RGB，Vol）的输入，并正确地对对象进行分类。
- en: The model has no notion of what a leaf or a flower is, or what (RGB,Vol) are.
    It has a job of taking in exactly 4 numbers and giving out exactly 2 numbers.
    It is our interpretation that the 4 input numbers are (RGB,Vol) and it is also
    our decision to look at the output numbers and infer that if the first number
    is larger it’s a leaf and so on. And finally, it is also up to us to choose the
    right weights such that the model will take our input numbers and give us the
    right two numbers such that when we interpret them we get the interpretation we
    want.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型不知道叶子或花是什么，也不知道（RGB，Vol）是什么。它的任务是接收确切的4个数字并输出确切的2个数字。我们的解释是，这4个输入数字是（RGB，Vol），我们也决定查看输出数字并推断，如果第一个数字较大，则是叶子等。最后，我们还可以选择正确的权重，使模型接受我们的输入数字并给出我们需要的正确两个数字，这样当我们解释它们时，我们得到我们想要的解释。
- en: An interesting side effect of this is that you can take the same network and
    instead of feeding RGB,Vol feed other 4 numbers like cloud cover, humidity etc..
    and interpret the two numbers as “Sunny in an hour” or “Rainy in an hour” and
    then if you have the weights well calibrated you can get the exact same network
    to do two things at the same time — classify leaf/flower and predict rain in an
    hour! The network just gives you two numbers, whether you interpret it as classification
    or prediction or something else is entirely up to you.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的一个有趣的副作用是，你可以拿同样的网络，而不是输入RGB、Vol，而是输入其他4个数字，比如云量、湿度等，然后将这两个数字解释为“一个小时后晴天”或“一个小时后下雨”，如果你的权重校准得当，你可以让同一个网络同时做两件事情——分类叶子/花朵和预测一个小时后的降雨情况！网络只会给你两个数字，你如何解释它，是分类还是预测或其他什么，完全取决于你。
- en: 'Stuff left out for simplification (feel free to ignore without compromising
    comprehensibility):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 简化的内容已被省略（可以忽略而不影响理解）：
- en: '**Activation layer**: A critical thing missing from this network is an “activation
    layer”. That’s a fancy word for saying that we take the number in each circle
    and apply a nonlinear function to it (**RELU** is a common function where you
    just take the number and set it to zero if it is negative, and leave it unchanged
    if it is positive). So basically in our case above, we would take the middle layer
    and replace the two numbers (-26.6 and -47.1) with zeros before we proceed further
    to the next layer. Of course, we would have to re-train the weights here to make
    the network useful again. Without the activation layer all the additions and multiplications
    in the network can be collapsed to a single layer. In our case, you could write
    the green circle as the sum of RGB directly with some weights and you would not
    need the middle layer. It would be something like (0.10 * -0.17 + 0.12 * 0.39–0.36
    * 0.1) * R + (-0.29 * -0.17–0.05 * 0.39–0.21 * 0.1) * G …and so on. This is usually
    not possible if we have a nonlinearity there. This helps networks deal with more
    complex situations.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活层**：这个网络缺少的一个关键部分是“激活层”。这是一个花哨的说法，意思是我们取每个圆圈中的数字并对其应用一个非线性函数（**RELU**是一个常见的函数，如果数字为负数，则将其设为零，如果为正数则保持不变）。所以基本上在我们上面的例子中，我们会取中间层并在继续到下一层之前用零替换这两个数字（-26.6和-47.1）。当然，我们必须重新训练这里的权重，使网络再次变得有用。没有激活层，网络中的所有加法和乘法可以折叠成一个单一层。在我们的情况下，你可以将绿色圆圈写成RGB的和直接乘以一些权重，而不需要中间层。它会类似于（0.10
    * -0.17 + 0.12 * 0.39–0.36 * 0.1）* R + (-0.29 * -0.17–0.05 * 0.39–0.21 * 0.1)
    * G …等等。如果我们在那里没有非线性，通常是不可能的。这有助于网络处理更复杂的情况。'
- en: '**Bias:** Networks will usually also contain another number associated with
    each node, this number is simply added to the product to calculate the value of
    the node and this number is called the “bias”. So if the bias for the top blue
    node was 0.25 then the value in the node would be: (32 * 0.10) + (107 * -0.29)
    + (56 * -0.07) + (11.2 * 0.46) **+ 0.25** = — 26.35\. The word parameters is usually
    used to refer to all these numbers in the model that are not neurons/nodes.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**：网络通常还会包含与每个节点相关的另一个数字，这个数字简单地添加到计算节点值的乘积中，这个数字称为“偏差”。因此，如果顶部蓝色节点的偏差为0.25，则节点中的值将是：（32
    * 0.10）+（107 * -0.29）+（56 * -0.07）+（11.2 * 0.46）**+ 0.25** = — 26.35。参数这个词通常用来指代模型中所有这些不是神经元/节点的数字。'
- en: '**Softmax:** We don’t usually interpret the output layer directly as shown
    in our models. We convert the numbers into probabilities (i.e. make it so that
    all numbers are positive and add up to 1). If all the numbers in the output layer
    were already positive one way you could achieve this is by dividing each number
    by the sum of all numbers in the output layer. Though a “softmax” function is
    normally used which can handle both positive and negative numbers.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax：**我们通常不会像在我们的模型中那样直接解释输出层的值。我们将数字转换为概率（即使所有数字都是正数并且加起来等于1）。如果输出层中的所有数字已经是正数，一种方法是将每个数字除以输出层所有数字的总和。尽管通常会使用“softmax”函数，它可以处理正数和负数。'
- en: How are these models trained?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这些模型是如何训练的？
- en: In the example above, we magically had the weights that allowed us to put data
    into the model and get a good output. But how are these weights determined? The
    process of setting these weights (or “parameters”) is called “**training the model**”,
    and we need some training data to train the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们神奇地得到了使得数据能够输入模型并获得良好输出的权重。但是这些权重是如何确定的呢？设置这些权重（或“参数”）的过程叫做“**训练模型**”，而我们需要一些训练数据来训练模型。
- en: Let’s say we have some data where we have the inputs and we already know if
    each input corresponds to leaf or flower, this is our “**training data**” and
    since we have the leaf/flower label for each set of (R,G,B,Vol) numbers, this
    is “**labeled data**”.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些数据，其中包含输入数据，并且我们已经知道每个输入是否对应叶子或花朵，这就是我们的“**训练数据**”，而由于我们对每一组（R, G, B,
    Vol）数据都知道是叶子还是花朵的标签，这就是“**标注数据**”。
- en: 'Here’s how it works:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是其工作原理：
- en: Start with a random numbers, i.e. set each parameter/weight to a random number
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随机数开始，即将每个参数/权重设置为随机数。
- en: Now, we know that when we input the data corresponding to the leaf (R=32, G=107,
    B=56, Vol=11.2). Suppose we want a larger number for leaf in the output layer.
    Let’s say we want the number corresponding to leaf as 0.8 and the one corresponding
    to flower as 0.2 (as shown in example above, but these are illustrative numbers
    to demonstrate training, in reality we would not want 0.8 and 0.2\. In reality
    these would be probabilities, which they are not here, and we would them to be
    1 and 0)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，我们知道当输入对应叶子时（R=32, G=107, B=56, Vol=11.2）。假设我们希望在输出层中叶子对应的数字更大。我们假设叶子的数字应为0.8，花朵的数字应为0.2（如上例所示，但这些数字只是为了演示训练，实际上我们不会希望是0.8和0.2，实际情况是这些数字应该是概率值，而不是这里的值，实际上我们会希望它们分别为1和0）。
- en: 'We know the numbers we want in the output layer, and the numbers we are getting
    from the randomly selected parameters (which are different from what we want).
    So for all the neurons in the output layer, let’s take the difference between
    the number we want and the number we have. Then add up the differences. E.g.,
    if the output layer is 0.6 and 0.4 in the two neurons, then we get: (0.8–0.6)=0.2
    and (0.2–0.4)= -0.2 so we get a total of 0.4 (ignoring minus signs before adding).
    We can call this our “**loss**”. Ideally we want the loss to be close to zero,
    i.e. we want to “**minimize the loss**”.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道输出层中我们想要的数字，以及我们从随机选择的参数中得到的数字（这些数字与我们想要的不同）。因此，对于输出层中的所有神经元，我们计算想要的数字和得到的数字之间的差异。然后将这些差异加总。例如，如果输出层中有两个神经元分别为0.6和0.4，那么我们得到：(0.8–0.6)=0.2
    和 (0.2–0.4)=-0.2，总和为0.4（忽略负号后相加）。我们可以称之为我们的“**损失**”。理想情况下，我们希望损失接近零，即我们想要“**最小化损失**”。
- en: Once we have the loss, we can slightly change each parameter to see if increasing
    or decreasing it will increase the loss. This is called the “**gradient**” of
    that parameter. Then we can move each of the parameters by a small amount in the
    direction where the loss goes down (opposite the direction of the gradient). Once
    we have moved all the parameters slightly, the loss should be lower
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们得到损失值，我们可以稍微调整每个参数，看看增加或减少它是否会增加损失。这被称为该参数的“**梯度**”。然后我们可以稍微调整每个参数，朝着损失减小的方向（即梯度的反方向）移动。一旦我们轻微调整了所有参数，损失应该会降低。
- en: Keep repeating the process and you will reduce the loss, and eventually have
    a set of weights/parameters that are “**trained**”. This whole process is called
    “**gradient descent**”.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不断重复这个过程，你会减少损失，最终得到一组“**训练好的**”权重/参数。整个过程被称为“**梯度下降**”。
- en: 'Couple of notes:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 几个备注：
- en: You often have multiple training examples, so when you change the weights slightly
    to minimize the loss for one example it might make the loss worse for another
    example. The way to deal with this is to define loss as average loss over all
    the examples and then take gradient over that average loss. This reduces the average
    loss over the entire training data set. Each such cycle is called an “**epoch**”.
    Then you can keep repeating the epochs thus finding weights that reduce average
    loss.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常你会有多个训练样本，因此当你稍微调整权重以最小化一个样本的损失时，它可能会使另一个样本的损失变得更糟。解决这个问题的方法是将损失定义为所有样本的平均损失，然后对该平均损失进行梯度计算。这样可以减少整个训练数据集的平均损失。每一个这样的周期称为“**轮次**”。然后你可以不断重复这些轮次，从而找到能减少平均损失的权重。
- en: We don’t actually need to “move weights around” to calculate the gradient for
    each weight — we can just infer it from the formula (e.g. if the weight is 0.17
    in the last step, and the value of neuron is positive, and we want a larger number
    in output we can see that increasing this number to 0.18 will help).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们实际上不需要“调整权重”来计算每个权重的梯度——我们可以从公式中推导出它（例如，如果上一步的权重是0.17，且神经元的值为正，我们希望输出的数值更大，那么我们可以看到将这个数字增加到0.18会有所帮助）。
- en: In practice, training deep networks is a hard and complex process because gradients
    can easily spiral out of control, going to zero or infinity during training (called
    “vanishing gradient” and “exploding gradient” problems). The simple definition
    of loss that we talked about here is perfectly valid, but rarely used as there
    are better functional forms that work well for specific purposes. With modern
    models containing billions of parameters, training a model requires massive compute
    resources which has its own problems (memory limitations, parallelization etc.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，训练深度网络是一个困难且复杂的过程，因为梯度在训练过程中很容易失控，可能会变为零或无穷大（这被称为“消失梯度”和“爆炸梯度”问题）。我们在这里讨论的简单损失定义是完全有效的，但很少使用，因为有更好的功能形式可以很好地解决特定目的。随着现代模型包含数十亿个参数，训练一个模型需要巨大的计算资源，这也带来了自身的问题（如内存限制、并行化等）。
- en: How does all this help generate language?
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所有这些如何帮助生成语言呢？
- en: Remember, neural nets take in some numbers, do some math based on the trained
    parameters, and give out some other numbers. Everything is about interpretation
    and training the parameters (i.e. setting them to some numbers). If we can interpret
    the two numbers as “leaf/flower” or “rain or sun in an hour”, we can also interpret
    them as “next character in a sentence”.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，神经网络接受一些数字，基于训练好的参数进行一些数学运算，然后输出一些其他的数字。一切都与解释和训练参数（即将它们设置为某些数字）有关。如果我们能将这两个数字解释为“叶子/花”或“一小时后的雨或晴”，我们也可以将它们解释为“句子中的下一个字符”。
- en: But there are more than 2 letters in English, and so we must expand the number
    of neurons in the output layer to, say, the 26 letters in the English language
    (let’s also throw in some symbols like space, period etc..). Each neuron can correspond
    to a character and we look at the (26 or so) neurons in the output layer and say
    that the character corresponding to the highest numbered neuron in the output
    layer is the output character. Now we have a network that can take some inputs
    and output a character.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但英语中不仅仅有两个字母，因此我们必须将输出层的神经元数量扩展到英语中的26个字母（我们还可以加入一些符号，如空格、句号等）。每个神经元可以对应一个字符，我们查看输出层中大约26个神经元，并说输出层中对应最大编号的神经元所代表的字符就是输出字符。现在我们有了一个可以接受一些输入并输出字符的网络。
- en: 'What if we replace the input in our network with these characters: “Humpty
    Dumpt” and asked it to output a character and interpreted it as the “Network’s
    suggestion of the next character in the sequence that we just entered”. We can
    probably set the weights well enough for it to output “y” — thereby completing
    “Humpty Dumpty”. Except for one problem, how do we input these lists of characters
    in the network? Our network only accepts numbers!!'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用这些字符替换网络中的输入：“Humpty Dumpt”，并要求它输出一个字符并将其解释为“网络对我们刚输入的序列中下一个字符的建议”，我们可能可以将权重设置得足够好，使其输出“y”——从而完成“Humpty
    Dumpty”。除了一个问题，我们如何将这些字符列表输入到网络中呢？我们的网络只接受数字！！
- en: 'One simple solution is to assign a number to each character. Let’s say a=1,
    b=2 and so on. Now we can input “humpty dumpt” and train it to give us “y”. Our
    network looks something like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的解决方案是为每个字符分配一个数字。假设a=1，b=2，依此类推。现在我们可以输入“humpty dumpt”，并训练它给出“y”。我们的网络大概是这样的：
- en: '![](../Images/9d58e4a7c1b0340b203e6400645199b2.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d58e4a7c1b0340b203e6400645199b2.png)'
- en: Image by author
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: Ok, so now we can predict one character ahead by providing the network a list
    of characters. We can use this fact to build a whole sentence. For example, once
    we have the “y” predicted, we can append that “y” to the list of characters we
    have and feed it to the network and ask it to predict the next character. And
    if well trained it should give us a space, and so on and so forth. By the end,
    we should be able to recursively generate “Humpty Dumpty sat on a wall”. We have
    Generative AI. Moreover, ***we now have a network capable of generating language!***Now,
    nobody ever actually puts in randomly assigned numbers and we will see more sensible
    schemes down the line. If you cannot wait, feel free to check out the one-hot
    encoding section in the appendix.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以通过给网络输入一个字符列表来预测下一个字符。我们可以利用这个事实来构建一个完整的句子。例如，一旦我们预测出了“y”，我们就可以把这个“y”加到我们已有的字符列表中，再次输入到网络中并让它预测下一个字符。如果训练得当，它应该给我们一个空格，依此类推。最终，我们应该能够递归地生成“Humpty
    Dumpty sat on a wall”。这就是生成式 AI。而且，***我们现在有了一个能够生成语言的网络！***现在，没有人会真正地输入随机分配的数字，我们将看到更为合理的编码方案。如果你迫不及待，欢迎查看附录中的独热编码部分。
- en: 'Astute readers will note that we can’t actually input “Humpty Dumpty” into
    the network since the way the diagram is, it only has 12 neurons in the input
    layer one for each character in “humpty dumpt” (including the space). So how can
    we put in the “y” for the next pass. Putting a 13th neuron there would require
    us to modify the entire network, that’s not workable. The solution is simple,
    let’s kick the “h” out and send the 12 most recent characters. So we would be
    sending “umpty dumpty” and the network will predict a space. Then we would input
    “mpty dumpty “ and it will produce an s and so on. It looks something like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 精明的读者会注意到，我们实际上无法将“Humpty Dumpty”输入到网络中，因为根据图示，输入层只有12个神经元，每个神经元对应“humpty dumpt”中的一个字符（包括空格）。那么，我们该如何输入下一个“y”呢？如果放一个第13个神经元，我们就需要修改整个网络，这是不可行的。解决方案很简单，让我们把“h”去掉，输入最近的12个字符。所以我们将输入“umpty
    dumpty”，网络会预测一个空格。然后我们再输入“mpty dumpty ”，它会输出一个“s”，依此类推。它看起来像这样：
- en: '![](../Images/51afc7bd077f28faeb73783c6191bcbb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51afc7bd077f28faeb73783c6191bcbb.png)'
- en: Image by author
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We’re throwing away a lot of information in the last line by feeding the model
    only “ sat on the wal”. So what do the latest and greatest networks of today do?
    More or less exactly that. The length of inputs we can put into a network is fixed
    (determined by the size of the input layer). This is called “context length” —
    the context that is provided to the network to make future predictions. Modern
    networks can have very large context lengths (several thousand words) and that
    helps. There are some ways of inputting infinite length sequences but the performance
    of those methods, while impressive, has since been surpassed by other models with
    large (but fixed) context length.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过只给模型输入“ sat on the wal”，我们在最后一行丢失了很多信息。那么，今天最新最强大的网络是如何做的呢？或多或少，正是这样。我们能够输入到网络中的输入长度是固定的（由输入层的大小决定）。这叫做“上下文长度”——即提供给网络的上下文，用于进行未来的预测。现代网络可以有非常大的上下文长度（几千个单词），这有助于提升效果。虽然有一些方法可以输入无限长度的序列，但这些方法的表现虽然令人印象深刻，但后来被其他具有大（但固定）上下文长度的模型所超越。
- en: One other thing careful readers will notice is that we have different interpretations
    for inputs and outputs for the same letters! For example, when inputting “h” we
    are simply denoting it with the number 8 but on the output layer we are not asking
    the model to output a single number (8 for “h”, 9 for “i” and so on..) instead
    we are are asking the model to output 26 numbers and then we see which one is
    the highest and then if the 8th number is highest we interpret the output as “h”.
    Why don’t we use the same, consistent, interpretation on both ends? We could,
    it’s just that in the case of language, freeing yourself to choose between different
    interpretations gives you a better chance of building better models. And it just
    so happens that the most effective currently known interpretations for the input
    and output are different. In-fact, the way we are inputting numbers in this model
    is not the best way to do it, we will look at better ways to do that shortly.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，细心的读者会注意到，我们对同一个字母的输入和输出有不同的解释！例如，当输入“h”时，我们只是用数字8表示它，但在输出层，我们并不是让模型输出一个单一的数字（“h”是8，“i”是9，依此类推），而是让模型输出26个数字，然后查看哪个数字最大，如果第8个数字最大，我们就把输出解释为“h”。为什么我们不在两端使用相同的一致解释呢？我们可以这样做，只是对于语言来说，允许自己在不同解释之间选择，会给你更大的机会去构建更好的模型。恰好目前已知最有效的输入和输出解释是不同的。事实上，我们目前在模型中输入数字的方式并不是最好的做法，我们很快就会讨论更好的方法。
- en: What makes large language models work so well?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是什么让大型语言模型如此高效？
- en: 'Generating “Humpty Dumpty sat on a wall” character-by-character is a far cry
    from what modern LLMs can do. There are a number of differences and innovations
    that get us from the simple generative AI that we discussed above to the human-like
    bot. Let’s go through them:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 逐个字符生成“Humpty Dumpty sat on a wall”与现代大型语言模型所能做到的差距很大。从我们上面讨论的简单生成式人工智能，到类人机器人的这一过程中，有许多差异和创新。让我们一一梳理：
- en: Embeddings
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入
- en: 'Remember we said that the way that we are inputting characters into the model
    isn’t the best way to do it. We just arbitrarily selected a number for each character.
    What if there were better numbers we could assign that would make it possible
    for us to train better networks? How do we find these better numbers? Here’s a
    clever trick:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们提到过，我们将字符输入模型的方式并不是最好的方法。我们只是随意地为每个字符选择了一个数字。如果我们能分配更好的数字来帮助我们训练出更好的网络，那会怎么样呢？我们如何找到这些更好的数字？这里有一个巧妙的技巧：
- en: 'When we trained the models above, the way we did it was by moving around weights
    and seeing that gives us a smaller loss in the end. And then slowly and recursively
    changing the weights. At each turn we would:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练上面提到的模型时，我们的做法是通过调整权重并观察最终能带来更小的损失值，然后不断递归地改变权重。在每次调整时，我们会：
- en: Feed in the inputs
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据
- en: Calculate the output layer
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算输出层
- en: Compare it to the output we ideally want and calculate the average loss
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与我们理想中想要的输出进行比较，并计算平均损失
- en: Adjust the weights and start again
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整权重并重新开始
- en: In this process, the inputs are fixed. This made sense when inputs were (RGB,
    Vol). But the numbers we are putting in now for a,b,c etc.. are arbitrarily picked
    by us. What if at every iteration in addition to moving the weights around by
    a bit we also moved the input around and see if we can get a lower loss by using
    a different number to represent “a” and so on? We are definitely reducing the
    loss and making the model better (that’s the direction we moved a’s input in,
    by design). Basically, apply gradient descent not just to the weights but also
    the number representations for the inputs since they are arbitrarily picked numbers
    anyway. This is called an “**embedding**”. It is a mapping of inputs to numbers,
    and as you just saw, it needs to be trained. The process of training an embedding
    is much like that of training a parameter. One big advantage of this though is
    that once you train an embedding you can use it in another model if you wish.
    Keep in mind that you will consistently use the same embedding to represent a
    single token/character/word.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，输入是固定的。当输入是（RGB，Vol）时这是有意义的。但是我们现在放入的数字a、b、c等等是我们随意选择的。如果在每次迭代中，除了稍微调整权重外，我们还移动输入并看看是否可以通过使用不同的数字来表示“a”等等来获得更低的损失？我们肯定在减少损失并使模型更好（这是我们设计的a的输入移动的方向）。基本上，不仅对权重应用梯度下降，还对输入的数字表示应用梯度下降，因为它们无论如何都是任意选择的数字。这被称为“**嵌入**”。这是将输入映射到数字的过程，正如你刚刚看到的，它需要被训练。训练嵌入的过程很像训练参数的过程。不过，其中一个很大的优势是，一旦你训练了一个嵌入，你可以在另一个模型中使用它。请记住，你将始终使用相同的嵌入来表示单个标记/字符/单词。
- en: We talked about embeddings that are just one number per character. However,
    in reality embeddings have more than one number. That’s because it is hard to
    capture the richness of concept by a single number. If we look at our leaf and
    flower example, we have four numbers for each object (the size of the input layer).
    Each of these four numbers conveyed a property and the model was able to use all
    of them to effectively guess the object. If we had only one number, say the red
    channel of the color, it might have been a lot harder for the model. We’re trying
    to capture human language here — we’re going to need more than one number.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们谈到的嵌入只是每个字符一个数字。然而，在现实中，嵌入不止一个数字。这是因为用一个数字很难捕捉概念的丰富性。如果我们看看我们的叶子和花朵的例子，我们为每个对象有四个数字（输入层的大小）。这四个数字中的每一个传达了一个属性，模型能够有效地使用所有这些数字来猜测对象。如果我们只有一个数字，比如颜色的红色通道，对于模型来说可能会更难。我们正在捕捉人类语言
    —— 我们需要不止一个数字。
- en: 'So instead of representing each character by a single number, maybe we can
    represent it by multiple numbers to capture the richness? Let’s assign a bunch
    of numbers to each character. Let’s call an ordered collection of numbers a “vector”
    (ordered as in each number has a position, and if we swap position of two numbers
    it gives us a different vector. This was the case with our leaf/flower data, if
    we swapped the R and G numbers for the leaf, we would get a different color, it
    would not be the same vector anymore). The length of a vector is simply how many
    numbers it contains. We’ll assign a vector to each character. Two questions arise:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，与其用一个数字代表每个字符，也许我们可以用多个数字来捕捉丰富性？让我们为每个字符分配一堆数字。让我们把一组有序的数字称为“向量”（有序的意思是每个数字都有一个位置，如果我们交换两个数字的位置，就会得到一个不同的向量。这在我们的叶子/花朵数据中是这样的，如果我们交换叶子的R和G数字，我们会得到一个不同的颜色，它不再是同一个向量了）。向量的长度就是它包含的数字数量。我们将为每个字符分配一个向量。有两个问题出现：
- en: If we have a vector assigned to each character instead of a number, how do we
    now feed “humpty dumpt” to the network? The answer is simple. Let’s say we assigned
    a vector of 10 numbers to each character. Then instead of the input layer having
    12 neurons we would just put 120 neurons there since each of the 12 characters
    in “humpty dumpt” has 10 numbers to input. Now we just put the neurons next to
    each other and we are good to go
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们为每个字符分配一个向量而不是一个数字，那么我们现在如何将“humpty dumpt”输入到网络中？答案很简单。假设我们为每个字符分配了一个由10个数字组成的向量。那么，输入层不再有12个神经元，而是放置120个神经元，因为“humpty
    dumpt”中的每个12个字符都有10个数字要输入。现在我们只需将神经元放在一起，就可以开始了。
- en: How do we find these vectors? Thankfully, we just learned how to train embedding
    numbers. Training an embedding vector is no different. You now have 120 inputs
    instead of 12 but all you are doing is moving them around to see how you can minimize
    loss. And then you take the first 10 of those and that’s the vector corresponding
    to “h” and so on.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找到这些向量？幸运的是，我们刚刚学会了如何训练嵌入向量。训练嵌入向量和训练数字没有什么不同。你现在有120个输入，而不是12个，但你所做的只是移动它们，看看如何最小化损失。然后，你取其中的前10个，那就是对应“h”的向量，依此类推。
- en: 'All the embedding vectors must of course be the same length, otherwise we would
    not have a way of entering all the character combinations into the network. E.g.
    “humpty dumpt” and in the next iteration “umpty dumpty” — in both cases we are
    entering 12 characters in the network and if each of the 12 characters was not
    represented by vectors of length 10 we won’t be able to reliably feed them all
    into a 120-long input layer. Let’s visualize these embedding vectors:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的嵌入向量当然必须是相同长度的，否则我们就无法将所有字符组合输入到网络中。例如，“humpty dumpt”和下一次迭代中的“umpty dumpty”——在这两种情况下，我们都在输入12个字符到网络中，如果这12个字符中的每个没有被表示为长度为10的向量，我们就无法将它们可靠地输入到一个长度为120的输入层中。让我们来可视化这些嵌入向量：
- en: '![](../Images/b4655d09eabf02856659ea994f201371.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4655d09eabf02856659ea994f201371.png)'
- en: Image by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Let’s call an ordered collection of same-sized vectors a matrix. This matrix
    above is called an **embedding matrix**. You tell it a column number corresponding
    to your letter and looking at that column in the matrix will give you the vector
    that you are using to represent that letter. This can be applied more generally
    for embedding any arbitrary collection of things — you would just need to have
    as many columns in this matrix as the things you have.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称一组大小相同的向量为矩阵。上面的矩阵叫做**嵌入矩阵**。你给它一个对应字母的列号，然后查看矩阵中的那一列，你就能得到表示那个字母的向量。这个方法更一般地应用于嵌入任意事物的集合——你只需要保证矩阵中的列数等于你要表示的事物数量。
- en: Subword Tokenizers
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子词Token化器
- en: So far, we have been working with characters as the basic building blocks of
    language. This has its limitations. The neural network weights have to do a lot
    of the heavy lifting where they must make sense of certain sequences of characters
    (i.e. words) appearing next to each other and then next to other words. What if
    we directly assigned embeddings to words and made the network predict the next
    word. The network doesn’t understand anything more than numbers anyway, so we
    can assign a 10-length vector to each of the words “humpty”, “dumpty”, “sat”,
    “on” etc.. and then we just feed it two words and it can give us the next word.
    “**Token**” is the term for a single unit that we embed and then feed to the model.
    Our models so far were using characters as tokens, now we are proposing to use
    entire words as a token (you can of course use entire sentences or phrases as
    tokens if you like).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用字符作为语言的基本构建块。这有其局限性。神经网络的权重需要做大量的工作，它们必须理解特定字符序列（即单词）如何相邻出现，然后再与其他单词相邻。如果我们直接为单词分配嵌入并让网络预测下一个单词会怎么样呢？反正网络只理解数字，所以我们可以为单词“humpty”、“dumpty”、“sat”、“on”等分配一个10维的向量，然后我们只需要输入两个单词，它就能给我们预测下一个单词。“**Token**”是我们嵌入并传递给模型的单一单位。我们目前的模型使用字符作为Token，现在我们提出使用整个单词作为Token（如果你愿意，你当然可以使用整个句子或短语作为Token）。
- en: Using word tokenization has one profound effect on our model. There are more
    than 180K words in the English language. Using our output interpretation scheme
    of having a neuron per possible output we need hundreds of thousands of neurons
    in the output layer insead of the 26 or so. With the size of the hidden layers
    needed to achieve meaningful results for modern networks, this issue becomes less
    pressing. What is however worth noting is that since we are treating each word
    separately, and we are starting with a random number embeddings for each — very
    similar words (e.g. “cat” and “cats”) will start with no relationship. You would
    expect that embeddings for the two words should be close to each other — which
    undoubtedly the model will learn. But, can we somehow use this obvious similarity
    to get a jumpstart and simplify matters?
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词汇分词对我们的模型有一个深远的影响。英语中有超过18万个单词。采用我们输出解释方案，即每个可能的输出对应一个神经元，我们需要数十万个神经元来构成输出层，而不是大约26个神经元。随着隐层规模的增大，以达到现代网络所需的有意义结果，这个问题变得不那么紧迫。然而需要注意的是，由于我们是将每个单词单独处理，并且我们从每个单词的随机数嵌入开始——非常相似的单词（例如“cat”和“cats”）将从无关联开始。你会期望这两个单词的嵌入应该相互接近——这无疑是模型会学习到的。但是，我们是否可以以某种方式利用这种显而易见的相似性来加速启动并简化问题呢？
- en: Yes we can. The most common embedding scheme in language models today is something
    where you break words down into subwords and then embed them. In the cat example,
    we would break down cats into two tokens “cat” and ”s”. Now it is easier for the
    model to understand the concept of “s” followed by other familiar words and so
    on. This also reduces the number of tokens we need ([sentencpiece](https://github.com/google/sentencepiece)
    is a common tokenizer with vocab size options in tens of thousands vs hundreds
    of thousands of words in english). A tokenizer is something that takes you input
    text (e.g. “Humpty Dumpt”) and splits it into the tokens and gives you the corresponding
    numbers that you need to look up the embedding vector for that token in the embedding
    matrix. For example, in case of “humpty dumpty” if we’re using character level
    tokenizer and we arranged our embedding matrix as in the picture above, then the
    tokenizer will first split humpty dumpt into characters [‘h’,’u’,…’t’] and then
    give you back the numbers [8,21,…20] because you need to look up the 8th column
    of the embedding matrix to get the embedding vector for ‘h’ (embedding vector
    is what you will feed into the model, not the number 8, unlike before). The arrangement
    of the columns in the matrix is completely irrelevant, we could assign any column
    to ‘h’ and as long as we look up the same vector every time we input ‘h’ we should
    be good. Tokenizers just give us an arbitrary (but fixed) number to make lookup
    easy. The main task we need them for really is splitting the sentence in tokens.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们可以。今天语言模型中最常见的嵌入方案是将单词分解成子词，然后进行嵌入。在“cat”这个例子中，我们将“cats”分解成两个标记——“cat”和“s”。现在，模型更容易理解“s”后面跟随其他熟悉单词的概念，依此类推。这也减少了我们需要的标记数量（[sentencepiece](https://github.com/google/sentencepiece)是一个常见的分词器，其词汇量选项在几万到数十万个单词之间，而英语则有数十万个单词）。分词器是一个工具，它接受输入文本（例如“Humpty
    Dumpt”）并将其拆分为标记，随后返回对应的数字，以便在嵌入矩阵中查找该标记的嵌入向量。例如，在“humpty dumpty”这个例子中，如果我们使用字符级分词器，并且按照上图排列嵌入矩阵，那么分词器将首先把“humpty
    dumpt”拆分成字符[‘h’,‘u’,…‘t’]，然后返回数字[8,21,…20]，因为你需要查找嵌入矩阵中的第8列，以获得'h'的嵌入向量（嵌入向量是你要输入到模型中的内容，而不是数字8，与之前不同）。矩阵列的排列完全无关紧要，我们可以为'h'指定任何列，只要每次输入'h'时查找相同的向量就可以。分词器给我们一个任意（但固定）的数字，使查找变得容易。我们真正需要它们做的主要任务是将句子拆分成标记。
- en: 'With embeddings and subword tokenization, a model could look something like
    this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入和子词分词时，模型可能是这样的：
- en: '![](../Images/6795cc73eec2099f25bc2fbff5622e40.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6795cc73eec2099f25bc2fbff5622e40.png)'
- en: Image by author
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'The next few sections deal with more recent advances in language modeling,
    and the ones that made LLMs as powerful as they are today. However, to understand
    these there are a few basic math concepts you need to know. Here are the concepts:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将讨论语言模型中的一些最新进展，以及使LLM（大型语言模型）如此强大的技术。然而，为了理解这些进展，你需要掌握一些基本的数学概念。以下是这些概念：
- en: Matrices and matrix multiplication
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵和矩阵乘法
- en: General concept of functions in mathematics
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学中函数的一般概念
- en: Raising numbers to powers (e.g. a3 = a*a*a)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字的幂运算（例如 a³ = a * a * a）
- en: Sample mean, variance, and standard deviation
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本均值、方差和标准差
- en: I have added summaries of these concepts in the appendix.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我已在附录中添加了这些概念的总结。
- en: Self Attention
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自注意力
- en: 'So far we have seen only one simple neural network structure (called feedforward
    network), one which contains a number of layers and each layer is fully connected
    to the next (i.e., there is a line connecting any two neurons in consecutive layers),
    and it is only connected to the next layer (e.g. no lines between layer 1 and
    layer 3 etc..). However, as you can imagine there is nothing stopping us from
    removing or making other connections. Or even making more complex structures.
    Let’s explore a particularly important structure: self-attention.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看到了一个简单的神经网络结构（称为前馈网络），它包含多个层，每一层都与下一层完全连接（即，连续层之间有连接线），并且只与下一层连接（例如，第1层和第3层之间没有连接等）。然而，正如你可以想象的那样，没有什么能阻止我们移除或建立其他连接，甚至可以构建更复杂的结构。让我们来探讨一个特别重要的结构：自注意力。
- en: 'If you look at the structure of human language, the next word that we want
    to predict will depend on all the words before. However, they may depend on some
    words before them to a greater degree than others. For example, if we are trying
    to predict the next word in “Damian had a secret child, a girl, and he had written
    in his will that all his belongings, along with the magical orb, will belong to
    ____”. This word here could be “her” or “him” and it depends specifically on a
    much earlier word in the sentence: *girl/boy*.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下人类语言的结构，我们想要预测的下一个单词将依赖于之前的所有单词。然而，它们可能比其他单词更依赖于某些前面的单词。例如，如果我们试图预测“Damian
    had a secret child, a girl, and he had written in his will that all his belongings,
    along with the magical orb, will belong to ____”中的下一个单词，这个单词可以是“her”或“him”，它特别依赖于句子中更早的一个单词：*girl/boy*。
- en: The good news is, our simple feedforward model connects to all the words in
    the context, and so it can learn the appropriate weights for important words,
    But here’s the problem, the weights connecting specific positions in our model
    through feed forward layers are fixed (for every position). If the important word
    was always in the same position, it would learn the weights appropriately and
    we would be fine. However, the relevant word to the next prediction could be anywhere
    in the system. We could paraphrase that sentence above and when guessing “her
    vs his”, one very important word for this prediction would be boy/girl no matter
    where it appeared in that sentence. So, we need weights that depend not only on
    the position but also on the content in that position. How do we achieve this?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们简单的前馈模型连接了上下文中的所有单词，因此它可以学习重要单词的适当权重。但问题在于，通过前馈层连接我们模型中特定位置的权重是固定的（对于每个位置都是如此）。如果重要单词总是出现在相同的位置，它将适当地学习权重，我们就可以顺利进行。然而，下一次预测的相关单词可能出现在系统的任何位置。我们可以改写上述句子，在猜测“her”与“his”时，对于这个预测而言，无论该单词出现在句子中的哪个位置，“boy/girl”这个单词都是一个非常重要的单词。因此，我们需要的权重不仅取决于位置，还要依赖于该位置的内容。我们如何实现这一点呢？
- en: Self attention does something like adding up the embedding vectors for each
    of the words, but instead of directly adding them up it applies some weights to
    each. So if the embedding vectors for humpty,dumpty, sat are x1, x2, x3 respectively,
    then it will multiply each one with a weight (a number) before adding them up.
    Something like output = 0.5 x1 + 0.25 x2 + 0.25 x3 where output is the self-attention
    output. If we write the weights as u1, u2, u3 such that output = u1x1+u2x2+u3x3
    then how do we find these weights u1, u2, u3?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力做的是类似于将每个单词的嵌入向量相加，但它不是直接相加，而是对每个向量应用一些权重。因此，如果“humpty”、“dumpty”和“sat”的嵌入向量分别是x1、x2、x3，那么它会在相加之前将每个向量与一个权重（一个数字）相乘。类似于输出
    = 0.5 x1 + 0.25 x2 + 0.25 x3，其中output是自注意力的输出。如果我们将权重写作u1、u2、u3，使得输出 = u1x1 +
    u2x2 + u3x3，那么我们如何找到这些权重u1、u2、u3呢？
- en: Ideally, we want these weights to be dependent on the vector we are adding —
    as we saw some may be more important than others. But important to whom? To the
    word we are about to predict. So we also want the weights to depend on the word
    we are about to predict. Now that’s an issue, we of course don’t know the word
    we are about to predict before we predict it. So, self attention uses the word
    immediately preceding the word we are about to predict, i.e., the last word in
    the sentence available (I don’t really know why this and why not something else,
    but a lot of things in deep learning are trial and error and I suspect this works
    well).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望这些权重依赖于我们正在加权的向量——正如我们所看到的，有些可能比其他的更重要。但对于谁来说更重要？对于我们即将预测的单词。所以我们也希望权重依赖于我们即将预测的单词。现在这是一个问题，因为在预测之前我们当然不知道我们将要预测哪个单词。那么，自注意力机制使用的是我们即将预测的单词前面的那个单词，即句子中可用的最后一个单词（我其实不知道为什么是这样，而不是别的什么，但深度学习中的许多事情都是试错法，我怀疑这种方法效果很好）。
- en: Great, so we want weights for these vectors, and we want each weight to depend
    on the word that we are aggregating and word immediately preceding the one we
    are going to predict. Basically, we want a function u1 = F(x1, x3) where x1 is
    the word we will weight and x3 is the last word in the sequence we have (assuming
    we have only 3 words). Now, a straightforward way of achieving this is to have
    a vector for x1 (let’s call it k1) and a separate vector for x3 (let’s call it
    q3) and then simply take their dot product. This will give us a number and it
    will depend on both x1 and x3\. How do we get these vectors k1 and q3? We build
    a tiny single layer neural network to go from x1 to k1 (or x2 to k2, x3 to k3
    and so on). And we build another network going from x3 to q3 etc… Using our matrix
    notation, we basically come up with weight matrices Wk and Wq such that k1 = Wkx1
    and q1 =Wqx1 and so on. Now we can take a dot product of k1 and q3 to get a scalar,
    so u1 = F(x1,x3) = Wkx1 **·** Wqx3.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，所以我们需要这些向量的权重，并且我们希望每个权重依赖于我们正在聚合的单词和即将预测单词前面的那个单词。基本上，我们希望有一个函数u1 = F(x1,
    x3)，其中x1是我们将要加权的单词，x3是我们所拥有的序列中的最后一个单词（假设我们只有3个单词）。现在，达成这一目标的一种直接方法是为x1建立一个向量（我们称之为k1），为x3建立一个单独的向量（我们称之为q3），然后简单地计算它们的点积。这样我们就能得到一个数值，它将依赖于x1和x3。那么我们如何获得这些向量k1和q3呢？我们构建一个小型的单层神经网络，将x1映射到k1（或者x2映射到k2，x3映射到k3，依此类推）。我们再构建另一个网络，将x3映射到q3，依此类推…
    使用我们的矩阵表示法，基本上我们会得到权重矩阵Wk和Wq，使得k1 = Wk * x1，q1 = Wq * x1，依此类推。现在，我们可以计算k1和q3的点积来得到一个标量，因此u1
    = F(x1, x3) = Wk * x1 **·** Wq * x3。
- en: 'One additional thing that happens in self-attention is that we don’t directly
    take the weighted sum of the embedding vectors themselves. Instead, we take the
    weighted sum of some “value” of that embedding vector, which is obtained by another
    small single layer network. What this means is similar to k1 and q1, we also now
    have a v1 for the word x1 and we obtain it through a matrix Wv such that v1=Wvx1\.
    This v1 is then aggregated. So it all looks something like this if we only have
    3 words and we are trying to predict the fourth:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制中还有一个额外的事情是，我们不会直接对嵌入向量本身进行加权求和。相反，我们对该嵌入向量的一些“值”进行加权求和，这些值是通过另一个小型的单层网络获得的。这意味着，类似于k1和q1，我们现在也有了x1这个单词的v1，并且通过矩阵Wv获得它，公式为v1
    = Wv * x1。然后，v1会被聚合。所以如果我们只有3个单词并且试图预测第四个单词，它大致看起来是这样的：
- en: '![](../Images/9f7007d3159338f1a793465100688600.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7007d3159338f1a793465100688600.png)'
- en: Self attention. Image by author
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制。图像由作者提供
- en: The plus sign represents a simple addition of the vectors, implying they have
    to have the same length. One last modification not shown here is that the scalars
    u1, u2, u3 etc.. won’t necessarily add up to 1\. If we need them to be weights,
    we should make them add up. So we will apply a familiar trick here and use the
    softmax function.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 加号表示向量的简单相加，意味着它们必须具有相同的长度。这里没有展示的最后一个修改是，标量u1、u2、u3等不一定加起来等于1。如果我们需要它们作为权重，我们应该使它们相加为1。因此，我们在这里使用一个熟悉的技巧——应用softmax函数。
- en: This is self-attention. There is also cross-attention where you can have the
    q3 come from the last word, but the k’s and the v’s can come from another sentence
    altogether. This is for example valuable in translation tasks. Now we know what
    attention is.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自注意力机制。还有交叉注意力机制，其中你可以让q3来自于最后一个单词，但k和v可以来自于完全不同的句子。例如，在翻译任务中，这非常有价值。现在我们知道什么是注意力机制了。
- en: 'This whole thing can now be put in a box and be called a “self attention block”.
    Basically, this self attention block takes in the embedding vectors and spits
    out a single output vector of any user-chosen length. This block has three parameters,
    Wk,Wq,Wv — it doesn’t need to be more complicated than that. There are many such
    blocks in the machine learning literature, and they are usually represented by
    boxes in diagrams with their name on it. Something like this:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以将整个过程放入一个框中，称之为“自注意力模块”。基本上，这个自注意力模块接收嵌入向量，并输出一个用户选择长度的单一输出向量。这个模块有三个参数，Wk、Wq、Wv——它不需要比这更复杂。在机器学习文献中有很多这样的模块，它们通常在图表中以框的形式表示，并标有它们的名称。类似这样：
- en: '![](../Images/bcfa6ed52dfdebf81dc440215fd824ae.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcfa6ed52dfdebf81dc440215fd824ae.png)'
- en: Image by author
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: One of the things that you will notice with self-attention is that the position
    of things so far does not seem relevant. We are using the same W’s across the
    board and so switching Humpty and Dumpty won’t really make a difference here —
    all numbers will end up being the same. This means that while attention can figure
    out what to pay attention to, this won’t depend on word position. However, we
    do know that word positions are important in english and we can probably improve
    performance by giving the model some sense of a word’s position.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，使用自注意力时，到目前为止，事物的位置似乎并不重要。我们在所有地方都使用相同的W，因此交换“Humpty”和“Dumpty”并不会真正产生区别——所有数字最终都会是一样的。这意味着，虽然注意力机制可以决定关注什么，但这并不依赖于词语的位置。然而，我们知道，词语的位置在英语中是重要的，我们可能通过给模型一些词语位置的感知来提高性能。
- en: And so, when attention is used, we don’t often feed the embedding vectors directly
    to the self attention block. We will later see how “positional encoding” is added
    to embedding vectors before feeding to attention blocks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当使用注意力机制时，我们通常不会直接将嵌入向量输入到自注意力模块中。我们稍后将看到如何在将嵌入向量输入到注意力模块之前，添加“位置编码”。
- en: '*Note for the pre-initiated*: Those for whom this isn’t the first time reading
    about self-attention will note that we are not referencing any K and Q matrices,
    or applying masks etc.. That is because those things are implementation details
    arising out of how these models are commonly trained. A batch of data is fed and
    the model is simultaneously trained to predict dumpty from humpty, sat from humpty
    dumpty and so on. This is a matter of gaining efficiency and does not affect interpretation
    or even model outputs, and we have chosen to omit training efficiency hacks here.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*预备知识提示*：对于那些并非第一次阅读自注意力机制的人来说，可能会注意到我们并没有提到任何K和Q矩阵，或者应用掩码等。这是因为这些细节属于实现层面的内容，源于这些模型通常的训练方式。一批数据被输入，模型被同时训练以预测“dumpty”从“humpty”，从“humpty
    dumpty”预测“sat”，等等。这是为了提高效率，与解释或模型输出无关，我们选择在这里省略训练效率的技巧。'
- en: Softmax
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax
- en: 'We talked briefly about softmax in the very first note. Here’s the problem
    softmax is trying to solve: In our output interpretation we have as many neurons
    as the options from which we want the network to select one. And we said that
    we are going to interpret the network’s choice as the highest value neuron. Then
    we said we are going to calculate loss as the difference between the value that
    network provides, and an ideal value we want. But what’s that ideal value we want?
    We set it to 0.8 in the leaf/flower example. But why 0.8? Why no 5, or 10, or
    10 million? The higher the better for that training example. Ideally we want infinity
    there! Now that would make the problem intractable — all loss would be infinite
    and our plan of minimizing loss by moving around parameters (remember “gradient
    descent”) fails. How do we deal with this?'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一条笔记中简要提到过softmax。这里是softmax试图解决的问题：在我们的输出解释中，我们有和我们希望网络选择的选项一样多的神经元。我们说我们将网络的选择解释为值最大的神经元。然后我们说我们将计算损失值，作为网络提供的值与我们希望的理想值之间的差异。但我们希望的理想值是什么呢？在叶子/花卉的例子中，我们将其设置为0.8。但为什么是0.8？为什么不是5，或者10，或者1000万？在这个训练示例中，值越大越好。理想情况下，我们希望它是无限大！但这将使问题变得无法解决——所有损失值都将是无限大，我们通过调整参数（记住“梯度下降”）来最小化损失的计划将失败。我们该如何处理呢？
- en: 'One simple thing we can do is cap the values we want. Let’s say between 0 and
    1? This would make all loss finite, but now we have the issue of what happens
    when the network overshoots. Let’s say it outputs (5,1) for (leaf,flower) in one
    case, and (0,1) in another. The first case made the right choice but the loss
    is worse! Ok, so now we need a way to also convert the outputs of the last layer
    in (0,1) range so that it preserves the order. We could use any function (a “**function**”
    in mathematics is simply a mapping of one number to another — in goes one number,
    out comes another — it’s rule based in terms of what will be output for a given
    input) here to get the job done. One possible option is the logistic function
    (see graph below) which maps all numbers to numbers between (0,1) and preserves
    the order:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的一件简单的事情是限制我们想要的值。假设在0和1之间？这会让所有的损失是有限的，但现在我们面临一个问题，那就是当网络超出范围时会发生什么。假设它在某个情况下输出（5,1）作为（leaf，flower），而在另一个情况下输出（0,1）。第一个情况做出了正确的选择，但损失更大！好吧，那么现在我们需要一种方法，将最后一层的输出也转换到（0,1）的范围内，从而保持顺序。我们可以在这里使用任何函数（在数学中，“**函数**”指的是将一个数字映射到另一个数字——一个数字输入，另一个数字输出——这基于规则来决定给定输入会输出什么）来完成这项工作。一个可能的选择是逻辑函数（见下图），它将所有数字映射到（0,1）之间，并保持顺序：
- en: '![](../Images/fae3aebd03c7242cd9f5f45016f59cd7.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fae3aebd03c7242cd9f5f45016f59cd7.png)'
- en: Image by author
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Now, we have a number between 0 and 1 for each of the neurons in the last layer
    and we can calculate loss by setting the correct neuron to 1, others to 0 and
    taking the difference of that from what the network provides us. This will work,
    but can we do better?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们为最后一层的每个神经元都得到了一个0到1之间的数字，我们可以通过将正确的神经元设置为1，其他神经元设置为0，并计算它与网络提供的结果之间的差异来计算损失。这是可行的，但我们能做得更好吗？
- en: Going back to our “Humpty dumpty” example, let’s say we are trying to generate
    dumpty character-by-character and our model makes a mistake when predicting “m”
    in dumpty. Instead of giving us the last layer with “m” as the highest value,
    it gives us “u” as the highest value but “m” is a close second.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们“Humpty Dumpty”的例子，假设我们正在尝试逐字符生成“dumpty”，而模型在预测“dumpty”中的“m”时出错。它没有给出“m”作为最高值，而是给出了“u”作为最高值，但“m”是接近的第二选择。
- en: Now we can continue with “duu” and try to predict next character and so on,
    but the model confidence will be low because there are not that many good continuations
    from “humpty duu..”. On the other hand, “m” was a close second, so we can also
    give “m” a shot, predict the next few characters, and see what happens? Maybe
    it gives us a better overall word?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续“duu”，并尝试预测下一个字符，以此类推，但模型的置信度会较低，因为从“humpty duu..”开始并没有很多好的续写方式。另一方面，“m”是一个接近的第二选择，因此我们也可以尝试“m”，预测接下来的几个字符，看看会发生什么？也许它能给我们一个更好的整体单词？
- en: So what we are talking about here is not just blindly selecting the max value,
    but trying a few. What’s a good way to do it? Well we have to assign a chance
    to each one — say we will pick the top one with 50%, second one with 25% and so
    on. That’s a good way to do it. But maybe we would want the chance to be dependent
    on the underlying model predictions. If the model predicts values for m and u
    to be really close to each other here (compared to other values) — then maybe
    a close 50–50 chance of exploring the two is a good idea?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在这里讨论的不仅仅是盲目选择最大值，而是尝试几种选择。什么是一个好的方法呢？嗯，我们需要为每个选择分配一个概率——比如说，选择第一个的概率为50%，第二个的概率为25%，以此类推。这是一个好的方法。但也许我们希望概率取决于底层模型的预测。如果模型预测的“m”和“u”值非常接近（与其他值相比）——那么也许探索这两者之间50-50的机会是一个好主意？
- en: So we need a nice rule that takes all these numbers and converts them into chances.
    That’s what softmax does. It is a generalization of the logistic function above
    but with additional features. If you give it 10 arbitrary numbers — it will give
    you 10 outputs, each between 0 and 1 and importantly, all 10 adding up to 1 so
    that we can interpret them as chance. You will find softmax as the last layer
    in nearly every language model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要一个好的规则，将这些数字转化为概率。这就是softmax所做的。它是上面逻辑函数的推广，具有附加功能。如果你给它10个任意的数字——它会给你10个输出，每个输出都在0和1之间，而且重要的是，所有10个输出加起来等于1，这样我们就可以将它们解释为概率。你会在几乎所有的语言模型中发现softmax作为最后一层。
- en: Residual connections
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差连接
- en: 'We have slowly changed our visualization of networks as the sections progress.
    We are now using boxes/blocks to denote certain concepts. This notation is useful
    in denoting a particularly useful concept of residual connections. Let’s look
    at residual connection combined with a self-attention block:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 随着章节的推进，我们逐渐改变了对网络的可视化表示。现在我们使用框/块来表示某些概念。这种符号在表示残差连接这一特别有用的概念时非常有用。让我们看一下结合自注意力块的残差连接：
- en: '![](../Images/a25915f657c45b1e6eb6a3f62f7805d1.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a25915f657c45b1e6eb6a3f62f7805d1.png)'
- en: A residual connection. Image by author
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接。图片来源：作者
- en: Note that we put “Input” and “Output” as boxes to make things simpler, but these
    are still basically just a collection of neurons/numbers same as shown above.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将“输入”和“输出”表示为框，以简化理解，但这些基本上仍然只是与上面显示的神经元/数字的集合。
- en: So what’s going on here? We are basically taking the output of self-attention
    block and before passing it to the next block, we are adding to it the original
    Input. First thing to note is that this would require that the dimensions of the
    self-attention block output must now be the same as that of the input. This is
    not a problem since as we noted the self-attention output is determined by the
    user. But why do this? We won’t get into all the details here but the key thing
    is that as networks get deeper (more layers between input and output) it gets
    increasingly harder to train them. Residual connections have been shown to help
    with these training challenges.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这里发生了什么呢？我们基本上是将自注意力块的输出，在传递到下一个块之前，加入原始输入。首先需要注意的是，这要求自注意力块的输出维度必须与输入的维度相同。由于自注意力的输出是由用户决定的，所以这不是问题。但是为什么要这样做呢？我们在这里不深入探讨所有细节，但关键是，随着网络加深（输入和输出之间的层数增多），训练它们变得越来越困难。残差连接已被证明有助于解决这些训练挑战。
- en: Layer Normalization
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层归一化
- en: 'Layer normalization is a fairly simple layer that takes the data coming into
    the layer and normalizes it by subtracting the mean and dividing it by standard
    deviation (maybe a bit more, as we see below). For example, if we were to apply
    layer normalization immediately after the input, it would take all the neurons
    in the input layer and then it would calculate two statistics: their mean and
    their standard deviation. Let’s say the mean is M and the standard deviation is
    D then what layer norm is doing is taking each of these neurons and replacing
    it with (x-M)/D where x denotes any given neuron’s original value.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化是一个相当简单的层，它接收进入该层的数据，通过减去均值并除以标准差来进行归一化（可能会有些许变化，正如下文所示）。例如，如果我们在输入后立即应用层归一化，它将取输入层中的所有神经元，然后计算两个统计量：它们的均值和标准差。假设均值为M，标准差为D，那么层归一化所做的就是将每个神经元替换为(x-M)/D，其中x表示任意给定神经元的原始值。
- en: 'Now how does this help? It basically stabilizes the input vector and helps
    with training deep networks. One concern is that by normalizing inputs, are we
    removing some useful information from them that may be helpful in learning something
    valuable about our goal? To address this, the layer norm layer has a scale and
    a bias parameter. Basically, for each neuron you just multiply it with a scalar
    and then add a bias to it. These scalar and bias values are parameters that can
    be trained. This allows the network to learn some of the variation that may be
    valuable to the predictions. And since these are the only parameters, the LayerNorm
    block doesn’t have a lot of parameters to train. The whole thing looks something
    like this:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这有什么帮助呢？它基本上稳定了输入向量，并有助于训练深度网络。一个担心的问题是，通过归一化输入，我们是否去除了其中一些可能对学习我们目标有所帮助的有用信息？为了解决这个问题，层归一化层有一个缩放和偏置参数。基本上，对于每个神经元，您只需将其与一个标量相乘，然后加上一个偏置。这个标量和偏置值是可以训练的参数。这允许网络学习一些可能对预测有价值的变化。而且由于这些是唯一的参数，LayerNorm块并没有很多需要训练的参数。整个过程看起来大致如下：
- en: '![](../Images/e206e7652281b7b0a4f4a9f1b59ec819.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e206e7652281b7b0a4f4a9f1b59ec819.png)'
- en: Layer Normalization. Image by author
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化。图片来源：作者
- en: The Scale and Bias are trainable parameters. You can see that layer norm is
    a relatively simple block where each number is only operated on pointwise (after
    the initial mean and std calculation). Reminds us of the activation layer (e.g.
    RELU) with the key difference being that here we have some trainable parameters
    (albeit lot fewer than other layers because of the simple pointwise operation).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放和偏差是可训练的参数。你可以看到，层归一化（layer norm）是一个相对简单的模块，其中每个数值仅在点对点操作下进行处理（在初步计算均值和标准差之后）。这让人联想到激活层（如RELU），唯一的关键区别是这里我们有一些可训练的参数（尽管由于简单的点对点操作，它们远少于其他层的参数）。
- en: 'Standard deviation is a statistical measure of how spread out the values are,
    e.g., if the values are all the same you would say the standard deviation is zero.
    If, in general, each value is really far from the mean of these very same values,
    then you will have a high standard deviation. The formula to calculate standard
    deviation for a set of numbers, a1, a2, a3…. (say N numbers) goes something like
    this: subtract the mean (of these numbers) from each of the numbers, then square
    the answer for each of N numbers. Add up all these numbers and then divide by
    N. Now take a square root of the answer.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差是一个统计量，用来衡量数据分布的离散程度。例如，如果所有的值都相同，你会说标准差为零。如果一般情况下，每个值都远离这些值的均值，那么标准差就会很高。计算标准差的公式是：从每个数值中减去这些数值的均值，然后将每个差值平方。将这些平方后的结果相加，再除以数据的数量N。最后对结果取平方根。
- en: 'Note for the pre-initiated: Experienced ML professionals will note that there
    is no discussion of batch norm here. In-fact, we haven’t even introduced the concept
    of batches in this article at all. For the most part, I believe batches are another
    training accelerant not related to the understanding of core concepts (except
    perhaps batch norm which we do not need here).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已了解相关内容的读者：有经验的机器学习（ML）专业人士会注意到，这里没有讨论批归一化（batch norm）。事实上，本文中甚至没有引入批次（batches）这一概念。大多数情况下，我认为批次是另一种训练加速手段，与理解核心概念无关（除了批归一化，我们在这里不需要讨论）。
- en: Dropout
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Dropout  '
- en: Dropout is a simple but effective method to avoid model overfitting. Overfitting
    is a term for when you train the model on your training data, and it works well
    on that dataset but does not generalize well to the examples the model has not
    seen. Techniques that help us avoid overfitting are called “**regularization techniques**”,
    and dropout is one of them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种简单而有效的方法，用于避免模型过拟合。过拟合是指当你在训练数据上训练模型时，它在该数据集上表现良好，但在模型未见过的样本上却无法很好地泛化。帮助我们避免过拟合的技术被称为“**正则化技术**”，而Dropout正是其中之一。
- en: If you train a model, it might make errors on the data and/or overfit it in
    a particular way. If you train another model, it might do the same, but in a different
    way. What if you trained a number of these models and averaged the outputs? These
    are typically called “**ensemble** **models**” because they predict the outputs
    by combining outputs from an ensemble of models, and ensemble models generally
    perform better than any of the individual models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你训练一个模型，它可能会对数据产生误差，或者以某种特定方式对数据进行过拟合。如果你训练另一个模型，它可能也会出现类似情况，但表现方式不同。如果你训练多个这样的模型并将它们的输出进行平均呢？这些通常被称为“**集成模型**”，因为它们通过结合多个模型的输出预测结果，而集成模型通常比任何单一模型表现得更好。
- en: In neural networks, you could do the same. You could build multiple (slightly
    different) models and then combine their outputs to get a better model. However,
    this can be computationally expensive. Dropout is a technique that doesn’t quite
    build ensemble models but does capture some of the essence of the concept.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，你也可以做类似的操作。你可以构建多个（略有不同的）模型，然后将它们的输出结合起来以获得更好的模型。然而，这可能会计算开销较大。Dropout是一种技术，它并不完全构建集成模型，但确实捕捉到了一些集成方法的精髓。
- en: 'The concept is simple, by inserting a dropout layer during training what you
    are doing is randomly deleting a certain percentage of the direct neuron connections
    between the layers that dropout is inserted. Considering our initial network and
    inserting a Dropout layer between the input and the middle layer with 50% dropout
    rate can look something like this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念很简单，通过在训练过程中插入一个Dropout层，你所做的就是随机删除Dropout层插入位置之间的神经元连接的某一百分比。考虑我们的初始网络，并在输入层和中间层之间插入一个Dropout层，设定50%的丢弃率，效果可能如下所示：
- en: '![](../Images/f86c8a1d3bb76fd3595eb114c618e595.png)![](../Images/84edf3b6bdb4b2a72c848c639a927595.png)![](../Images/cc6bfa2a9c4e06051ec5231087df43d3.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f86c8a1d3bb76fd3595eb114c618e595.png)![](../Images/84edf3b6bdb4b2a72c848c639a927595.png)![](../Images/cc6bfa2a9c4e06051ec5231087df43d3.png)'
- en: Image by author
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: Now, this forces the network to train with a lot of redundancy. Essentially,
    you are training a number of different models at the same time — but they share
    weights.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这迫使网络在训练时有大量冗余。实质上，你是在同时训练多个不同的模型——但它们共享权重。
- en: Now for making inferences, we could follow the same approach as an ensemble
    model. We could make multiple predictions using dropouts and then combine them.
    However, since that is computationally intensive — and since our models share
    common weights — why don’t we just do a prediction using all the weights (so instead
    of using 50% of the weights at a time we use all at the same time). This should
    give us some approximation of what an ensemble will provide.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了进行推理，我们可以采用与集成模型相同的方法。我们可以使用dropout做出多个预测，然后将它们组合起来。然而，由于这在计算上比较耗时——并且由于我们的模型共享共同的权重——为什么不直接使用所有权重进行预测（即一次性使用所有的权重，而不是每次只使用50%的权重）呢？这应该能给我们提供一个集成方法的近似结果。
- en: 'One issue though: the model trained with 50% of the weights will have very
    different numbers in the middle neurons than one using all the weights. What we
    want is more ensemble style averaging here. How do we do this? Well, a simple
    way is to simply take all the weights and multiply them by 0.5 since we are now
    using twice as many weights. This is what Droput does during inference. It will
    use the full network with all the weights and simply multiply the weights with
    (1- p) where p is the deletion probability. And this has been shown to work rather
    well as a regularization technique.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个问题是：使用50%权重训练的模型在中间神经元中的数值与使用所有权重的模型差异很大。我们想要的是这里更像是集成式的平均方法。我们该怎么做呢？一种简单的方法是将所有权重乘以0.5，因为我们现在使用了两倍数量的权重。这就是Dropout在推理过程中做的事情。它会使用包含所有权重的完整网络，并将权重乘以(1
    - p)，其中p是删除的概率。研究表明，这种方法作为正则化技术效果非常好。
- en: Multi-head Attention
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: 'This is the key block in the transformer architecture. We’ve already seen what
    an attention block is. Remember that the output of an attention block was determined
    by the user and it was the length of v’s. What a multi-attention head is basically
    you run several attention heads in parallel (they all take the same inputs). Then
    we take all their outputs and simply concatenate them. It looks something like
    this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是变换器架构中的关键模块。我们已经看到过什么是注意力模块。记住，注意力模块的输出是由用户决定的，且它的长度是v的长度。多头注意力基本上是并行运行多个注意力头（它们都采用相同的输入）。然后我们将它们的所有输出连接在一起。它大概长这样：
- en: '![](../Images/8a461c40693ccc710e1e6a447c9db098.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a461c40693ccc710e1e6a447c9db098.png)'
- en: Multi-head attention. Image by author
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力。图片来自作者
- en: Keep in mind the arrows going from v1 -> v1h1 are linear layers — there’s a
    matrix on each arrow that transforms. I just did not show them to avoid clutter.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，从v1 -> v1h1的箭头是线性层——每条箭头上都有一个矩阵来进行变换。我只是没有显示它们，以避免杂乱。
- en: What is going on here is that we are generating the same key, query and values
    for each of the heads. But then we are basically applying a linear transformation
    on top of that (separately to each k,q,v and separately for each head) before
    we use those k,q,v values. This extra layer did not exist in self attention.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的事情是，我们为每个头生成相同的key、query和value。但接着我们基本上是在这些值上应用一个线性变换（分别对每个k、q、v和每个头进行处理），然后使用这些k、q、v值。这一额外的层在自注意力中并不存在。
- en: A side note is that to me, this is a slightly surprising way of creating a multi-headed
    attention. For example, why not create separate Wk,Wq,Wv matrices for each of
    the heads rather than adding a new layer and sharing these weights. Let me know
    if you know — I really have no idea.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个旁注是，对我而言，这种创建多头注意力的方式有点出乎意料。例如，为什么不为每个头创建单独的Wk、Wq、Wv矩阵，而是通过增加一个新层并共享这些权重呢？如果你知道，请告诉我——我真的不清楚。
- en: Positional encoding and embedding
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码和嵌入
- en: We briefly talked about the motivation for using positional encoding in the
    self-attention section. What are these? While the picture shows positional encoding,
    using a positional embedding is more common than using an encoding. As such we
    talk about a common positional embedding here but the appendix also covers positional
    encoding used in the original paper. A positional embedding is no different than
    any other embedding except that instead of embedding the word vocabulary we will
    embed numbers 1, 2, 3 etc. So this embedding is a matrix of the same length as
    word embedding, and each column corresponds to a number. That’s really all there
    is to it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在自注意力部分简要讨论了使用位置编码的动机。这些是什么？虽然图片显示了位置编码，但使用位置嵌入比使用编码更常见。因此，我们在这里讨论了一种常见的位置嵌入，但附录中也涵盖了原始论文中使用的位置编码。位置嵌入与任何其他嵌入没有什么不同，只是不同于嵌入词汇，我们将嵌入数字1、2、3等。因此，这种嵌入是与词嵌入相同长度的矩阵，每列对应一个数字。就是这样简单。
- en: The GPT architecture
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT架构
- en: 'Let’s talk about the GPT architecture. This is what is used in most GPT models
    (with variation across). If you have been following the article thus far, this
    should be fairly trivial to understand. Using the box notation, this is what the
    architecture looks like at high level:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈GPT架构。这是大多数GPT模型中使用的架构（可能会有一些变化）。如果你迄今为止一直在关注本文，那么这应该很容易理解。使用方框表示法，这是高层次上架构的样子：
- en: '![](../Images/9131535544f726b4d55df91e86e08923.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9131535544f726b4d55df91e86e08923.png)'
- en: The GPT Architecture. Image by author
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: GPT架构。作者提供的图片
- en: 'At this point, other than the “GPT Transformer Block” all the other blocks
    have been discussed in great detail. The + sign here simply means that the two
    vectors are added together (which means the two embeddings must be the same size).
    Let’s look at this GPT Transformer Block:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，除了“GPT变压器块”外，所有其他块都已经详细讨论过了。这里的+号简单地表示两个向量相加（这意味着两个嵌入必须是相同大小）。让我们看看这个GPT变压器块：
- en: '![](../Images/39ede22ff117e6e0886f371d7091d61a.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39ede22ff117e6e0886f371d7091d61a.png)'
- en: 'And that’s pretty much it. It is called “transformer” here because it is derived
    from and is a type of transformer — which is an architecture we will look at in
    the next section. This doesn’t affect understanding as we’ve already covered all
    the building blocks shown here before. Let’s recap everything we’ve covered so
    far building up to this GPT architecture:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。这里称为“变压器”，因为它源自并且是一种变压器类型——这是我们将在下一节中讨论的架构。这不会影响理解，因为我们已经在之前涵盖了这里显示的所有构建块。让我们回顾一下到目前为止我们所讨论的所有内容，以构建这个GPT架构：
- en: We saw how neural nets take numbers and output other numbers and have weights
    as parameters which can be trained
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们看到神经网络如何将数字作为输入并输出其他数字，并具有可训练的参数权重
- en: We can attach interpretations to these input/output numbers and give real world
    meaning to a neural network
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将这些输入/输出数字附加解释，并赋予神经网络现实世界的含义
- en: We can chain neural networks to create bigger ones, and we can call each one
    a “block” and denote it with a box to make diagrams easier. Each block still does
    the same thing, take in a bunch of numbers and output other bunch of numbers
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以串联神经网络以创建更大的网络，我们可以称每个网络为一个“块”，并用一个方框表示它，以便更容易绘制图表。每个块仍然做同样的事情，接收一堆数字并输出另一堆数字
- en: We learned a lot of different types of blocks that serve different purposes
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习了许多不同类型的块，用于不同的目的
- en: GPT is just a special arrangement of these blocks that is shown above with an
    interpretation that we discussed in Part 1
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT只是这些块的特殊排列，如上所示，并且具有我们在第一部分讨论过的解释
- en: Modifications have been made over time to this as companies have built up to
    powerful modern LLMs, but the basic remains the same.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公司构建强大的现代LLM，对此进行了一些修改，但基本原理保持不变。
- en: Now, this GPT transformer is actually what is called a “decoder” in the original
    transformer paper that introduced the transformer architecture. Let’s take a look
    at that.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个GPT变压器实际上是原始变压器论文中所称的“解码器”。让我们来看看。
- en: The transformer architecture
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器架构
- en: This is one of the key innovations driving rapid acceleration in the capabilities
    of language models recently. Transformers not only improved the prediction accuracy,
    they are also easier/more efficient than previous models (to train), allowing
    for larger model sizes. This is what the GPT architecture above is based on.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是推动最近语言模型能力快速提升的关键创新之一。Transformer 不仅提高了预测准确性，而且比以前的模型更易于训练/更高效，使得更大的模型规模成为可能。这正是上面提到的
    GPT 架构的基础。
- en: If you look at GPT architecture, you can see that it is great for generating
    the next word in the sequence. It fundamentally follows the same logic we discussed
    in Part 1\. Start with a few words and then continue generating one at a time.
    But, what if you wanted to do translation. What if you had a sentence in german
    (e.g. “Wo wohnst du?” = “Where do you live?”) and you wanted to translate it to
    english. How would we train the model to do this?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看 GPT 架构，你会发现它非常适合生成序列中的下一个词。它基本上遵循我们在第一部分讨论的相同逻辑。从几个词开始，然后继续一次生成一个。但如果你想做翻译怎么办？假设你有一个德语句子（例如：“Wo
    wohnst du?” = “Where do you live?”），你想将其翻译成英语。我们如何训练模型来完成这个任务？
- en: 'Well, first thing we would need to do is figure out a way to input german words.
    Which means we have to expand our embedding to include both german and english.
    Now, I guess here is a simply way of inputting the information. Why don’t we just
    concatenate the german sentence at the beginning of whatever so far generated
    english is and feed it to the context. To make it easier for the model, we can
    add a separator. This would look something like this at each step:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先我们需要做的是找出一种输入德语单词的方法。这意味着我们必须扩展我们的嵌入，包含德语和英语。现在，我想这里有一种简单的输入信息的方法。为什么不把德语句子连接到当前生成的任何英语句子的开头，然后将其传递给上下文呢？为了使模型更容易理解，我们可以添加一个分隔符。每一步的样子大致如下：
- en: '![](../Images/f02b947eb77ac419ecb6ccf46c5aedeb.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f02b947eb77ac419ecb6ccf46c5aedeb.png)'
- en: Image by author
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者
- en: 'This will work, but it has room for improvement:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是可行的，但仍有改进空间：
- en: If the context length is fixed, sometimes the original sentence is lost
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果上下文长度是固定的，有时候原始句子会丢失。
- en: The model has a lot to learn here. Two languages simultaneously, but also to
    know that <SEP> is the separator token where it needs to start translating
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个模型需要学习很多东西。它同时处理两种语言，还需要知道 <SEP> 是分隔符标记，模型需要从这里开始翻译。
- en: You are processing the entire german sentence, with different offsets, for each
    word generation. This means there will be different internal representations of
    the same thing and the model should be able to work through it all for translation
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在处理整个德语句子时，每生成一个词都有不同的偏移量。这意味着相同的内容会有不同的内部表示，模型应该能够处理所有这些内容来进行翻译。
- en: Transformer was originally created for this task and consists of an “encoder”
    and a “decoder” — which are basically two separate blocks. One block simply takes
    the german sentence and gives out an intermediate representation (again, bunch
    of numbers, basically) — this is called the encoder.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 最初是为这个任务创建的，由一个“编码器”和一个“解码器”组成——基本上是两个独立的模块。一个模块仅仅是接收德语句子并给出一个中间表示（再次说明，是一堆数字，基本上是这样的）——这个过程叫做编码器。
- en: The second block generates words (we’ve seen a lot of this so far). The only
    difference is that in addition to feeding it the words generated so far we also
    feed it the encoded german (from the encoder block) sentence. So as it is generating
    language, it’s context is basically all the words generated so far, plus the german.
    This block is called the decoder.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个模块生成词语（到目前为止我们已经看到过很多）。唯一的区别是，除了将已生成的词传递给它之外，我们还将编码后的德语句子（来自编码器模块）传递给它。因此，当它生成语言时，它的上下文基本上是到目前为止所有已生成的词，加上德语。这一模块叫做解码器。
- en: 'Each of these encoders and decoders consist of a few blocks, notably the attention
    block sandwiched between other layers. Let’s look at the illustration of a transformer
    from the paper “Attention is all you need” and try to understand it:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些编码器和解码器每个由几个模块组成，特别是夹在其他层之间的注意力模块。让我们来看一下论文《Attention is all you need》中的一个
    Transformer 插图，并试着理解它：
- en: '![](../Images/43c4fa0bddc445155194a960f85a42a2.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43c4fa0bddc445155194a960f85a42a2.png)'
- en: Image from Vaswani et al. (2017)
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 Vaswani 等人（2017）
- en: 'The vertical set of blocks on the left is called the “encoder” and the ones
    to the right is called the “decoder”. Let’s go over and understand anything that
    we have not already covered before:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧垂直排列的一组模块叫做“编码器”，右侧的模块叫做“解码器”。让我们一起来理解一下之前没有涉及到的部分：
- en: '*Recap on how to read the diagram:* Each of the boxes here is a block that
    takes in some inputs in the form of neurons, and spits out a set of neurons as
    output that can then either be processed by the next block or interpreted by us.
    The arrows show where the output of a block is going. As you can see, we will
    often take the output of one block and feed it in as input into multiple blocks.
    Let’s go through each thing here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*图示解读：* 这里的每一个方框都是一个块，它接收一些神经元形式的输入，并输出一组神经元，输出的神经元可以被下一个块处理或由我们进行解读。箭头显示了块的输出去向。如你所见，我们通常会将一个块的输出作为输入传递给多个块。让我们逐个解释这里的内容：'
- en: 'Feed forward: A feedforward network is one that does not contain cycles. Our
    original network in section 1 is a feed forward. In-fact, this block uses very
    much the same structure. It contains two linear layers, each followed by a RELU
    (see note on RELU in first section) and a dropout layer. Keep in mind that this
    feedforward network applies to each position independently. What this means is
    that the information on position 0 has a feedforward network, and on position
    1 has one and so on.. But the neurons from position x do not have a linkage to
    the feedforward network of position y. This is important because if we did not
    do this, it would allow the network to cheat during training time by looking forward.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈：前馈网络是指不包含循环的网络。我们在第1节中的原始网络就是一个前馈网络。实际上，这个块使用的结构非常相似。它包含两个线性层，每个线性层后面跟着一个RELU（请参见第1节中关于RELU的说明）和一个丢弃层。请记住，这个前馈网络适用于每个位置的独立处理。这意味着位置0上的信息有一个前馈网络，位置1也有一个，以此类推。但是，位置x的神经元并没有与位置y的前馈网络连接。这一点很重要，因为如果不这样做，网络在训练过程中就可能通过向前查看来作弊。
- en: '*Cross-attention:* You will notice that the decoder has a multi-head attention
    with arrows coming from the encoder. What is going on here? Remember the value,
    key, query in self-attention and multi-head attention? They all came from the
    same sequence. The query was just from the last word of the sequence in-fact.
    So what if we kept the query but fetched the value and key from a completely different
    sequence altogether? That is what is happening here. The value and key come from
    the output of the encoder. Nothing has changed mathematically except where the
    inputs for key and value are coming from now.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '*跨注意力：* 你会注意到解码器有一个多头注意力机制，箭头从编码器指向解码器。那么这里发生了什么呢？还记得自注意力和多头注意力中的值、键、查询吗？它们都是来自同一个序列。查询实际上只是来自序列中的最后一个单词。那么，如果我们保留查询，但从完全不同的序列中提取值和键会怎样呢？这就是这里发生的事情。值和键来自编码器的输出。数学上没有变化，唯一的不同是键和值的输入来源发生了变化。'
- en: '*Nx*: The Nx here simply represents that this block is chain-repeated N times.
    So basically you are stacking the block back-to-back and passing the input from
    the previous block to the next one. This is a way to make the neural network deeper.
    Now, looking at the diagram there is room for confusion about how the encoder
    output is fed to the decoder. Let’s say N=5\. Do we feed the output of each encoder
    layer to the corresponding decoder layer? No. Basically you run the encoder all
    the way through once and only once. Then you just take that representation and
    feed the same thing to every one of the 5 decoder layers.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*Nx：* 这里的Nx表示该块会重复N次。因此，基本上你将块一个接一个地堆叠起来，并将上一个块的输入传递给下一个块。这是一种让神经网络更深的方法。现在，从图示来看，可能会对编码器的输出是如何传递给解码器产生混淆。假设N=5。我们是否将每个编码器层的输出传递给对应的解码器层？不。基本上，你只需要运行一次完整的编码器。然后，你只需将该表示传递给每一个解码器层。'
- en: '*Add & Norm block*: This is basically the same as below (guess the authors
    were just trying to save space)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*加法与规范化块：* 这基本上和下面的内容相同（猜测作者只是想节省空间）'
- en: '![](../Images/647f177188e56ccad36b974480c46f4e.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/647f177188e56ccad36b974480c46f4e.png)'
- en: Image by author
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Everything else has already been discussed. Now you have a complete explanation
    of the transformer architecture building up from simple sum and product operations
    and fully self contained! You know what every line, every sum, every box and word
    means in terms of how to build them from scratch. Theoretically, these notes contain
    what you need to code up the transformer from scratch. In-fact, if you are interested
    [this repo](https://github.com/karpathy/nanoGPT) does that for the GPT architecture
    above.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其他的内容已经讨论过了。现在你已经对变换器架构有了完整的理解，从简单的加法和乘法运算开始，完全自包含！你知道每一行、每个加法、每个框和每个词在从零开始构建时的含义。从理论上讲，这些笔记包含了你从头开始编写变换器所需的内容。事实上，如果你感兴趣的话，[这个仓库](https://github.com/karpathy/nanoGPT)已经实现了上述GPT架构。
- en: Appendix
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: Matrix Multiplication
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'We introduced vectors and matrices above in the context of embeddings. A matrix
    has two dimensions (number or rows and columns). A vector can also be thought
    of as a matrix where one of the dimensions equals one. Product of two matrices
    is defined as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上文中介绍了嵌入的背景下的向量和矩阵。矩阵有两个维度（行数和列数）。向量也可以看作是一个矩阵，其中一个维度等于一。两个矩阵的乘积定义为：
- en: '![](../Images/120cb29f8eace4be8cdc75726b1d2815.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/120cb29f8eace4be8cdc75726b1d2815.png)'
- en: Image by author
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Dots represent multiplication. Now let’s take a second look at the calculation
    of blue and organic neurons in the very first picture. If we write the weights
    as a matrix and the inputs as vectors, we can write the whole operation in the
    following way:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 点表示乘法。现在我们再看看第一张图片中蓝色和有机神经元的计算。如果我们把权重写成矩阵，把输入写成向量，那么我们可以用以下方式表达整个操作：
- en: '![](../Images/179d2a95fdf1f138c9384c2c9d6607f0.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/179d2a95fdf1f138c9384c2c9d6607f0.png)'
- en: Image by author
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: If the weight matrix is called “W” and the inputs are called “x” then Wx is
    the result (the middle layer in this case). We can also transpose the two and
    write it as xW — this is a matter of preference.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果权重矩阵叫做“W”，输入叫做“x”，那么Wx就是结果（在这种情况下是中间层）。我们也可以对它们进行转置，写成xW——这只是一个个人偏好的问题。
- en: Standard deviation
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准差
- en: 'We use the concept of standard deviation in the Layer Normalization section.
    Standard deviation is a statistical measure of how spread out the values are (in
    a set of numbers), e.g., if the values are all the same you would say the standard
    deviation is zero. If, in general, each value is really far from the mean of these
    very same values, then you will have a high standard deviation. The formula to
    calculate standard deviation for a set of numbers, a1, a2, a3…. (say N numbers)
    goes something like this: subtract the mean (of these numbers) from each of the
    numbers, then square the answer for each of N numbers. Add up all these numbers
    and then divide by N. Now take a square root of the answer.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在层归一化部分使用了标准差的概念。标准差是一个统计量，用来衡量一组数据的离散程度（即数据值的分布情况）。例如，如果所有数据值相同，那么标准差就是零。如果每个数据值都远离这些数据的均值，那么标准差会很高。计算一组数字（比如N个数字a1、a2、a3……）的标准差的公式如下：首先从每个数字中减去这些数字的均值，然后对每个差值进行平方。将所有这些平方值加在一起，再除以N。最后，对这个结果取平方根。
- en: Positional Encoding
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: We talked about positional embedding above. A positional encoding is simply
    a vector of the same length as the word embedding vector, except it is not an
    embedding in the sense that it is not trained. We simply assign a unique vector
    to every position e.g. a different vector for position 1 and different one for
    position 2 and so on. A simple way of doing this is to make the vector for that
    position simply full of the position number. So the vector for position 1 would
    be [1,1,1…1] for 2 would be [2,2,2…2] and so on (remember length of each vector
    must match embedding length for addition to work). This is problematic because
    we can end up with large numbers in vectors which creates challenges during training.
    We can, of course, normalize these vectors by dividing every number by the max
    of position, so if there are 3 words total then position 1 is [.33,.33,..,.33]
    and 2 is [.67, .67, ..,.67] and so on. This has the problem now that we are constantly
    changing the encoding for position 1 (those numbers will be different when we
    feed 4 word sentence as input) and it creates challenges for the network to learn.
    So here, we want a scheme that allocates a unique vector to each position, and
    the numbers don’t explode. Basically if the context length is d (i.e., maximum
    number of tokens/words that we can feed into the network for predicting next token/word,
    see discussion in “how does it all generate language?” section) and if the length
    of the embedding vector is 10 (say), then we need a matrix with 10 rows and d
    columns where all the columns are unique and all the numbers lie between 0 and
    1\. Given that there are infinitely many numbers between zero and 1, and the matrix
    is finitely sized, this can be done in many ways.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上面讨论了位置嵌入。位置编码只是一个与词嵌入向量长度相同的向量，只不过它不是嵌入，意思是它不是经过训练的。我们简单地为每个位置分配一个唯一的向量，例如，位置1和位置2的向量不同，依此类推。实现这种方法的一种简单方式是让该位置的向量全部由该位置的编号组成。因此，位置1的向量将是[1,1,1...1]，位置2的向量将是[2,2,2...2]，依此类推（请记住，每个向量的长度必须与嵌入长度匹配，以便进行加法操作）。这种方法有问题，因为我们可能会在向量中得到很大的数字，这会在训练过程中带来挑战。当然，我们可以通过将每个数字除以位置的最大值来对这些向量进行归一化处理。如果总共有3个词，那么位置1的向量将是[.33,
    .33, ... , .33]，位置2的向量将是[.67, .67, ... , .67]，依此类推。现在的问题是，我们不断地改变位置1的编码（这些数字会随着我们输入四个单词的句子而不同），这使得网络难以学习。所以在这里，我们需要一个方案，为每个位置分配一个唯一的向量，而且数值不会爆炸。基本上，如果上下文长度是d（即，我们可以输入到网络中以预测下一个词/标记的最大令牌/词的数量，请参见“如何生成语言？”部分中的讨论），而嵌入向量的长度是10（假设如此），那么我们需要一个有10行和d列的矩阵，其中所有列都是唯一的，且所有数值都位于0和1之间。考虑到在0和1之间有无限多个数，而矩阵的大小是有限的，因此可以用多种方式来实现。
- en: 'The approach used in the “Attention is all you need” paper goes something like
    this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: “Attention is all you need”论文中使用的方法大致如下：
- en: Draw 10 sin curves each being si(p) = sin (p/10000(i/d)) (that’s 10k to power
    i/d)
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制10个正弦曲线，每个曲线为si(p) = sin(p/10000(i/d))（即10的k次方到i/d次方）
- en: Fill the encoding matrix with numbers such that (i,p)th number is si(p), e.g.,
    for position 1 the 5th element of the encoding vector is s5(1)=sin (1/10000(5/d))
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充编码矩阵，使得(i,p)位置的数值为si(p)，例如，对于位置1，编码向量的第5个元素为s5(1)=sin(1/10000(5/d))
- en: 'Why choose this method? By changing the power on 10k you are changing the amplitude
    of the sine function when viewed on the p-axis. And if you have 10 different sine
    functions with 10 different amplitudes, then it will be a long time before you
    get a repetition (i.e. all 10 values are the same) for changing values of p. And
    this helps give us unique values. Now, the actual paper uses both sine and cosine
    functions and the form of encoding is: si(p) = sin (p/10000(i/d)) if i is even
    and si(p) = cos(p/10000(i/d)) if i is odd.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择这种方法？通过改变10的幂次，你会改变正弦函数在p轴上的振幅。如果你有10个不同的正弦函数，每个函数的振幅不同，那么当p的值变化时，过很长时间才能得到重复的值（即所有10个值相同）。这有助于为我们提供唯一的值。实际上，论文中使用了正弦和余弦函数，编码形式是：si(p)
    = sin(p/10000(i/d))，当i为偶数时；si(p) = cos(p/10000(i/d))，当i为奇数时。
