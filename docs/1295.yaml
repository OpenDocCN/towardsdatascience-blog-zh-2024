- en: Are AI Deep Network Models Converging?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23](https://towardsdatascience.com/platonic-representation-hypothesis-c812813d7248?source=collection_archive---------9-----------------------#2024-05-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are artificial intelligence models evolving towards a unified representation
    of reality? The Platonic Representation Hypothesis says ML models are converging.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[![Hesam
    Sheikh](../Images/b8d5f4f285eef77634e4c1d4321580ed.png)](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    [Hesam Sheikh](https://medium.com/@itshesamsheikh?source=post_page---byline--c812813d7248--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c812813d7248--------------------------------)
    ¬∑8 min read¬∑May 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent MIT paper has come to my attention for its impressive claim: AI models
    are converging, even across different modalities ‚Äî vision and language. ‚Äú*We argue
    that representations in AI models, particularly deep networks, are converging*‚Äù
    is how [**The Platonic Representation Hypothesis**](https://arxiv.org/abs/2405.07987)
    paper begins.'
  prefs: []
  type: TYPE_NORMAL
- en: But how can different models, trained on different datasets and for different
    use cases converge? What has led to this convergence?
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ú®This is a paid article. If you‚Äôre not a Medium member, you can read this
    for free in my newsletter:* [***Qiubyte***](https://hesamsheikh.substack.com/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d3f9d3d6b75ce9eeffa8cf92e8fdd3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Plato‚Äôs allegory of the cave by [Jan Saenredam](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:Platon_Cave_Sanraedam_1604.jpg)
    (public domain).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The Platonic Representation Hypothesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We argue that there is a growing similarity in how datapoints are represented
    in different neural network models. This similarity spans across different model
    architectures, training objectives, and even data modalities.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/5da8256363d2d7ad31122d9252187a10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Platonic Representation Hypothesis. The visual representation, **X,** and
    the textual one **Y**, are both projections of a common reality, **Z**. (source:
    [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper‚Äôs central argument is that models of various origins and modalities
    are converging to a *representation of reality* ‚Äî the joint distribution over
    the events of the world that generate the data we observe and use to train the
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The authors argue that this convergence towards a **platonic representation**
    is driven by the underlying structure and nature of the data that models are trained
    on, and by the growing complexity and capability of the models themselves. As
    models encounter various datasets and wider applications, they require a representation
    that captures the fundamental properties commonly found in all data types.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0a103eba7bdc3841ba60e0dd6101975.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An Illustration of The Allegory of the Cave, from Plato‚Äôs Republic (art by
    [4edges](https://commons.wikimedia.org/wiki/User:4edges), source: [Wikipedia](https://en.wikipedia.org/wiki/Allegory_of_the_cave#/media/File:An_Illustration_of_The_Allegory_of_the_Cave,_from_Plato%E2%80%99s_Republic.jpg))'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Are AI Models Converging?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI models of various scales, even built on diverse architecture and trained
    for different tasks, are showing signs of convergence in how they represent data.
    As these models grow in size and complexity and the feeding data becomes larger
    and varied, their methods of processing data begin to *align.*
  prefs: []
  type: TYPE_NORMAL
- en: Do models trained on different data modalities ‚Äî vision or text, also converge?
    The answer could be *yes!*
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Vision Models that Talk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This alignment spans over visual and textual data ‚Äî the paper later confirms
    that the limitations of this theory are that it‚Äôs focused on these two modularities
    and not other modalities such as audio, or robotics perception of the world. One
    of the cases [1] to support this is [***LLaVA***](https://llava-vl.github.io/),
    which shows projecting visual features into language features using a 2-layer
    MLP, resulting in state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55ffd69249f08c5f8411e44c510cb79c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Outline of how LLaVA maps visual features to a Language Model. (source: [LLaVA](https://llava-vl.github.io/3),
    CC-BY)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Language Models that See
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another interesting example is ***A Vision Check-up for Language Models*** [2]
    which explores the extent to which large language models understand and process
    visual data. The study uses code as a bridge between images and text, as a novel
    approach to feed visual data to LLMs. The paper reveals that LLMs can generate
    images by code that while may not look realistic, still contain enough visual
    information to train vision models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5cf01098f7054317c6704cf8e05dd91.png)'
  prefs: []
  type: TYPE_IMG
- en: Can a language model see? ([source](https://arxiv.org/abs/2401.01862))
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Bigger Models, Bigger Alignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The alignment of different models is correlated with their scale. As an example,
    models trained on *CIFAR-10 classification* that are bigger, show greater alignment
    with each other, compared to smaller models. This means that with the current
    trend of building models in the order of 10s and now 100s of billions, these giants
    will be even more aligned.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äúall strong models are alike, each weak model is weak in its own way.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Why are AI Models Converging?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/da754cb83931f36ce44538279e2c757f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The learning process of an AI model, f ‚àó is the trained model, ùêπ F is the function
    class, ùêø L is the loss function depending on the model ùëì f and an input ùë• x from
    the dataset, ùëÖ R represents the regularization function, and ùê∏ E denotes the expectation
    over the dataset. Each color represents one of the causes of this convergence.
    (source: [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In training an AI model, there are elements that contribute most to why AI
    models converge:'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Tasks are Becoming General
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As models are trained to solve tasks that are more and more general simultaneously,
    the size of their solution space becomes smaller and more constrained. More generality
    means trying to learn data points that are closer to *reality*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94dad5d024071616ad980eacb4c22f08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The more tasks a model can solve, it is forced to learn a disjoint representation
    that is useful in solving all of those tasks. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Platonic Representation Hypothesis* paper formulates this as ***The Multitask
    Scaling Hypothesis:***'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúThere are fewer representations that are competent for N tasks than there are
    for M < N tasks. As we train more general models that solve more tasks at once,
    we should expect fewer possible solutions.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, the solution to a complex problem is much more narrow than the
    solution to an easy problem. As we are training models that are more and more
    general on gigantic internet-wide datasets across different modalities, you can
    only imagine how constrained the solution space will be.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Models are Getting Bigger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the capacity of models increases, through more sophisticated architectures,
    larger datasets, or more complex training algorithms, these models develop representations
    that are more similar to each other.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b9e4e58eaeecfa96213276140d77730.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Bigger hypothesis spaces are more likely to converge on a solution, rather
    than smaller spaces. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: While *The Platonic Representation Hypothesis* paper doesn‚Äôt offer proofs or
    examples for this hypothesis that they call **The Capacity Hypothesis ‚Äî** that
    ‚ÄúBigger models are more likely to converge to a shared representation than smaller
    models‚Äù, it seems trivial that bigger models at least have *more capacity* to
    come up with mutual solution spaces than small models.
  prefs: []
  type: TYPE_NORMAL
- en: As AI models scale, thanks to their depth and complexity, they have a greater
    capacity for abstraction. This allows them to capture underlying concepts and
    patterns of the data and wave off noise or outliers, thus arriving at a representation
    that is more generalized and possibly closer to the real world.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 The Simplicity Bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine training two **large-scale** neural networks on two separate tasks:
    one model must be able to recognize faces from images, and another is trained
    to interpret the emotions of faces. Initially, these two tasks might seem unrelated
    ‚Äî but would you be surprised to see both models converge on similar ways of representing
    facial features? After all, it all comes down to an accurate identification and
    interpretation of key facial landmarks (eyes, nose, mouth, etc).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/406bfc4d6cdfd719d909d9d7d8969fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Deep Neural Networks tend towards simpler functions. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several literature points out a tendency of deep neural networks to find simpler
    and more general solutions [3,4,5]. In other words, deep networks favor simple
    solutions. Often called **The Simplicity Bias** the paper formulates it as such:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep networks are biased toward finding simple fits to the data, and the bigger
    the model, the stronger the bias. Therefore, as models get bigger, we should expect
    convergence to a smaller solution space.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why do neural networks show this behavior? Networks show simplicity bias mostly
    because of the fundamental properties of the learning algorithms used to train
    them. Algorithms tend to favor simpler, more generalizable models as a way to
    prevent overfitting and enhance generalization. During training, simpler models
    are more likely to emerge because by capturing the dominant patterns in the data,
    they minimize the loss function more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity bias acts as a natural regulator during training. It pushes models
    toward an optimal way of representing and processing data, which is both general
    across tasks and simple enough to be efficiently learned and applied, and this
    increases the chance of models learning mutual hypothesis spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Implications of This Convergence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So what if models are converging? First of all, this shows that data across
    different modalities can be more useful than thought before. Fine-tuning vision
    models from pre-trained LLMs or vice-versa could yield surprisingly good results.
  prefs: []
  type: TYPE_NORMAL
- en: Another implication pointed out by the paper is that **‚ÄúScaling may reduce hallucination
    and bias‚Äù**. The argument is that as models scale, they can learn from a larger
    and more diverse dataset, which helps them develop a more accurate and robust
    understanding of the world. This enhanced understanding allows them to make predictions
    and generate outputs that are not only more reliable but also less biased.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2165138d4d2f30d11a5c239ac0ba9116.png)'
  prefs: []
  type: TYPE_IMG
- en: 'VISION models converge as COMPETENCE increases. (source: [Paper](https://arxiv.org/abs/2405.07987))'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. A Pinch of Salt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to the arguments posed by the paper, you have to consider some
    limitations, almost all of which are also addressed by the paper as well.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, the paper assumes a **bijective projection of reality** in which one
    real-world concept Z, has projections X and Y that can be learned. However, some
    concepts are uniquely inherent in one modularity. Sometimes, language can express
    a concept or feeling that many images can‚Äôt, and in the same way, language can
    fail to take the place of an image in describing a visual concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, as mentioned before, the paper focuses on two modalities: vision
    and language. Thirdly, the argument that ‚ÄúAI models are Converging‚Äù only holds
    for multi-task AI models and not specific ones, such as ADAS or Sentiment Analysis
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, while the paper shows that the alignment of different models **increases**,
    it doesn‚Äôt indicate the models‚Äô representations become similar. The score of alignment
    between larger models is indeed higher than smaller ones, but still, a score of
    0.16/1.00 leaves some open questions to the research.
  prefs: []
  type: TYPE_NORMAL
- en: '**üåü Join +1000 people learning about** Pythonüêç, ML/MLOps/AIü§ñ, Data Scienceüìà,
    and LLM üóØ'
  prefs: []
  type: TYPE_NORMAL
- en: '[**follow me**](https://medium.com/@itshesamsheikh/subscribe)and check out
    my [**X/Twitter**](https://twitter.com/itsHesamSheikh), where I keep you updated
    Daily**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)
    [## QiuByte | Hesam Sheikh | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: AI, Programming, and Machine Learning, only in the EASY way. Click to read QiuByte,
    by Hesam Sheikh, a Substack‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: hesamsheikh.substack.com](https://hesamsheikh.substack.com/?source=post_page-----c812813d7248--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading,
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äî Hesam
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. In NeurIPS,
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sharma, P., Rott Shaham, T., Baradad, M., Fu, S., Rodriguez-Munoz, A.,
    Duggal, S., Isola, P., and Torralba, A. A vision check-up for language models.
    In arXiv preprint, 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]H. Shah, K. Tamuly, The Pitfalls of Simplicity Bias in Neural Networks,
    2020, [https://arxiv.org/abs/2006.07710](https://arxiv.org/abs/2006.07710)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [A brief note on Simplicity Bias](https://www.lesswrong.com/posts/Gyggp2DJRMRLSnhid/a-brief-note-on-simplicity-bias-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Deep Neural Networks are biased, at initialisation, towards simple functions](/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99)'
  prefs: []
  type: TYPE_NORMAL
