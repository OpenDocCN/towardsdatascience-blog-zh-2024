- en: How to Build a Generative AI Tool for Information Extraction from Receipts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建一个用于从收据中提取信息的生成式 AI 工具
- en: 原文：[https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10](https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10](https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10)
- en: '![](../Images/486a5de0d5063d0a6db0c23a138ff29d.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/486a5de0d5063d0a6db0c23a138ff29d.png)'
- en: DALLE-2’s interpretation of “A futuristic industrial document scanning facility”
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: DALLE-2 对“未来主义的工业文档扫描设施”的诠释
- en: Use LangChain and OpenAI tools to extract structured information from images
    of receipts stored in Google Drive
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LangChain 和 OpenAI 工具从存储在 Google Drive 中的收据图片中提取结构化信息
- en: '[](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    ·15 min read·Apr 10, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    ·阅读时间 15 分钟·2024年4月10日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**This article details how we can use open source Python packages such as LangChain,
    pytesseract and PyPDF, along with gpt-4-vision and gpt-3.5-turbo, to identify
    and extract key information from images of receipts. The resulting dataset could
    be used for a “chat to receipts” application. Check out the full code** [**here**](https://github.com/rmartinshort/receiptchat/tree/main)**.**'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**本文详细介绍了如何使用开源 Python 包，例如 LangChain、pytesseract 和 PyPDF，结合 gpt-4-vision 和
    gpt-3.5-turbo，从收据图片中识别并提取关键信息。生成的数据集可以用于“与收据对话”应用程序。完整代码请查看** [**这里**](https://github.com/rmartinshort/receiptchat/tree/main)**。**'
- en: Paper receipts come in all sorts of styles and formats and represent an interesting
    target for automated information extraction. They also provide a wealth of itemized
    costs that, if aggregated into a database, could be very useful for anyone interested
    in tracking their spend at more detailed level than offered by bank statements.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 纸质收据有各种风格和格式，代表了一个有趣的自动化信息提取目标。它们还提供了大量逐项列出的费用，如果将这些费用汇总到数据库中，将非常有助于那些希望比银行对账单提供的更详细地追踪支出的人。
- en: Wouldn’t it be cool if you could take a photo of a receipt, upload it some application,
    then have its information extracted and appended to your personal database of
    expenses, which you could then query in natural language? You could then ask questions
    of the data like “what did I buy when I last visited IKEA?” or “what items do
    I spend most money on at Safeway”. Such a system might also naturally extend to
    corporate finance and expense tracking. In this article, we’ll build a simple
    application that deals with the first part of this process — namely extracting
    information from receipts ready to be stored in a database. Our system will monitor
    a Google Drive folder for new receipts, process them and append the results to
    a .csv file.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能拍一张收据的照片，上传到某个应用程序，然后提取其信息并将其附加到你的个人支出数据库中，接着可以用自然语言查询这些信息，那不是很酷吗？你可以像“我上次去宜家时买了什么？”或者“我在Safeway花最多钱买了哪些物品？”这样提问。这种系统也可能自然地扩展到公司财务和支出跟踪。在本文中，我们将构建一个简单的应用程序，处理这个过程的第一部分——即提取收据中的信息，准备存储到数据库中。我们的系统将监控一个Google
    Drive文件夹中新添加的收据，处理它们并将结果附加到.csv文件中。
- en: '**1\. Background and motivation**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. 背景和动机**'
- en: Technically, we’ll be doing a type of automated information extraction called
    template filling. We have a pre-defined schema of fields that we want to extract
    from our receipts and the task will be to fill these out, or leave them blank
    where appropriate. One major issue here is that the information contained in images
    or scans of receipts is unstructured, and although Optical Character Recognition
    (OCR) or PDF text extraction libraries might do a decent job at finding the text,
    they are not good preserving the relative positions of words in a document, which
    can make it difficult to match an item’s price to its cost for example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，我们将进行一种自动化的信息提取，称为模板填充。我们有一个预定义的字段架构，旨在从收据中提取这些字段，任务是填写这些字段，或者在适当的地方留空。这里的一个主要问题是，收据图像或扫描中的信息是非结构化的，尽管光学字符识别（OCR）或PDF文本提取库在查找文本时可能做得相当好，但它们在保留文档中单词相对位置方面做得不够好，这可能会使得例如将某个项目的价格与其成本匹配变得困难。
- en: Traditionally, this issue is solved by template matching, where a pre-defined
    geometric template of the document is created and then extraction is only run
    in the areas known to contain important information. A great description of this
    can be found [here](https://aicha-fatrah.medium.com/how-to-extract-information-from-documents-template-matching-e0540ae79599).
    However, this system is inflexible. What if a new format of receipt is added?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，这个问题通过模板匹配来解决，其中会创建一个预定义的文档几何模板，然后只在已知包含重要信息的区域进行提取。关于这个方法的一个很好的描述可以在[这里](https://aicha-fatrah.medium.com/how-to-extract-information-from-documents-template-matching-e0540ae79599)找到。然而，这个系统缺乏灵活性。如果添加了新的收据格式怎么办？
- en: To get around this, more advanced services like [AWS Textract](https://docs.aws.amazon.com/textract/latest/dg/invoices-receipts.html)
    and [AWS Rekognition](https://aws.amazon.com/rekognition/image-features/?nc=sn&loc=3&dn=3&refid=f5358c45-1ee6-4a07-b08d-0f669e6cd865)
    use a combination of pre-trained deep learning models for object detection, bounding
    box generation and named entity recognition (NER). I haven’t actually tried out
    these services on the problem at hand, but it would be really interesting to do
    so in order to compare the results against what we build with OpenAI’s LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，像[AWS Textract](https://docs.aws.amazon.com/textract/latest/dg/invoices-receipts.html)和[AWS
    Rekognition](https://aws.amazon.com/rekognition/image-features/?nc=sn&loc=3&dn=3&refid=f5358c45-1ee6-4a07-b08d-0f669e6cd865)这样的更高级服务使用了预训练的深度学习模型结合物体检测、边界框生成和命名实体识别（NER）。我实际上还没有在手头的问题上尝试过这些服务，但如果能进行尝试，比较它们与我们使用OpenAI的LLMs构建的结果将会非常有趣。
- en: Large Language Models (LLM) such as gpt-3.5-turbo are also great at information
    extraction and template filling from unstructured text, especially after being
    given a few examples in their prompt. This makes them much more flexible than
    template matching or fine-tuning, since adding a few examples of a new receipt
    format is much faster and cheaper than re-training the model or building a new
    geometric template.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 像gpt-3.5-turbo这样的“大型语言模型”（LLM）在从非结构化文本中进行信息提取和模板填充方面也表现出色，尤其是在给定一些示例后。这使得它们比模板匹配或微调更加灵活，因为添加一些新的收据格式示例比重新训练模型或构建新的几何模板要快得多且更便宜。
- en: If we are to use gpt-3.5-turbo on text extracted from a receipts, the question
    then becomes how can we build the examples from which it can learn? We could of
    course do this manually, but that wouldn’t scale well. Here we will explore the
    option of using gpt-4-vision for this. This version of gpt-4 can handle conversations
    that include images, and [appears particularly good at describing the content
    of images](https://medium.com/@nageshmashette32/gpt4-vision-and-its-alternatives-6ed9d39508cd).
    Given an image of a receipt and a description of the key information we want to
    extract, gpt-4-vision should therefore be able to do the job in one shot, providing
    that the image is sufficiently clear.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要在从收据中提取的文本上使用gpt-3.5-turbo，那么问题就是如何构建它可以学习的示例？当然我们可以手动完成，但这样做的扩展性不好。这里我们将探索使用gpt-4-vision的选项。这个版本的gpt-4可以处理包含图像的对话，并且[特别擅长描述图像内容](https://medium.com/@nageshmashette32/gpt4-vision-and-its-alternatives-6ed9d39508cd)。给定一张收据的图像和我们想要提取的关键信息的描述，gpt-4-vision应该能够一举完成任务，前提是图像足够清晰。
- en: Why wouldn’t we just use gpt-4-vision alone for this task and abandon gpt-3.5-turbo
    or other smaller LLMs? Technically we could, and the result might even be more
    accurate. But gpt-4-vision is very expensive and API calls are limited, so this
    system also won’t scale. Perhaps in the not-to-distant future though, vision LLMs
    will become a standard tool in this field of information extraction from documents.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们不单独使用gpt-4-vision来完成这个任务，而是放弃gpt-3.5-turbo或其他较小的LLM？从技术上讲，我们当然可以这样做，而且结果可能会更准确。但gpt-4-vision非常昂贵，且API调用次数有限，因此该系统也无法扩展。或许在不远的未来，视觉LLM会成为从文档中提取信息这一领域的标准工具。
- en: Another motivation for this article is about exploring how we can build this
    system using Langchain, a popular open source LLM orchestration library. In order
    to force an LLM to return structured output, prompt engineering is required and
    Langchain has some excellent tools for this. We will also try to ensure that our
    system is built in a way that is extensible, because this is just the first part
    of what could become a larger “chat to receipts” project.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的另一个动机是探索如何使用Langchain构建这个系统，Langchain是一个流行的开源LLM编排库。为了强制LLM返回结构化输出，需要进行提示工程，而Langchain有一些非常棒的工具可以做到这一点。我们还将尝试确保我们的系统构建得具有可扩展性，因为这只是可能成为更大“与收据对话”项目的第一部分。
- en: With a brief background out of the way, lets get started with the code! I will
    be using Python3.9 and Langchain 0.1.14 here, and full details can be found in
    the [repo](https://github.com/rmartinshort/receiptchat/tree/main).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 简要的背景介绍完毕后，让我们开始写代码吧！在这里，我将使用Python3.9和Langchain 0.1.14，完整的细节可以在[仓库](https://github.com/rmartinshort/receiptchat/tree/main)中找到。
- en: '**2\. Connect to Google Drive**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2. 连接到Google Drive**'
- en: We need a convenient place to store our raw receipt data. Google Drive is one
    choice, and it provides a Python API that is relatively easy to use. To capture
    the receipts I use the [GeniusScan](https://thegrizzlylabs.com/genius-scan/) app,
    which can upload .pdf, .jpeg or other file types from the phone directly to a
    Google Drive folder. The app also does some useful pre-processing such as automatic
    document cropping, which helps with the extraction process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个方便的地方来存储我们的原始收据数据。Google Drive是一个选择，它提供了相对易于使用的Python API。为了捕捉收据，我使用了[GeniusScan](https://thegrizzlylabs.com/genius-scan/)应用，它可以将.pdf、.jpeg或其他文件类型从手机直接上传到Google
    Drive文件夹。该应用还进行一些有用的预处理，如自动裁剪文档，这有助于提取过程。
- en: To set up API access to Google Drive, you’ll need to create service account
    credentials which can be generated by following the instructions [here](https://developers.google.com/drive/api/quickstart/python).
    For reference, I created a folder in my drive called “receiptchat” and set up
    a key pair that enables reading of data from that folder.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置Google Drive的API访问权限，您需要创建服务账户凭据，可以按照[这里](https://developers.google.com/drive/api/quickstart/python)的说明生成。作为参考，我在我的驱动器中创建了一个名为“receiptchat”的文件夹，并设置了一个密钥对，使其能够读取该文件夹中的数据。
- en: The following code can be used to set up a drive service object, which gives
    you access to various methods to query Google Drive
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以用来设置一个驱动服务对象，它提供了访问各种方法来查询Google Drive的功能：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our simple application, we only really need to do two things: List all the
    files in the drive folder and download some list of them. The following class
    handles this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单应用中，我们实际上只需要做两件事：列出驱动器文件夹中的所有文件，并下载其中一些文件。以下类处理了这一点：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Running this gives the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个代码会得到以下结果：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Great! So now we can connect to Google Drive and bring image or pdf data onto
    our local machine. Next, we must process it and extract text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！现在我们可以连接到 Google Drive，并将图像或 PDF 数据导入本地机器。接下来，我们必须处理这些数据并提取文本。
- en: 3\. Extract raw text from .pdfs and images
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 从 PDF 和图像中提取原始文本
- en: Multiple well-documented open source libraries exist to extract raw text from
    pdfs and images. For pdfs we will use `PyPDF` here, although for a more comprehensive
    view of similar packages I recommend this [article](/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517).
    For images in jpeg format, we will make use of `pytesseract` , which is a wrapper
    for the `tesseract` OCR engine. Installation instructions for that can be found
    [here](https://tesseract-ocr.github.io/tessdoc/Installation.html). Finally, we
    also want to be able to convert pdfs into jpeg format. This can be done with the
    `pdf2image` package.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个文档化良好的开源库可以从 PDF 和图像中提取原始文本。对于 PDF，我们将在这里使用 `PyPDF`，尽管为了更全面地了解类似的包，我推荐这篇[文章](/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517)。对于
    JPEG 格式的图像，我们将使用 `pytesseract`，它是 `tesseract` OCR 引擎的包装器。关于如何安装它的说明可以在[这里](https://tesseract-ocr.github.io/tessdoc/Installation.html)找到。最后，我们还希望能够将
    PDF 转换为 JPEG 格式。这可以通过 `pdf2image` 包实现。
- en: Both `PyPDF` and `pytesseract` provide high level methods for extraction of
    text from documents. They both also have options for tuning this. `pytesseract`
    , for example, can extract both text and boundary boxes (see [here](https://pypi.org/project/pytesseract/)),
    which may be of useful in future if we want to feed the LLM more information about
    the format of the receipt whose text its processing. `pdf2image` provides a method
    to convert pdf bytes to jpeg image, which is exactly what we want to do here.
    To convert jpeg bytes to an image that can be visualized, we’ll use the `PIL`
    package.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyPDF` 和 `pytesseract` 都提供了从文档中提取文本的高级方法。它们也都提供了调优的选项。例如，`pytesseract` 可以提取文本和边界框（请参见[这里](https://pypi.org/project/pytesseract/)），如果我们将来想要为
    LLM 提供有关其处理的收据文本格式的更多信息，这可能会很有用。`pdf2image` 提供了一种将 PDF 字节转换为 JPEG 图像的方法，这正是我们在这里想做的。为了将
    JPEG 字节转换为可视化的图像，我们将使用 `PIL` 包。'
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code above uses the concept of abstract base classes to improve extensibility.
    Lets say we want to add support for another file type in future. If we write the
    associated class and inherit from `FileBytesToImage` , we are forced to write
    `convert_bytes_to_image` and `convert_bytes_to_text` methods in that. This makes
    it less likely that our classes will introduce errors downstream in a large application.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码使用了抽象基类的概念来提高可扩展性。假设我们未来想要添加对另一种文件类型的支持。如果我们编写相关的类并从`FileBytesToImage`继承，就必须在其中编写`convert_bytes_to_image`和`convert_bytes_to_text`方法。这可以减少我们在大型应用程序中引入错误的可能性。
- en: 'The code can be used as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以如下使用：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/9eeed45beddda0c8af8e2d5deaf3917c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9eeed45beddda0c8af8e2d5deaf3917c.png)'
- en: Example of text extracted from a pdf document using the code above. Since receipts
    contain PII, here we are just demonstrating with a random document uploaded to
    the Google Drive. Image generated by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码从 PDF 文档中提取的文本示例。由于收据包含个人身份信息（PII），这里我们仅使用上传到 Google Drive 的随机文档进行演示。图像由作者生成。
- en: 4\. Information extraction with gpt-4-vision
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 使用 gpt-4-vision 提取信息
- en: Now let’s use Langchain to prompt gpt-4-vision to extract some information from
    our receipts. We can start by using Langchain’s support for Pydantic to create
    a model for the output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Langchain 提示 gpt-4-vision 从我们的收据中提取一些信息。我们可以通过使用 Langchain 对 Pydantic
    的支持来创建一个输出模型。
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is very powerful because Langchain can use this Pydantic model to construct
    format instructions for the LLM, which can be included in the prompt to force
    it to produce a json output with the specified fields. Adding new fields is as
    straightforward as just updating the model class.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常强大，因为 Langchain 可以使用这个 Pydantic 模型为 LLM 构建格式说明，这些说明可以包含在提示中，强制它生成带有指定字段的
    JSON 输出。添加新字段就像更新模型类一样简单。
- en: 'Next, let’s build the prompt, which will just be static:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建提示，这将只是静态的：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we need to build a class that will take in an image and send it to the
    LLM along with the prompt and format instructions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要构建一个类，该类将接收图像，并将其与提示和格式说明一起发送给 LLM。
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The main method to understand here is `set_up_chain` , which we will walk through
    step by step. These steps were inspired by this [blog post](https://medium.com/@bpothier/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要理解的主要方法是 `set_up_chain`，我们将逐步讲解这些步骤。这些步骤的灵感来自于这篇 [博客文章](https://medium.com/@bpothier/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215)。
- en: Initialize the prompt, which in this case is just a block of text with some
    general instructions
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化提示，在这种情况下，它只是一个包含一些通用指令的文本块。
- en: Create a `JsonOutputParser` from the Pydantic model we made above. This converts
    the model into a set of formatting instructions that can be added to the prompt
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从我们上面创建的 Pydantic 模型中创建一个 `JsonOutputParser`。这将把模型转换为一组格式化指令，能够添加到提示中。
- en: Make a `TransformChain` that allows us to incorporate custom functions — in
    this case the `load_image` function — into the overall chain. Note that the chain
    will take in a variable called `image_path` and output a variable called `image`
    , which is a base64-encoded string representing the image. This is one of the
    formats accepted by gpt-4-vision.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 `TransformChain`，允许我们将自定义函数（在这种情况下是 `load_image` 函数）集成到整体链中。请注意，链将接受一个名为
    `image_path` 的变量，并输出一个名为 `image` 的变量，这是一个表示图像的 base64 编码字符串。这是 gpt-4-vision 接受的格式之一。
- en: To the best of my knowledge, `ChatOpenAI` doesn’t yet natively support sending
    both text and images. Therefore, we need to make a custom chain that invokes the
    instance of `ChatOpenAI` we made with the encoded image, prompt and formatting
    instructions.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 据我所知，`ChatOpenAI` 目前还不支持同时发送文本和图像。因此，我们需要创建一个自定义链，将我们创建的 `ChatOpenAI` 实例与编码图像、提示和格式化指令一起调用。
- en: Note that we’re also making use of openai callbacks to count the tokens and
    spend associated with each call.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还利用了 openai 回调函数来统计每次调用的令牌数和相关的支出。
- en: 'To run this, we can do the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个，我们可以按以下步骤操作：
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Given our random document above, the result looks like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们上面的随机文档，结果如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Not too exciting, but at least its structured in the correct way! When a valid
    receipt is provided, these fields are filled out and my assessment from running
    a few tests on different receipts it that its very accurate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不太令人兴奋，但至少它按照正确的方式进行了结构化！当提供有效的收据时，这些字段将被填写，并且根据我对不同收据进行的一些测试评估，结果非常准确。
- en: 'Our callbacks look like this:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的回调函数如下所示：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is essential for tracking costs, which can quickly grow during testing
    of a model like gpt-4.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是跟踪成本所必需的，因为在测试像 gpt-4 这样的模型时，成本可能会迅速增长。
- en: 5\. Information extraction with gpt-3.5-turbo
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 使用 gpt-3.5-turbo 进行信息提取
- en: Let’s assume that we’ve used the steps in part 4 to generate some examples and
    saved them as a json file. Each example consists of some extracted text and corresponding
    key information as defined by our `ReceiptInformation` Pydantic model. Now, we
    want to inject these examples into a call to gpt-3.5-turbo, in the hope that it
    can generalize what it learns from them to a new receipt. Few-shot learning is
    a powerful tool in prompt engineering and, if it works, would be great for this
    use case because whenever a new format of receipt is detected we can generate
    one example using gpt-4-vision and append it to the list of examples used to prompt
    gpt-3.5-turbo. Then when a similarly formatted receipt comes along, gpt-3.5-turbo
    can be used to extract its content. In a way this is like template matching, but
    without the need to manually define the template.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经使用第 4 部分中的步骤生成了一些示例并将它们保存为 json 文件。每个示例包含一些提取的文本和根据我们的 `ReceiptInformation`
    Pydantic 模型定义的相应关键信息。现在，我们想将这些示例注入到对 gpt-3.5-turbo 的调用中，期望它能将从这些示例中学到的东西推广到新的收据。少量示例学习是提示工程中的一种强大工具，如果它有效，对于这个用例将非常有用，因为每当检测到新的收据格式时，我们可以使用
    gpt-4-vision 生成一个示例，并将其附加到用于提示 gpt-3.5-turbo 的示例列表中。然后，当出现类似格式的收据时，可以使用 gpt-3.5-turbo
    来提取其内容。从某种程度上说，这就像模板匹配，但不需要手动定义模板。
- en: There are many ways to encourage text based LLMs to extract structured information
    from a block of text. One of the newest and most powerful that I’ve found is [here](https://python.langchain.com/docs/use_cases/extraction/how_to/examples/)
    in the Langchain documentation. The idea is to create a prompt that contains a
    placeholder for some examples, then inject the examples into the prompt as if
    they were being returned by some function that the LLM had called. This is done
    with the `model.with_structured_output()` functionality, which you can read about
    [here](https://python.langchain.com/docs/modules/model_io/chat/structured_output/).
    Note that this is currently in beta and so might change!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以促使基于文本的 LLM 从一段文本中提取结构化信息。我发现的最新且最强大的方法之一可以在 Langchain 文档的 [此处](https://python.langchain.com/docs/use_cases/extraction/how_to/examples/)
    找到。其思路是创建一个包含占位符的提示语，用于存放一些示例，然后将这些示例注入到提示语中，就像它们是通过某个 LLM 调用的函数返回的一样。这是通过 `model.with_structured_output()`
    功能实现的，您可以在 [此处](https://python.langchain.com/docs/modules/model_io/chat/structured_output/)
    阅读相关信息。请注意，目前这个功能还处于测试阶段，可能会发生变化！
- en: Let’s look at the code to see how this is achieved. We’ll first write the prompt.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下代码，了解这是如何实现的。我们将首先编写提示语。
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The prompt text is exactly the same as it was in section 4, only we now have
    a `MessagesPlaceholder` to hold the examples that we’re going to insert.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 提示语文本与第 4 节中的完全相同，只是我们现在有了一个 `MessagesPlaceholder` 来保存我们即将插入的示例。
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`TextReceiptExtractionChain` is going to take in a list of examples, each of
    which has `input` and `output` keys (note how these are used in the `set_up_examples`
    method). For each example, we will make a `ReceiptInformation` object. Then we
    format the result into a list of messages that can be passed into the prompt.
    All the work in `tool_examples_to_messages` is there just to convert between different
    Langchain formats.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextReceiptExtractionChain` 将接受一个包含多个示例的列表，每个示例都有 `input` 和 `output` 键（注意这些键是如何在
    `set_up_examples` 方法中使用的）。对于每个示例，我们将创建一个 `ReceiptInformation` 对象。然后，我们将结果格式化为一个消息列表，可以传递给提示语。`tool_examples_to_messages`
    中的所有工作只是为了在不同的 Langchain 格式之间进行转换。'
- en: 'Running this looks very similar to what we did with the vision model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码与我们之前使用视觉模型时非常相似：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Even with 10 examples, this call is less than half the cost of the gpt-4-vision
    and also alot faster to return. As more examples get added, you may need to use
    gpt-3.5-turbo-16k to avoid exceeding the context window.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是 10 个示例，这个调用的成本也不到 gpt-4-vision 的一半，而且返回速度也更快。随着示例数量的增加，您可能需要使用 gpt-3.5-turbo-16k，以避免超出上下文窗口。
- en: The output dataset
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出数据集
- en: Having collected some receipts, you can run the extraction methods described
    in sections 4 and 5 and collect the result in a dataframe. This then gets stored
    and can be appended to whenever a new receipt appears in the Google Drive.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 收集了一些收据后，您可以运行第 4 和第 5 节中描述的提取方法，并将结果收集到一个数据框中。然后，它将被存储，并且每当 Google Drive 中出现新的收据时，可以将其附加到这个数据框中。
- en: '![](../Images/07908e8c252f1d406db696da159899c3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07908e8c252f1d406db696da159899c3.png)'
- en: Sample of the output dataset, showing fields extracted from multiple receipts.
    Image generated by the author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数据集的示例，显示了从多个收据中提取的字段。图片由作者生成
- en: Once my database of extracted receipt information grows a bit larger, I plan
    to explore LLM-based question answering on top of it, so look out for that article
    soon! I’m also curious about exploring a more formal evaluation method for this
    project and comparing the results to what can be obtained via AWS Textract or
    similar products.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我的提取的收据信息数据库变得更大一些，我计划在此基础上探索基于 LLM 的问答功能，敬请期待相关的文章！我还对为这个项目探索一种更正式的评估方法并将其与
    AWS Textract 或类似产品的结果进行比较感到好奇。
- en: Thanks for making it to the end! Please feel free to explore the full codebase
    here [https://github.com/rmartinshort/receiptchat](https://github.com/rmartinshort/receiptchat).
    Any suggestions for improvement or extensions to the functionality would be much
    appreciated!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您看到最后！请随时在此处探索完整的代码库 [https://github.com/rmartinshort/receiptchat](https://github.com/rmartinshort/receiptchat)。任何关于改进或扩展功能的建议都将不胜感激！
