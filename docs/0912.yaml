- en: How to Build a Generative AI Tool for Information Extraction from Receipts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10](https://towardsdatascience.com/how-to-build-a-generative-ai-tool-for-information-extraction-from-receipts-516424327f66?source=collection_archive---------2-----------------------#2024-04-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/486a5de0d5063d0a6db0c23a138ff29d.png)'
  prefs: []
  type: TYPE_IMG
- en: DALLE-2’s interpretation of “A futuristic industrial document scanning facility”
  prefs: []
  type: TYPE_NORMAL
- en: Use LangChain and OpenAI tools to extract structured information from images
    of receipts stored in Google Drive
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[![Robert
    Martin-Short](../Images/e3910071b72a914255b185b850579a5a.png)](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    [Robert Martin-Short](https://medium.com/@rmartinshort?source=post_page---byline--516424327f66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--516424327f66--------------------------------)
    ·15 min read·Apr 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**This article details how we can use open source Python packages such as LangChain,
    pytesseract and PyPDF, along with gpt-4-vision and gpt-3.5-turbo, to identify
    and extract key information from images of receipts. The resulting dataset could
    be used for a “chat to receipts” application. Check out the full code** [**here**](https://github.com/rmartinshort/receiptchat/tree/main)**.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Paper receipts come in all sorts of styles and formats and represent an interesting
    target for automated information extraction. They also provide a wealth of itemized
    costs that, if aggregated into a database, could be very useful for anyone interested
    in tracking their spend at more detailed level than offered by bank statements.
  prefs: []
  type: TYPE_NORMAL
- en: Wouldn’t it be cool if you could take a photo of a receipt, upload it some application,
    then have its information extracted and appended to your personal database of
    expenses, which you could then query in natural language? You could then ask questions
    of the data like “what did I buy when I last visited IKEA?” or “what items do
    I spend most money on at Safeway”. Such a system might also naturally extend to
    corporate finance and expense tracking. In this article, we’ll build a simple
    application that deals with the first part of this process — namely extracting
    information from receipts ready to be stored in a database. Our system will monitor
    a Google Drive folder for new receipts, process them and append the results to
    a .csv file.
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Background and motivation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically, we’ll be doing a type of automated information extraction called
    template filling. We have a pre-defined schema of fields that we want to extract
    from our receipts and the task will be to fill these out, or leave them blank
    where appropriate. One major issue here is that the information contained in images
    or scans of receipts is unstructured, and although Optical Character Recognition
    (OCR) or PDF text extraction libraries might do a decent job at finding the text,
    they are not good preserving the relative positions of words in a document, which
    can make it difficult to match an item’s price to its cost for example.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, this issue is solved by template matching, where a pre-defined
    geometric template of the document is created and then extraction is only run
    in the areas known to contain important information. A great description of this
    can be found [here](https://aicha-fatrah.medium.com/how-to-extract-information-from-documents-template-matching-e0540ae79599).
    However, this system is inflexible. What if a new format of receipt is added?
  prefs: []
  type: TYPE_NORMAL
- en: To get around this, more advanced services like [AWS Textract](https://docs.aws.amazon.com/textract/latest/dg/invoices-receipts.html)
    and [AWS Rekognition](https://aws.amazon.com/rekognition/image-features/?nc=sn&loc=3&dn=3&refid=f5358c45-1ee6-4a07-b08d-0f669e6cd865)
    use a combination of pre-trained deep learning models for object detection, bounding
    box generation and named entity recognition (NER). I haven’t actually tried out
    these services on the problem at hand, but it would be really interesting to do
    so in order to compare the results against what we build with OpenAI’s LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLM) such as gpt-3.5-turbo are also great at information
    extraction and template filling from unstructured text, especially after being
    given a few examples in their prompt. This makes them much more flexible than
    template matching or fine-tuning, since adding a few examples of a new receipt
    format is much faster and cheaper than re-training the model or building a new
    geometric template.
  prefs: []
  type: TYPE_NORMAL
- en: If we are to use gpt-3.5-turbo on text extracted from a receipts, the question
    then becomes how can we build the examples from which it can learn? We could of
    course do this manually, but that wouldn’t scale well. Here we will explore the
    option of using gpt-4-vision for this. This version of gpt-4 can handle conversations
    that include images, and [appears particularly good at describing the content
    of images](https://medium.com/@nageshmashette32/gpt4-vision-and-its-alternatives-6ed9d39508cd).
    Given an image of a receipt and a description of the key information we want to
    extract, gpt-4-vision should therefore be able to do the job in one shot, providing
    that the image is sufficiently clear.
  prefs: []
  type: TYPE_NORMAL
- en: Why wouldn’t we just use gpt-4-vision alone for this task and abandon gpt-3.5-turbo
    or other smaller LLMs? Technically we could, and the result might even be more
    accurate. But gpt-4-vision is very expensive and API calls are limited, so this
    system also won’t scale. Perhaps in the not-to-distant future though, vision LLMs
    will become a standard tool in this field of information extraction from documents.
  prefs: []
  type: TYPE_NORMAL
- en: Another motivation for this article is about exploring how we can build this
    system using Langchain, a popular open source LLM orchestration library. In order
    to force an LLM to return structured output, prompt engineering is required and
    Langchain has some excellent tools for this. We will also try to ensure that our
    system is built in a way that is extensible, because this is just the first part
    of what could become a larger “chat to receipts” project.
  prefs: []
  type: TYPE_NORMAL
- en: With a brief background out of the way, lets get started with the code! I will
    be using Python3.9 and Langchain 0.1.14 here, and full details can be found in
    the [repo](https://github.com/rmartinshort/receiptchat/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Connect to Google Drive**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need a convenient place to store our raw receipt data. Google Drive is one
    choice, and it provides a Python API that is relatively easy to use. To capture
    the receipts I use the [GeniusScan](https://thegrizzlylabs.com/genius-scan/) app,
    which can upload .pdf, .jpeg or other file types from the phone directly to a
    Google Drive folder. The app also does some useful pre-processing such as automatic
    document cropping, which helps with the extraction process.
  prefs: []
  type: TYPE_NORMAL
- en: To set up API access to Google Drive, you’ll need to create service account
    credentials which can be generated by following the instructions [here](https://developers.google.com/drive/api/quickstart/python).
    For reference, I created a folder in my drive called “receiptchat” and set up
    a key pair that enables reading of data from that folder.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be used to set up a drive service object, which gives
    you access to various methods to query Google Drive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In our simple application, we only really need to do two things: List all the
    files in the drive folder and download some list of them. The following class
    handles this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Great! So now we can connect to Google Drive and bring image or pdf data onto
    our local machine. Next, we must process it and extract text.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Extract raw text from .pdfs and images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple well-documented open source libraries exist to extract raw text from
    pdfs and images. For pdfs we will use `PyPDF` here, although for a more comprehensive
    view of similar packages I recommend this [article](/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517).
    For images in jpeg format, we will make use of `pytesseract` , which is a wrapper
    for the `tesseract` OCR engine. Installation instructions for that can be found
    [here](https://tesseract-ocr.github.io/tessdoc/Installation.html). Finally, we
    also want to be able to convert pdfs into jpeg format. This can be done with the
    `pdf2image` package.
  prefs: []
  type: TYPE_NORMAL
- en: Both `PyPDF` and `pytesseract` provide high level methods for extraction of
    text from documents. They both also have options for tuning this. `pytesseract`
    , for example, can extract both text and boundary boxes (see [here](https://pypi.org/project/pytesseract/)),
    which may be of useful in future if we want to feed the LLM more information about
    the format of the receipt whose text its processing. `pdf2image` provides a method
    to convert pdf bytes to jpeg image, which is exactly what we want to do here.
    To convert jpeg bytes to an image that can be visualized, we’ll use the `PIL`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The code above uses the concept of abstract base classes to improve extensibility.
    Lets say we want to add support for another file type in future. If we write the
    associated class and inherit from `FileBytesToImage` , we are forced to write
    `convert_bytes_to_image` and `convert_bytes_to_text` methods in that. This makes
    it less likely that our classes will introduce errors downstream in a large application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9eeed45beddda0c8af8e2d5deaf3917c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of text extracted from a pdf document using the code above. Since receipts
    contain PII, here we are just demonstrating with a random document uploaded to
    the Google Drive. Image generated by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Information extraction with gpt-4-vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s use Langchain to prompt gpt-4-vision to extract some information from
    our receipts. We can start by using Langchain’s support for Pydantic to create
    a model for the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is very powerful because Langchain can use this Pydantic model to construct
    format instructions for the LLM, which can be included in the prompt to force
    it to produce a json output with the specified fields. Adding new fields is as
    straightforward as just updating the model class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s build the prompt, which will just be static:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we need to build a class that will take in an image and send it to the
    LLM along with the prompt and format instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The main method to understand here is `set_up_chain` , which we will walk through
    step by step. These steps were inspired by this [blog post](https://medium.com/@bpothier/generating-structured-data-from-an-image-with-gpt-vision-and-langchain-34aaf3dcb215).
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the prompt, which in this case is just a block of text with some
    general instructions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a `JsonOutputParser` from the Pydantic model we made above. This converts
    the model into a set of formatting instructions that can be added to the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make a `TransformChain` that allows us to incorporate custom functions — in
    this case the `load_image` function — into the overall chain. Note that the chain
    will take in a variable called `image_path` and output a variable called `image`
    , which is a base64-encoded string representing the image. This is one of the
    formats accepted by gpt-4-vision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of my knowledge, `ChatOpenAI` doesn’t yet natively support sending
    both text and images. Therefore, we need to make a custom chain that invokes the
    instance of `ChatOpenAI` we made with the encoded image, prompt and formatting
    instructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we’re also making use of openai callbacks to count the tokens and
    spend associated with each call.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run this, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Given our random document above, the result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Not too exciting, but at least its structured in the correct way! When a valid
    receipt is provided, these fields are filled out and my assessment from running
    a few tests on different receipts it that its very accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our callbacks look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is essential for tracking costs, which can quickly grow during testing
    of a model like gpt-4.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Information extraction with gpt-3.5-turbo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s assume that we’ve used the steps in part 4 to generate some examples and
    saved them as a json file. Each example consists of some extracted text and corresponding
    key information as defined by our `ReceiptInformation` Pydantic model. Now, we
    want to inject these examples into a call to gpt-3.5-turbo, in the hope that it
    can generalize what it learns from them to a new receipt. Few-shot learning is
    a powerful tool in prompt engineering and, if it works, would be great for this
    use case because whenever a new format of receipt is detected we can generate
    one example using gpt-4-vision and append it to the list of examples used to prompt
    gpt-3.5-turbo. Then when a similarly formatted receipt comes along, gpt-3.5-turbo
    can be used to extract its content. In a way this is like template matching, but
    without the need to manually define the template.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to encourage text based LLMs to extract structured information
    from a block of text. One of the newest and most powerful that I’ve found is [here](https://python.langchain.com/docs/use_cases/extraction/how_to/examples/)
    in the Langchain documentation. The idea is to create a prompt that contains a
    placeholder for some examples, then inject the examples into the prompt as if
    they were being returned by some function that the LLM had called. This is done
    with the `model.with_structured_output()` functionality, which you can read about
    [here](https://python.langchain.com/docs/modules/model_io/chat/structured_output/).
    Note that this is currently in beta and so might change!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the code to see how this is achieved. We’ll first write the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The prompt text is exactly the same as it was in section 4, only we now have
    a `MessagesPlaceholder` to hold the examples that we’re going to insert.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`TextReceiptExtractionChain` is going to take in a list of examples, each of
    which has `input` and `output` keys (note how these are used in the `set_up_examples`
    method). For each example, we will make a `ReceiptInformation` object. Then we
    format the result into a list of messages that can be passed into the prompt.
    All the work in `tool_examples_to_messages` is there just to convert between different
    Langchain formats.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this looks very similar to what we did with the vision model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Even with 10 examples, this call is less than half the cost of the gpt-4-vision
    and also alot faster to return. As more examples get added, you may need to use
    gpt-3.5-turbo-16k to avoid exceeding the context window.
  prefs: []
  type: TYPE_NORMAL
- en: The output dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having collected some receipts, you can run the extraction methods described
    in sections 4 and 5 and collect the result in a dataframe. This then gets stored
    and can be appended to whenever a new receipt appears in the Google Drive.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07908e8c252f1d406db696da159899c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample of the output dataset, showing fields extracted from multiple receipts.
    Image generated by the author
  prefs: []
  type: TYPE_NORMAL
- en: Once my database of extracted receipt information grows a bit larger, I plan
    to explore LLM-based question answering on top of it, so look out for that article
    soon! I’m also curious about exploring a more formal evaluation method for this
    project and comparing the results to what can be obtained via AWS Textract or
    similar products.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for making it to the end! Please feel free to explore the full codebase
    here [https://github.com/rmartinshort/receiptchat](https://github.com/rmartinshort/receiptchat).
    Any suggestions for improvement or extensions to the functionality would be much
    appreciated!
  prefs: []
  type: TYPE_NORMAL
