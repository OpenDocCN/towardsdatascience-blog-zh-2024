- en: What Does the Transformer Architecture Tell Us?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/what-does-the-transformer-architecture-tell-us-cd3a4fd6a59d?source=collection_archive---------1-----------------------#2024-07-25](https://towardsdatascience.com/what-does-the-transformer-architecture-tell-us-cd3a4fd6a59d?source=collection_archive---------1-----------------------#2024-07-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://jshen9889.medium.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)[![Stephanie
    Shen](../Images/857cccbe84f0d3a9886c84acfbbac03e.png)](https://jshen9889.medium.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)
    [Stephanie Shen](https://jshen9889.medium.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cd3a4fd6a59d--------------------------------)
    ·14 min read·Jul 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e6b426d63204ba7ecd56058e51ba1a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [narciso1](https://pixabay.com/users/narciso1-608227/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=639303)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=639303)
  prefs: []
  type: TYPE_NORMAL
- en: The stellar performance of large language models (LLMs) such as ChatGPT has
    shocked the world. The breakthrough was made by the invention of the Transformer
    architecture, which is surprisingly simple and scalable. It is still built of
    deep learning neural networks. The main addition is the so-called “attention”
    mechanism that contextualizes each word token. Moreover, its unprecedented parallelisms
    endow LLMs with massive scalability and, therefore, impressive accuracy after
    training over billions of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity that the Transformer architecture has demonstrated is, in fact,
    comparable to the Turing machine. The difference is that the Turing machine controls
    what the machine can do at each step. The Transformer, however, is like a magic
    black box, learning from massive input data through parameter optimizations. Researchers
    and scientists are still intensely interested in discovering its potential and
    any theoretical implications for studying the human mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will first discuss the four main features of the Transformer
    architecture: word embedding, attention mechanism, single-word prediction, and
    generalization capabilities such as multi-modal extension and transferred learning.
    The intention is to focus on why the architecture is so effective instead of how
    to build it (for which readers can find many…'
  prefs: []
  type: TYPE_NORMAL
