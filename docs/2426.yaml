- en: 'RAG 101: Chunking Strategies'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG 101：分块策略
- en: 原文：[https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05](https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05](https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05)
- en: UNLOCK THE FULL POTENTIAL OF YOUR RAG WORKFLOW
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解锁你 RAG 工作流的全部潜力
- en: Why, When, and How to chunk for enhanced RAG
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么、何时以及如何对数据进行分块以增强 RAG
- en: '[](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Shanmukha
    Ranganath](../Images/b362e4a5eb87077e889cb1db702955b3.png)](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    [Shanmukha Ranganath](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Shanmukha
    Ranganath](../Images/b362e4a5eb87077e889cb1db702955b3.png)](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    [Shanmukha Ranganath](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    ·12 min read·Oct 5, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    ·12 分钟阅读·2024年10月5日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/393f81ef2eda31b1e0e61b987efc5154.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/393f81ef2eda31b1e0e61b987efc5154.png)'
- en: How do we split the balls? (Generated using Cava)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将数据分块？（使用 Cava 生成）
- en: The maximum number of tokens that a Large Language Model can process in a single
    request is known as context length (or context window). The table below shows
    the [context length for all versions of GPT-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    (as of Sep 2024). While context lengths have been increasing with every iteration
    and with every newer model, there remains a limit to the information we can provide
    the model. Moreover, there is an inverse correlation between the size of the input
    and the context relevancy of the responses generated by the LLM, short and focused
    inputs produce better results than long contexts containing vast information.
    This emphasizes the importance of breaking down our data into smaller, relevant
    chunks to ensure more appropriate responses from the LLMs—at least until LLMs
    can handle enormous amounts of data without re-training.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在单次请求中能够处理的最大令牌数被称为上下文长度（或上下文窗口）。下表显示了[所有版本的 GPT-4 的上下文长度](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)（截至
    2024年9月）。虽然随着每次迭代和每个新模型的发布，上下文长度在不断增加，但我们仍然面临向模型提供信息的限制。此外，输入大小与由 LLM 生成的响应的上下文相关性之间存在反向关系，简短而集中的输入比包含大量信息的长上下文产生更好的结果。这突显了将数据分解为更小、更相关的块的重要性，以确保从
    LLM 获得更合适的响应——至少在 LLM 能够处理大量数据而无需重新训练之前。
- en: The Context Window represented in the image is **inclusive of both input and
    output tokens.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图中表示的上下文窗口是**包括输入和输出令牌的。**
