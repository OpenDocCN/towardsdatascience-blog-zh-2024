- en: 'RAG 101: Chunking Strategies'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05](https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec?source=collection_archive---------1-----------------------#2024-10-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: UNLOCK THE FULL POTENTIAL OF YOUR RAG WORKFLOW
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Why, When, and How to chunk for enhanced RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Shanmukha
    Ranganath](../Images/b362e4a5eb87077e889cb1db702955b3.png)](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    [Shanmukha Ranganath](https://memsranga.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fdc6f6c2aaec--------------------------------)
    ·12 min read·Oct 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/393f81ef2eda31b1e0e61b987efc5154.png)'
  prefs: []
  type: TYPE_IMG
- en: How do we split the balls? (Generated using Cava)
  prefs: []
  type: TYPE_NORMAL
- en: The maximum number of tokens that a Large Language Model can process in a single
    request is known as context length (or context window). The table below shows
    the [context length for all versions of GPT-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    (as of Sep 2024). While context lengths have been increasing with every iteration
    and with every newer model, there remains a limit to the information we can provide
    the model. Moreover, there is an inverse correlation between the size of the input
    and the context relevancy of the responses generated by the LLM, short and focused
    inputs produce better results than long contexts containing vast information.
    This emphasizes the importance of breaking down our data into smaller, relevant
    chunks to ensure more appropriate responses from the LLMs—at least until LLMs
    can handle enormous amounts of data without re-training.
  prefs: []
  type: TYPE_NORMAL
- en: The Context Window represented in the image is **inclusive of both input and
    output tokens.**
  prefs: []
  type: TYPE_NORMAL
