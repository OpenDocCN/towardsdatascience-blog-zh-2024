- en: 'Explaining OpenAI Sora’s Spacetime Patches: The Key Ingredient'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解读OpenAI Sora的时空补丁：关键成分
- en: 原文：[https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16](https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16](https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16)
- en: Under The Hood Of The Generative AI For Video By OpenAI
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解读OpenAI生成视频AI的核心技术
- en: '[](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[![Vincent
    Koc](../Images/6cbe2dab3c452384057fbdb7a16506be.png)](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    [Vincent Koc](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[![Vincent
    Koc](../Images/6cbe2dab3c452384057fbdb7a16506be.png)](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    [Vincent Koc](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    ·6 min read·Feb 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    ·6分钟阅读·2024年2月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/696e47de6e132862f8813fed2c0d613c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/696e47de6e132862f8813fed2c0d613c.png)'
- en: How can AI transform a static image into a dynamic, realistic video? OpenAI’s
    Sora introduces an answer through the innovative use of spacetime patches.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI如何将一张静态图片转变为动态、真实的视频？OpenAI的Sora通过创新地使用时空补丁给出了答案。
- en: In the rapidly evolving landscape of generative models, [OpenAI’s Sora](https://openai.com/sora)
    stands out as a significant milestone, promising to reshape our understanding
    and capabilities in video generation. We unpack the [technology behind Sora](https://openai.com/research/video-generation-models-as-world-simulators)
    and its potential to inspire a new generation of models in image, video, and 3D
    content creation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的生成模型领域，[OpenAI的Sora](https://openai.com/sora)作为一个重要的里程碑脱颖而出，承诺重塑我们对视频生成的理解和能力。我们解读了[关于Sora的技术](https://openai.com/research/video-generation-models-as-world-simulators)以及它在图像、视频和3D内容创作中启发新一代模型的潜力。
- en: 'OpenAI Sosa Demo — Cat on Bed. Credit: OpenAI'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Sosa演示 — 床上的猫。版权归OpenAI所有。
- en: 'The demo above was generated by OpenAI using the prompt: *A cat waking up its
    sleeping owner demanding breakfast. The owner tries to ignore the cat, but the
    cat tries new tactics and finally the owner pulls out a secret stash of treats
    from under the pillow to hold the cat off a little longer. —* With Sora we verge
    onto near indistinguishable realism with video content generation. The full model
    is yet to be fully released to the public as its undergoing testing.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的演示是OpenAI使用以下提示生成的：*一只猫把睡着的主人弄醒，要求早餐。主人试图忽视这只猫，但猫采取了新策略，最终主人从枕头下拿出一个秘密的零食藏匿处，稍微拖延了些时间。*——借助Sora，我们几乎能够生成与现实难以区分的视频内容。完整的模型尚未完全公开，因为它还在测试阶段。
- en: How Sora’s Unique Approach Transforms Video Generation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sora独特方法如何变革视频生成
- en: In the world of generative models we have seen a number of approaches from GAN’s
    to auto-regressive, and diffusion models, all with their own strengths and limitations.
    Sora now introduces a paradigm shift with a new modelling techniques and flexibility
    to handle a broad range of duration's, aspect ratios, and resolutions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成模型的世界中，我们已经看到了一些方法，从GAN到自回归模型，再到扩散模型，它们各自有自己的优点和局限性。Sora现在通过一种新的建模技术和灵活性，带来了范式的转变，能够处理各种持续时间、纵横比和分辨率。
- en: 'Sora combines both diffusion and transformer architectures together to create
    a diffusion transformer model and is able to provide features such as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Sora将扩散模型和变换器架构结合在一起，创造了一个扩散变换器模型，能够提供如下特性：
- en: '**Text-to-video**: *As we have seen*'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本转视频：** *正如我们所见*'
- en: '**Image-to-video:** Bringing life to still images'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像转视频：** 让静态图像充满生气'
- en: '**Video-to-video:** Changing the style of video to something else'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频转视频：** 改变视频的风格为其他样式'
- en: '**Extending video in time:** Forwards and backwards'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频时间扩展：** 向前和向后'
- en: '**Create seamless loops:** Tiled videos that seem like they never end'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建无缝循环：** 瓦片视频，看起来似乎永无止境'
- en: '**Image generation:** Still image is a movie of one frame (*up to 2048 x 2048*)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像生成：** 静态图像是单帧的电影（*最大2048 x 2048*）'
- en: '**Generate video in any format:** From 1920 x 1080 to 1080 x 1920 and everything
    in between'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以任何格式生成视频：** 从1920 x 1080到1080 x 1920及其间的所有格式'
- en: '**Simulate virtual worlds:** Like Minecraft and other video games'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模拟虚拟世界：** 如Minecraft和其他视频游戏'
- en: '**Create a video:** Up to 1 minute in length with multiple shorts'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建视频：** 最长 1 分钟，包含多个短视频'
- en: Imagine for one moment you’re in a kitchen. The traditional video generation
    models like those from [Pika](https://pika.art/home) and [RunwayML](https://runwayml.com/ai-tools/gen-2/)
    a like the cooks that follow recipes to the letter. They can produce excellent
    dishes (*videos*) but are limited by the recipes (*algorithms*) they know. The
    cooks might specialize in baking cakes (*short clips*) or cooking pasta (*specific
    types of videos*), using specific ingredients (*data formats*) and techniques
    (*model architectures*).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你现在在厨房里。像[皮卡](https://pika.art/home)和[RunwayML](https://runwayml.com/ai-tools/gen-2/)等传统的视频生成模型就像是严格按照食谱做菜的厨师。他们可以做出精美的菜肴（*视频*），但受到他们知道的食谱（*算法*）的限制。这些厨师可能专注于做蛋糕（*短视频*）或者做意大利面（*特定类型的视频*），使用特定的食材（*数据格式*）和技巧（*模型架构*）。
- en: Sora, on the other hand, is a new kind of chef who understand the fundamentals
    of flavor. This chef doesn’t just follow recipes; they invent new ones. The flexibility
    of Sora’s ingredients (*data*) and techniques (*model architecture*) is what allow
    Sora to produce a wide range of high-quality videos, akin to a master chef’s versatile
    culinary creations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 而Sora则是一种新型厨师，理解味道的基础。这个厨师不仅仅遵循食谱，而是发明新的食谱。Sora的食材（*数据*）和技巧（*模型架构*）的灵活性使得Sora能够制作出各种高质量的视频，就像大厨能创造出多样的美味佳肴。
- en: 'The Core of Sora’s Secret Ingredient: Exploring the Spacetime Patches'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sora秘密配方的核心：探索时空补丁
- en: Spacetime patches are at the heart of Sora’s innovation, built on the earlier
    research from [Google DeepMind on NaViT](https://arxiv.org/abs/2307.06304) and
    ViT (*Vision Transformers*) based on the 2021 paper [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时空补丁是Sora创新的核心，基于[Google DeepMind在NaViT上的研究](https://arxiv.org/abs/2307.06304)以及基于2021年论文[《一幅图像值16x16个单词》](https://arxiv.org/abs/2010.11929)的ViT（*视觉变换器*）。
- en: '![](../Images/86a56bfe41bcfd43b7545eb7a2c02f72.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86a56bfe41bcfd43b7545eb7a2c02f72.png)'
- en: '*“Vanilla”* Vision Transformer Architecture — Credit [Dosovitskiy et al., 2021](https://arxiv.org/abs/2010.11929)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*“原生”* 视觉变换器架构 — 版权归[Dosovitskiy等人，2021](https://arxiv.org/abs/2010.11929)'
- en: Traditionally with Vision Transformers we use a sequence of images “patches”
    to train a transformer model for image recognition instead of words for language
    transformers. The patches allow us to move away from convolutional neural networks
    for image processing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，使用视觉变换器时，我们使用一系列图像“补丁”来训练变换器模型进行图像识别，而不是像语言变换器那样使用单词。这些补丁使我们能够摆脱卷积神经网络来进行图像处理。
- en: '![](../Images/908472615a28faa5775319f399da1d0b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/908472615a28faa5775319f399da1d0b.png)'
- en: How frames/images are “patch-ified” — Credit [Dehghani et al., 2023](https://arxiv.org/abs/2307.06304)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 帧/图像是如何被“补丁化”的 — 版权归[Dehghani等人，2023](https://arxiv.org/abs/2307.06304)
- en: However with vision transformers were constraint on image training data that
    was fixed in size and aspect ratio which limited the quality and required vast
    amounts of preprocessing of images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用视觉变换器时，图像训练数据的大小和长宽比是固定的，这限制了质量，并且需要大量的图像预处理。
- en: '![](../Images/23af45258db5008a735e3d941c624e9b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23af45258db5008a735e3d941c624e9b.png)'
- en: 'Visualization of Slicing Video Temporal Data — Source: [kitasenjudesign](https://twitter.com/kitasenjudesign/status/1489260985135157258)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 视频时间数据切片的可视化 — 来源：[kitasenjudesign](https://twitter.com/kitasenjudesign/status/1489260985135157258)
- en: By treating videos as sequences of patches, Sora maintains the original aspect
    ratios and resolutions, similar to NaViT’s handling of images. **This preservation
    is crucial for capturing the true essence of the visual data, enabling the model
    to learn from a more accurate representation of the world and thus giving Sora
    its near magical accuracy.**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将视频视为拼接片段的序列，Sora保持了原始的纵横比和分辨率，类似于NaViT处理图像的方式。**这种保持原始特征的做法对捕捉视觉数据的真实本质至关重要，使得模型能够从更为准确的世界表示中学习，从而赋予Sora近乎魔法般的准确性。**
- en: '![](../Images/d6dcaefaeaae1752d3da13a123191004.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6dcaefaeaae1752d3da13a123191004.png)'
- en: 'Visualization of Spacetime Patching (Processing) — Credit: OpenAI (Sora)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 时空拼接（处理）的可视化 — 图片来源：OpenAI（Sora）
- en: The method allows Sora to efficiently process a diverse array of visual data
    without the need for pre-processing steps like resizing or padding. This flexibility
    ensures that every piece of data contributes to the model’s understanding, much
    like how a chef uses a variety of ingredients to enhance a dish’s flavor profile.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使Sora能够高效地处理各种各样的视觉数据，无需进行像调整大小或填充这样的预处理步骤。这种灵活性确保了每一条数据都能为模型的理解做出贡献，类似于大厨如何利用多种食材来提升一道菜肴的风味。
- en: The detailed and flexible handling of video data through spacetime patches lays
    the groundwork for sophisticated features such as accurate physics simulation
    and 3D consistency. These capabilities are essential for creating videos that
    not only look realistic but also adhere to the physical rules of the world, offering
    a glimpse into the potential for AI to create complex, dynamic visual content.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 通过时空拼接对视频数据的细致和灵活处理，为复杂功能奠定了基础，例如精确的物理模拟和3D一致性。这些能力对于创造不仅看起来逼真，而且符合物理规则的视频至关重要，为AI创造复杂且动态的视觉内容提供了一个展望。
- en: 'Feeding Sora: The Role of Diverse Data in Training'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 供养Sora：多样化数据在训练中的作用
- en: The quality and diversity of training data are crucial for the performance of
    generative models. Existing video models were traditionally trained on a more
    restrictive set of data, shorter lengths and narrow target.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据的质量和多样性对生成模型的表现至关重要。现有的视频模型通常在一个更为有限的数据集上进行训练，这些数据集的长度较短且目标较窄。
- en: Sora leverages a vast and varied dataset, including videos and images of different
    durations, resolutions, and aspect ratios. [It’s ability to re-create digital
    worlds like Minecraft](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/),
    its likely also included gameplay and simulated world footage from systems such
    as Unreal or Unity in its training set in order to capture all the angles and
    various styles of video content. This brings Sora to a “generalist” model just
    like GPT-4 for text.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Sora利用了庞大而多样化的数据集，包括不同时长、分辨率和纵横比的视频和图像。[它重现数字世界的能力，如Minecraft](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/)，其训练集可能还包括来自Unreal或Unity等系统的游戏玩法和模拟世界的镜头，以捕捉视频内容的各个角度和不同风格。这使得Sora成为一个“通用型”模型，类似于GPT-4在文本领域的作用。
- en: This extensive training enables Sora to understand complex dynamics and generate
    content that is both diverse and high in quality. The approach mimics the way
    large language models are trained on diverse text data, applying a similar philosophy
    to visual content to achieve generalist capabilities.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种广泛的训练使得Sora能够理解复杂的动态，并生成既多样又高质量的内容。这种方法模仿了大规模语言模型在多样化文本数据上的训练理念，将类似的哲学应用于视觉内容，从而实现了通用能力。
- en: '![](../Images/e151e5585f1180958834629a5986a95f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e151e5585f1180958834629a5986a95f.png)'
- en: Variable “Patches” NaVit vs. Traditional Vision Transformers — Credit [Dehghani
    et al., 2023](https://arxiv.org/abs/2307.06304)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可变“拼接”NaViT与传统视觉变换器的对比 — 图片来源：[Dehghani et al., 2023](https://arxiv.org/abs/2307.06304)
- en: Just as the NaViT model demonstrates significant training efficiency and performance
    gains by packing multiple patches from different images into single sequences,
    Sora leverages spacetime patches to achieve similar efficiencies in video generation.
    This approach allows for more effective learning from a vast dataset, improving
    the model’s ability to generate high-fidelity videos yet lowering the compute
    required versus existing modeling architectures.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如NaViT模型通过将来自不同图像的多个拼接片段打包成单一序列，从而展示出显著的训练效率和性能提升，Sora则利用时空拼接实现视频生成中的类似效率。这种方法使得模型能够更有效地从庞大的数据集中进行学习，提升生成高保真视频的能力，同时相比现有的建模架构，减少了所需的计算量。
- en: 'Bringing the Physical World to Life: Sora’s Mastery over 3D and Continuity'
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让物理世界栩栩如生：Sora 对 3D 和连贯性的掌控
- en: 3D space and object permanence is one of the key standouts in the demo’s by
    Sora. Through its training on a wide range of video data without adapting or preprocessing
    the videos, Sora learns to model the physical world with impressive accuracy as
    its able to consume the training data in its original form.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 三维空间和物体持久性是 Sora 演示中的关键亮点之一。通过在不对视频进行适配或预处理的情况下，使用广泛的视频数据进行训练，Sora 能够以惊人的准确性建模物理世界，因为它能够以原始形式消耗这些训练数据。
- en: It can generate digital worlds and videos where objects and characters move
    and interact in three-dimensional space convincingly, maintaining coherence even
    when they are occluded or leave the frame.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以生成数字世界和视频，其中物体和角色在三维空间中移动并互动，看起来十分逼真，即使它们被遮挡或离开画面，依然能保持连贯性。
- en: '**Looking Ahead: The Future Implications of Sora**'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**展望未来：Sora 的未来影响**'
- en: Sora sets a new standard for what’s possible in generative models. This approach,
    much is likely to inspire the open-source community to experiment with and advance
    the capabilities in visual modalities, fueling a new generation of generative
    models that push the boundaries of creativity and realism.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 为生成模型设定了一个新的标准。这种方法可能会激励开源社区进行实验，并推动视觉模态的能力发展，推动新一代生成模型的诞生，突破创造力和现实主义的边界。
- en: The journey of Sora is just beginning, and as OpenAI put’s it “scaling video
    generation models is a promising path towards building general purpose simulators
    of the physical world”
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sora 的旅程才刚刚开始，正如 OpenAI 所说：“扩大视频生成模型是构建物理世界通用模拟器的有希望的路径。”
- en: Sora’s approach, blending the latest in AI research with practical applications,
    signals a bright future for generative models. As these technologies continue
    to evolve, they promise to redefine our interactions with digital content, making
    the creation of high-fidelity, dynamic videos more accessible and versatile.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 的方法，结合了最新的 AI 研究与实际应用，预示着生成模型的光明未来。随着这些技术的不断发展，它们有望重新定义我们与数字内容的互动，使创建高保真、动态视频变得更加容易和多样化。
- en: Enjoyed This Story?
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？
- en: Vincent Koc is a highly accomplished, commercially-focused technologist and
    futurist with a wealth of experience focused in data-driven and digital disciplines.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Vincent Koc 是一位非常成功、注重商业的技术专家和未来学家，拥有丰富的经验，专注于数据驱动和数字化领域。
- en: '[Subscribe for free](https://medium.com/subscribe/@vkoc) to get notified when
    Vincent publishes a new story. Or follow him on [LinkedIn](https://www.linkedin.com/in/koconder/)
    and [X](https://twitter.com/koconder).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[免费订阅](https://medium.com/subscribe/@vkoc)，以便在 Vincent 发布新故事时收到通知。或者在 [LinkedIn](https://www.linkedin.com/in/koconder/)
    和 [X](https://twitter.com/koconder/) 上关注他。'
- en: '[](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
    [## Get an email whenever Vincent Koc publishes.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
    [## 每当 Vincent Koc 发布新内容时，您将收到电子邮件通知。'
- en: Get an email whenever Vincent Koc publishes. By signing up, you will create
    a Medium account if you don't already have…
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当 Vincent Koc 发布新内容时，您将收到电子邮件通知。通过注册，您将创建一个 Medium 账户（如果您还没有的话）…
- en: medium.com](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
- en: '*Unless otherwise noted, all images are by the author*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均为作者提供*'
