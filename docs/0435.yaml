- en: 'Explaining OpenAI Sora’s Spacetime Patches: The Key Ingredient'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16](https://towardsdatascience.com/explaining-openai-soras-spacetime-patches-the-key-ingredient-e14e0703ec5b?source=collection_archive---------0-----------------------#2024-02-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Under The Hood Of The Generative AI For Video By OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[![Vincent
    Koc](../Images/6cbe2dab3c452384057fbdb7a16506be.png)](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    [Vincent Koc](https://medium.com/@vincentkoc?source=post_page---byline--e14e0703ec5b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e14e0703ec5b--------------------------------)
    ·6 min read·Feb 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/696e47de6e132862f8813fed2c0d613c.png)'
  prefs: []
  type: TYPE_IMG
- en: How can AI transform a static image into a dynamic, realistic video? OpenAI’s
    Sora introduces an answer through the innovative use of spacetime patches.
  prefs: []
  type: TYPE_NORMAL
- en: In the rapidly evolving landscape of generative models, [OpenAI’s Sora](https://openai.com/sora)
    stands out as a significant milestone, promising to reshape our understanding
    and capabilities in video generation. We unpack the [technology behind Sora](https://openai.com/research/video-generation-models-as-world-simulators)
    and its potential to inspire a new generation of models in image, video, and 3D
    content creation.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI Sosa Demo — Cat on Bed. Credit: OpenAI'
  prefs: []
  type: TYPE_NORMAL
- en: 'The demo above was generated by OpenAI using the prompt: *A cat waking up its
    sleeping owner demanding breakfast. The owner tries to ignore the cat, but the
    cat tries new tactics and finally the owner pulls out a secret stash of treats
    from under the pillow to hold the cat off a little longer. —* With Sora we verge
    onto near indistinguishable realism with video content generation. The full model
    is yet to be fully released to the public as its undergoing testing.'
  prefs: []
  type: TYPE_NORMAL
- en: How Sora’s Unique Approach Transforms Video Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of generative models we have seen a number of approaches from GAN’s
    to auto-regressive, and diffusion models, all with their own strengths and limitations.
    Sora now introduces a paradigm shift with a new modelling techniques and flexibility
    to handle a broad range of duration's, aspect ratios, and resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sora combines both diffusion and transformer architectures together to create
    a diffusion transformer model and is able to provide features such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text-to-video**: *As we have seen*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image-to-video:** Bringing life to still images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video-to-video:** Changing the style of video to something else'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extending video in time:** Forwards and backwards'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create seamless loops:** Tiled videos that seem like they never end'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image generation:** Still image is a movie of one frame (*up to 2048 x 2048*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate video in any format:** From 1920 x 1080 to 1080 x 1920 and everything
    in between'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simulate virtual worlds:** Like Minecraft and other video games'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a video:** Up to 1 minute in length with multiple shorts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine for one moment you’re in a kitchen. The traditional video generation
    models like those from [Pika](https://pika.art/home) and [RunwayML](https://runwayml.com/ai-tools/gen-2/)
    a like the cooks that follow recipes to the letter. They can produce excellent
    dishes (*videos*) but are limited by the recipes (*algorithms*) they know. The
    cooks might specialize in baking cakes (*short clips*) or cooking pasta (*specific
    types of videos*), using specific ingredients (*data formats*) and techniques
    (*model architectures*).
  prefs: []
  type: TYPE_NORMAL
- en: Sora, on the other hand, is a new kind of chef who understand the fundamentals
    of flavor. This chef doesn’t just follow recipes; they invent new ones. The flexibility
    of Sora’s ingredients (*data*) and techniques (*model architecture*) is what allow
    Sora to produce a wide range of high-quality videos, akin to a master chef’s versatile
    culinary creations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Core of Sora’s Secret Ingredient: Exploring the Spacetime Patches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spacetime patches are at the heart of Sora’s innovation, built on the earlier
    research from [Google DeepMind on NaViT](https://arxiv.org/abs/2307.06304) and
    ViT (*Vision Transformers*) based on the 2021 paper [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86a56bfe41bcfd43b7545eb7a2c02f72.png)'
  prefs: []
  type: TYPE_IMG
- en: '*“Vanilla”* Vision Transformer Architecture — Credit [Dosovitskiy et al., 2021](https://arxiv.org/abs/2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally with Vision Transformers we use a sequence of images “patches”
    to train a transformer model for image recognition instead of words for language
    transformers. The patches allow us to move away from convolutional neural networks
    for image processing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/908472615a28faa5775319f399da1d0b.png)'
  prefs: []
  type: TYPE_IMG
- en: How frames/images are “patch-ified” — Credit [Dehghani et al., 2023](https://arxiv.org/abs/2307.06304)
  prefs: []
  type: TYPE_NORMAL
- en: However with vision transformers were constraint on image training data that
    was fixed in size and aspect ratio which limited the quality and required vast
    amounts of preprocessing of images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23af45258db5008a735e3d941c624e9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualization of Slicing Video Temporal Data — Source: [kitasenjudesign](https://twitter.com/kitasenjudesign/status/1489260985135157258)'
  prefs: []
  type: TYPE_NORMAL
- en: By treating videos as sequences of patches, Sora maintains the original aspect
    ratios and resolutions, similar to NaViT’s handling of images. **This preservation
    is crucial for capturing the true essence of the visual data, enabling the model
    to learn from a more accurate representation of the world and thus giving Sora
    its near magical accuracy.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6dcaefaeaae1752d3da13a123191004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualization of Spacetime Patching (Processing) — Credit: OpenAI (Sora)'
  prefs: []
  type: TYPE_NORMAL
- en: The method allows Sora to efficiently process a diverse array of visual data
    without the need for pre-processing steps like resizing or padding. This flexibility
    ensures that every piece of data contributes to the model’s understanding, much
    like how a chef uses a variety of ingredients to enhance a dish’s flavor profile.
  prefs: []
  type: TYPE_NORMAL
- en: The detailed and flexible handling of video data through spacetime patches lays
    the groundwork for sophisticated features such as accurate physics simulation
    and 3D consistency. These capabilities are essential for creating videos that
    not only look realistic but also adhere to the physical rules of the world, offering
    a glimpse into the potential for AI to create complex, dynamic visual content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feeding Sora: The Role of Diverse Data in Training'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The quality and diversity of training data are crucial for the performance of
    generative models. Existing video models were traditionally trained on a more
    restrictive set of data, shorter lengths and narrow target.
  prefs: []
  type: TYPE_NORMAL
- en: Sora leverages a vast and varied dataset, including videos and images of different
    durations, resolutions, and aspect ratios. [It’s ability to re-create digital
    worlds like Minecraft](https://techcrunch.com/2024/02/15/openais-sora-video-generating-model-can-render-video-games-too/),
    its likely also included gameplay and simulated world footage from systems such
    as Unreal or Unity in its training set in order to capture all the angles and
    various styles of video content. This brings Sora to a “generalist” model just
    like GPT-4 for text.
  prefs: []
  type: TYPE_NORMAL
- en: This extensive training enables Sora to understand complex dynamics and generate
    content that is both diverse and high in quality. The approach mimics the way
    large language models are trained on diverse text data, applying a similar philosophy
    to visual content to achieve generalist capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e151e5585f1180958834629a5986a95f.png)'
  prefs: []
  type: TYPE_IMG
- en: Variable “Patches” NaVit vs. Traditional Vision Transformers — Credit [Dehghani
    et al., 2023](https://arxiv.org/abs/2307.06304)
  prefs: []
  type: TYPE_NORMAL
- en: Just as the NaViT model demonstrates significant training efficiency and performance
    gains by packing multiple patches from different images into single sequences,
    Sora leverages spacetime patches to achieve similar efficiencies in video generation.
    This approach allows for more effective learning from a vast dataset, improving
    the model’s ability to generate high-fidelity videos yet lowering the compute
    required versus existing modeling architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bringing the Physical World to Life: Sora’s Mastery over 3D and Continuity'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 3D space and object permanence is one of the key standouts in the demo’s by
    Sora. Through its training on a wide range of video data without adapting or preprocessing
    the videos, Sora learns to model the physical world with impressive accuracy as
    its able to consume the training data in its original form.
  prefs: []
  type: TYPE_NORMAL
- en: It can generate digital worlds and videos where objects and characters move
    and interact in three-dimensional space convincingly, maintaining coherence even
    when they are occluded or leave the frame.
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking Ahead: The Future Implications of Sora**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sora sets a new standard for what’s possible in generative models. This approach,
    much is likely to inspire the open-source community to experiment with and advance
    the capabilities in visual modalities, fueling a new generation of generative
    models that push the boundaries of creativity and realism.
  prefs: []
  type: TYPE_NORMAL
- en: The journey of Sora is just beginning, and as OpenAI put’s it “scaling video
    generation models is a promising path towards building general purpose simulators
    of the physical world”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sora’s approach, blending the latest in AI research with practical applications,
    signals a bright future for generative models. As these technologies continue
    to evolve, they promise to redefine our interactions with digital content, making
    the creation of high-fidelity, dynamic videos more accessible and versatile.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vincent Koc is a highly accomplished, commercially-focused technologist and
    futurist with a wealth of experience focused in data-driven and digital disciplines.
  prefs: []
  type: TYPE_NORMAL
- en: '[Subscribe for free](https://medium.com/subscribe/@vkoc) to get notified when
    Vincent publishes a new story. Or follow him on [LinkedIn](https://www.linkedin.com/in/koconder/)
    and [X](https://twitter.com/koconder).'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
    [## Get an email whenever Vincent Koc publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Vincent Koc publishes. By signing up, you will create
    a Medium account if you don't already have…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/subscribe/@vkoc?source=post_page-----e14e0703ec5b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
