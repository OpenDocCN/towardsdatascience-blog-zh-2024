<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building a User Insights-Gathering Tool for Product Managers from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building a User Insights-Gathering Tool for Product Managers from Scratch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-user-insights-gathering-tool-for-product-managers-from-scratch-a6459dc1c3f6?source=collection_archive---------8-----------------------#2024-08-12">https://towardsdatascience.com/building-a-user-insights-gathering-tool-for-product-managers-from-scratch-a6459dc1c3f6?source=collection_archive---------8-----------------------#2024-08-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fa26" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Say goodbye to paid customer insights hubs! Learn how to combine five open-source AI models to automate insights gathering from your user interviews.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@zaninihugo?source=post_page---byline--a6459dc1c3f6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hugo Zanini" class="l ep by dd de cx" src="../Images/cbda793f1cf82f34b29c6e136556361d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*PoiKI7fYD2QC4cNjitBb4g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a6459dc1c3f6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@zaninihugo?source=post_page---byline--a6459dc1c3f6--------------------------------" rel="noopener follow">Hugo Zanini</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a6459dc1c3f6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/6094d50facdc92f35218923622bab796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpbiyAxQEBlH72z6-8gzIQ.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@epicantus?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Daria Nepriakhina </a>on <a class="af nb" href="https://unsplash.com/photos/printed-sticky-notes-glued-on-board-zoCDWPuiRuA?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1e40" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a technical product manager for a data platform, I frequently run user interviews to identify challenges associated with data development processes.</p><p id="6e33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, when exploring a new problem area with users, I can easily become overwhelmed by the numerous conversations I have with various individuals across the organization.</p><p id="76b5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Over time, I have adopted a systematic approach to address this challenge. I focus on taking comprehensive notes during each interview and then revisit them. This allows me to consolidate my understanding and identify user discussion patterns.</p><p id="dc12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, dividing my attention between note-taking and active listening often compromised the quality of my conversations. I noticed that when someone else took notes for me, my interviews significantly improved. This allowed me to fully engage with the interviewees, concentrate solely on what they were saying, and have more meaningful and productive interactions.</p><p id="0d0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To be more efficient, I transitioned from taking notes during meetings to recording and transcribing them whenever the functionality was available. This significantly reduced the number of interviews I needed to conduct, as I could gain more insights from fewer conversations. However, this change required me to invest time reviewing transcriptions and watching videos. The figure below shows what became a simplified flow of the process I follow for mapping a new product development opportunity.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ny"><img src="../Images/98af683776e7bdb22a3114f9e803ed9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kKwObiHU9U_aPY67"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="0261" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Due to the size of the meeting transcriptions and difficulties in categorizing user insights, consolidation and analysis became challenging.</strong></p><p id="6638" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Furthermore, the meeting transcription tools available to me were limited to English, while most of my conversations were in Portuguese. As a result, I decided to look to the market for a solution that could help me with those challenges.</p><p id="6d1b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The solutions I found that solved most of my pain points were <a class="af nb" href="https://dovetail.com/" rel="noopener ugc nofollow" target="_blank">Dovetail</a>, <a class="af nb" href="https://heymarvin.com/" rel="noopener ugc nofollow" target="_blank">Marvin</a>, <a class="af nb" href="https://condens.io/" rel="noopener ugc nofollow" target="_blank">Condens</a>, and <a class="af nb" href="https://reduct.video/" rel="noopener ugc nofollow" target="_blank">Reduct</a>. They position themselves as customer insights hubs, and their main product is generally Customer Interview transcriptions.</p><p id="338c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Basically, you can upload an interview video there, and you are going to receive a transcription indicating who is speaking with hyperlinks to the original video on every phrase. Over the text, you can add highlights, tags, and comments and ask for a summary of the conversation. <strong class="ne fr">These features would solve my problem; however, these tools are expensive, especially considering that I live in Brazil and they charge in dollars.</strong></p><blockquote class="nz oa ob"><p id="cb84" class="nc nd oc ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">The tools offer nothing revolutionary, so I decided to implement an open-source alternative that would run for free on Colab notebooks.</strong></p></blockquote><h2 id="54e3" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">The requirements</h2><p id="08bb" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">As a good PM, the first thing I did was identify the must-haves for my product based on the user's (my) needs. Here are the high-level requirements I mapped:</p><p id="48d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Cost and Accessibility</strong></p><ul class=""><li id="1c0e" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk">Free;</li><li id="527c" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">No coding experience is required to use;</li></ul><p id="4495" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Data Privacy and Security</strong></p><ul class=""><li id="c2fe" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk">Keep data private — no connection with external services;</li></ul><p id="8211" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Performance</strong></p><ul class=""><li id="f6e3" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk">Execution must be faster than the video duration;</li><li id="b64b" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">High-precision transcription in different languages;</li></ul><p id="0706" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Functionality</strong></p><ul class=""><li id="ea9f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk">Speakers identification;</li><li id="53f3" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">Easy to search over the transcriptions;</li><li id="c034" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">Easy to highlight the transcriptions;</li><li id="f63f" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">Easy to create repositories of research being done;</li></ul><p id="d935" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Integration</strong></p><ul class=""><li id="6202" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pd pe pf bk">Integrated with my company's existing tools (Google Workspace);</li><li id="7c3f" class="nc nd fq ne b go pg ng nh gr ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx pd pe pf bk">LLM model integrated to receive prompts for tasks on top of the transcriptions;</li></ul><h2 id="9ce2" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">The solution</h2><p id="058c" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">With the requirements, I designed what would be the features of my solution:</p></div></div><div class="mq"><div class="ab cb"><div class="ll pl lm pm ln pn cf po cg pp ci bh"><figure class="ml mm mn mo mp mq pr ps paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pq"><img src="../Images/92381377517963db8d4c326de1666e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Y9KI77PhDCu_7OHX6JFkwg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="47a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, I projected what would be the expected inputs and the user interfaces to define them:</p></div></div><div class="mq"><div class="ab cb"><div class="ll pl lm pm ln pn cf po cg pp ci bh"><figure class="ml mm mn mo mp mq pr ps paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/2cd00c7a1d251e1fb7d75824b24ef412.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*OX_Efh1DoQWCZLnFrYHtUA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0d23" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Users will upload their interview to YouTube as an unlisted video and create a Google Drive folder to store the transcription. They will then access a Google Colab notebook to provide basic information about the interview, paste the video URL, and optionally define tasks for an LLM model. The output will be Google Docs, where they can consolidate insights.</p><p id="3470" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below is the product architecture. The solution required combining five different ML models and some Python libraries. The next sections provide overviews of each of the building blocks; however, if you are more interested in trying the product, please go to the "I got it" section.</p></div></div><div class="mq"><div class="ab cb"><div class="ll pl lm pm ln pn cf po cg pp ci bh"><figure class="ml mm mn mo mp mq pr ps paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pu"><img src="../Images/96168c1df8b7d3ba902a9a547c92e61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zMmv0aPtsPLM_Q8muL3MKQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="86d4" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Interview Setup and Video Upload</h2><p id="6b87" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">To create a user-friendly interface for setting up interviews and providing video links, I used <a class="af nb" href="https://colab.research.google.com/notebooks/forms.ipynb?ref=dataschool.io#scrollTo=ig8PIYeLtM8g" rel="noopener ugc nofollow" target="_blank">Google Colab’s forms functionality</a>. This allows for the creation of text fields, sliders, dropdowns, and more. The code is hidden behind the form, making it very accessible for non-technical users.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/89dea50e1e495b9369553a667507f74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QupHpXm_eyx8Q-2TIb8qmA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Interview selection form — Image by author</figcaption></figure><h2 id="2c99" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Audio download and conversion</h2><p id="38c1" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">I used the yt-dlp lib to download only the audio from a YouTube video and convert it to the mp3 format. It is very straightforward to use, and you can <a class="af nb" href="https://github.com/yt-dlp/yt-dlp" rel="noopener ugc nofollow" target="_blank">check its documentation here</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/4c47572c2a50d6190d0dac82e9bbbae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adZivV_QtcMgStHTXShnAA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by <a class="af nb" href="https://github.com/yt-dlp/yt-dlp" rel="noopener ugc nofollow" target="_blank">yt-dlp</a></figcaption></figure><h2 id="1232" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Audio transcription</h2><p id="65ee" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">To transcribe the meeting, I used <a class="af nb" href="https://github.com/openai/whisper" rel="noopener ugc nofollow" target="_blank">Whisper from Open AI</a>. It is an open-source model for speech recognition trained on more than 680K hours of multilingual data.</p><p id="c61c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model runs incredibly fast; a one-hour audio clip takes around 6 minutes to be transcribed on a 16GB T4 GPU (offered by free on Google Colab), and it supports <a class="af nb" href="https://github.com/openai/whisper/blob/ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab/whisper/tokenizer.py#L11-L110" rel="noopener ugc nofollow" target="_blank">99 different languages</a>.</p><p id="070d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since privacy is a requirement for the solution, the model weights are downloaded, and all the inference occurs inside the colab instance. I also added a Model Selection form in the notebook so the user can choose different models based on the precision they are looking for.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj px"><img src="../Images/12597ca463c3d2bcaf8f4ae25e6d8dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZawmIEwy1mMA16V5izj9HQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="b01e" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Speakers Identification</h2><p id="ca4d" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Speaker identification is done through a technique called Speakers Diarization. The idea is to identify and segment audio into distinct speech segments, where each segment corresponds to a particular speaker. With that, we can identify who spoke and when.</p><p id="8dde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since the videos uploaded from YouTube don't have metadata identifying who is speaking, the speakers will be divided into Speaker 1, Speaker 2, etc.… Later, the user can find and replace those names in Google Docs to add the speakers’ identification.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj py"><img src="../Images/f8b66de2615683dd7210b7c4d1be7a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fr086sIOgyvdBd775xXhqA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="f8e9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the diarization, we will use a model called the <a class="af nb" href="https://arxiv.org/pdf/2203.15974" rel="noopener ugc nofollow" target="_blank">Multi-Scale Diarization Decoder (MSDD)</a>, which was developed by Nvidia researchers. It is a sophisticated approach to speaker diarization that leverages multi-scale analysis and dynamic weighting to achieve high accuracy and flexibility.</p><p id="53fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model is known for being quite good at identifying and properly categorizing moments where multiple speakers are talking—a thing that occurs frequently during interviews.</p><p id="3b73" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The model can be used through the <a class="af nb" href="https://github.com/NVIDIA/NeMo" rel="noopener ugc nofollow" target="_blank">NVIDIA NeMo framework</a>. It allowed me to get MSDD checkpoints and run the diarization directly in the colab notebook with just a few lines of code.</p><p id="ab5a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Looking into the Diarization results from MSDD, I noticed that punctuation was pretty bad, with long phrases, and some interruptions such as "<em class="oc">hmm</em>" and "<em class="oc">yeah</em>" were taken into account as a speaker interruption — making the text difficult to read.</p><p id="a88d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, I decided to add a punctuation model to the pipeline to improve the readability of the transcribed text and facilitate human analysis. So I got the <a class="af nb" href="https://huggingface.co/kredor/punctuate-all" rel="noopener ugc nofollow" target="_blank">punctuate-all model from Hugging Face</a>, which is a very precise and fast solution and supports the following languages: English, German, French, Spanish, Bulgarian, Italian, Polish, Dutch, Czech, Portuguese, Slovak, and Slovenian.</p><h2 id="117e" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Video Synchronization</h2><p id="6902" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">From the industry solutions I benchmarked, a strong requirement was that every phrase should be linked to the moment in the interview the speaker was talking.</p><p id="d242" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The Whisper transcriptions have metadata indicating the timestamps when the phrases were said; however, this metadata is not very precise.</p><p id="ae7b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore, I used a model called <a class="af nb" href="https://arxiv.org/abs/2006.11477" rel="noopener ugc nofollow" target="_blank">Wav2Vec2</a> to do this match in a more accurate way. Basically, the solution is a neural network designed to learn representations of audio and perform speech recognition alignment. The process involves finding the exact timestamps in the audio signal where each segment was spoken and aligning the text accordingly.</p><p id="b96c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With the transcription &lt;&gt; timestamp match properly done, through simple Python code, I created hyperlinks pointing to the moment in the video where the phrases start to be said.</p><h2 id="855f" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">The LLM Model</h2><p id="3b77" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">This step of the pipeline has a large language model ready to run locally and analyze the text, providing insights about the interview. By default, I added a Gemma Model 1.1b with a prompt to summarize the text. If the users choose to have the summarization, it will be in a bullet list at the top of the document.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pz"><img src="../Images/1175b705bc66896a25ab9333288fe76a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INx6G3y6wLsO447haKo-nQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="3c14" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Also, by clicking on <em class="oc">Show code</em>, the users can change the prompt and ask the model to perform a different task.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/f782e28c6df86a0db83ca2f6bb0210bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jaPcJt4OCHq8uLe6QFjteg.png"/></div></div></figure><h2 id="2c7b" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Document generation for tagging, highlights, and comments</h2><p id="8141" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The last task performed by the solution is to generate Google Docs with the transcriptions and hyperlinks to the interviews. This was done through the <a class="af nb" href="https://googleapis.github.io/google-api-python-client/" rel="noopener ugc nofollow" target="_blank">Google API Python client library</a>.</p><h1 id="66a0" class="qb oe fq bf of qc qd gq oj qe qf gt on qg qh qi qj qk ql qm qn qo qp qq qr qs bk">I got it</h1><p id="f376" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Since the product has become incredibly useful in my day-to-day work, I decided to give it a name for easier reference. I called it the <strong class="ne fr">I</strong>nsights <strong class="ne fr">G</strong>athering <strong class="ne fr">O</strong>pen-source <strong class="ne fr">T</strong>ool, or iGot.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/8d4274e8d7769313b98c24f070f4ab62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWV6oLY2bXPLj0dJBtn54w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><a class="af nb" href="https://openai.com/policies/terms-of-use/" rel="noopener ugc nofollow" target="_blank">Image generated by DALL·E-3</a></figcaption></figure><p id="86ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When using the solution for the first time, some initial setup is required. Let me guide you through a real-world example to help you get started.</p><h2 id="355a" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Open the iGot notebook and install the required libraries</h2><p id="2ce1" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Click on <a class="af nb" href="https://colab.research.google.com/drive/1eMN4-b4kAH4uNTzzgJ-X4J90BYozGl6o?usp=sharing" rel="noopener ugc nofollow" target="_blank">this link</a> to open the notebook and run the first cell to install the required libraries. It is going to take around 5 minutes.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qu"><img src="../Images/dd64fbb094759505bde82f7e4b22e499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PztJds_tBrcY1wSBig7_xA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="5218" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you get a prompt asking you to restart the notebook, just cancel it. There is no need.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qv"><img src="../Images/933bc06cfe3b64c5b15ec643472bc6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*t4FRn9aIViVGgF1k9GROdw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="3a38" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If everything runs as expected, you are going to get the message "All libraries installed!".</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qw"><img src="../Images/1bfd13f827456fe8cd37187bf397eddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbYlKVW9Cjq6B7tdFh2-6g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="f2c3" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Getting the Hugging User Access Token and model access</h2><p id="3a72" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><em class="oc">(This step is required just the first time you are executing the notebook)</em></p><p id="27e7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For running the Gemma and punctuate-all models, we will download weights from hugging face. To do so, you must request a user token and model access.</p><p id="b010" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To do so, you need to create a hugging face account and follow these steps to get a token with reading permissions.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qx"><img src="../Images/bc3cd784c45aa25cbac192ff56881600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYe4yE3tvD4Ki2srT4OGag.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="4a1f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you have the token, copy it and return to the lab notebook. Go to the secrets tab and click on "Add new secret."</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qy"><img src="../Images/805b3887ad6143ac349b4a479e64bc27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*duRbv5L4P6MUYJ5c5xeUVQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="b71f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Name your token as <em class="oc">HF_TOKEN</em> and past the key you got from Hugging Face.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qz"><img src="../Images/3d133e00fce9e77c3aebe876f80e2051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhA7R0Yfll1dJhjhbZOEHA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="85ae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, click <a class="af nb" href="https://medium.com/r?url=https%3A%2F%2Fhuggingface.co%2Fgoogle%2Fgemma-1.1-2b-it" rel="noopener">this link</a> to open the Gemma model on Hugging Face. Then, click on “Acknowledge license” to get access the model.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/2a1a389a054a697d3c52a2aee0a80a2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WBuIlknRN7iB0bdA6wRk-w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="23d6" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Sending the interview</h2><p id="c4b4" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">To send an interview to iGot, you need to upload it as an unlisted video on YouTube previously. For the purpose of this tutorial, I got a piece of the Andrej Karpathy interview with Lex Fridman and uploaded it to my account. It is part of the conversation where Andrej gave some advice for Machine Learning Beginners.</p><p id="564d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, you need to get the video URL, paste in the <em class="oc">video_url</em> field of the <em class="oc">Interview Selection</em> notebook cell, define a name for it, and indicate the language spoken in the video.</p><p id="4fa1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you run the cell, you are going to receive a message indicating that an audio file was generated.t into</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/7cf4b3fb57c466921b073bc7a0f4af38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfFzQ7XhG9-6EDbRLOZXoA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="189f" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Model selection and execution</h2><p id="5999" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">In the next cell, you can select the size of the Whisper model you want to use for the transcription. The bigger the model, the higher the transcription precision.</p><p id="b0bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By default, the largest model is selected. Make your choice and run the cell.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rc"><img src="../Images/48cd47e34184546ea5fb73b849b06a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9QFoVeNiNFOf3FL7IOzHsQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="4390" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, run the models execution cell to run the pipeline of models showed in the previous section. If everything goes as expected, you should receive the message "Punctuation done!" by the end.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rd"><img src="../Images/85223fc8f974c1c2401713da9585adf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwQW53OqGj8S1Wx2A_o3jw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="331c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you get prompted with a message asking for access to the hugging face token, grant access to it.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj re"><img src="../Images/6405dfa35e01114692e8732e8ce80f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TO7Oq_70pLWb3OV3Ccdcog.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="dae7" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Configuring the transcript output</h2><p id="f3d9" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The final step is to save the transcription to a Google Docs file. To accomplish this, you need to specify the file path, provide the interview name, and indicate whether you want Gemma to summarize the meeting.</p><p id="ca8b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When executing the cell for the first time, you can get prompted with a message asking for access to your Google Drive. Click in allow.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rf"><img src="../Images/9b8a9c6633be1f95a04f9f472ca7d678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*xzzv35J4uBFFOZNPZhoxXg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="019b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, give Colab full access to your Google Drive workspace.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rg"><img src="../Images/b18e48f3be24d56a6af91e644cd05110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*vDU7AjzPP8_MGjBUIVxWWw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="1c96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If everything runs as expected, you are going to see a link to the google docs file at the end. Just click on it, and you are going to have access to your interview transcription.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qw"><img src="../Images/0fb83c207c6df706d2958faf05fdba3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hn5q7HMKR8lnx14tOp_GTQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h2 id="165d" class="od oe fq bf of og oh oi oj ok ol om on nl oo op oq np or os ot nt ou ov ow ox bk">Gathering insights from the generated document</h2><p id="0508" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The final document will have the transcriptions, with each phrase linked to the corresponding moment in the video where it begins. Since YouTube does not provide speaker metadata, I recommend using Google Docs’ find and replace tool to substitute “Speaker 0,” “Speaker 1,” and so on with the actual names of the speakers.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rh"><img src="../Images/931431f03c01af159f04973515555704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*j6qtRjdsp9Xu2IGHuwebtg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><p id="5b1d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With that, you can work on highlights, notes, reactions, etc. As envisioned in the beginning:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ri"><img src="../Images/08bd85c364d8aa7ff383f3237f763ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FEAfmNndse2EGbjzTwxwA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author</figcaption></figure><h1 id="12a3" class="qb oe fq bf of qc qd gq oj qe qf gt on qg qh qi qj qk ql qm qn qo qp qq qr qs bk">Final thoughts</h1><p id="cdc4" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The tool is just in its first version, and I plan to evolve it into a more user-friendly solution. Maybe hosting a website so users don’t need to interact directly with the notebook, or creating a plugin for using it in Google Meets and Zoom.</p><p id="b226" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">My main goal with this project was to create a high-quality meeting transcription tool that can be beneficial to others while demonstrating how available open-source tools can match the capabilities of commercial solutions.</p><p id="652f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I hope you find it useful! Feel free to <a class="af nb" href="https://www.linkedin.com/in/hugozanini/" rel="noopener ugc nofollow" target="_blank">reach out to me on LinkedIn</a> if you have any feedback or are interested in collaborating on the evolution of iGot :)</p></div></div></div></div>    
</body>
</html>