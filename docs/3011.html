<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Build a Document AI Pipeline for Any Type of PDF with Gemini</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Build a Document AI Pipeline for Any Type of PDF with Gemini</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15">https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="010c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Tables, Images, figures or equations are not problem anymore! Full Code provided.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Youness Mansar" class="l ep by dd de cx" src="../Images/b68fe2cbbe219ab0231922c7165f2b6a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*qleZEabtiUsEZaUHvsaWTQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------" rel="noopener follow">Youness Mansar</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/04f5f0506207581622a222ae56c38208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UkVzIVFaou15qtjS"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@mcnoble?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Matt Noble</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="efb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Automated document processing is one of the biggest winners of the ChatGPT revolution, as LLMs are able to tackle a wide range of subjects and tasks in a zero-shot setting, meaning without in-domain labeled training data. This has made building AI-powered applications to process, parse, and automatically understand arbitrary documents much easier. Though naive approaches using LLMs are still hindered by non-text context, such as figures, images, and tables, this is what we will try to address in this blog post, with a special focus on PDFs.</p><p id="185b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At a basic level, PDFs are just a collection of characters, images, and lines along with their exact coordinates. They have no inherent “text” structure and were not built to be processed as text but only to be viewed as is. This is what makes working with them difficult, as text-only approaches fail to capture all the layout and visual elements in these types of documents, resulting in a significant loss of context and information.</p><p id="73d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One way to bypass this “text-only” limitation is to do heavy pre-processing of the document by detecting tables, images, and layout before feeding them to the LLM. Tables can be parsed to Markdown or JSON, images and figures can be represented by their captions, and the text can be fed as is. However, this approach requires custom models and will still result in some loss of information, so can we do better?</p><h1 id="4cee" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Multimodal LLMs</h1><p id="81c8" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Most recent large models are now multi-modal, meaning they can process multiple modalities like text, code, and images. This opens the way to a simpler solution to our problem where one model does everything at once. So, instead of captioning images and parsing tables, we can just feed the page as an image and process it as is. Our pipeline will be able to load the PDF, extract each page as an image, split it into chunks (using the LLM), and index each chunk. If a chunk is retrieved, then the full page is included in the LLM context to perform the task. In what follows, we will detail how this can be implemented in practice.</p><h1 id="0e45" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The Pipeline</h1><p id="e65f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The pipeline we are implementing is a two-step process. First, we segment each page into significant chunks and summarize each of them. Second, we index chunks once then search the chunks each time we get a request and include the full context with each retrieved chunk in the LLM context.</p><h2 id="bfea" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Step 1: Page Segmentation and Summarization</h2><p id="8fd6" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We extract the pages as images and pass each of them to the multi-modal LLM to segment them. Models like Gemini can understand and process page layout easily:</p><ul class=""><li id="9b7f" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Tables</strong> are identified as one chunk.</li><li id="bd42" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Figures</strong> form another chunk.</li><li id="6673" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Text blocks</strong> are segmented into individual chunks.</li><li id="3aa5" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk">…</li></ul><p id="4921" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For each element, the LLM generates a summary than can be embedded and indexed into a vector database.</p><h2 id="261f" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Step 2: Embedding and Contextual Retrieval</h2><p id="f0e3" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this tutorial we will use text embedding only for simplicity but one improvement would be to use vision embeddings directly.</p><p id="3199" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Each entry in the database includes:</p><ul class=""><li id="85b4" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk">The summary of the chunk.</li><li id="c355" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk">The page number where it was found.</li><li id="b553" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk">A link to the image representation of the full page for added context.</li></ul><p id="b855" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This schema allows for <strong class="nf fr">local level searches</strong> (at the chunk level) while keeping <strong class="nf fr">track of the context</strong> (by linking back to the full page). For example, if a search query retrieves an item, the Agent can include the entire page image to provide full layout and extra context to the LLM in order to maximize response quality.</p><p id="8b39" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By providing the full image, all the visual cues and important layout information (like images, titles, bullet points… ) and neighboring items (tables, paragraph, …) are available to the LLM at the time of generating a response.</p><h1 id="b470" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Agents</h1><p id="b92f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We will implement each step as a separate, re-usable agent:</p><p id="4f2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first agent is for parsing, chunking, and summarization. This involves the segmentation of the document into significant chunks, followed by the generation of summaries for each of them. This agent only needs to be run once per PDF to preprocess the document.</p><p id="f814" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The second agent manages indexing, search, and retrieval. This includes inserting the embedding of chunks into the vector database for efficient search. Indexing is performed once per document, while searches can be repeated as many times as needed for different queries.</p><p id="7d2a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For both agents, we use <strong class="nf fr">Gemini</strong>, a multimodal LLM with strong vision understanding abilities.</p><h2 id="caae" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Parsing and Chunking Agent</h2><p id="9d44" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">The first agent is in charge of segmenting each page into meaningful chunks and summarizing each of them, following these steps:</p><p id="16ab" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 1: Extracting PDF Pages as Images</strong></p><p id="7b75" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use the <code class="cx pz qa qb qc b">pdf2image</code> library. The images are then encoded in Base64 format to simplify adding them to the LLM request.</p><p id="94df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s the implementation:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="7234" class="qg oa fq qc b bg qh qi l qj qk">from document_ai_agents.document_utils import extract_images_from_pdf<br/>from document_ai_agents.image_utils import pil_image_to_base64_jpeg<br/>from pathlib import Path<br/><br/>class DocumentParsingAgent:<br/>    @classmethod<br/>    def get_images(cls, state):<br/>        """<br/>        Extract pages of a PDF as Base64-encoded JPEG images.<br/>        """<br/>        assert Path(state.document_path).is_file(), "File does not exist"<br/>        # Extract images from PDF<br/>        images = extract_images_from_pdf(state.document_path)<br/>        assert images, "No images extracted"<br/>        # Convert images to Base64-encoded JPEG<br/>        pages_as_base64_jpeg_images = [pil_image_to_base64_jpeg(x) for x in images]<br/>        return {"pages_as_base64_jpeg_images": pages_as_base64_jpeg_images}</span></pre><p id="6c83" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx pz qa qb qc b">extract_images_from_pdf</code>: Extracts each page of the PDF as a PIL image.</p><p id="1ee1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx pz qa qb qc b">pil_image_to_base64_jpeg</code>: Converts the image into a Base64-encoded JPEG format.</p><p id="d8a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 2: Chunking and Summarization</strong></p><p id="c789" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Each image is then sent to the LLM for segmentation and summarization. We use structured outputs to ensure we get the predictions in the format we expect:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="2b90" class="qg oa fq qc b bg qh qi l qj qk">from pydantic import BaseModel, Field<br/>from typing import Literal<br/>import json<br/>import google.generativeai as genai<br/>from langchain_core.documents import Document<br/><br/>class DetectedLayoutItem(BaseModel):<br/>    """<br/>    Schema for each detected layout element on a page.<br/>    """<br/>    element_type: Literal["Table", "Figure", "Image", "Text-block"] = Field(<br/>        ..., <br/>        description="Type of detected item. Examples: Table, Figure, Image, Text-block."<br/>    )<br/>    summary: str = Field(..., description="A detailed description of the layout item.")<br/><br/>class LayoutElements(BaseModel):<br/>    """<br/>    Schema for the list of layout elements on a page.<br/>    """<br/>    layout_items: list[DetectedLayoutItem] = []<br/><br/>class FindLayoutItemsInput(BaseModel):<br/>    """<br/>    Input schema for processing a single page.<br/>    """<br/>    document_path: str<br/>    base64_jpeg: str<br/>    page_number: int<br/><br/>class DocumentParsingAgent:<br/>    def __init__(self, model_name="gemini-1.5-flash-002"):<br/>        """<br/>        Initialize the LLM with the appropriate schema.<br/>        """<br/>        layout_elements_schema = prepare_schema_for_gemini(LayoutElements)<br/>        self.model_name = model_name<br/>        self.model = genai.GenerativeModel(<br/>            self.model_name,<br/>            generation_config={<br/>                "response_mime_type": "application/json",<br/>                "response_schema": layout_elements_schema,<br/>            },<br/>        )<br/>    def find_layout_items(self, state: FindLayoutItemsInput):<br/>        """<br/>        Send a page image to the LLM for segmentation and summarization.<br/>        """<br/>        messages = [<br/>            f"Find and summarize all the relevant layout elements in this PDF page in the following format: "<br/>            f"{LayoutElements.schema_json()}. "<br/>            f"Tables should have at least two columns and at least two rows. "<br/>            f"The coordinates should overlap with each layout item.",<br/>            {"mime_type": "image/jpeg", "data": state.base64_jpeg},<br/>        ]<br/>        # Send the prompt to the LLM<br/>        result = self.model.generate_content(messages)<br/>        data = json.loads(result.text)<br/>        <br/>        # Convert the JSON output into documents<br/>        documents = [<br/>            Document(<br/>                page_content=item["summary"],<br/>                metadata={<br/>                    "page_number": state.page_number,<br/>                    "element_type": item["element_type"],<br/>                    "document_path": state.document_path,<br/>                },<br/>            )<br/>            for item in data["layout_items"]<br/>        ]<br/>        return {"documents": documents}</span></pre><p id="0445" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <code class="cx pz qa qb qc b">LayoutElements</code> schema defines the structure of the output, with each layout item type (Table, Figure, … ) and its summary.</p><p id="f117" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 3: Parallel Processing of Pages</strong></p><p id="5af1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Pages are processed in parallel for speed. The following method creates a list of tasks to handle all the page image at once since the processing is io-bound:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="7c5b" class="qg oa fq qc b bg qh qi l qj qk">from langgraph.types import Send<br/><br/>class DocumentParsingAgent:<br/>    @classmethod<br/>    def continue_to_find_layout_items(cls, state):<br/>        """<br/>        Generate tasks to process each page in parallel.<br/>        """<br/>        return [<br/>            Send(<br/>                "find_layout_items",<br/>                FindLayoutItemsInput(<br/>                    base64_jpeg=base64_jpeg,<br/>                    page_number=i,<br/>                    document_path=state.document_path,<br/>                ),<br/>            )<br/>            for i, base64_jpeg in enumerate(state.pages_as_base64_jpeg_images)<br/>        ]</span></pre><p id="78ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Each page is sent to the <code class="cx pz qa qb qc b">find_layout_items</code> function as an independent task.</p><p id="c1dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Full workflow</strong></p><p id="4e04" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The agent’s workflow is built using a <code class="cx pz qa qb qc b">StateGraph</code>, linking the image extraction and layout detection steps into a unified pipeline -&gt;</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="51aa" class="qg oa fq qc b bg qh qi l qj qk">from langgraph.graph import StateGraph, START, END<br/><br/>class DocumentParsingAgent:<br/>    def build_agent(self):<br/>        """<br/>        Build the agent workflow using a state graph.<br/>        """<br/>        builder = StateGraph(DocumentLayoutParsingState)<br/>        <br/>        # Add nodes for image extraction and layout item detection<br/>        builder.add_node("get_images", self.get_images)<br/>        builder.add_node("find_layout_items", self.find_layout_items)<br/>        # Define the flow of the graph<br/>        builder.add_edge(START, "get_images")<br/>        builder.add_conditional_edges("get_images", self.continue_to_find_layout_items)<br/>        builder.add_edge("find_layout_items", END)<br/>        <br/>        self.graph = builder.compile()</span></pre><p id="2c09" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To run the agent on a sample PDF we do:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="440e" class="qg oa fq qc b bg qh qi l qj qk">if __name__ == "__main__":<br/>    _state = DocumentLayoutParsingState(<br/>        document_path="path/to/document.pdf"<br/>    )<br/>    agent = DocumentParsingAgent()<br/>    <br/>    # Step 1: Extract images from PDF<br/>    result_images = agent.get_images(_state)<br/>    _state.pages_as_base64_jpeg_images = result_images["pages_as_base64_jpeg_images"]<br/>    <br/>    # Step 2: Process the first page (as an example)<br/>    result_layout = agent.find_layout_items(<br/>        FindLayoutItemsInput(<br/>            base64_jpeg=_state.pages_as_base64_jpeg_images[0],<br/>            page_number=0,<br/>            document_path=_state.document_path,<br/>        )<br/>    )<br/>    # Display the results<br/>    for item in result_layout["documents"]:<br/>        print(item.page_content)<br/>        print(item.metadata["element_type"])<br/></span></pre><p id="bf96" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This results in a parsed, segmented, and summarized representation of the PDF, which is the input of the second agent we will build next.</p><h2 id="06ca" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">RAG Agent</h2><p id="2b87" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">This second agent handles the indexing and retrieval part. It saves the documents of the previous agent into a vector database and uses the result for retrieval. This can be split into two seprate steps, indexing and retrieval.</p><p id="d7a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 1: Indexing the Split Document</strong></p><p id="396d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using the summaries generated, we vectorize them and save them in a ChromaDB database:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="f509" class="qg oa fq qc b bg qh qi l qj qk">class DocumentRAGAgent:<br/>    def index_documents(self, state: DocumentRAGState):<br/>        """<br/>        Index the parsed documents into the vector store.<br/>        """<br/>        assert state.documents, "Documents should have at least one element"<br/>        # Check if the document is already indexed<br/>        if self.vector_store.get(where={"document_path": state.document_path})["ids"]:<br/>            logger.info(<br/>                "Documents for this file are already indexed, exiting this node"<br/>            )<br/>            return  # Skip indexing if already done<br/>        # Add parsed documents to the vector store<br/>        self.vector_store.add_documents(state.documents)<br/>        logger.info(f"Indexed {len(state.documents)} documents for {state.document_path}")</span></pre><p id="ff71" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <code class="cx pz qa qb qc b">index_documents</code> method embeds the chunk summaries into the vector store. We keep metadata such as the document path and page number for later use.</p><p id="2ca0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Step 2: Handling Questions</strong></p><p id="9df2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When a user asks a question, the agent searches for the most relevant chunks in the vector store. It retrieves the summaries and corresponding page images for contextual understanding.</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="99bd" class="qg oa fq qc b bg qh qi l qj qk">class DocumentRAGAgent:<br/>    def answer_question(self, state: DocumentRAGState):<br/>        """<br/>        Retrieve relevant chunks and generate a response to the user's question.<br/>        """<br/>        # Retrieve the top-k relevant documents based on the query<br/>        relevant_documents: list[Document] = self.retriever.invoke(state.question)<br/><br/>        # Retrieve corresponding page images (avoid duplicates)<br/>        images = list(<br/>            set(<br/>                [<br/>                    state.pages_as_base64_jpeg_images[doc.metadata["page_number"]]<br/>                    for doc in relevant_documents<br/>                ]<br/>            )<br/>        )<br/>        logger.info(f"Responding to question: {state.question}")<br/>        # Construct the prompt: Combine images, relevant summaries, and the question<br/>        messages = (<br/>            [{"mime_type": "image/jpeg", "data": base64_jpeg} for base64_jpeg in images]<br/>            + [doc.page_content for doc in relevant_documents]<br/>            + [<br/>                f"Answer this question using the context images and text elements only: {state.question}",<br/>            ]<br/>        )<br/>        # Generate the response using the LLM<br/>        response = self.model.generate_content(messages)<br/>        return {"response": response.text, "relevant_documents": relevant_documents}</span></pre><p id="eb0f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The retriever queries the vector store to find the chunks most relevant to the user’s question. We then build the context for the LLM (Gemini), which combines text chunks and images in order to generate a response.</p><p id="e8ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">The full agent Workflow</strong></p><p id="2c24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The agent workflow has two stages, an indexing stage and a question answering stage:</p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="87eb" class="qg oa fq qc b bg qh qi l qj qk">class DocumentRAGAgent:<br/>    def build_agent(self):<br/>        """<br/>        Build the RAG agent workflow.<br/>        """<br/>        builder = StateGraph(DocumentRAGState)<br/>        # Add nodes for indexing and answering questions<br/>        builder.add_node("index_documents", self.index_documents)<br/>        builder.add_node("answer_question", self.answer_question)<br/>        # Define the workflow<br/>        builder.add_edge(START, "index_documents")<br/>        builder.add_edge("index_documents", "answer_question")<br/>        builder.add_edge("answer_question", END)<br/>        self.graph = builder.compile()</span></pre><p id="b5ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Example run</strong></p><pre class="mm mn mo mp mq qd qc qe bp qf bb bk"><span id="b04b" class="qg oa fq qc b bg qh qi l qj qk">if __name__ == "__main__":<br/>    from pathlib import Path<br/><br/>  # Import the first agent to parse the document<br/>    from document_ai_agents.document_parsing_agent import (<br/>        DocumentLayoutParsingState,<br/>        DocumentParsingAgent,<br/>    )<br/>    # Step 1: Parse the document using the first agent<br/>    state1 = DocumentLayoutParsingState(<br/>        document_path=str(Path(__file__).parents[1] / "data" / "docs.pdf")<br/>    )<br/>    agent1 = DocumentParsingAgent()<br/>    result1 = agent1.graph.invoke(state1)<br/>    # Step 2: Set up the second agent for retrieval and answering<br/>    state2 = DocumentRAGState(<br/>        question="Who was acknowledged in this paper?",<br/>        document_path=str(Path(__file__).parents[1] / "data" / "docs.pdf"),<br/>        pages_as_base64_jpeg_images=result1["pages_as_base64_jpeg_images"],<br/>        documents=result1["documents"],<br/>    )<br/>    agent2 = DocumentRAGAgent()<br/>    # Index the documents<br/>    agent2.graph.invoke(state2)<br/>    # Answer the first question<br/>    result2 = agent2.graph.invoke(state2)<br/>    print(result2["response"])<br/>    # Answer a second question<br/>    state3 = DocumentRAGState(<br/>        question="What is the macro average when fine-tuning on PubLayNet using M-RCNN?",<br/>        document_path=str(Path(__file__).parents[1] / "data" / "docs.pdf"),<br/>        pages_as_base64_jpeg_images=result1["pages_as_base64_jpeg_images"],<br/>        documents=result1["documents"],<br/>    )<br/>    result3 = agent2.graph.invoke(state3)<br/>    print(result3["response"])</span></pre><p id="0696" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With this implementation, the pipeline is complete for document processing, retrieval, and question answering.</p><h1 id="6296" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Example: Using the Document AI Pipeline</h1><p id="3708" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Let’s walk through a practical example using the document <a class="af nc" href="https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf" rel="noopener ugc nofollow" target="_blank">LLM &amp; Adaptation.pdf</a> , a set of 39 slides containing text, equations, and figures (CC BY 4.0).</p><h2 id="9c64" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Step 1: Parsing and summarizing the Document (Agent 1)</h2><ul class=""><li id="4a4a" class="nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny pr ps pt bk"><strong class="nf fr">Execution Time</strong>: Parsing the 39-page document took <strong class="nf fr">29 seconds</strong>.</li><li id="9f9a" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Result</strong>: Agent 1 produces an indexed document consisting of chunk summaries and base64-encoded JPEG images of each page.</li></ul><h2 id="bfb7" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Step 2: Questioning the Document (Agent 2)</h2><p id="03b0" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">We ask the following question:<br/> <strong class="nf fr">“</strong>Explain LoRA, give the relevant equations<strong class="nf fr">”</strong></p><h2 id="8653" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Result:</h2><p id="43dc" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Retrieved pages:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ql"><img src="../Images/a96c96db2579443f470a142aa9c9d1a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*rRajnY3rX8xkSnzd0-AENw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: <a class="af nc" href="https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf" rel="noopener ugc nofollow" target="_blank">LLM &amp; Adaptation.pdf</a> License CC-BY</figcaption></figure><h2 id="99a4" class="pa oa fq bf ob pb pc pd oe pe pf pg oh nm ph pi pj nq pk pl pm nu pn po pp pq bk">Response from the LLM</h2></div></div><div class="mr"><div class="ab cb"><div class="lm qm ln qn lo qo cf qp cg qq ci bh"><figure class="mm mn mo mp mq mr qs qt paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/4af2dc191f5a778641c7a0e0c0b2fcb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*-d3_hjT2wiVH_yRZrLF1Cw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b9a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The LLM was able to include equations and figures into its response by taking advantage of the visual context in generating a coherent and correct response based on the document.</p><h1 id="a9ec" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="bcf8" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">In this quick tutorial, we saw how you can take your document AI processing pipeline a step further by leveraging the multi-modality of recent LLMs and using the full visual context available in each document, hopefully improving the quality of outputs that you are able to get from either your information extraction or RAG pipeline.</p><p id="7a6a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We built a stronger document segmentation step that is able to detect the important items like paragraphs, tables, and figures and summarize them, then used the result of this first step to query the collection of items and pages to give relevant and precise answers using Gemini. As a next step, you can try it on your use case and document, try to use a scalable vector database, and deploy these agents as part of your AI app.</p><p id="7497" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Full code and example are available here : <a class="af nc" href="https://github.com/CVxTz/document_ai_agents" rel="noopener ugc nofollow" target="_blank">https://github.com/CVxTz/document_ai_agents</a></p><p id="66c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thank you for reading ! 😃</p></div></div></div></div>    
</body>
</html>