<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into Anthropic’s Sparse Autoencoders by Hand ✍️</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into Anthropic’s Sparse Autoencoders by Hand ✍️</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31">https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d818" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the concepts behind the interpretability quest for LLMs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/81130790966487c80fb82406f9fe2482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfu1t2WCqje4wF51OOOxww.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by author (Zephyra, the protector of Lumaria by my 4-year old)</figcaption></figure><p id="0ea1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">“In the mystical lands of Lumaria, where ancient magic filled the air, lived Zephyra, the Ethereal Griffin. With the body of a lion and the wings of an eagle, Zephyra was the revered protector of the Codex of Truths, an ancient script holding the universe’s secrets.</em></p><p id="f287" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Nestled in a sacred cave, the Codex was safeguarded by Zephyra’s viridescent eyes, which could see through deception to unveil pure truths. One day, a dark sorcerer descended on the lands of Lumaria and sought to shroud the world in ignorance by concealing the Codex. The villagers called upon Zephyra, who soared through the skies, as a beacon of hope. With a majestic sweep of the wings, Zephyra created a protective barrier of light around the grove, repelling the sorcerer and exposing the truths.</em></p><p id="6d54" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">After a long duel, it was concluded that the dark sorcerer was no match to Zephyra’s light. Through her courage and vigilance, the true light kept shining over Lumaria. And as time went by, Lumaria was guided to prosperity under Zephyra’s protection and its path stayed illuminated by the truths Zephyra safeguarded. And this is how Zephyra’s legend lived on!”</em></p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="17ea" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Anthropic’s journey ‘towards extracting interpretable features’</h1><p id="8e45" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Following the story of Zephyra, Anthropic AI delved into the expedition of extracting meaningful features in a model. The idea behind this investigation lies in understanding how different components in a neural network interact with one another and what role each component plays.</p><p id="14c4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">According to the paper <strong class="nd fr">“</strong><a class="af ph" href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</strong></a><strong class="nd fr">”</strong> a Sparse Autoencoder is able to successfully extract meaningful features from a model. In other words, Sparse Autoencoders help break down the problem of ‘polysemanticity’ — neural activations that correspond to several meanings/interpretations at once by focusing on sparsely activating features that hold a single interpretation — in other words, are more one-directional.</p><p id="292d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To understand how all of it is done, we have these beautiful handiworks on <a class="af ph" href="https://lnkd.in/g2rM9iV2" rel="noopener ugc nofollow" target="_blank">Autoencoders</a> and<a class="af ph" href="https://www.linkedin.com/posts/tom-yeh_claude-autoencoder-aibyhand-activity-7199774212759183362-msKU/?" rel="noopener ugc nofollow" target="_blank"> Sparse Autoencoders </a>by Prof. <a class="af ph" href="https://www.linkedin.com/in/tom-yeh/" rel="noopener ugc nofollow" target="_blank">Tom Yeh</a> that explain the behind-the-scenes workings of these phenomenal mechanisms.</p><p id="51b9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">(All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned LinkedIn posts, which I have edited with his permission. )</p><p id="03ac" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To begin, let us first let us first explore what an Autoencoder is and how it works.</p><h1 id="4db9" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">What is an Autoencoder?</h1><p id="9b75" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Imagine a writer has his desk strewn with different papers — some are his notes for the story he is writing, some are copies of final drafts, some are again illustrations for his action-packed story. Now amidst this chaos, it is hard to find the important parts — more so when the writer is in a hurry and the publisher is on the phone demanding a book in two days. Thankfully, the writer has a very efficient assistant — this assistant makes sure the cluttered desk is cleaned regularly, grouping similar items, organizing and putting things into their right place. And as and when needed, the assistant would retrieve the correct items for the writer, helping him meet the deadlines set by his publisher.</p><p id="f800" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Well, the name of this assistant is Autoencoder. It mainly has two functions — encoding and decoding. Encoding refers to condensing input data and extracting the essential features (organization). Decoding is the process of reconstructing original data from encoded representation while aiming to minimize information loss (retrieval).</p><p id="406e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now let’s look at how this assistant works.</p><h1 id="e74f" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">How does an Autoencoder Work?</h1><p id="8f4c" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Given : Four training examples <strong class="nd fr">X1, X2, X3, X4.</strong></p><h2 id="42bd" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[1] Auto</h2><p id="ed0e" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The first step is to copy the training examples to targets <strong class="nd fr">Y’</strong>. The Autoencoder’s work is to reconstruct these training examples. Since the targets are the training examples themselves, the word <strong class="nd fr"><em class="nx">‘Auto’</em></strong> is used which is Greek for <strong class="nd fr"><em class="nx">‘self’</em></strong>.</p><h2 id="b3d8" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[2] Encoder : Layer 1 +ReLU</h2><p id="2a2e" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">As we have seen in all our previous models, a simple weight and bias matrix coupled with ReLU is powerful and is able to do wonders. Thus, by using the first Encoding layer we reduce the size of the original feature set from 4x4 to 3x4.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/c2c2896b0761620ef71f79dd24b7de1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INIu2VmAyBnQHRLUc_pY-g.gif"/></div></div></figure><blockquote class="qf qg qh"><p id="b1e6" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A quick recap:</p><p id="b7cf" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Linear transformation</strong> : The input embedding vector is multiplied by the weight matrix W and then added with the bias vector <strong class="nd fr">b</strong>,</p><p id="c0b4" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">z = <strong class="nd fr">W</strong>x+<strong class="nd fr">b</strong>, where <strong class="nd fr">W</strong> is the weight matrix, x is our word embedding and <strong class="nd fr">b</strong> is the bias vector.</p><p id="cf8c" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">ReLU activation function</strong> : Next, we apply the ReLU to this intermediate z.</p><p id="11a4" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">ReLU returns the element-wise maximum of the input and zero. Mathematically, <strong class="nd fr">h </strong>= max{0,z}.</p></blockquote><h2 id="867c" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[3] Encoder : Layer 2 + ReLU</h2><p id="d9af" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The output of the previous layer is processed by the second Encoder layer which reduces the input size further to 2x3. This is where the extraction of relevant features occurs. This layer is also called the ‘bottleneck’ since the outputs in this layer have much lower features than the input features.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/486e9e67fbbfac831061f662d28f156c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UBKNLacq0ZOXF-f9Tzvzg.gif"/></div></div></figure><h2 id="7315" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[4] Decoder : Layer 1 + ReLU</h2><p id="1f97" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once the encoding process is complete, the next step is to decode the relevant features to build ‘back’ the final output. To do so, we multiply the features from the last step with corresponding weights and biases and apply the ReLU layer. The result is a 3x4 matrix.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qj"><img src="../Images/9f9ffd4521b3c28b6eba26be3b1c2100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yCWisBAtVJ35IZB164Vvew.gif"/></div></div></figure><h2 id="02f0" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[5] Decoder : Layer 2 + ReLU</h2><p id="d063" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">A second Decoder layer (weight, biases + ReLU) applies on the previous output to give the final result which is the reconstructed 4x4 matrix. We do so to get back to original dimension in order to compare the results with our original target.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/47b3bd18b0209d34df20b5cb9f242b97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUXnoKZk1kQP3MDUA9SLtA.gif"/></div></div></figure><h2 id="4b02" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[6] Loss Gradients &amp; BackPropagation</h2><p id="7d65" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once the output from the decoder layer is obtained, we calculate the gradients of the Mean Square Error (MSE) between the <strong class="nd fr">outputs (Y) </strong>and the <strong class="nd fr">targets (Y’)</strong>. To do so, we find <strong class="nd fr">2*(Y-Y’)</strong> , which gives us the final gradients that activate the backpropagation process and updates the weights and biases accordingly.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ql"><img src="../Images/286ca27d2139e0c693ec2fdcee1050b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_qDdXzetVZZJ8oKaEVeig.gif"/></div></div></figure><p id="12f0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now that we understand how the Autoencoder works, it’s time to explore how its <strong class="nd fr">sparse variation</strong> is able to achieve interpretability for large language models (LLMs).</p><h1 id="9c8d" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Sparse Autoencoder — How does it work?</h1><p id="73c7" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">To start with, suppose we are given:</p><ul class=""><li id="be9a" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qm qn qo bk">The output of a transformer after the feed-forward layer has processed it, i.e. let us assume we have the model activations for five tokens (X). They are good but they do not shed light on how the model arrives at its decision or makes the predictions.</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qp"><img src="../Images/5c01141ce07bd9ff3330b673178216f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBQzrK_vU9FJRTaHkQOd7A.png"/></div></div></figure><p id="1507" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The prime question here is:</p><blockquote class="qq"><p id="d01e" class="qr qs fq bf qt qu qv qw qx qy qz nw dx">Is it possible to map each activation (3D) to a higher-dimension space (6D) that will help with the understanding?</p></blockquote><h2 id="7b10" class="pn oh fq bf oi po ra pq ol pr rb pt oo nk rc pv pw no rd py pz ns re qb qc qd bk">[1] Encoder : Linear Layer</h2><p id="e9ea" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The first step in the Encoder layer is to multiply the input <strong class="nd fr">X </strong>with encoder weights and add biases (as done in the first step of an Autoencoder).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rf"><img src="../Images/312068c487f5a71ba089baf15668d291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhgfTmOD7ZowFVtSBHPn8Q.gif"/></div></div></figure><h2 id="7f6e" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[2] Encoder : ReLU</h2><p id="e5d6" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The next sub-step is to apply the ReLU activation function to add non-linearity and suppress negative activations. This suppression leads to many features being set to 0 which enables the concept of sparsity — outputting sparse and interpretable features <strong class="nd fr"><em class="nx">f.</em></strong></p><p id="fd03" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interpretability happens when we have only one or two positive features. If we examine <strong class="nd fr"><em class="nx">f6</em></strong>, we can see <strong class="nd fr">X2</strong> and <strong class="nd fr">X3</strong> are positive, and may say that both have ‘Mountain’ in common.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rf"><img src="../Images/7b8b7f37cd47ccde17cf92263a8a113b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L5PxylZCTjdNULt4gjt7oQ.gif"/></div></div></figure><h2 id="53fb" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[3] Decoder : Reconstruction</h2><p id="8c8d" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Once we are done with the encoder, we proceed to the decoder step. We multiply <strong class="nd fr"><em class="nx">f</em></strong><em class="nx"> </em>with decoder weights and add biases. This outputs <strong class="nd fr">X’</strong>, which is the reconstruction of <strong class="nd fr">X</strong> from interpretable features.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rg"><img src="../Images/0ef0969dd32ed6d5a1a46041619c2efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sZJcXiZSQVr41YSTA33RA.gif"/></div></div></figure><p id="b60c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As done in an Autoencoder, we want <strong class="nd fr">X’</strong> to be as close to <strong class="nd fr">X</strong> as possible. To ensure that, further training is essential.</p><h2 id="4e52" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[4] Decoder : Weights</h2><p id="20ac" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">As an intermediary step, we compute the L2 norm for each of the weights in this step. We keep them aside to be used later.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rh"><img src="../Images/9208c5b6e536090638fa96455a2dc40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*k3zIB0kEP1sewwORw08FOQ.gif"/></div></figure><blockquote class="qf qg qh"><p id="5876" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">L2-norm</strong></p><p id="b4bc" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Also known as Euclidean norm, L2-norm calculates the magnitude of a vector using the formula: ||x||₂ = √(Σᵢ xᵢ²).</p><p id="1726" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In other words, it sums the squares of each component and then takes the square root over the result. This norm provides a straightforward way to quantify the length or distance of a vector in Euclidean space.</p></blockquote><h1 id="321e" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Training</h1><p id="fc8c" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">As mentioned earlier, a Sparse Autoencoder instils extensive training to get the reconstructed <strong class="nd fr">X’</strong> closer to <strong class="nd fr">X</strong>. To illustrate that, we proceed to the next steps below:</p><h2 id="09b3" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[5] Sparsity : L1 Loss</h2><p id="7a47" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The goal here is to obtain as many values close to zero / zero as possible. We do so by invoking <strong class="nd fr">L1 sparsity</strong> to penalize the absolute values of the weights — the core idea being that we want to make the sum as small as possible.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ri"><img src="../Images/f8aa5583bdaebc8564e7bd8b44ddd399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*RYebksXA--6kfOCWkZxRiA.gif"/></div></figure><blockquote class="qf qg qh"><p id="a408" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">L1-loss</strong></p><p id="4ac9" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The L1-loss is calculated as the sum of the absolute values of the weights: L1 = λΣ|w|, where λ is a regularization parameter.</p><p id="22fa" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This encourages many weights to become zero, simplifying the model and thus enhancing <strong class="nd fr">interpretability</strong>.</p><p id="7bca" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In other words, L1 helps build the focus on the most relevant features while also preventing overfitting, improving model generalization, and reducing computational complexity.</p></blockquote><h2 id="d5da" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[6] Sparsity : Gradient</h2><p id="d594" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The next step is to calculate <strong class="nd fr">L1</strong>’s gradients which -1 for positive values. Thus, for all values of <strong class="nd fr"><em class="nx">f &gt;0 </em></strong>, the result will be set to -1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rj"><img src="../Images/5e85338940e15e3475ce8be719ff40e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WXXLP5p7zYyBK2T22CbcA.gif"/></div></div></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="qf qg qh"><p id="ce1a" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">How does L1 penalty push weights towards zero?</strong></p><p id="ca86" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The gradient of the L1 penalty pushes weights towards zero through a process that applies a constant force, regardless of the weight’s current value. Here’s how it works (all images in this sub-section are by author):</p><p id="fb15" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The L1 penalty is expressed as:</p></blockquote><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rk"><img src="../Images/04df48be6e16080c94467c08c9f248ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*66I49jyCjN_cx6Au7lbJMQ.png"/></div></figure><blockquote class="qf qg qh"><p id="23d4" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The gradient of this penalty with respect to a weight <strong class="nd fr"><em class="fq">w</em></strong> is:</p></blockquote><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rl"><img src="../Images/52a15cda96d611eb04f7e52d79a4a817.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*wkHXsIBVbbIp0sUgknjQVg.png"/></div></figure><blockquote class="qf qg qh"><p id="f47f" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">where <strong class="nd fr"><em class="fq">sign(w)</em></strong> is:</p></blockquote><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rm"><img src="../Images/5d0377f88ae46308b533ea7284c90ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*rLboil1lraGSYN0xLl0TUQ.png"/></div></figure><blockquote class="qf qg qh"><p id="a131" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">During gradient descent, the update rule for weights is:</p></blockquote><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rn"><img src="../Images/c4cf2e8036e3f8abe10def8af14ab467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*onjEcsGwhQkOc5YPUCucog.png"/></div></div></figure><blockquote class="qf qg qh"><p id="3387" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">where 𝞰 is the learning rate.</p><p id="9036" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The <strong class="nd fr">constant subtraction (or addition)</strong> of <strong class="nd fr">λ </strong>from the weight value (depending on its sign) decreases the absolute value of the weight. If the weight is small enough, this process can drive it to exactly zero.</p></blockquote></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4cbe" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[7] Sparsity : Zero</h2><p id="81ae" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">For all other values that are already zero, we keep them unchanged since they have already been zeroed out.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ro"><img src="../Images/35dcfe7c72319bdffd087a7baa4fe776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtDUWgJ11gs1bh77CEt-Qw.gif"/></div></div></figure><h2 id="65f2" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[8] Sparsity : Weight</h2><p id="63a8" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We multiple each row of the gradient matrix obtained in Step 6 by the corresponding decoder weights obtained in Step 4. This step is crucial as it prevents the model from learning large weights which would add incorrect information while reconstructing the results.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rp"><img src="../Images/306a1ce823111e9d815e4b7f1fd15802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*kM4XIHlPsa7su69XV11H7Q.gif"/></div></figure><h2 id="9898" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[9] Reconstruction : MSE Loss</h2><p id="a250" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">We use the Mean Square Error or the <strong class="nd fr">L2</strong> loss function to calculate the difference between <strong class="nd fr">X’ </strong>and <strong class="nd fr">X</strong>. The goal as seen previously is to minimize the error to the lowest value.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rq"><img src="../Images/f4383312900f938f95dc354d99c296fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bOB5l-c-cXhrtX89Fk0AA.gif"/></div></div></figure><h2 id="c5a5" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">[10] Reconstruction : Gradient</h2><p id="056b" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The gradient of <strong class="nd fr">L2</strong> loss is <strong class="nd fr">2*(X’-X)</strong>.</p><p id="8063" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And hence as seen for the original Autoencoders, we run backpropagation to update the weights and the biases. The catch here is finding a good balance between sparsity and reconstruction.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rr"><img src="../Images/8b70731a04ecd7810360275f13324e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*3MDwExTyz4ImSJX2GMzHkA.gif"/></div></figure><p id="8670" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And with this, we come to the end of this very clever and intuitive way of learning how a model understands an idea and the direction it takes to generate a response.</p><h2 id="a64f" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">To summarize:</h2><ol class=""><li id="0981" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw rs qn qo bk">An <strong class="nd fr">Autoencoder</strong> overall consists of two parts : <strong class="nd fr">Encoder </strong>and <strong class="nd fr">Decoder</strong>. The <strong class="nd fr">Encoder</strong> uses weights and biases coupled with the ReLU activation function to compress the initial input features into a lower dimension, trying to capture only the relevant parts. The <strong class="nd fr">Decoder</strong> on the other hand takes the output of the Encoder and works to reconstruct the input features back to their original state. Since the targets in an Autoencoder are the initial features themselves, hence the use of the word ‘auto’. The aim, as is for standard neural networks, is to achieve the lowest error (difference) between the target and the input features — and it is achieved by propagating the gradient of the error through the network while updating the weights and biases.</li><li id="0369" class="nb nc fq nd b go rt nf ng gr ru ni nj nk rv nm nn no rw nq nr ns rx nu nv nw rs qn qo bk">A <strong class="nd fr">Sparse Autoencoder</strong> consists of all the components as a standard Autoencoder along with a few more additions. The key here is the different approach in the training step. Since the aim here is to retrieve the interpretable features, we want to zero out those values which hold relatively less meaning. Once the encoder uses ReLU to suppress the negative values, we go a step further and use L1-Loss on the result to encourage sparsity by penalizing the absolute values of the weights. This is achieved by adding a penalty term to the loss function, which is the sum of the absolute values of the weights: λΣ|w|. The weights that remain non-zero are those that are crucial for the model’s performance.</li></ol><h1 id="c23f" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Extracting Interpretable features using Sparsity</h1><p id="9a69" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">As humans, our brains activate only a small subset of neurons in response to specific stimuli. Likewise, Sparse Autoencoders learn a sparse representation of the input by leveraging sparsity constraints like <strong class="nd fr">L1</strong> regularization. By doing so, a Sparse Autoencoder is able to extract interpretable features from complex data thus enhancing the simplicity and interpretability of the learned features. This selective activation mirroring biological neural processes helps focus on the most relevant aspects of the input data making the models more robust and efficient.</p><p id="637f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With Anthropic’s endeavor to understand interpretability in AI models, their initiative highlights the need for transparent and understandable AI systems, especially as they become more integrated into critical decision-making processes. By focusing on creating models that are both powerful and interpretable, Anthropic contributes to the development of AI that can be trusted and effectively utilized in real-world applications.</p><p id="a691" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In conclusion, <strong class="nd fr">Sparse Autoencoders</strong> are vital for extracting interpretable features, enhancing model robustness, and ensuring efficiency. The ongoing work on understanding these powerful models and how they make inferences underscore the growing importance of interpretability in AI, paving the way for more transparent AI systems. It remains to see how these concepts evolve and driving us towards a future that entails a safe integration of AI in our lives!</p><p id="fce9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">P.S. If you would like to work through this exercise on your own, here is a link to a blank template for your use.</em></p><p id="95f8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af ph" href="https://drive.google.com/file/d/1xiAjdlWCAzhj-I-YOb7wSMeroUOQzdlE/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="ab14" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now go have fun and help Zephyr keep the Codex of Truth safe!</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/eb16ec0901108e38f05e6bac5825305e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2DMsbjmpne5weITU2ACcEg.png"/></div></div></figure><p id="b3c6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Once again special thanks to </em><a class="af ph" href="https://www.linkedin.com/in/tom-yeh/" rel="noopener ugc nofollow" target="_blank"><em class="nx">Prof. Tom Yeh</em></a><em class="nx"> for supporting this work!</em></p><h2 id="ed57" class="pn oh fq bf oi po pp pq ol pr ps pt oo nk pu pv pw no px py pz ns qa qb qc qd bk">References:</h2><p id="a6c2" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">[1] Towards Monosemanticity: Decomposing Language Models With Dictionary Learning, Bricken et al. Oct 2023 <a class="af ph" href="https://transformer-circuits.pub/2023/monosemantic-features/index.html" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2023/monosemantic-features/index.html</a></p><p id="722e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2] Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet, Templeton et al. May 2024 <a class="af ph" href="https://transformer-circuits.pub/2024/scaling-monosemanticity/" rel="noopener ugc nofollow" target="_blank">https://transformer-circuits.pub/2024/scaling-monosemanticity/</a></p></div></div></div></div>    
</body>
</html>