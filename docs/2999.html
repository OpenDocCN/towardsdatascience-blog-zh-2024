<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Navigating Soft Actor-Critic Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Navigating Soft Actor-Critic Reinforcement Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12">https://towardsdatascience.com/navigating-soft-actor-critic-reinforcement-learning-8e1a7406ce48?source=collection_archive---------11-----------------------#2024-12-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d3b3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding the theory and implementation of SAC RL in the context of Bioengineering</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mohammed AbuSadeh" class="l ep by dd de cx" src="../Images/2a7d5ce6964cdc76d9640b1a17ac707b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*aYLrJzbFy-69NFMZZGqIxQ@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mo.abusadeh?source=post_page---byline--8e1a7406ce48--------------------------------" rel="noopener follow">Mohammed AbuSadeh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8e1a7406ce48--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/117e5ac1c7be96924a7201b5185fd59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yfkhYMLsj6VZ_P-H2fPtFQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image generated by the author using ChatGPT-4o</figcaption></figure><h1 id="a684" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Introduction</h1><p id="26f5" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The research domain of Reinforcement Learning (RL) has evolved greatly over the past years. The use of deep reinforcement learning methods such as Proximal Policy Optimisation (PPO) (Schulman, 2017) and Deep Deterministic Policy Gradient (DDPG) (Lillicrap, 2015) have enabled agents to solve tasks in high-dimensional environments. However, many of these model-free RL algorithms have struggled with stability during the training process. These challenges arise due to the brittle convergence properties, high variance in gradient estimation, very high sample complexity, and the sensitivity to hyperparameters in continuous action spaces. Given these problems, it is imperative to consider a newly devised RL algorithm that avoids such issues and expands applicability to complex, real-world problems. This new algorithm is the Soft Actor-Critic (SAC) deep RL network. (Haarnoja, 2018)</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ot"><img src="../Images/719d9f7573dce860add1e140432f1fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*KRk_7825CvmUF0uj"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Model Architecture of Soft Actor-Critic Networks. Image taken from <a class="af ou" href="https://arxiv.org/abs/2301.03220" rel="noopener ugc nofollow" target="_blank">(Du, 2023)</a></figcaption></figure><p id="6434" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">SAC is an off-policy Actor-Critic deep RL algorithm which is designed to address the stability and efficiency constraints of its predecessors. The SAC algorithm is based on the maximum entropy RL framework which aims for the actor part of the network to maximise the expected reward, while maximising entropy. It combines off-policy updates with a more stable formulation of the stochastic Actor-Critic method. An off-policy algorithm enables faster learning and better sample efficiency using experience replay, unlike on-policy methods such as PPO, which require new samples for each gradient step. For on-policy methods such as PPO, for each gradient step in the learning process, new samples must be collected. The aim of using stochastic policies and maximising entropy comes to promote the robustness and exploration of the algorithm by encouraging more randomness in the actions. Additionally, unlike PPO and DDPG, SAC uses twin Q-networks with a separate Actor network and entropy tuning to improve the stability and convergence when combining off-policy learning with high dimensional, nonlinear function approximation.</p><p id="22cd" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Off-policy RL methods have had a wide impact on bioengineering systems that improve patient lives. More specifically, RL has been applied to domains such as robotic arm control, drug delivery methods and most notably de novo drug design. (Svensson, 2024) Svensson et al. has used a number of on- and off-policy frameworks and different variants of replay buffers to learn a RNN-based molecule generation policy, to be active against DRD2 (a dopamine receptor). The paper realises that using experience replay across the board for high, intermediate and low scoring molecules has shown effects in improving the structural diversity and the number of active molecules generated. Replay buffers improve sample efficiency in training agents. They also reported that the use of off-policy methods and more specifically SAC, helps in promoting structural diversity by preventing mode collapse.</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0be2" class="nb nc fq bf nd ne pi gq ng nh pj gt nj nk pk nm nn no pl nq nr ns pm nu nv nw bk">Theoretical Explanation</h1><p id="6215" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">SAC uses ‘soft’ value functions by introducing the objective function with an entropy term, <strong class="nz fr">Η(π(a|s))</strong>. Accordingly, the network seeks to maximise both the expected return of lifetime rewards and the entropy of the policy. The entropy of the policy is defined as the unpredictability of a random variable, which increases with the range of possible values. Thus, the new entropy regularised objective becomes:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pn"><img src="../Images/57384bbf7ccc74498e77313443dd16e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*cfQ1iGi6F7XmMXbRoBPCPQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Entropy Regularised Objective</figcaption></figure><p id="7a37" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk"><strong class="nz fr">α </strong>is the temperature parameter that balances between exploration and exploitation.</p><p id="9dcc" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">In the implementation of soft value functions, we aim to maximise the entropy as the algorithm would assign equal probabilities to actions that have a similar Q-value. Maximising entropy also helps with preventing the agent from choosing actions that exploit inconsistencies in approximated Q-values. We can finally understand how SAC improves brittleness by allowing the network to explore more and not assign very high probabilities to one range of actions. This part is inspired by <span class="ia"><span class="ia" aria-hidden="false"><a class="po ib pp" href="https://medium.com/u/61d2676ad14?source=post_page---user_mention--8e1a7406ce48--------------------------------" rel="noopener" target="_blank">Vaishak V.Kumar</a></span></span>’s explanation of the entropy maximisation in “Soft Actor-Critic Demystified”.</p><p id="e142" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The SAC paper authors discuss that since the state value function approximates the soft value, there is really no essential need to train separate function approximators for the policy, since they relate to the state value according to the following equation. However, training three separate approximators provided better convergence.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pq"><img src="../Images/8bb99c228fae7e880d122184030b8231.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*zg8hqrq-3jq3phU8jjGALA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Soft State Value Function</figcaption></figure><p id="0d19" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The three function approximator networks are characterised as follows:</p><ul class=""><li id="ec84" class="nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os pr ps pt bk"><strong class="nz fr">Policy Network (Actor): </strong>the stochastic policy outputs a set of actions sampled from a Gaussian distribution. The policy parameters are learned by minimising the Kullback-Leibler Divergence as provided in this equation:</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pu"><img src="../Images/79a5cd826f332f6b5fec21de2400c762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*5-1Ih1trFi-ZKVSabMeUoQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Minimising KL-Divergence</figcaption></figure><p id="278f" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The KL-divergence compares the relative entropy or the difference between two probability distributions. So, in the equation, we are trying to minimise the difference between the distributions of the policy function and the exponentiated Q-function normalised by a function Z. Since the target density function is the Q-function, which is differentiable, we apply a reparametrisation trick on the policy to reduce the estimation of the variance.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pv"><img src="../Images/9b4eb81490bd4799501b1cea4cd7cba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*uzEqV8Xxguc0Pai9BC5ywg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Reparametrised Policy</figcaption></figure><p id="7e0d" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">ϵₜ is a vector sampled from a Gaussian distribution which describes the noise.</p><p id="ea48" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The policy objective is then updated to the following expression:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pw"><img src="../Images/61de771ed85419f7cf722c7582dcd7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*-OP-5Uq0l_yUNaalRUlnRw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Policy Objective</figcaption></figure><p id="d25c" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The policy objective is optimised using the following gradient estimation:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj px"><img src="../Images/3247348d35c518e313d104bcdaef9c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*-X1VYmDQeGXtHSBqiEkr5g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Policy Gradient Estimator</figcaption></figure><ul class=""><li id="43ec" class="nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os pr ps pt bk"><strong class="nz fr">Q-Network (Critic): </strong>includes two Q-value networks to estimate the expected reward for the state-action pairs. We minimise the soft Q-function parameters by using the soft Bellman residual provided here:</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/326b8933dff984f1c5106331d3507e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*re4umf-pJdNWHN2XMevU4w.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Soft Q-function Objective</figcaption></figure><p id="a90a" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">where:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pz"><img src="../Images/4333f2dc897ce235348f2991d1cb661a.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*VSzO-HTTGM5ZYgbn0RhZyw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Immediate Q-value</figcaption></figure><p id="af69" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The soft Q-function objective minimises the square differences between the networks Q-value estimation and the immediate Q-value. The immediate Q-value (Q hat) is obtained from the reward of the current state-action pair added to the discounted expectation of the target value function in the following time stamp. Finally, the objective is optimised using a stochastic gradient estimation given by the following:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qa"><img src="../Images/8739f195eadfd70126df462e16384612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*UowPPIgsfR169BA0AbZLtg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Stochastic Gradient Estimator</figcaption></figure><p id="9440" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk"><strong class="nz fr">Target Value Network (Critic): </strong>a separate soft value function which helps in stabilising the training process. The soft value function approximator minimises the squared residual error as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qb"><img src="../Images/084c54806c06bf551bded6f9f54b245a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*DMu1dQLXR4DXD4vEMR7Y1g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Soft Value Function Objective</figcaption></figure><p id="f361" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">This soft value function objective minimises the square differences between the value function and the expectation of the Q-value plus the entropy of the policy function <strong class="nz fr">π</strong>. The negative log part of this objective describes the entropy of the policy function. We also know that the information entropy is calculated using a negative sign to output a positive entropy value, since the log of a probability value (between 0 and 1) will be negative. Similarly, the objective is optimised using an unbiased gradient estimator, given in the following expression:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qc"><img src="../Images/94ecdcd4eacc2213b4ba1a9c2dcc2ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*wC98khX7wHqu4a3gdCq0Qg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Unbiased Gradient Estimator</figcaption></figure></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0944" class="nb nc fq bf nd ne pi gq ng nh pj gt nj nk pk nm nn no pl nq nr ns pm nu nv nw bk">Code Implementation</h1><p id="4069" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The code implemented in this article is taken from the following Github repository (quantumiracle, 2023):</p><div class="qd qe qf qg qh qi"><a href="https://github.com/quantumiracle/Popular-RL-Algorithms?source=post_page-----8e1a7406ce48--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qj ab ig"><div class="qk ab co cb ql qm"><h2 class="bf fr hw z io qn iq ir qo it iv fp bk">GitHub - quantumiracle/Popular-RL-Algorithms: PyTorch implementation of Soft Actor-Critic (SAC)…</h2><div class="qp l"><h3 class="bf b hw z io qn iq ir qo it iv dx">PyTorch implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), Actor-Critic (AC/A2C), Proximal Policy…</h3></div><div class="qq l"><p class="bf b dy z io qn iq ir qo it iv dx">github.com</p></div></div><div class="qr l"><div class="qs l qt qu qv qr qw lq qi"/></div></div></a></div><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="b453" class="rb nc fq qy b bg rc rd l re rf">pip install gymnasium torch</span></pre><p id="3a8d" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">SAC relies on environments that use continuous action spaces, so the simulation provided uses the robotic arm ‘Reacher’ environment for the most part and the Pendulum-v1 environment in the gymnasium package.</p><p id="d5b0" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The Pendulum environment was run on a different repository that implements the same algorithm but with less deprecated libraries given by (MrSyee, 2020):</p><div class="qd qe qf qg qh qi"><a href="https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file&amp;source=post_page-----8e1a7406ce48--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qj ab ig"><div class="qk ab co cb ql qm"><h2 class="bf fr hw z io qn iq ir qo it iv fp bk">GitHub - MrSyee/pg-is-all-you-need: Policy Gradient is all you need! A step-by-step tutorial for…</h2><div class="qp l"><h3 class="bf b hw z io qn iq ir qo it iv dx">Policy Gradient is all you need! A step-by-step tutorial for well-known PG methods. - MrSyee/pg-is-all-you-need</h3></div><div class="qq l"><p class="bf b dy z io qn iq ir qo it iv dx">github.com</p></div></div><div class="qr l"><div class="rg l qt qu qv qr qw lq qi"/></div></div></a></div><p id="24e2" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">In terms of the network architectures, as mentioned in the <em class="rh">Theory Explanation, </em>there are three main components:</p><p id="a4eb" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk"><strong class="nz fr">Policy Network:</strong> implements a Gaussian Actor network computing the mean and log standard deviation for the action distribution.</p><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="ca31" class="rb nc fq qy b bg rc rd l re rf">class PolicyNetwork(nn.Module):<br/>    def __init__(self, state_dim, action_dim, hidden_dim):<br/>        super(PolicyNetwork, self).__init__()<br/>        self.fc1 = nn.Linear(state_dim, hidden_dim)<br/>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)<br/>        self.mean = nn.Linear(hidden_dim, action_dim)<br/>        self.log_std = nn.Linear(hidden_dim, action_dim)<br/><br/>    def forward(self, state):<br/>        x = F.relu(self.fc1(state))<br/>        x = F.relu(self.fc2(x))<br/>        mean = self.mean(x)<br/>        log_std = torch.clamp(self.log_std(x), -20, 2)  # Limit log_std to prevent instability<br/>        return mean, log_std</span></pre><p id="28c1" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk"><strong class="nz fr">Soft Q-Network: </strong>estimates the expected future reward given from a state-action pair for a defined optimal policy.</p><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="b550" class="rb nc fq qy b bg rc rd l re rf">class SoftQNetwork(nn.Module):<br/>    def __init__(self, state_dim, action_dim, hidden_dim):<br/>        super(SoftQNetwork, self).__init__()<br/>        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)<br/>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)<br/>        self.out = nn.Linear(hidden_dim, 1)<br/><br/>    def forward(self, state, action):<br/>        x = torch.cat([state, action], dim=-1)<br/>        x = F.relu(self.fc1(x))<br/>        x = F.relu(self.fc2(x))<br/>        return self.out(x)</span></pre><p id="5989" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk"><strong class="nz fr">Value Network: </strong>estimates the state value.</p><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="9409" class="rb nc fq qy b bg rc rd l re rf">class ValueNetwork(nn.Module):<br/>    def __init__(self, state_dim, hidden_dim):<br/>        super(ValueNetwork, self).__init__()<br/>        self.fc1 = nn.Linear(state_dim, hidden_dim)<br/>        self.fc2 = nn.Linear(hidden_dim, hidden_dim)<br/>        self.out = nn.Linear(hidden_dim, 1)<br/><br/>    def forward(self, state):<br/>        x = F.relu(self.fc1(state))<br/>        x = F.relu(self.fc2(x))<br/>        return self.out(x)</span></pre><p id="cda7" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The following snippet offers the key steps in updating the different variables corresponding to the SAC algorithm. As it starts by sampling a batch from the replay buffer for experience replay. Then, before computing the gradients, they are initialised to zero to ensure that gradients from previous batches are not accumulated. Then performs backpropagation and updates the weights of the network during training. The target and loss values are then updated for the Q-networks. These steps take place for all three methods.</p><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="640b" class="rb nc fq qy b bg rc rd l re rf">def update(batch_size, reward_scale, gamma=0.99, soft_tau=1e-2):<br/>    # Sample a batch<br/>    state, action, reward, next_state, done = replay_buffer.sample(batch_size)<br/>    state, next_state, action, reward, done = map(lambda x: torch.FloatTensor(x).to(device), <br/>                                                  [state, next_state, action, reward, done])<br/><br/>    # Update Q-networks<br/>    target_value = target_value_net(next_state)<br/>    target_q = reward + (1 - done) * gamma * target_value<br/>    q1_loss = F.mse_loss(soft_q_net1(state, action), target_q.detach())<br/>    q2_loss = F.mse_loss(soft_q_net2(state, action), target_q.detach())<br/><br/>    soft_q_optimizer1.zero_grad()<br/>    q1_loss.backward()<br/>    soft_q_optimizer1.step()<br/><br/>    soft_q_optimizer2.zero_grad()<br/>    q2_loss.backward()<br/>    soft_q_optimizer2.step()<br/><br/>    # Update Value Network<br/>    predicted_q = torch.min(soft_q_net1(state, action), soft_q_net2(state, action))<br/>    value_loss = F.mse_loss(value_net(state), predicted_q - alpha * log_prob)<br/>    value_optimizer.zero_grad()<br/>    value_loss.backward()<br/>    value_optimizer.step()<br/><br/>    # Update Policy Network<br/>    new_action, log_prob, _, _, _ = policy_net.evaluate(state)<br/>    policy_loss = (alpha * log_prob - predicted_q).mean()<br/>    policy_optimizer.zero_grad()<br/>    policy_loss.backward()<br/>    policy_optimizer.step()<br/><br/>    # Soft Update Target Network<br/>    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):<br/>        target_param.data.copy_(soft_tau * param.data + (1 - soft_tau) * target_param.data)</span></pre><p id="d7e5" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Finally, to run the code in the sac.py file, just run the following commands:</p><pre class="ml mm mn mo mp qx qy qz bp ra bb bk"><span id="dc79" class="rb nc fq qy b bg rc rd l re rf">python sac.py --train<br/>python sac.py --test</span></pre></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6fbf" class="nb nc fq bf nd ne pi gq ng nh pj gt nj nk pk nm nn no pl nq nr ns pm nu nv nw bk">Results and Visualisation</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ri"><img src="../Images/77207902ecc4b682d7a475496bd4b932.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcB7MHayhNcln57a1ABOJg.gif"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Training a ‘Reacher’ Robotic Arm, (generated by the author)</figcaption></figure><p id="c700" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">In training the SAC agent in both environments, I noticed that the action space of the problem affects the efficiency and the performance of the training. Indeed, when I trained the agent on the simple pendulum environment, the learning converged much faster and with lower oscillations. However, as the Reacher environment includes a more complicated continuous space of actions, the algorithm trained relatively well, but the big jump in the rewards was not seen as clearly. The Reacher was also trained on 4 times the number of episodes as that of the pendulum.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rj"><img src="../Images/6196e06d1570d595e76a5cff6916d1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IiNSqMC6_U6Ddpv1QLMAHw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Learning Performance by Maximising Reward (generated by the author)</figcaption></figure><p id="8ff1" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">The action distribution below shows that the policy has a diverse range of actions that it explores through the training process until it converges on one optimal policy. The hallmark of entropy-regularised algorithms such as SAC comes from the increase in exploration. We can also notice that the peaks correspond to action values with high expected rewards which drives the policy to converge toward a more deterministic behaviour.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rk"><img src="../Images/1231b79d7ed7b5a2561e80ac55eea9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDpqhd5qYW4oMQykpAE8uA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Action Space Usage Distribution (generated by the author)</figcaption></figure><p id="2e0a" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Speaking of a more deterministic behaviour, we observe that the entropy has decreased on average over the number of training episodes. However, this behaviour is expected, since the sole reason we want to maximise the entropy is to encourage more exploration. A higher exploration is mainly done early in the training process to exhaust most possible state-actions pairs that have higher returns.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rl"><img src="../Images/206dded1ba5479ad51d65ff6055f4164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHQg8NgM1lGxTdVm37C9Uw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Entropy Valuation Over Training Episodes (generated by the author)</figcaption></figure></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b7a7" class="nb nc fq bf nd ne pi gq ng nh pj gt nj nk pk nm nn no pl nq nr ns pm nu nv nw bk">Conclusion</h1><p id="a278" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The SAC algorithm is an off-policy RL framework that adopts a balance of exploitation and exploration through a new entropy term. The main objective function of the SAC algorithm includes maximising both the expected returns and the entropy during the training process, which address many of the issues the legacy frameworks suffer from. The use of twin Q-networks and automatic temperature tuning address high sample complexity, brittle convergence properties and complex hyperparameter tuning. SAC has proven to be highly effective in continuous control task domains. The results on action distribution and entropy reveal that the algorithm favours exploration in early training phases and diverse action sampling. As the agent trains, it converges to a more specific policy which reduces the entropy and reaches optimal actions. Consequently, it has been effectively used as an alternative for a wide range of domains in bioengineering for robotic control, drug discovery and drug delivery. Future implementations should focus on scaling the framework to more complex tasks and reducing its computational complexity.</p><h1 id="7404" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">References</h1><p id="1953" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D. (2015). Continuous control with deep reinforcement learning. [online] arXiv.org. Available at: <a class="af ou" href="https://arxiv.org/abs/1509.02971." rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1509.02971.</a></p><p id="9484" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O. (2017). Proximal Policy Optimization Algorithms. [online] arXiv.org. Available at: <a class="af ou" href="https://arxiv.org/abs/1707.06347." rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1707.06347.</a></p><p id="20cb" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Haarnoja, T., Zhou, A., Abbeel, P. and Levine, S. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv:1801.01290 [cs, stat]. [online] Available at: <a class="af ou" href="https://arxiv.org/abs/1801.01290." rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.01290.</a></p><p id="9686" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Du, H., Li, Z., Niyato, D., Yu, R., Xiong, Z., Xuemin, Shen and Dong In Kim (2023). Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks. doi:https://doi.org/10.48550/arxiv.2301.03220.</p><p id="d966" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">Svensson, H.G., Tyrchan, C., Engkvist, O. and Morteza Haghir Chehreghani (2024). Utilizing reinforcement learning for de novo drug design. Machine Learning, 113(7), pp.4811–4843. doi:https://doi.org/10.1007/s10994-024-06519-w.</p><p id="9b55" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">quantumiracle (2019). <em class="rh">GitHub — quantumiracle/Popular-RL-Algorithms: PyTorch implementation of Soft Actor-Critic (SAC), Twin Delayed DDPG (TD3), Actor-Critic (AC/A2C), Proximal Policy Optimization (PPO), QT-Opt, PointNet..</em> [online] GitHub. Available at: <a class="af ou" href="https://github.com/quantumiracle/Popular-RL-Algorithms" rel="noopener ugc nofollow" target="_blank">https://github.com/quantumiracle/Popular-RL-Algorithms</a> [Accessed 12 Dec. 2024].</p><p id="fde1" class="pw-post-body-paragraph nx ny fq nz b go ov ob oc gr ow oe of og ox oi oj ok oy om on oo oz oq or os fj bk">MrSyee (2019). <em class="rh">GitHub — MrSyee/pg-is-all-you-need: Policy Gradient is all you need! A step-by-step tutorial for well-known PG methods.</em> [online] GitHub. Available at: <a class="af ou" href="https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file." rel="noopener ugc nofollow" target="_blank">https://github.com/MrSyee/pg-is-all-you-need?tab=readme-ov-file.</a></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rm"><img src="../Images/b852f22c4c64850f1155b42ffaf02eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWaQe8bKNtMrIwfvkKakJw.png"/></div></div></figure></div></div></div></div>    
</body>
</html>