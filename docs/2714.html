<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tracing the Transformer in Diagrams</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Tracing the Transformer in Diagrams</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07">https://towardsdatascience.com/tracing-the-transformer-in-diagrams-95dbeb68160c?source=collection_archive---------2-----------------------#2024-11-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8bd6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">What exactly do you put in, what exactly do you get out, and how do you generate text with it?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Eric Silberstein" class="l ep by dd de cx" src="../Images/125b683e0433bb793d1bad1907f59d98.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*HSbVO3CG08mUVqfuLIc5QA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@eric.silberstein?source=post_page---byline--95dbeb68160c--------------------------------" rel="noopener follow">Eric Silberstein</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--95dbeb68160c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6365" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Last week I was listening to an Acquired <a class="af nf" href="https://www.acquired.fm/episodes/nvidia-the-dawn-of-the-ai-era" rel="noopener ugc nofollow" target="_blank">episode</a> on Nvidia. The episode talks about transformers: the <strong class="ml fr">T</strong> in GPT and a candidate for the biggest invention of the 21st century.</p><p id="dfc6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Walking down Beacon Street, listening, I was thinking, I understand transformers, right? You mask out tokens during training, you have these attention heads that learn to connect concepts in text, you predict the probability of the next word. I’ve downloaded LLMs from Hugging Face and played with them. I used GPT-3 in the early days before the “chat” part was figured out. At Klaviyo we even built one of the first GPT-powered generative AI features in our <a class="af nf" href="https://help.klaviyo.com/hc/en-us/articles/5051278887835" rel="noopener ugc nofollow" target="_blank">subject line assistant</a>. And way back I worked on a grammar checker powered by an older style language model. So maybe.</p><p id="caba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The transformer was invented by a team at Google working on automated translation, like from English to German. It was introduced to the world in 2017 in the now famous paper <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a>. I pulled up the paper and looked at Figure 1:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/09e8fb2cab4c2a77260296d1a6647181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0n5M-lTWbbuVB0ug"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Figure 1 from <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a></figcaption></figure><p id="cac6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hmm…if I understood, it was only at the most hand-wavy level. The more I looked at the diagram and read the paper, the more I realized I didn’t get the details. Here are a few questions I wrote down:</p><ul class=""><li id="63f5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nz oa ob bk">During training, are the inputs the tokenized sentences in English and the outputs the tokenized sentences in German?</li><li id="2651" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">What exactly is each item in a training batch?</li><li id="2f75" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">Why do you feed the output into the model and how is “masked multi-head attention” enough to keep it from cheating by learning the outputs from the outputs?</li><li id="a4c2" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">What exactly is multi-head attention?</li><li id="4243" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">How exactly is loss calculated? It can’t be that it takes a source language sentence, translates the whole thing, and computes the loss, that doesn’t make sense.</li><li id="69ba" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">After training, what exactly do you feed in to generate a translation?</li><li id="9ce9" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">Why are there three arrows going into the multi-head attention blocks?</li></ul><p id="c7d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I’m sure those questions are easy and sound naive to two categories of people. The first is people who were already working with similar models (e.g. RNN, encoder-decoder) to do similar things. They must have instantly understood what the Google team accomplished and how they did it when they read the paper. The second is the many, many more people who realized how important transformers were these last seven years and took the time to learn the details.</p><p id="1eb1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Well, I wanted to learn, and I figured the best way was to build the model from scratch. I got lost pretty quickly and instead decided to trace code someone else wrote. I found this terrific <a class="af nf" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">notebook</a> that explains the paper and implements the model in PyTorch. I copied the code and trained the model. I kept everything (inputs, batches, vocabulary, dimensions) tiny so that I could trace what was happening at each step. I found that noting the dimensions and the tensors on the diagrams helped me keep things straight. By the time I finished I had pretty good answers to all the questions above, and I’ll get back to answering them after the diagrams.</p><p id="e860" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here are cleaned up versions of my notes. Everything in this part is for training one single, tiny batch, which means all the tensors in the different diagrams go together.</p><p id="df40" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To keep things easy to follow, and copying an idea from the notebook, we’re going to train the model to copy tokens. For example, once trained, “dog run” should translate to “dog run”.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oh"><img src="../Images/26ed6a4129b8375201b96f830ac22a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*56UByGVQveJ5HwKK"/></div></div></figure><p id="5bef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In other words:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/de8a85de6f19d7b8e64e7bc1bf538b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EmFsyv1W0iIkl3tC"/></div></div></figure><p id="db08" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And here’s trying to put into words what the tensor dimensions (shown in purple) on the diagram so far mean:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/b2cea2931c64c5377a07ba2f7a7a619d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wuvphJ9Ni6udD8Ox"/></div></div></figure><p id="3ca3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One of the hyperparameters is <em class="oj">d-model</em> and in the base model in the paper it’s 512. In this example I made it 8. This means our embedding vectors have length 8. Here’s the main diagram again with dimensions marked in a bunch of places:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ok"><img src="../Images/f43ecc3439ffa117a867954961932e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*quXLQOA2FD_70ovh"/></div></div></figure><p id="d94f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s zoom in on the input to the encoder:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/7460fef236e794002d6f3e98912d05e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XD2LqPUTN-xwVU5B"/></div></div></figure><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/040c9af92d654c390791d91f6cae1e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Ah96R2Xcr8o6JKu2"/></div></div></figure><p id="1925" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Most of the blocks shown in the diagram (add &amp; norm, feed forward, the final linear transformation) act only on the last dimension (the 8). If that’s all that was happening then the model would only get to use the information in a single position in the sequence to predict a single position. Somewhere it must get to “mix things up” among positions and that magic happens in the multi-head attention blocks.</p><p id="001b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s zoom in on the multi-head attention block within the encoder. For this next diagram, keep in mind that in my example I set the hyperparameter <em class="oj">h</em> (number of heads) to <strong class="ml fr">2.</strong> (In the base model in the paper it’s 8.)</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ol"><img src="../Images/4cb5d05dc31aae173d6351e44e372b7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a2E31MK8548RA7x0"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Figure 2 from <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a> with annotations by author</figcaption></figure><p id="1b28" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How did (2,3,8) become (2,2,3,4)? We did a linear transformation, then took the result and split it into number of heads (8 / 2 = 4) and rearranged the tensor dimensions so that our second dimension is the head. Let’s look at some actual tensors:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh om"><img src="../Images/245a1c4c1f62b88cbbd0470faf87ee5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8kPtvT51SuOWhhh1"/></div></div></figure><p id="aa15" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We still haven’t done anything that mixes information among positions. That’s going to happen next in the scaled dot-product attention block. The “4” dimension and the “3” dimension will finally touch.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh on"><img src="../Images/50b88c4d8fc98f9b2eb44e751463caf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QO2_rxRvyGHhxYjh"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Figure 2 from <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a> with annotations by author</figcaption></figure><p id="6f97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s look at the tensors, but to make it easier to follow, we’ll look only at the first item in the batch and the first head. In other words, Q[0,0], K[0,0], etc. The same thing will be happening to the other three.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oo"><img src="../Images/fe5e540d35d0f111c4b2d70a834c1bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OlkRX6yofi4Er4BH"/></div></div></figure><p id="7535" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s look at that final matrix multiplication between the output of the softmax and V:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/09e63c393da6f1a8f830b3ea6646d45a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-IafgzGkCJLsZGT1"/></div></div></figure><p id="b188" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Following from the very beginning, we can see that up until that multiplication, each of the three positions in V going all the way back to our original sentence “&lt;start&gt; dog run” has only been operated on independently. This multiplication blends in information from other positions for the first time.</p><p id="fddb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Going back to the multi-head attention diagram, we can see that the concat puts the output of each head back together so each position is now represented by a vector of length 8. Notice that the <strong class="ml fr">1.8 </strong>and the <strong class="ml fr">-1.1 </strong>in the tensor after concat but before linear match the <strong class="ml fr">1.8</strong> and <strong class="ml fr">-1.1</strong> from the first two elements in the vector for the first position of the first head in the first item in the batch from the output of the scaled dot-product attention shown above. (The next two numbers match too but they’re hidden by the ellipses.)</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/9c1ba8f87c1319445aff846595fac271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mXuSFu4ACCYAJ_Q-"/></div></div></figure><p id="d5fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now let’s zoom back out to the whole encoder:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh op"><img src="../Images/5abbdf515d94462e1db8e6a8d9b89627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cSA7f5Fd8CLQb8Np"/></div></div></figure><p id="113e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At first I thought I would want to trace the feed forward block in detail. It’s called a “position-wise feed-forward network” in the paper and I thought that meant it might bring information from one position to positions to the right of it. <strong class="ml fr">However</strong>, it’s not that. “Position-wise” means that it operates independently on each position. It does a linear transform on each position from 8 elements to 32, does ReLU (max of 0 and number), then does another linear transform to get back to 8. (That’s in our small example. In the base model in the paper it goes from 512 to 2048 and then back to 512. There are a lot of parameters here and probably this is where a lot of the learning happens!) The output of the feed forward is back to (2,3,8).</p><p id="569c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Getting away from our toy model for a second, here’s how the encoder looks in the base model in the paper. It’s very nice that the input and output dimensions match!</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/bfb4c198389ce4988f4c2ef1a25f003f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dP_lGN4HvNR8okhE"/></div></div></figure><p id="848b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now let’s zoom out all the way so we can look at the decoder.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oq"><img src="../Images/4693f18a10a7f0c7d5278fa79359f84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ljQefPJvVp8xHJgh"/></div></div></figure><p id="cf31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We don’t need to trace most of the decoder side because it’s very similar to what we just looked at on the encoder side. However, the parts I labeled <strong class="ml fr">A</strong> and <strong class="ml fr">B</strong> are different. <strong class="ml fr">A</strong> is different because we do masked multi-head attention. This must be where the magic happens to not “cheat” while training. <strong class="ml fr">B </strong>we’ll come back to later. But first let’s hide the internal details and keep in mind the big picture of what we want to come out of the decoder.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh or"><img src="../Images/39a6c1f6a5b02411b6791edc07a68b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8cZhdweA9LWxgeIt"/></div></div></figure><p id="994c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And just to really drive home this point, suppose our English sentence is “she pet the dog” and our translated Pig Latin sentence is “eshay etpay ethay ogday”. If the model has “eshay etpay ethay” and is trying to come up with the next word, “ogday” and “atcay” are both high probability choices. Given the context of the full English sentence of “she pet the dog,” it really should be able to choose “ogday.” However, if the model could see the “ogday” during training, it wouldn’t need to learn how to predict using the context, it would just learn to copy.</p><p id="3175" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s see how the masking does this. We can skip ahead a bit because the first part of <strong class="ml fr">A</strong> works exactly the same as before where it applies linear transforms and splits things up into heads. The only difference is the dimensions coming into the scaled dot-product attention part are (2,2,2,4) instead of (2,2,3,4) because our original input sequence is of length two. Here’s the scaled dot-product attention part. As we did on the encoder side, we’re looking at only the first item in the batch and the first head.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh os"><img src="../Images/d07ad1d278dca4dbffcae5da009037af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eOKuo35etXk7EHZp"/></div></div></figure><p id="b2df" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This time we have a mask. Let’s look at the final matrix multiplication between the output of the softmax and V:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/e988447d3a58a2c9be0ff1169542fc14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SwsagZMnrnaKprmA"/></div></div></figure><p id="2259" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we’re ready to look at <strong class="ml fr">B, </strong>the second multi-head attention in the decoder. Unlike the other two multi-head attention blocks, we’re not feeding in three identical tensors, so we need to think about what’s V, what’s K and what’s Q. I labeled the inputs in red. We can see that V and K come from the output of the encoder and have dimension (2,3,8). Q has dimension (2,2,8).</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ot"><img src="../Images/f093df7fc5079f0340500b5cfd65ea92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZWhy9UK6SuQgawzG"/></div></div></figure><p id="4ed4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As before, we skip ahead to the scaled dot-product attention part. It makes sense, but is also confusing, that V and K have dimensions (2,2,3,4) — two items in the batch, two heads, three positions, vectors of length four, and Q has dimension (2,2,2,4).</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh on"><img src="../Images/ce9a1ee8a761e37d6139e0ce59702fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*thRHbOZoxrIiFMVZ"/></div></div></figure><p id="e2d5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even though we’re “reading from” the encoder output where the “sequence” length is three, somehow all the matrix math works out and we end up with our desired dimension (2,2,2,4). Let’s look at the final matrix multiplication:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/5e07f351a7453ad38a0f298f6b45b7c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UppA0Tci4o3ec1cy"/></div></div></figure><p id="1589" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The outputs of each multi-head attention block get added together. Let’s skip ahead to see the output from the decoder and turning that into predictions:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ou"><img src="../Images/61b29eaddad690ae6ad30dacaeea6448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OKTbDXtNg3mK_uuk"/></div></div></figure><p id="a2d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The linear transform takes us from <strong class="ml fr">(2,2,8)</strong> to <strong class="ml fr">(2,2,5)</strong>. Think about that as reversing the embedding, except that instead of going from a vector of length 8 to the integer identifier for a single token, we go to a probability distribution over our vocabulary of 5 tokens. The numbers in our tiny example make that seem a little funny. In the paper, it’s more like going from a vector of size 512 to a vocabulary of 37,000 when they did English to German.</p><p id="0611" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a moment we’ll calculate the loss. First, though, even at a glance, you can get a feel for how the model is doing.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/8141557f54786f52abad6e4552d81d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rrb_plxJGA_QGSrp"/></div></div></figure><p id="c20d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It got one token right. No surprise because this is our first training batch and it’s all just random. One nice thing about this diagram is it makes clear that this is a multi-class classification problem. The classes are the vocabulary (5 classes in this case) and, <strong class="ml fr">this is what I was confused about before</strong>, we make (and score) one prediction per token in the translated sentence, <strong class="ml fr">NOT</strong> one prediction per sentence. Let’s do the actual loss calculation.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/929b25a262696ce2b80901d89c52e87c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cz00yFrLo6aOn4mJ"/></div></div></figure><p id="b89a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If, for example, the -3.2 became a -2.2, our loss would decrease to 5.7, moving in the desired direction, because we want the model to learn that the correct prediction for that first token is 4.</p><p id="602d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The diagram above leaves out label smoothing. In the actual paper, the loss calculation smooths labels and uses KL Divergence loss. I think that works out to be the same or simialr to cross entropy when there is no smoothing. Here’s the same diagram as above but with label smoothing.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/5472835199850c82a2efa3d615797cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bmQ42BcefupLibI5"/></div></div></figure><p id="1884" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s also take a quick look at the number of parameters being learned in the encoder and decoder:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ov"><img src="../Images/6a40913435a4b778cb4ba27d450fac77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4feLImZYN6353z9M"/></div></div></figure><p id="b287" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As a sanity check, the feed forward block in our toy model has a linear transformation from 8 to 32 and back to 8 (as explained above) so that’s 8 * 32 (weights) + 32 (bias) + 32 * 8 (weights) + 8 (bias) = 52. Keep in mind that in the base model in the paper, where <em class="oj">d-model</em> is 512 and <em class="oj">d-ff</em> is 2048 and there are 6 encoders and 6 decoders there will be many more parameters.</p><h1 id="a216" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Using the trained model</h1><p id="d1f2" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">Now let’s see how we put source language text in and get translated text out. I’m still using a toy model here trained to “translate” by coping tokens, but instead of the example above, this one uses a vocabulary of size 11 and <em class="oj">d-model</em> is 512. (Above we had vocabulary of size 5 and <em class="oj">d-model</em> was 8.)</p><p id="8a55" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First let’s do a translation. Then we’ll see how it works.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh oi"><img src="../Images/2fa933c517521e3d0104917cda11f368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Et5dA5xxskf6snWb"/></div></div></figure><p id="cf2b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Step one is to feed the source sentence into the <strong class="ml fr">encoder</strong> and hold onto its output, which in this case is a tensor with dimensions (1, 10, 512).</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh px"><img src="../Images/7868757361bb9e69583a456a57341d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KsKABDPM7_qYSvEK"/></div></div></figure><p id="4047" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Step two is to feed the first token of the output into the <strong class="ml fr">decoder</strong> and predict the second token. We know the first token because it’s always &lt;start&gt; = 1.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh py"><img src="../Images/43afa5686eb5be64c9782fdb9da98ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*px_SwTU48jlU0tui"/></div></div></figure><p id="1252" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the paper, they use beam search with a beam size of 4, which means we would consider the 4 highest probability tokens at this point. To keep things simple I’m going to instead use greedy search. You can think of that as a beam search with a beam size of 1. So, reading off from the top of the diagram, the highest probability token is number <strong class="ml fr">5</strong>. (The outputs above are logs of probabilities. The highest probability is still the highest number. In this case that’s -0.0 which is actually -0.004 but I’m only showing one decimal place. The model is really confident that 5 is correct! exp(-0.004) = 99.6%)</p><p id="135d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we feed [1,5] into the decoder. (If we were doing beam search with a beam size of 2, we could instead feed in a batch containing [1,5] and [1,4] which is the next most likely.)</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pz"><img src="../Images/3cf05eff5a17c139511dfce64664eb24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JrcSiTra2rU-wjMf"/></div></div></figure><p id="4545" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we feed [1,5,4]:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qa"><img src="../Images/fcbe3b72d487fe2611d647531bd68e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OwQzrxM2r-_-tNPC"/></div></div></figure><p id="3938" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And get out <strong class="ml fr">3</strong>. And so on until we get a token that indicates the end of the sentence (not present in our example vocabulary) or hit a maximum length.</p><h1 id="b341" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Circling back to the questions above</h1><p id="cf26" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">Now I can mostly answer my original questions.</p><h1 id="b09f" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">During training, are the inputs the tokenized sentences in English and the outputs the tokenized sentences in German?</h1><p id="4dcc" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">Yes, more or less.</p><h1 id="a23a" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">What exactly is each item in a training batch?</h1><p id="2330" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">Each item corresponds to one translated sentence pair.</p><ul class=""><li id="91f1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nz oa ob bk">The “x” of the item has two parts. The first part is all the tokens of the source sentence. The second part is all tokens of the target sentence except for the last one.</li><li id="41c4" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne nz oa ob bk">The “y” (label) of the item is all tokens of the target sentence except for the first one. Since the first token for source and target is always &lt;start&gt;, we’re not wasting or losing any training data.</li></ul><p id="2a9f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What’s a little subtle is that if this were a classification task where say the model had to take an image and output a class (house, car, rabbit, etc.), we would think of each item in the batch as contributing one “classification” to the loss calculation. Here, however, each item in the batch will contribute (number_of_tokens_in_target_sentence — 1) “classifications” to the loss calculation.</p><h1 id="0f79" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Why do you feed the output into the model and how is “masked multi-head attention” enough to keep it from cheating by learning the outputs from the outputs?</h1><p id="c9f7" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">You feed the output so the model can learn to predict the translation based both on the meaning of the source sentence and the words translated so far. Although lots of things are going on in the model, the only time information moves between positions is during the attention steps. Although we do feed the translated sentence into the decoder, the first attention calculation uses a mask to zero out all information from positions beyond the one we’re predicting.</p><h1 id="2966" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">What exactly is multi-head attention?</h1><p id="7f00" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">I probably should have asked what exactly is attention, because that’s the more central concept. Multi-head attention means slicing the vectors up into groups, doing attention on the groups, and then putting the groups back together. For example, if the vectors have size 512 and there are 8 heads, attention will be done independently on 8 groups each containing a full batch of the full positions, each position having a vector of size 64. If you squint, you can see how each head could end up learning to give attention to certain connected concepts as in the famous visualizations showing how a head will learn what word a pronoun references.</p><h1 id="6214" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">How exactly is loss calculated? It can’t be that it takes a source language sentence, translates the whole thing, and computes the loss, that doesn’t make sense.</h1><p id="b66e" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">Right. We’re not translating a full sentence in one go and calculating overall sentence similarity or something like that. Loss is calculated just like in other multi-class classification problems. The classes are the tokens in our vocabulary. The trick is we’re independently predicting a class for every token in the target sentence using only the information we should have at that point. The labels are the actual tokens from our target sentence. Using the predictions and labels we calculate loss using cross entropy. (In reality we “smooth” our labels to account for the fact that they’re notabsolute, a synonym could sometimes work equally well.)</p><h1 id="5df0" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">After training, what exactly do you feed in to generate a translation?</h1><p id="d8fd" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">You can’t feed something in and have the model spit out the translation in a single evaluation. You need to use the model multiple times. You first feed the source sentence into the encoder part of the model and get an encoded version of the sentence that represents its meaning in some abstract, deep way. Then you feed that encoded information and the start token &lt;start&gt; into the decoder part of the model. That lets you predict the second token in the target sentence. Then you feed in the &lt;start&gt; and second token to predict the third. You repeat this until you have a full translated sentence. (In reality, though, you consider multiple high probability tokens for each position, feed multiple candidate sequences in each time, and pick the final translated sentence based on total probability and a length penalty.)</p><h1 id="1f28" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Why are there three arrows going into the multi-head attention blocks?</h1><p id="067f" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">I’m guessing three reasons. 1) To show that the second multi-head attention block in the decoder gets some of its input from the encoder and some from the prior block in the decoder. 2) To hint at how the attention algorithm works. 3) To hint that each of the three inputs undergoes its own independent linear transformation before the actual attention happens.</p><h1 id="f7ad" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Conclusion</h1><p id="50bc" class="pw-post-body-paragraph mj mk fq ml b go ps mn mo gr pt mq mr ms pu mu mv mw pv my mz na pw nc nd ne fj bk">It’s beautiful! I probably wouldn’t think that if it weren’t so incredibly useful. I now get the feeling people must have had when they first saw this thing working. This elegant and trainable model expressible in very little code learned how to translate human languages and beat out complicated machine translations systems built over decades. It’s amazing and clever and unbelievable. You can see how the next step was to say, forget about translated sentence pairs, let’s use this technique on every bit of text on the internet — and LLMs were born!</p><p id="3dad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(I bet have some mistakes above. Please LMK.)</p><p id="a52b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="oj">Unless otherwise noted, all images are by author, or contain annotations by the author on figures from </em><a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank"><em class="oj">Attention Is All You Need</em></a>.</p></div></div></div></div>    
</body>
</html>