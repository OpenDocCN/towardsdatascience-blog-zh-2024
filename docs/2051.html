<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Learning to Unlearn: Why Data Scientists and AI Practitioners Should Understand Machine Unlearning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Learning to Unlearn: Why Data Scientists and AI Practitioners Should Understand Machine Unlearning</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22">https://towardsdatascience.com/learning-to-unlearn-why-data-scientists-and-ai-practitioners-should-understand-machine-unlearning-866af9e5d712?source=collection_archive---------8-----------------------#2024-08-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/b325affbd7356ef4d8a853261870be17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*25o-i-Q8CdKyVUpz"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Photo by <a class="af gi" href="https://unsplash.com/@winniepix?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sue Winston</a> on <a class="af gi" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div/><div><h2 id="2711" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">Explore the intersections between privacy and AI with a guide to removing the impact of individual data points in AI training using the SISA technique applied to Convolutional Neural Networks (CNNs) using Python.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Raul Vizcarra Chirinos" class="l ep by dd de cx" src="../Images/9f507c6b9542809b9a32ab185e953ca1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*CkLxvl5GnPAP1DeEhnknnw.jpeg"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@raul.vizcarrach?source=post_page---byline--866af9e5d712--------------------------------" rel="noopener follow">Raul Vizcarra Chirinos</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--866af9e5d712--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Aug 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lx ly ab q ee lz ma" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lw"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lv lw">5</span></p></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="mb k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mc an ao ap iy md me mf" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mg cn"><div class="l ae"><div class="ab cb"><div class="mh mi mj mk ml gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6f96" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">To</span> the date that this article is being written and based on <a class="af gi" href="https://data.worldbank.org/country" rel="noopener ugc nofollow" target="_blank">World Bank data</a>, over 32% of the world‚Äôs population (approximately 8 billion) is under twenty years old. This means that approximately 2.6 billion people were born in the social media era, and it‚Äôs highly probable that almost all their lives have been registered online, by their parents, their inner circle, or in the end by themselves <em class="oi">(depending on their attachment to social media as well as their network)</em>. If we add the people who are between their twenties and fifties, we have an extra 3.3 billion people who, to some extent, have a part of their lives registered online in different sources and formats <em class="oi">(images, comments, videos, etc.)</em>. Of course, we can adjust the numbers considering the people over fifty, or that not everyone in the world has access to or uses the internet <em class="oi">(</em><a class="af gi" href="https://data.worldbank.org/indicator/IT.NET.USER.ZS?locations=ET%2F" rel="noopener ugc nofollow" target="_blank"><em class="oi">at least more than 35% don‚Äôt have access or use it, based on World Bank estimations in 2021</em></a><em class="oi">)</em>, but I‚Äôm sure you understand my point. There is a significant amount of our lives registered in today‚Äôs digital world.</p><p id="f70b" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another high probability or maybe certainty <em class="oi">(</em><a class="af gi" href="https://youtu.be/mAUpxN-EIgU?feature=shared" rel="noopener ugc nofollow" target="_blank"><em class="oi">we could ask again OpenAI‚Äôs CTO</em></a><em class="oi">üôÑ)</em> is that much of this data is being used or has been used to train all the ‚Äústate-of-the-art‚Äù models being deployed today, from LLMs to multimodal AI models that can process information such as images, videos, or text. In this context, when it comes to data, technology, and privacy, we often find two sides struggling to find a middle ground. On one side is the social contract that each individual has with technology, where we are willing to trade some rights in our data for the benefits that technology offers us. On the other side, is the question of where the line has to be drawn, as most defenders of this position say, <strong class="nf gm"><em class="oi">‚ÄúJust because data is accessible doesn‚Äôt mean that it is free to collect and use‚Äù</em></strong><em class="oi">.</em></p><p id="02c2" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, we‚Äôll explore some challenges that emerge when discussing <strong class="nf gm">privacy in terms of AI</strong>, including a brief overview of <strong class="nf gm">Machine Unlearning</strong> and the <strong class="nf gm">SISA training approach (Sharded, Isolated, Sliced, and Aggregated training), </strong>a machine unlearning framework recently developed to help manage or reduce the impact of individual data points in AI training and address the compliance challenge related to ‚Äú<strong class="nf gm">The Right to Be Forgotten‚Äù</strong>.</p><figure class="ok ol om on oo fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp oj"><img src="../Images/87a2b54cfa5c87c3ce817b3778519984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9-piX74JxML5I8Zz"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Photo by <a class="af gi" href="https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tingey Injury Law Firm</a> on <a class="af gi" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ea4a" class="op oq gl bf or os ot hl ou ov ow ho ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">What is whispered in the closet shall be proclaimed from the house-tops</h1><p id="9353" class="pw-post-body-paragraph nd ne gl nf b hj pl nh ni hm pm nk nl nm pn no np nq po ns nt nu pp nw nx ny fj bk">One of the first publications in history to advocate for a <strong class="nf gm">right to privacy</strong> is an essay published in the 1890s by two American lawyers, Samuel D. Warren and Louis Brandeis. The essay, titled <a class="af gi" href="https://www.cs.cornell.edu/~shmat/courses/cs5436/warren-brandeis.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="nf gm"><em class="oi">The Right to Privacy</em></strong></a><strong class="nf gm"><em class="oi">,</em></strong> was written to raise awareness about the effects of unauthorized photographs and early newspaper enterprises, which in their own words, have turned gossip into a commodity and harmed the individual‚Äôs right to enjoy life, <strong class="nf gm">the right to be left alone</strong>.</p><blockquote class="pq"><p id="3e22" class="pr ps gl bf pt pu pv pw px py pz ny dx">That the individual shall have full protection in person and in property is a principle as old as the common law; but it has been found necessary from time to time to define anew the exact nature and extent of such protection. ‚Ä¶.Recent inventions and business methods call attention to the next step which must be taken for the protection of the person, and for securing to the individual what Judge Cooley calls the right ‚Äúto be let alone‚Äù (Samuel D. Warren, Louis Brandeis. 1890)</p></blockquote><p id="dbd8" class="pw-post-body-paragraph nd ne gl nf b hj qa nh ni hm qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">Times have changed since the publication of The Right to Privacy, but Warren and Louis Brandeis were not mistaken about one thing; <strong class="nf gm">technological, political, social, and economic changes constantly challenge existing or new rights</strong>. In response, the common law should always remain open-minded to meet the new demands of society, recognizing that the protection of society primarily comes through acknowledging the rights of the individual.</p><p id="4f7a" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since then, privacy has often been associated with a <strong class="nf gm">traditional approach</strong> of <strong class="nf gm">securing and protecting what we care about and want behind closed curtains</strong>, keeping it out of the public eye, and controlling its access and use. But it‚Äôs also true that its boundaries have been tested over time by disruptive technologies; photography and video set new boundaries, and recently, the exponential growth of data. But data-based technologies not only impacted the data compliance landscape; they also had some impacts on our beliefs and customs. This has been the case with social media platforms or super apps , where we are willing to trade some rights in our data for the benefits that technology offers us. This means that <strong class="nf gm">context matters</strong>, and in some cases, sharing our sensitive information relies more on values like trust than necessarily considering a breach of privacy.</p><blockquote class="pq"><p id="7fe4" class="pr ps gl bf pt pu pv pw px py pz ny dx"><em class="qf">‚ÄúData is not simply ‚Äòprivate‚Äô or ‚Äònot private‚Äô or ‚Äòsensitive‚Äô or ‚Äònon-sensitive‚Äô. Context matters, as do normative social values‚Ä¶‚Äù (</em><a class="af gi" href="https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/ethics-of-advanced-ai-assistants/the-ethics-of-advanced-ai-assistants-2024-i.pdf" rel="noopener ugc nofollow" target="_blank"><em class="qf">The Ethics of Advanced AI Assistants. Google DeepMind 2024</em></a><em class="qf">)</em></p></blockquote><p id="adf0" class="pw-post-body-paragraph nd ne gl nf b hj qa nh ni hm qb nk nl nm qc no np nq qd ns nt nu qe nw nx ny fj bk">The <strong class="nf gm">relation between context and privacy</strong> is an interesting line of thought known as the model of informational privacy in terms of<br/><strong class="nf gm">‚ÄúContextual Integrity ‚Äù <em class="oi">(</em></strong><a class="af gi" href="https://digitalcommons.law.uw.edu/cgi/viewcontent.cgi?article=4450&amp;context=wlr" rel="noopener ugc nofollow" target="_blank"><em class="oi">Nissenbaum, 2004</em></a><em class="oi">)</em><strong class="nf gm">.</strong> It states that in every exchange or flow of information between a sender and a receiver, there are social rules governing it. Understanding these rules is essential for ensuring that the exchange of information is properly regulated.</p><figure class="ok ol om on oo fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qg"><img src="../Images/9aebf190562c633ed1345dca69ae20b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TIutJBjjdT0DkgiwmgpiVQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 01 Source: Author‚Äôs own creation</figcaption></figure><p id="df60" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A clear example could be, for instance, information regarding my child‚Äôs performance in school. If a teacher shared records of my child‚Äôs performance with other parents or strangers outside the school, I might consider that a privacy breach. However, if the same teacher shared that same information with other teachers who teach my child to share experiences and improve my child‚Äôs performance in school, I might not be as concerned and would rely on trust, values, and the good judgment of the teachers. So, <strong class="nf gm">under the Contextual Integrity approach</strong>, privacy is not judged as the rigid state of ‚Äúthe right to be left alone‚Äù. Rather, <strong class="nf gm">what matters is that the flow of information is appropriately regulated</strong>, taking into account the context and the governing norms within it to establish the limits. Privacy as a fundamental right shouldn‚Äôt be changed, but it could be rethinked.</p><blockquote class="qh qi qj"><p id="86eb" class="nd ne oi nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Should the rigid concept of privacy remain unchanged? Or should we begin by first understanding the social rules governing information flows?</strong></p></blockquote><p id="e157" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As Artificial Intelligence continues to shape the future, this rethinking challenges us to consider adapting existing rights or possibly introducing new digital rights.</p><h1 id="4770" class="op oq gl bf or os ot hl ou ov ow ho ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">Machine Unlearning</h1><p id="3270" class="pw-post-body-paragraph nd ne gl nf b hj pl nh ni hm pm nk nl nm pn no np nq po ns nt nu pp nw nx ny fj bk">Whether you think of privacy as a rigid concept or consider the contextual integrity approach, I think most of us would agree that we all deserve our data to be processed fairly, with our consent, and with the ability to rectify or erase it if necessary.</p><p id="1bba" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While GDPR has facilitated the coexistence of data and privacy, <strong class="nf gm">balancing privacy and AI within regulatory frameworks presents a different challenge</strong>. Though we can erase or modify sensitive data from datasets, doing so in AI models is more complex. They aren‚Äôt retrained daily, and in most cases, it takes months to ensure their reliability. To address the task of selectively removing specific training data points (<em class="oi">and their influence</em>) in AI models without significantly sacrificing the model‚Äôs performance, techniques like <strong class="nf gm">Machine Unlearning</strong> have appeared and are being researched to find solutions to privacy concerns, comply with any possible enforced regulations, and protect users‚Äô legal rights to erasure or rectification.</p><p id="3299" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In contrast with the study of privacy policy, which can be traced back more than one hundred years, machine unlearning is a relatively new field, with initial studies appearing only about 10 years ago (<a class="af gi" href="https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf" rel="noopener ugc nofollow" target="_blank">Y. Cao and J. Yang, 2015</a>).</p><p id="d1de" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm"><em class="oi">So why should we be interested in machine unlearning?</em></strong> Whether you are an AI researcher pushing boundaries, working in AI solutions to make AI friendly for end users, here are some good reasons to adopt machine unlearning techniques in your ML processes:</p><p id="6342" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">¬∑ <strong class="nf gm">The Right to be Forgotten (RTBF):</strong> LLMs and state-of-the-art foundation models process data in complex, rapidly evolving ways. As seen with GDPR, it‚Äôs only a matter of time before <strong class="nf gm">the Right to Erasure</strong> is requested by users and adapted into regulations applied to AI. This will require any company using AI to adjust processes to meet regulations and follow user requests to remove personal data from pre-trained models.</p><p id="3ca8" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">¬∑ <strong class="nf gm">The Non-Zero Influence:</strong> Frameworks like <strong class="nf gm">differential privacy</strong> exist today to ensure some privacy for sensitive datasets by introducing noise to hide the contribution of any single datapoint. However, while differential privacy helps to mitigate the influence of a single datapoint, that effort is still ‚Äú<strong class="nf gm">non-zero‚Äù</strong>. This means there is still a possibility that the targeted datapoint has some kind of influence on the model. In a scenario where a datapoint needs to be<strong class="nf gm"> completely removed</strong>, different approaches to differential privacy may be required.</p><p id="349c" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">¬∑ <strong class="nf gm">Performance Optimization:</strong> It‚Äôs well known that foundation models are trained with significant amounts of data, requiring intensive time and compute resources. <strong class="nf gm">Retraining a complete model from scratch to remove a single datapoint may be the most effective way</strong> to erase any influence of that datapoint within the model, <strong class="nf gm">but it‚Äôs not the most efficient approach</strong> <em class="oi">(models would need to be retrained frequentlyüò®)</em>. The machine unlearning landscape addresses this problem by considering time and compute resources as constraints in the process of reversing or negating the effect of specific datapoints on a model‚Äôs parameters.</p><p id="5215" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">¬∑ <strong class="nf gm">Cybersecurity: </strong>Models are not exempt from attacks by adversaries who inject data to manipulate the model‚Äôs behavior to provide sensitive information about users. Machine unlearning can help remove harmful datapoints and protect the sensitive information used to train the model.</p><p id="a3eb" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the machine unlearning landscape, we find two lines of thought: <strong class="nf gm">Exact Machine Unlearning</strong> and <strong class="nf gm">Approximate Machine Unlearning</strong>. While <strong class="nf gm">Exact Machine Unlearning</strong> focuses on eliminating the influence of specific data points by removing them completely <em class="oi">(as if that data had never been introduced to the model)</em>, <strong class="nf gm">Approximate Machine Unlearning</strong> aims to efficiently reduce the influence of specific data points in a trained model <em class="oi">(making the model‚Äôs behavior approximate how it would be if the data points had never been introduced)</em>. Both approaches provide diverse techniques to address users‚Äô right to erasure, considering constraints like deterioration in model performance, compute resources, time consumption, storage resources, specific learning models, or data structures.</p><p id="b56d" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a better understanding of ongoing work in this field, I suggest two interesting readings: <a class="af gi" href="https://arxiv.org/pdf/2308.07061" rel="noopener ugc nofollow" target="_blank"><em class="oi">Machine Unlearning: Solutions and Challenges (2024)</em></a> and <a class="af gi" href="https://arxiv.org/pdf/2305.07512" rel="noopener ugc nofollow" target="_blank"><em class="oi">Learn to Unlearn: Insights into Machine Unlearning (2023)</em></a>. Both papers provide a good recap of the extraordinary work of scientists and researchers in the Machine Unlearning field over the past few years.</p><h1 id="77cd" class="op oq gl bf or os ot hl ou ov ow ho ox oy oz pa pb pc pd pe pf pg ph pi pj pk bk">SISA <strong class="al">(Sharded, Isolated, Sliced, and Aggregated)</strong></h1><p id="9e00" class="pw-post-body-paragraph nd ne gl nf b hj pl nh ni hm pm nk nl nm pn no np nq po ns nt nu pp nw nx ny fj bk"><strong class="nf gm">The SISA framework is part of the Exact Machine Unlearning line of thought </strong>and aims to remove data without requiring a full retraining of the model. The framework begins with the premise that, although retraining from scratch, excluding the data points that need to be unlearned, is the most straightforward way to align with the ‚ÄúRight to be Forgotten‚Äù principle <em class="oi">(providing proof and assurance that the unwanted data has been removed)</em>, it also recognizes that this could be perceived as a na√Øve strategy when it comes to complex foundation models trained with large datasets, which demand high resources to be trained. So, in order to tackle the endeavor of resolving the process of unlearning, any technique should meet the following requirements:</p><ol class=""><li id="602a" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qk ql qm bk"><strong class="nf gm">Easy to Understand (Intelligibility):</strong> The technique should be easy to understand and implement.</li><li id="8c8f" class="nd ne gl nf b hj qn nh ni hm qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny qk ql qm bk"><strong class="nf gm">Accuracy: </strong>Although it is reasonable that some accuracy may be lost, the gap should be small.</li><li id="aa4e" class="nd ne gl nf b hj qn nh ni hm qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny qk ql qm bk"><strong class="nf gm">Time/Compute Efficient: </strong>It should require less time compared to exclude data points from scratch and use compute resources similar to those already existing for training procedures.</li><li id="419a" class="nd ne gl nf b hj qn nh ni hm qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny qk ql qm bk"><strong class="nf gm">Easy to Verify (Provable Guarantee): </strong>The technique should clearly demonstrate that the solicited data points have been unlearned without affecting the model parameters, and the proof can be easily explained (even to non-experts).</li><li id="a505" class="nd ne gl nf b hj qn nh ni hm qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny qk ql qm bk"><strong class="nf gm">Model Agnostic:</strong> It should be applicable to models of varying nature and complexity.</li></ol><blockquote class="qh qi qj"><p id="f7e0" class="nd ne oi nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">How can we guarantee the complete removal of specific training data points? How do we verify the success of such unlearning processes?</strong></p></blockquote><p id="0511" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The SISA framework (Sharded, Isolated, Sliced, and Aggregated) was first introduced in 2019 in the paper <em class="oi">‚Äú</em><a class="af gi" href="https://arxiv.org/abs/1912.03817" rel="noopener ugc nofollow" target="_blank"><em class="oi">Machine Unlearning</em></a><em class="oi">‚Äù</em> (Bourtoule et al.) to present an <strong class="nf gm">alternative solution to the problem of unlearning data from ML models, ensuring that the removal guarantee is easy to comprehend</strong>. The paper is easy to read in its introductory pages but could become complex if you are unfamiliar with the machine learning landscape. So, I‚Äôll try to summarize some of the interesting characteristics I find in the technique, but if you have the time, I strongly recommend giving the paper a try, it‚Äôs worth reading! <em class="oi">(An interesting presentation of the paper‚Äôs findings can also be watched in </em><a class="af gi" href="https://youtu.be/xUnMkCB0Gns?feature=shared" rel="noopener ugc nofollow" target="_blank"><em class="oi">this video</em></a><em class="oi"> made by the authors at the IEEE Symposium on Security and Privacy)</em></p><p id="b926" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The SISA training approach <strong class="nf gm">involves replicating the model several times</strong>, with each replica<strong class="nf gm"> trained on a different subset of the dataset</strong> <em class="oi">(known as a shard)</em>. <strong class="nf gm">Each model is referred to as a ‚Äúconstituent model‚Äù</strong>. Within each shard, <strong class="nf gm">the data is further divided into ‚Äúslices‚Äù</strong>, and incremental learning is applied with parameters archived accordingly. Each constituent model works primarily with its assigned shard during the training phase, while the slices are used within each shard to manage the data and support incremental learning. After training, the sub-models from each shard are aggregated to form the final model. During inference, <strong class="nf gm">predictions from the various constituent models are combined to produce an overall prediction</strong>. <strong class="nf gm">Figure 02</strong> ilustrates how the SISA training approach works.</p><figure class="ok ol om on oo fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qs"><img src="../Images/8ff74577dc3e5573ec798beda7bfe2ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjMQTEelpNHUPIio70rYkA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 02 Source: Author‚Äôs own creation based on Bourtoule et al. paper (2019)</figcaption></figure><p id="8856" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">When data needs to be unlearned</strong>, only the constituent models whose shards contains the point to be unlearned is retrained <em class="oi">(a data point is unlearned from a particular slice in a particular shard)</em>.</p><h2 id="de09" class="qt oq gl bf or qu qv qw ou qx qy qz ox nm ra rb rc nq rd re rf nu rg rh ri rj bk">Applying SISA: Unlearning and Retraining a CNN Model for Image Recognition</h2><p id="95f1" class="pw-post-body-paragraph nd ne gl nf b hj pl nh ni hm pm nk nl nm pn no np nq po ns nt nu pp nw nx ny fj bk">To understand how SISA can be applied, I will work on a use case example using Python. Recently, using PyTorch, computer vision techniques, and a Convolutional Neural Network (CNN), I built a basic setup to track hockey players and teams and gather some basic performance statistics <em class="oi">(</em><a class="af gi" href="https://medium.com/towards-data-science/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a" rel="noopener"><em class="oi">you can access the full article here</em></a><em class="oi">)</em>.</p><figure class="ok ol om on oo fw fo fp paragraph-image"><div class="fo fp rk"><img src="../Images/7a960baa2383b62cff2c4cf942ad002a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*rKPlNLBkmkR07BsYfzrlUQ.gif"/></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Player Tracking with Computer Vision</figcaption></figure><p id="399d" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although consent to use the 40-second video for the project was provided by the Peruvian Inline Hockey Association (APHL), <strong class="nf gm">let‚Äôs imagine a scenario for our SISA use case: a player has complained about his images being used and, exercising his erasure rights, has requested the removal of his images from the CNN pre-trained model that classifies players into each team</strong>. This would require us to remove the images from the training dataset and retrain the entire model. However, by applying the SISA technique, we would only need to work on the shards and slices containing those images, thus avoiding the need to retrain the model from scratch and optimizing time.</p><p id="8675" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The original CNN model was structured as follows:</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="355f" class="rp oq gl rm b bg rq rr l rs rt"># ************CONVOLUTIONAL NEURAL NETWORK-THREE CLASSES DETECTION**************************<br/># REFEREE<br/># WHITE TEAM (white_away)<br/># YELLOW TEAM (yellow_home)<br/><br/>import os<br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>import torchvision.transforms as transforms<br/>import torchvision.datasets as datasets<br/>from torch.utils.data import DataLoader<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score<br/>import matplotlib.pyplot as plt<br/><br/><br/>#******************************Data transformation********************************************<br/># Training and Validation Datasets<br/>data_dir = 'D:/PYTHON/teams_sample_dataset'<br/><br/>transform = transforms.Compose([<br/>    transforms.Resize((150, 150)),<br/>    transforms.ToTensor(),<br/>    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>])<br/><br/># Load datasets<br/>train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)<br/>val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)<br/><br/>train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)<br/>val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)<br/><br/>#********************************CNN Model Architecture**************************************<br/>class CNNModel(nn.Module):<br/>    def __init__(self):<br/>        super(CNNModel, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)<br/>        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)<br/>        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)<br/>        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)<br/>        self.fc1 = nn.Linear(128 * 18 * 18, 512)<br/>        self.dropout = nn.Dropout(0.5)<br/>        self.fc2 = nn.Linear(512, 3)  #Three Classes<br/>        <br/>    def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = self.pool(F.relu(self.conv3(x)))<br/>        x = x.view(-1, 128 * 18 * 18)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.dropout(x)<br/>        x = self.fc2(x)  <br/>        return x<br/>    <br/>    <br/>#********************************CNN TRAINING**********************************************<br/><br/># Model-loss function-optimizer<br/>model = CNNModel()<br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.001)<br/><br/>#*********************************Training*************************************************<br/>num_epochs = 10<br/>train_losses, val_losses = [], []<br/><br/>for epoch in range(num_epochs):<br/>    model.train()<br/>    running_loss = 0.0<br/>    for inputs, labels in train_loader:<br/>        optimizer.zero_grad()<br/>        outputs = model(inputs)<br/>        labels = labels.type(torch.LongTensor)  <br/>        loss = criterion(outputs, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        running_loss += loss.item()<br/>    <br/>    train_losses.append(running_loss / len(train_loader))<br/>    <br/>    model.eval()<br/>    val_loss = 0.0<br/>    all_labels = []<br/>    all_preds = []<br/>    with torch.no_grad():<br/>        for inputs, labels in val_loader:<br/>            outputs = model(inputs)<br/>            labels = labels.type(torch.LongTensor)  <br/>            loss = criterion(outputs, labels)<br/>            val_loss += loss.item()<br/>            _, preds = torch.max(outputs, 1)  <br/>            all_labels.extend(labels.tolist())<br/>            all_preds.extend(preds.tolist())<br/>            <br/>#********************************METRICS &amp; PERFORMANCE************************************<br/>    <br/>    val_losses.append(val_loss / len(val_loader))<br/>    val_accuracy = accuracy_score(all_labels, all_preds)<br/>    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    <br/>    print(f"Epoch [{epoch + 1}/{num_epochs}], "<br/>          f"Loss: {train_losses[-1]:.4f}, "<br/>          f"Val Loss: {val_losses[-1]:.4f}, "<br/>          f"Val Acc: {val_accuracy:.2%}, "<br/>          f"Val Precision: {val_precision:.4f}, "<br/>          f"Val Recall: {val_recall:.4f}, "<br/>          f"Val F1 Score: {val_f1:.4f}")<br/>    <br/>#*******************************SHOW METRICS &amp; PERFORMANCE**********************************<br/>plt.plot(train_losses, label='Train Loss')<br/>plt.plot(val_losses, label='Validation Loss')<br/>plt.legend()<br/>plt.show()<br/><br/># SAVE THE MODEL FOR THE GH_CV_track_teams CODE<br/>torch.save(model.state_dict(), 'D:/PYTHON/hockey_team_classifier.pth')</span></pre><p id="148d" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can see, it is a three-layer (conv1, conv2, conv3) neural network structure using ReLU as the activation function, trained with a dataset of approximately 90 images classified into three classes: Referee, Team_Away (White jersey players), and Team_Home (Yellow jersey players), over a full cycle of 10 epochs.</p><p id="49af" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Considering this initial approach, a request to remove images from the training process would involve erasing the images from both the training and validation datasets and retraining the model. While this might be easy with a small dataset like ours, for larger datasets, such as those used in current large language models (LLMs), this would represent a significant use of resources. Additionally, performing this process repeatedly could also be a limitation.</p><p id="a163" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let‚Äôs imagine that while building the model, we are aware of users‚Äô rights to erasure or rectification and consider applying the SISA technique. This approach would prepare the model for any future scenarios where images might need to be permanently removed from the training dataset, as well as any features that the CNN may have captured during its learning process. The first step would be adapting the initial model presented above to include the four steps of the SISA technique: Sharding, Isolating, Slicing, and Aggregation.</p><p id="bc33" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Step 01: Shards and Slices</strong></p><p id="add7" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">After the transformation step specified at the beginning of the previous code, we‚Äôll begin applying SISA by dividing the dataset into shards. In the code, you will see that the shards are diverse and then split into equal-sized parts to ensure that each shard contains a representative number of samples and is balanced across the different classes we want to predict <em class="oi">(in our case, we are predicting three classes)</em>.</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="a83c" class="rp oq gl rm b bg rq rr l rs rt"><br/>#******************************Sharding the dataset**************************<br/><br/>def shard_dataset(dataset, num_shards):<br/>    indices = list(range(len(dataset)))<br/>    np.random.shuffle(indices)<br/>    shards = []<br/>    shard_size = len(dataset) // num_shards<br/>    for i in range(num_shards):<br/>        shard_indices = indices[i * shard_size : (i + 1) * shard_size]<br/>        shards.append(Subset(dataset, shard_indices))<br/>    return shards<br/><br/>#******************************Overlapping Slices***************************<br/>def create_overlapping_slices(shard, slice_size, overlap):<br/>    indices = list(shard.indices)<br/>    slices = []<br/>    step = slice_size - overlap<br/>    for start in range(0, len(indices) - slice_size + 1, step):<br/>        slice_indices = indices[start:start + slice_size]<br/>        slices.append(Subset(shard.dataset, slice_indices))<br/>    return slices</span></pre><p id="dc2a" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You‚Äôll notice that for the slicing process, I didn‚Äôt assign exclusive slices per shard as the SISA technique suggests. Instead, we are using overlapping slices. This means that each slice is not exclusively composed of data points from just one shard; some data points from one slice will also appear in the next slice.</p><p id="9c0a" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm"><em class="oi">So why did I overlap the slices? </em></strong>As you might have guessed already, our dataset is small <em class="oi">(approximately 90 images)</em>, so working with exclusive slices per shard would not guarantee that each slice has a sufficiently balanced dataset to maintain the predictive capability of the model. <strong class="nf gm">Overlapping slices</strong> allow the model to make better use of the available data and improve generalization. For larger datasets, non-overlapping slices might be more efficient, as they require fewer computational resources. <strong class="nf gm">In the end, creating shards and slices involves considering the size of your dataset, your compute resources, and the need to maintain the predictive capabilities of your model.</strong></p><p id="a078" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, after the functions are defined, we proceed to set the hyperparameters for the sharding and slicing process:</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="60e6" class="rp oq gl rm b bg rq rr l rs rt"><br/>#**************************Applying Sharding and Slicing*******************<br/><br/>num_shards = 4  <br/>slice_size = len(full_train_dataset) // num_shards // 2<br/>overlap = slice_size // 2<br/>shards = shard_dataset(full_train_dataset, num_shards)<br/><br/>#************************Overlapping slices for each shard*****************<br/>all_slices = []<br/>for shard in shards:<br/>    slices = create_overlapping_slices(shard, slice_size, overlap)<br/>    all_slices.extend(slices)</span></pre><p id="6fe2" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset is split into 4 shards, but I should mention that initially, I used 10 shards. This resulted in each shard containing only a few sample images, which didn‚Äôt represent corectly the full dataset‚Äôs class distribution, leading to a significant drop in the model‚Äôs performance metrics (accuracy, precision, and F1 score). Since we are dealing with a small dataset, reducing the number of shards to four was a wise decision. Finally, the slicing process divides each shard into two slices with a 50% overlap, meaning that half of the images in each slice overlap with the next slice.</p><p id="2337" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Step 02: Isolating specific data points</strong></p><p id="24a5" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this step, we proceed to isolate the specific data points that end users may want to rectify or remove from the model‚Äôs learning process. First, we define a function that removes the specified data points from each slice. Next, we identify the indices of the images based on their filenames. These indices are then used to update each slice by removing the data points where they are present.</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="c051" class="rp oq gl rm b bg rq rr l rs rt"><br/>#**************************+*Isolate datapoints******************************<br/>def isolate_data_for_unlearning(slice, data_points_to_remove):<br/>    new_indices = [i for i in slice.indices if i not in data_points_to_remove]<br/>    return Subset(slice.dataset, new_indices)<br/><br/>#*****Identify the indices of the images we want to rectify/erasure**********<br/>def get_indices_to_remove(dataset, image_names_to_remove):<br/>    indices_to_remove = [] #list is empty<br/>    image_to_index = {img_path: idx for idx, (img_path, _) in enumerate(dataset.imgs)}<br/>    for image_name in image_names_to_remove:<br/>        if image_name in image_to_index:<br/>            indices_to_remove.append(image_to_index[image_name])<br/>    return indices_to_remove<br/><br/>#*************************Specify and remove images***************************<br/>images_to_remove = []<br/>indices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)<br/>updated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]</span></pre><p id="0bc1" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf gm">Currently, the list is empty (images_to_remove = [] ),</strong> so no images are removed at this stage, but the setup is ready for use when a request arrives <em class="oi">(we‚Äôll see an example later in this article)</em>.</p><p id="ca33" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The complete version of the model implementing the SISA technique should look something like this:</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="bf46" class="rp oq gl rm b bg rq rr l rs rt"><br/>import os<br/>import numpy as np<br/>import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/>import torch.nn.functional as F<br/>import torchvision.transforms as transforms<br/>import torchvision.datasets as datasets<br/>from torch.utils.data import DataLoader, Subset<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score<br/>import matplotlib.pyplot as plt<br/><br/><br/>#******************************Data transformation********************************************<br/># Training and Validation Datasets<br/>data_dir = 'D:/PYTHON/teams_sample_dataset'<br/><br/>transform = transforms.Compose([<br/>    transforms.Resize((150, 150)),<br/>    transforms.ToTensor(),<br/>    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])<br/>])<br/><br/># Load data<br/>full_train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)<br/>val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)<br/><br/>#******************************Sharding the dataset**************************<br/><br/>def shard_dataset(dataset, num_shards):<br/>    indices = list(range(len(dataset)))<br/>    np.random.shuffle(indices)<br/>    shards = []<br/>    shard_size = len(dataset) // num_shards<br/>    for i in range(num_shards):<br/>        shard_indices = indices[i * shard_size : (i + 1) * shard_size]<br/>        shards.append(Subset(dataset, shard_indices))<br/>    return shards<br/><br/>#******************************Overlapping Slices***************************<br/>def create_overlapping_slices(shard, slice_size, overlap):<br/>    indices = list(shard.indices)<br/>    slices = []<br/>    step = slice_size - overlap<br/>    for start in range(0, len(indices) - slice_size + 1, step):<br/>        slice_indices = indices[start:start + slice_size]<br/>        slices.append(Subset(shard.dataset, slice_indices))<br/>    return slices<br/><br/>#**************************Applying Sharding and Slicing*******************<br/><br/>num_shards = 4  <br/>slice_size = len(full_train_dataset) // num_shards // 2<br/>overlap = slice_size // 2<br/>shards = shard_dataset(full_train_dataset, num_shards)<br/><br/>#************************Overlapping slices for each shard*****************<br/>all_slices = []<br/>for shard in shards:<br/>    slices = create_overlapping_slices(shard, slice_size, overlap)<br/>    all_slices.extend(slices)<br/><br/>#**************************+*Isolate datapoints******************************<br/>def isolate_data_for_unlearning(slice, data_points_to_remove):<br/>    new_indices = [i for i in slice.indices if i not in data_points_to_remove]<br/>    return Subset(slice.dataset, new_indices)<br/><br/>#*****Identify the indices of the images we want to rectify/erasure**********<br/>def get_indices_to_remove(dataset, image_names_to_remove):<br/>    indices_to_remove = []<br/>    image_to_index = {img_path: idx for idx, (img_path, _) in enumerate(dataset.imgs)}<br/>    for image_name in image_names_to_remove:<br/>        if image_name in image_to_index:<br/>            indices_to_remove.append(image_to_index[image_name])<br/>    return indices_to_remove<br/><br/>#*************************Specify and remove images***************************<br/>images_to_remove = []<br/>indices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)<br/>updated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]<br/><br/><br/>#********************************CNN Model Architecture**************************************<br/><br/>class CNNModel(nn.Module):<br/>    def __init__(self):<br/>        super(CNNModel, self).__init__()<br/>        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)<br/>        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)<br/>        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)<br/>        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)<br/>        self.fc1 = nn.Linear(128 * 18 * 18, 512)<br/>        self.dropout = nn.Dropout(0.5)<br/>        self.fc2 = nn.Linear(512, 3)  # Output three classes<br/>        <br/>    def forward(self, x):<br/>        x = self.pool(F.relu(self.conv1(x)))<br/>        x = self.pool(F.relu(self.conv2(x)))<br/>        x = self.pool(F.relu(self.conv3(x)))<br/>        x = x.view(-1, 128 * 18 * 18)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.dropout(x)<br/>        x = self.fc2(x)<br/>        return x<br/><br/>#********************************CNN TRAINING**********************************************<br/><br/># Model-loss function-optimizer<br/>model = CNNModel()<br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(model.parameters(), lr=0.001)<br/><br/>#*********************************Training*************************************************<br/>num_epochs = 10<br/>train_losses, val_losses = [], []<br/><br/>for epoch in range(num_epochs):<br/>    model.train()<br/>    running_loss = 0.0<br/>    for slice in updated_slices:<br/>        train_loader = DataLoader(slice, batch_size=32, shuffle=True)<br/>        for inputs, labels in train_loader:<br/>            optimizer.zero_grad()<br/>            outputs = model(inputs)<br/>            labels = labels.type(torch.LongTensor)<br/>            loss = criterion(outputs, labels)<br/>            loss.backward()<br/>            optimizer.step()<br/>            running_loss += loss.item()<br/><br/>    train_losses.append(running_loss / (len(updated_slices)))<br/><br/>    model.eval()<br/>    val_loss = 0.0<br/>    all_labels = []<br/>    all_preds = []<br/>    with torch.no_grad():<br/>        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)<br/>        for inputs, labels in val_loader:<br/>            outputs = model(inputs)<br/>            labels = labels.type(torch.LongTensor)<br/>            loss = criterion(outputs, labels)<br/>            val_loss += loss.item()<br/>            _, preds = torch.max(outputs, 1)<br/>            all_labels.extend(labels.tolist())<br/>            all_preds.extend(preds.tolist())<br/><br/>#********************************METRICS &amp; PERFORMANCE************************************<br/>    <br/>    val_losses.append(val_loss / len(val_loader))<br/>    val_accuracy = accuracy_score(all_labels, all_preds)<br/>    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)<br/>    <br/>    print(f"Epoch [{epoch + 1}/{num_epochs}], "<br/>          f"Loss: {train_losses[-1]:.4f}, "<br/>          f"Val Loss: {val_losses[-1]:.4f}, "<br/>          f"Val Acc: {val_accuracy:.2%}, "<br/>          f"Val Precision: {val_precision:.4f}, "<br/>          f"Val Recall: {val_recall:.4f}, "<br/>          f"Val F1 Score: {val_f1:.4f}")<br/><br/>#*******************************SHOW METRICS &amp; PERFORMANCE**********************************<br/>plt.plot(train_losses, label='Train Loss')<br/>plt.plot(val_losses, label='Validation Loss')<br/>plt.legend()<br/>plt.show()<br/><br/># SAVE THE MODEL<br/>torch.save(model.state_dict(), 'hockey_team_classifier_SISA.pth')<br/><br/></span></pre><p id="c8b6" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let‚Äôs go to our erasure scenario. Imagine that months have passed since the model was deployed, and a hockey player requests the removal of their images from the CNN model‚Äôs training data. For this example, let‚Äôs assume the player is represented in three images from the training and validation dataset: A<strong class="nf gm">way_image03.JPG, Away_image04.JPG, and Away_image05.JPG</strong>. To remove these images from the training process, simply specify them in the <strong class="nf gm">‚ÄúSpecify and Remove Images‚Äù</strong> section of the code (as shown above). Only the slices containing these images would need to be retrained.</p><pre class="ok ol om on oo rl rm rn bp ro bb bk"><span id="7303" class="rp oq gl rm b bg rq rr l rs rt">#*************************Specify and remove images***************************<br/>images_to_remove = ["Away_image03.JPG", "Away_image04.JPG", "Away_image05.JPG"]<br/>indices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)<br/>updated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]</span></pre><p id="8eb4" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, I would like to share some key takeaways from adapting the SISA framework to my model:</p><ul class=""><li id="3267" class="nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ru ql qm bk"><strong class="nf gm">Weak learners and performance trade-offs:</strong> Since each constituent model is trained on small subsets <em class="oi">(shards and slices)</em>, one might assume that their accuracy would be lower than that of a single model trained on the entire dataset and degrading the model‚Äôs generalization. Surprisingly, in our case, the model‚Äôs performance improved significantly, which could be due to working with a small, overlapping dataset, leading to some degree of overfitting. In use cases involving large datasets, <strong class="nf gm">it‚Äôs important to consider the potential performance trade-offs</strong>.</li><li id="457d" class="nd ne gl nf b hj qn nh ni hm qo nk nl nm qp no np nq qq ns nt nu qr nw nx ny ru ql qm bk"><strong class="nf gm">Proper sharding:</strong> My initial attempts with a high number of shards resulted in shards with very few samples, leading to a negative impact on the model‚Äôs performance. <strong class="nf gm">Don‚Äôt underestimate the importance of the sharding and slicing process</strong>. Proper sharding helps the model avoid overfitting and generalize better on the validation set.</li></ul><p id="f865" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I hope you found this project applying the SISA technique for machine unlearning interesting. You can access the complete code in this <a class="af gi" href="https://github.com/rvizcarra15/MachineUnlearning_SISA_framework" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>.</p></div></div></div><div class="ab cb rv rw rx ry" role="separator"><span class="rz by bm sa sb sc"/><span class="rz by bm sa sb sc"/><span class="rz by bm sa sb"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f054" class="op oq gl bf or os sd hl ou ov se ho ox oy sf pa pb pc sg pe pf pg sh pi pj pk bk"><strong class="al">Final Thoughts</strong></h1><p id="6825" class="pw-post-body-paragraph nd ne gl nf b hj pl nh ni hm pm nk nl nm pn no np nq po ns nt nu pp nw nx ny fj bk">My older sister and I have this routine where we exchange images of what social media platform‚Äôs daily remind us of what we posted five, ten, or fifteen years ago. We often laugh about the things we shared or the comments we made at that time <em class="oi">(clearly, as most of us didn‚Äôt fully understand social media when it first appeared)</em>. Over time, I have learned to use my social media presence more wisely, appreciating my surroundings outside the social media ecosystem and the privacy that some aspects of our lives deserve. But the truth is that neither my sister nor I are the same people we were ten or fifteen years ago, and although the past is an important part of who we are now, it doesn‚Äôt define us<em class="oi"> (not everything has to be ‚Äúwritten in stone‚Äù in the digital world)</em>. We all have the right to choose whether that data may or may not stay in the digital world and be used or not to define our choices/preferences or the ones from others.</p><p id="fb55" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It‚Äôs true that AI performs better when trained with data from users similar to those who will use it <em class="oi">(The Ethics of Advanced AI Assistants, Google DeepMind 2024)</em>. However, <a class="af gi" href="https://s899a9742c3d83292.jimcontent.com/download/version/1648135848/module/8350351463/name/Rotenberg-GPA2021.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="nf gm"><em class="oi">‚ÄúPrivacy requires Transparency‚Äù</em></strong></a><strong class="nf gm"><em class="oi"> </em></strong>. Therefore, how and when companies using machine learning with pre-trained sensitive data implement the ‚ÄúRight to be Forgotten‚Äù is crucial for moving toward the trustworthy AI we all want.</p><p id="15b1" class="pw-post-body-paragraph nd ne gl nf b hj ng nh ni hm nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="oi">Thank you for reading!</em> As always, your suggestions are welcome and keep the conversation going.</p></div></div></div></div>    
</body>
</html>