- en: How to Evaluate Your Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-your-predictions-cef80d8f6a69?source=collection_archive---------5-----------------------#2024-05-17](https://towardsdatascience.com/how-to-evaluate-your-predictions-cef80d8f6a69?source=collection_archive---------5-----------------------#2024-05-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Be mindful of the measure you choose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jeffrey_85949?source=post_page---byline--cef80d8f6a69--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page---byline--cef80d8f6a69--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cef80d8f6a69--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cef80d8f6a69--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page---byline--cef80d8f6a69--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cef80d8f6a69--------------------------------)
    ·15 min read·May 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3547acaf064f99ff34fb1719c0dfa9e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Isaac Smith](https://unsplash.com/@isaacmsmith?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Testing and benchmarking machine learning models by comparing their predictions
    on a test set, even after deployment, is of fundamental importance. To do this,
    one needs to think of a measure or *score* that takes a prediction and a test
    point and assigns a value measuring how successful the prediction is with respect
    to the test point. However, one should think carefully about which scoring measure
    is appropriate. In particular, when choosing a method to evaluate a prediction
    we should adhere to the idea of *proper scoring rules*. I only give a loose definition
    of this idea here, but basically, we want a score that is minimized at the thing
    we want to measure!
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule: One can use MSE to evaluate mean predictions, MAE to evaluate
    median predictions, the quantile score to evaluate more general quantile predictions
    and the energy or MMD score to evaluate distributional predictions.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Consider a variable you want to predict, say a random variable *Y*, from a vector
    of covariates ***X***. In the example below, *Y* will be income and ***X*** will
    be certain characteristics, such as *age* and *education*. We learned a predictor
    *f* on some training data and now we predict *Y* as *f(****x****)*. Usually, when
    we want to predict a variable *Y* as well as possible we predict the expectation
    of *y* given **x**, i.e. *f(****x****)* should approximate *E[Y |* ***X****=****x****]*.
    But more generally, *f(****x****)* could be an estimator of the median, other
    quantiles, or even the full conditional distribution *P(Y |* ***X****=****x****)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now for a new test point *y*, we want to score your prediction, that is you
    want a function *S(y,f(****x****))*, that is *minimized* (in expectation)when
    *f(****x****)* is the best thing you can do. For instance, if we want to predict
    *E[Y |* ***X****=****x****]*, this score is given as the MSE: *S(y, f(****x****))=
    (y-f(****x****))²*.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we study the principle of scoring the predictor *f* over at test set of
    *(y_i,****x****_i), i=1,…,ntest* in more detail. In all examples we will compare
    the ideal estimation method to an other that is clearly wrong, or naive, and show
    that our scores do what they are supposed to. The full code used here can also
    be found on [Github](https://github.com/JeffNaef/Medium-Articles/blob/main/HowtoScoreprediction.R).
  prefs: []
  type: TYPE_NORMAL
- en: The Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate things, I will simulate a simple dataset that should mimic income
    data. We will use this simple example throughout this article to illustrate the
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Although this simulation may be oversimplified, it reflects certain well-known
    characteristics of such data: older age, advanced education, and greater experience
    are all linked to higher wages. The use of the “exp” operator results in a highly
    skewed wage distribution, which is a consistent observation in such datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e317a98be03239d90b15e4e4d4fb568b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wage distribution over the whole simulated population. Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Crucially, this skewness is also present when we fix age, education and experience
    to certain values. Let’s imagine we look at a specific person, Dave, who is 30
    years old, has a Bachelor’s in Economics and 10 years of experience and let’s
    look at his actual income distribution according to our data generating process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ae06889fd2bf3960bec23c2fa34b0493.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Wage distrbution for Dave. Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Thus the distribution of possible wages of Dave, given the information we have
    about him, is still highly skewed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also generate a test set of several people:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We now start simple and first look at the scores for mean and median prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The scores for mean and median prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In data science and machine learning, interest often centers on a single number
    that signifies the “center” or “middle” of the distribution we aim to predict,
    namely the (conditional) mean or median. To do this we have the mean squared error
    (MSE):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1184bca3cc4aedf8a86d82f97bbddf09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and the mean absolute error (MAE):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab31df4aa0a26921ad4c084fc1ceb28d.png)'
  prefs: []
  type: TYPE_IMG
- en: An important takeaway is that the MSE is the appropriate metric for predicting
    the conditional mean, while the MAE is the measure to use for the conditional
    median. Mean and median are not the same thing for skewed distributions like the
    one we study here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us illustrate this for the above example with very simple estimators (that
    we would not have access to in real life), just for illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'That is we estimate mean and median, by simply simulating from the model for
    fixed values of age, education, and experience (this would be a simulation from
    the correct conditional distribution) and then we simply take the mean/median
    of that. Let’s test this on Dave:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bb2c4d9b49be9138d294dbce07715975.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Blue: estimated conditional median of Dave, Red: estimated conditional mean
    of Dave. Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: Clearly the mean and median are different, as one would expect from such a distribution.
    In fact, as is typical for income distributions, the mean is higher (more influenced
    by high values) than the median.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s use these estimators on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives a diverse range of conditional mean/median values. Now we calculate
    MSE and MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows what is known theoretically: MSE is minimized for the (conditional)
    expectation *E[Y |* ***X****=****x****]*, while MAE is minimized at the conditional
    median. *In general, it does not make sense to use the MAE when you try to evaluate
    your mean prediction.* In a lot of applied research and data science, people use
    the MAE or both to evaluate mean predictions (I know because I did it myself).
    While this may be warranted in certain applications, this can have serious consequences
    for distributions that are not symmetric, as we saw in this example: When looking
    at the MAE, method 1 looks worse than method 2, even though the former estimates
    the mean correctly. In fact, in this highly skewed example, *method 1 should have
    a lower MAE than method 2*.'
  prefs: []
  type: TYPE_NORMAL
- en: To score conditional mean prediction use the mean squared error (MSE) and not
    the mean absolute error (MAE). The MAE is minimized for the conditional median.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Scores for quantile and interval prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assume we want to score an estimate *f(****x****)* of the quantile *q_****x***
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbff4304e2b9dd47bb1b99433db35f08.png)![](../Images/ddba86bb1cf3ae1b09ce94317ba9c536.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Simple quantile illustration. Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we can consider the quantile score:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/564454526b6c64549f25b8e5243cffb2.png)'
  prefs: []
  type: TYPE_IMG
- en: whereby
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/626dea41b7d98c0d9993f2830876d672.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To unpack this formula, we can consider two cases:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) *y* is smaller than *f(****x):***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f40be68d77162ec15f438f90e027de81.png)'
  prefs: []
  type: TYPE_IMG
- en: i.e. we incur a penalty which gets bigger the further away *y* is from *f(****x).***
  prefs: []
  type: TYPE_NORMAL
- en: (2) *y* is larger than *f(****x):***
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e013cda3f26a06ee00f523618c56dc23.png)'
  prefs: []
  type: TYPE_IMG
- en: i.e. a penalty which gets bigger the further away *y* is from *f(****x).***
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the weight is such that for a high *alpha*, having the estimated
    quantile *f(****x****)* smaller than *y* gets penalized more. This is by design
    and ensures that the right quantile is indeed the minimizer of the expected value
    of *S(y,f(****x****))* over y. This score is in fact the *quantile loss* (up to
    a factor 2), see e.g. this [nice article](/quantile-loss-and-quantile-regression-b0689c13f54d).
    It is implemented in the *quantile_score* function of the package *scoringutils*
    in R. Finally, note that for *alpha=0.5:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b834d543f3b92647fdf006820954b91.png)'
  prefs: []
  type: TYPE_IMG
- en: simply the MAE! This makes sense, as the 0.5 quantile is the median.
  prefs: []
  type: TYPE_NORMAL
- en: With the power to predict quantiles, we can also build prediction intervals.
    Consider (*l_****x****, u_****x)***, where *l_****x*** ≤ *u_****x*** are quantiles
    such that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b1c197758bfd89e29c80695d88eae30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In fact, this is met if *l_****x***isthe *alpha/2* quantile, and *u_****x***
    is the *1-alpha/2* quantile. Thus we now estimate and score these two quantiles.
    Consider *f(****x****)=(f_1(****x****), f_2(****x****))*, whereby *f_1(****x****)*
    to be an estimate of *l_****x***and *f_2(****x****)* an estimate of *u_****x.***
    We provide two estimators, the “ideal” one that simulates again from the true
    process to then estimate the required quantiles and a “naive” one, which has the
    right coverage but is too big:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Again we can clearly see that, on average, the correct estimator has a much
    lower score than the naive one!
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus with the quantile score, we have a reliable way of scoring individual
    quantile predictions. However, the way of averaging the score of the upper and
    lower quantiles for the prediction interval might seem ad hoc. Luckily it turns
    out that this leads to the so-called *interval score*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0514ecf84ab0f43820fec2ac48b0819.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus through some algebraic magic, we can score a prediction interval by averaging
    the scores for the *alpha/2* and the *1-alpha/2* quantiles as we did. Interestingly,
    the resulting interval score rewards narrow prediction intervals, and induces
    a penalty, the size of which depends on *alpha*, if the observation misses the
    interval. Instead of using the average of quantile scores, we can also directly
    calculate this score with the package *scoringutils.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the exact same number we got above when averaging the scores of the
    two intervals.
  prefs: []
  type: TYPE_NORMAL
- en: The quantile score implemented in R in the package scoringutils can be used
    to score quantile predictions. If one wants to score a prediction interval directly,
    the interval_score function can be used.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Scores for distributional prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More and more fields have to deal with *distributional prediction*. Luckily
    there are even scores for this problem. In particular, here I focus on what is
    called the *energy score:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ffa382ed9a49e05f899c1995262b8fe.png)'
  prefs: []
  type: TYPE_IMG
- en: for *f(****x****)* being an estimate of the distribution *P(Y |* ***X****=****x****).*
    The second term takes the expectation of the Eucledian distance between two independent
    samples from *f(****x****).* This is akin to a normalizing term, establishing
    the value if the same distribution was compared. The first term then compares
    the sample point *y* to a draw *X* from *f(****x****).* In expectation (over *Y*
    drawn from *P(Y |* ***X****=****x****))* this will be minimized if *f(****x****)=P(Y
    |* ***X****=****x****).*
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus instead of just predicting the mean or the quantiles, we now try to predict
    the whole distribution of wage at each test point. Essentially we try to predict
    and evaluate the conditional distribution we plotted for Dave above. This is a
    bit more complicated; how exactly do we represent a learned distribution? In practice
    this is resolved by assuming we can obtain a sample from the predicted distribution.
    Thus we compare a sample of *N*, obtained from the predicted distribution, to
    a single test point. This can be done in R using *es_sample* from the *scoringRules*
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the above code, we again compare the “perfect” estimate (i.e. sampling from
    the true distribution *P(Y |* ***X****=****x****))* to a very naive one, namely
    one that does not consider any information on wage, edicuation or experience.
    Again, the score reliably identifies the better of the two methods.
  prefs: []
  type: TYPE_NORMAL
- en: The energy score, implemented in the R package scoringRules can be used to score
    distributional prediction, if a sample from the predicted distribution is available.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at different ways of scoring predictions. Thinking about the
    right measure to test predictions is important, as the wrong measure might make
    us choose and keep the wrong model for our prediction task.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that especially for distributional prediction this scoring
    is a difficult task and the score might not have much power in practice. That
    is, even a method that leads to a large improvement might only have a slightly
    smaller score. However, this is not a problem per se, as long as the score is
    able to reliably identify the better of the two methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Tilmann Gneiting & Adrian E Raftery (2007) Strictly Proper Scoring Rules,
    Prediction, and Estimation, Journal of the American Statistical Association, 102:477,
    359–378, DOI: [10.1198/016214506000001437](https://doi.org/10.1198/016214506000001437)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: All the code in one place'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This file can also be found on [Github](https://github.com/JeffNaef/Medium-Articles/blob/main/HowtoScoreprediction.R).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
