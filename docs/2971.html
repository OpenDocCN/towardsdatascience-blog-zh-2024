<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Predicted Probability, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Predicted Probability, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10">https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f3ea" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">MODEL EVALUATION &amp; OPTIMIZATION</h2><div/><div><h2 id="4699" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">7 basic classifiers reveal their prediction confidence math</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Dec 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">5</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="896a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Classification models don‚Äôt just tell you what they think the answer is ‚Äî they also tell you <strong class="mq ga">how sure</strong> they are about that answer. This certainty is shown as a probability score. A high score means the model is very confident, while a low score means it‚Äôs uncertain about its prediction.</p><p id="6671" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Every classification model calculates these probability scores differently. Simple models and complex ones each have their own specific methods to determine the likelihood of each possible outcome.</p><p id="9963" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We‚Äôre going to explore seven basic classification models and visually break down how each one figures out its probability scores. No need for a crystal ball ‚Äî we‚Äôll make these probability calculations crystal clear!</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/b265adbcd86fb1261e938e663049e715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IouZGTjmruanqsZ2o_--JQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="477c" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Definition</h1><p id="de35" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Predicted probability (or ‚Äúclass probability‚Äù) is a number from 0 to 1 (or 0% to 100%) that shows how confident a model is about its answer. If the number is 1, the model is completely sure about its answer. If it‚Äôs 0.5, the model is basically guessing ‚Äî it‚Äôs like flipping a coin.</p><h2 id="144b" class="pe oe fq bf of pf pg ph oi pi pj pk ol mx pl pm pn nb po pp pq nf pr ps pt fw bk">Components of a Probability Score</h2><p id="dda5" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">When a model has to choose between two classes (called binary classification), three main rules apply:</p><ol class=""><li id="87d4" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pu pv pw bk">The predicted probability must be between 0 and 1</li><li id="d249" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk">The chances of both options happening must add up to 1</li><li id="64f4" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk">A higher probability means the model is more sure about its choice</li></ol><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/36a6cda6b007e30dbfb6888351b20b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EkdFw4zonN4aFM5KvSuY8w.png"/></div></div></figure><p id="f740" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For binary classification, when we talk about predicted probability, we usually mean the probability of the positive class. A higher probability means the model thinks the positive class is more likely, while a lower probability means it thinks the negative class is more likely.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/dac411b12238417f189b5c813e5b3b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcNDKxhUPY0QKDG4gJIJnw.png"/></div></div></figure><p id="50bf" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To make sure these rules are followed, models use mathematical functions to convert their calculations into proper probabilities. Each type of model might use different functions, which affects how they express their confidence levels.</p><h1 id="56d3" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Prediction vs. Probability</h1><p id="6975" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">In classification, a model picks the class it thinks will most likely happen ‚Äî the one with the highest probability score. But two different models might pick the same class while being more or less confident about it. Their predicted probability scores tell us how sure each model is, even when they make the same choice.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/003cdef1e79d01f669621bce97b57d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cx2HES3giaVHv8kOxR3WXA.png"/></div></div></figure><p id="ba94" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">These different probability scores tell us something important: even when models pick the same class, they might understand the data differently.</p><blockquote class="qd"><p id="c9bf" class="qe qf fq bf qg qh qi qj qk ql qm nj dx">One model might be very sure about its choice, while another might be less confident ‚Äî even though they made the same prediction.</p></blockquote><h1 id="75ec" class="od oe fq bf of og oh gv oi oj ok gy ol om qn oo op oq qo os ot ou qp ow ox oy bk">üìä Dataset Used</h1><p id="95e1" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">To understand how predicted probability is calculated, we‚Äôll continue with <a class="af qq" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">the same dataset used in my previous articles on Classification Algorithms</a>. Our goal remains: predicting if someone will play golf based on the weather.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/1560cdcd385ca4877365575c6c84f8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXUwrUYsapSvEzltRP8bWQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Columns: ‚ÄòOvercast (one-hot-encoded into 3 columns)‚Äô, ‚ÄôTemperature‚Äô (in Fahrenheit), ‚ÄòHumidity‚Äô (in %), ‚ÄòWindy‚Äô (Yes/No) and ‚ÄòPlay‚Äô (Yes/No, target feature)</figcaption></figure><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="14f4" class="qv oe fq qs b bg qw qx l qy qz">import pandas as pd<br/>import numpy as np<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/># Create and prepare dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/><br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)</span></pre><p id="0da8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">As some algorithms might need standardized values, we will also do <a class="af qq" href="https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb" rel="noopener">standard scaling</a> to the numerical features and <a class="af qq" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">one-hot encoding</a> to the categorical features, including the target feature:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/0b25c11e730a5be1d37aee6342ef4b31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NLH3DklErF5nCGngs91MRg.png"/></div></div></figure><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="9611" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.preprocessing import StandardScaler<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Prepare features and target<br/>X,y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])<br/>X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])</span></pre><p id="0ca8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, let‚Äôs see how each of the following 7 classification algorithms calculates these probabilities:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/85322ccb3ef67016db657d6a2a3c02a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zFf0cGg4OMPzWz1WrgmtA.png"/></div></div></figure><h1 id="7578" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Dummy Classifier Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Dummy Classifier Explained: A Visual Guide with Code Examples for Beginners</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">Setting the Bar in Machine Learning with Simple Baseline Models</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="ro l rp rq rr rn rs lw rf"/></div></div></a></div><p id="aa0c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">A Dummy Classifier is a prediction model that doesn‚Äôt learn patterns from data. Instead, it follows basic rules like: picking the most common outcome, making random predictions based on how often each outcome appeared in training, always picking one answer, or randomly choosing between options with equal chance. The Dummy Classifier ignores all input features and just follows these rules.</p><p id="474e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, all it remembers is a few numbers showing either how often each outcome happened or the constant values it was told to use. It doesn‚Äôt learn anything about how features relate to outcomes.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/6dd5f768865682ac6099b8f627cd9342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycLhP_vwskS6JIN0r_JGfg.png"/></div></div></figure><p id="7024" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability in binary classification, the Dummy Classifier uses the most basic approach possible. Since it only remembered how often each outcome appeared in the training data, it uses these same numbers as probability scores for every prediction ‚Äî either 0 or 1.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/7b157b5391af9509372206c4069d9ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZbHwzUzRukQBtXNaKdLLRA.png"/></div></div></figure><p id="15d2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">These probability scores stay exactly the same for all new data, because the model doesn‚Äôt look at or react to any features of the new data it‚Äôs trying to predict.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="de9b" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.dummy import DummyClassifier<br/>import pandas as pd<br/>import numpy as np<br/><br/># Train the model<br/>dummy_clf = DummyClassifier(strategy='stratified', random_state=42)<br/>dummy_clf.fit(X_train, y_train)<br/><br/># Print the "model" - which is just the class probabilities<br/>print("THE MODEL:")<br/>print(f"Probability of not playing (class 0): {dummy_clf.class_prior_[0]:.3f}")<br/>print(f"Probability of playing (class 1): {dummy_clf.class_prior_[1]:.3f}")<br/>print("\nNOTE: These probabilities are used for ALL predictions, regardless of input features!")<br/><br/># Make predictions and get probabilities<br/>y_pred = dummy_clf.predict(X_test)<br/>y_prob = dummy_clf.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>    'True Label': y_test,<br/>    'Prediction': y_pred,<br/>    'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="13d1" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">k-Nearest Neighbors (KNN) Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">The friendly neighbor approach to machine learning</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="rt l rp rq rr rn rs lw rf"/></div></div></a></div><p id="0874" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">K-Nearest Neighbors (kNN) is a prediction model that takes a different approach ‚Äî instead of learning rules, it keeps all training examples in memory. When it needs to make a prediction about new data, it measures how similar this data is to every stored example, finds the k most similar ones (where k is a number we choose), and makes its decision based on those neighbors.</p><p id="ffe9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, all it has stored is the complete training dataset, the value of k we chose, and a method for measuring how similar two data points are (by default using Euclidean distance).</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/226337b949ff12aa339c2625a91f47df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLDBp4c6rGav-qs7nAOqSQ.png"/></div></div></figure><p id="797b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability, kNN looks at those k most similar examples and counts how many belong to each class. The probability score is simply the number of neighbors belonging to a class divided by k.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/d6411613689379bcd1ad1878bbb7005b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DqpfxnjkZ9PfftF6cim4ww.png"/></div></div></figure><p id="cb21" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since kNN calculates probability scores by division, it can only give certain specific values based on k (say, for k=5, the only possible probability scores are 0/5 (0%), 1/5 (20%), 2/5 (40%), 3/5 (60%), 4/5 (80%), and 5/5 (100%)). This means kNN can‚Äôt give as many different confidence levels as other models.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="bc20" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.neighbors import KNeighborsClassifier<br/>import pandas as pd<br/>import numpy as np<br/><br/># Train the model<br/>k = 3  # number of neighbors<br/>knn = KNeighborsClassifier(n_neighbors=k)<br/>knn.fit(X_train, y_train)<br/><br/># Print the "model"<br/>print("THE MODEL:")<br/>print(f"Number of neighbors (k): {k}")<br/>print(f"Training data points stored: {len(X_train)}")<br/><br/># Make predictions and get probabilities<br/>y_pred = knn.predict(X_test)<br/>y_prob = knn.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="5c4f" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Naive Bayes Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">Unlocking predictive power through Yes/No probability</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="ru l rp rq rr rn rs lw rf"/></div></div></a></div><p id="2c2f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Naive Bayes is a prediction model that uses probability math with a ‚Äúnaive‚Äù rule: it assumes each feature affects the outcome independently. There are different types of Naive Bayes: Gaussian Naive Bayes works with continuous values, while Bernoulli Naive Bayes works with binary features. As our dataset has many 0‚Äì1 features, we‚Äôll focus on the Bernoulli one here.</p><p id="612e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, it remembers probability values: one value for how often the positive class occurs, and for each feature, values showing how likely different feature values appear when we have a positive outcome.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/287032349f3374bb29ca0948ea437c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-mNK08UEw91ABlS0eP5wSg.png"/></div></div></figure><p id="9bd4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability, Naive Bayes multiplies several probabilities together: the chance of each class occurring, and the chance of seeing each feature value within that class. These multiplied probabilities are then normalized so they sum to 1, giving us the final probability scores.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/64912066b621b73326a6d1ba4026bd6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9MJUWaXtQvmWxyRMVO7bQ.png"/></div></div></figure><p id="5bb4" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since Naive Bayes uses probability math, its probability scores naturally fall between 0 and 1. However, when certain features strongly point to one class over another, the model can give probability scores very close to 0 or 1, showing it‚Äôs very confident about its prediction.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="0497" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.naive_bayes import BernoulliNB<br/>import pandas as pd<br/><br/># Train the model<br/>nb = BernoulliNB()<br/>nb.fit(X_train, y_train)<br/><br/># Print the "model"<br/>print("THE MODEL:")<br/>df = pd.DataFrame(<br/>   nb.feature_log_prob_.T, <br/>   columns=['Log Prob (No Play)', 'Log Prob (Play)'], <br/>   index=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']<br/>)<br/>df = df.round(3)<br/>print("\nFeature Log-Probabilities:")<br/>print(df)<br/><br/>print("\nClass Priors:")<br/>priors = pd.Series(nb.class_log_prior_, index=['No Play', 'Play']).round(3)<br/>print(priors)<br/><br/># Make predictions and get probabilities<br/>y_pred = nb.predict(X_test)<br/>y_prob = nb.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="6284" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Decision Tree Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Decision Tree Classifier, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">A fresh look on our favorite upside-down tree</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="rv l rp rq rr rn rs lw rf"/></div></div></a></div><p id="fbf3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">A Decision Tree Classifier works by creating a series of yes/no questions about the input data. It builds these questions one at a time, always choosing the most useful question that best separates the data into groups. It keeps asking questions until it reaches a final answer at the end of a branch.</p><p id="d4a7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, it has created a tree where each point represents a question about the data. Each branch shows which way to go based on the answer, and at the end of each branch is information about how often each class appeared in the training data.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/9287974aef17afe32b318ba1e212cf2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tREEKBYpMlXLzU2OEpwvaQ.png"/></div></div></figure><p id="f84f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability, the Decision Tree follows all its questions for new data until it reaches the end of a branch. The probability score is based on how many training examples of each class ended up at that same branch during training.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/e39929dd06dd01db3fe0375771fc6a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cNEQ8T3rzXSJejaUOtp_BA.png"/></div></div></figure><p id="0fab" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since Decision Tree probability scores come from counting training examples at each branch endpoint, they can only be certain values that were seen during training. This means the model can only give probability scores that match the patterns it found while learning, which limits how precise its confidence levels can be.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="66ab" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.tree import DecisionTreeClassifier, plot_tree<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/><br/># Train the model<br/>dt = DecisionTreeClassifier(random_state=42, max_depth=3)  # limiting depth for visibility<br/>dt.fit(X_train, y_train)<br/><br/># Print the "model" - visualize the decision tree<br/>print("THE MODEL (DECISION TREE STRUCTURE):")<br/>plt.figure(figsize=(20,10))<br/>plot_tree(dt, feature_names=['sunny', 'overcast', 'rainy', 'Temperature', <br/>                           'Humidity', 'Wind'], <br/>         class_names=['No Play', 'Play'],<br/>         filled=True, rounded=True, fontsize=10)<br/>plt.show()<br/><br/># Make predictions and get probabilities<br/>y_pred = dt.predict(X_test)<br/>y_prob = dt.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="3479" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Logistic Regression Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">Finding the perfect weights to fit the data in</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="rw l rp rq rr rn rs lw rf"/></div></div></a></div><p id="c2cd" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">A Logistic Regression model, despite its name, predicts between two classes using a mathematical equation. For each feature in the input data, it learns how important that feature is by giving it a number (weight). It also learns one extra number (bias) that helps make better predictions. To turn these numbers into a predicted probability, it uses the sigmoid function that keeps the final answer between 0 and 1.</p><p id="98a8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, all it remembers is these weights ‚Äî one number for each feature, plus the bias number. These numbers are all it needs to make predictions.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/74dcf333c4ede161645adab7e74ed268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eysr_rn_33gNMw-dRskSYA.png"/></div></div></figure><p id="8078" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability in binary classification, Logistic Regression first multiplies each feature value by its weight and adds them all together, plus the bias. This sum could be any number, so the model uses the sigmoid function to convert it into a probability between 0 and 1.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/3d83f16c8d25348a9fea8afc8104ffbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CySidAdhPKsLweoCRLH_XA.png"/></div></div></figure><p id="185e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Unlike other models that can only give certain specific probability scores, Logistic Regression can give any probability between 0 and 1. The further the input data is from the point where the model switches from one class to another (the decision boundary), the closer the probability gets to either 0 or 1. Data points near this switching point get probabilities closer to 0.5, showing the model is less confident about these predictions.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="720a" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.linear_model import LogisticRegression<br/>import pandas as pd<br/><br/># Train the model<br/>lr = LogisticRegression(random_state=42)<br/>lr.fit(X_train, y_train)<br/><br/># Print the "model"<br/>print("THE MODEL:")<br/>model_df = pd.DataFrame({<br/>   'Feature': ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind'],<br/>   'Coefficient': lr.coef_[0]<br/>})<br/>model_df['Coefficient'] = model_df['Coefficient'].round(3)<br/>print("Coefficients (weights):")<br/>print(model_df)<br/><br/>print(f"\nIntercept (bias): {lr.intercept_[0]:.3f}")<br/>print("\nPrediction = sigmoid(intercept + sum(coefficient * feature_value))")<br/><br/># Make predictions and get probabilities<br/>y_pred = lr.predict(X_test)<br/>y_prob = lr.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="b15d" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Support Vector Machine (SVM) Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">Finding the best ‚Äúline‚Äù to separate the classes? Yeah, sure...</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="rx l rp rq rr rn rs lw rf"/></div></div></a></div><p id="a73e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">A Support Vector Machine (SVM) Classifier works by finding the best boundary line (or surface) that separates different classes. It focuses on the points closest to this boundary (called support vectors). While the basic SVM finds straight boundary lines, it can also create curved boundaries using mathematical functions called kernels.</p><p id="8979" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, it remembers three things: the important points near the boundary (support vectors), how much each point matters (weights), and any settings for curved boundaries (kernel parameters). Together, these define where and how the boundary separates the classes.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-knY0hFa949bImG1yTQJQ.png"/></div></div></figure><p id="8670" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability in binary classification, SVM needs an extra step because it wasn‚Äôt designed to give probability scores. It uses a method called Platt Scaling, which adds a Logistic Regression layer to convert distances from the boundary into probabilities. These distances go through the sigmoid function to get final probability scores.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/b5e58a416fce00dca108a284ebcdeb9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFFE22Z14Sw4k-jVlGWn9g.png"/></div></div></figure><p id="67fb" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since SVM calculates probabilities this indirect way, the scores show how far points are from the boundary rather than true confidence levels. Points far from the boundary get probability scores closer to 0 or 1, while points near the boundary get scores closer to 0.5. This means the probability scores are more about location relative to the boundary than the model‚Äôs actual confidence in its predictions.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="0ce0" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.svm import SVC<br/>import pandas as pd<br/>import numpy as np<br/><br/># Train the model<br/>svm = SVC(kernel='rbf', probability=True, random_state=42)<br/>svm.fit(X_train, y_train)<br/><br/># Print the "model"<br/>print("THE MODEL:")<br/>print(f"Kernel: {svm.kernel}")<br/>print(f"Number of support vectors: {svm.n_support_}")<br/>print("\nSupport Vectors (showing first 5 rows):")<br/><br/># Create dataframe of support vectors<br/>sv_df = pd.DataFrame(<br/>   svm.support_vectors_,<br/>   columns=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']<br/>)<br/>print(sv_df.head().round(3))<br/><br/># Show which classes these support vectors belong to<br/>print("\nSupport vector classes:")<br/>for i, count in enumerate(svm.n_support_):<br/>   print(f"Class {i}: {count} support vectors")<br/><br/># Make predictions and get probabilities<br/>y_pred = svm.predict(X_test)<br/>y_prob = svm.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="7da9" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Multilayer Perceptron Probabilities</h1><div class="ra rb rc rd re rf"><a rel="noopener follow" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------"><div class="rg ab il"><div class="rh ab co cb ri rj"><h2 class="bf ga ib z it rk iv iw rl iy ja fz bk">Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset</h2><div class="rm l"><h3 class="bf b ib z it rk iv iw rl iy ja dx">Dissecting the math (with visuals) of a tiny neural network</h3></div><div class="gq l"><p class="bf b dy z it rk iv iw rl iy ja dx">towardsdatascience.com</p></div></div><div class="rn l"><div class="ry l rp rq rr rn rs lw rf"/></div></div></a></div><p id="493e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">A Multi-Layer Perceptron (MLP) Classifier is a type of neural network that processes data through several layers of connected nodes (neurons). Each neuron calculates a weighted total of its inputs, transforms this number using a function (like ReLU), and sends the result to the next layer. For binary classification, the last layer uses the sigmoid function to give an output between 0 and 1.</p><p id="ddd3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When this model finishes training, it remembers two main things: the connection strengths (weights and biases) between neurons in neighboring layers, and how the network is structured (how many layers and neurons are in each layer).</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/ff2e6b7f72a10d9cf94c129377be153e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LTplmOCJafTrTZqXDjtlw.png"/></div></div></figure><p id="8e7a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For calculating predicted probability in binary classification, the MLP moves data through its layers, with each layer creating more complex combinations of information from the previous layer. The final layer produces a number that the sigmoid function converts into a probability between 0 and 1.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qc"><img src="../Images/70865a4ff395a3529685a4435992f71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*956ErlSRFK-WbGpIJwiV2w.png"/></div></div></figure><p id="e6b6" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The MLP can find more complex patterns in data than many other models because it combines features in advanced ways. The final probability score shows how confident the network is ‚Äî scores close to 0 or 1 mean the network is very confident about its prediction, while scores near 0.5 indicate it‚Äôs uncertain.</p><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="b795" class="qv oe fq qs b bg qw qx l qy qz">from sklearn.neural_network import MLPClassifier<br/>import pandas as pd<br/>import numpy as np<br/><br/># Train the model with a simple architecture<br/>mlp = MLPClassifier(hidden_layer_sizes=(4,2), random_state=42)<br/>mlp.fit(X_train, y_train)<br/><br/># Print the "model"<br/>print("THE MODEL:")<br/>print("Network Architecture:")<br/>print(f"Input Layer: {mlp.n_features_in_} neurons (features)")<br/>for i, layer_size in enumerate(mlp.hidden_layer_sizes):<br/>   print(f"Hidden Layer {i+1}: {layer_size} neurons")<br/>print(f"Output Layer: {mlp.n_outputs_} neurons (classes)")<br/><br/># Show weights for first hidden layer<br/>print("\nWeights from Input to First Hidden Layer:")<br/>weights_df = pd.DataFrame(<br/>   mlp.coefs_[0],<br/>   columns=[f'Hidden_{i+1}' for i in range(mlp.hidden_layer_sizes[0])],<br/>   index=['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind']<br/>)<br/>print(weights_df.round(3))<br/><br/>print("\nNote: Additional weights and biases exist between subsequent layers")<br/><br/># Make predictions and get probabilities<br/>y_pred = mlp.predict(X_test)<br/>y_prob = mlp.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre><h1 id="e991" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Model Comparison</h1><p id="b9eb" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">To summarize, here‚Äôs how each classifier calculates predicted probabilities:</p><ol class=""><li id="a25c" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj pu pv pw bk"><strong class="mq ga">Dummy Classifier</strong>: Uses the same probability scores for all predictions, based only on how often each class appeared in training. Ignores all input features.</li><li id="1fac" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">K-Nearest Neighbors</strong>: The probability score is the fraction of similar neighbors belonging to each class. Can only give specific fractions based on k (like 3/5 or 7/10).</li><li id="6d9a" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">Naive Bayes</strong>: Multiplies together the initial class probability and probabilities of seeing each feature value, then adjusts the results to add up to 1. Probability scores show how likely features are to appear in each class.</li><li id="1ddd" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">Decision Tree</strong>: Gives probability scores based on how often each class appeared in the final branches. Can only use probability values that it saw during training.</li><li id="1df6" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">Logistic Regression</strong>: Uses the sigmoid function to convert weighted feature combinations into probability scores. Can give any probability between 0 and 1, changing smoothly based on distance from the decision boundary.</li><li id="0132" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">Support Vector Machine</strong>: Needs an extra step (Platt Scaling) to create probability scores, using the sigmoid function to convert distances from the boundary. These distances determine how confident the model is.</li><li id="d47e" class="mo mp fq mq b gt px ms mt gw py mv mw mx pz mz na nb qa nd ne nf qb nh ni nj pu pv pw bk"><strong class="mq ga">Multi-Layer Perceptron</strong>: Processes data through multiple layers of transformations, ending with the sigmoid function. Creates probability scores from complex feature combinations, giving any value between 0 and 1.</li></ol><h1 id="f16c" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">Final Remark</h1><p id="6253" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Looking at how each model calculates its predicted probability shows us something important: each model has its own way of showing how confident it is. Some models like the Dummy Classifier and Decision Tree can only use certain probability scores based on their training data. Others like Logistic Regression and Neural Networks can give any probability between 0 and 1, letting them be more precise about their uncertainty.</p><p id="c46e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Here‚Äôs what‚Äôs interesting: even though all these models give us numbers between 0 and 1, these numbers mean different things for each model. Some get their scores by simple counting, others by measuring distance from a boundary, and some through complex calculations with features. This means a 70% probability from one model tells us something completely different than a 70% from another model.</p><p id="b39b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When picking a model to use, look beyond just accuracy. Think about whether the way it calculates predicted probability makes sense for your specific needs.</p><h1 id="ec51" class="od oe fq bf of og oh gv oi oj ok gy ol om on oo op oq or os ot ou ov ow ox oy bk">üåü Predicted Probability Code Summarized</h1><pre class="nn no np nq nr qr qs qt bp qu bb bk"><span id="f564" class="qv oe fq qs b bg qw qx l qy qz">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score<br/><br/># The models<br/>from sklearn.dummy import DummyClassifier<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.naive_bayes import BernoulliNB<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.svm import SVC<br/>from sklearn.neural_network import MLPClassifier<br/><br/># Load and prepare data<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Prepare features and target<br/>X,y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])<br/>X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])<br/><br/># Train the model<br/>clf = DummyClassifier(strategy='stratified', random_state=42)<br/># clf = KNeighborsClassifier(n_neighbors=3)<br/># clf = BernoulliNB()<br/># clf = DecisionTreeClassifier(random_state=42, max_depth=3)<br/># clf = LogisticRegression(random_state=42)<br/># clf = SVC(kernel='rbf', probability=True, random_state=42)<br/># clf = MLPClassifier(hidden_layer_sizes=(4,2), random_state=42)<br/><br/># Fit and predict<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)<br/>y_prob = clf.predict_proba(X_test)<br/><br/># Create results dataframe<br/>results_df = pd.DataFrame({<br/>   'True Label': y_test,<br/>   'Prediction': y_pred,<br/>   'Probability of Play': y_prob[:, 1]<br/>})<br/><br/>print("\nPrediction Results:")<br/>print(results_df)<br/><br/># Print accuracy<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre></div></div></div><div class="ab cb rz sa sb sc" role="separator"><span class="sd by bm se sf sg"/><span class="sd by bm se sf sg"/><span class="sd by bm se sf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="940d" class="pe oe fq bf of pf pg ph oi pi pj pk ol mx pl pm pn nb po pp pq nf pr ps pt fw bk">Technical Environment</h2><p id="e985" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="23a5" class="pe oe fq bf of pf pg ph oi pi pj pk ol mx pl pm pn nb po pp pq nf pr ps pt fw bk">About the Illustrations</h2><p id="6f23" class="pw-post-body-paragraph mo mp fq mq b gt oz ms mt gw pa mv mw mx pb mz na nb pc nd ne nf pd nh ni nj fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="9fcd" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ùôéùôöùôö ùô¢ùô§ùôßùôö ùôàùô§ùôôùôöùô° ùôÄùô´ùôñùô°ùô™ùôñùô©ùôûùô§ùô£ &amp; ùôäùô•ùô©ùôûùô¢ùôûùôØùôñùô©ùôûùô§ùô£ ùô¢ùôöùô©ùôùùô§ùôôùô® ùôùùôöùôßùôö:</p><div class="ra rb rc rd re"><div role="button" tabindex="0" class="ab bx cp kj it sh si bp sj lw ao"><div class="sk l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sl sm cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sl sm em n ay uw"/></div><div class="sn l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sq hp l"><h2 class="bf ga xt ic it xu iv iw rl iy ja fz bk">Model Evaluation &amp; Optimization</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xv wv ww wx wy lj wz xa vh ii xb xc xd vl vm vn ep bm vo nz" href="https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xw l il"><span class="bf b dy z dx">3 stories</span></div></div></div><div class="sz dz ta it ab tb il ed"><div class="ed st bx su sv"><div class="dz l"><img alt="" class="dz" src="../Images/18fa82b1435fa7d5571ee54ae93a6c62.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*7iilm-b4uavyJU4RGTwmXA.png"/></div></div><div class="ed st bx kk sw sx"><div class="dz l"><img alt="" class="dz" src="../Images/c95e89d05d1de700c631c342cd008de0.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*IouZGTjmruanqsZ2o_--JQ.png"/></div></div><div class="ed bx hx sy sx"><div class="dz l"><img alt="" class="dz" src="../Images/30e20e1a8ba3ced1e77644b706acd18d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XQDe622Tw9GCKJ8N4b0QeQ.png"/></div></div></div></div></div><p id="93cd" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:</p><div class="ra rb rc rd re"><div role="button" tabindex="0" class="ab bx cp kj it sh si bp sj lw ao"><div class="sk l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sl sm cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sl sm em n ay uw"/></div><div class="sn l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sq hp l"><h2 class="bf ga xt ic it xu iv iw rl iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xv wv ww wx wy lj wz xa vh ii xb xc xd vl vm vn ep bm vo nz" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xw l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sz dz ta it ab tb il ed"><div class="ed st bx su sv"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed st bx kk sw sx"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sy sx"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><div class="ra rb rc rd re"><div role="button" tabindex="0" class="ab bx cp kj it sh si bp sj lw ao"><div class="sk l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by sl sm cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l sl sm em n ay uw"/></div><div class="sn l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sq hp l"><h2 class="bf ga xt ic it xu iv iw rl iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xv wv ww wx wy lj wz xa vh ii xb xc xd vl vm vn ep bm vo nz" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xw l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sz dz ta it ab tb il ed"><div class="ed st bx su sv"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed st bx kk sw sx"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sy sx"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>