- en: 'Llama-2 vs. Llama-3: a Tic-Tac-Toe Battle Between Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llama-2-vs-llama-3-a-tic-tac-toe-battle-between-models-7301962ca65d?source=collection_archive---------5-----------------------#2024-04-27](https://towardsdatascience.com/llama-2-vs-llama-3-a-tic-tac-toe-battle-between-models-7301962ca65d?source=collection_archive---------5-----------------------#2024-04-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making a non-scientific benchmark with Python and Llama-CPP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--7301962ca65d--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--7301962ca65d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7301962ca65d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7301962ca65d--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--7301962ca65d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7301962ca65d--------------------------------)
    ·10 min read·Apr 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6acd24b61201d2f078a6988680091e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Solstice Hannan](https://unsplash.com/@darkersolstice), Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: About a week before the time of writing this story, new open Llama-3 models
    [were released](https://ai.meta.com/blog/meta-llama-3/) by Meta. As claimed by
    Meta, these are “the best models existing today at the 8B and 70B parameter scales.”
    For example, according to a [HuggingFace model page](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct),
    Llama-3 8B got a 66.6 score compared to 45.7 for Llama-2 7B in the MMLU (Massive
    Multitask Language Understanding) benchmark. A Llama-3 also got a 72.6 vs. 57.6
    score in CommonSense QA (dataset for commonsense question answering). An instruction-tuned
    Llama-3 8B model got a 30.0 compared to a 3.8 score in a math benchmark, which
    indeed is an impressive improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Academic benchmarks are important, but can we see the real difference “in action”?
    Apparently, we can, and it can be fun. Let’s program a tic-tac-toe game between
    two models and see which one wins! During the game, I will test all 7B, 8B, and
    70B models. In parallel, I will also collect some data about model performance
    and system requirements. All tests can be run for free in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Loading The Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test all models, I will use the [Llama-cpp](https://github.com/abetlen/llama-cpp-python)
    Python library because it can run on both CPU and GPU. We…
  prefs: []
  type: TYPE_NORMAL
