<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The History of Convolutional Neural Networks for Image Classification (1989 - Today)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The History of Convolutional Neural Networks for Image Classification (1989 - Today)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28">https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5957" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A visual tour of the greatest innovations in Deep Learning and Computer Vision.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Avishek Biswas" class="l ep by dd de cx" src="../Images/6feb591069f354aa096f6108f1a70ea7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*7-2CKsevyqzgVs8m"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------" rel="noopener follow">Avishek Biswas</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mj mk ml mm mn mo"><div class="mp io l ed"><div class="mq mr l"/></div></figure><p id="952c" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Before CNNs, the standard way to train a neural network to classify images was to flatten it into a list of pixels and pass it through a feed-forward neural network to output the image’s class. The problem with flattening the image is that the essential spatial information in the image is discarded.</p><p id="fade" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In 1989, Yann LeCun and team introduced Convolutional Neural Networks — the backbone of Computer Vision research for the last 15 years! Unlike feedforward networks, CNNs preserve the 2D nature of images and are capable of processing information spatially!</p><p id="2744" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In this article, we are going to go through the history of CNNs specifically for Image Classification tasks — starting from those early research years in the 90's to the golden era of the mid-2010s when many of the most genius Deep Learning architectures ever were conceived, and finally discuss the latest trends in CNN research now as they compete with attention and vision-transformers.</p><p id="5a87" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><em class="no">Check out the </em><a class="af np" href="https://youtu.be/N_PocrMHWbw" rel="noopener ugc nofollow" target="_blank"><em class="no">YouTube video</em></a><em class="no"> that explains all the concepts in this article visually with animations. Unless otherwise specified, all the images and illustrations used in this article are generated by myself during creating the video version.</em></p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div class="nq nr ns"><img src="../Images/6f788fc5c6f6bbd352b7b138f5198dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*R9YqQdYqHZsTw9P7Uw5AOw.png"/></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">The papers we will be discussing today!</figcaption></figure><h1 id="4ae8" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">The Basics of Convolutional Neural Networks</strong></h1><p id="bd71" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">At the heart of a CNN is the convolution operation. We scan the filter across the image and calculate the dot product of the filter with the image at each overlapping location. <strong class="mu fr">This resulting output is called a feature map and it captures how much and where the filter pattern is present in the image.</strong></p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pa"><img src="../Images/6a88bb9a19c81a4788e265c2bdfb7177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ZBZhhftMDbMYnsdxzzPfg.gif"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">How Convolution works — The kernel slides over the input image and calculates the overlap (dot-product) at each location — outputting a feature map in the end!</figcaption></figure><p id="6907" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In a convolution layer, we train multiple filters that extract different feature maps from the input image. When we stack multiple convolutional layers in sequence with some non-linearity, we get a convolutional neural network (CNN).</p><p id="daa9" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">So each convolution layer simultaneously does 2 things —<br/>1. <strong class="mu fr">spatial filtering</strong> with the convolution operation between images and kernels, and <br/>2. <strong class="mu fr">combining the multiple input channels </strong>and output a new set of channels.</p><p id="2890" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">90 percent of the research in CNNs has been to modify or to improve just these two things.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/7a53ab3d99f44347e87950b7143ae2f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xu0kxaS40fyljAKMvp4t_Q.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">The two main things CNN do</figcaption></figure><h2 id="44f2" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">The 1989 Paper</h2><p id="52f4" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk"><a class="af np" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="noopener ugc nofollow" target="_blank">This 1989 paper </a>taught us how to train non-linear CNNs from scratch using backpropagation. They input 16x16 grayscale images of handwritten digits, and pass through two convolutional layers with 12 filters of size 5x5. The filters also move with a stride of 2 during scanning. Strided-convolution is useful for downsampling the input image. After the conv layers, the output maps are flattened and passed through two fully connected networks to output the probabilities for the 10 digits. Using the softmax cross-entropy loss, the network is optimized to predict the correct labels for the handwritten digits. After each layer, the tanh nonlinearity is also used — allowing the learned feature maps to be more complex and expressive. With just 9760 parameters, this was a very small network compared to today’s networks which contain hundreds of millions of parameters.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/5f6ce141fce07f95a9408e58d031bf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ct0jgrMwlxJRSBtHHhCI7Q.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">The OG CNN architecture from 1989</figcaption></figure><h2 id="2217" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk"><strong class="al">Inductive Bias</strong></h2><p id="24a2" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Inductive Bias is a concept in Machine Learning where we deliberately introduce specific rules and limitations into the learning process to move our models away from generalizations and steer more toward solutions that follow our human-like understanding.</p><p id="c5c0" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">When humans classify images, we also do spatial filtering <em class="no">to look for common patterns to form multiple representations</em> and then <em class="no">combine them together to form our predictions</em>. The CNN architecture is designed to replicate just that. In feedforward networks, each pixel is treated like it’s own isolated feature as each neuron in the layers connects with all the pixels — in CNNs there is more parameter-sharing because the same filter scans the entire image. Inductive biases make CNNs less data-hungry too because they get local pattern recognition for free due to the network design but feedforward networks need to spend their training cycles learning about it from scratch.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="811c" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">Le-Net 5 (1998)</strong></h1><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr qk"><img src="../Images/d625a2448a44d6150887d3d8a598512d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxSYBoE_XlGe_XeUQ-COdw.png"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Lenet-5 architecture (Credit: <a class="af np" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">Le-Net-5 paper</a>)</figcaption></figure><p id="532d" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">In 1998, Yann LeCun and team published the <a class="af np" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">Le-Net 5</a> — a deeper and larger 7-layer CNN model network. They also use Max Pooling which downsamples the image by grabbing the maximum values from a 2x2 sliding window.</p></div></div><div class="mo"><div class="ab cb"><div class="lm ql ln qm lo qn cf qo cg qp ci bh"><div class="mj mk ml mm mn ab ke"><figure class="lb mo qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/59cfe79ef5bedf3c1b0dd5b33ae17f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*_gjfM9rU5DfvnaI2OUIozg.jpeg"/></div></figure><figure class="lb mo qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/1f847d9fc9f1eab0131f8b91092d006d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*E6PIrM-QbrYcdF_sUfBG7A.jpeg"/></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx qv ed qw qx">How Max Pooling works (LEFT) and how Local Receptive Fields increase as CNNs add more layers (RIGHT)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="a74e" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Local Receptive Field</h2><p id="9b7e" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Notice when you train a 3x3 conv layer, each neuron is connected to a 3x3 region in the original image — this is the neuron’s local receptive field — the region of the image where this neuron extracts patterns from.</p><p id="b8bc" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">When we pass this feature map through another 3x3 layer , the new feature map indirectly creates a receptive field of a larger 5x5 region from the original image. Additionally, when we downsample the image through max-pooling or strided-convolution, the receptive field also increases — making deeper layers access the input image more and more globally.</p><p id="0f6f" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">For this reason, earlier layers in a CNN can only pick low-level details like specific edges or corners, and the latter layers pick up more spread-out global-level patterns.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6411" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk">The Draught (1998–2012)</h1><p id="f3db" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">As impressive Le-Net-5 was, researchers in the early 2000s still deemed neural networks to be computationally very expensive and data hungry to train. There was also problems with overfitting — where a complex neural network will just memorize the entire training dataset and fail to generalize on new unseen datasets. The researchers instead focused on traditional machine learning algorithms like support vector machines that were showing much better performance on the smaller datasets of the time with much less computational demands.</p><h2 id="c539" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk"><strong class="al">ImageNet Dataset (2009)</strong></h2><p id="651a" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The <a class="af np" href="https://www.image-net.org/index.php" rel="noopener ugc nofollow" target="_blank">ImageNet dataset</a> was open-sourced in 2009 — it contained 3.2 million annotated images at the time covering over 1000 different classes. Today it has over 14 million images and over 20,000 annotated different classes. Every year from 2010 to 2017 we got this massive competition called the <a class="af np" href="https://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank">ILSVRC</a> where different research groups will publish models to beat the benchmarks on a subset of the ImageNet dataset. In 2010 and 2011, traditional ML methods like Support Vector Machines were winning — but starting from 2012 it was all about CNNs. The metric used to rank different networks was generally the top-5 error rate — measuring the percentage of times that the true class label was not in the top 5 classes predicted by the network.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="eaa0" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">AlexNet (2012)</strong></h1><p id="f504" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk"><a class="af np" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">AlexNet</a>, introduced by Dr. Geoffrey Hinton and his team was the winner of ILSVRC 2012 with a top-5 test set error of 17%. Here are the three main contributions from AlexNet.</p><h2 id="773a" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">1. Multi-scaled Kernels</h2><p id="e124" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">AlexNet trained on 224x224 RGB images and used multiple kernel sizes in the network — an 11x11, a 5x5, and a 3x3 kernel. Models like Le-Net 5 only used 5x5 kernels. Larger kernels are more computationally expensive because they train more weights, but also capture more global patterns from the image. Because of these large kernels, AlexNet had over 60 million trainable parameters. All that complexity can however lead to overfitting.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/3dbfe9a516bc0af4053955d3cbd0ae81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCkgETIJwxuT92iqzBCqpQ.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">AlexNet starts with larger kernels (11x11) and reduces the size (to 5x5 and 3x3) for deeper layers (Image by the author)</figcaption></figure><h2 id="17da" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">2. Dropout</h2><p id="1f7e" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">To alleviate overfitting, AlexNet used a regularization technique called Dropout. During training, a fraction of the neurons in each layer is turned to zero. This prevents the network from being too reliant on specific neurons or groups of neurons for generating a prediction and instead encourages all the neurons to learn general meaningful features useful for classification.</p><h2 id="49ec" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">3. RELU</h2><p id="f557" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Alexnet also replaced tanh nonlinearity with ReLU. RELU is an activation function that turns negative values to zero and keeps positive values as-is. The tanh function tends to saturate for deep networks because the gradients get low when the value of x goes too high or too low making optimization slow. RELU offers a steady gradient signal to train the network about 6 times faster than tanH.</p></div></div><div class="mo"><div class="ab cb"><div class="lm ql ln qm lo qn cf qo cg qp ci bh"><div class="mj mk ml mm mn ab ke"><figure class="lb mo qy qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/46bebdc63f8f31a04b7b17236f3a5c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*6l4KpIFPvoBSJYNWgOlQng.png"/></div></figure><figure class="lb mo qz qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/9853cded8ce78082f5567793b0d22c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*TbcleyBbGxSNVB5NGB-0kQ.png"/></div></figure><figure class="lb mo ra qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/40d276fedfc70960ef639c45f76d221a.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*zuUTinCYxLjAShUI0_9gug.png"/></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx rb ed rc qx">RELU, TANH, and How much difference RELU makes (Image credits: Middle: <a class="af np" href="https://learning.oreilly.com/library/view/artificial-intelligence-for/9781788472173/" rel="noopener ugc nofollow" target="_blank">Artificial Intelligence for Big Data</a>, Right: <a class="af np" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Alex-Net paper</a>)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e190" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">AlexNet also introduced the concept of Local Response Normalization and strategies for distributed CNN training.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5ada" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">GoogleNet / Inception (2014)</strong></h1><p id="dcc5" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">In 2014, GoogleNet paper got an ImageNet top-5 error rate of 6.67%. The core component of GoogLeNet was the <strong class="mu fr">inception module</strong>. Each inception module consists of parallel convolutional layers with different filter sizes (1x1, 3x3, 5x5) and max-pooling layers. Inception applies these kernels to the same input and then concats them, combining both low-level and medium-level features.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/997501a2e4dd36f4519c2d85b202386a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwHMa1AfX2gAv_Ubz_dIwA.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">An Inception Module</figcaption></figure><h2 id="087c" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">1x1 Convolution</h2><p id="7658" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">They also use 1x1 convolutional layer. Each 1x1 kernel first scales the input channels and then combines them. 1x1 kernels multiply each pixel with a fixed value — which is why it is also called pointwise convolutions.</p><p id="f6ea" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">While larger kernels like 3x3 and 5x5 kernels do both spatial filtering and channel combination, 1x1 kernels are only good for channel mixing, and it does so very efficiently with a lower number of weights. For example, A 3-by-4 grid of 1x1 convolution layers trains only <em class="no">(1x1 x 3x4 =) </em>12 weights — but if it were 3x3 kernels — we would train<em class="no"> (3x3 x 3x4 =)</em> 108 weights.</p></div></div><div class="mo"><div class="ab cb"><div class="lm ql ln qm lo qn cf qo cg qp ci bh"><div class="mj mk ml mm mn ab ke"><figure class="lb mo qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/6f234f8f634cf640af57b563c47cb50d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*iD0MYgFo7jdshDfV8ZDy2g.jpeg"/></div></figure><figure class="lb mo qq qr qs qt qu paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><img src="../Images/65f6501cf00fd0cbb64593fff021f896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*PagDEUQmdUp6mgnW0yQFfg.jpeg"/></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx qv ed qw qx">1x1 kernels versus larger kernels (LEFT) and Dimensionality reduction with 1x1 kernels (RIGHT)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="9416" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Dimensionality Reduction</h2><p id="3f4c" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">GoogleNet uses 1x1 conv layers as a dimensionality reduction method to reduce the number of channels before running spatial filtering with the 3x3 and 5x5 convolutions on these lower dimensional feature maps. This helps them to cut down on the number of trainable weights compared to AlexNet.</p><h1 id="f02b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">VGGNet (2014)</strong></h1><p id="1843" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk"><a class="af np" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">The VGG Network</a> claims that we do not need larger kernels like 5x5 or 7x7 networks and all we need are 3x3 kernels. 2 layer 3x3 convolutional layer has the same receptive field of the image that a single 5x5 layer does. Three 3x3 layers have the same receptive field that a single 7x7 layer does.</p><p id="b22e" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><strong class="mu fr">Deep 3x3 Convolution Layers capture the same receptive field as larger kernels but with fewer parameters!</strong></p><p id="3819" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">One 5x5 filter trains 25 weights — while two 3x3 filters train 18 weights. Similarly one 7x7 trains 49 weights, while 3 3x3 trains just 27. Training with deep 3x3 convolution layers became the standard for a long time in CNN architectures.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/cfed290a6af3ba3dee066dc01e5f4633.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*feGtykjciBQjN-jBXX0QvQ.jpeg"/></div></div></figure><h1 id="b5f8" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Batch Normalization (2015)</strong></h1><p id="1ee5" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Deep neural networks can suffer from a problem known as <strong class="mu fr">“Internal Covariate Shift”</strong> during training. Since the earlier layers of the network are constantly training, the latter layers need to continuously adapt to the constantly shifting input distribution it receive from the previous layers.</p><p id="312a" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><a class="af np" href="https://arxiv.org/pdf/1502.03167" rel="noopener ugc nofollow" target="_blank">Batch Normalization</a> aims to counteract this problem by normalizing the inputs of each layer to have zero mean and unit standard deviation during training. A batch normalization or BN layer can be applied after any convolution layer. During training it subtracts the mean of the feature map along the minibatch dimension and divides it by the standard deviation. This means that each layer will now see a more stationary unit gaussian distribution during training.</p><h2 id="32f9" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Advantages of Batch Norm</h2><ol class=""><li id="fb60" class="ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn rd re rf bk">converge around 14 times faster</li><li id="fe12" class="ms mt fq mu b go rg mw mx gr rh mz na nb ri nd ne nf rj nh ni nj rk nl nm nn rd re rf bk">let us use higher learning rates, and</li><li id="d320" class="ms mt fq mu b go rg mw mx gr rh mz na nb ri nd ne nf rj nh ni nj rk nl nm nn rd re rf bk">makes the network robust to the initial weights of the network.</li></ol></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9ae0" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">ResNets (2016)</strong></h1><h2 id="cab5" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Deep Networks struggle to do Identity Mapping</h2><p id="9041" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Imagine you have a shallow neural network that has great accuracy on a classification task. Turns out that if we added 100 new convolution layers on top of this network, the training accuracy of the model could go down!</p><p id="752c" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">This is quite counter-intuitive because all these new layers need to do is copy the output of the shallow network at each layer — and at least be able to match the original accuracy. In reality, deep networks can be notoriously difficult to train because gradients can saturate or become unstable when backpropagating through many layers. With Relu and batch norm, we were able to train 22-layer deep CNNs at this point — the good folks at Microsoft introduced <a class="af np" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">ResNets</a> in 2015 which allowed us to stably train 150 layered CNNs. What did they do?</p><h2 id="6e23" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Residual learning</h2><p id="0d2d" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The input passes through one or more CNN layers as usual, but at the end, the original input is added back to the final output. These blocks are called residual blocks because they don’t need to learn the final output feature maps in the traditional sense — but they are just the residual features that must be added to the input to get the final feature maps. <strong class="mu fr">If the weights in the middle layers were to turn themselves to ZERO, then the residual block would just return the identity function — meaning it would be able to easily copy the input X.</strong></p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/b67371d5f87b5198e3e1a03c3a62ef15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVFU7lfuzC_gkpLcasMolw.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Residual Networks</figcaption></figure><h2 id="ed27" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Easy Gradient Flow</h2><p id="8b3e" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">During backpropagation gradients can directly flow through these shortcut paths to reach the earlier layers of the model faster, helping to prevent gradient vanishing issues. ResNet stacks many of these blocks together to form really deep networks without any loss of accuracy!</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr rl"><img src="../Images/55b293233faca9c8f22ed808e1dd8c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*guSWB-JPErEPi4B17YpSJw.png"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx"><a class="af np" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">From the ResNet paper</a></figcaption></figure><p id="714a" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">And with this remarkable improvement, ResNets managed to train a 152-layered model that got a top-5 error rate that shattered all previous records!</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="15f0" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">DenseNet (2017)</strong></h1><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr rm"><img src="../Images/ef9fa851e3acd1c08106018c2f39b471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqkSnu1k4QzqJvXQAmJ2XQ.png"/></div></div></figure><p id="4f7c" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk"><a class="af np" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">Dense-Nets</a> also add shortcut paths connecting earlier layers with the latter layers in the network. A DenseNet block trains a series of convolution layers, and the output of every layer is concatenated with the feature maps of every previous layer in the block before passing to the next layer. Each layer adds only a small number of new feature maps to the “collective knowledge” of the network as the image flows through the network. DenseNets have an improved flow of information and gradients throughout the network because each layer has direct access to the gradients from the loss function.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/164f7f2c97a282693b45bcb96e2b21d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mWF6ri2F-ID2xwof7M5fA.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Dense Nets</figcaption></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="dc7a" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk">Squeeze and Excitation Network (2017)</h1><p id="b13b" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk"><a class="af np" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">SEN-NET</a> was the final winner of the ILSVRC competition, which introduced the Squeeze and Excitation Layer into CNNs. The SE block is designed to explicitly model the dependencies between all the channels of a feature map. In normal CNNs, each channel of a feature map is computed independently of each other; SEN-Net applies a self-attention-like method to make each channel of a feature map contextually aware of the global properties of the input image. SEN-Net won the final ILVSRC of 2017, and one of the 154-layered SenNet + ResNet models got a ridiculous top-5 error rate of 4.47%.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr rn"><img src="../Images/6b871751ded64eef0b9db11f9795ad28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nzv9Qd-o4oYw9C5zbhqeHg.png"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx"><a class="af np" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">SEN-NET</a></figcaption></figure><h2 id="acfa" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Squeeze Operation</h2><p id="4acb" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The squeeze operation compresses the spatial dimensions of the input feature map into a channel descriptor using global average pooling. Since each channel contains neurons that capture local properties of the image, the squeeze operation accumulates global information about each channel.</p><h2 id="d60a" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Excitation Operation</h2><p id="a918" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The excitation operation rescales the input feature maps by channel-wise multiplication with the channel descriptors obtained from the squeeze operation. This effectively propagates global-level information to each channel — contextualizing each channel with the rest of the channels in the feature map.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr ro"><img src="../Images/c37db341a2da8c50a202b6e36a3c93a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FOt7FMEDVk5RsLrbynipWg.png"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx"><a class="af np" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">Squeeze and Excitation Block</a></figcaption></figure><h1 id="7450" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">MobileNet (2017)</strong></h1><p id="458f" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Convolution layers do two things –1) <em class="no">filtering spatial information</em> and 2) <em class="no">combining them channel-wise</em>. The MobileNet paper uses <strong class="mu fr">Depthwise Separable Convolution</strong>,<strong class="mu fr"> </strong>a technique that separates these two operations into two different layers — Depthwise Convolution for filtering and pointwise convolution for channel combination.</p><h2 id="a50f" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Depthwise Convolution</h2><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/1b885d58c9a73b3b1a51592e2d9e1966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkqAuw1LpGwF0yDzCiq9uA.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Depthwise Separable Convolution</figcaption></figure><p id="f101" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Given an input set of feature maps with M channels, first, they use depthwise convolution layers that train M 3x3 convolutional kernels. Unlike normal convolution layers that perform convolution on all feature maps, depthwise convolution layers train filters that perform convolution on just one feature map each. Secondly, they use 1x1 pointwise convolution filters to mix all these feature maps. <strong class="mu fr">Separating the filtering and combining steps like this drastically reduces the number of weights</strong>, making it super lightweight while still retaining the performance.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/66344afbabc2e1473b31832405fdda81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVjS5PdjSw1uQp4MpAbsRQ.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Why Depthwise Separable Layers reduce training weights</figcaption></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="29a9" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">MobileNetV2 (2019)</strong></h1><p id="75fc" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">In 2018, MobileNetV2 improved the MobileNet architecture by introducing two more innovations: <em class="no">Linear Bottlenecks</em> and <em class="no">Inverted residuals</em>.</p><h2 id="1b49" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Linear Bottlenecks</h2><p id="9b9d" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">MobileNetV2 uses 1x1 pointwise convolution for dimensionality reduction, followed by depthwise convolution layers for spatial filtering, and another 1x1 pointwise convolution layer to expand the channels back. <strong class="mu fr">These bottlenecks don’t pass through RELU and are instead kept linear.</strong> RELU zeros out all the negative values that came out of the dimensionality reduction step — and this can cause the network to lose valuable information especially if a bulk of this lower dimensional subspace was negative. Linear layers prevent the loss of excessive information during this bottleneck.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/8950a4c49cce3be0d309a0c1db0f9c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyvveiqDqMApN6bvU-eD2Q.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">The width of each feature map is intended to show the relative channel dimensions.</figcaption></figure><h2 id="da02" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Inverted Residuals</h2><p id="aa2b" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The second innovation is called Inverted Residuals. Generally, residual connections occur between layers with the highest channels, but the authors add shortcuts between the bottlenecks layers. The bottleneck captures the relevant information within a low-dimensional latent space, and the free flow of information and gradient between these layers is the most crucial.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/c2e4c4eae0d3d37c24af81e463c38f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*riEtQs0TtxQPbW0tN_ewXA.jpeg"/></div></div></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6d0b" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">Vision Transformers (2020)</strong></h1><p id="dc8a" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Vision Transformers or ViTs established that transformers can indeed beat state-of-the-art CNNs in Image Classification. Transformers and Attention mechanisms provide a highly parallelizable, scalable, and general architecture for modeling sequences. Neural Attention is a whole different area of Deep Learning, which we won’t get into this article, but feel free to learn more in this Youtube video.</p><figure class="mj mk ml mm mn mo"><div class="mp io l ed"><div class="rp mr l"/></div></figure><h2 id="ebb0" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">ViTs use Patch Embeddings and Self-Attention</h2><p id="d577" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The input image is first divided into a sequence of fixed-size patches. Each patch is independently embedded into a fixed-size vector either through a CNN or passing through a linear layer. These patch embeddings and their positional encodings are then inputted as a sequence of tokens into a self-attention-based transformer encoder. Self-attention models the relationships between all the patches, and outputs new updated patch embeddings that are contextually aware of the entire image.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/fc228d2bbe95cf7993f257be0bc535bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8tJJn-9g49K0pwz5z8g8nQ.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Vision Transformers. Each self-attention layer further contextualizes each patch embedding with the global context of the image</figcaption></figure><h2 id="a4a7" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Inductive Bias vs Generality</h2><p id="e636" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Where CNNs introduce several inductive biases about images, Transformers do the opposite — No localization, no sliding kernels — they rely on generality and raw computing to model the relationships between all the patches of the image. The Self-Attention layers allow global connectivity between all patches of the image irrespective of how far they are spatially. Inductive biases are great on smaller datasets, but the promise of Transformers is on massive training datasets, a general framework is going to eventually beat out the inductive biases offered by CNNs.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/6a450fb97b290d4419671ac36c27f673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Orozl7-yYB9Gfe1rI8NXGg.jpeg"/></div></div><figcaption class="nu nv nw nq nr nx ny bf b bg z dx">Convolution Layers vs Self-Attention Layers</figcaption></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="911c" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk"><strong class="al">ConvNext — </strong>A ConvNet for the 2020s <strong class="al">(2022)</strong></h1><p id="eef6" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">A great choice to include in this article would be Swin Transformers, but that is a topic for a different day! Since this is a CNN article, let’s focus on one last CNN paper.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr rq"><img src="../Images/3a0f9822f909de4a7b52dfe04861be4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ziTHdfXu7oQeEAGSlhUfqg.png"/></div></div></figure><h2 id="04f1" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Patchifying Images like VITs</h2><p id="201c" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The input of ConvNext follows the patching strategy inspired by Vision Transformers. A 4x4 convolution kernel with a stride of 4 creates a downsampled image which is inputted into the rest of the network.</p><h2 id="f59a" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Depthwise Separable Convolution</h2><p id="b6cd" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">Inspired by MobileNet, ConvNext uses depthwise separable convolution layers. The authors also hypothesize depthwise convolution is similar to the weighted sum operation in self-attention, which operates on a per-channel basis by only mixing information in the spatial dimension. Also the 1x1 pointwise convolutions are similar to the channel mixing steps in Self-Attention.</p><h2 id="9746" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Larger Kernel Sizes</h2><p id="9e75" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">While ConvNets have been using 3x3 kernels ever since VGG, ConvNext proposes larger 7x7 filters to capture a wider spatial context, trying to come close to the fully global context that ViTs capture, while retaining the localization spirits of CNNs.</p><p id="9843" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">There are also some other tweaks, like using MobileNetV2-inspired inverted bottlenecks, the GELU activations, layer norms instead of batch norms, and more that shape up the rest of the ConvNext architecture.</p><h2 id="803d" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">Scalability</h2><p id="5bfc" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">ConvNext are more computationally efficient way with the depthwise separable convolutions and is more scalable than transformers on high-resolution images — this is because Self-Attention scales quadratically with sequence length and Convolution doesn’t.</p><figure class="mj mk ml mm mn mo nq nr paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="nq nr pf"><img src="../Images/273120ccbef227363f48781e3fc84edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDvumb5UMpvYTzs1MQ0LPQ.jpeg"/></div></div></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="693a" class="nz oa fq bf ob oc qf gq oe of qg gt oh oi qh ok ol om qi oo op oq qj os ot ou bk">Final Thoughts!</h1><p id="8700" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">The history of CNNs teaches us so much about Deep Learning, Inductive Bias, and the nature of computation itself. It’ll be interesting to see what wins out in the end — the inductive biases of ConvNets or the Generality of Transformers. Do check out the companion YouTube video for a visual tour of this article, and the individual papers as listed below.</p><figure class="mj mk ml mm mn mo"><div class="mp io l ed"><div class="mq mr l"/></div></figure><h2 id="48c6" class="pg oa fq bf ob ph pi pj oe pk pl pm oh nb pn po pp nf pq pr ps nj pt pu pv pw bk">References</h2><p id="a53a" class="pw-post-body-paragraph ms mt fq mu b go ov mw mx gr ow mz na nb ox nd ne nf oy nh ni nj oz nl nm nn fj bk">CNN with Backprop (1989): <a class="af np" href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf</a></p><p id="ef64" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">LeNet-5: <a class="af np" href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf" rel="noopener ugc nofollow" target="_blank">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a></p><p id="b124" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">AlexNet:<a class="af np" href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></p><p id="5aa2" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">GoogleNet: <a class="af np" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.4842</a></p><p id="5bb0" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">VGG: <a class="af np" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.1556</a></p><p id="9077" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Batch Norm: <a class="af np" href="https://arxiv.org/pdf/1502.03167" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1502.03167</a></p><p id="84e9" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">ResNet: <a class="af np" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1512.03385</a></p><p id="6df2" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">DenseNet: <a class="af np" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1608.06993</a></p><p id="14a1" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">MobileNet: <a class="af np" href="https://arxiv.org/abs/1704.04861" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1704.04861</a></p><p id="9582" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">MobileNet-V2: <a class="af np" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.04381</a></p><p id="f99f" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Vision Transformers: <a class="af np" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.11929</a></p><p id="3027" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">ConvNext: <a class="af np" href="https://arxiv.org/abs/2201.03545" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2201.03545</a></p><p id="5e2f" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Squeeze-and-Excitation Network: <a class="af np" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1709.01507</a></p><p id="8d68" class="pw-post-body-paragraph ms mt fq mu b go mv mw mx gr my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn fj bk">Swin Transformers: <a class="af np" href="https://arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2103.14030</a></p></div></div></div></div>    
</body>
</html>