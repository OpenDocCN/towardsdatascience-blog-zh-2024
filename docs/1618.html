<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Learn Transformer Fine-Tuning and Segment Anything</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Learn Transformer Fine-Tuning and Segment Anything</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-transformer-fine-tuning-and-segment-anything-481c6c4ac802?source=collection_archive---------0-----------------------#2024-06-30">https://towardsdatascience.com/learn-transformer-fine-tuning-and-segment-anything-481c6c4ac802?source=collection_archive---------0-----------------------#2024-06-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="98b1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Train Meta’s Segment Anything Model (SAM) to segment high fidelity masks for any domain</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://todoran.medium.com/?source=post_page---byline--481c6c4ac802--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Stefan Todoran" class="l ep by dd de cx" src="../Images/37e67b874c85a58d597a6c8c82b1160f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*T5ggf7qVC12sewELe-aM2A.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--481c6c4ac802--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://todoran.medium.com/?source=post_page---byline--481c6c4ac802--------------------------------" rel="noopener follow">Stefan Todoran</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--481c6c4ac802--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="2ba1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The release of several powerful, open-source foundational models coupled with advancements in fine-tuning have brought about a new paradigm in machine learning and artificial intelligence. At the center of this revolution is the <a class="af nf" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">transformer model</a>.</p><p id="1bc4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While high accuracy domain-specific models were once out of reach for all but the most well funded corporations, today the foundational model paradigm allows for even the modest resources available to student or independent researchers to achieve results rivaling state of the art proprietary models.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/79d4f3b808380557b9859fb5d468c3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P0qFDR8GYl6HqBPjbK3iRg.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Fine-tuning can greatly improve performance on out-of-distribution tasks (image source: by author).</figcaption></figure><p id="71fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This article explores the application of Meta’s Segment Anything Model (SAM) to the remote sensing task of river pixel segmentation. If you’d like to jump right in to the code the source file for this project is available on <a class="af nf" href="https://github.com/geo-smart/water-surf/blob/main/book/chapters/masking_distributed.ipynb" rel="noopener ugc nofollow" target="_blank">GitHub</a> and the data is on <a class="af nf" href="https://huggingface.co/datasets/stodoran/elwha-segmentation-v1" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>, although reading the full article first is advised.</p><h1 id="e64e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Project Requirements</h1><p id="58f5" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">The first step is to either find or create a suitable dataset. Based on existing literature, a good fine-tuning dataset for SAM will have at least 200–800 images. A key lesson of the past decade of deep learning advancement is that more data is always better, so you can’t go wrong with a larger fine-tuning dataset. However, the goal behind foundational models is to allow even relatively small datasets to be sufficient for strong performance.</p><p id="7822" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It will also be necessary to have a HuggingFace account, which can be <a class="af nf" href="https://huggingface.co/join" rel="noopener ugc nofollow" target="_blank">created here</a>. Using HuggingFace we can easily store and fetch our dataset at any time from any device, which makes collaboration and reproducibility easier.</p><p id="c6f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The last requirement is a device with a GPU on which we can run the training workflow. An Nvidia T4 GPU, which is available for free through <a class="af nf" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">Google Colab</a>, is sufficiently powerful to train the largest SAM model checkpoint (sam-vit-huge) on 1000 images for 50 epochs in under 12 hours.</p><p id="31a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To avoid losing progress to usage limits on hosted runtimes, you can mount Google Drive and save each model checkpoint there. Alternatively, deploy and connect to a <a class="af nf" href="https://console.cloud.google.com/marketplace/product/colab-marketplace-image-public/colab" rel="noopener ugc nofollow" target="_blank">GCP virtual machine</a> to bypass limits altogether. If you’ve never used GCP before you are eligible for a free $300 dollar credit, which is enough to train the model at least a dozen times.</p><h1 id="cce2" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Understanding SAM</h1><p id="8f7b" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Before we begin training, we need to understand the architecture of SAM. The model contains three components: an image encoder from a minimally modified <a class="af nf" href="https://arxiv.org/pdf/2111.06377" rel="noopener ugc nofollow" target="_blank">masked autoencoder</a>, a flexible prompt encoder capable of processing diverse prompt types, and a quick and lightweight mask decoder. One motivation behind the design is to allow fast, real-time segmentation on edge devices (e.g. in the browser) since the image embedding only needs to be computed once and the mask decoder can run in ~50ms on CPU.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pa"><img src="../Images/1ca364b41e10eba87510cb407ede1fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GY61FYqj8mi6OxlGshA_7w.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">The model architecture of SAM shows us what inputs the model accepts and which portions of the model need to be trained (image source: <a class="af nf" href="https://github.com/facebookresearch/segment-anything" rel="noopener ugc nofollow" target="_blank">SAM GitHub</a>).</figcaption></figure><p id="4631" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In theory, the image encoder has already learned the optimal way to embed an image, identifying shapes, edges and other general visual features. Similarly, in theory the prompt encoder is already able to optimally encode prompts. The mask decoder is the part of the model architecture which takes these image and prompt embeddings and actually creates the mask by operating on the image and prompt embeddings.</p><p id="843f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As such, one approach is to freeze the model parameters associated with the image and prompt encoders during training and to only update the mask decoder weights. This approach has the benefit of allowing both supervised and unsupervised downstream tasks, since control point and bounding box prompts are both automatable and usable by humans.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pb"><img src="../Images/9bd06d274db8c4e8c28203001b9df717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2nvcWRT2JEj0iTbbrFX6w.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Diagram showing the frozen SAM image encoder and mask decoder, alongside the overloaded prompt encoder, used in the AutoSAM architecture (source: <a class="af nf" href="https://arxiv.org/pdf/2306.06370" rel="noopener ugc nofollow" target="_blank">AutoSAM paper</a>).</figcaption></figure><p id="fda0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An alternative approach is to overload the prompt encoder, freezing the image encoder and mask decoder and simply not using the original SAM mask encoder. For example, the AutoSAM architecture uses a network based on Harmonic Dense Net to produce prompt embeddings based on the image itself. In this tutorial we will cover the first approach, freezing the image and prompt encoders and training only the mask decoder, but code for this alternative approach can be found in the AutoSAM <a class="af nf" href="https://github.com/talshaharabany/AutoSAM/blob/main/inference.py" rel="noopener ugc nofollow" target="_blank">GitHub</a> and <a class="af nf" href="https://arxiv.org/pdf/2306.06370" rel="noopener ugc nofollow" target="_blank">paper</a>.</p><h1 id="436f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Configuring Prompts</h1><p id="8608" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">The next step is to determine what sorts of prompts the model will receive during inference time, so that we can supply that type of prompt at training time. Personally I would not advise the use of text prompts for any serious computer vision pipeline, given the unpredictable/inconsistent nature of natural language processing. This leaves points and bounding boxes, with the choice ultimately being down to the particular nature of your specific dataset, although the literature has found that bounding boxes outperform control points fairly consistently.</p><p id="3f8b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The reasons for this are not entirely clear, but it could be any of the following factors, or some combination of them:</p><ul class=""><li id="f6e7" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pc pd pe bk">Good control points are more difficult to select at inference time (when the ground truth mask is unknown) than bounding boxes.</li><li id="ac13" class="mj mk fq ml b go pf mn mo gr pg mq mr ms ph mu mv mw pi my mz na pj nc nd ne pc pd pe bk">The space of possible point prompts is orders of magnitude larger than the space of possible bounding box prompts, so it has not been as thoroughly trained.</li><li id="fe4a" class="mj mk fq ml b go pf mn mo gr pg mq mr ms ph mu mv mw pi my mz na pj nc nd ne pc pd pe bk">The original SAM authors focused on the model’s zero-shot and few-shot (counted in term of human prompt interactions) capabilities, so pretraining may have focused more on bounding boxes.</li></ul><p id="08ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Regardless, river segmentation is actually a rare case in which point prompts actually outperform bounding boxes (although only slightly, even with an extremely favorable domain). Given that in any image of a river the body of water will stretch from one end of the image to another, any encompassing bounding box will almost always cover most of the image. Therefore the bounding box prompts for very different portions of river can look extremely similar, in theory meaning that bounding boxes provide the model with significantly less information than control points and therefore leading to worse performance.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pk"><img src="../Images/caef3506b78c144b9dca83f9982381dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMVCmZFJ055ZhOz3IRfteA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Control points, bounding box prompts, and the ground truth segmentation overlaid on two sample training images (image source: by author).</figcaption></figure><p id="de49" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Notice how in the illustration above, although the true segmentation masks for the two river portions are completely different, their respective bounding boxes are nearly identical, while their points prompts differ (comparatively) more.</p><p id="5256" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The other important factor to consider is how easily input prompts can be generated at inference time. If you expect to have a human in the loop, then both bounding boxes and control points are both fairly trivial to acquire at inference time. However in the event that you intend to have a completely automated pipeline, answering this questions becomes more involved.</p><p id="379e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Whether using control points or bounding boxes, generating the prompt typically first involves estimating a rough mask for the object of interest. Bounding boxes can then just be the minimum box which wraps the rough mask, whereas control points need to be sampled from the rough mask. This means that bounding boxes are easier to obtain when the ground truth mask is unknown, since the estimated mask for the object of interest only needs to roughly match the same size and position of the true object, whereas for control points the estimated mask would need to more closely match the contours of the object.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pl"><img src="../Images/e9b118912df2a60eb5dde9b81600cb6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeoNjfgiv1nKMRM4ZfKUyA.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">When using an estimated mask as opposed to the ground truth, control point placement may include mislabeled points, while bounding boxes are generally in the right place (image source: by author).</figcaption></figure><p id="c039" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For river segmentation, if we have access to both RGB and NIR, then we can use spectral indices thresholding methods to obtain our rough mask. If we only have access to RGB, we can convert the image to HSV and threshold all pixels within a certain hue, saturation, and value range. Then, we can remove connected components below a certain size threshold and use <code class="cx pm pn po pp b">erosion</code> from <code class="cx pm pn po pp b">skimage.morphology</code> to make sure the only 1 pixels in our mask are those which were towards the center of large blue blobs.</p><h1 id="5355" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Model Training</h1><p id="0c9b" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">To train our model, we need a data loader containing all of our training data that we can iterate over for each training epoch. When we load our dataset from HuggingFace, it takes the form of a <code class="cx pm pn po pp b">datasets.Dataset</code> class. If the dataset is private, make sure to first install the HuggingFace CLI and sign in using <code class="cx pm pn po pp b">!huggingface-cli login</code>.</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="1243" class="pt oa fq pp b bg pu pv l pw px">from datasets import load_dataset, load_from_disk, Dataset<br/><br/>hf_dataset_name = "stodoran/elwha-segmentation-v1"<br/>training_data = load_dataset(hf_dataset_name, split="train")<br/>validation_data = load_dataset(hf_dataset_name, split="validation")</span></pre><p id="8842" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We then need to code up our own custom dataset class which returns not just an image and label for any index, but also the prompt. Below is an implementation that can handle both control point and bounding box prompts. To be initialized, it takes a HuggingFace <code class="cx pm pn po pp b">datasets.Dataset</code> instance and a SAM processor instance.</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="379d" class="pt oa fq pp b bg pu pv l pw px">from torch.utils.data import Dataset<br/><br/>class PromptType:<br/>    CONTROL_POINTS = "pts"<br/>    BOUNDING_BOX = "bbox"<br/><br/>class SAMDataset(Dataset):<br/>    def __init__(<br/>        self, <br/>        dataset, <br/>        processor, <br/>        prompt_type = PromptType.CONTROL_POINTS,<br/>        num_positive = 3,<br/>        num_negative = 0,<br/>        erode = True,<br/>        multi_mask = "mean",<br/>        perturbation = 10,<br/>        image_size = (1024, 1024),<br/>        mask_size = (256, 256),<br/>    ):<br/>        # Asign all values to self<br/>        ...<br/><br/>    def __len__(self):<br/>        return len(self.dataset)<br/><br/>    def __getitem__(self, idx):<br/>        datapoint = self.dataset[idx]<br/>        input_image = cv2.resize(np.array(datapoint["image"]), self.image_size)<br/>        ground_truth_mask = cv2.resize(np.array(datapoint["label"]), self.mask_size)<br/><br/>        if self.prompt_type == PromptType.CONTROL_POINTS:<br/>            inputs = self._getitem_ctrlpts(input_image, ground_truth_mask)<br/>        elif self.prompt_type == PromptType.BOUNDING_BOX:<br/>            inputs = self._getitem_bbox(input_image, ground_truth_mask)<br/><br/>        inputs["ground_truth_mask"] = ground_truth_mask<br/>        return inputs</span></pre><p id="27a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also have to define the <code class="cx pm pn po pp b">SAMDataset._getitem_ctrlpts</code> and <code class="cx pm pn po pp b">SAMDataset._getitem_bbox</code> functions, although if you only plan to use one prompt type then you can refactor the code to just directly handle that type in <code class="cx pm pn po pp b">SAMDataset.__getitem__</code> and remove the helper function.</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="ba73" class="pt oa fq pp b bg pu pv l pw px">class SAMDataset(Dataset):<br/>    ...<br/>    <br/>    def _getitem_ctrlpts(self, input_image, ground_truth_mask):<br/>        # Get control points prompt. See the GitHub for the source<br/>        # of this function, or replace with your own point selection algorithm.<br/>        input_points, input_labels = generate_input_points(<br/>            num_positive=self.num_positive,<br/>            num_negative=self.num_negative,<br/>            mask=ground_truth_mask,<br/>            dynamic_distance=True,<br/>            erode=self.erode,<br/>        )<br/>        input_points = input_points.astype(float).tolist()<br/>        input_labels = input_labels.tolist()<br/>        input_labels = [[x] for x in input_labels]<br/><br/>        # Prepare the image and prompt for the model.<br/>        inputs = self.processor(<br/>            input_image,<br/>            input_points=input_points,<br/>            input_labels=input_labels,<br/>            return_tensors="pt"<br/>        )<br/><br/>        # Remove batch dimension which the processor adds by default.<br/>        inputs = {k: v.squeeze(0) for k, v in inputs.items()}<br/>        inputs["input_labels"] = inputs["input_labels"].squeeze(1)<br/><br/>        return inputs<br/><br/>    def _getitem_bbox(self, input_image, ground_truth_mask):<br/>        # Get bounding box prompt.<br/>        bbox = get_input_bbox(ground_truth_mask, perturbation=self.perturbation)<br/><br/>        # Prepare the image and prompt for the model.<br/>        inputs = self.processor(input_image, input_boxes=[[bbox]], return_tensors="pt")<br/>        inputs = {k: v.squeeze(0) for k, v in inputs.items()} # Remove batch dimension which the processor adds by default.<br/><br/>        return inputs</span></pre><p id="07e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Putting it all together, we can create a function which creates and returns a PyTorch dataloader given either split of the HuggingFace dataset. Writing functions which return dataloaders rather than just executing cells with the same code is not only good practice for writing flexible and maintainable code, but is also necessary if you plan to use <a class="af nf" href="https://huggingface.co/docs/accelerate/index" rel="noopener ugc nofollow" target="_blank">HuggingFace Accelerate</a> to run distributed training.</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="830b" class="pt oa fq pp b bg pu pv l pw px">from transformers import SamProcessor<br/>from torch.utils.data import DataLoader<br/><br/>def get_dataloader(<br/>        hf_dataset,<br/>        model_size = "base",  # One of "base", "large", or "huge" <br/>        batch_size = 8, <br/>        prompt_type = PromptType.CONTROL_POINTS,<br/>        num_positive = 3,<br/>        num_negative = 0,<br/>        erode = True,<br/>        multi_mask = "mean",<br/>        perturbation = 10,<br/>        image_size = (256, 256),<br/>        mask_size = (256, 256),<br/>    ):<br/>    processor = SamProcessor.from_pretrained(f"facebook/sam-vit-{model_size}")<br/><br/>    sam_dataset = SAMDataset(<br/>        dataset=hf_dataset, <br/>        processor=processor, <br/>        prompt_type=prompt_type,<br/>        num_positive=num_positive,<br/>        num_negative=num_negative,<br/>        erode=erode,<br/>        multi_mask=multi_mask,<br/>        perturbation=perturbation,<br/>        image_size=image_size,<br/>        mask_size=mask_size,<br/>    )<br/>    dataloader = DataLoader(sam_dataset, batch_size=batch_size, shuffle=True)<br/>    <br/>    return dataloader</span></pre><p id="d3f5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After this, training is simply a matter of loading the model, freezing the image and prompt encoders, and training for the desired number of iterations.</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="8338" class="pt oa fq pp b bg pu pv l pw px">model = SamModel.from_pretrained(f"facebook/sam-vit-{model_size}")<br/>optimizer = AdamW(model.mask_decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)<br/><br/># Train only the decoder.<br/>for name, param in model.named_parameters():<br/>    if name.startswith("vision_encoder") or name.startswith("prompt_encoder"):<br/>        param.requires_grad_(False)</span></pre><p id="1161" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Below is the basic outline of the training loop code. Note that the <code class="cx pm pn po pp b">forward_pass</code>, <code class="cx pm pn po pp b">calculate loss</code>, <code class="cx pm pn po pp b">evaluate_model</code>, and <code class="cx pm pn po pp b">save_model_checkpoint</code> functions have been left out for brevity, but implementations are available on the GitHub. The forward pass code will differ slightly based on the prompt type, and the loss calculation needs a special case based on prompt type as well; when using point prompts, SAM returns a predicted mask for every single input point, so in order to get a single mask which can be compared to the ground truth either the predicted masks need to be averaged, or the best predicted mask needs to be selected (identified based on SAM’s predicted IoU scores).</p><pre class="nj nk nl nm nn pq pp pr bp ps bb bk"><span id="e607" class="pt oa fq pp b bg pu pv l pw px">train_losses = []<br/>validation_losses = []<br/>epoch_loop = tqdm(total=num_epochs, position=epoch, leave=False)<br/>batch_loop = tqdm(total=len(train_dataloader), position=0, leave=True)<br/><br/>while epoch &lt; num_epochs:<br/>    epoch_losses = []<br/>    <br/>    batch_loop.n = 0  # Loop Reset<br/>    for idx, batch in enumerate(train_dataloader):<br/>        # Forward Pass<br/>        batch = {k: v.to(accelerator.device) for k, v in batch.items()}<br/>        outputs = forward_pass(model, batch, prompt_type)<br/><br/>        # Compute Loss<br/>        ground_truth_masks = batch["ground_truth_mask"].float()<br/>        train_loss = calculate_loss(outputs, ground_truth_masks, prompt_type, loss_fn, multi_mask="best")<br/>        epoch_losses.append(train_loss)<br/><br/>        # Backward Pass &amp; Optimizer Step<br/>        optimizer.zero_grad()<br/>        accelerator.backward(train_loss)<br/>        optimizer.step()<br/>        lr_scheduler.step()<br/><br/>        batch_loop.set_description(f"Train Loss: {train_loss.item():.4f}")<br/>        batch_loop.update(1)<br/><br/>    validation_loss = evaluate_model(model, validation_dataloader, accelerator.device, loss_fn)<br/>    train_losses.append(torch.mean(torch.Tensor(epoch_losses)))<br/>    validation_losses.append(validation_loss)<br/><br/>    if validation_loss &lt; best_loss:<br/>        save_model_checkpoint(<br/>            accelerator,<br/>            best_checkpoint_path,<br/>            model,<br/>            optimizer,<br/>            lr_scheduler,<br/>            epoch,<br/>            train_history,<br/>            validation_loss,<br/>            train_losses,<br/>            validation_losses,<br/>            loss_config,<br/>            model_descriptor=model_descriptor,<br/>        )<br/>        best_loss = validation_loss<br/><br/>    epoch_loop.set_description(f"Best Loss: {best_loss:.4f}")<br/>    epoch_loop.update(1)<br/>    epoch += 1</span></pre><h1 id="f15a" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Tuning Results</h1><p id="1207" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">For the Elwha river project, the best setup achieved trained the “sam-vit-base” model using a dataset of over 1k segmentation masks using a GCP instance in under 12 hours.</p><p id="cf94" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Compared with baseline SAM the fine-tuning drastically improved performance, with the median mask going from unusable to highly accurate.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh py"><img src="../Images/2a00107139c1bb22dddf9d3a329fc6cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9P9YpU1TjLokumS1ME-REQ.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Fine-tuning SAM greatly improves segmentation performance relative to baseline SAM with the default prompt (image source: by author).</figcaption></figure><p id="87b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One important fact to note is that the training dataset of 1k river images was imperfect, with segmentation labels varying greatly in the amount of correctly classified pixels. As such, the metrics shown above were calculated on a held-out pixel perfect dataset of 225 river images.</p><p id="e9ff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An interesting observed behavior was that the model learned to generalize from the imperfect training data. When evaluating on datapoints where the training example contained obvious misclassifications, we can observe that the models prediction avoids the error. Notice how images in the top row which shows training samples contains masks which do not fill the river in all the way to the bank, while the bottom row showing model predictions more tightly segments river boundaries.</p></div></div><div class="no"><div class="ab cb"><div class="lm pz ln qa lo qb cf qc cg qd ci bh"><figure class="nj nk nl nm nn no qf qg paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh qe"><img src="../Images/711983a57ffa0c862d13e50b50b1f44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*PMQyBeWdMXKprxYgsEzOig.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Even with imperfect training data, fine-tuning SAM can lead to impressive generalization. Notice how the predictions (bottom row) have less misclassifications and fill the river more than the training data (top row). <em class="qh">Image by author.</em></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="530c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="a7aa" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Congratulations! If you’ve made it this far you’ve learned everything you need to know to fully fine-tune Meta’s Segment Anything Model for any downstream vision task!</p><p id="e5a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While your fine-tuning workflow will without a doubt differ from the implementation presented in this tutorial, the knowledge gained from reading this will transfer not only to your segmentation project, but to future your deep learning projects and beyond.</p><p id="94c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Keep exploring the world of machine learning, stay curious, and as always, happy coding!</p><h1 id="37f5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Appendix</h1><p id="61d1" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">The dataset used in this example is the <a class="af nf" href="https://huggingface.co/datasets/stodoran/elwha-segmentation-v1" rel="noopener ugc nofollow" target="_blank">Elwha V1 dataset</a>, created by the <a class="af nf" href="https://geo-smart.github.io/" rel="noopener ugc nofollow" target="_blank">GeoSMART research lab</a> from the University of Washington for a research project on the application of fine-tuned large vision transformers to geospatial segmentation tasks. The tutorial in this article represents a condensed and more approachable version of the forthcoming paper. At a high level, the Elwha V1 dataset consists of postprocessed model predictions from a SAM checkpoint fine-tuned using a subset of the labeled orthoimagery published by <a class="af nf" href="https://zenodo.org/records/10155783" rel="noopener ugc nofollow" target="_blank">Buscombe et al.</a> and released on Zenodo.</p></div></div></div></div>    
</body>
</html>