- en: 'AutoRound: Accurate Low-bit Quantization for LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/autoround-accurate-low-bit-quantization-for-llms-305ddb38527a?source=collection_archive---------0-----------------------#2024-06-29](https://towardsdatascience.com/autoround-accurate-low-bit-quantization-for-llms-305ddb38527a?source=collection_archive---------0-----------------------#2024-06-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Between quantization-aware training and post-training quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--305ddb38527a--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--305ddb38527a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--305ddb38527a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--305ddb38527a--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--305ddb38527a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--305ddb38527a--------------------------------)
    ·7 min read·Jun 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72daa1eec27a277be9f6d145f7e5d5d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: There are many quantization methods to reduce the size of large language models
    (LLM). Recently, better low-bit quantization methods have been proposed. For instance,
    AQLM achieves 2-bit quantization while preserving most of the model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The main drawback of [AQLM](https://medium.com/towards-artificial-intelligence/fine-tune-mixtral-8x7b-quantized-with-aqlm-2-bit-on-your-gpu-4f8fac86e523)
    is that the quantization of large models is extremely costly. [HQQ](https://github.com/mobiusml/hqq)
    is another good alternative for low-bit quantization but requires further fine-tuning
    to preserve accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Intel is also very active in the research of better quantization algorithms.
    They propose AutoRound, a new quantization method adopting sign gradient descent
    (SignSD). AutoRound is especially accurate for low-bit quantization and quantizes
    faster than most other methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I review AutoRound. We will see how it works and how to quantize
    LLMs, such as Llama 3, with minimal accuracy drop. I found AutoRound to be a very
    good alternative to GPTQ and HQQ. It yields more accurate models.
  prefs: []
  type: TYPE_NORMAL
- en: 'I implemented the following notebook showing how to quantize LLMs with AutoRound,
    and evaluate/benchmark the resulting models:'
  prefs: []
  type: TYPE_NORMAL
