<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Principal Component Analysis Made Easy: A Step-by-Step Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Principal Component Analysis Made Easy: A Step-by-Step Tutorial</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08">https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5ec9" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implement the PCA algorithm from scratch with Python</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Marcus Sena" class="l ep by dd de cx" src="../Images/ff594ec7029e6259f0be6dc031d8a6cd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zzBM0cL2XAVKlFFS6ZSnhQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------" rel="noopener follow">Marcus Sena</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/2af52467e31c0771ef30de908239e82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LotvYEc-1feR18vs"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@lunarts?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Volodymyr Hryshchenko</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0a6f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Plenty of well-established Python packages (like scikit-learn) implement Machine Learning algorithms such as the Principal Component Analysis (PCA) algorithm. So, why bother learning how the algorithms work under the hood?</p><p id="d1b6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A deep understanding of the underlying mathematical concepts is crucial for making better decisions based on the algorithm’s output and avoiding treating the algorithm as a “black box”.</p><p id="c0bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this article, I show the intuition of the inner workings of the PCA algorithm, covering key concepts such as <em class="nz">Dimensionality Reduction</em>, <em class="nz">eigenvectors</em>, and <em class="nz">eigenvalues</em>, then we’ll implement a Python class to encapsulate these concepts and perform PCA analysis on a dataset.</p><p id="2f34" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Whether you are a machine learning beginner trying to build a solid understanding of the concepts or a practitioner interested in creating custom machine learning applications and need to understand how the algorithms work under the hood, that article is for you.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="oi oj ok"><p id="30d0" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Table of Contents</p><p id="03c1" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="#ada8" rel="noopener ugc nofollow">1. Dimensionality Reduction</a><br/><a class="af nc" href="#607e" rel="noopener ugc nofollow">2. How Does Principal Component Analysis Work?</a><br/><a class="af nc" href="#f118" rel="noopener ugc nofollow">3. Implementation in Python</a><br/><a class="af nc" href="#77b2" rel="noopener ugc nofollow">4. Evaluation and Interpretation</a><br/><a class="af nc" href="#3fda" rel="noopener ugc nofollow">5. Conclusions and Next Steps</a></p></blockquote><h1 id="ada8" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">1. Dimensionality Reduction</h1><p id="4b23" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Many real problems in machine learning involve datasets with thousands or even millions of features. Training such datasets can be computationally demanding, and interpreting the resulting solutions can be even more challenging.</p><p id="c33b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As the number of features increases, the data points become more sparse, and distance metrics become less informative, since the distances between points are less pronounced making it difficult to distinguish what are close and distant points. That is known as the <em class="nz">curse of dimensionality</em>.</p><p id="93db" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The more sparse data makes models harder to train and more prone to overfitting capturing noise rather than the underlying patterns. This leads to poor generalization to new, unseen data.</p><p id="9a1b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Dimensionality reduction is used in data science and machine learning to reduce the number of variables or features in a dataset while retaining as much of the original information as possible. This technique is useful for simplifying complex datasets, improving computational efficiency, and helping with data visualization.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pm"><img src="../Images/a55aff221424d9a010c30fecaf37ecfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aC23y-toUC8GCMw6Oz_hfg.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by the author using DALL-E.</figcaption></figure><h1 id="607e" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">2. How Does Principal Component Analysis Work?</h1><p id="9528" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">One of the most used techniques to mitigate the curse of dimensionality is Principal Component Analysis (PCA). The PCA reduces the number of features in a dataset while keeping most of the useful information by finding the axes that account for the largest variance in the dataset. Those axes are called the <em class="nz">principal components</em>.</p><p id="464d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since PCA aims to find a low-dimensional representation of a dataset while keeping a great share of the variance instead of performing predictions, It is considered an <strong class="nf fr">unsupervised learning</strong> algorithm.</p><p id="f69c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But why does keeping the variance mean keeping important information?</p><blockquote class="pn"><p id="ebdd" class="po pp fq bf pq pr ps pt pu pv pw ny dx">Imagine you are analyzing a dataset about crimes in a city. The data have numerous features including "crime against person - with injuries" and “crime against person — without injuries”. Certainly, the places with high rates of the first example must also have high rates of the second example.</p></blockquote><p id="f6f0" class="pw-post-body-paragraph nd ne fq nf b go px nh ni gr py nk nl nm pz no np nq qa ns nt nu qb nw nx ny fj bk">In other words, the two features of the example are very correlated, so it is possible to reduce the dimensions of that dataset by diminishing the redundancies in the data (the presence or absence of injuries in the victim).</p><p id="ac3f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The PCA algorithm is nothing more than a sophisticated way of doing that.</p><p id="eaa9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let's break down how the PCA algorithm works under the hood in the following steps:</p><h2 id="07d8" class="qc om fq bf on qd qe qf oq qg qh qi ot nm qj qk ql nq qm qn qo nu qp qq qr qs bk"><strong class="al">Step 1: Centering the data</strong></h2><p id="28d2" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">PCA is affected by the scale of the data, so the first thing to do is to subtract the mean of each feature of the dataset, thus ensuring that all the features have a mean equal to 0.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/03ebbfb66492626ae7477637a2b7ddce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYVdBwmy_If-j1IZMCkFUg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Data before and after centering (image by the author).</figcaption></figure><h2 id="b085" class="qc om fq bf on qd qe qf oq qg qh qi ot nm qj qk ql nq qm qn qo nu qp qq qr qs bk"><strong class="al">Step 2: Calculating the covariance matrix</strong></h2><p id="eee2" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Now, we have to calculate the covariance matrix to capture how each pair of features in the data varies together. If the dataset has <em class="nz">n </em>features, the resulting covariance matrix will have <em class="nz">n </em>x <em class="nz">n </em>shape.</p><p id="cc16" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the image below, features more correlated have colors closer to red. Of course, each feature will be highly correlated with itself.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/fe5fe850cd277fae728a68121c86df55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WO2uYodVAUSMqLkpnoL4pQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Heatmap of the covariance matrix (image by the author).</figcaption></figure><h2 id="3d19" class="qc om fq bf on qd qe qf oq qg qh qi ot nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Step 3: Eigenvalue Decomposition</h2><p id="e1d1" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Next, we have to perform the eigenvalue decomposition of the covariance matrix. In case you don't remember, given the covariance matrix Σ (a square matrix), eigenvalue decomposition is the process of finding a set of scalars (eigenvalues) and vectors (eigenvectors) such that:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/643ea25ef8e21f3abac05bc9a3bf2719.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*6I99y0m8KlyAVL8L6t3OgQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Eigenvalue property (image by the author using <a class="af nc" href="https://editor.codecogs.com/" rel="noopener ugc nofollow" target="_blank">codecogs</a>).</figcaption></figure><p id="57ee" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where:</p><ul class=""><li id="5cbd" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk">Σ is the n×n covariance matrix.</li><li id="1b9a" class="nd ne fq nf b go qz nh ni gr ra nk nl nm rb no np nq rc ns nt nu rd nw nx ny qw qx qy bk"><em class="nz">v</em> is a non-zero vector called the eigenvector.</li><li id="385c" class="nd ne fq nf b go qz nh ni gr ra nk nl nm rb no np nq rc ns nt nu rd nw nx ny qw qx qy bk"><em class="nz">λ</em> is a scalar called the eigenvalue associated with the eigenvector <em class="nz">v</em>.</li></ul><blockquote class="oi oj ok"><p id="a5af" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Eigenvectors</strong> indicate the directions of maximum variance in the data (the principal components), while <strong class="nf fr">eigenvalues</strong> quantify the variance captured by each principal component.</p></blockquote><p id="d32c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If a matrix <strong class="nf fr">A</strong> can be decomposed into eigenvalues and eigenvectors, it can be represented as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk re"><img src="../Images/d5a67b6aea9048c7daf0a0fa1d0f57d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*lte66r62UT_jPBZeImh1kA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Eigendecomposition of a matrix (image by the author using <a class="af nc" href="https://editor.codecogs.com/" rel="noopener ugc nofollow" target="_blank">codecogs</a>).</figcaption></figure><p id="b53f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where:</p><ul class=""><li id="c35b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk"><strong class="nf fr">Q</strong> is a matrix whose columns are the eigenvectors of <strong class="nf fr">A</strong>.</li><li id="42d1" class="nd ne fq nf b go qz nh ni gr ra nk nl nm rb no np nq rc ns nt nu rd nw nx ny qw qx qy bk">Λ is a diagonal matrix whose diagonal elements are the eigenvalues of <strong class="nf fr">A</strong>.</li></ul><p id="b815" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That way, we can use the same steps to find the eigenvalues and eigenvectors of the covariance matrix.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/1f6d776527b32054686c907ec2ae7832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyYQh0K8ZN5cejPPQia6rw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Plotting eigenvectors (image by the author).</figcaption></figure><p id="4853" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the image above, we can see that the first eigenvector points to the direction with the most variance of the data, and the second eigenvector points to the direction with the second most variance.</p><h2 id="e443" class="qc om fq bf on qd qe qf oq qg qh qi ot nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Step 4: Selecting the Principal Components</h2><p id="07ee" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">As said earlier, the eigenvalues quantify the data's variance in the direction of its corresponding eigenvector. Thus, we sort the eigenvalues in descending order and keep only the top n required <em class="nz">principal components</em>.</p><p id="32fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The image below illustrates the proportion of variance captured by each principal component in a PCA with two dimensions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/ff80b7c8901b71e9f7f0e8f52fe46c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBpj9BvjMe1FkMzSi-NIMw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Explained variance for 2 principal components (image by the author).</figcaption></figure><h2 id="1314" class="qc om fq bf on qd qe qf oq qg qh qi ot nm qj qk ql nq qm qn qo nu qp qq qr qs bk">Step 5: Project the Data</h2><p id="0b40" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Finally, we have to project the original data onto the dimensions represented by the selected principal components. To do that, we have to multiply the dataset, after being centered, by the matrix of eigenvectors found in the decomposition of the covariance matrix.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rg"><img src="../Images/b56cd9f76d4c4101644a6e56a0cffcc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*2FwRNoKT1lCFLp2TZqbt9A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Projecting the original dataset to n dimensions (image by the author using <a class="af nc" href="https://editor.codecogs.com/" rel="noopener ugc nofollow" target="_blank">codecogs</a>).</figcaption></figure><h1 id="f118" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">3. Implementation in Python</h1><p id="2ab9" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Now that we deeply understand the key concepts of Principal Component Analysis, it's time to create some code.</p><p id="fe93" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, we have to set the environment importing the numpy package for mathematical calculations and matplotlib for visualization:</p><pre class="mm mn mo mp mq rh ri rj bp rk bb bk"><span id="b130" class="rl om fq ri b bg rm rn l ro rp">import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="6800" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, we will encapsulate all the concepts covered in the previous section in a Python class with the following methods:</p><ul class=""><li id="94a2" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk"><strong class="nf fr">init method</strong></li></ul><p id="6650" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Constructor method to initialize the algorithm's parameters: the number of components desired, a matrix to store the components vectors, and an array to store the explained variance of each selected dimension.</p><ul class=""><li id="8a44" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk"><strong class="nf fr">fit method</strong></li></ul><p id="e103" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the fit method, the first four steps presented in the previous section are implemented with code. Also, the explained variances of each component are calculated.</p><ul class=""><li id="596a" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk"><strong class="nf fr">Transform method</strong></li></ul><p id="0ad4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The transform method performs the last step presented in the previous section: project the data onto the selected dimensions.</p><ul class=""><li id="a79f" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk"><strong class="nf fr">Plot explained variance</strong></li></ul><p id="0c6a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The last method is a helper function to plot the explained variance of each selected principal component as a bar plot.</p><p id="6bf5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the full code:</p><pre class="mm mn mo mp mq rh ri rj bp rk bb bk"><span id="95f5" class="rl om fq ri b bg rm rn l ro rp">class PCA:<br/>    def __init__(self, n_components):<br/>        self.n_components = n_components<br/>        self.components = None<br/>        self.mean = None<br/>        self.explained_variance = None<br/>    <br/>    def fit(self, X):<br/>        # Step 1: Standardize the data (subtract the mean)<br/>        self.mean = np.mean(X, axis=0)<br/>        X_centered = X - self.mean<br/><br/>        # Step 2: Compute the covariance matrix<br/>        cov_matrix = np.cov(X_centered, rowvar=False)<br/><br/>        # Step 3: Compute the eigenvalues and eigenvectors<br/>        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)<br/><br/>        # Step 4: Sort the eigenvalues and corresponding eigenvectors<br/>        sorted_indices = np.argsort(eigenvalues)[::-1]<br/>        eigenvalues = eigenvalues[sorted_indices]<br/>        eigenvectors = eigenvectors[:, sorted_indices]<br/><br/>        # Step 5: Select the top n_components<br/>        self.components = eigenvectors[:, :self.n_components]<br/><br/>        # Calculate explained variance<br/>        total_variance = np.sum(eigenvalues)<br/>        self.explained_variance = eigenvalues[:self.n_components] / total_variance<br/><br/>    def transform(self, X):<br/>        # Step 6: Project the data onto the selected components<br/>        X_centered = X - self.mean<br/>        return np.dot(X_centered, self.components)<br/><br/>    def plot_explained_variance(self):<br/>        # Create labels for each principal component<br/>        labels = [f'PCA{i+1}' for i in range(self.n_components)]<br/><br/>        # Create a bar plot for explained variance<br/>        plt.figure(figsize=(8, 6))<br/>        plt.bar(range(1, self.n_components + 1), self.explained_variance, alpha=0.7, align='center', color='blue', tick_label=labels)<br/>        plt.xlabel('Principal Component')<br/>        plt.ylabel('Explained Variance Ratio')<br/>        plt.title('Explained Variance by Principal Components')<br/>        plt.show()</span></pre><h1 id="77b2" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">4. Evaluation and interpretation</h1><p id="2112" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Now it's time to use the class we just implemented on a simulated dataset created using the numpy package. The dataset has 10 features and 100 samples.</p><pre class="mm mn mo mp mq rh ri rj bp rk bb bk"><span id="364e" class="rl om fq ri b bg rm rn l ro rp"># create simulated data for analysis<br/>np.random.seed(42)<br/># Generate a low-dimensional signal<br/>low_dim_data = np.random.randn(100, 4)<br/><br/># Create a random projection matrix to project into higher dimensions<br/>projection_matrix = np.random.randn(4, 10)<br/><br/># Project the low-dimensional data to higher dimensions<br/>high_dim_data = np.dot(low_dim_data, projection_matrix)<br/><br/># Add some noise to the high-dimensional data<br/>noise = np.random.normal(loc=0, scale=0.5, size=(100, 10))<br/>data_with_noise = high_dim_data + noise<br/>    <br/>X = data_with_noise</span></pre><p id="df9c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Before performing the PCA, one question remains: <strong class="nf fr">how do we choose the correct or optimal number of dimensions</strong>? Generally, we have to look for the number of components that add up to at least 95% of the explained variance of the dataset.</p><p id="2c4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To do that, let's take a look at how each principal component contributes to the total variance of the dataset:</p><pre class="mm mn mo mp mq rh ri rj bp rk bb bk"><span id="f220" class="rl om fq ri b bg rm rn l ro rp"># Apply PCA<br/>pca = PCA(n_components=10)<br/>pca.fit(X)<br/>X_transformed = pca.transform(X)<br/><br/>print("Explained Variance:\n", pca.explained_variance)<br/><br/>&gt;&gt; Explained Variance (%):<br/> [55.406, 25.223, 11.137, 5.298, 0.641, 0.626, 0.511, 0.441, 0.401, 0.317]</span></pre><p id="c284" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, let's plot the cumulative sum of the variance and check at which number of dimensions we achieve the optimal value of 95% of the total variance.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/8de65b184309833b30aaaa75eac9fde5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzz5RMVNPQ2NqITgKGYeGQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Explained variance as a function of the number of components (image by the author).</figcaption></figure><p id="593c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As shown in the graph above, the optimal number of dimensions for the dataset is 4, totaling 97.064% of the explained variance. In other words, we transformed a dataset with 10 features into one with only 3 dimensions while keeping more than 97% of the original information.</p><blockquote class="pn"><p id="c421" class="po pp fq bf pq pr ps pt pu pv pw ny dx">That means that most of the original 10 features were very correlated and the algorithm transformed that high-dimensional data into uncorrelated principal components.</p></blockquote><h1 id="3fda" class="ol om fq bf on oo op gq oq or os gt ot ou rq ow ox oy rr pa pb pc rs pe pf pg bk">5. Conclusions and Next Steps</h1><p id="cd90" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">We created a PCA class using only the numpy package that successfully reduced the dimensionality of a dataset from 10 features to just 4 while preserving approximately 97% of the data’s variance.</p><p id="d851" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Also, we explored a method to obtain an optimal number of principal components of the PCA analysis that can be customized depending on the problem we are facing (we may be interested in retaining only 90% of the variance, for example).</p><p id="9731" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That shows the potential of the PCA analysis to deal with the curse of dimensionality explained earlier. Additionally, I'd like to leave a few points for further exploration:</p><ul class=""><li id="fa8b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qw qx qy bk">Perform classification or regression tasks using other machine learning algorithms on the reduced dataset using the PCA algorithm and compare the performance of models trained on the original dataset versus the PCA-transformed dataset to assess the impact of dimensionality reduction.</li><li id="9e5f" class="nd ne fq nf b go qz nh ni gr ra nk nl nm rb no np nq rc ns nt nu rd nw nx ny qw qx qy bk">Use PCA for data visualization to make high-dimensional data more interpretable and to uncover patterns that were not evident in the original feature space.</li><li id="c493" class="nd ne fq nf b go qz nh ni gr ra nk nl nm rb no np nq rc ns nt nu rd nw nx ny qw qx qy bk">Consider exploring other dimensionality reduction techniques, such as <em class="nz">t-Distributed Stochastic Neighbor Embedding </em>(t-SNE) and <em class="nz">Linear Discriminant Analysis </em>(LDA).</li></ul><p id="fc3c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Complete code available <a class="af nc" href="https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA" rel="noopener ugc nofollow" target="_blank">here</a>.</p><div class="rt ru rv rw rx ry"><a href="https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="rz ab ig"><div class="sa ab co cb sb sc"><h2 class="bf fr hw z io sd iq ir se it iv fp bk">ML-and-Ai-from-scratch/PCA at main · Marcussena/ML-and-Ai-from-scratch</h2><div class="sf l"><h3 class="bf b hw z io sd iq ir se it iv dx">Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/PCA at main ·…</h3></div><div class="sg l"><p class="bf b dy z io sd iq ir se it iv dx">github.com</p></div></div><div class="sh l"><div class="si l sj sk sl sh sm lr ry"/></div></div></a></div></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8502" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Please feel free to use and improve the code, comment, make suggestions, and connect with me on <a class="af nc" href="https://www.linkedin.com/in/marcus-sena-660198150/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>, <a class="af nc" href="https://twitter.com/MarcusMVLS" rel="noopener ugc nofollow" target="_blank">X</a>, and <a class="af nc" href="https://github.com/Marcussena/ML-and-Ai-from-scratch" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><h1 id="cc61" class="ol om fq bf on oo op gq oq or os gt ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">References</h1><p id="0ad6" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">[1] Willmott, Paul. (2019). <em class="nz">Machine Learning: An Applied Mathematics Introduction</em>. Panda Ohana Publishing.</p><p id="4f13" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Géron, A. (2017). <em class="nz">Hands-On Machine Learning</em>. O’Reilly Media Inc.</p><p id="155f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Grus, Joel. (2015). <em class="nz">Data Science from Scratch</em>. O’Reilly Media Inc.</p><p id="29be" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Datagy.io. <em class="nz">How to Perform PCA in Python</em>. Retrieved June 2, 2024, from <a class="af nc" href="https://datagy.io/python-pca/" rel="noopener ugc nofollow" target="_blank">https://datagy.io/python-pca/</a>.</p></div></div></div></div>    
</body>
</html>