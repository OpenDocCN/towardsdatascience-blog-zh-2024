- en: 'Spicing up Ice Hockey with AI: Player Tracking with Computer Vision'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”¨AIä¸ºå†°çƒå¢æ·»è¶£å‘³ï¼šåˆ©ç”¨è®¡ç®—æœºè§†è§‰è¿›è¡Œçƒå‘˜è¿½è¸ª
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09](https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09](https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09)
- en: '![](../Images/c26f194ba6b3bcdd31b1a7dfbad44347.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c26f194ba6b3bcdd31b1a7dfbad44347.png)'
- en: Using PyTorch, computer vision techniques, and a Convolutional Neural Network
    (CNN), I worked on a model that tracks players, teams and basic performance statistics
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨PyTorchã€è®¡ç®—æœºè§†è§‰æŠ€æœ¯å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæˆ‘å¼€å‘äº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿½è¸ªçƒå‘˜ã€çƒé˜Ÿä»¥åŠåŸºæœ¬çš„è¡¨ç°ç»Ÿè®¡æ•°æ®
- en: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    Â·30 min readÂ·Jul 9, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    Â·30åˆ†é’Ÿé˜…è¯»Â·2024å¹´7æœˆ9æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Nowadays, I donâ€™t play hockey as much as I want to, but itâ€™s been a part of
    me since I was a kid. Recently, I had the chance to help with the referee table
    and keep some stats in the first Ice Hockey Tournament in Lima (3 on 3). This
    event involved an extraordinary effort of the the Peruvian Inline Hockey Association
    (APHL) and a kind visit from the [Friendship League](https://friendshipleague.org/).
    To add an AI twist, I used **PyTorch**, **computer vision** techniques, and a
    **Convolutional Neural Network (CNN)** to build a model that tracks players and
    teams and gathers some basic performance stats.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä¸åƒä»¥å‰é‚£æ ·é¢‘ç¹åœ°æ‰“å†°çƒäº†ï¼Œä½†ä»å°å†°çƒå°±æ˜¯æˆ‘ç”Ÿæ´»çš„ä¸€éƒ¨åˆ†ã€‚æœ€è¿‘ï¼Œæˆ‘æœ‰æœºä¼šåœ¨åˆ©é©¬ä¸¾åŠçš„é¦–å±Šå†°çƒé”¦æ ‡èµ›ï¼ˆ3å¯¹3ï¼‰ä¸­ï¼ŒååŠ©è£åˆ¤å°å¹¶è®°å½•ä¸€äº›ç»Ÿè®¡æ•°æ®ã€‚æ­¤æ¬¡æ´»åŠ¨å¾—åˆ°äº†ç§˜é²æ»‘å†°æ›²æ£çƒåä¼šï¼ˆAPHLï¼‰çš„å·¨å¤§æ”¯æŒï¼Œå¹¶ä¸”[å‹è°Šè”ç›Ÿ](https://friendshipleague.org/)ä¹Ÿäº²åˆ‡åœ°å‚ä¸å…¶ä¸­ã€‚ä¸ºäº†åŠ å…¥ä¸€äº›AIå…ƒç´ ï¼Œæˆ‘ä½¿ç”¨äº†**PyTorch**ã€**è®¡ç®—æœºè§†è§‰**æŠ€æœ¯å’Œ**å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰**æ¥æ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¿½è¸ªçƒå‘˜å’Œçƒé˜Ÿï¼Œå¹¶æ”¶é›†ä¸€äº›åŸºæœ¬çš„è¡¨ç°æ•°æ®ã€‚
- en: This article aims to be a **quick guide** to designing and deploying the model.
    Although the model still needs some fine-tuning, I hope it can help anyone introduce
    themselves to the interesting world of computer vision applied to sports. I would
    like to acknowledge and thank the [Peruvian Inline Hockey Association (APHL)](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    for allowing me to use a 40-second video sample of the tournament for this project
    *(you can find the video input sample in the* [*projectâ€™s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*).*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ—¨åœ¨æˆä¸ºè®¾è®¡å’Œéƒ¨ç½²è¯¥æ¨¡å‹çš„**å¿«é€ŸæŒ‡å—**ã€‚å°½ç®¡æ¨¡å‹ä»éœ€è¿›è¡Œä¸€äº›å¾®è°ƒï¼Œä½†æˆ‘å¸Œæœ›å®ƒèƒ½å¸®åŠ©ä»»ä½•äººå…¥é—¨è®¡ç®—æœºè§†è§‰åœ¨ä½“è‚²ä¸­çš„åº”ç”¨ã€‚æˆ‘è¿˜è¦ç‰¹åˆ«æ„Ÿè°¢å¹¶æ„Ÿè°¢[ç§˜é²æ»‘å†°æ›²æ£çƒåä¼šï¼ˆAPHLï¼‰](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)å…è®¸æˆ‘ä½¿ç”¨æ¯”èµ›çš„40ç§’è§†é¢‘æ ·æœ¬è¿›è¡Œæ­¤é¡¹ç›®ï¼ˆ*ä½ å¯ä»¥åœ¨*[*é¡¹ç›®çš„GitHubä»“åº“*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*æ‰¾åˆ°è§†é¢‘è¾“å…¥æ ·æœ¬*ï¼‰ã€‚
- en: The Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¶æ„
- en: Before moving on with the project, I did some quick research to find a baseline
    from which I could work and avoid â€œreinventing the wheelâ€. I found that in terms
    of using computer vision to track players, there is a lot of interesting work
    on football *(not surprising, being the most popular team sport in the world)*.
    However, I didnâ€™t find many resources for ice hockey. [Roboflow](https://universe.roboflow.com/search?q=hockey)
    has some interesting pre-trained models and datasets for training your own, but
    working with a hosted model presented some latency issues that I will explain
    further. In the end, I leveraged the soccer material for reading the video frames
    and obtaining the individual track IDs, following the basic principles and tracking
    method approach explained in [this tutorial](https://youtu.be/neBZ6huolkg?feature=shared)
    *(If you are interested in gaining a better understanding of some basic computer
    vision techniques, I suggest watching at least the first hour and a half of the
    tutorial).*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç»§ç»­è¿›è¡Œé¡¹ç›®ä¹‹å‰ï¼Œæˆ‘åšäº†ä¸€äº›å¿«é€Ÿç ”ç©¶ï¼Œæ‰¾åˆ°ä¸€ä¸ªå¯ä»¥ä½œä¸ºåŸºå‡†çš„æ–¹å‘ï¼Œä»¥é¿å…â€œé‡æ–°å‘æ˜è½®å­â€ã€‚æˆ‘å‘ç°ï¼Œåœ¨ä½¿ç”¨è®¡ç®—æœºè§†è§‰è¿½è¸ªçƒå‘˜æ–¹é¢ï¼Œæœ‰å¾ˆå¤šæœ‰è¶£çš„ç ”ç©¶æˆæœï¼Œå°¤å…¶æ˜¯åœ¨è¶³çƒé¢†åŸŸï¼ˆ*è¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œè¶³çƒæ˜¯ä¸–ç•Œä¸Šæœ€å—æ¬¢è¿çš„å›¢é˜Ÿè¿åŠ¨*ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘å¹¶æ²¡æœ‰æ‰¾åˆ°å¾ˆå¤šå†°çƒæ–¹é¢çš„èµ„æºã€‚[Roboflow](https://universe.roboflow.com/search?q=hockey)
    æä¾›äº†ä¸€äº›æœ‰è¶£çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œä½†ä½¿ç”¨æ‰˜ç®¡æ¨¡å‹æ—¶å‡ºç°äº†ä¸€äº›å»¶è¿Ÿé—®é¢˜ï¼Œç¨åæˆ‘ä¼šè¿›ä¸€æ­¥è§£é‡Šã€‚æœ€ç»ˆï¼Œæˆ‘å€Ÿç”¨äº†è¶³çƒç›¸å…³çš„èµ„æ–™æ¥è¯»å–è§†é¢‘å¸§å¹¶è·å¾—å•ç‹¬çš„è·Ÿè¸ª
    IDï¼Œéµå¾ªäº†[è¿™ä¸ªæ•™ç¨‹](https://youtu.be/neBZ6huolkg?feature=shared)ä¸­è§£é‡Šçš„åŸºæœ¬åŸç†å’Œè·Ÿè¸ªæ–¹æ³•ï¼ˆ*å¦‚æœä½ æœ‰å…´è¶£æ›´å¥½åœ°ç†è§£ä¸€äº›åŸºæœ¬çš„è®¡ç®—æœºè§†è§‰æŠ€æœ¯ï¼Œæˆ‘å»ºè®®è‡³å°‘è§‚çœ‹å‰ä¸€ä¸ªåŠå°æ—¶çš„æ•™ç¨‹*ï¼‰ã€‚
- en: With the tracking IDs covered, I then built my own path. As we walk through
    this article, weâ€™ll see how the project evolves from a simple object detection
    task to a model that fully detects players, teams, and delivers some basic performance
    metrics *(sample clips from 01 to 08, authorâ€™s own creation).*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¦†ç›–äº†è·Ÿè¸ª ID åï¼Œæˆ‘å¼€å§‹æ„å»ºè‡ªå·±çš„è·¯å¾„ã€‚åœ¨æœ¬æ–‡çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸ªé¡¹ç›®æ˜¯å¦‚ä½•ä»ä¸€ä¸ªç®€å•çš„ç‰©ä½“æ£€æµ‹ä»»åŠ¡å‘å±•æˆä¸€ä¸ªèƒ½å¤Ÿå…¨é¢æ£€æµ‹çƒå‘˜ã€çƒé˜Ÿå¹¶æä¾›ä¸€äº›åŸºæœ¬è¡¨ç°æŒ‡æ ‡çš„æ¨¡å‹çš„ï¼ˆ*ç¤ºä¾‹å‰ªè¾‘ä»
    01 åˆ° 08ï¼Œä½œè€…è‡ªåˆ›*ï¼‰ã€‚
- en: '![](../Images/e100ca9ea1c1eb7ab22675a9de0f18ef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e100ca9ea1c1eb7ab22675a9de0f18ef.png)'
- en: Model Architecture. Authorâ€™s own creation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¶æ„ã€‚ä½œè€…è‡ªåˆ›
- en: The Tracking Mechanism
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªæœºåˆ¶
- en: 'The tracking mechanism is the backbone of the model. It ensures that each detected
    object within the video is identified and assigned a unique identifier, maintaining
    this identity across each frame. The main components of the tracking mechanism
    are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªæœºåˆ¶æ˜¯æ¨¡å‹çš„æ ¸å¿ƒã€‚å®ƒç¡®ä¿è§†é¢‘ä¸­çš„æ¯ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“éƒ½èƒ½è¢«è¯†åˆ«å¹¶åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼Œä¿æŒè¯¥èº«ä»½åœ¨æ¯ä¸€å¸§ä¸­çš„ä¸€è‡´æ€§ã€‚è·Ÿè¸ªæœºåˆ¶çš„ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬ï¼š
- en: '**YOLO (You Only Look Once):** Itâ€™s a powerful real-time object detection algorithm
    originally introduced in 2015 in the paper â€œ[You Only Look Once: Unified, Real-Time
    Object Detection](https://arxiv.org/abs/1506.02640)â€. Stands out for its speed
    and its versatility in detecting around 80 pre-trained classes *(itâ€™s important
    to note that it can also be trained on custom datasets to detect specific objects)*.
    For our use case, we will rely on YOLOv8x, a computer vision model built by Ultralytics
    based on previous YOLO versions. You can download it [here](https://github.com/ultralytics/ultralytics).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**YOLOï¼ˆYou Only Look Onceï¼‰ï¼š** å®ƒæ˜¯ä¸€ç§å¼ºå¤§çš„å®æ—¶ç‰©ä½“æ£€æµ‹ç®—æ³•ï¼Œæœ€åˆåœ¨ 2015 å¹´çš„è®ºæ–‡â€œ[You Only Look
    Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)â€ä¸­æå‡ºã€‚å®ƒä»¥é€Ÿåº¦å’Œåœ¨å¤§çº¦
    80 ä¸ªé¢„è®­ç»ƒç±»åˆ«ä¸­çš„é€šç”¨æ€§ä¸ºç‰¹ç‚¹ï¼ˆ*å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒè¿˜å¯ä»¥åœ¨è‡ªå®šä¹‰æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥æ£€æµ‹ç‰¹å®šç‰©ä½“*ï¼‰ã€‚å¯¹äºæˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å°†ä¾èµ– YOLOv8xï¼Œè¿™æ˜¯ä¸€ç§ç”±
    Ultralytics åŸºäºä¹‹å‰ç‰ˆæœ¬çš„ YOLO æ„å»ºçš„è®¡ç®—æœºè§†è§‰æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/ultralytics/ultralytics)ä¸‹è½½å®ƒã€‚'
- en: '**ByteTrack Tracker:** To understand ByteTrack, we have to understand MOT (Multiple
    Object Tracking), which involves tracking the movements of multiple objects over
    time in a video sequence and linking those objects detected in a current frame
    with corresponding objects in previous frames. To accomplish this, we will use
    ByteTrack *( introduced in 2021 in the paper â€œ*[*ByteTrack: Multi-Object Tracking
    by Associating Every Detection Box*](https://arxiv.org/abs/2110.06864)*â€)*. To
    implement the ByteTrack tracker and assign track IDs to detected objects, we will
    rely on the Pythonâ€™s supervision library.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ByteTrack è·Ÿè¸ªå™¨ï¼š** è¦ç†è§£ ByteTrackï¼Œæˆ‘ä»¬å¿…é¡»å…ˆäº†è§£å¤šç›®æ ‡è·Ÿè¸ªï¼ˆMOTï¼ŒMultiple Object Trackingï¼‰ï¼Œå®ƒæ¶‰åŠåœ¨è§†é¢‘åºåˆ—ä¸­è¿½è¸ªå¤šä¸ªç‰©ä½“çš„è¿åŠ¨ï¼Œå¹¶å°†å½“å‰å¸§ä¸­æ£€æµ‹åˆ°çš„ç‰©ä½“ä¸å‰ä¸€å¸§ä¸­çš„ç›¸åº”ç‰©ä½“è¿›è¡Œå…³è”ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨
    ByteTrackï¼ˆ*2021å¹´åœ¨è®ºæ–‡â€œ[*ByteTrack: Multi-Object Tracking by Associating Every Detection
    Box*](https://arxiv.org/abs/2110.06864)â€ä¸­æå‡º*ï¼‰ã€‚ä¸ºäº†å®ç° ByteTrack è·Ÿè¸ªå™¨å¹¶ä¸ºæ£€æµ‹åˆ°çš„ç‰©ä½“åˆ†é…è½¨è¿¹ IDï¼Œæˆ‘ä»¬å°†ä¾èµ–
    Python çš„ supervision åº“ã€‚'
- en: '**OpenCV:** is a well-known library for various computer vision tasks in Python.
    For our use case, we will rely on [OpenCV](https://opencv.org/) to visualize and
    annotate video frames with bounding boxes and text for each detected object.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**OpenCVï¼š** æ˜¯ä¸€ä¸ªå¹¿æ³›åº”ç”¨äºå„ç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„ Python åº“ã€‚å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬å°†ä¾èµ–[OpenCV](https://opencv.org/)æ¥å¯è§†åŒ–å¹¶æ ‡æ³¨è§†é¢‘å¸§ä¸­çš„è¾¹ç•Œæ¡†å’Œæ¯ä¸ªæ£€æµ‹ç‰©ä½“çš„æ–‡æœ¬ä¿¡æ¯ã€‚'
- en: 'In order to build our tracking mechanism, weâ€™ll begin with these initial two
    steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ„å»ºæˆ‘ä»¬çš„è¿½è¸ªæœºåˆ¶ï¼Œæˆ‘ä»¬å°†ä»ä»¥ä¸‹ä¸¤æ­¥å¼€å§‹ï¼š
- en: Deploying the YOLO model with ByteTrack to detect objects (in our case, players)
    and assign unique track IDs.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ByteTrack éƒ¨ç½² YOLO æ¨¡å‹æ¥æ£€æµ‹ç‰©ä½“ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯çƒå‘˜ï¼‰å¹¶åˆ†é…å”¯ä¸€çš„è¿½è¸ª IDã€‚
- en: Initializing a dictionary to store object tracks in a pickle (pkl) file. This
    will be extremely useful to avoid executing the video frame-by-frame object detection
    process each time we run the code, and save significant time.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œå°†ç‰©ä½“è¿½è¸ªä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ª pickleï¼ˆpklï¼‰æ–‡ä»¶ä¸­ã€‚è¿™å°†æå¤§åœ°å¸®åŠ©æˆ‘ä»¬é¿å…æ¯æ¬¡è¿è¡Œä»£ç æ—¶éƒ½éœ€è¦é€å¸§æ‰§è¡Œè§†é¢‘ç‰©ä½“æ£€æµ‹è¿‡ç¨‹ï¼ŒèŠ‚çœå¤§é‡æ—¶é—´ã€‚
- en: 'For the following step, these are the Python packages that weâ€™ll need:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¥ä¸‹æ¥çš„æ­¥éª¤ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹ Python åŒ…ï¼š
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, weâ€™ll specify our libraries and the path for our sample video file and
    pickle file (if it exists; if not, the code will create one and save it in the
    same path):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŒ‡å®šæˆ‘ä»¬çš„åº“å’Œæ ·æœ¬è§†é¢‘æ–‡ä»¶ä»¥åŠ pickle æ–‡ä»¶çš„è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼›å¦‚æœæ²¡æœ‰ï¼Œä»£ç ä¼šåˆ›å»ºä¸€ä¸ªå¹¶å°†å…¶ä¿å­˜åœ¨ç›¸åŒè·¯å¾„ä¸‹ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now letâ€™s go ahead and define our tracking mechanism *(you can find the video
    input sample in the* [*projectâ€™s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*)*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­å®šä¹‰æˆ‘ä»¬çš„è¿½è¸ªæœºåˆ¶*ï¼ˆä½ å¯ä»¥åœ¨[*é¡¹ç›®çš„ GitHub ä»“åº“*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)ä¸­æ‰¾åˆ°è§†é¢‘è¾“å…¥ç¤ºä¾‹ï¼‰*ï¼š
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The method begins by initializing the YOLO model and the ByteTrack tracker.
    Next, each frame is processed in batches of 20, using the YOLO model to detect
    and collect objects in each batch. If the pickle file is available in its path,
    it precomputes the tracks from the file. If the pickle file is not available *(you
    are running the code for the first time or have erased a previous pickle file)*,
    the **get_object_tracks** converts each detection into the required format for
    ByteTrack, updates the tracker with these detections, and stores the tracking
    information in a new pickle file in the designated path.Finally, iterations are
    made over each frame, drawing bounding boxes and track IDs for each detected object.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•é¦–å…ˆåˆå§‹åŒ– YOLO æ¨¡å‹å’Œ ByteTrack è¿½è¸ªå™¨ã€‚æ¥ä¸‹æ¥ï¼Œæ¯ä¸€å¸§ä»¥ 20 å¸§ä¸ºä¸€æ‰¹è¿›è¡Œå¤„ç†ï¼Œä½¿ç”¨ YOLO æ¨¡å‹æ£€æµ‹å¹¶æ”¶é›†æ¯æ‰¹ä¸­çš„ç‰©ä½“ã€‚å¦‚æœ
    pickle æ–‡ä»¶å­˜åœ¨äºè·¯å¾„ä¸­ï¼Œå®ƒä¼šä»æ–‡ä»¶ä¸­é¢„è®¡ç®—å‡ºè¿½è¸ªä¿¡æ¯ã€‚å¦‚æœ pickle æ–‡ä»¶ä¸å­˜åœ¨*ï¼ˆä½ æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œä»£ç æˆ–è€…åˆ é™¤äº†ä¹‹å‰çš„ pickle æ–‡ä»¶ï¼‰*ï¼Œ**get_object_tracks**
    å°†æ¯ä¸ªæ£€æµ‹è½¬æ¢ä¸º ByteTrack æ‰€éœ€çš„æ ¼å¼ï¼Œç”¨è¿™äº›æ£€æµ‹æ›´æ–°è¿½è¸ªå™¨ï¼Œå¹¶å°†è¿½è¸ªä¿¡æ¯ä¿å­˜åœ¨æŒ‡å®šè·¯å¾„çš„ä¸€ä¸ªæ–° pickle æ–‡ä»¶ä¸­ã€‚æœ€åï¼Œä»£ç ä¼šè¿­ä»£æ¯ä¸€å¸§ï¼Œä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“ç»˜åˆ¶è¾¹ç•Œæ¡†å’Œè¿½è¸ª
    IDã€‚
- en: 'To execute the tracker and save a new output video with bounding boxes and
    track IDs, you can use the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰§è¡Œè¿½è¸ªå™¨å¹¶ä¿å­˜å¸¦æœ‰è¾¹ç•Œæ¡†å’Œè¿½è¸ª ID çš„æ–°è¾“å‡ºè§†é¢‘ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ï¼š
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If everything in your code worked correctly, you should expect a video output
    similar to the one shown in **sample clip 01**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çš„ä»£ç ä¸€åˆ‡æ­£å¸¸ï¼Œä½ åº”è¯¥ä¼šå¾—åˆ°ä¸€ä¸ªç±»ä¼¼äº**ç¤ºä¾‹å‰ªè¾‘ 01**çš„è§†é¢‘è¾“å‡ºã€‚
- en: '![](../Images/d5caaaa073f19f9f874cb779de45aa22.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5caaaa073f19f9f874cb779de45aa22.png)'
- en: 'Sample Clip 01: Basic tracking mechanism ( Objects and Tracking IDs)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹å‰ªè¾‘ 01ï¼šåŸºç¡€è¿½è¸ªæœºåˆ¶ï¼ˆç‰©ä½“ä¸è¿½è¸ª IDï¼‰
- en: '**TIP #01:** Donâ€™t underestimate your compute power! When running the code
    for the first time, expect the frame processing to take some time, depending on
    your compute capacity. For me, it took between 45 to 50 minutes using only a CPU
    setup (consider CUDA as an option). The YOLOv8x tracking mechanism, while powerful,
    demands significant compute resources (at times, my memory hit 99%, fingers crossed
    it didnâ€™t crash!ğŸ™„). If you encounter issues with this version of YOLO, lighter
    models are available on [Ultralyticsâ€™ GitHub](https://github.com/ultralytics/ultralytics)
    to balance accuracy and compute capacity.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æç¤º #01ï¼š** ä¸è¦ä½ä¼°ä½ çš„è®¡ç®—èƒ½åŠ›ï¼ç¬¬ä¸€æ¬¡è¿è¡Œä»£ç æ—¶ï¼Œé¢„è®¡å¸§å¤„ç†å¯èƒ½ä¼šèŠ±è´¹ä¸€äº›æ—¶é—´ï¼Œè¿™å–å†³äºä½ çš„è®¡ç®—èƒ½åŠ›ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œä½¿ç”¨ CPU é…ç½®å¤„ç†å¤§çº¦éœ€è¦
    45 åˆ° 50 åˆ†é’Ÿï¼ˆå¯ä»¥è€ƒè™‘ä½¿ç”¨ CUDA ä½œä¸ºé€‰é¡¹ï¼‰ã€‚å°½ç®¡ YOLOv8x è¿½è¸ªæœºåˆ¶éå¸¸å¼ºå¤§ï¼Œä½†å®ƒéœ€è¦ç›¸å½“å¤§çš„è®¡ç®—èµ„æºï¼ˆæœ‰æ—¶æˆ‘çš„å†…å­˜å ç”¨ç‡è¾¾åˆ°äº† 99%ï¼Œå¸Œæœ›å®ƒæ²¡æœ‰å´©æºƒï¼ğŸ™„ï¼‰ã€‚å¦‚æœä½ é‡åˆ°
    YOLO çš„æ­¤ç‰ˆæœ¬é—®é¢˜ï¼Œå¯ä»¥è®¿é—®[Ultralyticsâ€™ GitHub](https://github.com/ultralytics/ultralytics)ï¼Œé‚£é‡Œæœ‰æ›´è½»é‡çš„æ¨¡å‹æ¥å¹³è¡¡å‡†ç¡®æ€§å’Œè®¡ç®—èƒ½åŠ›ã€‚'
- en: The Ice Rink
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†°çƒåœº
- en: As youâ€™ve seen from the first step, we have some challenges. Firstly, as expected,
    the model picks up all moving objects; players, referees, even those outside the
    rink. Secondly, those red bounding boxes can make tracking players a bit unclear
    and not very neat for presentation. In this section, weâ€™ll focus on narrowing
    our detection to objects within the rink only. Plus, weâ€™ll swap out those bounding
    boxes for ellipses at the bottom, ensuring clearer visibility.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ ä»ç¬¬ä¸€æ­¥ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€äº›æŒ‘æˆ˜ã€‚é¦–å…ˆï¼Œæ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ¨¡å‹æ•æ‰åˆ°æ‰€æœ‰ç§»åŠ¨ç‰©ä½“ï¼ŒåŒ…æ‹¬çƒå‘˜ã€è£åˆ¤ï¼Œç”šè‡³å†°åœºå¤–çš„ç‰©ä½“ã€‚å…¶æ¬¡ï¼Œè¿™äº›çº¢è‰²è¾¹ç•Œæ¡†ä¼šè®©è·Ÿè¸ªçƒå‘˜å˜å¾—æœ‰äº›ä¸æ¸…æ™°ï¼Œä¹Ÿä¸å¤ªé€‚åˆå±•ç¤ºã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå°†æ£€æµ‹èŒƒå›´ç¼©å°åˆ°ä»…å†°åœºå†…çš„ç‰©ä½“ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¼šå°†åº•éƒ¨çš„è¾¹ç•Œæ¡†æ›¿æ¢ä¸ºæ¤­åœ†ï¼Œä»¥ç¡®ä¿æ›´æ¸…æ™°çš„å¯è§†æ€§ã€‚
- en: 'Letâ€™s switch from using boxes to using ellipses first. To accomplish this,
    weâ€™ll simply add a new method above the labels and bounding boxes method in our
    existing code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å…ˆä»ä½¿ç”¨çŸ©å½¢æ¡†åˆ‡æ¢åˆ°ä½¿ç”¨æ¤­åœ†ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªéœ€åœ¨ç°æœ‰ä»£ç ä¸­çš„æ ‡ç­¾å’Œè¾¹ç•Œæ¡†æ–¹æ³•ä¸Šæ–¹æ·»åŠ ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼š
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Weâ€™ll also need to update the annotation step by replacing the bounding boxes
    and IDs with a call to the ellipse method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦æ›´æ–°æ³¨é‡Šæ­¥éª¤ï¼Œé€šè¿‡è°ƒç”¨æ¤­åœ†æ–¹æ³•æ¥æ›¿æ¢è¾¹ç•Œæ¡†å’ŒIDï¼š
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With these changes, your output video should look much neater, as shown in **sample
    clip 02**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™äº›ä¿®æ”¹ï¼Œä½ çš„è§†é¢‘è¾“å‡ºåº”è¯¥çœ‹èµ·æ¥æ›´æ•´æ´ï¼Œå¦‚**ç¤ºä¾‹å‰ªè¾‘02**æ‰€ç¤ºã€‚
- en: '![](../Images/0e17fb82b7df8f4b41e25e34b5da15d6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e17fb82b7df8f4b41e25e34b5da15d6.png)'
- en: 'Sample Clip 02: Replacing bounding boxes with ellipses'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹å‰ªè¾‘02ï¼šç”¨æ¤­åœ†æ›¿ä»£è¾¹ç•Œæ¡†
- en: Now, to work with the rink boundaries, we need to have some basic knowledge
    of resolution in computer vision. In our use case, we are working with a 720p
    (1280x720 pixels) format, which means that each frame or image we process has
    dimensions of 1280 pixels (width) by 720 pixels (height).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†å¤„ç†å†°åœºè¾¹ç•Œï¼Œæˆ‘ä»¬éœ€è¦å¯¹è®¡ç®—æœºè§†è§‰ä¸­çš„åˆ†è¾¨ç‡æœ‰ä¸€äº›åŸºæœ¬äº†è§£ã€‚åœ¨æˆ‘ä»¬çš„ä½¿ç”¨åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯720pï¼ˆ1280x720åƒç´ ï¼‰æ ¼å¼ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¤„ç†çš„æ¯ä¸€å¸§å›¾åƒçš„å°ºå¯¸ä¸º1280åƒç´ ï¼ˆå®½åº¦ï¼‰ä¹˜720åƒç´ ï¼ˆé«˜åº¦ï¼‰ã€‚
- en: '***What does it mean to work with a 720p (1280x720 pixels) format?*** It means
    that the image is made up of 1280 pixels horizontally and 720 pixels vertically.
    Coordinates in this format start at (0, 0) in the top-left corner of the image,
    with the x-coordinate increasing as you move right and the y-coordinate increasing
    as you move down. These coordinates are used to mark specific areas in the image,
    like using (x1, y1) for the top-left corner and (x2, y2) for the bottom-right
    corner of a box. Understanding this will helps us measure distances and speeds,
    and decide where in the video we want to focus our analysis.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä½¿ç”¨720pï¼ˆ1280x720åƒç´ ï¼‰æ ¼å¼æ„å‘³ç€ä»€ä¹ˆï¼Ÿ*** è¿™æ„å‘³ç€å›¾åƒç”±1280ä¸ªæ°´å¹³åƒç´ å’Œ720ä¸ªå‚ç›´åƒç´ ç»„æˆã€‚åœ¨è¿™ç§æ ¼å¼ä¸‹ï¼Œåæ ‡ä»å›¾åƒçš„å·¦ä¸Šè§’ï¼ˆ0,
    0ï¼‰å¼€å§‹ï¼Œxåæ ‡éšç€å‘å³ç§»åŠ¨è€Œå¢åŠ ï¼Œyåæ ‡éšç€å‘ä¸‹ç§»åŠ¨è€Œå¢åŠ ã€‚è¿™äº›åæ ‡ç”¨äºæ ‡è®°å›¾åƒä¸­çš„ç‰¹å®šåŒºåŸŸï¼Œæ¯”å¦‚ä½¿ç”¨ï¼ˆx1, y1ï¼‰è¡¨ç¤ºå·¦ä¸Šè§’ï¼Œä½¿ç”¨ï¼ˆx2, y2ï¼‰è¡¨ç¤ºçŸ©å½¢æ¡†çš„å³ä¸‹è§’ã€‚ç†è§£è¿™ä¸€ç‚¹æœ‰åŠ©äºæˆ‘ä»¬æµ‹é‡è·ç¦»å’Œé€Ÿåº¦ï¼Œå¹¶å†³å®šåœ¨è§†é¢‘ä¸­å…³æ³¨çš„åˆ†æåŒºåŸŸã€‚'
- en: 'That said, we will start marking the frame borders with green lines using the
    following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ä»£ç å¼€å§‹æ ‡è®°å¸§è¾¹æ¡†ä¸ºç»¿è‰²çº¿æ¡ï¼š
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result should be a green rectangle as shown in (a) in **sample clip 03**.
    But in order to track only the moving objects within the rink we would need a
    delimitation more similar to the one in (b) .
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªç»¿è‰²çŸ©å½¢ï¼Œå¦‚**ç¤ºä¾‹å‰ªè¾‘03**ä¸­ï¼ˆaï¼‰æ‰€ç¤ºã€‚ä½†ä¸ºäº†åªè¿½è¸ªå†°åœºå†…çš„ç§»åŠ¨ç‰©ä½“ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´åƒï¼ˆbï¼‰ä¸­çš„è¾¹ç•Œã€‚
- en: '![](../Images/e6de1323015e8fa64b0a9667df89ff1c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6de1323015e8fa64b0a9667df89ff1c.png)'
- en: 'Figure 03: Border Definition for the Ice Rink (Authorâ€™s own creation)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾03ï¼šå†°åœºè¾¹ç•Œå®šä¹‰ï¼ˆä½œè€…è‡ªåˆ›ï¼‰
- en: 'Getting (b) right is like an iterative process of trial and error, where you
    test different coordinates until you find the boundaries that best fit your model.
    Initially, I aimed to match the rink borders exactly. However, the tracking system
    struggled near the edges. To improve accuracy, I expanded the boundaries slightly
    to ensure all tracking objects within the rink were captured while excluding those
    outside. The outcome, shown in (b), was the best I could get *(you could still
    work better scenarios)* defined by these coordinates:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ°ï¼ˆbï¼‰çš„æ­£ç¡®è¾¹ç•Œå°±åƒä¸€ä¸ªåå¤è¯•éªŒçš„è¿‡ç¨‹ï¼Œä½ éœ€è¦æµ‹è¯•ä¸åŒçš„åæ ‡ï¼Œç›´åˆ°æ‰¾åˆ°æœ€é€‚åˆä½ æ¨¡å‹çš„è¾¹ç•Œã€‚æœ€åˆï¼Œæˆ‘çš„ç›®æ ‡æ˜¯å®Œå…¨åŒ¹é…å†°åœºè¾¹ç•Œã€‚ç„¶è€Œï¼Œè·Ÿè¸ªç³»ç»Ÿåœ¨è¾¹ç¼˜é™„è¿‘å­˜åœ¨å›°éš¾ã€‚ä¸ºäº†æé«˜å‡†ç¡®æ€§ï¼Œæˆ‘ç¨å¾®æ‰©å¤§äº†è¾¹ç•Œï¼Œä»¥ç¡®ä¿æ‰€æœ‰å†°åœºå†…çš„è·Ÿè¸ªç‰©ä½“éƒ½è¢«æ•æ‰åˆ°ï¼ŒåŒæ—¶æ’é™¤åœºå¤–çš„ç‰©ä½“ã€‚æœ€ç»ˆçš„ç»“æœï¼Œå¦‚ï¼ˆbï¼‰æ‰€ç¤ºï¼Œæ˜¯æˆ‘èƒ½å¾—åˆ°çš„æœ€å¥½çš„ç»“æœ*(ä½ ä»ç„¶å¯ä»¥å°è¯•æ›´å¥½çš„æƒ…å†µ)*ï¼Œç”±è¿™äº›åæ ‡å®šä¹‰ï¼š
- en: 'Bottom Left Corner: (-450, 710)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·¦ä¸‹è§’ï¼š (-450, 710)
- en: 'Bottom Right Corner: (2030, 710)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å³ä¸‹è§’ï¼š (2030, 710)
- en: 'Upper Left Corner: (352, 61)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·¦ä¸Šè§’ï¼š (352, 61)
- en: 'Upper Right Corner: (948, 61)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å³ä¸Šè§’ï¼š (948, 61)
- en: 'Finally, we will define two additional areas: the o**ffensive zones** for both
    the White and Yellow teams *(where each team aims to score)*. This will enable
    us to gather some basic positional statistics and pressure metrics for each team
    within their opponentâ€™s zone.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸¤ä¸ªé¢å¤–çš„åŒºåŸŸï¼šç™½é˜Ÿå’Œé»„é˜Ÿçš„**è¿›æ”»åŒº**ï¼ˆ*æ¯ä¸ªé˜Ÿä¼çš„ç›®æ ‡åŒºåŸŸ*ï¼‰ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¶é›†æ¯ä¸ªé˜Ÿä¼åœ¨å…¶å¯¹æ‰‹åŒºåŸŸå†…çš„ä¸€äº›åŸºæœ¬ä½ç½®ç»Ÿè®¡æ•°æ®å’Œå‹åŠ›æŒ‡æ ‡ã€‚
- en: '![](../Images/b8ea5a739ce6098b9345c2d7cfe8b332.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ea5a739ce6098b9345c2d7cfe8b332.png)'
- en: 'Figure 04: Offensive Zones ( Authorâ€™s own creation)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾04ï¼šè¿›æ”»åŒºï¼ˆä½œè€…è‡ªåˆ›ï¼‰
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will set aside these coordinates for now and explain in the next step how
    weâ€™ll classify each team. Then, weâ€™ll bring it all together into our original
    tracking method.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å…ˆæš‚æ—¶æç½®è¿™äº›åæ ‡ï¼Œå¹¶åœ¨ä¸‹ä¸€æ­¥ä¸­è§£é‡Šæˆ‘ä»¬å°†å¦‚ä½•å¯¹æ¯ä¸ªå›¢é˜Ÿè¿›è¡Œåˆ†ç±»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰å†…å®¹æ±‡æ€»åˆ°æˆ‘ä»¬æœ€åˆçš„è·Ÿè¸ªæ–¹æ³•ä¸­ã€‚
- en: Using Deep Learning for Team Prediction
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œå›¢é˜Ÿé¢„æµ‹
- en: Over 80 years have passed since the release of *â€œ*[*A Logical Calculus of the
    Ideas Immanent in Nervous Activity*](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)*â€,*
    the paper written by Warren McCulloch and Walter Pitts in 1943, which set the
    solid ground for early neural network research. Later, in 1957, the mathematical
    model of a simplified neuron *(receiving inputs, applying weights to these inputs,
    summing them up, and outputting a binary result)* inspired [Frank Rosenblatt to
    build the Mark I](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon).
    This was the first hardware implementation designed to demonstrate the concept
    of a [perceptron](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf),
    a neural network model capable of learning from data to make binary classifications.
    Since then, the quest to make computers think like us hasnâ€™t slowed down. If this
    is your first deep dive into Neural Networks, or if you want to refresh and strengthen
    your knowledge, I recommend reading this [series of articles by Shreya Rao](https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640)
    as a great starting point for deep learning. Additionally, you can access my collection
    of stories (different contributors) [that Iâ€™ve gathered here](https://medium.com/@raul.vizcarrach/list/neural-networks-098e9b594f19),
    and which you might find useful.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»1943å¹´Warren McCullochå’ŒWalter Pittså‘è¡¨äº†ã€Š*[*ç¥ç»æ´»åŠ¨ä¸­å›ºæœ‰æ€æƒ³çš„é€»è¾‘æ¼”ç®—*](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)*ã€‹ä¸€æ–‡ä»¥æ¥ï¼Œå·²ç»è¿‡å»äº†80å¤šå¹´ã€‚è¿™ç¯‡è®ºæ–‡ä¸ºæ—©æœŸçš„ç¥ç»ç½‘ç»œç ”ç©¶å¥ å®šäº†åšå®çš„åŸºç¡€ã€‚åæ¥ï¼Œåœ¨1957å¹´ï¼Œä¸€ä¸ªç®€åŒ–çš„ç¥ç»å…ƒæ•°å­¦æ¨¡å‹ï¼ˆ*æ¥æ”¶è¾“å…¥ã€å¯¹è¿™äº›è¾“å…¥åº”ç”¨æƒé‡ã€å¯¹å…¶æ±‚å’Œå¹¶è¾“å‡ºäºŒè¿›åˆ¶ç»“æœ*ï¼‰å¯å‘äº†[Frank
    Rosenblattæ„å»ºäº†Mark I](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç¡¬ä»¶å®ç°ï¼Œæ—¨åœ¨å±•ç¤º[æ„ŸçŸ¥æœº](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)çš„æ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶è¿›è¡ŒäºŒåˆ†ç±»çš„ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä»é‚£æ—¶èµ·ï¼Œè®©è®¡ç®—æœºåƒæˆ‘ä»¬ä¸€æ ·æ€è€ƒçš„è¿½æ±‚å°±æ²¡æœ‰åœæ­‡ã€‚å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡æ·±å…¥å­¦ä¹ ç¥ç»ç½‘ç»œï¼Œæˆ–è€…ä½ æƒ³è¦åˆ·æ–°å¹¶å·©å›ºä½ çš„çŸ¥è¯†ï¼Œæˆ‘æ¨èé˜…è¯»[Shreya
    Raoçš„è¿™ç³»åˆ—æ–‡ç« ](https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640)ï¼Œä½œä¸ºæ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚æ­¤å¤–ï¼Œä½ è¿˜å¯ä»¥è®¿é—®æˆ‘æ”¶é›†çš„[è¿™ç³»åˆ—æ•…äº‹ï¼ˆä¸åŒçš„è´¡çŒ®è€…ï¼‰](https://medium.com/@raul.vizcarrach/list/neural-networks-098e9b594f19)ï¼Œä½ å¯èƒ½ä¼šå‘ç°å®ƒä»¬æœ‰ç”¨ã€‚
- en: '***Why choose a Convolutional Neural Network (CNN)?*** Honestly, it wasnâ€™t
    my first choice. Initially, I tried building a model with [LandingAI](https://landing.ai/),
    a user-friendly platform for cloud deployment and [Python connection through APIs](https://landing.ai/blog/build-your-custom-computer-vision-app-with-python-library).
    However, latency issues appeared *(over 1,000 frames to process online)*. Similar
    latency problems occurred with pre-trained models in [Roboflow](https://universe.roboflow.com/),
    despite their quality datasets and pre-trained models. Realizing the need to run
    it locally, I tried an MSE-based method to classify jersey colors for team and
    referee detection. While it sounded like the final solution, it showed low accuracy.
    After days of trial and error, I switched to CNNs. Among different deep learning
    approaches, CNNs are well-suited for object detection, unlike LSTM or RNN, which
    are better fit for sequential data like language transcription or translation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸ºä»€ä¹ˆé€‰æ‹©å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Ÿ*** è¯´å®è¯ï¼Œè¿™å¹¶ä¸æ˜¯æˆ‘æœ€åˆçš„é€‰æ‹©ã€‚æœ€åˆï¼Œæˆ‘å°è¯•ä½¿ç”¨[LandingAI](https://landing.ai/)ï¼Œä¸€ä¸ªé€‚åˆäº‘éƒ¨ç½²çš„ç”¨æˆ·å‹å¥½å¹³å°ï¼Œå¹¶ä¸”æ”¯æŒé€šè¿‡[Python
    APIè¿æ¥](https://landing.ai/blog/build-your-custom-computer-vision-app-with-python-library)ã€‚ç„¶è€Œï¼Œå‡ºç°äº†å»¶è¿Ÿé—®é¢˜ï¼ˆ*éœ€è¦å¤„ç†è¶…è¿‡1,000å¸§çš„æ•°æ®*ï¼‰ã€‚å³ä¾¿æ˜¯åœ¨[Roboflow](https://universe.roboflow.com/)çš„é¢„è®­ç»ƒæ¨¡å‹ä¸­ï¼Œå°½ç®¡å®ƒä»¬æä¾›äº†é«˜è´¨é‡çš„æ•°æ®é›†å’Œæ¨¡å‹ï¼Œä»ç„¶é‡åˆ°äº†ç±»ä¼¼çš„å»¶è¿Ÿé—®é¢˜ã€‚æ„è¯†åˆ°å¿…é¡»åœ¨æœ¬åœ°è¿è¡Œåï¼Œæˆ‘å°è¯•äº†åŸºäºå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„æ–¹æ³•æ¥åˆ†ç±»é˜Ÿä¼å’Œè£åˆ¤çš„çƒè¡£é¢œè‰²ã€‚å°½ç®¡è¿™ç§æ–¹æ³•çœ‹ä¼¼æ˜¯æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œä½†å…¶å‡†ç¡®æ€§è¾ƒä½ã€‚ç»è¿‡å‡ å¤©çš„åå¤è¯•éªŒï¼Œæˆ‘æœ€ç»ˆè½¬å‘äº†CNNã€‚åœ¨ä¼—å¤šæ·±åº¦å­¦ä¹ æ–¹æ³•ä¸­ï¼ŒCNNéå¸¸é€‚åˆè¿›è¡Œç‰©ä½“æ£€æµ‹ï¼Œè€ŒLSTMæˆ–RNNæ›´é€‚ç”¨äºåƒè¯­è¨€è½¬å½•æˆ–ç¿»è¯‘ç­‰åºåˆ—æ•°æ®ã€‚'
- en: 'Before diving into the code, letâ€™s cover some basic concepts about its architecture:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥ç ”ç©¶ä»£ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆäº†è§£ä¸€äº›å…³äºå…¶æ¶æ„çš„åŸºæœ¬æ¦‚å¿µï¼š
- en: '**Sample Dataset for learning:** The dataset has been classified into three
    classes: **Referee**, **Team_Away** (White jersey players), and **Team_Home**
    (Yellow jersey players). A sample of each class has been divided into two sets:
    training data and validation data. The training data will be used by the CNN in
    each iteration (Epoch) to â€œlearnâ€ patterns across multiple layers. The validation
    data will be used at the end of each iteration to evaluate the modelâ€™s performance
    and measure how well it generalizes to new data. Creating the sample dataset wasnâ€™t
    too hard; it took me around 30 to 40 minutes to crop sample images from each class
    from the video and organize them into subdirectories. I managed to create a sample
    dataset of approximately 90 images that you can find in the [projectâ€™s GitHub
    repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å­¦ä¹ çš„æ ·æœ¬æ•°æ®é›†ï¼š** æ•°æ®é›†å·²è¢«åˆ†ä¸ºä¸‰ç±»ï¼š**è£åˆ¤**ã€**å®¢é˜Ÿ**ï¼ˆç™½è‰²çƒè¡£çš„çƒå‘˜ï¼‰å’Œ**ä¸»é˜Ÿ**ï¼ˆé»„è‰²çƒè¡£çš„çƒå‘˜ï¼‰ã€‚æ¯ä¸€ç±»çš„æ ·æœ¬è¢«åˆ†ä¸ºä¸¤ä¸ªå­é›†ï¼šè®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®ã€‚è®­ç»ƒæ•°æ®å°†åœ¨æ¯æ¬¡è¿­ä»£ï¼ˆEpochï¼‰ä¸­è¢«CNNä½¿ç”¨ï¼Œç”¨ä»¥â€œå­¦ä¹ â€å¤šä¸ªå±‚æ¬¡ä¸­çš„æ¨¡å¼ã€‚éªŒè¯æ•°æ®å°†åœ¨æ¯æ¬¡è¿­ä»£ç»“æŸæ—¶ç”¨æ¥è¯„ä¼°æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶è¡¡é‡æ¨¡å‹å¯¹æ–°æ•°æ®çš„æ³›åŒ–èƒ½åŠ›ã€‚åˆ›å»ºæ ·æœ¬æ•°æ®é›†å¹¶ä¸å›°éš¾ï¼›æˆ‘å¤§çº¦èŠ±äº†30åˆ°40åˆ†é’Ÿçš„æ—¶é—´ï¼Œä»è§†é¢‘ä¸­è£å‰ªå‡ºæ¯ä¸€ç±»çš„æ ·æœ¬å›¾åƒå¹¶å°†å®ƒä»¬æ•´ç†åˆ°å­ç›®å½•ä¸­ã€‚æˆ‘æˆåŠŸåˆ›å»ºäº†ä¸€ä¸ªçº¦90å¼ å›¾åƒçš„æ ·æœ¬æ•°æ®é›†ï¼Œä½ å¯ä»¥åœ¨[é¡¹ç›®çš„GitHubä»“åº“](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)ä¸­æ‰¾åˆ°ã€‚'
- en: '**How does the model learn?:** Input data moves through each layer of the neural
    network, which can have one or multiple layers linked together to make predictions.
    Every layer uses an activation function that processes data to make predictions
    or introduce changes to the data. Each connection between these layers has a weight,
    which determines how much influence one layerâ€™s output has on the next. The goal
    is to find the right combination of these weights that minimize mistakes when
    predicting outcomes. Through a process called backpropagation and a loss function,
    the model adjusts these weights to reduce errors and improve accuracy. This process
    repeats in whatâ€™s called an **Epoch (forward pass + backpropagation)**, with the
    model getting better at making predictions in each cycle as it learns from its
    mistakes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹æ˜¯å¦‚ä½•å­¦ä¹ çš„ï¼Ÿ** è¾“å…¥æ•°æ®ä¼šé€šè¿‡ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œæ¯ä¸€å±‚å¯ä»¥æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªç›¸äº’è¿æ¥çš„å±‚ï¼Œç”¨æ¥è¿›è¡Œé¢„æµ‹ã€‚æ¯ä¸€å±‚éƒ½ä½¿ç”¨æ¿€æ´»å‡½æ•°æ¥å¤„ç†æ•°æ®ï¼Œä»è€Œè¿›è¡Œé¢„æµ‹æˆ–å¯¹æ•°æ®è¿›è¡Œæ›´æ”¹ã€‚è¿™äº›å±‚ä¹‹é—´çš„æ¯ä¸ªè¿æ¥éƒ½æœ‰ä¸€ä¸ªæƒé‡ï¼Œå†³å®šäº†ä¸€ä¸ªå±‚çš„è¾“å‡ºå¯¹ä¸‹ä¸€ä¸ªå±‚çš„å½±å“ç¨‹åº¦ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°è¿™äº›æƒé‡çš„æ­£ç¡®ç»„åˆï¼Œä»¥æœ€å°åŒ–é¢„æµ‹ç»“æœçš„é”™è¯¯ã€‚é€šè¿‡ä¸€ä¸ªå«åšåå‘ä¼ æ’­çš„è¿‡ç¨‹å’ŒæŸå¤±å‡½æ•°ï¼Œæ¨¡å‹ä¼šè°ƒæ•´è¿™äº›æƒé‡ï¼Œä»¥å‡å°‘è¯¯å·®å¹¶æé«˜å‡†ç¡®æ€§ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šåœ¨æ‰€è°“çš„**Epochï¼ˆå‰å‘ä¼ æ’­
    + åå‘ä¼ æ’­ï¼‰**ä¸­é‡å¤è¿›è¡Œï¼Œéšç€æ¯ä¸ªå‘¨æœŸæ¨¡å‹ä»é”™è¯¯ä¸­å­¦ä¹ ï¼Œå®ƒåœ¨é¢„æµ‹ä¸Šçš„è¡¨ç°ä¹Ÿä¼šé€æ¸å˜å¥½ã€‚'
- en: '**Activation Function:** As mentioned before, the activation function plays
    an important role in the modelâ€™s learning process. I chose **ReLU (Rectified Linear
    Unit)** because it is known for being computationally efficient and mitigating
    what is called the vanishing gradient problem *(where networks with multiple layers
    may stop learning effectively).* While ReLU works well, [other functions](https://www.v7labs.com/blog/neural-networks-activation-functions)
    like **sigmoid**, **tanh**, or **swish** also have their uses depending on how
    complex the network is.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¿€æ´»å‡½æ•°:** å¦‚å‰æ‰€è¿°ï¼Œæ¿€æ´»å‡½æ•°åœ¨æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚æˆ‘é€‰æ‹©äº†**ReLUï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰**ï¼Œå› ä¸ºå®ƒåœ¨è®¡ç®—ä¸Šéå¸¸é«˜æ•ˆï¼Œå¹¶èƒ½ç¼“è§£æ‰€è°“çš„æ¶ˆå¤±æ¢¯åº¦é—®é¢˜*ï¼ˆå³å¤šå±‚ç½‘ç»œå¯èƒ½æ— æ³•æœ‰æ•ˆå­¦ä¹ ï¼‰*ã€‚è™½ç„¶
    ReLU å·¥ä½œå¾—å¾ˆå¥½ï¼Œ[å…¶ä»–å‡½æ•°](https://www.v7labs.com/blog/neural-networks-activation-functions)å¦‚**sigmoid**ã€**tanh**æˆ–**swish**ä¹Ÿæœ‰å…¶åº”ç”¨ï¼Œå…·ä½“å–å†³äºç½‘ç»œçš„å¤æ‚æ€§ã€‚'
- en: '**Epochs:** Setting the right number of epochs involves experimentation. You
    should take into account factors such as the complexity of the dataset, the architecture
    of your CNN model, and computational resources. In most cases, it is best to monitor
    the modelâ€™s performance in each iteration and stop training when improvements
    become minimal to prevent overfitting. Given my small training dataset, **I decided
    to start with 10 epochs as a baseline**. However, adjustments may be necessary
    in other scenarios based on metric performance and validation results.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒè½®æ•°ï¼ˆEpochsï¼‰:** è®¾ç½®åˆé€‚çš„è®­ç»ƒè½®æ•°éœ€è¦å®éªŒã€‚ä½ åº”è¯¥è€ƒè™‘æ•°æ®é›†çš„å¤æ‚æ€§ã€CNN æ¨¡å‹çš„æ¶æ„å’Œè®¡ç®—èµ„æºç­‰å› ç´ ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæœ€å¥½åœ¨æ¯æ¬¡è¿­ä»£ä¸­ç›‘æ§æ¨¡å‹çš„è¡¨ç°ï¼Œå¹¶åœ¨æ”¹è¿›å˜å¾—å¾®ä¹å…¶å¾®æ—¶åœæ­¢è®­ç»ƒï¼Œä»¥é¿å…è¿‡æ‹Ÿåˆã€‚è€ƒè™‘åˆ°æˆ‘å°çš„è®­ç»ƒæ•°æ®é›†ï¼Œ**æˆ‘å†³å®šä»¥
    10 è½®ä¸ºåŸºå‡†**å¼€å§‹ã€‚ç„¶è€Œï¼Œåœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæ ¹æ®æŒ‡æ ‡è¡¨ç°å’ŒéªŒè¯ç»“æœï¼Œå¯èƒ½éœ€è¦åšå‡ºè°ƒæ•´ã€‚'
- en: '**Adam (Adaptive Moment Estimation):** Ultimately, the goal is to reduce the
    error between predicted and true outputs. As mentioned before, backpropagation
    plays a key role here by adjusting and updating neural network weights to improve
    predictions over time. While backpropagation handles weight updates based on gradients
    from the loss function, the Adam algorithm enhances this process by dynamically
    adjusting the learning rate to gradually minimize the error or loss function.
    In other words, it fine-tunes how quickly the model learns.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰:** æœ€ç»ˆç›®æ ‡æ˜¯å‡å°‘é¢„æµ‹è¾“å‡ºä¸çœŸå®è¾“å‡ºä¹‹é—´çš„è¯¯å·®ã€‚å¦‚å‰æ‰€è¿°ï¼Œåå‘ä¼ æ’­åœ¨æ­¤è¿‡ç¨‹ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œé€šè¿‡è°ƒæ•´å’Œæ›´æ–°ç¥ç»ç½‘ç»œæƒé‡æ¥éšç€æ—¶é—´çš„æ¨ç§»æ”¹è¿›é¢„æµ‹ã€‚åå‘ä¼ æ’­åŸºäºæŸå¤±å‡½æ•°çš„æ¢¯åº¦å¤„ç†æƒé‡æ›´æ–°ï¼Œè€Œ
    Adam ç®—æ³•é€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡æ¥è¿›ä¸€æ­¥ä¼˜åŒ–è¿™ä¸€è¿‡ç¨‹ï¼Œä»è€Œé€æ­¥å‡å°‘è¯¯å·®æˆ–æŸå¤±å‡½æ•°ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå¾®è°ƒäº†æ¨¡å‹å­¦ä¹ çš„é€Ÿåº¦ã€‚'
- en: 'That said in order to run our CNN model we will need the following Python packages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ºäº†è¿è¡Œæˆ‘ä»¬çš„ CNN æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹ Python åŒ…ï¼š
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Tip-02:** Ensure that PyTorch itâ€™s installed properly. All my tools are set
    up in an Anaconda environment, and when I installed PyTorch, at first, it seemed
    that it was set up correctly. However, some issues appeared while running some
    libraries. Initially, I thought it was the code, but after several revisions and
    no success, I had to reinstall Anaconda and install PyTorch in a clean environment,
    and with that, problem fixed!'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æç¤º-02:** ç¡®ä¿ PyTorch æ­£ç¡®å®‰è£…ã€‚æˆ‘æ‰€æœ‰çš„å·¥å…·éƒ½åœ¨ Anaconda ç¯å¢ƒä¸­è®¾ç½®ï¼Œå½“æˆ‘å®‰è£… PyTorch æ—¶ï¼Œä¸€å¼€å§‹çœ‹èµ·æ¥å®ƒä¼¼ä¹æ­£ç¡®å®‰è£…äº†ã€‚ç„¶è€Œï¼Œåœ¨è¿è¡Œä¸€äº›åº“æ—¶å‡ºç°äº†ä¸€äº›é—®é¢˜ã€‚æœ€åˆï¼Œæˆ‘ä»¥ä¸ºæ˜¯ä»£ç çš„é—®é¢˜ï¼Œä½†ç»è¿‡å¤šæ¬¡ä¿®æ”¹ä»ç„¶æ²¡æœ‰æˆåŠŸï¼Œæˆ‘åªå¥½é‡æ–°å®‰è£…
    Anacondaï¼Œå¹¶åœ¨å¹²å‡€çš„ç¯å¢ƒä¸­é‡æ–°å®‰è£… PyTorchï¼Œé—®é¢˜å°±è¿™æ ·è§£å†³äº†ï¼'
- en: 'Next, weâ€™ll specify our libraries and the path of our sample dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æŒ‡å®šæˆ‘ä»¬çš„åº“å’Œæ ·æœ¬æ•°æ®é›†çš„è·¯å¾„ï¼š
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: First, we will ensure each picture is equally sized (resize to 150x150 pixels),
    then convert it to a format that the code can understand (in PyTorch, input data
    is typically represented as Tensor objects). Finally, we will adjust the colors
    to make them easier for the model to work with (normalize) and set up a procedure
    to load the images. These steps together help prepare the pictures and organize
    them so the model can effectively start learning from them, avoiding deviations
    caused by data format.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†ç¡®ä¿æ¯å¼ å›¾ç‰‡çš„å¤§å°ä¸€è‡´ï¼ˆè°ƒæ•´ä¸º 150x150 åƒç´ ï¼‰ï¼Œç„¶åå°†å…¶è½¬æ¢ä¸ºä»£ç èƒ½å¤Ÿç†è§£çš„æ ¼å¼ï¼ˆåœ¨ PyTorch ä¸­ï¼Œè¾“å…¥æ•°æ®é€šå¸¸è¡¨ç¤ºä¸º Tensor
    å¯¹è±¡ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è°ƒæ•´é¢œè‰²ï¼Œä»¥ä¾¿æ¨¡å‹æ›´å®¹æ˜“å¤„ç†ï¼ˆå½’ä¸€åŒ–ï¼‰ï¼Œå¹¶è®¾ç½®åŠ è½½å›¾ç‰‡çš„ç¨‹åºã€‚è¿™äº›æ­¥éª¤å…±åŒå¸®åŠ©å‡†å¤‡å›¾ç‰‡ï¼Œå¹¶å°†å®ƒä»¬æ•´ç†å¥½ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»ä¸­å­¦ä¹ ï¼Œé¿å…å› æ•°æ®æ ¼å¼å¯¼è‡´çš„åå·®ã€‚
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next weâ€™ll define our CNNâ€™s architecture:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰ CNN çš„æ¶æ„ï¼š
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You will notice that our CNN model has three layers (conv1, conv2, conv3). The
    data begins in the convolutional layer (conv), where the activation function (ReLU)
    is applied. This function enables the network to learn complex patterns and relationships
    in the data. Following this, the pooling layer is activated. ***What is Max Pooling?***
    Itâ€™s a technique that reduces the image size while retaining important features,
    which helps in efficient training and optimizes memory resources. This process
    repeats across conv1 to conv3\. Finally, the data passes through fully connected
    layers (fc1, fc2) for final classification (or decision-making).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šæ³¨æ„åˆ°æˆ‘ä»¬çš„CNNæ¨¡å‹æœ‰ä¸‰å±‚ï¼ˆconv1ï¼Œconv2ï¼Œconv3ï¼‰ã€‚æ•°æ®é¦–å…ˆè¿›å…¥å·ç§¯å±‚ï¼ˆconvï¼‰ï¼Œåœ¨è¿™é‡Œåº”ç”¨äº†æ¿€æ´»å‡½æ•°ï¼ˆReLUï¼‰ã€‚è¯¥å‡½æ•°ä½¿å¾—ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼å’Œå…³ç³»ã€‚æ¥ç€ï¼Œæ± åŒ–å±‚è¢«æ¿€æ´»ã€‚***ä»€ä¹ˆæ˜¯æœ€å¤§æ± åŒ–ï¼Ÿ***å®ƒæ˜¯ä¸€ç§å‡å°‘å›¾åƒå¤§å°çš„æŠ€æœ¯ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ç‰¹å¾ï¼Œæœ‰åŠ©äºé«˜æ•ˆè®­ç»ƒå¹¶ä¼˜åŒ–å†…å­˜èµ„æºã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨conv1åˆ°conv3ä¹‹é—´é‡å¤è¿›è¡Œã€‚æœ€åï¼Œæ•°æ®é€šè¿‡å…¨è¿æ¥å±‚ï¼ˆfc1ï¼Œfc2ï¼‰è¿›è¡Œæœ€ç»ˆåˆ†ç±»ï¼ˆæˆ–å†³ç­–ï¼‰ã€‚
- en: As the next step, we initialize our model, configure categorical cross-entropy
    as the loss function *(commonly used for classification tasks)*, and designate
    Adam as our optimizer. As mentioned earlier, weâ€™ll execute our model over a full
    cycle of 10 epochs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬åˆå§‹åŒ–æ¨¡å‹ï¼Œé…ç½®ç±»åˆ«äº¤å‰ç†µä¸ºæŸå¤±å‡½æ•°*(é€šå¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡)*ï¼Œå¹¶å°†Adamä½œä¸ºä¼˜åŒ–å™¨ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†åœ¨10ä¸ªå‘¨æœŸå†…æ‰§è¡Œæ¨¡å‹ã€‚
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To track performance, we will add some code to follow the training progress,
    print validation metrics, and plot them. Finally, we save the model as **hockey_team_classifier.pth**
    in a designated path of your choice.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·Ÿè¸ªæ€§èƒ½ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€äº›ä»£ç æ¥è·Ÿè¸ªè®­ç»ƒè¿›åº¦ï¼Œæ‰“å°éªŒè¯æŒ‡æ ‡å¹¶ç»˜åˆ¶å›¾è¡¨ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹ä¿å­˜ä¸º**hockey_team_classifier.pth**ï¼Œä¿å­˜åœ¨ä½ é€‰æ‹©çš„æŒ‡å®šè·¯å¾„ä¸­ã€‚
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Additionally, alongside your â€œpthâ€ file, after running through all the steps
    described above *(you can find the complete code in* [*the projectâ€™s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*,
    you should expect to see an output like the following (metrics may vary slightly)*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œé™¤äº†ä½ çš„â€œpthâ€æ–‡ä»¶ï¼Œåœ¨å®Œæˆä¸Šè¿°æ‰€æœ‰æ­¥éª¤å*(ä½ å¯ä»¥åœ¨[*é¡¹ç›®çš„GitHubä»“åº“*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)ä¸­æ‰¾åˆ°å®Œæ•´ä»£ç )*ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼ˆæŒ‡æ ‡å¯èƒ½ç•¥æœ‰ä¸åŒï¼‰ï¼š
- en: '![](../Images/91ff066093e4c561b0f5b20528f75f57.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91ff066093e4c561b0f5b20528f75f57.png)'
- en: 'Figure 05: CNN model performance metrics'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 05ï¼šCNNæ¨¡å‹æ€§èƒ½æŒ‡æ ‡
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After finishing 10 epochs, the CNN model shows improvement in performance metrics.
    Initially, in Epoch 1, the model starts with a training loss of 1.5346 and a validation
    accuracy of 47.37%. ***How should we understand this initial point?***
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆ10ä¸ªè®­ç»ƒå‘¨æœŸåï¼ŒCNNæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡æœ‰æ‰€æ”¹å–„ã€‚æœ€åˆï¼Œåœ¨ç¬¬1ä¸ªå‘¨æœŸæ—¶ï¼Œæ¨¡å‹çš„è®­ç»ƒæŸå¤±ä¸º1.5346ï¼ŒéªŒè¯å‡†ç¡®ç‡ä¸º47.37%ã€‚***æˆ‘ä»¬åº”å¦‚ä½•ç†è§£è¿™ä¸ªåˆå§‹ç‚¹ï¼Ÿ***
- en: '**Accuracy** is one of the most common metrics for evaluating classification
    performance. In our case, it represents the proportion of correctly predicted
    classes out of the total. **However, high accuracy alone doesnâ€™t guarantee overall
    model performance**; you still can have poor predictions for specific classes
    (as I experienced in early trials). Regarding **training loss**, it measures how
    effectively the model learns to map input data to the correct labels. Since weâ€™re
    using a classification function, **Cross-Entropy Loss** quantifies the difference
    between predicted class probabilities and actual labels. A starting value like
    1.5346 indicates significant differences between predicted and actual classes;
    ideally, this value should approach 0 as training progresses. As epochs progress,
    we observe a significant drop in training loss and an increase in validation accuracy.
    By the final epoch, the training and validation loss reach lows of 0.2509 and
    0.2651, respectively.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‡†ç¡®ç‡**æ˜¯è¯„ä¼°åˆ†ç±»æ€§èƒ½æœ€å¸¸è§çš„æŒ‡æ ‡ä¹‹ä¸€ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå®ƒè¡¨ç¤ºæ­£ç¡®é¢„æµ‹çš„ç±»åˆ«å æ€»ç±»åˆ«çš„æ¯”ä¾‹ã€‚**ç„¶è€Œï¼Œå•é é«˜å‡†ç¡®ç‡å¹¶ä¸èƒ½ä¿è¯æ•´ä½“æ¨¡å‹çš„è¡¨ç°**ï¼›ä½ ä»ç„¶å¯èƒ½åœ¨æŸäº›ç±»åˆ«ä¸Šåšå‡ºä¸å¥½çš„é¢„æµ‹ï¼ˆæ­£å¦‚æˆ‘åœ¨æ—©æœŸå®éªŒä¸­æ‰€ç»å†çš„é‚£æ ·ï¼‰ã€‚å…³äº**è®­ç»ƒæŸå¤±**ï¼Œå®ƒè¡¡é‡æ¨¡å‹å°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°æ­£ç¡®æ ‡ç­¾çš„æ•ˆæœã€‚ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯åˆ†ç±»å‡½æ•°ï¼Œ**äº¤å‰ç†µæŸå¤±**é‡åŒ–äº†é¢„æµ‹çš„ç±»åˆ«æ¦‚ç‡ä¸å®é™…æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚åƒ1.5346è¿™æ ·çš„åˆå§‹å€¼è¡¨ç¤ºé¢„æµ‹ç±»åˆ«ä¸å®é™…ç±»åˆ«ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼›ç†æƒ³æƒ…å†µä¸‹ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè¿™ä¸ªå€¼åº”è¯¥è¶‹è¿‘äº0ã€‚éšç€è®­ç»ƒå‘¨æœŸçš„è¿›è¡Œï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°è®­ç»ƒæŸå¤±æ˜¾è‘—ä¸‹é™ï¼ŒéªŒè¯å‡†ç¡®ç‡æé«˜ã€‚åˆ°æœ€åä¸€ä¸ªè®­ç»ƒå‘¨æœŸæ—¶ï¼Œè®­ç»ƒæŸå¤±å’ŒéªŒè¯æŸå¤±åˆ†åˆ«é™åˆ°0.2509å’Œ0.2651çš„æœ€ä½ç‚¹ã€‚'
- en: To test our CNN model, we can select a sample of player images and evaluate
    its prediction capability. For testing, you can run the following code and utilize
    the **validation_dataset folder** in the [projectâ€™s GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯•æˆ‘ä»¬çš„ CNN æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€éƒ¨åˆ†çƒå‘˜å›¾åƒå¹¶è¯„ä¼°å…¶é¢„æµ‹èƒ½åŠ›ã€‚ä¸ºäº†æµ‹è¯•ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç å¹¶ä½¿ç”¨ [é¡¹ç›®çš„ GitHub ä»“åº“](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)ä¸­çš„**validation_dataset
    æ–‡ä»¶å¤¹**ã€‚
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output should look something like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºåº”å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, the model shows quite good ability in identifying teams and
    excluding the referee as a team player.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæ¨¡å‹åœ¨è¯†åˆ«é˜Ÿä¼å¹¶æ’é™¤è£åˆ¤ä½œä¸ºé˜Ÿå‘˜æ–¹é¢è¡¨ç°å¾—ç›¸å½“ä¸é”™ã€‚
- en: '**Tip #03:** Something I learned during the CNN design process is that adding
    complexity doesnâ€™t always improve performance. Initially, I experimented with
    deeper models (more convolutional layers) and color-based augmentation to enhance
    playersâ€™ jersey recognition. However, in my small dataset, I encountered overfitting
    rather than learning generalizable features (all images were predicted as white
    team players or referees). Regularization techniques like dropout and batch normalization
    are also important; they help impose constraints during training, ensuring the
    model can generalize well to new data. Less can sometimes mean more in terms of
    resultsğŸ˜.'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**å°è´´å£« #03ï¼š** åœ¨ CNN è®¾è®¡è¿‡ç¨‹ä¸­æˆ‘å­¦åˆ°çš„ä¸€ç‚¹æ˜¯ï¼Œå¢åŠ å¤æ‚æ€§å¹¶ä¸æ€»èƒ½æå‡æ€§èƒ½ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘å°è¯•äº†æ›´æ·±çš„æ¨¡å‹ï¼ˆæ›´å¤šçš„å·ç§¯å±‚ï¼‰å’ŒåŸºäºé¢œè‰²çš„å¢å¼ºæ¥æé«˜çƒå‘˜çƒè¡£çš„è¯†åˆ«ç‡ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘çš„å°æ•°æ®é›†ä¸Šï¼Œæˆ‘é‡åˆ°äº†è¿‡æ‹Ÿåˆï¼Œè€Œä¸æ˜¯å­¦ä¹ åˆ°å¯ä»¥æ³›åŒ–çš„ç‰¹å¾ï¼ˆæ‰€æœ‰å›¾åƒéƒ½è¢«é¢„æµ‹ä¸ºç™½é˜Ÿçƒå‘˜æˆ–è£åˆ¤ï¼‰ã€‚æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œå¦‚
    dropout å’Œæ‰¹é‡å½’ä¸€åŒ–ä¹Ÿå¾ˆé‡è¦ï¼›å®ƒä»¬æœ‰åŠ©äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ–½åŠ çº¦æŸï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿå¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°æ•°æ®ã€‚å°‘å³æ˜¯å¤šï¼Œç»“æœæœ‰æ—¶ä¼šæ›´å¥½ğŸ˜ã€‚'
- en: '**PUTING IT ALL TOGETHER**'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å°†ä¸€åˆ‡æ•´åˆåœ¨ä¸€èµ·**'
- en: Putting it all together will require some adjustments to our tracking mechanism
    described earlier. Hereâ€™s a breakdown of the updated code step by step.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·éœ€è¦å¯¹æˆ‘ä»¬ä¹‹å‰æè¿°çš„è·Ÿè¸ªæœºåˆ¶è¿›è¡Œä¸€äº›è°ƒæ•´ã€‚ä¸‹é¢æ˜¯æ›´æ–°åçš„ä»£ç é€æ­¥è§£æã€‚
- en: 'First, weâ€™ll set up the libraries and paths we need. Note that the paths for
    our pickle file and the CNN model are specified now. **This time, if the pickle
    file isnâ€™t found in the path, the code will throw an error**. Use the previous
    code to generate the pickle file if needed, and use this updated version to perform
    the video analysis:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†è®¾ç½®æ‰€éœ€çš„åº“å’Œè·¯å¾„ã€‚è¯·æ³¨æ„ï¼Œç°åœ¨å·²ç»æŒ‡å®šäº† pickle æ–‡ä»¶å’Œ CNN æ¨¡å‹çš„è·¯å¾„ã€‚**è¿™æ¬¡ï¼Œå¦‚æœæ‰¾ä¸åˆ°è·¯å¾„ä¸­çš„ pickle æ–‡ä»¶ï¼Œä»£ç å°†æŠ›å‡ºä¸€ä¸ªé”™è¯¯**ã€‚å¦‚æœéœ€è¦ï¼Œä½¿ç”¨ä¹‹å‰çš„ä»£ç ç”Ÿæˆ
    pickle æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨æ­¤æ›´æ–°ç‰ˆæœ¬æ¥æ‰§è¡Œè§†é¢‘åˆ†æï¼š
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we will load the models, specify the rink coordinates, and initiate the
    process of detecting objects in each frame in batches of 20, as we did before.
    Note that for now, we will only use the rink boundaries to focus the analysis
    on the rink. In the final steps of the article, when we include performance stats,
    weâ€™ll use the offensive zone coordinates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åŠ è½½æ¨¡å‹ï¼ŒæŒ‡å®šå†°åœºåæ ‡ï¼Œå¹¶å¯åŠ¨æ¯å¸§å›¾åƒçš„æ‰¹é‡æ£€æµ‹è¿‡ç¨‹ï¼Œæ¯æ¬¡æ‰¹é‡ä¸º 20 å¸§ï¼Œå’Œä¹‹å‰ä¸€æ ·ã€‚è¯·æ³¨æ„ï¼Œç›®å‰æˆ‘ä»¬åªä¼šä½¿ç”¨å†°åœºè¾¹ç•Œæ¥èšç„¦åˆ†æèŒƒå›´ã€‚æ–‡ç« çš„æœ€åå‡ æ­¥ä¸­ï¼Œå½“æˆ‘ä»¬åŒ…å«æ€§èƒ½ç»Ÿè®¡æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿›æ”»åŒºåæ ‡ã€‚
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, weâ€™ll add the process to predict each playerâ€™s team:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·»åŠ é¢„æµ‹æ¯ä¸ªçƒå‘˜é˜Ÿä¼çš„è¿‡ç¨‹ï¼š
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As the next step, weâ€™ll add the method described earlier to switch from bounding
    boxes to ellipses:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¹‹å‰æè¿°çš„æ–¹æ³•ï¼Œä»è¾¹ç•Œæ¡†è½¬æ¢ä¸ºæ¤­åœ†å½¢ï¼š
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, itâ€™s time to add the analyzer that includes reading the pickle file, narrowing
    the analysis within the rink boundaries we defined earlier, and calling the CNN
    model to identify each playerâ€™s team and add labels. Note that we include a feature
    to label referees with a different color and change the color of their ellipses
    as well. The code ends with writing processed frames to an output video.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ˜¯æ—¶å€™æ·»åŠ åˆ†æå™¨äº†ï¼Œå®ƒåŒ…æ‹¬è¯»å– pickle æ–‡ä»¶ã€å°†åˆ†æèŒƒå›´ç¼©å°åˆ°æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„å†°åœºè¾¹ç•Œï¼Œå¹¶è°ƒç”¨ CNN æ¨¡å‹ä»¥è¯†åˆ«æ¯ä¸ªçƒå‘˜çš„é˜Ÿä¼å¹¶æ·»åŠ æ ‡ç­¾ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åŒ…æ‹¬äº†ä¸€ä¸ªç‰¹æ€§ï¼Œç”¨ä¸åŒçš„é¢œè‰²æ ‡è®°è£åˆ¤ï¼Œå¹¶ä¸”æ”¹å˜ä»–ä»¬æ¤­åœ†å½¢çš„é¢œè‰²ã€‚ä»£ç çš„æœ€åä¼šå°†å¤„ç†è¿‡çš„å¸§å†™å…¥è¾“å‡ºè§†é¢‘ã€‚
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we add the CNNâ€™s architecture (defined in the CNN design process)
    and execute the Hockey analyzer:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ·»åŠ  CNN çš„æ¶æ„ï¼ˆåœ¨ CNN è®¾è®¡è¿‡ç¨‹ä¸­å®šä¹‰ï¼‰å¹¶æ‰§è¡Œå†°çƒåˆ†æå™¨ï¼š
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After running all the steps, your video output should look something like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ‰€æœ‰æ­¥éª¤åï¼Œä½ çš„è§†é¢‘è¾“å‡ºåº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/98890f2c70834d1f61d97b6135b3afe5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98890f2c70834d1f61d97b6135b3afe5.png)'
- en: 'Sample Clip 06: Tracking Players and Teams'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ç‰‡æ®µ 06ï¼šè·Ÿè¸ªçƒå‘˜å’Œé˜Ÿä¼
- en: Note that in this last update, object detections are only within the ice rink,
    and teams are differentiated, as well as the referee. While the CNN model still
    needs fine-tuning and occasionally loses stability with some players, it remains
    mostly reliable and accurate throughout the video.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨è¿™æ¬¡æ›´æ–°ä¸­ï¼Œç‰©ä½“æ£€æµ‹ä»…é™äºå†°çƒåœºå†…ï¼Œä¸”é˜Ÿä¼å’Œè£åˆ¤å·²è¢«åŒºåˆ†å¼€æ¥ã€‚è™½ç„¶CNNæ¨¡å‹ä»éœ€å¾®è°ƒï¼Œå¹¶ä¸”å¶å°”åœ¨ä¸€äº›çƒå‘˜èº«ä¸Šä¼šå¤±å»ç¨³å®šæ€§ï¼Œä½†åœ¨æ•´ä¸ªè§†é¢‘ä¸­ï¼Œå®ƒä»ç„¶å¤§éƒ¨åˆ†æ—¶é—´æ˜¯å¯é ä¸”å‡†ç¡®çš„ã€‚
- en: '**SPEED, DISTANCE AND OFFENSIVE PRESSURE**'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**é€Ÿåº¦ã€è·ç¦»å’Œè¿›æ”»å‹åŠ›**'
- en: 'The ability to track teams and players opens up exciting possibilities for
    measuring performance, such as generating heatmaps, analyzing speed and distance
    covered, tracking movements like zone entries or exits, and diving into detailed
    player metrics. In order we can have a taste of it, weâ€™ll add three performance
    metrics: **average speed per player**, skating **distance covered** by each team,
    and o**ffensive pressure** *(measured as the percentage of distance covered by
    each team spent in its opponentâ€™s zone)*. Iâ€™ll leave more detailed statistics
    up to you!'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªé˜Ÿä¼å’Œçƒå‘˜çš„èƒ½åŠ›ä¸ºè¡¡é‡è¡¨ç°å¼€è¾Ÿäº†ä»¤äººå…´å¥‹çš„å¯èƒ½æ€§ï¼Œä¾‹å¦‚ç”Ÿæˆçƒ­å›¾ã€åˆ†æé€Ÿåº¦å’Œè¦†ç›–çš„è·ç¦»ã€è·Ÿè¸ªå¦‚åŒºåŸŸè¿›å…¥æˆ–é€€å‡ºç­‰åŠ¨ä½œï¼Œä»¥åŠæ·±å…¥ç ”ç©¶çƒå‘˜çš„è¯¦ç»†æŒ‡æ ‡ã€‚ä¸ºäº†è®©æˆ‘ä»¬èƒ½æ„Ÿå—è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸‰é¡¹è¡¨ç°æŒ‡æ ‡ï¼š**æ¯ä¸ªçƒå‘˜çš„å¹³å‡é€Ÿåº¦**ã€æ¯æ”¯é˜Ÿä¼æ»‘è¡Œçš„**è·ç¦»**ï¼Œä»¥åŠ**è¿›æ”»å‹åŠ›**ï¼ˆ*ä»¥æ¯æ”¯é˜Ÿä¼åœ¨å¯¹æ–¹åŒºåŸŸå†…æ‰€èŠ±è´¹çš„è·ç¦»å æ€»è·ç¦»çš„ç™¾åˆ†æ¯”æ¥è¡¡é‡*ï¼‰ã€‚æˆ‘å°†æŠŠæ›´è¯¦ç»†çš„ç»Ÿè®¡æ•°æ®ç•™ç»™ä½ ä»¬ï¼
- en: We begin adapting the coordinates of the ice rink from pixel-based measurements
    to approximate meters. This adjustment allows us to read our data in meters rather
    than pixels. The real-world dimensions of the ice rink seen in the video are approximately
    15mx30m (15 meters in width and 30 meters in height). To facilitate this conversion,
    we introduce a method to convert pixel coordinates to meters. By defining the
    rinkâ€™s actual dimensions and using the pixel coordinates of its corners (from
    left to right and top to bottom), we obtain conversion factors. These factors
    will support our process of estimating distances in meters and speeds in meters
    per second. *(Another interesting technique you can explore and apply is Perspective
    Transformation)*
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼€å§‹å°†å†°åœºçš„åæ ‡ä»åŸºäºåƒç´ çš„åº¦é‡è½¬æ¢ä¸ºè¿‘ä¼¼ç±³æ•°ã€‚è¿™ä¸€è°ƒæ•´ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥ç±³ä¸ºå•ä½è¯»å–æ•°æ®ï¼Œè€Œéåƒç´ ã€‚è§†é¢‘ä¸­çœ‹åˆ°çš„å†°åœºçš„å®é™…å°ºå¯¸å¤§çº¦ä¸º15mx30mï¼ˆå®½åº¦ä¸º15ç±³ï¼Œé«˜åº¦ä¸º30ç±³ï¼‰ã€‚ä¸ºäº†æ–¹ä¾¿è¿™ä¸€è½¬æ¢ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å°†åƒç´ åæ ‡è½¬æ¢ä¸ºç±³çš„æ–¹å¼ã€‚é€šè¿‡å®šä¹‰å†°åœºçš„å®é™…å°ºå¯¸ï¼Œå¹¶ä½¿ç”¨å…¶è§’è½çš„åƒç´ åæ ‡ï¼ˆä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹ï¼‰ï¼Œæˆ‘ä»¬è·å¾—äº†è½¬æ¢å› å­ã€‚è¿™äº›å› å­å°†æ”¯æŒæˆ‘ä»¬ä¼°ç®—ç±³æ•°å’Œæ¯ç§’ç±³æ•°é€Ÿåº¦çš„è¿‡ç¨‹ã€‚*ï¼ˆå¦ä¸€ä¸ªæœ‰è¶£çš„æŠ€æœ¯æ˜¯é€è§†å˜æ¢ï¼Œä½ å¯ä»¥æ¢ç´¢å¹¶åº”ç”¨å®ƒï¼‰*
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are now ready to **add speed to each player measured in meters per second**.
    To do this, weâ€™ll need to make three modifications. First, initiate an empty dictionary
    named **previous_positions** in the **HockeyAnalyzer class** to help us compare
    the current and previous positions of players. Similarly, weâ€™ll create a t**eam_stats**
    structure to store stats from each team for further visualization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½**ä»¥æ¯ç§’ç±³æ•°ä¸ºå•ä½æ·»åŠ æ¯ä¸ªçƒå‘˜çš„é€Ÿåº¦**ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸‰å¤„ä¿®æ”¹ã€‚é¦–å…ˆï¼Œåœ¨**HockeyAnalyzerç±»**ä¸­åˆå§‹åŒ–ä¸€ä¸ªåä¸º**previous_positions**çš„ç©ºå­—å…¸ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æ¯”è¾ƒçƒå‘˜çš„å½“å‰å’Œå‰ä¸€ä¸ªä½ç½®ã€‚åŒæ ·ï¼Œæˆ‘ä»¬è¿˜å°†åˆ›å»ºä¸€ä¸ª**team_stats**ç»“æ„æ¥å­˜å‚¨æ¯æ”¯é˜Ÿä¼çš„ç»Ÿè®¡æ•°æ®ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å¯è§†åŒ–ã€‚
- en: 'Next, we will add a **speed method** to estimate playersâ€™ speed in pixels per
    second, and then use the conversion factor (explained earlier) to transform it
    into meters per second. Finally, from the **analyze_video method**, weâ€™ll call
    our new speed method and add the speed to each tracked object (players and referee).
    This is what the changes look like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ª**é€Ÿåº¦æ–¹æ³•**æ¥ä¼°ç®—çƒå‘˜çš„é€Ÿåº¦ï¼ˆä»¥æ¯ç§’åƒç´ æ•°ä¸ºå•ä½ï¼‰ï¼Œç„¶åä½¿ç”¨å‰é¢è§£é‡Šçš„è½¬æ¢å› å­å°†å…¶è½¬æ¢ä¸ºæ¯ç§’ç±³æ•°ã€‚æœ€åï¼Œåœ¨**analyze_videoæ–¹æ³•**ä¸­ï¼Œæˆ‘ä»¬å°†è°ƒç”¨æ–°çš„é€Ÿåº¦æ–¹æ³•ï¼Œå¹¶å°†é€Ÿåº¦æ·»åŠ åˆ°æ¯ä¸ªè¿½è¸ªçš„å¯¹è±¡ï¼ˆçƒå‘˜å’Œè£åˆ¤ï¼‰ä¸­ã€‚è¿™å°±æ˜¯è¿™äº›æ›´æ”¹çš„æ•ˆæœï¼š
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you have troubles adding this new lines of code, you can always visit [the
    projectâ€™s GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch),
    where you can find the complete integrated code. Your video output at this point
    should look like this *(notice that the speed has been added to the label of each
    player)*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨æ·»åŠ è¿™äº›æ–°ä»£ç æ—¶é‡åˆ°é—®é¢˜ï¼Œä½ å¯ä»¥éšæ—¶è®¿é—®[é¡¹ç›®çš„GitHubä»“åº“](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥æ‰¾åˆ°å®Œæ•´çš„é›†æˆä»£ç ã€‚æ­¤æ—¶ï¼Œä½ çš„è§†é¢‘è¾“å‡ºåº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼ˆ*æ³¨æ„é€Ÿåº¦å·²æ·»åŠ åˆ°æ¯ä¸ªçƒå‘˜çš„æ ‡ç­¾ä¸Š*ï¼‰ï¼š
- en: '![](../Images/56fe2cf37374ce478a5baaa91c6c66a7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56fe2cf37374ce478a5baaa91c6c66a7.png)'
- en: 'Sample Clip 07: Tracking Players and Speed'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ç‰‡æ®µ07ï¼šè·Ÿè¸ªçƒå‘˜å’Œé€Ÿåº¦
- en: Finally, letâ€™s add a stats board where we can track the average speed per player
    for each team, along with other metrics such as distance covered and offensive
    pressure in the opponentâ€™s zone.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªç»Ÿè®¡æ¿ï¼Œè·Ÿè¸ªæ¯æ”¯é˜Ÿä¼æ¯ä¸ªçƒå‘˜çš„å¹³å‡é€Ÿåº¦ï¼Œå¹¶æ˜¾ç¤ºå…¶ä»–æ•°æ®ï¼Œä¾‹å¦‚è¡Œè¿›çš„è·ç¦»å’Œåœ¨å¯¹æ–¹åŒºåŸŸå†…çš„è¿›æ”»å‹åŠ›ã€‚
- en: Weâ€™ve already defined the offensive zones and integrated them into our code.
    Now, we need to track how often each player enters their opponentâ€™s zone. To achieve
    this, weâ€™ll implement a method using the [**ray casting algorithm**](https://medium.com/@girishajmera/exploring-algorithms-to-determine-points-inside-or-outside-a-polygon-038952946f87).
    This algorithm checks if a playerâ€™s position is inside the white or yellow teamâ€™s
    offensive zone. It works by drawing an imaginary line from the player to the target
    zone. If the line crosses one border, the player is inside, if it crosses more
    (in our case, two out of four borders), the player is outside. The code then scans
    the entire video to determine each tracked objectâ€™s zone status.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å®šä¹‰äº†è¿›æ”»åŒºåŸŸå¹¶å°†å…¶é›†æˆåˆ°æˆ‘ä»¬çš„ä»£ç ä¸­ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦è·Ÿè¸ªæ¯ä¸ªçƒå‘˜è¿›å…¥å¯¹æ–¹åŒºåŸŸçš„æ¬¡æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªæ–¹æ³•ï¼Œä½¿ç”¨[**å°„çº¿æŠ•å°„ç®—æ³•**](https://medium.com/@girishajmera/exploring-algorithms-to-determine-points-inside-or-outside-a-polygon-038952946f87)ã€‚è¿™ä¸ªç®—æ³•æ£€æŸ¥çƒå‘˜çš„ä½ç½®æ˜¯å¦åœ¨ç™½é˜Ÿæˆ–é»„é˜Ÿçš„è¿›æ”»åŒºåŸŸå†…ã€‚å®ƒé€šè¿‡ä»çƒå‘˜åˆ°ç›®æ ‡åŒºåŸŸç”»ä¸€æ¡è™šæ‹Ÿçº¿æ¥å·¥ä½œã€‚å¦‚æœè¿™æ¡çº¿ç©¿è¿‡ä¸€ä¸ªè¾¹ç•Œï¼Œåˆ™è¡¨ç¤ºçƒå‘˜åœ¨å†…éƒ¨ï¼›å¦‚æœç©¿è¿‡å¤šä¸ªè¾¹ç•Œï¼ˆåœ¨æˆ‘ä»¬è¿™ä¸ªæ¡ˆä¾‹ä¸­æ˜¯ç©¿è¿‡å››ä¸ªè¾¹ç•Œä¸­çš„ä¸¤ä¸ªï¼‰ï¼Œåˆ™è¡¨ç¤ºçƒå‘˜åœ¨å¤–éƒ¨ã€‚ä»£ç æ¥ç€æ‰«ææ•´ä¸ªè§†é¢‘ï¼Œç¡®å®šæ¯ä¸ªè·Ÿè¸ªç‰©ä½“çš„åŒºåŸŸçŠ¶æ€ã€‚
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now weâ€™ll handle the performance metrics by adding a method that displays **average
    player speed**, total **distance covered**, and **offensive pressure** (percentage
    of time spent in the opponentâ€™s zone) on a table format for each team. Using OpenCV,
    weâ€™ll format these metrics into a table overlaid on the video and weâ€™ll incorporate
    a dynamic update mechanism to maintain real-time statistics during gameplay.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†é€šè¿‡æ·»åŠ ä¸€ä¸ªæ–¹æ³•æ¥å¤„ç†è¡¨ç°æŒ‡æ ‡ï¼Œè¯¥æ–¹æ³•å°†åœ¨è¡¨æ ¼æ ¼å¼ä¸­ä¸ºæ¯æ”¯é˜Ÿä¼æ˜¾ç¤º**çƒå‘˜å¹³å‡é€Ÿåº¦**ã€**æ€»è¡Œè¿›è·ç¦»**å’Œ**è¿›æ”»å‹åŠ›**ï¼ˆåœ¨å¯¹æ–¹åŒºåŸŸå†…çš„æ—¶é—´ç™¾åˆ†æ¯”ï¼‰ã€‚ä½¿ç”¨
    OpenCVï¼Œæˆ‘ä»¬å°†è¿™äº›æŒ‡æ ‡æ ¼å¼åŒ–ä¸ºè¦†ç›–åœ¨è§†é¢‘ä¸Šçš„è¡¨æ ¼ï¼Œå¹¶å°†åŠ å…¥åŠ¨æ€æ›´æ–°æœºåˆ¶ï¼Œä»¥ä¿æŒæ¸¸æˆè¿‡ç¨‹ä¸­çš„å®æ—¶ç»Ÿè®¡ã€‚
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order the stats display in the video weâ€™ll have to call the method in the
    **analyze_video method,** so be sure to add this extra lines of code after the
    speed label is defined and just before the output video is processed:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨è§†é¢‘ä¸­æ˜¾ç¤ºç»Ÿè®¡æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦è°ƒç”¨**analyze_videoæ–¹æ³•**ï¼Œå› æ­¤è¯·ç¡®ä¿åœ¨å®šä¹‰é€Ÿåº¦æ ‡ç­¾åã€å¤„ç†è¾“å‡ºè§†é¢‘ä¹‹å‰ï¼Œæ·»åŠ è¿™äº›é¢å¤–çš„ä»£ç è¡Œï¼š
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The distance in meters covered by each player is calculated by dividing their
    speed (measured in meters per second) by the frame rate (frames per second). This
    calculation allows us to estimate how far each player moves between each frame
    change in the video. If everything works well, your final video output should
    look like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªçƒå‘˜æ‰€è¦†ç›–çš„è·ç¦»ï¼ˆå•ä½ï¼šç±³ï¼‰æ˜¯é€šè¿‡å°†å…¶é€Ÿåº¦ï¼ˆä»¥ç±³/ç§’ä¸ºå•ä½ï¼‰é™¤ä»¥å¸§ç‡ï¼ˆä»¥å¸§/ç§’ä¸ºå•ä½ï¼‰æ¥è®¡ç®—çš„ã€‚è¿™ä¸ªè®¡ç®—æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼°ç®—æ¯ä¸ªçƒå‘˜åœ¨è§†é¢‘ä¸­æ¯æ¬¡å¸§å˜åŒ–ä¹‹é—´ç§»åŠ¨çš„è·ç¦»ã€‚å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæœ€ç»ˆçš„è§†é¢‘è¾“å‡ºåº”è¯¥æ˜¯è¿™æ ·çš„ï¼š
- en: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
- en: 'Sample Clip 08: Final Output'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹ç‰‡æ®µ 08ï¼šæœ€ç»ˆè¾“å‡º
- en: Considerations and Future Work
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è€ƒè™‘äº‹é¡¹ä¸æœªæ¥å·¥ä½œ
- en: 'This model is a basic setup of what can be achieved using computer vision to
    track players in an ice hockey game (or any team sport). However, thereâ€™s a lot
    of fine-tuning that can be done to improve it and add new capabilities. Here are
    a few ideas that Iâ€™m working on for a next 2.0 version that you might also consider:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹æ˜¯ä½¿ç”¨è®¡ç®—æœºè§†è§‰è¿½è¸ªå†°çƒæ¯”èµ›ä¸­çƒå‘˜çš„åŸºæœ¬è®¾ç½®ï¼ˆæˆ–ä»»ä½•å›¢é˜Ÿè¿åŠ¨ï¼‰ã€‚ç„¶è€Œï¼Œè¿˜æœ‰è®¸å¤šç²¾ç»†è°ƒä¼˜å¯ä»¥æ”¹è¿›ï¼Œå¹¶ä¸”å¯ä»¥æ·»åŠ æ–°åŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯æˆ‘æ­£åœ¨ç ”ç©¶çš„ä¸€äº›æƒ³æ³•ï¼Œç”¨äºä¸‹ä¸€ç‰ˆæœ¬2.0ï¼Œä½ ä¹Ÿå¯ä»¥è€ƒè™‘è¿™äº›æƒ³æ³•ï¼š
- en: '***The challenge of following the puck:*** Depending on which direction your
    camera is facing and the resolution, tracking the puck is challenging considering
    its size compared to a soccer or basketball ball. But if you achieve this, interesting
    possibilities open up to track performance, such as possession time metrics, goal
    opportunities, or shots data. This also applies also to individual performances;
    in ice hockey, players change significantly more often than in other team sports,
    so tracking each playerâ€™s performance during one period presents a challenge.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***è·Ÿè¸ªå†°çƒçš„æŒ‘æˆ˜ï¼š*** æ ¹æ®ç›¸æœºçš„æœå‘å’Œåˆ†è¾¨ç‡ï¼Œè·Ÿè¸ªå†°çƒæ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå› ä¸ºå®ƒçš„å°ºå¯¸ç›¸è¾ƒäºè¶³çƒæˆ–ç¯®çƒçƒæ¥è¯´è¾ƒå°ã€‚ä½†å¦‚æœä½ èƒ½å¤Ÿå®ç°è¿™ä¸€ç‚¹ï¼Œä¾¿èƒ½å¼€å¯ä¸€äº›æœ‰è¶£çš„å¯èƒ½æ€§æ¥è¿½è¸ªè¡¨ç°ï¼Œä¾‹å¦‚æ§çƒæ—¶é—´ã€è¿›æ”»æœºä¼šæˆ–å°„é—¨æ•°æ®ã€‚è¿™åŒæ ·é€‚ç”¨äºä¸ªåˆ«çƒå‘˜çš„è¡¨ç°ï¼›åœ¨å†°çƒä¸­ï¼Œçƒå‘˜çš„æ¢äººé¢‘ç‡è¿œé«˜äºå…¶ä»–å›¢é˜Ÿè¿åŠ¨ï¼Œå› æ­¤ï¼Œåœ¨ä¸€ä¸ªæ—¶æ®µå†…è¿½è¸ªæ¯ä¸ªçƒå‘˜çš„è¡¨ç°ä¹Ÿæ˜¯ä¸€ç§æŒ‘æˆ˜ã€‚'
- en: '***Compute resources, Oh why compute!*** I ran all the code on a CPU arrangement
    but faced issues *(sometimes resulting in blue screens ğŸ˜¥)* due to running out
    of memory during the design process (consider using a CUDA setup). Our sample
    video is about 40 seconds long and initially 5 MB in size, but after running the
    model, the output increases to up to 34 MB. Imagine the size for a full 20-minute
    game period. So, you should consider compute resources and storage when scaling
    up.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '***è®¡ç®—èµ„æºï¼Œå“¦ï¼Œä¸ºä»€ä¹ˆæ˜¯è®¡ç®—ï¼*** æˆ‘åœ¨ä¸€ä¸ª CPU é…ç½®ä¸Šè¿è¡Œäº†æ‰€æœ‰ä»£ç ï¼Œä½†ç”±äºåœ¨è®¾è®¡è¿‡ç¨‹ä¸­å†…å­˜ä¸è¶³ï¼ˆæœ‰æ—¶å¯¼è‡´è“å± ğŸ˜¥ï¼‰ï¼Œé‡åˆ°äº†é—®é¢˜ï¼ˆå»ºè®®ä½¿ç”¨
    CUDA è®¾ç½®ï¼‰ã€‚æˆ‘ä»¬çš„ç¤ºä¾‹è§†é¢‘å¤§çº¦ 40 ç§’é•¿ï¼Œæœ€åˆä¸º 5 MBï¼Œä½†åœ¨è¿è¡Œæ¨¡å‹åï¼Œè¾“å‡ºæ–‡ä»¶çš„å¤§å°å¢åŠ åˆ° 34 MBã€‚æƒ³è±¡ä¸€ä¸‹å®Œæ•´çš„ 20 åˆ†é’Ÿæ¯”èµ›æœŸé—´çš„å¤§å°ã€‚æ‰€ä»¥ï¼Œåœ¨æ‰©å±•æ—¶ï¼Œä½ åº”è¯¥è€ƒè™‘è®¡ç®—èµ„æºå’Œå­˜å‚¨ã€‚'
- en: '***Donâ€™t underestimate MLOps:*** To deploy and scale rapidly, we need Machine
    Learning pipelines that are efficient, support frequent execution, and are reliable.
    This involves considering a **Continuous Integration-Deployment-Training approach**.
    Our use case has been built for a specific scenario, but what if conditions change,
    such as the camera direction or jersey colors? To scale up, we must adopt a CI/CD/CT
    mindset.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä¸è¦ä½ä¼° MLOpsï¼š*** è¦å¿«é€Ÿéƒ¨ç½²å’Œæ‰©å±•ï¼Œæˆ‘ä»¬éœ€è¦é«˜æ•ˆçš„æœºå™¨å­¦ä¹ ç®¡é“ï¼Œæ”¯æŒé¢‘ç¹æ‰§è¡Œï¼Œå¹¶ä¸”å¯é ã€‚è¿™éœ€è¦è€ƒè™‘**æŒç»­é›†æˆ-éƒ¨ç½²-è®­ç»ƒæ–¹æ³•**ã€‚æˆ‘ä»¬çš„ç”¨ä¾‹æ˜¯ä¸ºç‰¹å®šåœºæ™¯æ„å»ºçš„ï¼Œä½†å¦‚æœæ¡ä»¶å‘ç”Ÿå˜åŒ–ï¼Œæ¯”å¦‚æ‘„åƒå¤´æ–¹å‘æˆ–çƒè¡£é¢œè‰²å˜åŒ–æ€ä¹ˆåŠï¼Ÿä¸ºäº†æ‰©å±•ï¼Œæˆ‘ä»¬å¿…é¡»é‡‡çº³
    CI/CD/CT æ€ç»´æ¨¡å¼ã€‚'
- en: I hope you found this computer vision project interesting, you can access the
    complete code in [this GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).
    And if you want to support the development of inline and ice hockey in the region,
    follow the [APHL](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    *(we are always in need of used equipment youâ€™d like to donate for young players
    and working on building our first official hockey rink)*, and worldwide, follow
    and support the [Friendship League](https://friendshipleague.org/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›ä½ å¯¹è¿™ä¸ªè®¡ç®—æœºè§†è§‰é¡¹ç›®æ„Ÿå…´è¶£ï¼Œä½ å¯ä»¥åœ¨[è¿™ä¸ª GitHub ä»“åº“](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)è®¿é—®å®Œæ•´çš„ä»£ç ã€‚å¦‚æœä½ æƒ³æ”¯æŒè¯¥åœ°åŒºçš„å†°çƒå’Œå†°çƒè¿åŠ¨å‘å±•ï¼Œå¯ä»¥å…³æ³¨[APHL](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    *(æˆ‘ä»¬æ€»æ˜¯éœ€è¦æ‚¨æèµ çš„äºŒæ‰‹è®¾å¤‡ï¼Œä¾›å¹´è½»çƒå‘˜ä½¿ç”¨ï¼Œå¹¶æ­£åœ¨å»ºè®¾æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå®˜æ–¹å†°çƒåœº)*ï¼Œå…¨çƒèŒƒå›´å†…ï¼Œä¹Ÿå¯ä»¥å…³æ³¨å¹¶æ”¯æŒ[Friendship League](https://friendshipleague.org/)ã€‚
- en: '***Did I miss anything?*** Your suggestions are always welcome. Letâ€™s keep
    the conversation going!'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘æ¼æ‰äº†ä»€ä¹ˆå—ï¼Ÿ*** æ¬¢è¿æå‡ºå»ºè®®ã€‚è®©æˆ‘ä»¬ç»§ç»­äº¤æµï¼'
