- en: 'Spicing up Ice Hockey with AI: Player Tracking with Computer Vision'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用AI为冰球增添趣味：利用计算机视觉进行球员追踪
- en: 原文：[https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09](https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09](https://towardsdatascience.com/spicing-up-ice-hockey-with-ai-player-tracking-with-computer-vision-ce9ceec9122a?source=collection_archive---------0-----------------------#2024-07-09)
- en: '![](../Images/c26f194ba6b3bcdd31b1a7dfbad44347.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c26f194ba6b3bcdd31b1a7dfbad44347.png)'
- en: Using PyTorch, computer vision techniques, and a Convolutional Neural Network
    (CNN), I worked on a model that tracks players, teams and basic performance statistics
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch、计算机视觉技术和卷积神经网络（CNN），我开发了一个模型，可以追踪球员、球队以及基本的表现统计数据
- en: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[![Raul
    Vizcarra Chirinos](../Images/9f507c6b9542809b9a32ab185e953ca1.png)](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    [Raul Vizcarra Chirinos](https://medium.com/@raul.vizcarrach?source=post_page---byline--ce9ceec9122a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    ·30 min read·Jul 9, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce9ceec9122a--------------------------------)
    ·30分钟阅读·2024年7月9日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Nowadays, I don’t play hockey as much as I want to, but it’s been a part of
    me since I was a kid. Recently, I had the chance to help with the referee table
    and keep some stats in the first Ice Hockey Tournament in Lima (3 on 3). This
    event involved an extraordinary effort of the the Peruvian Inline Hockey Association
    (APHL) and a kind visit from the [Friendship League](https://friendshipleague.org/).
    To add an AI twist, I used **PyTorch**, **computer vision** techniques, and a
    **Convolutional Neural Network (CNN)** to build a model that tracks players and
    teams and gathers some basic performance stats.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我不像以前那样频繁地打冰球了，但从小冰球就是我生活的一部分。最近，我有机会在利马举办的首届冰球锦标赛（3对3）中，协助裁判台并记录一些统计数据。此次活动得到了秘鲁滑冰曲棍球协会（APHL）的巨大支持，并且[友谊联盟](https://friendshipleague.org/)也亲切地参与其中。为了加入一些AI元素，我使用了**PyTorch**、**计算机视觉**技术和**卷积神经网络（CNN）**来构建一个模型，追踪球员和球队，并收集一些基本的表现数据。
- en: This article aims to be a **quick guide** to designing and deploying the model.
    Although the model still needs some fine-tuning, I hope it can help anyone introduce
    themselves to the interesting world of computer vision applied to sports. I would
    like to acknowledge and thank the [Peruvian Inline Hockey Association (APHL)](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    for allowing me to use a 40-second video sample of the tournament for this project
    *(you can find the video input sample in the* [*project’s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*).*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在成为设计和部署该模型的**快速指南**。尽管模型仍需进行一些微调，但我希望它能帮助任何人入门计算机视觉在体育中的应用。我还要特别感谢并感谢[秘鲁滑冰曲棍球协会（APHL）](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)允许我使用比赛的40秒视频样本进行此项目（*你可以在*[*项目的GitHub仓库*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*找到视频输入样本*）。
- en: The Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Before moving on with the project, I did some quick research to find a baseline
    from which I could work and avoid “reinventing the wheel”. I found that in terms
    of using computer vision to track players, there is a lot of interesting work
    on football *(not surprising, being the most popular team sport in the world)*.
    However, I didn’t find many resources for ice hockey. [Roboflow](https://universe.roboflow.com/search?q=hockey)
    has some interesting pre-trained models and datasets for training your own, but
    working with a hosted model presented some latency issues that I will explain
    further. In the end, I leveraged the soccer material for reading the video frames
    and obtaining the individual track IDs, following the basic principles and tracking
    method approach explained in [this tutorial](https://youtu.be/neBZ6huolkg?feature=shared)
    *(If you are interested in gaining a better understanding of some basic computer
    vision techniques, I suggest watching at least the first hour and a half of the
    tutorial).*
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进行项目之前，我做了一些快速研究，找到一个可以作为基准的方向，以避免“重新发明轮子”。我发现，在使用计算机视觉追踪球员方面，有很多有趣的研究成果，尤其是在足球领域（*这并不令人惊讶，足球是世界上最受欢迎的团队运动*）。然而，我并没有找到很多冰球方面的资源。[Roboflow](https://universe.roboflow.com/search?q=hockey)
    提供了一些有趣的预训练模型和数据集，用于训练自己的模型，但使用托管模型时出现了一些延迟问题，稍后我会进一步解释。最终，我借用了足球相关的资料来读取视频帧并获得单独的跟踪
    ID，遵循了[这个教程](https://youtu.be/neBZ6huolkg?feature=shared)中解释的基本原理和跟踪方法（*如果你有兴趣更好地理解一些基本的计算机视觉技术，我建议至少观看前一个半小时的教程*）。
- en: With the tracking IDs covered, I then built my own path. As we walk through
    this article, we’ll see how the project evolves from a simple object detection
    task to a model that fully detects players, teams, and delivers some basic performance
    metrics *(sample clips from 01 to 08, author’s own creation).*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在覆盖了跟踪 ID 后，我开始构建自己的路径。在本文的过程中，我们将看到这个项目是如何从一个简单的物体检测任务发展成一个能够全面检测球员、球队并提供一些基本表现指标的模型的（*示例剪辑从
    01 到 08，作者自创*）。
- en: '![](../Images/e100ca9ea1c1eb7ab22675a9de0f18ef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e100ca9ea1c1eb7ab22675a9de0f18ef.png)'
- en: Model Architecture. Author’s own creation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构。作者自创
- en: The Tracking Mechanism
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪机制
- en: 'The tracking mechanism is the backbone of the model. It ensures that each detected
    object within the video is identified and assigned a unique identifier, maintaining
    this identity across each frame. The main components of the tracking mechanism
    are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪机制是模型的核心。它确保视频中的每个检测到的物体都能被识别并分配一个唯一的标识符，保持该身份在每一帧中的一致性。跟踪机制的主要组成部分包括：
- en: '**YOLO (You Only Look Once):** It’s a powerful real-time object detection algorithm
    originally introduced in 2015 in the paper “[You Only Look Once: Unified, Real-Time
    Object Detection](https://arxiv.org/abs/1506.02640)”. Stands out for its speed
    and its versatility in detecting around 80 pre-trained classes *(it’s important
    to note that it can also be trained on custom datasets to detect specific objects)*.
    For our use case, we will rely on YOLOv8x, a computer vision model built by Ultralytics
    based on previous YOLO versions. You can download it [here](https://github.com/ultralytics/ultralytics).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**YOLO（You Only Look Once）：** 它是一种强大的实时物体检测算法，最初在 2015 年的论文“[You Only Look
    Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)”中提出。它以速度和在大约
    80 个预训练类别中的通用性为特点（*值得注意的是，它还可以在自定义数据集上进行训练，以检测特定物体*）。对于我们的使用案例，我们将依赖 YOLOv8x，这是一种由
    Ultralytics 基于之前版本的 YOLO 构建的计算机视觉模型。你可以在[这里](https://github.com/ultralytics/ultralytics)下载它。'
- en: '**ByteTrack Tracker:** To understand ByteTrack, we have to understand MOT (Multiple
    Object Tracking), which involves tracking the movements of multiple objects over
    time in a video sequence and linking those objects detected in a current frame
    with corresponding objects in previous frames. To accomplish this, we will use
    ByteTrack *( introduced in 2021 in the paper “*[*ByteTrack: Multi-Object Tracking
    by Associating Every Detection Box*](https://arxiv.org/abs/2110.06864)*”)*. To
    implement the ByteTrack tracker and assign track IDs to detected objects, we will
    rely on the Python’s supervision library.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ByteTrack 跟踪器：** 要理解 ByteTrack，我们必须先了解多目标跟踪（MOT，Multiple Object Tracking），它涉及在视频序列中追踪多个物体的运动，并将当前帧中检测到的物体与前一帧中的相应物体进行关联。为了实现这一目标，我们将使用
    ByteTrack（*2021年在论文“[*ByteTrack: Multi-Object Tracking by Associating Every Detection
    Box*](https://arxiv.org/abs/2110.06864)”中提出*）。为了实现 ByteTrack 跟踪器并为检测到的物体分配轨迹 ID，我们将依赖
    Python 的 supervision 库。'
- en: '**OpenCV:** is a well-known library for various computer vision tasks in Python.
    For our use case, we will rely on [OpenCV](https://opencv.org/) to visualize and
    annotate video frames with bounding boxes and text for each detected object.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**OpenCV：** 是一个广泛应用于各种计算机视觉任务的 Python 库。对于我们的用例，我们将依赖[OpenCV](https://opencv.org/)来可视化并标注视频帧中的边界框和每个检测物体的文本信息。'
- en: 'In order to build our tracking mechanism, we’ll begin with these initial two
    steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的追踪机制，我们将从以下两步开始：
- en: Deploying the YOLO model with ByteTrack to detect objects (in our case, players)
    and assign unique track IDs.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ByteTrack 部署 YOLO 模型来检测物体（在我们的例子中是球员）并分配唯一的追踪 ID。
- en: Initializing a dictionary to store object tracks in a pickle (pkl) file. This
    will be extremely useful to avoid executing the video frame-by-frame object detection
    process each time we run the code, and save significant time.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化一个字典，将物体追踪信息存储在一个 pickle（pkl）文件中。这将极大地帮助我们避免每次运行代码时都需要逐帧执行视频物体检测过程，节省大量时间。
- en: 'For the following step, these are the Python packages that we’ll need:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的步骤，我们需要以下 Python 包：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we’ll specify our libraries and the path for our sample video file and
    pickle file (if it exists; if not, the code will create one and save it in the
    same path):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指定我们的库和样本视频文件以及 pickle 文件的路径（如果存在；如果没有，代码会创建一个并将其保存在相同路径下）：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let’s go ahead and define our tracking mechanism *(you can find the video
    input sample in the* [*project’s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*)*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续定义我们的追踪机制*（你可以在[*项目的 GitHub 仓库*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)中找到视频输入示例）*：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The method begins by initializing the YOLO model and the ByteTrack tracker.
    Next, each frame is processed in batches of 20, using the YOLO model to detect
    and collect objects in each batch. If the pickle file is available in its path,
    it precomputes the tracks from the file. If the pickle file is not available *(you
    are running the code for the first time or have erased a previous pickle file)*,
    the **get_object_tracks** converts each detection into the required format for
    ByteTrack, updates the tracker with these detections, and stores the tracking
    information in a new pickle file in the designated path.Finally, iterations are
    made over each frame, drawing bounding boxes and track IDs for each detected object.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首先初始化 YOLO 模型和 ByteTrack 追踪器。接下来，每一帧以 20 帧为一批进行处理，使用 YOLO 模型检测并收集每批中的物体。如果
    pickle 文件存在于路径中，它会从文件中预计算出追踪信息。如果 pickle 文件不存在*（你是第一次运行代码或者删除了之前的 pickle 文件）*，**get_object_tracks**
    将每个检测转换为 ByteTrack 所需的格式，用这些检测更新追踪器，并将追踪信息保存在指定路径的一个新 pickle 文件中。最后，代码会迭代每一帧，为每个检测到的物体绘制边界框和追踪
    ID。
- en: 'To execute the tracker and save a new output video with bounding boxes and
    track IDs, you can use the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行追踪器并保存带有边界框和追踪 ID 的新输出视频，你可以使用以下代码：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If everything in your code worked correctly, you should expect a video output
    similar to the one shown in **sample clip 01**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的代码一切正常，你应该会得到一个类似于**示例剪辑 01**的视频输出。
- en: '![](../Images/d5caaaa073f19f9f874cb779de45aa22.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5caaaa073f19f9f874cb779de45aa22.png)'
- en: 'Sample Clip 01: Basic tracking mechanism ( Objects and Tracking IDs)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 示例剪辑 01：基础追踪机制（物体与追踪 ID）
- en: '**TIP #01:** Don’t underestimate your compute power! When running the code
    for the first time, expect the frame processing to take some time, depending on
    your compute capacity. For me, it took between 45 to 50 minutes using only a CPU
    setup (consider CUDA as an option). The YOLOv8x tracking mechanism, while powerful,
    demands significant compute resources (at times, my memory hit 99%, fingers crossed
    it didn’t crash!🙄). If you encounter issues with this version of YOLO, lighter
    models are available on [Ultralytics’ GitHub](https://github.com/ultralytics/ultralytics)
    to balance accuracy and compute capacity.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示 #01：** 不要低估你的计算能力！第一次运行代码时，预计帧处理可能会花费一些时间，这取决于你的计算能力。对我来说，使用 CPU 配置处理大约需要
    45 到 50 分钟（可以考虑使用 CUDA 作为选项）。尽管 YOLOv8x 追踪机制非常强大，但它需要相当大的计算资源（有时我的内存占用率达到了 99%，希望它没有崩溃！🙄）。如果你遇到
    YOLO 的此版本问题，可以访问[Ultralytics’ GitHub](https://github.com/ultralytics/ultralytics)，那里有更轻量的模型来平衡准确性和计算能力。'
- en: The Ice Rink
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冰球场
- en: As you’ve seen from the first step, we have some challenges. Firstly, as expected,
    the model picks up all moving objects; players, referees, even those outside the
    rink. Secondly, those red bounding boxes can make tracking players a bit unclear
    and not very neat for presentation. In this section, we’ll focus on narrowing
    our detection to objects within the rink only. Plus, we’ll swap out those bounding
    boxes for ellipses at the bottom, ensuring clearer visibility.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从第一步中看到的，我们遇到了一些挑战。首先，正如预期的那样，模型捕捉到所有移动物体，包括球员、裁判，甚至冰场外的物体。其次，这些红色边界框会让跟踪球员变得有些不清晰，也不太适合展示。在这一部分中，我们将专注于将检测范围缩小到仅冰场内的物体。同时，我们会将底部的边界框替换为椭圆，以确保更清晰的可视性。
- en: 'Let’s switch from using boxes to using ellipses first. To accomplish this,
    we’ll simply add a new method above the labels and bounding boxes method in our
    existing code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从使用矩形框切换到使用椭圆。为了实现这一点，我们只需在现有代码中的标签和边界框方法上方添加一个新的方法：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We’ll also need to update the annotation step by replacing the bounding boxes
    and IDs with a call to the ellipse method:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更新注释步骤，通过调用椭圆方法来替换边界框和ID：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With these changes, your output video should look much neater, as shown in **sample
    clip 02**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些修改，你的视频输出应该看起来更整洁，如**示例剪辑02**所示。
- en: '![](../Images/0e17fb82b7df8f4b41e25e34b5da15d6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e17fb82b7df8f4b41e25e34b5da15d6.png)'
- en: 'Sample Clip 02: Replacing bounding boxes with ellipses'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例剪辑02：用椭圆替代边界框
- en: Now, to work with the rink boundaries, we need to have some basic knowledge
    of resolution in computer vision. In our use case, we are working with a 720p
    (1280x720 pixels) format, which means that each frame or image we process has
    dimensions of 1280 pixels (width) by 720 pixels (height).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了处理冰场边界，我们需要对计算机视觉中的分辨率有一些基本了解。在我们的使用场景中，我们使用的是720p（1280x720像素）格式，这意味着我们处理的每一帧图像的尺寸为1280像素（宽度）乘720像素（高度）。
- en: '***What does it mean to work with a 720p (1280x720 pixels) format?*** It means
    that the image is made up of 1280 pixels horizontally and 720 pixels vertically.
    Coordinates in this format start at (0, 0) in the top-left corner of the image,
    with the x-coordinate increasing as you move right and the y-coordinate increasing
    as you move down. These coordinates are used to mark specific areas in the image,
    like using (x1, y1) for the top-left corner and (x2, y2) for the bottom-right
    corner of a box. Understanding this will helps us measure distances and speeds,
    and decide where in the video we want to focus our analysis.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '***使用720p（1280x720像素）格式意味着什么？*** 这意味着图像由1280个水平像素和720个垂直像素组成。在这种格式下，坐标从图像的左上角（0,
    0）开始，x坐标随着向右移动而增加，y坐标随着向下移动而增加。这些坐标用于标记图像中的特定区域，比如使用（x1, y1）表示左上角，使用（x2, y2）表示矩形框的右下角。理解这一点有助于我们测量距离和速度，并决定在视频中关注的分析区域。'
- en: 'That said, we will start marking the frame borders with green lines using the
    following code:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们将使用以下代码开始标记帧边框为绿色线条：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result should be a green rectangle as shown in (a) in **sample clip 03**.
    But in order to track only the moving objects within the rink we would need a
    delimitation more similar to the one in (b) .
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是一个绿色矩形，如**示例剪辑03**中（a）所示。但为了只追踪冰场内的移动物体，我们需要一个更像（b）中的边界。
- en: '![](../Images/e6de1323015e8fa64b0a9667df89ff1c.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6de1323015e8fa64b0a9667df89ff1c.png)'
- en: 'Figure 03: Border Definition for the Ice Rink (Author’s own creation)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图03：冰场边界定义（作者自创）
- en: 'Getting (b) right is like an iterative process of trial and error, where you
    test different coordinates until you find the boundaries that best fit your model.
    Initially, I aimed to match the rink borders exactly. However, the tracking system
    struggled near the edges. To improve accuracy, I expanded the boundaries slightly
    to ensure all tracking objects within the rink were captured while excluding those
    outside. The outcome, shown in (b), was the best I could get *(you could still
    work better scenarios)* defined by these coordinates:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 得到（b）的正确边界就像一个反复试验的过程，你需要测试不同的坐标，直到找到最适合你模型的边界。最初，我的目标是完全匹配冰场边界。然而，跟踪系统在边缘附近存在困难。为了提高准确性，我稍微扩大了边界，以确保所有冰场内的跟踪物体都被捕捉到，同时排除场外的物体。最终的结果，如（b）所示，是我能得到的最好的结果*(你仍然可以尝试更好的情况)*，由这些坐标定义：
- en: 'Bottom Left Corner: (-450, 710)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左下角： (-450, 710)
- en: 'Bottom Right Corner: (2030, 710)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右下角： (2030, 710)
- en: 'Upper Left Corner: (352, 61)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上角： (352, 61)
- en: 'Upper Right Corner: (948, 61)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右上角： (948, 61)
- en: 'Finally, we will define two additional areas: the o**ffensive zones** for both
    the White and Yellow teams *(where each team aims to score)*. This will enable
    us to gather some basic positional statistics and pressure metrics for each team
    within their opponent’s zone.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将定义两个额外的区域：白队和黄队的**进攻区**（*每个队伍的目标区域*）。这将使我们能够收集每个队伍在其对手区域内的一些基本位置统计数据和压力指标。
- en: '![](../Images/b8ea5a739ce6098b9345c2d7cfe8b332.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8ea5a739ce6098b9345c2d7cfe8b332.png)'
- en: 'Figure 04: Offensive Zones ( Author’s own creation)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图04：进攻区（作者自创）
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will set aside these coordinates for now and explain in the next step how
    we’ll classify each team. Then, we’ll bring it all together into our original
    tracking method.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在先暂时搁置这些坐标，并在下一步中解释我们将如何对每个团队进行分类。然后，我们将把所有内容汇总到我们最初的跟踪方法中。
- en: Using Deep Learning for Team Prediction
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习进行团队预测
- en: Over 80 years have passed since the release of *“*[*A Logical Calculus of the
    Ideas Immanent in Nervous Activity*](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)*”,*
    the paper written by Warren McCulloch and Walter Pitts in 1943, which set the
    solid ground for early neural network research. Later, in 1957, the mathematical
    model of a simplified neuron *(receiving inputs, applying weights to these inputs,
    summing them up, and outputting a binary result)* inspired [Frank Rosenblatt to
    build the Mark I](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon).
    This was the first hardware implementation designed to demonstrate the concept
    of a [perceptron](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf),
    a neural network model capable of learning from data to make binary classifications.
    Since then, the quest to make computers think like us hasn’t slowed down. If this
    is your first deep dive into Neural Networks, or if you want to refresh and strengthen
    your knowledge, I recommend reading this [series of articles by Shreya Rao](https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640)
    as a great starting point for deep learning. Additionally, you can access my collection
    of stories (different contributors) [that I’ve gathered here](https://medium.com/@raul.vizcarrach/list/neural-networks-098e9b594f19),
    and which you might find useful.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自从1943年Warren McCulloch和Walter Pitts发表了《*[*神经活动中固有思想的逻辑演算*](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)*》一文以来，已经过去了80多年。这篇论文为早期的神经网络研究奠定了坚实的基础。后来，在1957年，一个简化的神经元数学模型（*接收输入、对这些输入应用权重、对其求和并输出二进制结果*）启发了[Frank
    Rosenblatt构建了Mark I](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)。这是第一个硬件实现，旨在展示[感知机](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)的概念，这是一种能够从数据中学习并进行二分类的神经网络模型。从那时起，让计算机像我们一样思考的追求就没有停歇。如果这是你第一次深入学习神经网络，或者你想要刷新并巩固你的知识，我推荐阅读[Shreya
    Rao的这系列文章](https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640)，作为深度学习的一个很好的起点。此外，你还可以访问我收集的[这系列故事（不同的贡献者）](https://medium.com/@raul.vizcarrach/list/neural-networks-098e9b594f19)，你可能会发现它们有用。
- en: '***Why choose a Convolutional Neural Network (CNN)?*** Honestly, it wasn’t
    my first choice. Initially, I tried building a model with [LandingAI](https://landing.ai/),
    a user-friendly platform for cloud deployment and [Python connection through APIs](https://landing.ai/blog/build-your-custom-computer-vision-app-with-python-library).
    However, latency issues appeared *(over 1,000 frames to process online)*. Similar
    latency problems occurred with pre-trained models in [Roboflow](https://universe.roboflow.com/),
    despite their quality datasets and pre-trained models. Realizing the need to run
    it locally, I tried an MSE-based method to classify jersey colors for team and
    referee detection. While it sounded like the final solution, it showed low accuracy.
    After days of trial and error, I switched to CNNs. Among different deep learning
    approaches, CNNs are well-suited for object detection, unlike LSTM or RNN, which
    are better fit for sequential data like language transcription or translation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***为什么选择卷积神经网络（CNN）？*** 说实话，这并不是我最初的选择。最初，我尝试使用[LandingAI](https://landing.ai/)，一个适合云部署的用户友好平台，并且支持通过[Python
    API连接](https://landing.ai/blog/build-your-custom-computer-vision-app-with-python-library)。然而，出现了延迟问题（*需要处理超过1,000帧的数据*）。即便是在[Roboflow](https://universe.roboflow.com/)的预训练模型中，尽管它们提供了高质量的数据集和模型，仍然遇到了类似的延迟问题。意识到必须在本地运行后，我尝试了基于均方误差（MSE）的方法来分类队伍和裁判的球衣颜色。尽管这种方法看似是最终解决方案，但其准确性较低。经过几天的反复试验，我最终转向了CNN。在众多深度学习方法中，CNN非常适合进行物体检测，而LSTM或RNN更适用于像语言转录或翻译等序列数据。'
- en: 'Before diving into the code, let’s cover some basic concepts about its architecture:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究代码之前，让我们先了解一些关于其架构的基本概念：
- en: '**Sample Dataset for learning:** The dataset has been classified into three
    classes: **Referee**, **Team_Away** (White jersey players), and **Team_Home**
    (Yellow jersey players). A sample of each class has been divided into two sets:
    training data and validation data. The training data will be used by the CNN in
    each iteration (Epoch) to “learn” patterns across multiple layers. The validation
    data will be used at the end of each iteration to evaluate the model’s performance
    and measure how well it generalizes to new data. Creating the sample dataset wasn’t
    too hard; it took me around 30 to 40 minutes to crop sample images from each class
    from the video and organize them into subdirectories. I managed to create a sample
    dataset of approximately 90 images that you can find in the [project’s GitHub
    repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习的样本数据集：** 数据集已被分为三类：**裁判**、**客队**（白色球衣的球员）和**主队**（黄色球衣的球员）。每一类的样本被分为两个子集：训练数据和验证数据。训练数据将在每次迭代（Epoch）中被CNN使用，用以“学习”多个层次中的模式。验证数据将在每次迭代结束时用来评估模型的表现，并衡量模型对新数据的泛化能力。创建样本数据集并不困难；我大约花了30到40分钟的时间，从视频中裁剪出每一类的样本图像并将它们整理到子目录中。我成功创建了一个约90张图像的样本数据集，你可以在[项目的GitHub仓库](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)中找到。'
- en: '**How does the model learn?:** Input data moves through each layer of the neural
    network, which can have one or multiple layers linked together to make predictions.
    Every layer uses an activation function that processes data to make predictions
    or introduce changes to the data. Each connection between these layers has a weight,
    which determines how much influence one layer’s output has on the next. The goal
    is to find the right combination of these weights that minimize mistakes when
    predicting outcomes. Through a process called backpropagation and a loss function,
    the model adjusts these weights to reduce errors and improve accuracy. This process
    repeats in what’s called an **Epoch (forward pass + backpropagation)**, with the
    model getting better at making predictions in each cycle as it learns from its
    mistakes.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型是如何学习的？** 输入数据会通过神经网络的每一层，每一层可以是一个或多个相互连接的层，用来进行预测。每一层都使用激活函数来处理数据，从而进行预测或对数据进行更改。这些层之间的每个连接都有一个权重，决定了一个层的输出对下一个层的影响程度。目标是找到这些权重的正确组合，以最小化预测结果的错误。通过一个叫做反向传播的过程和损失函数，模型会调整这些权重，以减少误差并提高准确性。这个过程会在所谓的**Epoch（前向传播
    + 反向传播）**中重复进行，随着每个周期模型从错误中学习，它在预测上的表现也会逐渐变好。'
- en: '**Activation Function:** As mentioned before, the activation function plays
    an important role in the model’s learning process. I chose **ReLU (Rectified Linear
    Unit)** because it is known for being computationally efficient and mitigating
    what is called the vanishing gradient problem *(where networks with multiple layers
    may stop learning effectively).* While ReLU works well, [other functions](https://www.v7labs.com/blog/neural-networks-activation-functions)
    like **sigmoid**, **tanh**, or **swish** also have their uses depending on how
    complex the network is.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数:** 如前所述，激活函数在模型学习过程中扮演着重要角色。我选择了**ReLU（修正线性单元）**，因为它在计算上非常高效，并能缓解所谓的消失梯度问题*（即多层网络可能无法有效学习）*。虽然
    ReLU 工作得很好，[其他函数](https://www.v7labs.com/blog/neural-networks-activation-functions)如**sigmoid**、**tanh**或**swish**也有其应用，具体取决于网络的复杂性。'
- en: '**Epochs:** Setting the right number of epochs involves experimentation. You
    should take into account factors such as the complexity of the dataset, the architecture
    of your CNN model, and computational resources. In most cases, it is best to monitor
    the model’s performance in each iteration and stop training when improvements
    become minimal to prevent overfitting. Given my small training dataset, **I decided
    to start with 10 epochs as a baseline**. However, adjustments may be necessary
    in other scenarios based on metric performance and validation results.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数（Epochs）:** 设置合适的训练轮数需要实验。你应该考虑数据集的复杂性、CNN 模型的架构和计算资源等因素。在大多数情况下，最好在每次迭代中监控模型的表现，并在改进变得微乎其微时停止训练，以避免过拟合。考虑到我小的训练数据集，**我决定以
    10 轮为基准**开始。然而，在其他情况下，根据指标表现和验证结果，可能需要做出调整。'
- en: '**Adam (Adaptive Moment Estimation):** Ultimately, the goal is to reduce the
    error between predicted and true outputs. As mentioned before, backpropagation
    plays a key role here by adjusting and updating neural network weights to improve
    predictions over time. While backpropagation handles weight updates based on gradients
    from the loss function, the Adam algorithm enhances this process by dynamically
    adjusting the learning rate to gradually minimize the error or loss function.
    In other words, it fine-tunes how quickly the model learns.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam（自适应矩估计）:** 最终目标是减少预测输出与真实输出之间的误差。如前所述，反向传播在此过程中起着关键作用，通过调整和更新神经网络权重来随着时间的推移改进预测。反向传播基于损失函数的梯度处理权重更新，而
    Adam 算法通过动态调整学习率来进一步优化这一过程，从而逐步减少误差或损失函数。换句话说，它微调了模型学习的速度。'
- en: 'That said in order to run our CNN model we will need the following Python packages:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，为了运行我们的 CNN 模型，我们需要以下 Python 包：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Tip-02:** Ensure that PyTorch it’s installed properly. All my tools are set
    up in an Anaconda environment, and when I installed PyTorch, at first, it seemed
    that it was set up correctly. However, some issues appeared while running some
    libraries. Initially, I thought it was the code, but after several revisions and
    no success, I had to reinstall Anaconda and install PyTorch in a clean environment,
    and with that, problem fixed!'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**提示-02:** 确保 PyTorch 正确安装。我所有的工具都在 Anaconda 环境中设置，当我安装 PyTorch 时，一开始看起来它似乎正确安装了。然而，在运行一些库时出现了一些问题。最初，我以为是代码的问题，但经过多次修改仍然没有成功，我只好重新安装
    Anaconda，并在干净的环境中重新安装 PyTorch，问题就这样解决了！'
- en: 'Next, we’ll specify our libraries and the path of our sample dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指定我们的库和样本数据集的路径：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: First, we will ensure each picture is equally sized (resize to 150x150 pixels),
    then convert it to a format that the code can understand (in PyTorch, input data
    is typically represented as Tensor objects). Finally, we will adjust the colors
    to make them easier for the model to work with (normalize) and set up a procedure
    to load the images. These steps together help prepare the pictures and organize
    them so the model can effectively start learning from them, avoiding deviations
    caused by data format.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将确保每张图片的大小一致（调整为 150x150 像素），然后将其转换为代码能够理解的格式（在 PyTorch 中，输入数据通常表示为 Tensor
    对象）。最后，我们将调整颜色，以便模型更容易处理（归一化），并设置加载图片的程序。这些步骤共同帮助准备图片，并将它们整理好，使模型能够有效地从中学习，避免因数据格式导致的偏差。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we’ll define our CNN’s architecture:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义 CNN 的架构：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You will notice that our CNN model has three layers (conv1, conv2, conv3). The
    data begins in the convolutional layer (conv), where the activation function (ReLU)
    is applied. This function enables the network to learn complex patterns and relationships
    in the data. Following this, the pooling layer is activated. ***What is Max Pooling?***
    It’s a technique that reduces the image size while retaining important features,
    which helps in efficient training and optimizes memory resources. This process
    repeats across conv1 to conv3\. Finally, the data passes through fully connected
    layers (fc1, fc2) for final classification (or decision-making).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们的CNN模型有三层（conv1，conv2，conv3）。数据首先进入卷积层（conv），在这里应用了激活函数（ReLU）。该函数使得网络能够学习数据中的复杂模式和关系。接着，池化层被激活。***什么是最大池化？***它是一种减少图像大小的技术，同时保留重要特征，有助于高效训练并优化内存资源。这个过程在conv1到conv3之间重复进行。最后，数据通过全连接层（fc1，fc2）进行最终分类（或决策）。
- en: As the next step, we initialize our model, configure categorical cross-entropy
    as the loss function *(commonly used for classification tasks)*, and designate
    Adam as our optimizer. As mentioned earlier, we’ll execute our model over a full
    cycle of 10 epochs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们初始化模型，配置类别交叉熵为损失函数*(通常用于分类任务)*，并将Adam作为优化器。如前所述，我们将在10个周期内执行模型。
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To track performance, we will add some code to follow the training progress,
    print validation metrics, and plot them. Finally, we save the model as **hockey_team_classifier.pth**
    in a designated path of your choice.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪性能，我们将添加一些代码来跟踪训练进度，打印验证指标并绘制图表。最后，我们将模型保存为**hockey_team_classifier.pth**，保存在你选择的指定路径中。
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Additionally, alongside your “pth” file, after running through all the steps
    described above *(you can find the complete code in* [*the project’s GitHub repository*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)*,
    you should expect to see an output like the following (metrics may vary slightly)*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了你的“pth”文件，在完成上述所有步骤后*(你可以在[*项目的GitHub仓库*](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)中找到完整代码)*，你应该能看到如下输出（指标可能略有不同）：
- en: '![](../Images/91ff066093e4c561b0f5b20528f75f57.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91ff066093e4c561b0f5b20528f75f57.png)'
- en: 'Figure 05: CNN model performance metrics'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05：CNN模型性能指标
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: After finishing 10 epochs, the CNN model shows improvement in performance metrics.
    Initially, in Epoch 1, the model starts with a training loss of 1.5346 and a validation
    accuracy of 47.37%. ***How should we understand this initial point?***
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完成10个训练周期后，CNN模型的性能指标有所改善。最初，在第1个周期时，模型的训练损失为1.5346，验证准确率为47.37%。***我们应如何理解这个初始点？***
- en: '**Accuracy** is one of the most common metrics for evaluating classification
    performance. In our case, it represents the proportion of correctly predicted
    classes out of the total. **However, high accuracy alone doesn’t guarantee overall
    model performance**; you still can have poor predictions for specific classes
    (as I experienced in early trials). Regarding **training loss**, it measures how
    effectively the model learns to map input data to the correct labels. Since we’re
    using a classification function, **Cross-Entropy Loss** quantifies the difference
    between predicted class probabilities and actual labels. A starting value like
    1.5346 indicates significant differences between predicted and actual classes;
    ideally, this value should approach 0 as training progresses. As epochs progress,
    we observe a significant drop in training loss and an increase in validation accuracy.
    By the final epoch, the training and validation loss reach lows of 0.2509 and
    0.2651, respectively.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率**是评估分类性能最常见的指标之一。在我们的案例中，它表示正确预测的类别占总类别的比例。**然而，单靠高准确率并不能保证整体模型的表现**；你仍然可能在某些类别上做出不好的预测（正如我在早期实验中所经历的那样）。关于**训练损失**，它衡量模型将输入数据映射到正确标签的效果。由于我们使用的是分类函数，**交叉熵损失**量化了预测的类别概率与实际标签之间的差异。像1.5346这样的初始值表示预测类别与实际类别之间存在显著差异；理想情况下，随着训练的进行，这个值应该趋近于0。随着训练周期的进行，我们观察到训练损失显著下降，验证准确率提高。到最后一个训练周期时，训练损失和验证损失分别降到0.2509和0.2651的最低点。'
- en: To test our CNN model, we can select a sample of player images and evaluate
    its prediction capability. For testing, you can run the following code and utilize
    the **validation_dataset folder** in the [project’s GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的 CNN 模型，我们可以选择一部分球员图像并评估其预测能力。为了测试，你可以运行以下代码并使用 [项目的 GitHub 仓库](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)中的**validation_dataset
    文件夹**。
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output should look something like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, the model shows quite good ability in identifying teams and
    excluding the referee as a team player.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，模型在识别队伍并排除裁判作为队员方面表现得相当不错。
- en: '**Tip #03:** Something I learned during the CNN design process is that adding
    complexity doesn’t always improve performance. Initially, I experimented with
    deeper models (more convolutional layers) and color-based augmentation to enhance
    players’ jersey recognition. However, in my small dataset, I encountered overfitting
    rather than learning generalizable features (all images were predicted as white
    team players or referees). Regularization techniques like dropout and batch normalization
    are also important; they help impose constraints during training, ensuring the
    model can generalize well to new data. Less can sometimes mean more in terms of
    results😁.'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**小贴士 #03：** 在 CNN 设计过程中我学到的一点是，增加复杂性并不总能提升性能。一开始，我尝试了更深的模型（更多的卷积层）和基于颜色的增强来提高球员球衣的识别率。然而，在我的小数据集上，我遇到了过拟合，而不是学习到可以泛化的特征（所有图像都被预测为白队球员或裁判）。正则化技术，如
    dropout 和批量归一化也很重要；它们有助于在训练过程中施加约束，确保模型能够很好地泛化到新数据。少即是多，结果有时会更好😁。'
- en: '**PUTING IT ALL TOGETHER**'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**将一切整合在一起**'
- en: Putting it all together will require some adjustments to our tracking mechanism
    described earlier. Here’s a breakdown of the updated code step by step.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起需要对我们之前描述的跟踪机制进行一些调整。下面是更新后的代码逐步解析。
- en: 'First, we’ll set up the libraries and paths we need. Note that the paths for
    our pickle file and the CNN model are specified now. **This time, if the pickle
    file isn’t found in the path, the code will throw an error**. Use the previous
    code to generate the pickle file if needed, and use this updated version to perform
    the video analysis:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置所需的库和路径。请注意，现在已经指定了 pickle 文件和 CNN 模型的路径。**这次，如果找不到路径中的 pickle 文件，代码将抛出一个错误**。如果需要，使用之前的代码生成
    pickle 文件，并使用此更新版本来执行视频分析：
- en: '[PRE17]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we will load the models, specify the rink coordinates, and initiate the
    process of detecting objects in each frame in batches of 20, as we did before.
    Note that for now, we will only use the rink boundaries to focus the analysis
    on the rink. In the final steps of the article, when we include performance stats,
    we’ll use the offensive zone coordinates.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载模型，指定冰场坐标，并启动每帧图像的批量检测过程，每次批量为 20 帧，和之前一样。请注意，目前我们只会使用冰场边界来聚焦分析范围。文章的最后几步中，当我们包含性能统计时，我们将使用进攻区坐标。
- en: '[PRE18]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we’ll add the process to predict each player’s team:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加预测每个球员队伍的过程：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As the next step, we’ll add the method described earlier to switch from bounding
    boxes to ellipses:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将添加之前描述的方法，从边界框转换为椭圆形：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, it’s time to add the analyzer that includes reading the pickle file, narrowing
    the analysis within the rink boundaries we defined earlier, and calling the CNN
    model to identify each player’s team and add labels. Note that we include a feature
    to label referees with a different color and change the color of their ellipses
    as well. The code ends with writing processed frames to an output video.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候添加分析器了，它包括读取 pickle 文件、将分析范围缩小到我们之前定义的冰场边界，并调用 CNN 模型以识别每个球员的队伍并添加标签。请注意，我们包括了一个特性，用不同的颜色标记裁判，并且改变他们椭圆形的颜色。代码的最后会将处理过的帧写入输出视频。
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we add the CNN’s architecture (defined in the CNN design process)
    and execute the Hockey analyzer:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加 CNN 的架构（在 CNN 设计过程中定义）并执行冰球分析器：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After running all the steps, your video output should look something like this:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 运行所有步骤后，你的视频输出应该如下所示：
- en: '![](../Images/98890f2c70834d1f61d97b6135b3afe5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98890f2c70834d1f61d97b6135b3afe5.png)'
- en: 'Sample Clip 06: Tracking Players and Teams'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 示例片段 06：跟踪球员和队伍
- en: Note that in this last update, object detections are only within the ice rink,
    and teams are differentiated, as well as the referee. While the CNN model still
    needs fine-tuning and occasionally loses stability with some players, it remains
    mostly reliable and accurate throughout the video.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这次更新中，物体检测仅限于冰球场内，且队伍和裁判已被区分开来。虽然CNN模型仍需微调，并且偶尔在一些球员身上会失去稳定性，但在整个视频中，它仍然大部分时间是可靠且准确的。
- en: '**SPEED, DISTANCE AND OFFENSIVE PRESSURE**'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**速度、距离和进攻压力**'
- en: 'The ability to track teams and players opens up exciting possibilities for
    measuring performance, such as generating heatmaps, analyzing speed and distance
    covered, tracking movements like zone entries or exits, and diving into detailed
    player metrics. In order we can have a taste of it, we’ll add three performance
    metrics: **average speed per player**, skating **distance covered** by each team,
    and o**ffensive pressure** *(measured as the percentage of distance covered by
    each team spent in its opponent’s zone)*. I’ll leave more detailed statistics
    up to you!'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪队伍和球员的能力为衡量表现开辟了令人兴奋的可能性，例如生成热图、分析速度和覆盖的距离、跟踪如区域进入或退出等动作，以及深入研究球员的详细指标。为了让我们能感受这一点，我们将添加三项表现指标：**每个球员的平均速度**、每支队伍滑行的**距离**，以及**进攻压力**（*以每支队伍在对方区域内所花费的距离占总距离的百分比来衡量*）。我将把更详细的统计数据留给你们！
- en: We begin adapting the coordinates of the ice rink from pixel-based measurements
    to approximate meters. This adjustment allows us to read our data in meters rather
    than pixels. The real-world dimensions of the ice rink seen in the video are approximately
    15mx30m (15 meters in width and 30 meters in height). To facilitate this conversion,
    we introduce a method to convert pixel coordinates to meters. By defining the
    rink’s actual dimensions and using the pixel coordinates of its corners (from
    left to right and top to bottom), we obtain conversion factors. These factors
    will support our process of estimating distances in meters and speeds in meters
    per second. *(Another interesting technique you can explore and apply is Perspective
    Transformation)*
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始将冰场的坐标从基于像素的度量转换为近似米数。这一调整使我们能够以米为单位读取数据，而非像素。视频中看到的冰场的实际尺寸大约为15mx30m（宽度为15米，高度为30米）。为了方便这一转换，我们引入了一种将像素坐标转换为米的方式。通过定义冰场的实际尺寸，并使用其角落的像素坐标（从左到右，从上到下），我们获得了转换因子。这些因子将支持我们估算米数和每秒米数速度的过程。*（另一个有趣的技术是透视变换，你可以探索并应用它）*
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We are now ready to **add speed to each player measured in meters per second**.
    To do this, we’ll need to make three modifications. First, initiate an empty dictionary
    named **previous_positions** in the **HockeyAnalyzer class** to help us compare
    the current and previous positions of players. Similarly, we’ll create a t**eam_stats**
    structure to store stats from each team for further visualization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好**以每秒米数为单位添加每个球员的速度**。为了做到这一点，我们需要进行三处修改。首先，在**HockeyAnalyzer类**中初始化一个名为**previous_positions**的空字典，以帮助我们比较球员的当前和前一个位置。同样，我们还将创建一个**team_stats**结构来存储每支队伍的统计数据，以便进一步可视化。
- en: 'Next, we will add a **speed method** to estimate players’ speed in pixels per
    second, and then use the conversion factor (explained earlier) to transform it
    into meters per second. Finally, from the **analyze_video method**, we’ll call
    our new speed method and add the speed to each tracked object (players and referee).
    This is what the changes look like:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加一个**速度方法**来估算球员的速度（以每秒像素数为单位），然后使用前面解释的转换因子将其转换为每秒米数。最后，在**analyze_video方法**中，我们将调用新的速度方法，并将速度添加到每个追踪的对象（球员和裁判）中。这就是这些更改的效果：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you have troubles adding this new lines of code, you can always visit [the
    project’s GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch),
    where you can find the complete integrated code. Your video output at this point
    should look like this *(notice that the speed has been added to the label of each
    player)*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在添加这些新代码时遇到问题，你可以随时访问[项目的GitHub仓库](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)，在这里你可以找到完整的集成代码。此时，你的视频输出应该如下所示（*注意速度已添加到每个球员的标签上*）：
- en: '![](../Images/56fe2cf37374ce478a5baaa91c6c66a7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56fe2cf37374ce478a5baaa91c6c66a7.png)'
- en: 'Sample Clip 07: Tracking Players and Speed'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 示例片段07：跟踪球员和速度
- en: Finally, let’s add a stats board where we can track the average speed per player
    for each team, along with other metrics such as distance covered and offensive
    pressure in the opponent’s zone.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们添加一个统计板，跟踪每支队伍每个球员的平均速度，并显示其他数据，例如行进的距离和在对方区域内的进攻压力。
- en: We’ve already defined the offensive zones and integrated them into our code.
    Now, we need to track how often each player enters their opponent’s zone. To achieve
    this, we’ll implement a method using the [**ray casting algorithm**](https://medium.com/@girishajmera/exploring-algorithms-to-determine-points-inside-or-outside-a-polygon-038952946f87).
    This algorithm checks if a player’s position is inside the white or yellow team’s
    offensive zone. It works by drawing an imaginary line from the player to the target
    zone. If the line crosses one border, the player is inside, if it crosses more
    (in our case, two out of four borders), the player is outside. The code then scans
    the entire video to determine each tracked object’s zone status.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了进攻区域并将其集成到我们的代码中。现在，我们需要跟踪每个球员进入对方区域的次数。为此，我们将实现一个方法，使用[**射线投射算法**](https://medium.com/@girishajmera/exploring-algorithms-to-determine-points-inside-or-outside-a-polygon-038952946f87)。这个算法检查球员的位置是否在白队或黄队的进攻区域内。它通过从球员到目标区域画一条虚拟线来工作。如果这条线穿过一个边界，则表示球员在内部；如果穿过多个边界（在我们这个案例中是穿过四个边界中的两个），则表示球员在外部。代码接着扫描整个视频，确定每个跟踪物体的区域状态。
- en: '[PRE25]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we’ll handle the performance metrics by adding a method that displays **average
    player speed**, total **distance covered**, and **offensive pressure** (percentage
    of time spent in the opponent’s zone) on a table format for each team. Using OpenCV,
    we’ll format these metrics into a table overlaid on the video and we’ll incorporate
    a dynamic update mechanism to maintain real-time statistics during gameplay.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过添加一个方法来处理表现指标，该方法将在表格格式中为每支队伍显示**球员平均速度**、**总行进距离**和**进攻压力**（在对方区域内的时间百分比）。使用
    OpenCV，我们将这些指标格式化为覆盖在视频上的表格，并将加入动态更新机制，以保持游戏过程中的实时统计。
- en: '[PRE26]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In order the stats display in the video we’ll have to call the method in the
    **analyze_video method,** so be sure to add this extra lines of code after the
    speed label is defined and just before the output video is processed:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在视频中显示统计数据，我们需要调用**analyze_video方法**，因此请确保在定义速度标签后、处理输出视频之前，添加这些额外的代码行：
- en: '[PRE27]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The distance in meters covered by each player is calculated by dividing their
    speed (measured in meters per second) by the frame rate (frames per second). This
    calculation allows us to estimate how far each player moves between each frame
    change in the video. If everything works well, your final video output should
    look like this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每个球员所覆盖的距离（单位：米）是通过将其速度（以米/秒为单位）除以帧率（以帧/秒为单位）来计算的。这个计算方法使我们能够估算每个球员在视频中每次帧变化之间移动的距离。如果一切顺利，最终的视频输出应该是这样的：
- en: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a960baa2383b62cff2c4cf942ad002a.png)'
- en: 'Sample Clip 08: Final Output'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 示例片段 08：最终输出
- en: Considerations and Future Work
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑事项与未来工作
- en: 'This model is a basic setup of what can be achieved using computer vision to
    track players in an ice hockey game (or any team sport). However, there’s a lot
    of fine-tuning that can be done to improve it and add new capabilities. Here are
    a few ideas that I’m working on for a next 2.0 version that you might also consider:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是使用计算机视觉追踪冰球比赛中球员的基本设置（或任何团队运动）。然而，还有许多精细调优可以改进，并且可以添加新功能。以下是我正在研究的一些想法，用于下一版本2.0，你也可以考虑这些想法：
- en: '***The challenge of following the puck:*** Depending on which direction your
    camera is facing and the resolution, tracking the puck is challenging considering
    its size compared to a soccer or basketball ball. But if you achieve this, interesting
    possibilities open up to track performance, such as possession time metrics, goal
    opportunities, or shots data. This also applies also to individual performances;
    in ice hockey, players change significantly more often than in other team sports,
    so tracking each player’s performance during one period presents a challenge.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '***跟踪冰球的挑战：*** 根据相机的朝向和分辨率，跟踪冰球是具有挑战性的，因为它的尺寸相较于足球或篮球球来说较小。但如果你能够实现这一点，便能开启一些有趣的可能性来追踪表现，例如控球时间、进攻机会或射门数据。这同样适用于个别球员的表现；在冰球中，球员的换人频率远高于其他团队运动，因此，在一个时段内追踪每个球员的表现也是一种挑战。'
- en: '***Compute resources, Oh why compute!*** I ran all the code on a CPU arrangement
    but faced issues *(sometimes resulting in blue screens 😥)* due to running out
    of memory during the design process (consider using a CUDA setup). Our sample
    video is about 40 seconds long and initially 5 MB in size, but after running the
    model, the output increases to up to 34 MB. Imagine the size for a full 20-minute
    game period. So, you should consider compute resources and storage when scaling
    up.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '***计算资源，哦，为什么是计算！*** 我在一个 CPU 配置上运行了所有代码，但由于在设计过程中内存不足（有时导致蓝屏 😥），遇到了问题（建议使用
    CUDA 设置）。我们的示例视频大约 40 秒长，最初为 5 MB，但在运行模型后，输出文件的大小增加到 34 MB。想象一下完整的 20 分钟比赛期间的大小。所以，在扩展时，你应该考虑计算资源和存储。'
- en: '***Don’t underestimate MLOps:*** To deploy and scale rapidly, we need Machine
    Learning pipelines that are efficient, support frequent execution, and are reliable.
    This involves considering a **Continuous Integration-Deployment-Training approach**.
    Our use case has been built for a specific scenario, but what if conditions change,
    such as the camera direction or jersey colors? To scale up, we must adopt a CI/CD/CT
    mindset.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '***不要低估 MLOps：*** 要快速部署和扩展，我们需要高效的机器学习管道，支持频繁执行，并且可靠。这需要考虑**持续集成-部署-训练方法**。我们的用例是为特定场景构建的，但如果条件发生变化，比如摄像头方向或球衣颜色变化怎么办？为了扩展，我们必须采纳
    CI/CD/CT 思维模式。'
- en: I hope you found this computer vision project interesting, you can access the
    complete code in [this GitHub repository](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch).
    And if you want to support the development of inline and ice hockey in the region,
    follow the [APHL](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    *(we are always in need of used equipment you’d like to donate for young players
    and working on building our first official hockey rink)*, and worldwide, follow
    and support the [Friendship League](https://friendshipleague.org/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你对这个计算机视觉项目感兴趣，你可以在[这个 GitHub 仓库](https://github.com/rvizcarra15/IceHockey_ComputerVision_PyTorch)访问完整的代码。如果你想支持该地区的冰球和冰球运动发展，可以关注[APHL](https://www.instagram.com/aphl.pe/?igsh=MThvZWxhNThwdXpibA%3D%3D)
    *(我们总是需要您捐赠的二手设备，供年轻球员使用，并正在建设我们的第一个官方冰球场)*，全球范围内，也可以关注并支持[Friendship League](https://friendshipleague.org/)。
- en: '***Did I miss anything?*** Your suggestions are always welcome. Let’s keep
    the conversation going!'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '***我漏掉了什么吗？*** 欢迎提出建议。让我们继续交流！'
