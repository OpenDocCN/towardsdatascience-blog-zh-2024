- en: Backpropagation Through Time — How RNNs Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17](https://towardsdatascience.com/backpropagation-through-time-how-rnns-learn-e5bc03ad1f0a?source=collection_archive---------4-----------------------#2024-05-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An explanation of the backpropagation through time algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--e5bc03ad1f0a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5bc03ad1f0a--------------------------------)
    ·9 min read·May 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee340856a1b151a2630e3e799c8e9f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>Neural network icons created by pojok d — Flaticon.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNNs) are regular feedforward neural network variants
    that handle sequence-based data like time series and natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'They achieve this by adding a “recurrent” neuron that allows information to
    be fed through from past inputs and outputs to the next step. The diagram below
    depicts a traditional RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91392ef12cfd2972ccec9a5d9c622d1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Example architecture of RNNs. Diagram by author.
  prefs: []
  type: TYPE_NORMAL
- en: On the left side is a recurrent neuron, and on the right-hand side is the recurrent
    neuron *unrolled through time.* Noticehow the previous executions are passed on
    to the subsequent calculations.
  prefs: []
  type: TYPE_NORMAL
- en: This adds some inherent “memory” in the system that aids the model in picking
    up historical patterns that happened previously in time.
  prefs: []
  type: TYPE_NORMAL
- en: When predicting ***Y_1***, the recurrent neuron uses the inputs of ***X_1***
    and the output from the previous time step, ***Y_0***. This means that ***Y_0’***s
    influence on ***Y_1*** is direct, and it also indirectly influences ***Y_2.***
  prefs: []
  type: TYPE_NORMAL
- en: If you want a complete introduction to RNNs and some worked examples, check
    out my previous post.
  prefs: []
  type: TYPE_NORMAL
