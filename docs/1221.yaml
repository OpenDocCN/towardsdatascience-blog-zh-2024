- en: Understanding Long RoPE in LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LLM中的Long RoPE
- en: 原文：[https://towardsdatascience.com/understanding-long-rope-in-llms-29337dc7e4a9?source=collection_archive---------1-----------------------#2024-05-15](https://towardsdatascience.com/understanding-long-rope-in-llms-29337dc7e4a9?source=collection_archive---------1-----------------------#2024-05-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-long-rope-in-llms-29337dc7e4a9?source=collection_archive---------1-----------------------#2024-05-15](https://towardsdatascience.com/understanding-long-rope-in-llms-29337dc7e4a9?source=collection_archive---------1-----------------------#2024-05-15)
- en: This blog post will go in detail about the Long RoPE Methodology used to expand
    the context lengths in LLMs without significant performance degradation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将详细介绍Long RoPE方法，该方法用于扩展LLM中的上下文长度，而不会显著降低性能。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--29337dc7e4a9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)
    ·8 min read·May 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29337dc7e4a9--------------------------------)
    ·8分钟阅读·2024年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b0e0ea0d975d1c44708ca117d30e3066.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0e0ea0d975d1c44708ca117d30e3066.png)'
- en: Image by Author — generated by Stable Diffusion 2.1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供 — 由Stable Diffusion 2.1生成
- en: As the general public has begun using LLMs in their daily lives, one important
    problem arises when they have long-conversations. After a few dialogue turns,
    the LLM can appear to completely forget what was said before! Behind the scenes,
    each line of dialogue is fed into the LLM’s context, which you can think of as
    a giant input into the model. Once the conversation is too long for the context,
    you have to remove some of the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着公众开始在日常生活中使用LLM，一个重要的问题浮现出来，那就是在进行长时间对话时，LLM有时会完全忘记之前说过的内容！在后台，每一行对话都被输入到LLM的上下文中，你可以将其视为对模型的巨大输入。一旦对话超出了上下文的处理能力，就必须删除一些数据。
- en: Not only is this a bad customer experience, it also limits the amount of information
    that a LLM can reasonably process. Consequently, work has been ongoing to build
    LLMs with larger and larger contexts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅是糟糕的客户体验，还限制了LLM合理处理信息的能力。因此，相关工作一直在进行，以构建具有越来越大上下文的LLM。
- en: 'Today’s paper, “LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,”
    achieves just that.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的论文《LongRoPE：将LLM上下文窗口扩展到超2百万个标记》就实现了这一目标。
- en: '![](../Images/91cd200464c935f6ca2413deb5ae2d07.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91cd200464c935f6ca2413deb5ae2d07.png)'
- en: Figure 1 from [the paper](https://arxiv.org/pdf/2402.13753)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2402.13753)的图1
- en: Looking at the above graph, we can see that the perplexity, a measurement of
    loss correlating to how well the LLM predicts the next token, stays low for LongRoPE,
    but spikes…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图表来看，我们可以看到困惑度（一个衡量损失的指标，反映LLM预测下一个标记的准确性）对于LongRoPE保持较低，但却出现了峰值…
