- en: Multimodal Large Language Models & Apple’s MM1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态大语言模型与苹果的MM1
- en: 原文：[https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13](https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13](https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13)
- en: 'This blog post will go into the architecture and findings behind Apple’s “MM1:
    Methods, Analysis & Insights from Multimodal LLM Pre-training” paper'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本博客文章将深入介绍苹果“MM1：多模态LLM预训练的方式、分析与洞察”论文背后的架构和研究发现。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    ·9 min read·Apr 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    ·9分钟阅读·2024年4月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4b89777cfb3161fa6ce57a05fdb4277c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b89777cfb3161fa6ce57a05fdb4277c.png)'
- en: Image by the Author Generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者生成，使用DALL-E
- en: Abstraction is one of the most critical concepts in Computer Science, with some
    of the most powerful implications. From a simplistic point of view, abstraction
    is the ability to take something and apply it to multiple distinct situations.
    For example, if you create a way to successfully sort apples based on their size
    in a factory, your solution could be abstracted to also sort oranges or peaches
    in the same way. Thus, through abstraction a very powerful solution is able to
    radically impact multiple parts of the world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象化是计算机科学中最重要的概念之一，具有极其深远的影响。从简化的角度来看，抽象化是将某种方法应用于多个不同情况的能力。例如，如果你在工厂中创建了一种根据大小成功分类苹果的方法，那么你也可以将这一解决方案抽象化，用于同样的方式对橙子或桃子进行分类。因此，通过抽象化，一种非常强大的解决方案能够深刻影响世界的多个领域。
- en: While Large Language Models are exceptional at reasoning when given text as
    an input, recently we have been able to abstract their input so that they can
    reason with images and sounds.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大语言模型在处理文本输入时具有出色的推理能力，但最近我们已经能够将它们的输入抽象化，使它们能够通过图像和声音进行推理。
- en: The below blog post goes into architectural ablations in Apple’s MM1 paper and
    their research findings when building a Multimodal Large Language Model (MLLM).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下博客文章深入探讨了苹果MM1论文中的架构剖析及其在构建多模态大语言模型（MLLM）时的研究成果。
- en: Abstracting LLM Input
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象化LLM输入
- en: The architecture behind Large Language Models can be traced back to the 2017
    paper “Attention is All You Need” where the Transformer Architecture was introduced.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型背后的架构可以追溯到2017年发表的论文《Attention is All You Need》，其中引入了Transformer架构。
- en: This paper showed how you could transform human language into tokens that a
    neural network would then process (in that paper’s case, process into a different
    language).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了如何将人类语言转化为神经网络处理的标记（在该论文中，转化为不同语言）。
- en: '![](../Images/8537829bb3d08c52694fbb294fe530d1.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8537829bb3d08c52694fbb294fe530d1.png)'
- en: Figure 1 from [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1来自[《Attention is All You Need》](https://arxiv.org/pdf/1706.03762.pdf)
- en: As you can see from the image, we have a transformation occurring early on where
    we take the input and convert it into tokens (the embedding section). However,
    there is no inherent reason why only text data can be mapped to tokens. Consequently,
    the field began trying to map other kinds of data to tokens.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从图像中看到的，我们在早期进行了一次转换，将输入转换为标记（嵌入部分）。然而，并没有固有的理由认为只有文本数据才能映射为标记。因此，研究领域开始尝试将其他类型的数据映射为标记。
- en: MM1 Architecture Base
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MM1架构基础
- en: '![](../Images/effb93869b2820023bfd10695c290fa0.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/effb93869b2820023bfd10695c290fa0.png)'
- en: Part of Figure 3 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图3部分
- en: 'Apple’s model had 3 key components: a visual transformer (ViT) image encoder,
    Vision-Language Connector, and a Large Language Model. Assuming you already have
    a good idea of what a LLM is and how it works, let’s dive into the image encoder
    and VL connector.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果的模型有三个关键组件：视觉转换器（ViT）图像编码器、视觉-语言连接器和大型语言模型。假设你已经对LLM有一个不错的了解，知道它是如何工作的，那么我们就深入了解图像编码器和VL连接器。
- en: Image Encoders & Visual Connectors
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像编码器与视觉连接器
- en: 'While from an abstracted view we can imagine text and images as simply different
    kinds of inputs, to make this work we need to accept that we may have to treat
    them differently to get them into token form. At this time, we have 2 different
    systems that help us transform the image into tokens the LLM can reason with:
    image encoder and connector.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从抽象的角度看，我们可以将文本和图像视为不同种类的输入，但为了实现这一点，我们需要接受一个事实，即可能需要以不同的方式处理它们，以便将其转化为标记。目前，我们有两个不同的系统帮助我们将图像转换为LLM可以推理的标记：图像编码器和连接器。
- en: First, the image encoder is responsible for taking our image and converting
    it into the token representation that our transformer model can understand.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，图像编码器负责将我们的图像转换为转换器模型可以理解的标记表示。
- en: Second, a connector is the piece that takes data from the vision encoder and
    transforms it into the data that is passed directly to the large language model.
    Given the image encoder returns tokens, you may wonder why we need the connector
    at all. The idea appears to be that image encoders give too much information in
    their tokens, so to reduce costs while optimizing reasoning, we want to be selective
    with what we pass through.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，连接器是将来自视觉编码器的数据转换成直接传递给大型语言模型的数据的部分。由于图像编码器返回的是标记，你可能会想，为什么我们还需要连接器。其背后的想法似乎是图像编码器在其标记中提供了过多的信息，因此为了在优化推理的同时减少成本，我们希望有选择地传递信息。
- en: The below image shows the data flow we’re working with here.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像展示了我们在此工作中的数据流。
- en: '![](../Images/d88a4a539cf61aeecfdaea062e1ce7a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d88a4a539cf61aeecfdaea062e1ce7a2.png)'
- en: 'Figure 2 from [“Honeybee: Locality-enhanced Projector for Multimodal LLM”](https://arxiv.org/pdf/2312.06742.pdf)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '来自[“Honeybee: Locality-enhanced Projector for Multimodal LLM”](https://arxiv.org/pdf/2312.06742.pdf)的图2'
- en: Ablations
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消融实验
- en: An ablation study in Machine Learning revolves around removing and modifying
    certain parts of a model to see how they contribute to overall performance. Apple’s
    research centered around different ways of training the Image Encoder, different
    projectors for the VL Connector, and different pre-training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的消融研究围绕去除和修改模型的某些部分，以观察它们如何影响整体性能。苹果的研究聚焦于图像编码器的不同训练方式、VL连接器的不同投影器以及不同的预训练数据。
- en: Let’s dive into the major findings.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨主要的发现。
- en: Image Encoder Ablations
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像编码器消融实验
- en: For the Image Encoder, they varied between CLIP and AIM models, Image resolution
    size, and the dataset the models were trained on. The below chart shows you the
    results for each ablation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像编码器，他们在CLIP和AIM模型、图像分辨率大小以及模型训练所用的数据集之间进行了变化。下面的图表展示了每个消融实验的结果。
- en: '![](../Images/72efc54bb2d9ecd0a6ef4975725f6171.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72efc54bb2d9ecd0a6ef4975725f6171.png)'
- en: Table 1 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表1
- en: Let’s go through the major pieces above and explain what they are.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一介绍上述的主要部分，并解释它们是什么。
- en: '**CLIP** stands for Contrastive Language Image Pre-training and is meant to
    help your model learn visual concepts by providing names to the things that are
    meant to be seen as text. As the image below shows, this pairs images with text
    encodings so that the model will eventually connect the vision tokens (represented
    in the below image as I, with the text tokens T). This method is called contrastive
    training.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLIP**代表对比语言图像预训练，旨在通过为需要被视作文本的事物提供名称，帮助模型学习视觉概念。如下面的图像所示，它将图像与文本编码配对，以便模型最终能够将视觉令牌（在下面的图像中表示为I）与文本令牌T连接起来。这种方法称为对比训练。'
- en: '![](../Images/2ee9a0733e16658d9ed133ce1a2ddda0.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ee9a0733e16658d9ed133ce1a2ddda0.png)'
- en: Figure 1 from [“Learning Transferable Visual Models From Natural Language Supervision”](https://arxiv.org/pdf/2103.00020.pdf)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1来自[《从自然语言监督学习可迁移的视觉模型》](https://arxiv.org/pdf/2103.00020.pdf)
- en: '**AIM** stands for Autoregressive Image Model, and it is trained via a reconstructive
    loss optimization algorithm. The goal here is to see if the transformer can recreate
    (reconstruct) the image that it is given.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**AIM**代表自回归图像模型，它通过重构损失优化算法进行训练。这里的目标是查看变换器是否能够重建（恢复）给定的图像。'
- en: '![](../Images/2df9b1a9690d422713ce7d372da6f989.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2df9b1a9690d422713ce7d372da6f989.png)'
- en: Figure 2 from [“Scalable Pre-training of Large Autoregressive Image Models”](https://arxiv.org/pdf/2401.08541.pdf)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2来自[《可扩展的大型自回归图像模型预训练》](https://arxiv.org/pdf/2401.08541.pdf)
- en: '**Image Resolution** here refers to the number of pixels that is fed into the
    transformer. For example, a 378 x 378 image resolution means we will pass in a
    matrix of that size and then convert it into embeddings that the model will then
    be trained on. Training Datawas split between the (DFN-2B), (DFN-5B), (DFN-5B
    + VeCap) and (ImageText-400M).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像分辨率**在这里指的是输入到变换器中的像素数量。例如，378 x 378的图像分辨率意味着我们将传入一个该大小的矩阵，然后将其转换为模型将要训练的嵌入。训练数据分为（DFN-2B）、（DFN-5B）、（DFN-5B
    + VeCap）和（ImageText-400M）四组。'
- en: The authors found that image resolution was of highest importance, followed
    by model size and then the training data contents. Specifically, they saw that
    the better the image resolution, the better the model tended to perform for both
    zero-shot and few-shot prompting. As more compute is needed to train and run models
    with higher image resolution requirements, this suggests that for Vision Transformers,
    compute will remain of paramount importance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者发现图像分辨率最为重要，其次是模型大小，然后是训练数据内容。具体来说，他们发现图像分辨率越高，模型在零-shot和少-shot提示下的表现越好。由于训练和运行具有更高图像分辨率要求的模型需要更多的计算资源，这表明对于视觉变换器而言，计算资源仍然至关重要。
- en: VL Connection Ablations
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VL连接器消融实验
- en: For the VL Connector, they tested using 64 or 144 tokens for the image, tested
    using 224, 336, and 378 for the image resolution, and chose between a few architectures.
    I’ll briefly go over the architectures below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于VL连接器，他们测试了图像使用64或144个令牌，图像分辨率使用224、336和378进行测试，并在几种架构之间做了选择。下面我将简要介绍这些架构。
- en: '**Average Pooling** is exactly what it sounds like, taking the average of all
    of the tokens, and then doing a linear projection of this average so that the
    grid was 8x8 or 12x12.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均池化**正如其名称所示，是取所有令牌的平均值，然后对这个平均值进行线性投影，使得网格为8x8或12x12。'
- en: '**Attention Pooling** makes the assumption that image tokens should be treated
    as samples from a fundamentally different population set than the text tokens.
    Here we adjust how many tokens are fed in for each image, in the paper referred
    to as k learnable queries. The researchers only considered k of either 64 or 144.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力池化**假设图像令牌应被视为来自与文本令牌完全不同的群体。这里我们调整每个图像输入的令牌数量，在论文中称之为k可学习查询。研究人员仅考虑了k为64或144的情况。'
- en: '**Convolutional Mapping** is a a method from Honeybee that uses a ResNet to
    dynamically decide how many tokens to pass through to the LLM from the image.
    This is actualized in the C-Abstractor module.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积映射**是Honeybee中的一种方法，使用ResNet动态决定从图像传递给LLM多少令牌。这在C-Abstractor模块中实现。'
- en: '![](../Images/4848e1619cc1b54fc72ebf7f388afa54.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4848e1619cc1b54fc72ebf7f388afa54.png)'
- en: Figure 4 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图4来自[该论文](https://arxiv.org/pdf/2403.09611.pdf)
- en: As you can see from the above, the different architectures actually had very
    little impact. As one might guess, the higher resolution images and the more tokens
    passed through increased performance among all of the connectors but not dramatically
    so.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的内容可以看出，不同的架构实际上对性能的影响很小。正如人们可能猜到的，分辨率更高的图像以及传递的 token 更多，提升了所有连接器的性能，但提升幅度并不显著。
- en: This finding suggests we either haven’t found a significantly better way to
    connect the image encoder to the LLM, or that this area is simply not where great
    models will differentiate themselves.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现表明，我们要么尚未找到一种显著更好的方式来将图像编码器与大语言模型连接起来，要么这一领域根本就不是优秀模型能够显著区分的地方。
- en: Pre-Training Data Ablations
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练数据消融
- en: '![](../Images/a7eba075bb1027a4aff1a184921b0107.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7eba075bb1027a4aff1a184921b0107.png)'
- en: Table 2 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表2
- en: 'Here, the authors played with 4 different kinds of data: captioned images,
    synthetically captioned images, interleaved image-text data, and text-only data.
    They found 4 lessons, each with a graph to summarize the performance changes.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，作者使用了四种不同的数据类型：带有描述的图像、合成描述的图像、交织的图像-文本数据，以及仅文本数据。他们总结出了四个经验教训，并用图表总结了性能变化。
- en: '![](../Images/817a2d9f60801091eb142e89d99b205a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/817a2d9f60801091eb142e89d99b205a.png)'
- en: Figure 5a from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5a
- en: '**First**, interleaving data helps with few-shot and text-only performance,
    while captioned data helps with zero-shot performance. The researchers varied
    how much interleaving they did, with the graph below showing the results. As you
    can see, few-shot prompts performed noticeably better on models trained with interleaved
    data than the models trained with all or nothing.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一**，交织数据有助于少样本和仅文本性能，而带描述的数据有助于零样本性能。研究人员通过改变交织数据的比例，以下的图表展示了结果。正如你所看到的，使用交织数据训练的模型在少样本提示上表现明显优于使用完全文本或完全图像的模型。'
- en: '![](../Images/4ac3c1384e53bdf08a5a445bbb9a4fd8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ac3c1384e53bdf08a5a445bbb9a4fd8.png)'
- en: Figure 5b from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5b
- en: '**Second**, Text-only data helps with few-shot reasoning. Text-only in this
    context means that the training data includes image examples and text-only examples.
    This was done to ensure that the model understands human language as well as images.
    Comparing the caption-only to caption-with-text shows a marked improvement for
    all but the 0-shot reasoning, however, interleaved-only performs better than interleaved-plus-text
    for all but the TextCore test.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二**，仅文本数据有助于少样本推理。在此背景下，仅文本数据意味着训练数据包含了图像示例和仅文本示例。这么做是为了确保模型能够理解人类语言和图像。比较仅描述与描述加文本的效果，除了0-shot推理外，所有任务都显示了明显的提升，但对于除TextCore测试以外的所有任务，交织数据的表现优于交织加文本。'
- en: '![](../Images/edf1887d66e248908c526bf475ee57b9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edf1887d66e248908c526bf475ee57b9.png)'
- en: Figure 5c from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5c
- en: '**Third**, if you get the mixture right between image and text you can get
    really strong performance. The above graph shows different ratios of interleaved
    + captioned data to text-only data. As the goal is to have a multi-modal model,
    they never tested the performance if you do not have any image data. The authors
    here point out that the 91/9 ratio produced the most consistently good results.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三**，如果你能正确地将图像和文本混合在一起，就能获得非常强的性能。上面的图表展示了不同的图像与文本数据混合比例与仅文本数据的对比。由于目标是拥有一个多模态模型，因此他们从未测试过没有任何图像数据的性能表现。这里的作者指出，91/9的比例产生了最
    consistently 的优秀结果。'
- en: '![](../Images/212adb561e42c3bbe4a6878cf7b81c6a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/212adb561e42c3bbe4a6878cf7b81c6a.png)'
- en: Figure 5d from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的图5d
- en: '**Fourth**, synthetic data helps with few-shot learning. VeCap stands for Visual-enriched
    Caption, which is a way of creating captions so that they are sure to describe
    key visual pieces of the image. For the reverse, imagine a caption that may explain
    the meaning behind a photo but doesn’t explain any of the elements in the photo.
    You would typically do this if your data-scraper found images with poor alt-text
    data.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四**，合成数据有助于少样本学习。VeCap代表视觉增强描述，它是一种创建描述的方法，确保这些描述能够准确表达图像中的关键视觉信息。相反，想象一个描述，可能解释了照片背后的意义，但并没有解释照片中的任何元素。如果你的数据抓取工具找到了描述不准确的图片，你通常会做这种处理。'
- en: The authors here concluded that VeCap gives a “non-trivial” boost in few-shot
    reasoning, but has a relatively small increase in quality. This raises questions
    about the cost-effectiveness of VeCap.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在此得出结论，VeCap在少样本推理中提供了“非平凡”的提升，但在质量上提升较小。这引发了关于VeCap性价比的问题。
- en: Results
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'Using the results from their ablations, the authors created a Transformer in
    two-forms: Mixture-of-Expert and regular. Both models had an encoder with a 378
    x 378 image, pre-trained with DFN-5B dataset only. They had a mix of 45% captioned
    data, 45% interleaved data, and 10% text-only data (approximating the 91:9 ratio
    of image to text data). The VL Connector had 144 tokens and they chose a C Abstractor,
    though they point out that this was a somewhat arbitrary choice. For the LLM itself,
    they created a 3B, 7B, and 30B parameter model (with the MoE model only going
    up to 7B). The graph below shows how the these models performed.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过他们的消融实验结果，作者创建了两种形式的变换器：专家混合（Mixture-of-Expert）和常规形式。两个模型的编码器处理的是378 x 378的图像，且仅用DFN-5B数据集进行了预训练。它们的数据包含45%的带字幕数据，45%的交错数据和10%的仅文本数据（近似图像与文本数据的91:9比例）。VL连接器有144个令牌，他们选择了一个C摘要器，尽管他们指出这是一个相对随意的选择。对于大语言模型本身，他们创建了一个3B、7B和30B参数的模型（其中MoE模型仅支持到7B）。下图显示了这些模型的表现。
- en: '![](../Images/2e8811a3e70ecaa26b03e606a7725e08.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8811a3e70ecaa26b03e606a7725e08.png)'
- en: Table 4 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[论文](https://arxiv.org/pdf/2403.09611.pdf)的表格4
- en: Interestingly, the 30B parameter model performs on par with other models which
    have billions more parameters than it (LLaVA-NeXT-34B, etc.), suggesting that
    there may be some quantum relationship between parameter size and performance
    here.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这个30B参数的模型与其他拥有数十亿更多参数的模型（如LLaVA-NeXT-34B等）表现相当，暗示参数大小与性能之间可能存在某种量子关系。
- en: Closing Thoughts
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Multi-modal LLMs are an incredibly exciting part of the field. As we find better
    ways to transmit different data types into tokens, we may unlock even greater
    applications for these transformers. As we look to the future, it is not unreasonable
    now to consider how other senses could be inputed outside of a text description,
    such as sound, smell, or even touch. Data quality is likely to only become more
    valuable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态大语言模型是这一领域令人兴奋的一个重要部分。随着我们找到更好的方法将不同类型的数据转化为令牌，我们可能会解锁这些变换器的更多应用。展望未来，考虑如何将除文本描述以外的其他感官输入（如声音、气味甚至触觉）结合进来并不不合理。数据质量很可能会变得愈加宝贵。
- en: As the authors concluded that the different language connectors don’t make a
    major difference, it will be interesting to see if this means research should
    focus on the image encoder, or rather if we simply haven’t found a true breakthrough
    way to use the VL connector.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于作者得出结论认为不同的语言连接器没有显著差异，因此很有趣的是，研究是否应该集中于图像编码器，还是我们只是尚未找到真正突破性的方式来使用VL连接器。
- en: Outside of this specific paper, one of the big questions that arises is how
    these MLLMs will perform outside of benchmarks. As LLMs have proliferated, one
    common criticism revolves around the use of benchmarks to compare them. Often
    times these benchmarks use a consistent dataset to compare, allowing one model
    to do better simply by overfitting, even if unintentionally. Using methodologies
    like ELO, the chess rating algorithm, in the [LLM Arena from lmsys](https://chat.lmsys.org/)
    may give a better true comparison of model performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这篇特定的论文外，一个大问题是这些多模态大语言模型（MLLMs）在基准测试之外的表现如何。随着大语言模型（LLMs）的普及，常见的批评之一是围绕基准测试进行的比较。通常这些基准测试使用一致的数据集进行比较，允许某个模型通过过拟合来获得更好的表现，即使这种过拟合并非故意。像[LLM
    Arena from lmsys](https://chat.lmsys.org/)这样的使用ELO棋类评级算法的方法，可能会提供一个更真实的模型表现比较。
- en: In closing, as more inputs are able to be connected to LLMs, one can expect
    that the number of applications they can be applied to will increase. Only time
    will tell how useful we can make this technology.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，随着更多输入能够与大语言模型相连接，可以预期它们的应用领域将会增加。只有时间才能告诉我们如何让这项技术变得更加有用。
- en: '[1] McKinzie, B., et al. [“MM1: Methods, Analysis & Insights from Multimodal
    LLM Pre-training” (2024)](https://arxiv.org/pdf/2403.09611.pdf), arXiv'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] McKinzie, B., 等人. [“MM1：多模态大语言模型预训练方法、分析与洞察”（2024）](https://arxiv.org/pdf/2403.09611.pdf)，arXiv'
- en: '[2] Cha, J., et al. [“Honeybee: Locality-enhanced Projector for Multimodal
    LLM”](https://arxiv.org/pdf/2312.06742.pdf) (2023), arXiv'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cha, J., 等人. [“Honeybee：多模态大语言模型的局部增强投影器”](https://arxiv.org/pdf/2312.06742.pdf)（2023），arXiv'
- en: '[3] Antoniadis, P., et al. [“Machine Learning: What Is Ablation Study?”](https://www.baeldung.com/cs/ml-ablation-study)
    (2024), arXiv'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Antoniadis, P. 等人. [“机器学习：什么是消融研究？”](https://www.baeldung.com/cs/ml-ablation-study)
    (2024), arXiv'
- en: '[4] Radford, A., et al. [“Learning Transferable Visual Models From Natural
    Language Supervision”](https://arxiv.org/pdf/2103.00020.pdf) (2021), arXiv'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Radford, A. 等人. [“从自然语言监督中学习可迁移的视觉模型”](https://arxiv.org/pdf/2103.00020.pdf)
    (2021), arXiv'
- en: '[5] El-Nouby, Al., et al. [“Scalable Pre-training of Large Autoregressive Image
    Models”](https://arxiv.org/pdf/2401.08541.pdf) (2024), arXiv'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] El-Nouby, Al. 等人. [“大规模预训练自回归图像模型”](https://arxiv.org/pdf/2401.08541.pdf)
    (2024), arXiv'
- en: '[6] Vaswani, A., et al., “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Vaswani, A. 等人. “[注意力即一切](https://arxiv.org/pdf/1706.03762.pdf)” (2017),
    arXiv'
- en: '[7] Lai, Z., et al., [“VeCLIP: Improving CLIP Training via Visual-enriched
    Captions”](https://arxiv.org/abs/2310.07699) (2023), arXiv'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Lai, Z. 等人. [“VeCLIP：通过视觉增强标题改进CLIP训练”](https://arxiv.org/abs/2310.07699)
    (2023), arXiv'
