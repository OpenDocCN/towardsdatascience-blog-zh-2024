- en: Multimodal Large Language Models & Apple’s MM1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13](https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This blog post will go into the architecture and findings behind Apple’s “MM1:
    Methods, Analysis & Insights from Multimodal LLM Pre-training” paper'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------)
    ·9 min read·Apr 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b89777cfb3161fa6ce57a05fdb4277c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author Generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Abstraction is one of the most critical concepts in Computer Science, with some
    of the most powerful implications. From a simplistic point of view, abstraction
    is the ability to take something and apply it to multiple distinct situations.
    For example, if you create a way to successfully sort apples based on their size
    in a factory, your solution could be abstracted to also sort oranges or peaches
    in the same way. Thus, through abstraction a very powerful solution is able to
    radically impact multiple parts of the world.
  prefs: []
  type: TYPE_NORMAL
- en: While Large Language Models are exceptional at reasoning when given text as
    an input, recently we have been able to abstract their input so that they can
    reason with images and sounds.
  prefs: []
  type: TYPE_NORMAL
- en: The below blog post goes into architectural ablations in Apple’s MM1 paper and
    their research findings when building a Multimodal Large Language Model (MLLM).
  prefs: []
  type: TYPE_NORMAL
- en: Abstracting LLM Input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The architecture behind Large Language Models can be traced back to the 2017
    paper “Attention is All You Need” where the Transformer Architecture was introduced.
  prefs: []
  type: TYPE_NORMAL
- en: This paper showed how you could transform human language into tokens that a
    neural network would then process (in that paper’s case, process into a different
    language).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8537829bb3d08c52694fbb294fe530d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 from [“Attention is All You Need”](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the image, we have a transformation occurring early on where
    we take the input and convert it into tokens (the embedding section). However,
    there is no inherent reason why only text data can be mapped to tokens. Consequently,
    the field began trying to map other kinds of data to tokens.
  prefs: []
  type: TYPE_NORMAL
- en: MM1 Architecture Base
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/effb93869b2820023bfd10695c290fa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of Figure 3 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Apple’s model had 3 key components: a visual transformer (ViT) image encoder,
    Vision-Language Connector, and a Large Language Model. Assuming you already have
    a good idea of what a LLM is and how it works, let’s dive into the image encoder
    and VL connector.'
  prefs: []
  type: TYPE_NORMAL
- en: Image Encoders & Visual Connectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While from an abstracted view we can imagine text and images as simply different
    kinds of inputs, to make this work we need to accept that we may have to treat
    them differently to get them into token form. At this time, we have 2 different
    systems that help us transform the image into tokens the LLM can reason with:
    image encoder and connector.'
  prefs: []
  type: TYPE_NORMAL
- en: First, the image encoder is responsible for taking our image and converting
    it into the token representation that our transformer model can understand.
  prefs: []
  type: TYPE_NORMAL
- en: Second, a connector is the piece that takes data from the vision encoder and
    transforms it into the data that is passed directly to the large language model.
    Given the image encoder returns tokens, you may wonder why we need the connector
    at all. The idea appears to be that image encoders give too much information in
    their tokens, so to reduce costs while optimizing reasoning, we want to be selective
    with what we pass through.
  prefs: []
  type: TYPE_NORMAL
- en: The below image shows the data flow we’re working with here.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d88a4a539cf61aeecfdaea062e1ce7a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2 from [“Honeybee: Locality-enhanced Projector for Multimodal LLM”](https://arxiv.org/pdf/2312.06742.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ablation study in Machine Learning revolves around removing and modifying
    certain parts of a model to see how they contribute to overall performance. Apple’s
    research centered around different ways of training the Image Encoder, different
    projectors for the VL Connector, and different pre-training data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive into the major findings.
  prefs: []
  type: TYPE_NORMAL
- en: Image Encoder Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the Image Encoder, they varied between CLIP and AIM models, Image resolution
    size, and the dataset the models were trained on. The below chart shows you the
    results for each ablation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72efc54bb2d9ecd0a6ef4975725f6171.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the major pieces above and explain what they are.
  prefs: []
  type: TYPE_NORMAL
- en: '**CLIP** stands for Contrastive Language Image Pre-training and is meant to
    help your model learn visual concepts by providing names to the things that are
    meant to be seen as text. As the image below shows, this pairs images with text
    encodings so that the model will eventually connect the vision tokens (represented
    in the below image as I, with the text tokens T). This method is called contrastive
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ee9a0733e16658d9ed133ce1a2ddda0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1 from [“Learning Transferable Visual Models From Natural Language Supervision”](https://arxiv.org/pdf/2103.00020.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**AIM** stands for Autoregressive Image Model, and it is trained via a reconstructive
    loss optimization algorithm. The goal here is to see if the transformer can recreate
    (reconstruct) the image that it is given.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2df9b1a9690d422713ce7d372da6f989.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 from [“Scalable Pre-training of Large Autoregressive Image Models”](https://arxiv.org/pdf/2401.08541.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Resolution** here refers to the number of pixels that is fed into the
    transformer. For example, a 378 x 378 image resolution means we will pass in a
    matrix of that size and then convert it into embeddings that the model will then
    be trained on. Training Datawas split between the (DFN-2B), (DFN-5B), (DFN-5B
    + VeCap) and (ImageText-400M).'
  prefs: []
  type: TYPE_NORMAL
- en: The authors found that image resolution was of highest importance, followed
    by model size and then the training data contents. Specifically, they saw that
    the better the image resolution, the better the model tended to perform for both
    zero-shot and few-shot prompting. As more compute is needed to train and run models
    with higher image resolution requirements, this suggests that for Vision Transformers,
    compute will remain of paramount importance.
  prefs: []
  type: TYPE_NORMAL
- en: VL Connection Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the VL Connector, they tested using 64 or 144 tokens for the image, tested
    using 224, 336, and 378 for the image resolution, and chose between a few architectures.
    I’ll briefly go over the architectures below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Average Pooling** is exactly what it sounds like, taking the average of all
    of the tokens, and then doing a linear projection of this average so that the
    grid was 8x8 or 12x12.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Attention Pooling** makes the assumption that image tokens should be treated
    as samples from a fundamentally different population set than the text tokens.
    Here we adjust how many tokens are fed in for each image, in the paper referred
    to as k learnable queries. The researchers only considered k of either 64 or 144.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional Mapping** is a a method from Honeybee that uses a ResNet to
    dynamically decide how many tokens to pass through to the LLM from the image.
    This is actualized in the C-Abstractor module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4848e1619cc1b54fc72ebf7f388afa54.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the above, the different architectures actually had very
    little impact. As one might guess, the higher resolution images and the more tokens
    passed through increased performance among all of the connectors but not dramatically
    so.
  prefs: []
  type: TYPE_NORMAL
- en: This finding suggests we either haven’t found a significantly better way to
    connect the image encoder to the LLM, or that this area is simply not where great
    models will differentiate themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Training Data Ablations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/a7eba075bb1027a4aff1a184921b0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 2 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the authors played with 4 different kinds of data: captioned images,
    synthetically captioned images, interleaved image-text data, and text-only data.
    They found 4 lessons, each with a graph to summarize the performance changes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/817a2d9f60801091eb142e89d99b205a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5a from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**First**, interleaving data helps with few-shot and text-only performance,
    while captioned data helps with zero-shot performance. The researchers varied
    how much interleaving they did, with the graph below showing the results. As you
    can see, few-shot prompts performed noticeably better on models trained with interleaved
    data than the models trained with all or nothing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ac3c1384e53bdf08a5a445bbb9a4fd8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5b from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**Second**, Text-only data helps with few-shot reasoning. Text-only in this
    context means that the training data includes image examples and text-only examples.
    This was done to ensure that the model understands human language as well as images.
    Comparing the caption-only to caption-with-text shows a marked improvement for
    all but the 0-shot reasoning, however, interleaved-only performs better than interleaved-plus-text
    for all but the TextCore test.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edf1887d66e248908c526bf475ee57b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5c from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**Third**, if you get the mixture right between image and text you can get
    really strong performance. The above graph shows different ratios of interleaved
    + captioned data to text-only data. As the goal is to have a multi-modal model,
    they never tested the performance if you do not have any image data. The authors
    here point out that the 91/9 ratio produced the most consistently good results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/212adb561e42c3bbe4a6878cf7b81c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5d from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**Fourth**, synthetic data helps with few-shot learning. VeCap stands for Visual-enriched
    Caption, which is a way of creating captions so that they are sure to describe
    key visual pieces of the image. For the reverse, imagine a caption that may explain
    the meaning behind a photo but doesn’t explain any of the elements in the photo.
    You would typically do this if your data-scraper found images with poor alt-text
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors here concluded that VeCap gives a “non-trivial” boost in few-shot
    reasoning, but has a relatively small increase in quality. This raises questions
    about the cost-effectiveness of VeCap.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the results from their ablations, the authors created a Transformer in
    two-forms: Mixture-of-Expert and regular. Both models had an encoder with a 378
    x 378 image, pre-trained with DFN-5B dataset only. They had a mix of 45% captioned
    data, 45% interleaved data, and 10% text-only data (approximating the 91:9 ratio
    of image to text data). The VL Connector had 144 tokens and they chose a C Abstractor,
    though they point out that this was a somewhat arbitrary choice. For the LLM itself,
    they created a 3B, 7B, and 30B parameter model (with the MoE model only going
    up to 7B). The graph below shows how the these models performed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e8811a3e70ecaa26b03e606a7725e08.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 4 from [the paper](https://arxiv.org/pdf/2403.09611.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the 30B parameter model performs on par with other models which
    have billions more parameters than it (LLaVA-NeXT-34B, etc.), suggesting that
    there may be some quantum relationship between parameter size and performance
    here.
  prefs: []
  type: TYPE_NORMAL
- en: Closing Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-modal LLMs are an incredibly exciting part of the field. As we find better
    ways to transmit different data types into tokens, we may unlock even greater
    applications for these transformers. As we look to the future, it is not unreasonable
    now to consider how other senses could be inputed outside of a text description,
    such as sound, smell, or even touch. Data quality is likely to only become more
    valuable.
  prefs: []
  type: TYPE_NORMAL
- en: As the authors concluded that the different language connectors don’t make a
    major difference, it will be interesting to see if this means research should
    focus on the image encoder, or rather if we simply haven’t found a true breakthrough
    way to use the VL connector.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of this specific paper, one of the big questions that arises is how
    these MLLMs will perform outside of benchmarks. As LLMs have proliferated, one
    common criticism revolves around the use of benchmarks to compare them. Often
    times these benchmarks use a consistent dataset to compare, allowing one model
    to do better simply by overfitting, even if unintentionally. Using methodologies
    like ELO, the chess rating algorithm, in the [LLM Arena from lmsys](https://chat.lmsys.org/)
    may give a better true comparison of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: In closing, as more inputs are able to be connected to LLMs, one can expect
    that the number of applications they can be applied to will increase. Only time
    will tell how useful we can make this technology.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] McKinzie, B., et al. [“MM1: Methods, Analysis & Insights from Multimodal
    LLM Pre-training” (2024)](https://arxiv.org/pdf/2403.09611.pdf), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Cha, J., et al. [“Honeybee: Locality-enhanced Projector for Multimodal
    LLM”](https://arxiv.org/pdf/2312.06742.pdf) (2023), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Antoniadis, P., et al. [“Machine Learning: What Is Ablation Study?”](https://www.baeldung.com/cs/ml-ablation-study)
    (2024), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Radford, A., et al. [“Learning Transferable Visual Models From Natural
    Language Supervision”](https://arxiv.org/pdf/2103.00020.pdf) (2021), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] El-Nouby, Al., et al. [“Scalable Pre-training of Large Autoregressive Image
    Models”](https://arxiv.org/pdf/2401.08541.pdf) (2024), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Vaswani, A., et al., “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Lai, Z., et al., [“VeCLIP: Improving CLIP Training via Visual-enriched
    Captions”](https://arxiv.org/abs/2310.07699) (2023), arXiv'
  prefs: []
  type: TYPE_NORMAL
