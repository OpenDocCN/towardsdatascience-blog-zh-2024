- en: 'Using Sun RGB-D: Indoor Scene Dataset with 2D & 3D Annotations'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09](https://towardsdatascience.com/using-sun-rgb-d-indoor-scene-dataset-with-2d-3d-annotations-387b9af5c89e?source=collection_archive---------9-----------------------#2024-03-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simple Python code for accessing Sun RGB-D and similar datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[![Maxwell
    .J. Jacobson](../Images/263fd8189950c372df0d4156770cd0d8.png)](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    [Maxwell .J. Jacobson](https://medium.com/@mjacobson130?source=post_page---byline--387b9af5c89e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--387b9af5c89e--------------------------------)
    ·9 min read·Mar 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a780e5dfb42b06f860a655e97baa1c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D understanding from 2D images is the first step into a larger world.
  prefs: []
  type: TYPE_NORMAL
- en: As many of the primitive tasks in computer vision approach a solved state —
    decent, quasi-general solutions now being available for image [segmentation](https://segment-anything.com/)
    and [text-conditioned generation](https://en.wikipedia.org/wiki/Stable_Diffusion),
    with general answers to visual question answering, depth estimation, and general
    object detection well on the way — I and many of my colleagues have been looking
    to use CV in larger tasks. When a human looks at a scene, we see more than flat
    outlines. We comprehend more than a series of labels. We can perceive and imagine
    within 3D spaces. We see a scene, and we can understand it in a very complete
    way. This capability should be within reach for CV systems of the day… If only
    we had the right data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Sun RGB-D](https://rgbd.cs.princeton.edu/) is an interesting image dataset
    from 2015 that satiates many of the data hungers of total scene understanding.
    This dataset is a collection of primarily indoor scenes, collected with a digital
    camera and four different 3D scanners. The linked publication goes into greater
    detail on how the dataset was collected and what it contains. Most importantly
    though, this dataset contains a wealth of data that includes both 2D and 3D annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c9cc9a80fee7f342ff3c937b11c7160.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [*SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite*](https://rgbd.cs.princeton.edu/paper.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: With this dataset, CV and ML algorithms can learn much deeper (excuse the pun)
    features from 2D images. More than that though, using data like this could open
    opportunities in applying 3D reasoning to 2D images. But that is a story for another
    time. This article will simply provide the basic python code to access this Sun
    RGB-D data, so that readers can use this wonderful resource in their own projects.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After downloading the dataset from [here](https://rgbd.cs.princeton.edu/data/SUNRGBD.zip),
    you will end up with a directory structure like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ced5a3bbbfa1cceb41338a70c924e452.png)'
  prefs: []
  type: TYPE_IMG
- en: These separate the data by the type of scanner used to collect them. Specifically,
    the Intel RealSense 3D Camera for tablets, the Asus Xtion LIVE PRO for laptops,
    and the Microsoft Kinect versions 1 and 2 for desktop.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b571d38ef8f0c1a36951a8a8b0cf5df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [*SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite*](https://rgbd.cs.princeton.edu/paper.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving into “kv2”, we see two directories: align_kv2 and kinect2data. This
    is one problem with the Sun RGB-D dataset… its directory structure is not consistent
    for each sensor type. In “realsense”, there are four directories containing data:
    lg, sa, sh, and shr. In “xtion” there is a more complex directory structure still.
    And worse, I have been unable to find a clear description of how these sub-directories
    are different anywhere in the dataset’s paper, supplementary materials, or website.
    **If anyone knows the answer to this, please let me know!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the time being though, lets skip down into the consistent part of the dataset:
    the data records. For align_kv2, we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fc4e9a92575deee653da07410badf2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For all of the data records across all of the sensor types, this part is largely
    consistent. Some important files to look at are described below:'
  prefs: []
  type: TYPE_NORMAL
- en: '*annotation2Dfinal* contains the most recent 2D annotations including polygonal
    object segmentations and object labels. These are stored in a single JSON file
    which has the x and y 2D coordinates for each point in each segmentation, as well
    as a list for object labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*annotation3Dfinal* is the same for 3D annotations. These are in the form of
    bounding shapes — polyhedra that are axis-aligned on the y (up-down) dimension.
    These can also be found in the singular JSON file of the directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*depth* contains the raw depth images collected by the sensor. *depth_bfx*
    contains a cleaned-up copy that addresses some of the limitations from the sensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original image can be found in the image directory. A full resolution, uncropped
    version can also be found in fullres.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor extrinsics and intrinsics are saved in text files as numpy-like arrays.
    *intrinsics.txt* contains the intrinsics, but extrinsics is stored in the singular
    text file within the extrinsics folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the type of scene (office, kitchen, bedroom, etc) can be found as a
    string in *scene.txt*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first, we will need to read in files from a few formats. JSON and
    txt primarily. From those text files, we need to pull out a numpy array for both
    the extrinsics and intrinsics of the sensor. There are also allot of files here
    that don’t seem to follow a strict naming convention but will be the only one
    of its type in the same directory, so get_first_file_path will be useful here.
  prefs: []
  type: TYPE_NORMAL
- en: I’d also like this code to output a simple 3D model of the rooms we find in
    the dataset. This can give us some easy data visualization, and lets us distill
    down the basic spatial features of a scene. To achieve this, we’ll utilize the
    OBJ file format, a standard for representing 3D geometry. An OBJ file primarily
    consists of lists of vertices (points in 3D space), along with information on
    how these vertices are connected to form faces (the surfaces of the 3D object).
    The layout of an OBJ file is straightforward, beginning with vertices, each denoted
    by a line starting with ‘v’ followed by the x, y, and z coordinates of the vertex.
    Faces are then defined by lines starting with ‘f’, listing the indices of the
    vertices that form each face’s corners, thus constructing the 3D surface.
  prefs: []
  type: TYPE_NORMAL
- en: In our context, the bounding shapes that define the spatial features of a scene
    are polyhedra, 3D shapes with flat faces and straight edges. Given that the y
    dimension is axis-aligned — meaning it consistently represents the up-down direction
    across all points — we can simplify the representation of our polyhedron using
    only the x and z coordinates for defining the vertices, along with a global minimum
    (min_y) and maximum (max_y) y-value that applies to all points. This approach
    assumes that vertices come in pairs where the x and z coordinates remain the same
    while the y coordinate alternates between min_y and max_y, effectively creating
    vertical line segments.
  prefs: []
  type: TYPE_NORMAL
- en: The `write_obj` function encapsulates this logic to construct our 3D model.
    It starts by iterating over each bounding shape in our dataset, adding vertices
    to the OBJ file with their x, y, and z coordinates. For each pair of points (with
    even indices representing min_y and odd indices representing max_y where x and
    z are unchanged), the function writes face definitions to connect these points,
    forming vertical faces around each segment (e.g., around vertices 0, 1, 2, 3,
    then 2, 3, 4, 5, and so on). If the bounding shape has more than two pairs of
    vertices, a closing face is added to connect the last pair of vertices back to
    the first pair, ensuring the polyhedron is properly enclosed. Finally, the function
    adds faces for the top and bottom of the polyhedron by connecting all min_y vertices
    and all max_y vertices, respectively, completing the 3D representation of the
    spatial feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, lets make the basic structure of our dataset, with a class that represents
    a dataset (a directory with subdirectories each containing a data record) and
    the data records themselves. This first object has a very simple function: it
    will create a new record object for every sub-directory within ds_dir.'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing 2D Segmentations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accessing 2D segmentation annotations is easy enough. We must make sure to load
    the json file in annotation2Dfinal. Once that is loaded as a python dict, we can
    extract the segmentation polygons for each object in the scene. These polygons
    are defined by their x and y coordinates, representing the vertices of the polygon
    in the 2D image space.
  prefs: []
  type: TYPE_NORMAL
- en: We also extract the object label by storing the object ID that each bounding
    shape contains, then cross-referencing with the ‘objects’ list. Both the labels
    and segmentations are returned by `get_segments_2d`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the transpose operation is applied to the coordinates array to shift
    the data from a shape that groups all x coordinates together and all y coordinates
    together into a shape that groups each pair of x and y coordinates together as
    individual points.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing 3D Bounding Shapes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accessing the 3D bounding shapes is a bit harder. As mentioned before, they
    are stored as y-axis aligned polyhedra (x is left-right, z is forward-back, y
    is up-down). In the JSON, this is stored as a polygon with an min_y and max_y.
    This can be extracted to a polyhedron by taking each 2D point of the polygon,
    and adding two new 3D points with min_y and max_y.
  prefs: []
  type: TYPE_NORMAL
- en: The JSON also provides a useful field which states whether the bounding shape
    is rectangular. I have preserved this in our code, along with functions to get
    the type of each object (couch, chair, desk, etc), and the total number of objects
    visible in the scene.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Room Layout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the room layout has its own polyhedron that encapsulates all others.
    This can be used by algorithms to understand the broader topology of the room
    including the walls, ceiling, and floor. It is accessed in much the same way as
    the other bounding shapes.
  prefs: []
  type: TYPE_NORMAL
- en: Full Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Below is the full code with a short testing section. Besides visualizing the
    2D annotations from one of the data records, we also save 3d .obj files for each
    identified object in the scene. You can use a program like [meshlab](https://www.meshlab.net/)
    to visualize the output. The sensor intrinsics and extrinsics have also been extracted
    here. Intrinsics refer to the internal camera parameters that affect the imaging
    process (like focal length, optical center, and lens distortion), while extrinsics
    describe the camera’s position and orientation in a world coordinate system. They
    are important for accurately mapping and interpreting 3D scenes from 2D images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code is also available here: [https://github.com/arcosin/Sun-RGDB-Data-Extractor](https://github.com/arcosin/Sun-RGDB-Data-Extractor).'
  prefs: []
  type: TYPE_NORMAL
- en: This repo may or may not be updated in the future. I would love to add functionality
    for accessing this as a PyTorch dataset with minibatches and such. If anyone has
    some easy updates, feel free to make a PR.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d0abf02fbb23ee0cbd227d843dd925a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: the simple 3D representation of the scene shown in meshlab. Note the
    transparent room bounding shape and the many objects represented as boxes. Right:
    the original image.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I hope this guide has been helpful in showing you how to use the Sun RGB-D Dataset.
    More importantly, I hope it’s given you a peek into the broader skill of writing
    quick and easy code to access datasets. Having a tool ready to go is great, but
    understanding how that tool works and getting familiar with the dataset’s structure
    will serve you better in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: Extra Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article has introduced some easy-to-modify python code for extracting data
    from the Sun RGB-D dataset. Note that [an official MATLAB toolbox](https://rgbd.cs.princeton.edu/data/SUNRGBDtoolbox.zip)
    for this dataset already exists. But I don’t use MATLAB so I didn’t look at it.
    If you are a MATLABer (MATLABster? MATLABradour? eh…) then that might be more
    comprehensive.
  prefs: []
  type: TYPE_NORMAL
- en: I also found [**this**](https://github.com/luiszeni/SUNRGBDtoolbox_python) for
    python. It’s a good example of extracting only the 2D features. I borrowed some
    lines from it, so go throw it a star if you feel up to it.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article utilizes the Sun RGB-D dataset [1] licensed under [CC-BY-SA](https://paperswithcode.com/dataset/sun-rgb-d).
    This dataset also draws data from previous work [2, 3, 4]. Thank you to them for
    their outstanding contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] S. Song, S. Lichtenberg, and J. Xiao, “SUN RGB-D: A RGB-D Scene Understanding
    Benchmark Suite,” Proceedings of the 28th IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR2015), Oral Presentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] N. Silberman, D. Hoiem, P. Kohli, R. Fergus, “Indoor segmentation and support
    inference from RGBD images,” ECCV, 2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, T. Darrell,
    “A category-level 3-D object dataset: Putting the Kinect to work,” ICCV Workshop
    on Consumer Depth Cameras for Computer Vision, 2011.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] J. Xiao, A. Owens, A. Torralba, “SUN3D: A database of big spaces reconstructed
    using SfM and object labels,” ICCV, 2013.'
  prefs: []
  type: TYPE_NORMAL
