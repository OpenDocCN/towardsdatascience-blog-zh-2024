- en: Interpretable Latent Spaces Using Space-Filling Vector Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/interpretable-latent-spaces-using-space-filling-vector-quantization-e4eb26691b14?source=collection_archive---------7-----------------------#2024-04-08](https://towardsdatascience.com/interpretable-latent-spaces-using-space-filling-vector-quantization-e4eb26691b14?source=collection_archive---------7-----------------------#2024-04-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A new unsupervised method that combines two concepts of vector quantization
    and space-filling curves to interpret the latent space of DNNs.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mohammad.vali?source=post_page---byline--e4eb26691b14--------------------------------)[![Mohammad
    Hassan Vali](../Images/b057aa7bd9e1c629fc3743a7f69f013e.png)](https://medium.com/@mohammad.vali?source=post_page---byline--e4eb26691b14--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e4eb26691b14--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e4eb26691b14--------------------------------)
    [Mohammad Hassan Vali](https://medium.com/@mohammad.vali?source=post_page---byline--e4eb26691b14--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e4eb26691b14--------------------------------)
    ·9 min read·Apr 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This post is a short explanation of our novel unsupervised distribution modeling
    technique called space-filling vector quantization [1] published at Interspeech
    2023 conference. For more details, please look at the paper under [this link](https://www.isca-archive.org/interspeech_2023/vali23_interspeech.pdf).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6cc783b8a0d6df1f2d0381dab42c4cda.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [StockSnap.io](https://stocksnap.io)
  prefs: []
  type: TYPE_NORMAL
- en: Deep generative models are well-known neural network-based architectures that
    learn a latent space whose samples can be mapped to sensible real-world data such
    as image, video, and speech. Such latent spaces act as a black-box and they are
    often difficult to interpret. In this post, we introduce our novel unsupervised
    distribution modeling technique that combines two concepts of space-filling curves
    and vector quantization (VQ) which is called **Space-Filling Vector Quantization**
    (SFVQ) [1]. SFVQ helps to make the latent space interpretable by capturing its
    underlying morphological structure. **Important to note that SFVQ is a generic
    tool for modeling distributions and using it is not restricted to any specific
    neural network architecture nor any data type (e.g. image, video, speech and etc.)**.
    In this post, we demonstrate the application of SFVQ to interpret the latent space
    of a voice conversion model. To understand this post you don’t need to know about
    speech signals technically, because we explain everything in general (not technical).
    Before everything, let me explain what is the SFVQ technique and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Space-Filling Vector Quantization (SFVQ)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Vector quantization**](https://medium.com/towards-data-science/improving-vector-quantization-in-vector-quantized-variational-autoencoders-vq-vae-915f5814b5ce)
    **(VQ)** [2] is a data compression technique similar to k-means algorithm which
    can model any data distribution. The figure below shows a VQ applied on a Gaussian
    distribution. VQ clusters this distribution (gray points) using 32 codebook vectors
    (blue points) or clusters. Each voronoi cell (green lines) contains one codebook
    vector such that this codebook vector is the closest codebook vector (in terms
    of Euclidean distance) to all data points located in that voronoi cell. In other
    words, each codebook vector is the representative vector of all data points located
    in its corresponding voronoi cell. Therefore, applying VQ on this Gaussian distribution
    means to map each data point to its closest codebook vector, i.e. represent each
    data point with its closest codebook vector. For more information about VQ and
    its other variants you can check out [this post](/optimizing-vector-quantization-methods-by-machine-learning-algorithms-77c436d0749d).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81eaa7098328be722e18aa9bddb60c2c.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Quantization applied on a Gaussian distribution using 32 codebook vectors.
    (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Space-filling curve** is a piece-wise continuous line generated with a recursive
    rule and if the recursion iterations are repeated infinitely, the curve gets bent
    until it completely fills a multi-dimensional space. The following figure illustrates
    the Hilbert curve [3] which is a well-known type of space-filling curves in which
    the corner points are defined using a specific mathematical formulation at each
    recursion iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b51dc7c74f0380333dd333794763186e.png)'
  prefs: []
  type: TYPE_IMG
- en: Five first iterations of Hilbert curve to fill a 2D square distribution. (image
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: Getting intuition from space-filling curves, we can thus think of vector quantization
    (VQ) as mapping input data points on a space-filling curve (rather than only mapping
    data points exclusively on codebook vectors as what we do in normal VQ). Therefore,
    we incorporate vector quantization into space-filling curves, such that our proposed
    space-filling vector quantizer (SFVQ) [1] models a D-dimensional data distribution
    by continuous piece-wise linear curves whose corner points are vector quantization
    codebook vectors. The following figure illustrates VQ and SFVQ applied on a Gaussian
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1c09ca46b6cb5e6418d27219253f3fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Codebook vectors (blue points) of a vector quantizer, and a space-filling vector
    quantizer (curve in black) on a Gaussian distribution (gray points). Voronoi regions
    for VQ are shown in green. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: For technical details on how to train SFVQ and how to map data points on SFVQ’s
    curve, please see section 2 in [our paper](https://www.isca-archive.org/interspeech_2023/vali23_interspeech.pdf)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: Note that when we train a normal VQ on a distribution, the adjacent codebook
    vectors that exists inside the learned codebook matrix can refer to totally different
    contents. For example, the first codebook element could refer to a vowel phone
    and the second one could refer to a silent part of speech signal. However, when
    we train SFVQ on a distribution, the learned codebook vectors will be located
    in an arranged form such that adjacent elements in the codebook matrix (i.e. adjacent
    codebook indices) will refer to similar contents in the distribution. We can use
    this property of SFVQ to interpret and explore the latent spaces in Deep Neural
    Networks (DNNs). As a typical example, in the following we will explain how we
    used our SFVQ method to interpret the latent space of a voice conversion model
    [4].
  prefs: []
  type: TYPE_NORMAL
- en: Voice Conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following figure shows a voice conversion model [4] based on vector quantized
    variational autoencoder (VQ-VAE) [5] architecture. According to this model, encoder
    takes the speech signal of speaker A as the input and passes the output into vector
    quantization (VQ) block to extracts the phonetic information (phones) out of this
    speech signal. Then, these phonetic information together with the identity of
    speaker B goes into the decoder which outputs the converted speech signal. The
    converted speech would contain the phonetic information (context) of speaker A
    with the identity of speaker B.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9b1c1e72ab0b9cc21295fb4fa7a726d.png)'
  prefs: []
  type: TYPE_IMG
- en: Voice conversion model based on VQ-VAE architecture. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In this model, the VQ module acts as an information bottleneck that learns a
    discrete representation of speech that captures only phonetic content and discards
    the speaker-related information. In other words, VQ codebook vectors are expected
    to collect only the phone-related contents of the speech. Here, the representation
    of VQ output is considered the latent space of this model. Our objective is to
    replace the VQ module with our proposed SFVQ method to interpret the latent space.
    By interpretation we mean to figure out what phone each latent vector (codebook
    vector) corresponds to.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Latent Space using SFVQ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We evaluate the performance of our space-filling vector quantizer (SFVQ) on
    its ability to find the structure in the latent space (representing phonetic information)
    in the above voice conversion model. For our evaluations, we used the TIMIT dataset
    [6], since it contains phone-wise labeled data using the phone set from [7]. For
    our experiments, we use the following phonetic grouping:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plosives (Stops):** {p, b, t, d, k, g, jh, ch}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fricatives:** {f, v, th, dh, s, z, sh, zh, hh, hv}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nasals:** {m, em, n, nx, ng, eng, en}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vowels:** {iy, ih, ix, eh, ae, aa, ao, ah, ax, ax-h, uh, uw, ux}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-vowels (Approximants):** {l, el, r, er, axr, w, y}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diphthongs:** {ey, aw, ay, oy, ow}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silence:** {h#}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To analyze the performance of our proposed SFVQ, we pass the labeled TIMIT speech
    files through the trained encoder and SFVQ modules, respectively, and extract
    the codebook vector indices corresponding to all existing phones in the speech.
    In other words, we pass a speech signal with labeled phones and then compute the
    index of the learned SFVQ’s codebook vector which those phones are getting mapped
    to them. As explained above, we expect our SFVQ to map similar phonetic contents
    next to each other (index-wise in the learned codebook matrix). To examine this
    expectation, in the following figure we visualize the [spectrogram](https://en.wikipedia.org/wiki/Spectrogram)
    of the sentence *“she had your dark suit”*, and its corresponding codebook vector
    indices for the ordinary vector quantizer (VQ) and our proposed SFVQ.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/090c06ffb8078ca531dbac910b0bea67.png)'
  prefs: []
  type: TYPE_IMG
- en: '*(Top) Codebook vector indices for the speech signal using our proposed SFVQ
    (in dark blue circles) and the ordinary VQ (in gray crosses). (Bottom) Spectrogram
    of the speech signal including codebook vector indices corresponding to speech
    frames. (image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We observe that the indices of the ordinary VQ does not have any particular
    structure. However, when using our proposed SFVQ, there is a clear structure for
    the codebook vector indices. The indices for the speech frames containing fricative
    phones of {sh, s} within the words {she, suit} are uniformly distributed next
    to each other throughout the frames. In addition, silence frames containing phone
    {h#} and some other low energy frames containing {kcl, tcl: k, t closures} within
    the words {dark, suit} are uniformly located next to each other in the range 0–20\.
    Notice that the figure below remains sufficiently consistent for sentences with
    the same phonetic content, even across speakers with different genders, speech
    rhythms, and dialects.'
  prefs: []
  type: TYPE_NORMAL
- en: The figure below demonstrates the histogram of SFVQ’s codebook indices for each
    phonetic group (explained above) for the whole TIMIT speech files. At first glance,
    we observe that consonants:{silence, plosives, fricatives, nasals} and vowels:{vowels,
    diphthongs, approximants} can be separated around index 125 (apart from the peak
    near index 20). We also observe that the most prominent peaks of different groups
    are separated in different parts of the histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02c772787d2662cdd5edb31104c9ebd9.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Histogram of SFVQ’s codebook vector indices for different phonetic groups.
    (image by author)*'
  prefs: []
  type: TYPE_NORMAL
- en: By having this visualization, we have a better understanding of the latent space
    and we can now distinguish which part of the latent space refers to what phonetic
    group. We can even go further in details and explore the latent space in terms
    of phone level. As an example for distribution of phones within a phonetic group,
    the figure below illustrates the histograms of all phones in fricatives group.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54bdd8314ee51c54a2f70671b3d2f5ad.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Histogram of SFVQ’s codebook vector indices for fricative phones. (image by
    author)*'
  prefs: []
  type: TYPE_NORMAL
- en: By observing the most prominent peak for each phone, we find out the peaks of
    similar phones are located next to each other. To elaborate, we listed similar
    phones and their corresponding peak index here as {f:51, v:50}, {th:78, dh:80},
    {s:71, z:67, sh:65, zh:67}, {hh:46, hv:50}. Except {hh, hv} phones, fricatives
    are mainly located in the range 50–85\. Further structures can be readily identified
    from all provided figures by visual inspection.
  prefs: []
  type: TYPE_NORMAL
- en: These experiments demonstrate that our proposed SFVQ achieves a coherently structured
    and easily interpretable representation for latent codebook vectors, which represent
    phonetic information of the input speech. Accordingly, there is an obvious distinction
    of various phonetic groupings such as {consonants vs. vowels}, {fricatives vs.
    nasals vs. diphthongs vs. …}, and we can simply tell apart which phone each codebook
    vector represents. In addition, similar phones within a specific phonetic group
    are encoded next to each other in the latent codebook space. This is the main
    interpretability that we aimed to obtain from a black-box called *latent space*.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of SFVQ over other supervised approaches which tries to make the
    latent space interpretable is that SFVQ does not incur any human labeling and
    manual restrictions on the learned latent space. To make our method interpretable,
    it only requires the user to study the unsupervised learned latent space entirely
    once by observation. This observation needs much much less labeled data than what
    is necessary for supervised training of big models. **Again we want to note that
    SFVQ is a generic tool for modeling distributions and using it is not restricted
    to any specific neural network architecture nor any data type (e.g. image, video,
    speech and etc.)**.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch implementation of our SFVQ technique is publicly available in GitHub
    using the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/MHVali/Space-Filling-Vector-Quantizer.git?source=post_page-----e4eb26691b14--------------------------------)
    [## GitHub - MHVali/Space-Filling-Vector-Quantizer'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to MHVali/Space-Filling-Vector-Quantizer development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/MHVali/Space-Filling-Vector-Quantizer.git?source=post_page-----e4eb26691b14--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Special thanks to my doctoral program supervisor [Prof. Tom Bäckström](https://research.aalto.fi/en/persons/tom-bäckström),
    who supported me and was the other contributor for this work.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] M.H. Vali, T. Bäckström, “Interpretable Latent Space Using Space-Filling
    Curves for Phonetic Analysis in Voice Conversion”, in *Proceedings of Interspeech*,
    2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] M. H. Vali and T. Bäckström, “NSVQ: Noise Substitution in Vector Quantization
    for Machine Learning,” *IEEE Access*, 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] H. Sagan, “Space-filling curves*”,* Springer Science & Business Media,
    2012.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] B. Van Niekerk, L. Nortje, and H. Kamper, “Vector-quantized neural networks
    for acoustic unit discovery in the Zerospeech 2020 challenge”, in *Proceedings
    of Interspeech*, 2020.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] A. Van Den Oord, O. Vinyals, and K. Kavukcuoglu, “Neural Discrete Representation
    Learning,” in *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett,
    and N. L. Dahlgren, “*The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus
    CDROM”,* Linguistic Data Consortium, 1993.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] C. Lopes and F. Perdigao, “Phoneme recognition on the TIMIT database”,
    in *Speech Technologies*. IntechOpen, 2011, ch. 14\. [Online]. Available: [https://doi.org/10.5772/17600](https://doi.org/10.5772/17600)'
  prefs: []
  type: TYPE_NORMAL
