<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Flash Attention (Fast and Memory-Efficient Exact Attention with IO-Awareness): A Deep Dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Flash Attention (Fast and Memory-Efficient Exact Attention with IO-Awareness): A Deep Dive</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b?source=collection_archive---------3-----------------------#2024-05-29">https://towardsdatascience.com/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b?source=collection_archive---------3-----------------------#2024-05-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a394" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Flash attention is power optimization transformer attention mechanism that provides 15% efficiency</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anishdubey?source=post_page---byline--724af489997b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anish Dubey" class="l ep by dd de cx" src="../Images/f85f17fb79718c819b4bd1c9a16338a7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*dFpHgf3fMyv1h6Ut."/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--724af489997b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anishdubey?source=post_page---byline--724af489997b--------------------------------" rel="noopener follow">Anish Dubey</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--724af489997b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/bcd08ecda3389e4ce550e4a823d5c333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20K6yie0J3m5ZZTaYrSzhw.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@sandertraa?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">sander traa</a> on <a class="af nb" href="https://unsplash.com/photos/a-field-with-trees-and-mountains-in-the-background-KV2giR3tbX4?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e79b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Flash attention is a power optimization transformer attention mechanism which provides 15% efficiency in terms of wall-clock speed with no approximation.</p><h1 id="20d0" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Context</h1><p id="cb82" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Given transformer models are slow and memory hungry on long sequences (time and memory complexity is quadratic in nature), flash attention(<a class="af nb" href="https://arxiv.org/pdf/2205.14135" rel="noopener ugc nofollow" target="_blank">paper</a>) provides a 15% end-to-end wall-clock speedup on BERT-large, 3x speed on GPT-2.</p><p id="49c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Considering, enormous amount of energy consumed in training these large models, Flash attention with software and hardware optimization is able to provide 15% efficiency which is a huge win in terms of improvement.</p><p id="2cc8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below, discussion helps to explain some of the basic concepts behind flash attention and how it is implemented.</p><h2 id="d04a" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Basic concepts around compute &amp; memory</h2><p id="490c" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Before we dive deeper into compute and memory, let’s revisit them:</p><p id="6a90" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is Compute?</p><ul class=""><li id="83ef" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Time spent on your GPU computing actual floating point operations (FLOPS)</li></ul><p id="28de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is Memory?</p><ul class=""><li id="15eb" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Time spent transferring tensors within a GPU</li></ul><p id="e18f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ideally, we want our gCPU to be performing matrix multiplication all the time and not restricted by memory. But in reality, compute have made more progress as compared to memory and we are in a world where gCPU sits idle waiting for data to be loaded. This is usually called <strong class="ne fr">memory bound</strong> operation. Refer below on illustrative diagram depicting this. Matrix multiplication is considered compute and memory is storing the data (considering it as a warehouse). Compute need data to process and memory bandwidth has to support that operation.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/fcc27ebfc81919bdb9d016a9290180ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1BYin7nJggwukdYz"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://horace.io/brrr_intro.html" rel="noopener ugc nofollow" target="_blank">https://horace.io/brrr_intro.html</a></figcaption></figure><h2 id="233e" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">What is Memory hierarchy ?</h2><p id="17fb" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The A100 GPU has <strong class="ne fr">40–80GB</strong> of high bandwidth memory with a bandwidth of <strong class="ne fr">1.5–2.0 TB/s</strong> and <strong class="ne fr">192KB </strong>of on-chip SRAM with each 108 streaming multiprocessors with bandwidth estimated around <strong class="ne fr">19TB/s.</strong></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pu"><img src="../Images/9b566bc7147d9ac2cc580a884d4259fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/0*t6tAGouXxEdJCSlW"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.14135</a></figcaption></figure><h1 id="f80d" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">What is the problem with self attention architecture ?</h1><p id="d928" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">With the above context in mind, self attention architecture is <strong class="ne fr">memory-bound.</strong></p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pv"><img src="../Images/ad69afb8ec7fde9260e39d68b245318e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/0*nVb-HhOIfrQsv3X5"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by the Author</figcaption></figure><p id="138b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Looking at attention math, it is a softmax operation which causes the memory-bound.</p><ul class=""><li id="7925" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Quantitative evidence: As you can see below, operations like softmax, dropout, masking are taking majority of the time as compared to Matrix multiplication (Matmul)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/383bf678304f14ad24715ef6a2c2d324.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/0*Mub0abjSQ-Qh3_Ua"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.14135</a></figcaption></figure><h2 id="5764" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Why does softmax become a memory bound operation ?</h2><p id="7612" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The scale at which it operates is our biggest bottleneck. In the below diagram</p><ul class=""><li id="0ac7" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">N -&gt; number of tokens</li><li id="a4e8" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">d -&gt; number of embedding dimensions</li><li id="2b83" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">When Query and Key’ are multiplied, the attention matrix explodes to N * N which takes a lot of memory. For reference (d ~128; N ~128k tokens; google gemini: ~1 million tokens)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qc"><img src="../Images/14c37c8a0960f8b4efef2858021b8a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1HYqcX_WMU4KK-Ji"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://www.youtube.com/watch?v=gMOAud7hZg4&amp;ab_channel=StanfordMLSysSeminars" rel="noopener ugc nofollow" target="_blank">FlashAttention — Tri Dao | Stanford MLSys #67</a></figcaption></figure><h1 id="2f60" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">[Algorithm] How is self attention implemented ?</h1><p id="0741" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Below is the algorithm of implementing self attention mechanism</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qd"><img src="../Images/8a4ac1211ce636bae0007eb2f0462a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vTFkT9ieAk-jtI1l"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.14135</a></figcaption></figure><p id="2f02" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As noted in the above section, transferring information to HBM (write S to HBM) and then loading back from HBM to gCPU to compute softmax and then writing back to HBM is a lot of information traveling making it <strong class="ne fr">memory-bound operation.</strong></p><h1 id="5a08" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">[Matrix multiplication] How is self attention implemented ?</h1><p id="6bf4" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Along with the diagram, below steps help explain how self attention is computed through matrix multiplication</p><h2 id="db52" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Step 1:</h2><ul class=""><li id="e569" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx pq pr ps bk">I have simplified this. In practice, each token is added with positional encoding to generate embeddings to feed into a linear layer to generate &lt;key, query and value&gt;. For illustration I used a dimension of 3 (generally it ranges from 64–128). This is standard transformer architecture input.</li></ul><h2 id="33d3" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Step 2</h2><ul class=""><li id="cd84" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx pq pr ps bk">Key -&gt; Key’ (transpose) is computed, and multiplied with Query to give QK’ which is N*N. This contains the attention of each token with the rest of the tokens. Below diagram shows the relationship as well. Since these are tokens and we need to compute the importance of each token with respect to each other, softmax operation is applied row-wise to normalize it from 0 -1.</li><li id="a8f0" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">This step <strong class="ne fr">requires movement to HBM and is the most expensive operation</strong> as we discussed. Entire flash attention paper is how to optimize this process.</li></ul><h2 id="15c8" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Step 3</h2><ul class=""><li id="68ca" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx pq pr ps bk">Softmax(QK’) * V is computed as the final output matrix. Dimension here is same as input embeddings of Key, query and value.</li><li id="d232" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Final row in the output matrix</li><li id="5cf7" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">1*5 means, the embedding of “this” should be changed to incorporate relations with other tokens.</li><li id="cfab" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">2*5 means, the embedding of “is” should be changed to incorporate relations with other tokens.</li><li id="21cb" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Same as above for rest of the other rows</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/eaefc012968e0a6a8cfdc38a808b28e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*c0IK_Ak2k9US_4rX"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by the Author: Illustrative diagram of how self attention mechanism works</figcaption></figure><h1 id="eb5a" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Basic idea behind the flash attention paper</h1><p id="4591" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Basic idea is explained through the below diagram where blocks of key, query and value are propagated from HBM to SRAM and through some mathematical tricks (explained below), the computation done here is not an approximate but actual correct answer.</p><p id="6f66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With this implementation, paper is able to reduce the wall-speed time by accessing information in blocks without sacrificing correctness.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qf"><img src="../Images/9d12c7f05ad017b1df7db1f66667f067.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/0*bfaMz5EIvDISi1PD"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo from <a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.14135</a></figcaption></figure><h2 id="fdaa" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">Algorithm behind the paper: How is Flash attention implemented ?</h2><p id="1074" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">This is the most complex part of the paper. Let’s break this problem into sub-aspects and dive deeper.</p><p id="ac17" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below diagram breaks the matrix into blocks and how each block is used to compute partial softmax and then correct softmax.</p><ul class=""><li id="1d2b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Initial input: Token: This is flash attention paper</li><li id="2a39" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Key: 4 (tokens) X 3(dimensions), Query: 4 (tokens) X 3(dimensions) and Value: 4 (tokens) X 3(dimensions)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/bb787277f6cd09c9f7741daa795bc448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*auizjqRWEK67jh6a"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image modified by author. Original image from <a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2205.14135</a></figcaption></figure><p id="cad9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Step 0</p><ul class=""><li id="5f0f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Assume memory is 24 bytes</li><li id="6bd3" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">SRAM will be divided into 4 blocks (Query, Key, Value and output matrix)</li><li id="e542" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Query, Key, Value, Output will get = 6 bytes each to store their info (12 bytes/4)</li><li id="ef5e" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Each dimension is 3 since each embedding can not be broken, so</li><li id="49f2" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Query: 6 bytes/ 3 (dimension) = 2. Same for value, key and output</li><li id="272d" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Hence, [M/4d] gives the size of each block. In this case, the block size is 2. It means 2 rows can be fetched into SRAM.</li><li id="b455" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">In general sense, Block size is [M/4d] and # of blocks is [N*4D/M]</li></ul><p id="718e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Step 1 &amp; 2: Adding a table below which illustrates steps 1 and 2 on how flash attention works and compare memory and computation aspect of it.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qg"><img src="../Images/1f3283d51b3061c764359ef361e8e74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mMVf6QMEWYiee1Gcq6qq9w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by the Author: Step by step break-down of memory &amp; computation usage in Flash attention</figcaption></figure><p id="91df" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below diagram helps visualize matrix multiplication (block by block) used in flash attention.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/e22bba7a834d0f89407473bbae1181ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WCqB2nCvjhBRol6L"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by the Author: Illustrative diagram of how flash attention mechanism works</figcaption></figure><h2 id="71f5" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">What is the mathematical aspect of softmax ?</h2><p id="ba7c" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">One of the most critical aspects of the paper on how breaking down matrices still results in computing softmax accuracy. Leaving the mathematical example below on how to show two different matrices can be clubbed to compute softmax again.</p><p id="f6f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Intuition</p><ul class=""><li id="dcd0" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">This is the beautiful property of exponents which is leveraged here.</li><li id="df94" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Each softmax is computed individually but along with this maximum value of the row is stored along with the summed exponent value.</li><li id="4964" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">When merging with another matrix , we need to check how much max differs with the global max of 2 matrices. And because of the exponent, both numerator and denominator are adjusted with e^(current_max — global_max) to incorporate this.</li></ul><p id="7ff5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Logic is quite complex and hence leaving an example below to go through. Once familiarized with an example, the above intuition will make a lot of sense.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/fa60177ed659c487bcb14b653e1919c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kL0yfKX44qpMf0q_"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by the Author: Example to demonstrate how breaking matrix into sub-components and eventually combining them to compute softmax</figcaption></figure><h1 id="a91f" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Complexity analysis</h1><p id="5417" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Let’s look at complexity analysis to get a sense of how things changed</p><p id="8950" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Self attention</p><ul class=""><li id="7fa6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">While computing S = QK’ it becomes a N*N matrix which needs to be propagated back to HRAM and then pulled back from HRAM.</li><li id="442f" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Hence O(N*N + N*N) = O(N*N) is HBM access</li></ul><p id="b849" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Flash attention</p><ul class=""><li id="89bf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pq pr ps bk">Outer loop: Key and Query will be accessed O(Nd) times</li><li id="284f" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Inner loop: Only O(Nd/M) will be needed to load from HBM since operating on blocks</li><li id="e037" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Overall: O(N*N*d*d/M)</li><li id="04ba" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Practically, d is much smaller than M. d ranges from (64–128) while M ranges from 100 KB and hence HBM access is optimized</li></ul><h1 id="0254" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Conclusion</h1><ul class=""><li id="6feb" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx pq pr ps bk">We started with the objective of optimizing HBM access and with this complexity analysis, we see the paper has optimized the <strong class="ne fr">HBM access by (d*d/M) factor with no approximation.</strong></li></ul><p id="0e1b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Such a complex paper with huge improvement in efficiency. I hope the above explanation gives some intuition on how flash attention optimizes and improves the performance. I haven’t covered block sparse flash attention, how does this compare with other optimization techniques, forwards pass optimization etc. Hopefully to cover it in a future post.</p><h2 id="d2b8" class="oz nz fq bf oa pa pb pc od pd pe pf og nl pg ph pi np pj pk pl nt pm pn po pp bk">References</h2><ul class=""><li id="bc0f" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx pq pr ps bk">Paper:<a class="af nb" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank"> FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li><li id="c5fe" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Tri Dao’s talk:<a class="af nb" href="https://www.youtube.com/watch?v=gMOAud7hZg4&amp;ab_channel=StanfordMLSysSeminars" rel="noopener ugc nofollow" target="_blank"> FlashAttention — Tri Dao | Stanford MLSys #67</a></li><li id="e486" class="nc nd fq ne b go px ng nh gr py nj nk nl pz nn no np qa nr ns nt qb nv nw nx pq pr ps bk">Medium article: <a class="af nb" href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad" rel="noopener">https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad</a></li></ul></div></div></div></div>    
</body>
</html>