["```py\n!pip install s3fs\n```", "```py\nimport os\nimport s3fs\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as F\nfrom pyspark.sql import Row\nimport pyspark.sql.types as T\nimport datetime\nimport time\n```", "```py\n# Define environment variables\nos.environ[\"MINIO_KEY\"] = \"minio\"\nos.environ[\"MINIO_SECRET\"] = \"minio123\"\nos.environ[\"MINIO_ENDPOINT\"] = \"http://minio1:9000\"\n```", "```py\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"big_data_file_formats\") \\\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.1026,org.apache.spark:spark-avro_2.12:3.5.0,io.delta:delta-spark_2.12:3.0.0\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", os.environ[\"MINIO_ENDPOINT\"]) \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"MINIO_KEY\"]) \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"MINIO_SECRET\"]) \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n```", "```py\n# Generate sample data\nnum_rows = 10000000\ndf = spark.range(0, num_rows)\n\n# Add columns\nfor i in range(1, 10):  # Since we already have one column\n    if i % 2 == 0:\n        # Integer column\n        df = df.withColumn(f\"int_col_{i}\", (F.randn() * 100).cast(T.IntegerType()))\n    else:\n        # String column\n        df = df.withColumn(f\"str_col_{i}\", (F.rand() * num_rows).cast(T.IntegerType()).cast(\"string\"))\n\ndf.count()\n```", "```py\n# Show rows from sample data\ndf.show(10,truncate = False)\n\n+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|id |str_col_1|int_col_2|str_col_3|int_col_4|str_col_5|int_col_6|str_col_7|int_col_8|str_col_9|\n+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+\n|0  |7764018  |128      |1632029  |-15      |5858297  |114      |1025493  |-88      |7376083  |\n|1  |2618524  |118      |912383   |235      |6684042  |-115     |9882176  |170      |3220749  |\n|2  |6351000  |75       |3515510  |26       |2605886  |89       |3217428  |87       |4045983  |\n|3  |4346827  |-70      |2627979  |-23      |9543505  |69       |2421674  |-141     |7049734  |\n|4  |9458796  |-106     |6374672  |-142     |5550170  |25       |4842269  |-97      |5265771  |\n|5  |9203992  |23       |4818602  |42       |530044   |28       |5560538  |-75      |2307858  |\n|6  |8900698  |-130     |2735238  |-135     |1308929  |22       |3279458  |-22      |3412851  |\n|7  |6876605  |-35      |6690534  |-41      |273737   |-178     |8789689  |88       |4200849  |\n|8  |3274838  |-42      |1270841  |-62      |4592242  |133      |4665549  |-125     |3993964  |\n|9  |4904488  |206      |2176042  |58       |1388630  |-63      |9364695  |78       |2657371  |\n+---+---------+---------+---------+---------+---------+---------+---------+---------+---------+\nonly showing top 10 rows\n```", "```py\n# Write 4 CSVs for comparing performance for every file type\ndf.write.csv(\"s3a://mybucket/ten_million_parquet.csv\")\ndf.write.csv(\"s3a://mybucket/ten_million_avro.csv\")\ndf.write.csv(\"s3a://mybucket/ten_million_orc.csv\")\ndf.write.csv(\"s3a://mybucket/ten_million_delta.csv\")\n```", "```py\n# Read all four CSVs to create dataframes\nschema = T.StructType([\n    T.StructField(\"id\", T.LongType(), nullable=False),\n    T.StructField(\"str_col_1\", T.StringType(), nullable=True),\n    T.StructField(\"int_col_2\", T.IntegerType(), nullable=True),\n    T.StructField(\"str_col_3\", T.StringType(), nullable=True),\n    T.StructField(\"int_col_4\", T.IntegerType(), nullable=True),\n    T.StructField(\"str_col_5\", T.StringType(), nullable=True),\n    T.StructField(\"int_col_6\", T.IntegerType(), nullable=True),\n    T.StructField(\"str_col_7\", T.StringType(), nullable=True),\n    T.StructField(\"int_col_8\", T.IntegerType(), nullable=True),\n    T.StructField(\"str_col_9\", T.StringType(), nullable=True)\n])\n\ndf_csv_parquet = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"s3a://mybucket/ten_million_parquet.csv\")\ndf_csv_avro = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"s3a://mybucket/ten_million_avro.csv\")\ndf_csv_orc = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"s3a://mybucket/ten_million_orc.csv\")\ndf_csv_delta = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(\"s3a://mybucket/ten_million_delta.csv\")\n```", "```py\n# Write data as Parquet\nstart_time = time.time()\ndf_csv_parquet.write.parquet(\"s3a://mybucket/ten_million_parquet2.parquet\")\nend_time = time.time()\nprint(f\"Time taken to write as Parquet: {end_time - start_time} seconds\")\n```", "```py\n# Perfom aggregation query using Parquet data\nstart_time = time.time()\ndf_parquet = spark.read.parquet(\"s3a://mybucket/ten_million_parquet2.parquet\")\ndf_parquet \\\n.select(\"str_col_5\",\"str_col_7\",\"int_col_2\") \\\n.groupBy(\"str_col_5\",\"str_col_7\") \\\n.count() \\\n.orderBy(\"count\") \\\n.limit(1) \\\n.show(truncate = False)\nend_time = time.time()\nprint(f\"Time taken for query: {end_time - start_time} seconds\")\n\n+---------+---------+-----+\n|str_col_5|str_col_7|count|\n+---------+---------+-----+\n|1        |6429997  |1    |\n+---------+---------+-----+\n```", "```py\n# Write data as ORC\nstart_time = time.time()\ndf_csv_orc.write.orc(\"s3a://mybucket/ten_million_orc2.orc\")\nend_time = time.time()\nprint(f\"Time taken to write as ORC: {end_time - start_time} seconds\")\n```", "```py\n# Perform aggregation using ORC data\ndf_orc = spark.read.orc(\"s3a://mybucket/ten_million_orc2.orc\")\nstart_time = time.time()\ndf_orc \\\n.select(\"str_col_5\",\"str_col_7\",\"int_col_2\") \\\n.groupBy(\"str_col_5\",\"str_col_7\") \\\n.count() \\\n.orderBy(\"count\") \\\n.limit(1) \\\n.show(truncate = False)\nend_time = time.time()\nprint(f\"Time taken for query: {end_time - start_time} seconds\")\n\n+---------+---------+-----+\n|str_col_5|str_col_7|count|\n+---------+---------+-----+\n|1        |2906292  |1    |\n+---------+---------+-----+\n```", "```py\n# Write data as Avro\nstart_time = time.time()\ndf_csv_avro.write.format(\"avro\").save(\"s3a://mybucket/ten_million_avro2.avro\")\nend_time = time.time()\nprint(f\"Time taken to write as Avro: {end_time - start_time} seconds\")\n```", "```py\n# Perform aggregation using Avro data\ndf_avro = spark.read.format(\"avro\").load(\"s3a://mybucket/ten_million_avro2.avro\")\nstart_time = time.time()\ndf_avro \\\n.select(\"str_col_5\",\"str_col_7\",\"int_col_2\") \\\n.groupBy(\"str_col_5\",\"str_col_7\") \\\n.count() \\\n.orderBy(\"count\") \\\n.limit(1) \\\n.show(truncate = False)\nend_time = time.time()\nprint(f\"Time taken for query: {end_time - start_time} seconds\")\n\n+---------+---------+-----+\n|str_col_5|str_col_7|count|\n+---------+---------+-----+\n|1        |6429997  |1    |\n+---------+---------+-----+\n```", "```py\n# Write data as Delta\nstart_time = time.time()\ndf_csv_delta.write.format(\"delta\").save(\"s3a://mybucket/ten_million_delta2.delta\")\nend_time = time.time()\nprint(f\"Time taken to write as Delta Lake: {end_time - start_time} seconds\")\n```", "```py\n# Perform aggregation using Delta data\ndf_delta = spark.read.format(\"delta\").load(\"s3a://mybucket/ten_million_delta2.delta\")\nstart_time = time.time()\ndf_delta \\\n.select(\"str_col_5\",\"str_col_7\",\"int_col_2\") \\\n.groupBy(\"str_col_5\",\"str_col_7\") \\\n.count() \\\n.orderBy(\"count\") \\\n.limit(1) \\\n.show(truncate = False)\nend_time = time.time()\nprint(f\"Time taken for query: {end_time - start_time} seconds\")\n\n+---------+---------+-----+\n|str_col_5|str_col_7|count|\n+---------+---------+-----+\n|1        |2906292  |1    |\n+---------+---------+-----+\n```"]