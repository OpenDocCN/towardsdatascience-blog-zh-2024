- en: A Deep Dive on LIME for Local Interpretations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-deep-dive-on-lime-for-local-interpretations-872bea23952f?source=collection_archive---------7-----------------------#2024-06-26](https://towardsdatascience.com/a-deep-dive-on-lime-for-local-interpretations-872bea23952f?source=collection_archive---------7-----------------------#2024-06-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The intuition, theory, and code for Local Interpretable Model-agnostic Explanations
    (LIME)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/?source=post_page---byline--872bea23952f--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page---byline--872bea23952f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--872bea23952f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--872bea23952f--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page---byline--872bea23952f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--872bea23952f--------------------------------)
    ·13 min read·Jun 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/800d5fcac06bc9ac4a9fdf642edb0864.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: DALL.E)'
  prefs: []
  type: TYPE_NORMAL
- en: LIME is the OG of [XAI](/what-is-interpretable-machine-learning-2d217b62185a)
    methods. It allows us to understand how machine learning models work. Specifically,
    it can help us understand how individual predictions are made (i.e. local interpretations).
  prefs: []
  type: TYPE_NORMAL
- en: Although recent advancements means LIME is less popular, it is still worth understanding.
    This is because it is a relatively simple approach and is “good enough” for many
    interpretability problems. It is also the inspiration for a more recent local
    interpretability method — [SHAP](/introduction-to-shap-with-python-d27edc23c454).
  prefs: []
  type: TYPE_NORMAL
- en: 'So we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss the steps taken by LIME to get local interpretations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss in detail some of your choices related to these steps including how
    to weight samples and which surrogate model to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the [lime Python package](https://github.com/marcotcr/lime).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along the way, we will compare the method to SHAP. This is to better understand
    its weaknesses. We will also see that, although LIME is a local method, we can
    still aggregate lime weights to get global interpretations. Doing so will help
    us understand some of the default choices made by the package.
  prefs: []
  type: TYPE_NORMAL
