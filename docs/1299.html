<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Optimising Non-Linear Treatment Effects in Pricing and Promotions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Optimising Non-Linear Treatment Effects in Pricing and Promotions</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/optimising-non-linear-treatment-effects-in-pricing-and-promotions-011ce140d180?source=collection_archive---------2-----------------------#2024-05-24">https://towardsdatascience.com/optimising-non-linear-treatment-effects-in-pricing-and-promotions-011ce140d180?source=collection_archive---------2-----------------------#2024-05-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4678" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Causal AI, exploring the integration of causal reasoning into machine learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@raz1470?source=post_page---byline--011ce140d180--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan O'Sullivan" class="l ep by dd de cx" src="../Images/7cd161d38d67d2c0b7da2d8f3e7d33fe.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*tAw1S072P0f0sUswKPN6VQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--011ce140d180--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@raz1470?source=post_page---byline--011ce140d180--------------------------------" rel="noopener follow">Ryan O'Sullivan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--011ce140d180--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">May 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/10380a5d9fbe3aded646019e930eb6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qVGwnZWKRWDw7C1O"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@namzo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ernest Ojeh</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ef75" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is this series of articles about?</h1><p id="17bc" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Welcome to my series on Causal AI, where we will explore the integration of causal reasoning into machine learning models. Expect to explore a number of practical applications across different business contexts.</p><p id="c384" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the last article we covered <em class="pa">using Double Machine Learning and Linear Programming to optimise treatment strategies</em>. This time we will continue with the theme of optimisation exploring <em class="pa">optimising non-linear treatment effects in Pricing &amp; Promotions</em>.</p><p id="b62d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you missed the last article on Double Machine Learning and Linear Programming, check it out here:</p><div class="pb pc pd pe pf pg"><a rel="noopener follow" target="_blank" href="/using-double-machine-learning-and-linear-programming-to-optimise-treatment-strategies-920c20a29553?source=post_page-----011ce140d180--------------------------------"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">Using Double Machine Learning and Linear Programming to optimise treatment strategies</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">Causal AI, exploring the integration of causal reasoning into machine learning</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">towardsdatascience.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lr pg"/></div></div></a></div><h1 id="0617" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="939b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">This article will showcase how we can optimise non-linear treatment effects in pricing (but the ideas can also be applied across marketing and other domains too).</p><p id="08a4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">In this article I will help you understand:</strong></p><ul class=""><li id="18be" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Why is it common to have non-linear treatment effects in pricing?</li><li id="438a" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">What tools from our Causal AI toolbox are suitable for estimating non-linear treatment effects?</li><li id="4485" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">How can non-linear programming be used to optimise pricing?</li><li id="d5d0" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">A worked case study in Python working through how we can combine our Causal AI toolbox and non-linear programming to optimise pricing budgets.</li></ul><p id="b8cf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The full notebook can be found here:</p><div class="pb pc pd pe pf pg"><a href="https://github.com/raz1470/causal_ai/blob/main/notebooks/using%20dml%20and%20lp%20to%20optimise%20treatment%20strategies.ipynb?source=post_page-----011ce140d180--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">causal_ai/notebooks/using dml and lp to optimise treatment strategies.ipynb at main ¬∑‚Ä¶</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">This project introduces Causal AI and how it can drive business value. - causal_ai/notebooks/using dml and lp to‚Ä¶</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">github.com</p></div></div><div class="pp l"><div class="qd l pr ps pt pp pu lr pg"/></div></div></a></div><h1 id="8d62" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Why is it common to have non-linear treatment effects in pricing?</h1><h2 id="c6cd" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Diminishing returns</h2><p id="501e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let‚Äôs take the example of a retailer adjusting the price of a product. Initially lowering the price might lead to a significant increase in sales. However, as they continue to lower the price, the increase in sales may start to plateau. We call this diminishing returns. As illustrated below, the effect of diminishing returns is generally non-linear.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/00d137dc0252c7fe3a474395358fd7c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*V5DjDOJSOTgUB_L0x9sEPA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="820d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Diminishing returns can be observed across various fields beyond pricing. Some common examples are:</p><ul class=""><li id="a0a2" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Marketing ‚Äî Increasing social media spend can increase customer acquisition, but as time goes on it becomes more difficult to target new, untapped audiences.</li><li id="ba3d" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Farming ‚Äî Adding fertilizer to a field can increase crop yield significantly initially, but this effect will very quickly start to diminish.</li><li id="9730" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Manufacturing ‚Äî Adding more workers to a production process will improve efficiencies, but each additional worker may contribute less to the overall output.</li></ul><p id="2872" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This makes me start to wonder, if diminishing returns are so common, then which techniques from our Causal AI toolbox are capable of handling this?</p><h1 id="06ee" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What methods from our Causal AI toolbox are suitable for estimating non-linear treatment effects?</h1><h2 id="9d9e" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Toolbox</h2><p id="37b2" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">There are two key questions which we will ask to help us identify what methods from our Causal AI toolbox are suitable for our Pricing problem:</p><ul class=""><li id="6d0f" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Can it handle continuous treatments?</li><li id="5866" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Can it capture non-linear treatment effects?</li></ul><p id="0c91" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Below we can see a summary of how suitable each method is:</p><ul class=""><li id="1627" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Propensity score matching (PSM) ‚Äî Treatment needs to be binary ‚ùå</li><li id="6fd0" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Inverse-propensity score matching (IPSM) ‚Äî Treatment needs to be binary ‚ùå</li><li id="e8bb" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">T-Learner ‚Äî Treatment needs to be binary ‚ùå</li><li id="41e4" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Double Machine Learning (DML) ‚Äî Treatment effect is linear ‚ùå</li><li id="2398" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Doubly-Robust Learner (DR) ‚Äî Treatment needs to be binary ‚ùå</li><li id="1da9" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">S-Learner ‚Äî Can handle continuous treatments and non-linear relationships between the treatment and outcome if an appropriate machine learning algorithm (e.g. gradient boosting) is used üíö</li></ul><h2 id="f5e6" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">S-Learner</h2><p id="31bf" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The ‚ÄúS‚Äù in S-Learner comes from it being a ‚Äúsingle model‚Äù. An arbitrary machine learning model is used to predict the outcome using the treatment, confounders and other covariates as features. This model is then used to estimate the difference between the potential outcomes under different treatment conditions (which gives us the treatment effect).</p><p id="538b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The are a number of benefits to the S-Learner:</p><ul class=""><li id="b86d" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">It can handle both binary and continuous treatments.</li><li id="f075" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">It can use any machine learning algorithm, giving us the flexibility to capture non-linear relationships for both the features and treatment.</li></ul><p id="5d83" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">One word of caution: regularisation bias! Modern machine learning algorithms use regularisation to prevent overfitting ‚Äî but this can be damaging to causal problems. Take the hyper-parameter <em class="pa">max features</em> from gradient boosting tree methods ‚Äî in a number of trees, it is likely that the treatment won‚Äôt be included in the model. This will dampen the effect of the treatment.</p><p id="66a7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">When using the S-Learner, I recommend thinking carefully about the regularisation parameters e.g. set <em class="pa">max features</em> to 1.0 (effectively switching off the feature regularisation).</p><h1 id="a080" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">How can non-linear programming be used to optimise pricing?</h1><h2 id="7af2" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Price optimisation</h2><p id="e654" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let‚Äôs say we have a number of products and we want to optimise their price given a set promotional budget. For each product we train an S-Learner (using gradient boosting) with the treatment set as discount level and the outcome set as total number of orders. Our S-Leaners output a complex model that can be used to estimate the effect of different discount levels. But how can we optimise the discount levels for each product?</p><h2 id="eafb" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Response Curves</h2><p id="4478" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Optimisation techniques such as linear (or even non-linear) programming rely on having a clear functional form of the response. Machine learning techniques like random forests and gradient boosting don‚Äôt give us this (unlike say linear regression). However, a response curve can translate the outputs of an S-Learner into a comprehensive form, showing how the outcome responds to the treatment.</p><p id="f68c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you can‚Äôt quite picture how we can create a response curve yet, don‚Äôt worry we will cover this in the Python case study!</p><h2 id="6f77" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Michaelis-Menton equation</h2><p id="53f9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">There are several equations we could use to map the S-Learner to a response curve. One of them is the Micaelis-Menton equation.</p><p id="cce1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The Micaelis-Menton equation is commonly used in enzyme kinetics (the study of the rates at which enzymes catalyse chemical reactions) to describe the rate of enzymatic reactions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qw"><img src="../Images/103dde2faf7a1ea73b0d3fe4dd0b2798.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*CCbpIct2jWGii3YxTW5n9g.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><ul class=""><li id="7f67" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">v ‚Äî is the reaction velocity (this is our transformed response, so total number of orders in our pricing example)</li><li id="883f" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Vmax ‚Äî is the maximum reaction velocity (we will call this alpha, a parameter we need to learn)</li><li id="1b7b" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Km ‚Äî is the substrate concentration (we will call this lambda, a parameter we need to learn)</li><li id="ddb3" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">S ‚Äî is the Michaelis constant (this is our treatment, so discount level in our pricing example)</li></ul><p id="50b0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Its principles can also be applied to other fields, especially when dealing with systems where increasing input does not proportionally increase output due to saturation factors. Below we visualise how different values of alpha and lamda effect the curve:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="d39a" class="rb ne fq qy b bg rc rd l re rf">def michaelis_menten(x, alpha, lam):<br/>    return alpha * x / (lam + x)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/339c252eb69c29a11f08f39d2b0e2cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3FXXtLAFisOBpCu1alD5nQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="f348" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Once we have our response curves, next we can think about optimisation. The Micaelis-Menton gives us a non-linear function. Therefore non-linear programming is an appropriate choice.</p><h2 id="35b1" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Non-linear programming</h2><p id="b294" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We covered linear programming in the my last article. Non-linear programing is similar but the objective function and/or constraints are non-linear in nature.</p><p id="103e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Sequential Least Squares Programming (SLSQP) is an algorithm used for solving non-linear programming problems. It allows for both equality and inequality constraints making it a sensible choice for our use case.</p><ul class=""><li id="ec76" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Equality constraints e.g. Total promotional budget is equal to ¬£100k</li><li id="b61f" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Inequality constraints e.g. Discount on each product between ¬£1 and ¬£10</li></ul><p id="56f1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">SciPy have an easy to use implementation of SLSQP:</p><div class="pb pc pd pe pf pg"><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html?source=post_page-----011ce140d180--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">minimize(method='SLSQP') - SciPy v1.13.0 Manual</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">If jac in ['2-point', '3-point', 'cs'] the relative step size to use for numerical approximation of jac. The absolute‚Ä¶</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">docs.scipy.org</p></div></div><div class="pp l"><div class="rh l pr ps pt pp pu lr pg"/></div></div></a></div><p id="8263" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Next we will illustrate how powerful the combination of the S-Learner, Micaelis-Menton equation and non-linear programing can be!</p><h1 id="0203" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Case study</h1><h2 id="f59c" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Background</h2><p id="c143" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Historically the promotions teams have used their expert judgement to set the discount for their 3 top products. Given the current economic conditions, they are being forced to reduce their overall promotional budget by 20%. They turn to the Data Science team to advise how they can do this whilst minimising the loss in orders being placed.</p><h2 id="58ae" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Data generating process</h2><p id="6372" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We set up a data generating process with the following characteristics:</p><ul class=""><li id="6b51" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">4 features with a complex relationship with the number of orders</li><li id="1865" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">A treatment effect which follows the Micaelis-Menton equation</li></ul><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="38f5" class="rb ne fq qy b bg rc rd l re rf">def data_generator(n, tau_weight, alpha, lam):<br/><br/>    # Set number of features<br/>    p=4<br/><br/>    # Create features<br/>    X = np.random.uniform(size=n * p).reshape((n, -1))<br/><br/>    # Nuisance parameters<br/>    b = (<br/>        np.sin(np.pi * X[:, 0])<br/>        + 2 * (X[:, 1] - 0.5) ** 2<br/>        + X[:, 2] * X[:, 3]<br/>    )<br/><br/>    # Create treatment and treatment effect<br/>    T = np.linspace(200, 10000, n)<br/>    T_mm = michaelis_menten(T, alpha, lam) * tau_weight<br/>    tau = T_mm / T<br/><br/>    # Calculate outcome<br/>    y = b + T * tau + np.random.normal(size=n) * 0.5<br/>    <br/>    y_train = y<br/>    X_train = np.hstack((X, T.reshape(-1, 1)))<br/>    <br/>    return y_train, X_train, T_mm, tau</span></pre><p id="c430" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The X features are confounding variables:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ri"><img src="../Images/070ded84b57d5cde15c71bc2917b0210.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*71OnywWndBfBoA0En7b6-g.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="522d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We use the data generator to create samples for 3 products, each with a different treatment effect:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="dce6" class="rb ne fq qy b bg rc rd l re rf">np.random.seed(1234)<br/><br/>n=100000<br/><br/>y_train_1, X_train_1, T_mm_1, tau_1 = data_generator(n, 1.00, 2, 5000)<br/>y_train_2, X_train_2, T_mm_2, tau_2 = data_generator(n, 0.25, 2, 5000)<br/>y_train_3, X_train_3, T_mm_3, tau_3 = data_generator(n, 2.00, 2, 5000)</span></pre><h2 id="2714" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">S-Learner</h2><p id="a914" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We can train an S-Learner by using any machine learning algorithm and including the treatment and covariates as features:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="83ee" class="rb ne fq qy b bg rc rd l re rf">def train_slearner(X_train, y_train):<br/>    <br/>    model = LGBMRegressor(random_state=42)<br/>    model.fit(X_train, y_train)<br/><br/>    yhat_train = model.predict(X_train)<br/><br/>    mse_train = mean_squared_error(y_train, yhat_train)<br/>    r2_train = r2_score(y_train, yhat_train)<br/><br/>    print(f'MSE on train set is {round(mse_train)}')<br/>    print(f'R2 on train set is {round(r2_train, 2)}')<br/>    <br/>    return model, yhat_train</span></pre><p id="29de" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We train an S-Learner for each product:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="f1d7" class="rb ne fq qy b bg rc rd l re rf">np.random.seed(1234)<br/><br/>model_1, yhat_train_1 = train_slearner(X_train_1, y_train_1)<br/>model_2, yhat_train_2 = train_slearner(X_train_2, y_train_2)<br/>model_3, yhat_train_3 = train_slearner(X_train_3, y_train_3)</span></pre><p id="cc46" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">At the moment this is just a prediction model ‚Äî Below we visualise how well it does at this job:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rj"><img src="../Images/633b5e5ab9af4aa5319a9f6e195fe89e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a2l2kY6XvH9iooffvWR6tg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><h2 id="1cc4" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Extracting the treatment effects</h2><p id="2bbc" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Next we will use our S-learner to extract the treatment effect for the full range of treatment values (discount amount) whilst holding other features to their mean value.</p><p id="992d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We start by extracting the expected outcome (number of orders) for the full range of treatment values:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="ad65" class="rb ne fq qy b bg rc rd l re rf">def extract_treated_effect(n, X_train, model):<br/>    <br/>    # Set features to mean value<br/>    X_mean_mapping = {'X1': [X_train[:, 0].mean()] * n,<br/>                      'X2': [X_train[:, 1].mean()] * n,<br/>                      'X3': [X_train[:, 2].mean()] * n,<br/>                      'X4': [X_train[:, 3].mean()] * n}<br/><br/>    # Create DataFrame<br/>    df_scoring = pd.DataFrame(X_mean_mapping)<br/><br/>    # Add full range of treatment values<br/>    df_scoring['T'] = X_train[:, 4].reshape(-1, 1)<br/><br/>    # Calculate outcome prediction for treated<br/>    treated = model.predict(df_scoring)<br/>    <br/>    return treated, df_scoring</span></pre><p id="3f97" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We do this for each product:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="91a8" class="rb ne fq qy b bg rc rd l re rf">treated_1, df_scoring_1 = extract_treated_effect(n, X_train_1, model_1)<br/>treated_2, df_scoring_2 = extract_treated_effect(n, X_train_2, model_2)<br/>treated_3, df_scoring_3 = extract_treated_effect(n, X_train_3, model_3)</span></pre><p id="a9d0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We then extract the expected outcome (number of orders) when the treatment is set to 0:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="0a8f" class="rb ne fq qy b bg rc rd l re rf">def extract_untreated_effect(n, X_train, model):<br/>    <br/>    # Set features to mean value<br/>    X_mean_mapping = {'X1': [X_train[:, 0].mean()] * n,<br/>                      'X2': [X_train[:, 1].mean()] * n,<br/>                      'X3': [X_train[:, 2].mean()] * n,<br/>                      'X4': [X_train[:, 3].mean()] * n,<br/>                      'T': [0] * n}<br/><br/>    # Create DataFrame<br/>    df_scoring = pd.DataFrame(X_mean_mapping)<br/><br/>    # Add full range of treatment values<br/>    df_scoring<br/><br/>    # Calculate outcome prediction for treated<br/>    untreated = model.predict(df_scoring)<br/>    <br/>    return untreated</span></pre><p id="8091" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Again, we do this for each product:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="9291" class="rb ne fq qy b bg rc rd l re rf">untreated_1 = extract_untreated_effect(n, X_train_1, model_1)<br/>untreated_2 = extract_untreated_effect(n, X_train_2, model_2)<br/>untreated_3 = extract_untreated_effect(n, X_train_3, model_3)</span></pre><p id="cfd4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can now calculate the treatment effect for the full range of treatment values:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="5b12" class="rb ne fq qy b bg rc rd l re rf">treatment_effect_1 = treated_1 - untreated_1<br/>treatment_effect_2 = treated_2 - untreated_2<br/>treatment_effect_3 = treated_3 - untreated_3</span></pre><p id="10b8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">When we compare this to the actual treatment effect which we saved from our data-generator, we can see the S-Learner is very effective at estimating the treatment effects for the full range of treatment values:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rk"><img src="../Images/b97c0b95c35e52cf9b9fac26fa824876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OnK4d0rhG18POYQ86XnL8w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="0532" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we have this treatment effect data, we can use it to build response curves for each product.</p><h2 id="f1f5" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Michaelis-Menton</h2><p id="7670" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">To build the response curves, we need a curve fitting tool. SciPy has a great implementation of one which we will use:</p><div class="pb pc pd pe pf pg"><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html?source=post_page-----011ce140d180--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab ig"><div class="pi ab co cb pj pk"><h2 class="bf fr hw z io pl iq ir pm it iv fp bk">scipy.optimize.curve_fit - SciPy v1.13.0 Manual</h2><div class="pn l"><h3 class="bf b hw z io pl iq ir pm it iv dx">scipy.optimize. curve_fit ( f , xdata , ydata , , , , , bounds = (-inf, inf) , , , * , , , ** kwargs ) [source] Use‚Ä¶</h3></div><div class="po l"><p class="bf b dy z io pl iq ir pm it iv dx">docs.scipy.org</p></div></div><div class="pp l"><div class="rl l pr ps pt pp pu lr pg"/></div></div></a></div><p id="e21e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We start by setting up the function that we want to learn:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="9e7c" class="rb ne fq qy b bg rc rd l re rf">def michaelis_menten(x, alpha, lam):<br/>    return alpha * x / (lam + x)</span></pre><p id="d07b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can then use curve_fit to learn the alpha and lambda parameters:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="ab3d" class="rb ne fq qy b bg rc rd l re rf">def response_curves(treatment_effect, df_scoring):<br/>    <br/>    maxfev = 100000<br/>    lam_initial_estimate = 0.001<br/>    alpha_initial_estimate = max(treatment_effect)<br/>    initial_guess = [alpha_initial_estimate, lam_initial_estimate]<br/><br/>    popt, pcov = curve_fit(michaelis_menten, df_scoring['T'], treatment_effect, p0=initial_guess, maxfev=maxfev)<br/>    <br/>    return popt, pcov</span></pre><p id="4c85" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We do this for each product:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="3a4c" class="rb ne fq qy b bg rc rd l re rf">popt_1, pcov_1 = response_curves(treatment_effect_1, df_scoring_1)<br/>popt_2, pcov_2 = response_curves(treatment_effect_2, df_scoring_2)<br/>popt_3, pcov_3 = response_curves(treatment_effect_3, df_scoring_3)</span></pre><p id="541b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can now feed the learnt parameters into the michaelis menten function to help us visualise how well the curve fitting did:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="fb63" class="rb ne fq qy b bg rc rd l re rf">treatment_effect_curve_1 = michaelis_menten(df_scoring_1['T'], popt_1[0], popt_1[1])<br/>treatment_effect_curve_2 = michaelis_menten(df_scoring_2['T'], popt_2[0], popt_2[1])<br/>treatment_effect_curve_3 = michaelis_menten(df_scoring_3['T'], popt_3[0], popt_3[1])</span></pre><p id="41e8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see that the curve fitting did a great job!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rk"><img src="../Images/fd743bbd492a4de4bf9bc40f0b9d8ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrC1y8JheEzOQ3NZR3tqEg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="0db1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we have the alpha and lambda parameters for each product, we can start thinking about the non-linear optimisation‚Ä¶</p><h2 id="99de" class="qe ne fq bf nf qf qg qh ni qi qj qk nl oi ql qm qn om qo qp qq oq qr qs qt qu bk">Non-linear programming</h2><p id="a387" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We start by setting collating all the required information for the optimisation:</p><ul class=""><li id="601d" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">A list of all the products</li><li id="879d" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The total promotional budget</li><li id="08bb" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The budget ranges for each product</li><li id="7421" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The parameters for each product from the Michaelis Menten response curves</li></ul><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="0a84" class="rb ne fq qy b bg rc rd l re rf"># List of products<br/>products = ["product_1", "product_2", "product_3"]<br/><br/># Set total budget to be the sum of the mean of each product reduced by 20%<br/>total_budget = (df_scoring_1['T'].mean() + df_scoring_2['T'].mean() + df_scoring_3['T'].mean()) * 0.80<br/><br/># Dictionary with min and max bounds for each product - set as +/-20% of max/min discount<br/>budget_ranges = {"product_1": [df_scoring_1['T'].min() * 0.80, df_scoring_1['T'].max() * 1.2], <br/>                 "product_2": [df_scoring_2['T'].min() * 0.80, df_scoring_2['T'].max() * 1.2], <br/>                 "product_3": [df_scoring_3['T'].min() * 0.80, df_scoring_3['T'].max() * 1.2]}<br/><br/># Dictionary with response curve parameters<br/>parameters = {"product_1": [popt_1[0], popt_1[1]], <br/>              "product_2": [popt_2[0], popt_2[1]], <br/>              "product_3": [popt_3[0], popt_3[1]]}</span></pre><p id="536d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Next we set up the objective function ‚Äî We want to maximise orders but as we are going to use a minimisation method, we return the negative of the sum of orders expected.</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="80dd" class="rb ne fq qy b bg rc rd l re rf">def objective_function(x, products, parameters):<br/><br/>    sum_orders = 0.0<br/>    <br/>    # Unpack parameters for each product and calculate expected orders<br/>    for product, budget in zip(products, x, strict=False):<br/>        L, k = parameters[product]<br/>        sum_orders += michaelis_menten(budget, L, k)<br/><br/>    return -1 * sum_orders</span></pre><p id="eaf6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Finally we can run our optimisation to determine the optimal budget to allocate to each product:</p><pre class="mm mn mo mp mq qx qy qz bp ra bb bk"><span id="65a6" class="rb ne fq qy b bg rc rd l re rf"># Set initial guess by equally sharing out the total budget<br/>initial_guess = [total_budget // len(products)] * len(products)<br/><br/># Set the lower and upper bounds for each product<br/>bounds = [budget_ranges[product] for product in products]<br/><br/># Set the equality constraint - constraining the total budget<br/>constraints = {"type": "eq", "fun": lambda x: np.sum(x) - total_budget}<br/><br/># Run optimisation<br/>result = minimize(<br/>    lambda x: objective_function(x, products, parameters),<br/>    initial_guess,<br/>    method="SLSQP",<br/>    bounds=bounds,<br/>    constraints=constraints,<br/>    options={'disp': True, 'maxiter': 1000, 'ftol': 1e-9},<br/>)<br/><br/># Extract results<br/>optimal_treatment = {product: budget for product, budget in zip(products, result.x, strict=False)}<br/>print(f'Optimal promo budget allocations: {optimal_treatment}')<br/>print(f'Optimal orders: {round(result.fun * -1, 2)}')</span></pre><p id="581b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The output shows us what the optimal promotional budget is for each product:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/7468ce2e35ed95ef3fdd0f6432967360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NeTpNKAtzJ5d2-OvDDv9gQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">User generated image</figcaption></figure><p id="b912" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you closely inspect the response curves, you will see that what the optimisation results are intuitive:</p><ul class=""><li id="c7d6" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">Small decrease in the budget for product 1</li><li id="8829" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Decrease the budget for product 2 significantly</li><li id="5227" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Increase the budget for product 3 significantly</li></ul><h1 id="5647" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Closing thoughts</h1><p id="3a8d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Today we covered the powerful combination of the S-Learner, Micaelis-Menton equation and non-linear programing! Here are some closing thoughts:</p><ul class=""><li id="a435" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pv pw px bk">As mentioned earlier, when using the S-Learner beware of regularisation bias!</li><li id="c153" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">A good alternative to the S-Learner is using DML but transforming your treatment before training the model ‚Äì However, this means you need to have some prior knowledge of the functional from of the treatment.</li><li id="c494" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">I chose to use the Micaelis-Menton equation to build my response curves ‚Äî However, this may not fit your problem and can be replaced by other transformations which are more suitable.</li><li id="9bb9" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">Using SLSQP to solve nonlinear programming problems gives you the flexibility to use both equality and inequality constraints.</li><li id="d477" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">The data you collect is likely to be observational ‚Äì This poses some challenges around the range of discount values you will collect ‚Äì They are probably going to be clustered around a set region. Using some sort of Shapley approach to create the data used to create the response curves may be more suitable in this situation.</li><li id="4314" class="nz oa fq ob b go py od oe gr pz og oh oi qa ok ol om qb oo op oq qc os ot ou pv pw px bk">I‚Äôve chosen to focus on Pricing &amp; Promotions, but this framework can be extended to Marketing budgets.</li></ul></div></div></div><div class="ab cb rn ro rp rq" role="separator"><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2c8b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Follow me if you want to continue this journey into Causal AI ‚Äî In the next article we will cover measuring the intrinsic causal influence of your marketing campaigns.</p></div></div></div></div>    
</body>
</html>