<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Diving into Word Embeddings with EDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Diving into Word Embeddings with EDA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12">https://towardsdatascience.com/eda-for-word-embeddings-224c524b5769?source=collection_archive---------1-----------------------#2024-07-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7fe5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Visualizing unexpected insights in text data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Harys Dalvi" class="l ep by dd de cx" src="../Images/cf7fa3865063408efd1fd4c0b4b603db.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*APonMNiRc_xq6MFzvUH5Ag.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@crackalamoo?source=post_page---byline--224c524b5769--------------------------------" rel="noopener follow">Harys Dalvi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--224c524b5769--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="496a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When starting work with a new dataset, it’s always a good idea to start with some <strong class="ml fr">exploratory data analysis</strong> (EDA). Taking the time to understand your data before training any fancy models can help you understand the structure of the dataset, identify any obvious issues, and apply domain-specific knowledge.</p><p id="3e8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You see EDA in various forms with everything from <a class="af nf" href="https://www.kaggle.com/code/spscientist/a-simple-tutorial-on-exploratory-data-analysis" rel="noopener ugc nofollow" target="_blank">house prices</a> to advanced applications in the data science industry. But I still haven’t seen it for the hottest new dataset: <strong class="ml fr">word embeddings</strong>, the basis of our best large language models. So why not try it?</p><p id="0add" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, we’ll <strong class="ml fr">apply EDA to GloVe word embeddings</strong>, using techniques like <strong class="ml fr">covariance matrices, clustering, PCA, and vector math</strong>. This will help us understand the structure of word embeddings, giving us a useful starting point for building more powerful models with this data. As we discover this structure, we’ll find that it’s not always what it seems, and some surprising biases are hidden in the corpus.</p><p id="bb52" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You will need:</p><ul class=""><li id="15e8" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk">Basic understanding of linear algebra, statistics, and vector mathematics</li><li id="904e" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">Python packages: <code class="cx no np nq nr b">numpy</code>, <code class="cx no np nq nr b">sklearn</code>, and <code class="cx no np nq nr b">matplotlib</code></li><li id="f2f0" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">About 3 GB of spare disk space</li></ul><h1 id="daf0" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">The Dataset</h1><p id="d25f" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">To get started, download the dataset at <a class="af nf" href="https://huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip" rel="noopener ugc nofollow" target="_blank">huggingface.co/stanfordnlp/glove/resolve/main/glove.6B.zip</a>[1]. This contains three text files, each containing a list of words along with their vector representations. We will use the <strong class="ml fr">300-dimensional representations</strong> (glove.6B.300d.txt).</p><p id="c066" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A quick note on where this dataset comes from: essentially, this is a list of word embeddings derived from 6 billion tokens’ worth of <strong class="ml fr">co-occurrence data</strong> from Wikipedia and various news sources. A useful side effect of using co-occurrence is that <strong class="ml fr">words that mean similar things tend to be close together</strong>. For example, since “the <em class="ot">red</em> bird” and “the <em class="ot">blue</em> bird” are both valid sentences, we might expect the vectors for “red” and “blue” to be close to each other. For more technical information, you can check <a class="af nf" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">the original GloVe paper</a>[1].</p><p id="c1ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To be clear, these are <strong class="ml fr">not</strong> word embeddings trained for the purpose of large language models. They are a fully unsupervised technique based on a large corpus. But they display a lot of similar properties to language model embeddings, and are interesting in their own right.</p><p id="0fcc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each line of this text file consists of a word, followed by all 300 vector components of the associated embedding separated by spaces. We can load that in with Python. (To reduce noise and speed things up, I’m using the top 10% of the full dataset here with the <code class="cx no np nq nr b">//10</code>, but you can change that if you’d like.)</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="3c73" class="pc nt fq nr b bg pd pe l pf pg">import numpy as np<br/><br/>embeddings = {}<br/><br/>with open(f"glove.6B/glove.6B.300d.txt", "r") as f:<br/>    glove_content = f.read().split('\n')<br/><br/>for i in range(len(glove_content)//10):<br/>    line = glove_content[i].strip().split(' ')<br/>    if line[0] == '':<br/>        continue<br/>    word = line[0]<br/>    embedding = np.array(list(map(float, line[1:])))<br/>    embeddings[word] = embedding<br/><br/>print(len(embeddings))</span></pre><p id="cadf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That leaves us with 40,000 embeddings loaded in.</p><h1 id="e98a" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Similarity Metrics</h1><p id="ec17" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">One natural question we might ask is: <strong class="ml fr">are vectors generally close to other vectors with similar meaning?</strong> And as a follow-up question, how do we quantify this?</p><p id="a715" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are two main ways we will quantify similarity between vectors: one is <strong class="ml fr">Euclidean distance</strong>, which is simply the natural Pythagorean theorem distance we are familiar with. The other is <strong class="ml fr">cosine similarity</strong>, which measures the cosine of the <em class="ot">angle</em> between two vectors. A vector has a cosine similarity of 1 with itself, -1 with an opposite vector, and 0 with an orthogonal vector.</p><p id="e909" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s implement these in NumPy:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="cc6b" class="pc nt fq nr b bg pd pe l pf pg">def cos_sim(a, b):<br/>    return np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))<br/>def euc_dist(a, b):<br/>    return np.sum(np.square(a - b)) # no need for square root since we are just ranking distances</span></pre><p id="2d17" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we can find all the closest vectors to a given word or embedding vector! We’ll do this in increasing order.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="0789" class="pc nt fq nr b bg pd pe l pf pg">def get_sims(to_word=None, to_e=None, metric=cos_sim):<br/>    # list all similarities to the word to_word, OR the embedding vector to_e<br/>    assert (to_word is not None) ^ (to_e is not None) # find similarity to a word or a vector, not both<br/>    sims = []<br/>    if to_e is None:<br/>        to_e = embeddings[to_word] # get the embedding for the word we are looking at<br/>    for word in embeddings:<br/>        if word == to_word:<br/>            continue<br/>        word_e = embeddings[word]<br/>        sim = metric(word_e, to_e)<br/>        sims.append((sim, word))<br/>    sims.sort()<br/>    return sims</span></pre><p id="ba76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we can write a function to display the 10 most similar words. It will be useful to include a reverse option as well, so we can display the <em class="ot">least</em> similar words.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="932e" class="pc nt fq nr b bg pd pe l pf pg">def display_sims(to_word=None, to_e=None, n=10, metric=cos_sim, reverse=False, label=None):<br/>    assert (to_word is not None) ^ (to_e is not None)<br/>    sims = get_sims(to_word=to_word, to_e=to_e, metric=metric)<br/>    display = lambda sim: f'{sim[1]}: {sim[0]:.5f}'<br/>    if label is None:<br/>        label = to_word.upper() if to_word is not None else ''<br/>    print(label) # a heading so we know what these similarities are for<br/>    if reverse:<br/>        sims.reverse()<br/>    for i, sim in enumerate(reversed(sims[-n:])):<br/>        print(i+1, display(sim))<br/>    return sims</span></pre><p id="525a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we can test it!</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="5cc9" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_word='red')<br/># yellow, blue, pink, green, white, purple, black, colored, sox, bright</span></pre><p id="26c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Looks like the Boston Red Sox made an unexpected appearance here. But other than that, this is about what we would expect.</p><p id="e061" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Maybe we can try some verbs, and not just nouns and adjectives? How about a nice and kind verb like “share”?</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="a8e1" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_word='share')<br/># shares, stock, profit, percent, shared, earnings, profits, price, gain, cents</span></pre><p id="ad2a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I guess “share” isn’t often used as a verb in this dataset. Oh well.</p><p id="d5ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can try some more conventional examples as well:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="9909" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_word='cat')<br/># dog, cats, pet, dogs, feline, monkey, horse, pets, rabbit, leopard<br/>display_sims(to_word='frog')<br/># toad, frogs, snake, monkey, squirrel, species, rodent, parrot, spider, rat<br/>display_sims(to_word='queen')<br/># elizabeth, princess, king, monarch, royal, majesty, victoria, throne, lady, crown</span></pre><h1 id="0888" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Reasoning by Analogy</h1><p id="c37c" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">One of the fascinating properties about word embeddings is that <strong class="ml fr">analogy is built in</strong> using vector math. The example from the GloVe paper is <em class="ot">king - queen = man - woman.</em> In other words, rearranging the equation, we expect <em class="ot">king = man - woman + queen</em>. Is this true?</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="5e9d" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_e=embeddings['man'] - embeddings['woman'] + embeddings['queen'], label='king-queen analogy')<br/># queen, king, ii, majesty, monarch, prince...</span></pre><p id="8ca3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Not quite: the closest vector to <em class="ot">man - woman + queen</em> turns out to be <em class="ot">queen</em> (cosine similarity 0.78), followed somewhat distantly by <em class="ot">king</em> (cosine similarity 0.66). Inspired by this excellent <a class="af nf" href="https://www.youtube.com/watch?v=wjZofJX0v4M" rel="noopener ugc nofollow" target="_blank">3Blue1Brown video</a>, we might try <em class="ot">aunt</em> and <em class="ot">uncle</em> instead:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="ac35" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_e=embeddings['aunt'] - embeddings['woman'] + embeddings['man'], label='aunt-uncle analogy')<br/># aunt, uncle, brother, grandfather, grandmother, cousin, uncles, grandpa, dad, father</span></pre><p id="5b20" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is better (cosine similarity 0.7348 vs 0.7344), but still doesn’t work perfectly. But we can try switching to Euclidean distance. Now we need to set <code class="cx no np nq nr b">reverse=True</code>, because a <em class="ot">higher</em> Euclidean distance is actually a <em class="ot">lower</em> similarity.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="a0c4" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_e=embeddings['aunt'] - embeddings['woman'] + embeddings['man'], metric=euc_dist, reverse=True, label='aunt-uncle analogy')<br/># uncle, aunt, grandfather, brother, cousin, grandmother, newphew, dad, grandpa, cousins</span></pre><p id="3efd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we got it. But it seems like the analogy math might not be as perfect as we hoped, at least in the naïve way that we are doing it here.</p><h1 id="d23f" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Magnitude</h1><p id="c705" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Cosine similarity is all about the <em class="ot">angles</em> between vectors. But is the <strong class="ml fr">magnitude</strong> of a vector also important?</p><p id="90e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can reuse our existing code by expressing magnitude as the Euclidean distance from the zero vector. Let’s see which words have the largest and smallest magnitudes:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="3717" class="pc nt fq nr b bg pd pe l pf pg">zero_vec = np.zeros_like(embeddings['the'])<br/>display_sims(to_e=zero_vec, metric=euc_dist, label='largest magnitude')<br/># republish, nonsubscribers, hushen, tael, www.star, stoxx, 202-383-7824, resend, non-families, 225-issue<br/>display_sims(to_e=zero_vec, metric=euc_dist, reverse=True, label='smallest magnitude')<br/># likewise, lastly, interestingly, ironically, incidentally, moreover, conversely, furthermore, aforementioned, wherein</span></pre><p id="3eb4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It doesn’t look like there’s much of a pattern to the meaning of the large magnitude vectors, but they all seem to have very specific (and sometimes confusing) meanings. On the other hand, the smallest magnitude vectors tend to be very common words that can be found in a variety of contexts.</p><p id="2b76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There’s a <strong class="ml fr">huge range between magnitudes</strong>: from about 2.6 for the smallest vector all the way to about 17 for the largest. What does this distribution look like? We can plot a <strong class="ml fr">histogram</strong> to get a better picture of this.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="774d" class="pc nt fq nr b bg pd pe l pf pg">import matplotlib.pyplot as plt<br/><br/>def plot_magnitudes():<br/>    words = [w for w in embeddings]<br/>    magnitude = lambda word: np.linalg.norm(embeddings[word])<br/>    magnitudes = list(map(magnitude, words))<br/>    plt.hist(magnitudes, bins=40)<br/>    plt.show()<br/><br/>plot_magnitudes()</span></pre><figure class="ou ov ow ox oy pk ph pi paragraph-image"><div class="ph pi pj"><img src="../Images/1cc1402864a59a660414c0cefbb9d77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*oOjMW9YGKM86-PLGHdK9ug.png"/></div><figcaption class="pm pn po ph pi pp pq bf b bg z dx">A histogram of magnitudes of our word embeddings</figcaption></figure><p id="5b3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This distribution looks approximately normal. If we wanted to test this further, we could use a <a class="af nf" href="https://medium.com/analytics-vidhya/what-are-qq-plots-4beb00670d81" rel="noopener">Q-Q plot</a>. But for our purposes right now, this is fine.</p><h1 id="a21c" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Dataset Bias</h1><p id="5952" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">It turns out that directions and subspaces in vector embeddings can encode various kinds of concepts, often in biased ways. <a class="af nf" href="https://arxiv.org/pdf/1607.06520" rel="noopener ugc nofollow" target="_blank">This paper</a>[2] studied how this works for <strong class="ml fr">gender bias</strong>.</p><p id="a522" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can replicate this concept in our GloVe embeddings, too. First, let’s find the direction of the concept of “masculinity”. We can accomplish this by taking the average of <strong class="ml fr">differences between vectors</strong> like <em class="ot">he</em> and <em class="ot">she</em>, <em class="ot">man</em> and <em class="ot">woman</em>, and so on:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="a38e" class="pc nt fq nr b bg pd pe l pf pg">gender_pairs = [('man', 'woman'), ('men', 'women'), ('brother', 'sister'), ('he', 'she'),<br/>                    ('uncle', 'aunt'), ('grandfather', 'grandmother'), ('boy', 'girl'),<br/>                    ('son', 'daughter')]<br/>masc_v = zero_vec<br/>for pair in gender_pairs:<br/>    masc_v += embeddings[pair[0]]<br/>    masc_v -= embeddings[pair[1]]</span></pre><p id="f370" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we can find the “most masculine” and “most feminine” vectors, as judged by the embedding space.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="11d0" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_e=masc_v, metric=cos_sim, label='masculine vecs')<br/># brother, colonel, himself, uncle, gen., nephew, brig., brothers, son, sir<br/>display_sims(to_e=masc_v, metric=cos_sim, reverse=True, label='feminine vecs')<br/># actress, herself, businesswoman, chairwoman, pregnant, she, her, sister, actresses, woman</span></pre><p id="bdae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we can run an easy test to detect bias in the dataset: compute the similarity between <em class="ot">nurse</em> and each of <em class="ot">man</em> and <em class="ot">woman</em>. Theoretically, these should be about equal: nurse is not a gendered word. Is this true?</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="0ba3" class="pc nt fq nr b bg pd pe l pf pg">print("nurse - man", cos_sim(embeddings['nurse'], embeddings['man'])) # 0.24<br/>print("nurse - woman", cos_sim(embeddings['nurse'], embeddings['woman'])) # 0.45</span></pre><p id="ef69" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That’s a pretty big difference! (Remember cosine similarity runs from -1 to 1, with positive associations in the range 0 to 1.) For reference, 0.45 is also close to the cosine similarity between <em class="ot">cat</em> and <em class="ot">leopard</em>.</p><h1 id="26fa" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Clustering</h1><p id="dc40" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Let’s see if we can <strong class="ml fr">cluster words with similar meaning</strong> using <strong class="ml fr"><em class="ot">k</em>-means clustering</strong>. This is easy to do with the package <code class="cx no np nq nr b">scikit-learn</code>. We are going to use 300 clusters, which sounds like a lot, but trust me: almost all of the clusters are so interesting, you could write an entire article just interpreting them!</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="7cdb" class="pc nt fq nr b bg pd pe l pf pg">from sklearn.cluster import KMeans<br/><br/>def get_kmeans(n=300):<br/>    kmeans = KMeans(n_clusters=n, n_init=1)<br/>    X = np.array([embeddings[w] for w in embeddings])<br/>    kmeans.fit(X)<br/>    return kmeans<br/><br/>def display_kmeans(kmeans):<br/>    # print all clusters and 5 associated words for each<br/>    words = np.array([w for w in embeddings])<br/>    X = np.array([embeddings[w] for w in embeddings])<br/>    y = kmeans.predict(X) # get the cluster for each word<br/>    for cluster in range(kmeans.cluster_centers_.shape[0]):<br/>        print(f'KMeans {cluster}')<br/>        cluster_words = words[y == cluster] # get all words in each cluster<br/>        for i, w in enumerate(cluster_words[:5]):<br/>            print(i+1, w)<br/><br/>kmeans = get_kmeans()<br/>display_kmeans(kmeans)</span></pre><p id="9f49" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There’s a lot to look at here. We have clusters for things as diverse as New York City (<code class="cx no np nq nr b">manhattan, n.y., brooklyn, hudson, borough</code>), molecular biology (<code class="cx no np nq nr b">protein, proteins, enzyme, beta, molecules</code>), and Indian names (<code class="cx no np nq nr b">singh, ram, gandhi, kumar, rao</code>).</p><p id="50e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But <strong class="ml fr">sometimes these clusters are not what they seem</strong>. Let’s write code to display all words of a cluster containing a given word, along with the nearest and farthest cluster.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="713f" class="pc nt fq nr b bg pd pe l pf pg">def get_kmeans_cluster(kmeans, word=None, cluster=None):<br/>    # given a word, find the cluster of that word. (or start with a cluster index.)<br/>    # then, get all words of that cluster.<br/>    assert (word is None) ^ (cluster is None)<br/>    if cluster is None:<br/>        cluster = kmeans.predict([embeddings[word]])[0]<br/>    words = np.array([w for w in embeddings])<br/>    X = np.array([embeddings[w] for w in embeddings])<br/>    y = kmeans.predict(X)<br/>    cluster_words = words[y == cluster]<br/>    return cluster, cluster_words<br/><br/>def display_cluster(kmeans, word):<br/>    cluster, cluster_words = get_kmeans_cluster(kmeans, word=word)<br/>    # print all words in the cluster<br/>    print(f"Full KMeans ({word}, cluster {cluster})")<br/>    for i, w in enumerate(cluster_words):<br/>        print(i+1, w)<br/>    # rank all clusters (excluding this one) by Euclidean distance of their centers from this cluster's center<br/>    distances = np.concatenate([kmeans.cluster_centers_[:cluster], kmeans.cluster_centers_[cluster+1:]], axis=0)<br/>    distances = np.sum(np.square(distances - kmeans.cluster_centers_[cluster]), axis=1)<br/>    nearest = np.argmin(distances, axis=0)<br/>    _, nearest_words = get_kmeans_cluster(kmeans, cluster=nearest)<br/>    print(f"Nearest cluster: {nearest}")<br/>    for i, w in enumerate(nearest_words[:5]):<br/>        print(i+1, w)<br/>    farthest = np.argmax(distances, axis=0)<br/>    print(f"Farthest cluster: {farthest}")<br/>    _, farthest_words = get_kmeans_cluster(kmeans, cluster=farthest)<br/>    for i, w in enumerate(farthest_words[:5]):<br/>        print(i+1, w)</span></pre><p id="769a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now let’s try out this code.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="8b91" class="pc nt fq nr b bg pd pe l pf pg">display_cluster(kmeans, 'animal')<br/># species, fish, wild, dog, bear, males, birds...<br/>display_cluster(kmeans, 'dog')<br/># same as 'animal'<br/>display_cluster(kmeans, 'birds')<br/># same again<br/>display_cluster(kmeans, 'bird')<br/># spread, bird, flu, virus, tested, humans, outbreak, infected, sars....?</span></pre><p id="f9f2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You might not get exactly this result every time: the clustering algorithm is non-deterministic. But much of the time, “birds” is associated with disease words rather than animal words. It seems the original dataset tends to use the word “bird” in the context of disease vectors.</p><p id="fbeb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are literally hundreds more clusters for you to explore the contents of. Some other clusters I found interesting are “Illinois” and “Genghis”.</p><h1 id="c522" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Principal Component Analysis</h1><p id="5f61" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk"><strong class="ml fr">Principal Component Analysis (PCA)</strong> is a tool we can use to find the directions in vector space associated with the most variance in our dataset. Let’s try it. Like clustering, <code class="cx no np nq nr b">sklearn</code> makes this easy.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="f30a" class="pc nt fq nr b bg pd pe l pf pg">from sklearn.decomposition import PCA<br/><br/>def get_pca_vecs(n=10): # get the first 10 principal components<br/>    pca = PCA()<br/>    X = np.array([embeddings[w] for w in embeddings])<br/>    pca.fit(X)<br/>    principal_components = list(pca.components_[:n, :])<br/>    return pca, principal_components<br/><br/>pca, pca_vecs = get_pca_vecs()<br/>for i, vec in enumerate(pca_vecs):<br/>    # display the words with the highest and lowest values for each principal component<br/>    display_sims(to_e=vec, metric=cos_sim, label=f'PCA {i+1}')<br/>    display_sims(to_e=vec, metric=cos_sim, label=f'PCA {i+1} negative', reverse=True)</span></pre><p id="3069" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Like our <em class="ot">k</em>-means experiment, a lot of these PCA vectors are really interesting. For example, let’s take a look at principal component 9:</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="934b" class="pc nt fq nr b bg pd pe l pf pg">    PCA 9<br/>1 featuring: 0.38193<br/>2 hindi: 0.37217<br/>3 arabic: 0.36029<br/>4 sung: 0.35130<br/>5 che: 0.34819<br/>6 malaysian: 0.34474<br/>7 ka: 0.33820<br/>8 video: 0.33549<br/>9 bollywood: 0.33347<br/>10 counterpart: 0.33343<br/>    PCA 9 negative<br/>1 suffolk: -0.31999<br/>2 cumberland: -0.31697<br/>3 northumberland: -0.31449<br/>4 hampshire: -0.30857<br/>5 missouri: -0.30771<br/>6 calhoun: -0.30749<br/>7 erie: -0.30345<br/>8 massachusetts: -0.30133<br/>9 counties: -0.29710<br/>10 wyoming: -0.29613</span></pre><p id="143b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It looks like positive values for component 9 are associated with Middle Eastern, South Asian and Southeast Asian terms, while negative values are associated with North American and British terms.</p><p id="8e1b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another interesting one is component 3. All the positive terms are decimal numbers, apparently quite a salient feature for this model. Component 8 also shows a similar pattern.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="1b76" class="pc nt fq nr b bg pd pe l pf pg">    PCA 3<br/>1 1.8: 0.57993<br/>2 1.6: 0.57851<br/>3 1.2: 0.57841<br/>4 1.4: 0.57294<br/>5 2.3: 0.57019<br/>6 2.6: 0.56993<br/>7 2.8: 0.56966<br/>8 3.7: 0.56660<br/>9 1.9: 0.56424<br/>10 2.2: 0.56063</span></pre><h2 id="c167" class="pr nt fq bf nu ps pt pu nx pv pw px oa ms py pz qa mw qb qc qd na qe qf qg qh bk">Dimensionality Reduction</h2><p id="ed93" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">One of the main benefits of PCA is that it allows us to <strong class="ml fr">take a very high-dimensional dataset</strong> (300-dimensional in this case) and <strong class="ml fr">plot it in just two or three dimensions</strong> by projecting onto the first components. Let’s try a two-dimensional plot and see if there is any information we can gather from it. We’ll also include color-coding by cluster using <em class="ot">k</em>-means.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="1900" class="pc nt fq nr b bg pd pe l pf pg">def plot_pca(pca_vecs, kmeans):<br/>    words = [w for w in embeddings]<br/>    x_vec = pca_vecs[0]<br/>    y_vec = pca_vecs[1]<br/>    X = np.array([np.dot(x_vec, embeddings[w]) for w in words])<br/>    Y = np.array([np.dot(y_vec, embeddings[w]) for w in words])<br/>    colors =  kmeans.predict([embeddings[w] for w in words])<br/>    plt.scatter(X, Y, c=colors, cmap='spring') # color by cluster<br/>    for i in np.random.choice(len(words), size=100, replace=False):<br/>        # annotate 100 randomly selected words on the graph<br/>        plt.annotate(words[i], (X[i], Y[i]), weight='bold')<br/>    plt.show()<br/><br/>plot_pca(pca_vecs, kmeans)</span></pre><figure class="ou ov ow ox oy pk ph pi paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="ph pi qi"><img src="../Images/8754000d85b058dbbdcf1ea2133c5029.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIAqzEA1n-taUzRrHz5kBw.png"/></div></div><figcaption class="pm pn po ph pi pp pq bf b bg z dx">A plot of the first (X) and second (Y) principal components for our embeddings dataset</figcaption></figure><p id="b822" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unfortunately, this plot is a total mess! It’s difficult to learn much from it. It looks like just two dimensions in isolation are not very easy to interpret among 300 total dimensions, at least in the case of this dataset.</p><p id="49bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are two exceptions. First, we see that names tend to cluster near the top of this graph. Second, there is a little section that sticks out like a sore thumb at the bottom left. This area appears to be associated with numbers, particularly decimal numbers.</p><h1 id="2ec1" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Covariance</h1><p id="3820" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">It is often helpful to get an idea of the <strong class="ml fr">covariance between input features</strong>. In this case, our input features are just abstract vector directions that are difficult to interpret. Still, a covariance matrix can tell us how much of this information is actually being used. If we see high covariance, it means some dimensions are strongly correlated, and maybe we could get away with reducing the dimensionality a little bit.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="845d" class="pc nt fq nr b bg pd pe l pf pg">def display_covariance():<br/>    X = np.array([embeddings[w] for w in embeddings]).T # rows are variables (components), columns are observations (words)<br/>    cov = np.cov(X)<br/>    cov_range = np.maximum(np.max(cov), np.abs(np.min(cov))) # make sure the colorbar is balanced, with 0 in the middle<br/>    plt.imshow(cov, cmap='bwr', interpolation='nearest', vmin=-cov_range, vmax=cov_range)<br/>    plt.colorbar()<br/>    plt.show()<br/><br/>display_covariance()</span></pre><figure class="ou ov ow ox oy pk ph pi paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="ph pi qn"><img src="../Images/bdf321419db9850f8aaabea3567d3ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOpcJQaMluvkG3wRoG-ojA.png"/></div></div><figcaption class="pm pn po ph pi pp pq bf b bg z dx">A covariance matrix for all 300 vector components in our dataset</figcaption></figure><p id="f88d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, there’s a big line down the major diagonal, representing that each component is strongly correlated with itself. Other than that, this isn’t a very interesting graph. Everything looks mostly blank, which is a good sign.</p><p id="af59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you look closely, there’s one exception: components 9 and 276 seem somewhat strongly related (covariance of 0.308).</p><figure class="ou ov ow ox oy pk ph pi paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="ph pi qo"><img src="../Images/bd08c04d3fad3c5c45538546d37a43e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0AA42Pl96pFExQ4jIbdqrA.png"/></div></div><figcaption class="pm pn po ph pi pp pq bf b bg z dx">The covariance matrix zoomed in on components 9 and 276. Observe the somewhat bright red dot here, along with strange behavior along the row and column.</figcaption></figure><p id="bc89" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s investigate this further by printing the vectors that are most associated with components 9 and 276. This is equivalent to cosine similarity to a basis vector of all zeros, except for a one in the relevant component.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="fb64" class="pc nt fq nr b bg pd pe l pf pg">e9 = np.zeros_like(zero_vec)<br/>e9[9] = 1.0<br/>e276 = np.zeros_like(zero_vec)<br/>e276[276] = 1.0<br/>display_sims(to_e=e9, metric=cos_sim, label='e9')<br/># grizzlies, supersonics, notables, posey, bobcats, wannabe, hoosiers...<br/>display_sims(to_e=e276, metric=cos_sim, label='e276')<br/># pehr, zetsche, steadied, 202-887-8307, bernice, goldie, edelman, kr...</span></pre><p id="3056" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These results are strange, and not very informative.</p><p id="5063" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But wait: we can also have a positive covariance in these components if words with a very <em class="ot">negative</em> value in one tend to also be very negative in the other. Let’s try reversing the direction of similarity.</p><pre class="ou ov ow ox oy oz nr pa bp pb bb bk"><span id="d7e7" class="pc nt fq nr b bg pd pe l pf pg">display_sims(to_e=e9, metric=cos_sim, label='e9', reverse=True)<br/># therefore, that, it, which, government, because, moreover, fact, thus, very<br/>display_sims(to_e=e276, metric=cos_sim, label='e276', reverse=True)<br/># they, instead, those, hundreds, addition, dozens, others, dozen, only, outside</span></pre><p id="0abc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It looks like both of these components are associated with basic function words and numbers that can be found in many different contexts. This helps explain the covariance between them, at least more so than the positive case did.</p><h1 id="5d63" class="ns nt fq bf nu nv nw gq nx ny nz gt oa ob oc od oe of og oh oi oj ok ol om on bk">Conclusion</h1><p id="3324" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">In this article we applied a variety of <strong class="ml fr">exploratory data analysis (EDA)</strong> techniques to a 300-dimensional dataset of <strong class="ml fr">GloVe word embeddings</strong>. We used cosine similarity to measure the similarity between the meaning of words, clustering to group words into related groups, and principal component analysis (PCA) to identify the directions in vector space that are most important to the embedding model.</p><p id="c2c8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We visually observed overall minimal covariance between the input features using principal component analysis. We tried using PCA to plot all of our 300-dimensional data in just two dimensions, but this was still a little messy.</p><p id="19e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also tested assumptions and biases in our dataset. We identified gender bias in our dataset by comparing the cosine similarity of <em class="ot">nurse</em> with each of <em class="ot">man</em> and <em class="ot">woman</em>. We tried using vector math to represent analogies (like “king” is to “queen” as “man” is to “woman”), with some success. By subtracting various examples of vectors referring to males and females, we were able to discover a vector direction associated with gender, and display the “most masculine” and “most feminine” vectors in the dataset.</p><p id="b41b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There’s a lot more EDA you could try on a dataset of word embeddings, but I hope this was a good starting point to understand both some techniques of EDA in general and the structure of word embeddings in particular. If you want to see the full code associated with this article, plus some additional examples, you can check out my GitHub at <a class="af nf" href="https://github.com/crackalamoo/glove-embeddings-eda" rel="noopener ugc nofollow" target="_blank">crackalamoo/glove-embeddings-eda</a>. Thank you for reading!</p><h2 id="c7d3" class="pr nt fq bf nu ps pt pu nx pv pw px oa ms py pz qa mw qb qc qd na qe qf qg qh bk">References</h2><p id="3e98" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">[1] J. Pennington, R. Socher and C.Manning, <a class="af nf" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">GloVe: Global Vectors for Word Representation</a> (2014), Stanford NLP (Public Domain Dataset)</p><p id="f415" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] T. Bolukbasi, K. Chang, J. Zou, V. Saligrama and A. Kalai, <a class="af nf" href="https://arxiv.org/pdf/1607.06520" rel="noopener ugc nofollow" target="_blank">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a> (2016), Microsoft Research New England</p><p id="65e3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ot">All images created by the author using Matplotlib.</em></p></div></div></div></div>    
</body>
</html>