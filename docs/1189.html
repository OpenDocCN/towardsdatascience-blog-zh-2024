<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Machine Learning on GCP: From Notebooks to Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Machine Learning on GCP: From Notebooks to Pipelines</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-on-gcp-from-dev-to-prod-with-vertex-ai-c9e42c4b366f?source=collection_archive---------4-----------------------#2024-05-11">https://towardsdatascience.com/machine-learning-on-gcp-from-dev-to-prod-with-vertex-ai-c9e42c4b366f?source=collection_archive---------4-----------------------#2024-05-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c96b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Notebooks are not enough for ML at scale</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@benjamin_47408?source=post_page---byline--c9e42c4b366f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Benjamin Etienne" class="l ep by dd de cx" src="../Images/cad8bc2d4b900575e76b7cf9debc9eea.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zgmYQ0PJ0MHXIc5mEYVSMA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c9e42c4b366f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@benjamin_47408?source=post_page---byline--c9e42c4b366f--------------------------------" rel="noopener follow">Benjamin Etienne</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c9e42c4b366f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a0f9612d168ba15a6cbadea7628ba7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sK2XogPP8UhAbR39"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@alpifree?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sylvain Mauroux</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5177" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">All images, unless otherwise noted, are by the author</em></p><h1 id="eac7" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Advocating for AI</h1><p id="e722" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">There is a misunderstanding (not to say fantasy) which keeps coming back in companies whenever it comes to AI and Machine Learning. People often misjudge the complexity and the skills needed to bring Machine Learning projects to production, either because they do not understand the job, or (even worse) because they think they understand it, whereas they don’t.</p><p id="c2c6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Their first reaction when discovering AI might be something like “AI is actually pretty simple, I just need a Jupyter Notebook, copy paste code from here and there — or ask Copilot — and boom. No need to hire Data Scientists after all…” And the story always end badly, with bitterness, disappointment and a feeling that AI is a scam: difficulty to move to production, data drift, bugs, unwanted behavior.</p><p id="5e6b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So let’s write it down once and for all: AI/Machine Learning/any data-related job, is a real job, not a hobby. It requires skills, craftsmanship, and tools. <strong class="nf fr">If you think you can do ML in production with notebooks, you are wrong.</strong></p><p id="06a9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This article aims at showing, with a simple example, all the effort, skills and tools, it takes to move from a notebook to a real pipeline in production. Because ML in production is, mostly, about being able to automate the run of your code on a regular basis, with automation and monitoring.</p><p id="3a44" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And for those who are looking for an end-to-end “notebook to vertex pipelines” tutorial, you might find this helpful.</p><h1 id="6598" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">A simple use case</h1><p id="3dea" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Let’s imagine you are a Data Scientist working at an e-commerce company. Your company is selling clothes online, and the marketing team asks for your help: they are preparing a special offer for specific products, and they would like to efficiently target customers by tailoring email content that will be pushed to them to maximize conversion. Your job is therefore simple: each customer should be assigned a score which represents the probability he/she purchases a product from the special offer.</p><p id="f9f7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The special offer will specifically target those brands, meaning that the marketing team wants to know which customers will buy their next product from the below brands:</p><p id="d539" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">Allegra K, Calvin Klein, Carhartt, Hanes, Volcom, Nautica, Quiksilver, Diesel, Dockers, Hurley</em></p><p id="846b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We will, for this article, use a publicly available dataset from Google, the `<em class="nz">thelook_ecommerce</em>` <a class="af nc" href="https://console.cloud.google.com/marketplace/product/bigquery-public-data/thelook-ecommerce" rel="noopener ugc nofollow" target="_blank">dataset</a>. It contains fake data with transactions, customer data, product data, everything we would have at our disposal when working at an online fashion retailer.</p><p id="be4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To follow this notebook, you will need access to Google Cloud Platform, but the logic can be replicated to other Cloud providers or third-parties like Neptune, MLFlow, etc.</p><p id="7295" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a respectable Data Scientist, you start by creating a notebook which will help us in exploring the data.</p><p id="3bc6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We first import libraries which we will use during this article:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="f8b8" class="pf ob fq pc b bg pg ph l pi pj">import catboost as cb<br/>import pandas as pd<br/>import sklearn as sk<br/>import numpy as np<br/>import datetime as dt<br/><br/>from dataclasses import dataclass<br/>from sklearn.model_selection import train_test_split<br/>from google.cloud import bigquery<br/><br/>%load_ext watermark<br/>%watermark --packages catboost,pandas,sklearn,numpy,google.cloud.bigquery<br/></span></pre><pre class="pk pb pc pd bp pe bb bk"><span id="8f7f" class="pf ob fq pc b bg pg ph l pi pj">catboost             : 1.0.4<br/>pandas               : 1.4.2<br/>numpy                : 1.22.4<br/>google.cloud.bigquery: 3.2.0</span></pre><h1 id="49d5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Before Production</h1><h2 id="f8b7" class="pl ob fq bf oc pm pn po of pp pq pr oi nm ps pt pu nq pv pw px nu py pz qa qb bk">Getting and preparing the data</h2><p id="32a1" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We will then load the data from BigQuery using the Python Client. Be sure to use your own project id:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="01b1" class="pf ob fq pc b bg pg ph l pi pj">query = """<br/>    SELECT <br/>      transactions.user_id,<br/>      products.brand,<br/>      products.category,<br/>      products.department,<br/>      products.retail_price,<br/>      users.gender,<br/>      users.age,<br/>      users.created_at,<br/>      users.country,<br/>      users.city,<br/>      transactions.created_at<br/>    FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions<br/>    LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users<br/>      ON transactions.user_id = users.id<br/>    LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products<br/>      ON transactions.product_id = products.id<br/>    WHERE status &lt;&gt; 'Cancelled'<br/>"""<br/><br/>client = bigquery.Client()<br/>df = client.query(query).to_dataframe()<br/></span></pre><p id="6d3d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You should see something like that when looking at the dataframe:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/880c9b38a51bc63c48840f0c89577208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-9goChd9OtcsoYf88KVpJg.png"/></div></figure><p id="375b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These represent the transactions / purchases made by the customers, enriched with customer and product information.</p><p id="86ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Given our objective is to predict which brand customers will buy in their next purchase, we will proceed as follows:</p><ol class=""><li id="aa52" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qd qe qf bk">Group purchases chronologically for each customer</li><li id="b100" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">If a customer has N purchases, we consider the Nth purchase as the target, and the N-1 as our features.</li><li id="627f" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk">We therefore exclude customers with only 1 purchase</li></ol><p id="0b0c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s put that into code:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="632e" class="pf ob fq pc b bg pg ph l pi pj"># Compute recurrent customers<br/>recurrent_customers = df.groupby('user_id')['created_at'].count().to_frame("n_purchases")<br/><br/># Merge with dataset and filter those with more than 1 purchase<br/>df = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')<br/>df = df.query('n_purchases &gt; 1')<br/><br/># Fill missing values<br/>df.fillna('NA', inplace=True)<br/><br/>target_brands = [<br/>    'Allegra K', <br/>    'Calvin Klein', <br/>    'Carhartt', <br/>    'Hanes', <br/>    'Volcom', <br/>    'Nautica', <br/>    'Quiksilver', <br/>    'Diesel',<br/>    'Dockers', <br/>    'Hurley'<br/>]<br/><br/>aggregation_columns = ['brand', 'department', 'category']<br/><br/># Group purchases by user chronologically<br/>df_agg = (df.sort_values('created_at')<br/>          .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]<br/>          .agg({k: ";".join for k in ['brand', 'department', 'category']})<br/>         )<br/><br/># Create the target<br/>df_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(";")[-1])<br/>df_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1<br/><br/>df_agg['age'] = df_agg['age'].astype(float)<br/><br/># Remove last item of sequence features to avoid target leakage :<br/>for col in aggregation_columns:<br/>    df_agg[col] = df_agg[col].apply(lambda x: ";".join(x.split(";")[:-1]))</span></pre><p id="ed4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice how we removed the last item in the sequence features: this is very important as otherwise we get what we call a “data leakeage”: the target is part of the features, the model is given the answer when learning.</p><p id="3134" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now get this new <code class="cx ql qm qn pc b">df_agg</code> dataframe:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/0b0974517e0f107036fa37d8339f6f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fgjb9NGmqXyJ-a_b15NVw.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/5cb46a927920c0bb0df6c3465aa5ec81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ZijbnEeo5AAmZkdyZJGBA.png"/></div></div></figure><p id="1611" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Comparing with the original dataframe, we see that user_id 2 has indeed purchased IZOD, Parke &amp; Ronen, and finally Orvis which is not in the target brands.</p><h2 id="2831" class="pl ob fq bf oc pm pn po of pp pq pr oi nm ps pt pu nq pv pw px nu py pz qa qb bk">Splitting into train, validation and test</h2><p id="1f5a" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">As a seasoned Data Scientist, you will now split your data into different sets, as you obviously know that all three are required to perform some rigorous Machine Learning. <em class="nz">(Cross-validation is out of the scope for today folks, let’s keep it simple.)</em></p><p id="a32d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One key thing when splitting the data is to use the not-so-well-known <code class="cx ql qm qn pc b">stratify</code> parameter from the scikit-learn <code class="cx ql qm qn pc b">train_test_split()</code> method. The reason for that is because of class-imbalance: if the target distribution (% of 0 and 1 in our case) differs between training and testing, we might get frustrated with poor results when deploying the model. <em class="nz">ML 101 kids: keep you data distributions as similar as possible between training data and test data.</em></p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="8efb" class="pf ob fq pc b bg pg ph l pi pj"># Remove unecessary features<br/><br/>df_agg.drop('last_purchase_category', axis=1, inplace=True)<br/>df_agg.drop('last_purchase_brand', axis=1, inplace=True)<br/>df_agg.drop('user_id', axis=1, inplace=True)<br/><br/># Split the data into train and eval<br/>df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)<br/>print(f"{len(df_train)} samples in train")<br/><br/>df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)<br/>print(f"{len(df_train)} samples in train") <br/># 30950 samples in train<br/><br/>df_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)<br/>print(f"{len(df_val)} samples in val")<br/>print(f"{len(df_test)} samples in test")<br/># 3869 samples in train<br/># 3869 samples in test</span></pre><p id="f29a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now this is done, we will gracefully split our dataset between features and targets:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="017d" class="pf ob fq pc b bg pg ph l pi pj">X_train, y_train = df_train.iloc[:, :-1], df_train['target']<br/>X_val, y_val = df_val.iloc[:, :-1], df_val['target']<br/>X_test, y_test = df_test.iloc[:, :-1], df_test['target']</span></pre><p id="3a99" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Among the feature are different types. We usually separate those between:</p><ul class=""><li id="7813" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qq qe qf bk">numerical features: they are continuous, and reflect a measurable, or ordered, quantity.</li><li id="6b8f" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qq qe qf bk">categorical features: they are usually discrete, and are often represented as strings (ex: a country, a color, etc…)</li><li id="b74a" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qq qe qf bk">text features: they are usually sequences of words.</li></ul><p id="7716" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Of course there can be more like image, video, audio, etc.</p><h2 id="baff" class="pl ob fq bf oc pm pn po of pp pq pr oi nm ps pt pu nq pv pw px nu py pz qa qb bk">The model: introducing CatBoost</h2><p id="3246" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">For our classification problem (you already knew we were in a classification framework, didn’t you?), we will use a simple yet very powerful library: CatBoost. It is built and maintained by Yandex, and provides a high-level API to easily play with boosted trees. It is close to XGBoost, though it does not work exactly the same under the hood.</p><p id="7dc1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">CatBoost offers a nice wrapper to deal with features from different kinds. In our case, some features can be considered as “text” as they are the concatenation of words, such as “Calvin Klein;BCBGeneration;Hanes”. Dealing with this type of features can sometimes be painful as you need to handle them with text splitters, tokenizers, lemmatizers, etc. Hopefully, CatBoost can manage everything for us!</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="9608" class="pf ob fq pc b bg pg ph l pi pj"># Define features<br/>features = {<br/>    'numerical': ['retail_price', 'age'],<br/>    'static': ['gender', 'country', 'city'],<br/>    'dynamic': ['brand', 'department', 'category']<br/>}<br/><br/># Build CatBoost "pools", which are datasets<br/>train_pool = cb.Pool(<br/>    X_train,<br/>    y_train,<br/>    cat_features=features.get("static"),<br/>    text_features=features.get("dynamic"),<br/>)<br/><br/>validation_pool = cb.Pool(<br/>    X_val,<br/>    y_val,<br/>    cat_features=features.get("static"),<br/>    text_features=features.get("dynamic"),<br/>)<br/><br/># Specify text processing options to handle our text features<br/>text_processing_options = {<br/>    "tokenizers": [<br/>        {"tokenizer_id": "SemiColon", "delimiter": ";", "lowercasing": "false"}<br/>    ],<br/>    "dictionaries": [{"dictionary_id": "Word", "gram_order": "1"}],<br/>    "feature_processing": {<br/>        "default": [<br/>            {<br/>                "dictionaries_names": ["Word"],<br/>                "feature_calcers": ["BoW"],<br/>                "tokenizers_names": ["SemiColon"],<br/>            }<br/>        ],<br/>    },<br/>}</span></pre><p id="7344" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We are now ready to define and train our model. Going through each and every parameter is out of today’s scope as the number of parameters is quite impressive, but feel free to check the API yourself.</p><p id="3c32" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And for brevity, we will not perform hyperparameter tuning today, but this is obviously a large part of the Data Scientist’s job!</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="cf2e" class="pf ob fq pc b bg pg ph l pi pj"># Train the model<br/>model = cb.CatBoostClassifier(<br/>    iterations=200,<br/>    loss_function="Logloss",<br/>    random_state=42,<br/>    verbose=1,<br/>    auto_class_weights="SqrtBalanced",<br/>    use_best_model=True,<br/>    text_processing=text_processing_options,<br/>    eval_metric='AUC'<br/>)<br/><br/>model.fit(<br/>    train_pool, <br/>    eval_set=validation_pool, <br/>    verbose=10<br/>)</span></pre><p id="236d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And voila, our model is trained. Are we done?</p><p id="49d8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">No. We need to check that our model’s performance between training and testing is consistent. A huge gap between training and testing means our model is overfitting (i.e. “learning the training data by heart and not good at predicting unseen data”).</p><p id="8133" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For our model evaluation, we will use the ROC-AUC score. Not deep-diving on this one either, but from my own experience this is a generally quite robust metric and way better than accuracy.</p><blockquote class="qr qs qt"><p id="8cdf" class="nd ne nz nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A quick side note on accuracy: I usually do not recommend using this as your evaluation metric. Think of an imbalanced dataset where you have 1% of positives and 99% of negatives. What would be the accuracy of a very dumb model predicting 0 all the time? 99%. So accuracy not helpful here.</p></blockquote><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="3b72" class="pf ob fq pc b bg pg ph l pi pj">from sklearn.metrics import roc_auc_score<br/><br/>print(f"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=model.predict(X_train)):.2f}")<br/>print(f"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=model.predict(X_val)):.2f}")<br/>print(f"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=model.predict(X_test)):.2f}")</span></pre><pre class="pk pb pc pd bp pe bb bk"><span id="dcb3" class="pf ob fq pc b bg pg ph l pi pj">ROC-AUC for train set      : 0.612<br/>ROC-AUC for validation set : 0.586<br/>ROC-AUC for test set       : 0.622</span></pre><p id="b05c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To be honest, 0.62 AUC is not great at all and a little bit disappointing for the expert Data Scientist you are. Our model definitely needs a little bit of parameter tuning here, and maybe we should also perform feature engineering more seriously.</p><p id="a971" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But it is already better than random predictions (phew):</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="7c7f" class="pf ob fq pc b bg pg ph l pi pj"># random predictions<br/><br/>print(f"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=np.random.rand(len(y_train))):.3f}")<br/>print(f"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=np.random.rand(len(y_val))):.3f}")<br/>print(f"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=np.random.rand(len(y_test))):.3f}")</span></pre><pre class="pk pb pc pd bp pe bb bk"><span id="9d5d" class="pf ob fq pc b bg pg ph l pi pj">ROC-AUC for train set      : 0.501<br/>ROC-AUC for validation set : 0.499<br/>ROC-AUC for test set       : 0.501</span></pre><p id="87b8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s assume we are satisfied for now with our model and our notebook. This is where amateur Data Scientists would stop. So how do we make the next step and become production ready?</p><h1 id="ec3b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Moving to Production</h1><h2 id="a26b" class="pl ob fq bf oc pm pn po of pp pq pr oi nm ps pt pu nq pv pw px nu py pz qa qb bk">Meet Docker</h2><p id="53a0" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. This being said, think of Docker as code which can run everywhere, and allowing you to avoid the “works on your machine but not on mine” situation.</p><p id="ae3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Why use Docker? Because among cool things such as being able to share your code, keep versions of it and ensure its easy deployment everywhere, it can also be used to build pipelines. Bear with me and you will understand as we go.</p><p id="f363" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first step to building a containerized application is to refactor and clean up our messy notebook. We are going to define 2 files, <code class="cx ql qm qn pc b">preprocess.py</code> and <code class="cx ql qm qn pc b">train.py</code> for our very simple example, and put them in a <code class="cx ql qm qn pc b">src</code> directory. We will also include our <code class="cx ql qm qn pc b">requirements.txt</code> file with everything in it.</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="116d" class="pf ob fq pc b bg pg ph l pi pj"># src/preprocess.py<br/><br/>from sklearn.model_selection import train_test_split<br/>from google.cloud import bigquery<br/><br/>def create_dataset_from_bq():<br/>    query = """<br/>        SELECT <br/>          transactions.user_id,<br/>          products.brand,<br/>          products.category,<br/>          products.department,<br/>          products.retail_price,<br/>          users.gender,<br/>          users.age,<br/>          users.created_at,<br/>          users.country,<br/>          users.city,<br/>          transactions.created_at<br/>        FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions<br/>        LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users<br/>          ON transactions.user_id = users.id<br/>        LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products<br/>          ON transactions.product_id = products.id<br/>        WHERE status &lt;&gt; 'Cancelled'<br/>    """<br/>    client = bigquery.Client(project='&lt;replace_with_your_project_id&gt;')<br/>    df = client.query(query).to_dataframe()<br/>    print(f"{len(df)} rows loaded.")<br/>    <br/>    # Compute recurrent customers<br/>    recurrent_customers = df.groupby('user_id')['created_at'].count().to_frame("n_purchases")<br/><br/>    # Merge with dataset and filter those with more than 1 purchase<br/>    df = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')<br/>    df = df.query('n_purchases &gt; 1')<br/><br/>    # Fill missing value<br/>    df.fillna('NA', inplace=True)<br/><br/>    target_brands = [<br/>        'Allegra K', <br/>        'Calvin Klein', <br/>        'Carhartt', <br/>        'Hanes', <br/>        'Volcom', <br/>        'Nautica', <br/>        'Quiksilver', <br/>        'Diesel',<br/>        'Dockers', <br/>        'Hurley'<br/>    ]<br/><br/>    aggregation_columns = ['brand', 'department', 'category']<br/><br/>    # Group purchases by user chronologically<br/>    df_agg = (df.sort_values('created_at')<br/>              .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]<br/>              .agg({k: ";".join for k in ['brand', 'department', 'category']})<br/>             )<br/>    <br/>    # Create the target<br/>    df_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(";")[-1])<br/>    df_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1<br/>    <br/>    df_agg['age'] = df_agg['age'].astype(float)<br/>    <br/>    # Remove last item of sequence features to avoid target leakage :<br/>    for col in aggregation_columns:<br/>        df_agg[col] = df_agg[col].apply(lambda x: ";".join(x.split(";")[:-1]))<br/>    <br/>    df_agg.drop('last_purchase_category', axis=1, inplace=True)<br/>    df_agg.drop('last_purchase_brand', axis=1, inplace=True)<br/>    df_agg.drop('user_id', axis=1, inplace=True)<br/>    return df_agg<br/>    <br/>    <br/>def make_data_splits(df_agg):<br/><br/>    df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)<br/>    print(f"{len(df_train)} samples in train")<br/><br/>    df_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)<br/>    print(f"{len(df_val)} samples in val")<br/>    print(f"{len(df_test)} samples in test")<br/>    <br/>    return df_train, df_val, df_test</span></pre><pre class="pk pb pc pd bp pe bb bk"><span id="f8f8" class="pf ob fq pc b bg pg ph l pi pj"># src/train.py<br/><br/>import catboost as cb<br/>import pandas as pd<br/>import sklearn as sk<br/>import numpy as np<br/>import argparse<br/><br/>from sklearn.metrics import roc_auc_score<br/>        <br/>        <br/>def train_and_evaluate(<br/>        train_path: str,<br/>        validation_path: str,<br/>        test_path: str<br/>    ):<br/>    df_train = pd.read_csv(train_path)<br/>    df_val = pd.read_csv(validation_path)<br/>    df_test = pd.read_csv(test_path)<br/>    <br/>    df_train.fillna('NA', inplace=True)<br/>    df_val.fillna('NA', inplace=True)<br/>    df_test.fillna('NA', inplace=True)<br/><br/>    X_train, y_train = df_train.iloc[:, :-1], df_train['target']<br/>    X_val, y_val = df_val.iloc[:, :-1], df_val['target']<br/>    X_test, y_test = df_test.iloc[:, :-1], df_test['target']<br/><br/>    features = {<br/>        'numerical': ['retail_price', 'age'],<br/>        'static': ['gender', 'country', 'city'],<br/>        'dynamic': ['brand', 'department', 'category']<br/>    }<br/><br/>    train_pool = cb.Pool(<br/>        X_train,<br/>        y_train,<br/>        cat_features=features.get("static"),<br/>        text_features=features.get("dynamic"),<br/>    )<br/><br/>    validation_pool = cb.Pool(<br/>        X_val,<br/>        y_val,<br/>        cat_features=features.get("static"),<br/>        text_features=features.get("dynamic"),<br/>    )<br/>    <br/>    test_pool = cb.Pool(<br/>        X_test,<br/>        y_test,<br/>        cat_features=features.get("static"),<br/>        text_features=features.get("dynamic"),<br/>    )<br/><br/><br/>    params = CatBoostParams()<br/><br/>    text_processing_options = {<br/>        "tokenizers": [<br/>            {"tokenizer_id": "SemiColon", "delimiter": ";", "lowercasing": "false"}<br/>        ],<br/>        "dictionaries": [{"dictionary_id": "Word", "gram_order": "1"}],<br/>        "feature_processing": {<br/>            "default": [<br/>                {<br/>                    "dictionaries_names": ["Word"],<br/>                    "feature_calcers": ["BoW"],<br/>                    "tokenizers_names": ["SemiColon"],<br/>                }<br/>            ],<br/>        },<br/>    }<br/><br/>    # Train the model<br/>    model = cb.CatBoostClassifier(<br/>        iterations=200,<br/>        loss_function="Logloss",<br/>        random_state=42,<br/>        verbose=1,<br/>        auto_class_weights="SqrtBalanced",<br/>        use_best_model=True,<br/>        text_processing=text_processing_options,<br/>        eval_metric='AUC'<br/>    )<br/><br/><br/>    model.fit(<br/>        train_pool, <br/>        eval_set=validation_pool, <br/>        verbose=10<br/>    )<br/>    <br/>    roc_train = roc_auc_score(y_true=y_train, y_score=model.predict(X_train))<br/>    roc_eval  = roc_auc_score(y_true=y_val, y_score=model.predict(X_val))<br/>    roc_test  = roc_auc_score(y_true=y_test, y_score=model.predict(X_test))<br/>    print(f"ROC-AUC for train set      : {roc_train:.2f}")<br/>    print(f"ROC-AUC for validation set : {roc_eval:.2f}")<br/>    print(f"ROC-AUC for test.      set : {roc_test:.2f}")<br/>    <br/>    return {"model": model, "scores": {"train": roc_train, "eval": roc_eval, "test": roc_test}}<br/><br/>if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument("--train-path", type=str)<br/>    parser.add_argument("--validation-path", type=str)<br/>    parser.add_argument("--test-path", type=str)<br/>    parser.add_argument("--output-dir", type=str)<br/>    args, _ = parser.parse_known_args()<br/>    _ = train_and_evaluate(<br/>        args.train_path,<br/>        args.validation_path,<br/>        args.test_path)</span></pre><p id="64fa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Much cleaner now. You can actually launch your script from the command line now!</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="4597" class="pf ob fq pc b bg pg ph l pi pj">$ python train.py --train-path xxx --validation-path yyy etc.</span></pre><p id="1566" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We are now ready to build our Docker image. For that we need to write a Dockerfile at the root of the project:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="451e" class="pf ob fq pc b bg pg ph l pi pj"># Dockerfile<br/><br/>FROM python:3.8-slim<br/>WORKDIR /<br/>COPY requirements.txt /requirements.txt<br/>COPY src /src<br/>RUN pip install --upgrade pip &amp;&amp; pip install -r requirements.txt<br/>ENTRYPOINT [ "bash" ]</span></pre><p id="ae16" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This will take our requirements, copy the <code class="cx ql qm qn pc b">src</code> folder and its contents, and install the requirements with pip when the image will build.</p><p id="f058" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To build and deploy this image to a container registry, we can use the Google Cloud SDK and the <code class="cx ql qm qn pc b">gcloud</code> commands:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="7024" class="pf ob fq pc b bg pg ph l pi pj">PROJECT_ID = ...<br/>IMAGE_NAME=f'thelook_training_demo'<br/>IMAGE_TAG='latest'<br/>IMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)<br/><br/>!gcloud builds submit --tag $IMAGE_URI .</span></pre><p id="e77d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If everything goes well, you should see something like that:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/696d3dbba8791000cd8405119c1144d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8d-pUHixcRtoDr3QSfQSA.png"/></div></div></figure><h2 id="53e3" class="pl ob fq bf oc pm pn po of pp pq pr oi nm ps pt pu nq pv pw px nu py pz qa qb bk">Vertex Pipelines, the move to production</h2><p id="15c9" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Docker images are the first step to doing some serious Machine Learning in production. The next step is building what we call “pipelines”. Pipelines are a series of operations orchestrated by a framework called Kubeflow. Kubeflow can run on Vertex AI on Google Cloud.</p><p id="f221" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The reasons for preferring pipelines over notebooks in production can be debatable, but I will give you three based on my experience:</p><ol class=""><li id="ae1d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qd qe qf bk"><strong class="nf fr">Monitoring and reproducibility</strong>: each pipeline is stored with its artefacts (datasets, models, metrics), meaning you can compare runs, re-run them, and audit them. Each time you re-run a notebook, you lose the history (or you have to manage artefacts yourself as weel as the logs. Good luck.)</li><li id="8731" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk"><strong class="nf fr">Costs</strong>: Running a notebook implies having a machine on which it runs. — This machine has a cost, and for large models or huge datasets you will need virtual machines with heavy specs. <br/>— You have to remember to switch it off when you don’t use it. <br/>— Or you may simply crash your local machine if you choose not to use a virtual machine and have other applications running.<br/> — Vertex AI pipelines is a <em class="nz">serverless</em> service, meaning you do not have to manage the underlying infrastructure, and only pay for what you use, meaning the execution time.</li><li id="34db" class="nd ne fq nf b go qg nh ni gr qh nk nl nm qi no np nq qj ns nt nu qk nw nx ny qd qe qf bk"><strong class="nf fr">Scalability</strong>: Good luck when running dozens of experiments on your local laptop simultaneously. You will roll back to using a VM, and scale that VM, and re-read the bullet point above.</li></ol><p id="1f53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The last reason to prefer pipelines over notebooks is subjective and highly debatable as well, but in my opinion notebooks are simply not designed for running workloads on a schedule. They are great though for exploration.</p><p id="9b72" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Use a cron job with a Docker image at least, or pipelines if you want to do things the right way, but never, ever, run a notebook in production.</p><p id="eb56" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Without further ado, let’s write the components of our pipeline:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="8389" class="pf ob fq pc b bg pg ph l pi pj"># IMPORT REQUIRED LIBRARIES<br/>from kfp.v2 import dsl<br/>from kfp.v2.dsl import (Artifact,<br/>                        Dataset,<br/>                        Input,<br/>                        Model,<br/>                        Output,<br/>                        Metrics,<br/>                        Markdown,<br/>                        HTML,<br/>                        component, <br/>                        OutputPath, <br/>                        InputPath)<br/>from kfp.v2 import compiler<br/>from google.cloud.aiplatform import pipeline_jobs<br/><br/>%watermark --packages kfp,google.cloud.aiplatform</span></pre><pre class="pk pb pc pd bp pe bb bk"><span id="d019" class="pf ob fq pc b bg pg ph l pi pj">kfp                    : 2.7.0<br/>google.cloud.aiplatform: 1.50.0</span></pre><p id="476c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first component will download the data from Bigquery and store it as a CSV file.</p><p id="dce0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The BASE_IMAGE we use is the image we build previously! We can use it to import modules and functions we defined in our Docker image <code class="cx ql qm qn pc b">src</code> folder:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="8d3e" class="pf ob fq pc b bg pg ph l pi pj">@component(<br/>    base_image=BASE_IMAGE,<br/>    output_component_file="get_data.yaml"<br/>)<br/>def create_dataset_from_bq(<br/>    output_dir: Output[Dataset],<br/>):<br/>    <br/>    from src.preprocess import create_dataset_from_bq<br/>    <br/>    df = create_dataset_from_bq()<br/>    <br/>    df.to_csv(output_dir.path, index=False)</span></pre><p id="1b5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next step: split data</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="f8b4" class="pf ob fq pc b bg pg ph l pi pj">@component(<br/>    base_image=BASE_IMAGE,<br/>    output_component_file="train_test_split.yaml",<br/>)<br/>def make_data_splits(<br/>    dataset_full: Input[Dataset],<br/>    dataset_train: Output[Dataset],<br/>    dataset_val: Output[Dataset],<br/>    dataset_test: Output[Dataset]):<br/><br/>    import pandas as pd<br/>    from src.preprocess import make_data_splits<br/><br/>    df_agg = pd.read_csv(dataset_full.path)<br/><br/>    df_agg.fillna('NA', inplace=True)<br/><br/>    df_train, df_val, df_test = make_data_splits(df_agg)<br/>    print(f"{len(df_train)} samples in train")<br/>    print(f"{len(df_val)} samples in train")<br/>    print(f"{len(df_test)} samples in test")<br/>    <br/>    df_train.to_csv(dataset_train.path, index=False)<br/>    df_val.to_csv(dataset_val.path, index=False)<br/>    df_test.to_csv(dataset_test.path, index=False)</span></pre><p id="c547" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next step: model training. We will save the model scores to display them in the next step:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="c975" class="pf ob fq pc b bg pg ph l pi pj">@component(<br/>    base_image=BASE_IMAGE,<br/>    output_component_file="train_model.yaml",<br/>)<br/>def train_model(<br/>    dataset_train: Input[Dataset],<br/>    dataset_val: Input[Dataset],<br/>    dataset_test: Input[Dataset],<br/>    model: Output[Model]<br/>):<br/><br/>    import json<br/>    from src.train import train_and_evaluate<br/><br/>    outputs = train_and_evaluate(<br/>        dataset_train.path,<br/>        dataset_val.path,<br/>        dataset_test.path<br/>    )<br/>    cb_model = outputs['model']<br/>    scores = outputs['scores']<br/><br/>    <br/>    model.metadata["framework"] = "catboost" <br/>    # Save the model as an artifact<br/>    with open(model.path, 'w') as f: <br/>        json.dump(scores, f)</span></pre><p id="8d1e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The last step is computing the metrics (which are actually computed in the training of the model). It is merely necessary but is nice to show you how easy it is to build lightweight components. Notice how in this case we don’t build the component from the BASE_IMAGE (which can be quite large sometimes), but only build a lightweight image with necessary components:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="790c" class="pf ob fq pc b bg pg ph l pi pj">@component(<br/>    base_image="python:3.9",<br/>    output_component_file="compute_metrics.yaml",<br/>)<br/>def compute_metrics(<br/>    model: Input[Model],<br/>    train_metric: Output[Metrics],<br/>    val_metric: Output[Metrics],<br/>    test_metric: Output[Metrics]<br/>):<br/>    <br/>    import json<br/>    <br/>    file_name = model.path<br/>    with open(file_name, 'r') as file:  <br/>        model_metrics = json.load(file)<br/>        <br/>    train_metric.log_metric('train_auc', model_metrics['train'])<br/>    val_metric.log_metric('val_auc', model_metrics['eval'])<br/>    test_metric.log_metric('test_auc', model_metrics['test'])</span></pre><p id="552d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are usually other steps which we can include, like if we want to deploy our model as an API endpoint, but this is more advanced-level and requires crafting another Docker image for the serving of the model. To be covered next time.</p><p id="ba23" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s now glue the components together:</p><pre class="mm mn mo mp mq pb pc pd bp pe bb bk"><span id="9bc4" class="pf ob fq pc b bg pg ph l pi pj"># USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES<br/>TIMESTAMP = dt.datetime.now().strftime("%Y%m%d%H%M%S")<br/>DISPLAY_NAME = 'pipeline-thelook-demo-{}'.format(TIMESTAMP)<br/>PIPELINE_ROOT = f"{BUCKET_NAME}/pipeline_root/"<br/><br/># Define the pipeline. Notice how steps reuse outputs from previous steps<br/>@dsl.pipeline(<br/>    pipeline_root=PIPELINE_ROOT,<br/>    # A name for the pipeline. Use to determine the pipeline Context.<br/>    name="pipeline-demo"   <br/>)<br/><br/>def pipeline(<br/>    project: str = PROJECT_ID,<br/>    region: str = REGION, <br/>    display_name: str = DISPLAY_NAME<br/>):<br/><br/>    load_data_op = create_dataset_from_bq()<br/>    train_test_split_op = make_data_splits(<br/>        dataset_full=load_data_op.outputs["output_dir"]<br/>    )<br/>    train_model_op = train_model(<br/>        dataset_train=train_test_split_op.outputs["dataset_train"], <br/>        dataset_val=train_test_split_op.outputs["dataset_val"],<br/>        dataset_test=train_test_split_op.outputs["dataset_test"],<br/>        )<br/>    model_evaluation_op = compute_metrics(<br/>        model=train_model_op.outputs["model"]<br/>    )<br/><br/># Compile the pipeline as JSON<br/>compiler.Compiler().compile(<br/>    pipeline_func=pipeline,<br/>    package_path='thelook_pipeline.json'<br/>)<br/><br/># Start the pipeline<br/>start_pipeline = pipeline_jobs.PipelineJob(<br/>    display_name="thelook-demo-pipeline",<br/>    template_path="thelook_pipeline.json",<br/>    enable_caching=False,<br/>    location=REGION,<br/>    project=PROJECT_ID<br/>)<br/><br/># Run the pipeline<br/>start_pipeline.run(service_account=&lt;your_service_account_here&gt;)</span></pre><p id="7249" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If everything works well, you will now see your pipeline in the Vertex UI:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/ed0f2a97ea1def0df9a9ccdac40f0be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tBmKyqQ56ZidTPZwvjLy6g.png"/></div></div></figure><p id="c9d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can click on it and see the different steps:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/c8ce67a3cf2ebdcff0665e1cbc227d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHF1eXesttYAQL_Wu3HCuQ.png"/></div></div></figure><h1 id="d19b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Conclusion</h1><p id="dca6" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Data Science, despite all the no-code/low-code enthusiasts telling you you don’t need to be a developer to do Machine Learning, is a real job. Like every job, it requires skills, concepts and tools which go beyond notebooks.</p><p id="4e1f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And for those who aspire to become Data Scientists, here is the reality of the job.</p><p id="96b6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Happy coding.</p></div></div></div></div>    
</body>
</html>