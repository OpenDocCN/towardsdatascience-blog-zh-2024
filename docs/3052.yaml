- en: Conditional Variational Autoencoders for Text to Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件变分自编码器用于文本到图像生成
- en: 原文：[https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21](https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21](https://towardsdatascience.com/conditional-variational-autoencoders-for-text-to-image-generation-1996da9cefcb?source=collection_archive---------3-----------------------#2024-12-21)
- en: Investigating an early generative architecture and applying it to image generation
    from text input
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究早期的生成架构并将其应用于从文本输入生成图像
- en: '[](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)[![Ryan
    D''Cunha](../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png)](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)
    [Ryan D''Cunha](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)[![Ryan
    D''Cunha](../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png)](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)
    [Ryan D''Cunha](https://medium.com/@rtdcunha?source=post_page---byline--1996da9cefcb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)
    ·12 min read·Dec 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1996da9cefcb--------------------------------)
    ·12 分钟阅读·2024年12月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Recently I was tasked with text-to-image synthesis using a conditional variational
    autoencoder (CVAE). Being one of the earlier generative structures, it has its
    limitations but is easily implementable. This article will cover CVAEs at a high
    level, but the reader is presumed to have a high level understanding to cover
    the applications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我的任务是使用条件变分自编码器（CVAE）进行文本到图像的合成。作为早期的生成结构之一，它有其局限性，但实现起来相对简单。本文将从高层次介绍 CVAE，但假定读者已经具备较高水平的理解，以便涵盖应用。
- en: Generative modeling is a field within machine learning focused on learning the
    underlying distributions responsible for creating data. Understanding these distributions
    enables models to generalize across various datasets, facilitating knowledge transfer
    and effectively addressing issues of data sparsity. We ideally want contiguous
    encodings while still being distinct to allow for smooth interpolation to generate
    new samples.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成建模是机器学习领域的一部分，专注于学习负责创建数据的底层分布。理解这些分布使得模型能够在不同的数据集之间进行泛化，促进知识转移并有效解决数据稀疏性问题。我们理想中希望得到连续的编码，同时又能保持其独特性，以便实现平滑插值，生成新的样本。
- en: Introduction to VAEs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VAE 简介
- en: While typical autoencoders are deterministic, VAEs are probabilistic models
    due to modeling the latent space as a probability distribution. VAEs are unsupervised
    models that encode input data *x* into a latent representation *z* and reconstruct
    the input from this latent space. They technically don’t need to be implemented
    with neural networks and can be constructed from generative probability models.
    However, in our current state of deep learning, most are typically implemented
    with neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然典型的自编码器是确定性的，但由于变分自编码器（VAE）将潜在空间建模为概率分布，它们是概率模型。VAE 是无监督模型，将输入数据 *x* 编码为潜在表示
    *z* 并从这个潜在空间重建输入数据。它们技术上不一定需要使用神经网络实现，也可以通过生成概率模型构建。然而，在当前的深度学习状态下，大多数情况下都是使用神经网络来实现的。
- en: '![](../Images/60a6f0bb9af3df9b263f45f1d50c4c95.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60a6f0bb9af3df9b263f45f1d50c4c95.png)'
- en: 'Example VAE framework with reparameterization trick. Source: Author'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 VAE 框架与重参数化技巧。来源：作者
- en: Explained briefly, the reparameterization trick is used since we can’t backpropagate
    on the probabilistic distribution of the latent space, but we need to update our
    encoding distribution. Therefore, we define a differentiable and invertible function
    so that we can differentiate with respect to lambda and *x* while still keeping
    a probabilistic element.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简要解释，重参数化技巧的使用是因为我们无法在潜在空间的概率分布上进行反向传播，但我们需要更新我们的编码分布。因此，我们定义了一个可微分且可逆的函数，以便我们可以对lambda和*x*求导，同时仍然保持概率元素。
- en: '![](../Images/cb15e673b3172b4a80e27abb94b8b050.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb15e673b3172b4a80e27abb94b8b050.png)'
- en: 'Reparameterization trick for z. Source: Author'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: z的重参数化技巧。来源：作者
- en: VAEs are trained using an ELBO loss consisting of a reconstruction term and
    a Kullback-Leibler Divergence (KLD) of the encoding model to the prior distribution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: VAE通过ELBO损失进行训练，ELBO损失包含重构项和编码模型与先验分布之间的Kullback-Leibler散度（KLD）。
- en: '![](../Images/dc52cd2b04c93b4d7697ae504602e3e4.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc52cd2b04c93b4d7697ae504602e3e4.png)'
- en: Loss function for VAE with KLD term on left and reconstruction term on righ
    [1]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的损失函数，左侧是KLD项，右侧是重构项[1]
- en: Adding a Conditional Input to VAE
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向VAE添加条件输入
- en: CVAEs extend VAEs by incorporating additional information such as class labels
    as conditional variables. This conditioning enables CVAEs to produce controlled
    generations. The conditional input feature can be added at differing points in
    the architecture, but it is commonly inserted with the encoder and the decoder.
    The loss function with the conditional input is an adaptation of the ELBO loss
    in the traditional VAE.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CVAE通过将额外信息（如类别标签）作为条件变量引入，扩展了VAE。这种条件化使得CVAE能够生成受控的结果。条件输入特性可以在架构中的不同点添加，但通常是在编码器和解码器之间插入。带有条件输入的损失函数是传统VAE中ELBO损失的适应版。
- en: '![](../Images/2b6e5854bf9987b4bb9ce4d79aa204b4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b6e5854bf9987b4bb9ce4d79aa204b4.png)'
- en: Loss function for VAE with KLD term on left and reconstruction term on right
    [2]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的损失函数，左侧是KLD项，右侧是重构项[2]
- en: To illustrate the difference between a VAE and CVAE, both networks were trained
    on Fashion-MNIST using a convolutional encoder and decoder architecture. A tSNE
    of the latent space of each network is shown.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明VAE和CVAE之间的区别，两个网络都在Fashion-MNIST数据集上使用卷积编码器和解码器架构进行了训练。每个网络的潜在空间tSNE图如图所示。
- en: '![](../Images/8c67ef17277457da060975b9d2a4e8cd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c67ef17277457da060975b9d2a4e8cd.png)'
- en: 'Latent space manifold of VAE (left) and CVAE (right). Source: Author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: VAE（左）和CVAE（右）的潜在空间流形。来源：作者
- en: The vanilla VAE shows distinct clusters while the CVAE has a more homogeneous
    distribution. Vanilla VAE encodes class and class variation into the latent space
    since there is no provided conditional signal. However, the CVAE does not need
    to learn class distinction and the latent space can focus on the variation within
    classes. Therefore, a CVAE can potentially learn more information as it does not
    rely on having to learn basic class conditioning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 普通VAE显示出明显的聚类，而CVAE则具有更均匀的分布。普通VAE将类别和类别变异编码到潜在空间中，因为没有提供条件信号。然而，CVAE不需要学习类别区分，潜在空间可以集中在类别内的变异。因此，CVAE可能学习更多信息，因为它不依赖于学习基本的类别条件。
- en: Model Architecture for CVAEs
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CVAE的模型架构
- en: Two model architectures were created to test image generation. The first architecture
    was a convolutional CVAE with a concatenating conditional approach. All networks
    were built for Fashion-MNIST images of size 28x28 (784 total pixels).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了两种模型架构来测试图像生成。第一种架构是带有串联条件方法的卷积CVAE。所有网络都是为大小为28x28（共784个像素）的Fashion-MNIST图像构建的。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The CVAE encoder consists of 3 convolutional layers each followed by a ReLU
    non-linearity. The output of the encoder is then flattened. The class number is
    then passed through an embedding layer and added to the encoder output. The reparameterization
    trick is then used with 2 linear layers to obtain a μ and σ in the latent space.
    Once sampled, the output of the reparameterized latent space is passed to the
    decoder now concatenated with the class number embedding layer output. The decoder
    consists of 3 transposed convolutional layers. The first two contain a ReLU non-linearity
    with the last layer containing a sigmoid non-linearity. The output of the decoder
    is a 28x28 generated image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CVAE编码器由3个卷积层组成，每个卷积层后跟一个ReLU非线性激活函数。编码器的输出随后被展平。类编号通过嵌入层传递，并与编码器输出相加。然后，使用重新参数化技巧，通过2个线性层在潜在空间中获得μ和σ。一旦采样，重新参数化后的潜在空间输出将传递给解码器，并与类编号的嵌入层输出拼接。解码器由3个反卷积层组成。前两个层包含ReLU非线性激活函数，最后一层包含Sigmoid非线性激活函数。解码器的输出是一个28x28的生成图像。
- en: The other model architecture follows the same approach but with adding the conditional
    input instead of concatenating. A major question was if adding or concatenating
    will lead to better reconstruction or generation results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种模型架构遵循相同的方法，但采用条件输入而不是拼接。一个主要问题是，添加或拼接是否会导致更好的重建或生成结果。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The same loss function is used for all CVAEs from the equation shown above.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所有CVAEs都使用相同的损失函数，计算公式如上所示。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In order to assess model-generated images, 3 quantitative metrics are commonly
    used. Mean Squared Error (MSE) was calculated by summing the squares of the difference
    between the generated image and a ground truth image pixel-wise. Structural Similarity
    Index Measure (SSIM) is a metric that evaluates image quality by comparing two
    images based on structural information, luminance, and contrast [3]. SSIM can
    be used to compare images of any size while MSE is relative to pixel size. SSIM
    score ranges from -1 to 1, where 1 indicates identical images. Frechet inception
    distance (FID) is a metric for quantifying the realism and diversity of images
    generated. As FID is a distance measure, lower scores are indicative of a better
    reconstruction of a set of images.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型生成的图像，通常使用3个定量指标。均方误差（MSE）通过计算生成图像与真实图像之间每个像素的差值平方和来得到。结构相似性指数（SSIM）是一种通过比较两幅图像的结构信息、亮度和对比度来评估图像质量的指标[3]。SSIM可以用于比较任何大小的图像，而MSE则与像素大小相关。SSIM的得分范围从-1到1，1表示图像完全相同。Frechet起始距离（FID）是量化生成图像的真实感和多样性的指标。由于FID是一种距离度量，较低的得分表示对一组图像的更好重建。
- en: Short Text to Image from Fashion-MNIST
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自Fashion-MNIST的短文本到图像
- en: Before scaling up to full text to image, CVAEs image reconstruction and generation
    on Fashion-MNIST. Fashion-MNIST is an MNIST-like dataset consisting of a training
    set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28
    grayscale image, associated with a label from 10 classes [4].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展到完整的文本到图像之前，首先在Fashion-MNIST数据集上测试了CVAEs的图像重建和生成。Fashion-MNIST是一个类似于MNIST的数据集，包含60,000个训练样本和10,000个测试样本。每个样本都是28x28的灰度图像，并且与10个类中的一个标签相关联[4]。
- en: Preprocessing functions were created to extract the relevant key word containing
    the class name from the input short-text regular expression matching. Extra descriptors
    (synonyms) were used for most classes to account for similar fashion items included
    in each class (e.g. Coat & Jacket).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了预处理函数，通过输入短文本正则表达式匹配提取包含类名的相关关键词。对于大多数类，使用了额外的描述符（同义词），以涵盖每个类中包含的相似时尚物品（例如，外套与夹克）。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The class name was then converted to its class number and used as the conditional
    input to the CVAE along. In order to generate an image, the class label extracted
    from the short text description is passed into the decoder with random samples
    from a Gaussian distribution to input the variable from the latent space.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类名随后被转换为其类编号，并作为条件输入单独传递给CVAE。为了生成图像，从短文本描述中提取的类标签被传递到解码器，同时从高斯分布中随机采样以输入来自潜在空间的变量。
- en: Before testing generation, image reconstruction is tested to ensure the functionality
    of the CVAE. Due to creating a convolutional network with 28x28 images, the network
    can be trained in less than an hour with less than 100 epochs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试生成之前，先进行图像重建测试，以确保CVAE的功能。由于创建了一个28x28图像的卷积网络，该网络可以在不到一个小时的时间内，经过不到100个epoch进行训练。
- en: '![](../Images/183a79e8aaa8367f5e79255acba2d555.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/183a79e8aaa8367f5e79255acba2d555.png)'
- en: 'CVAE reconstruction results with ground truth (left) and model output (right).
    Source: Author'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: CVAE重建结果与真实图像对比（左为真实图像，右为模型输出）。来源：作者
- en: Reconstructions contain the general shape of the ground truth images, but sharp,
    high frequency features are missing from the image. Any text or intricate design
    patterns are blurred in the model output. Inputting any short text containing
    a class of Fashion-MNIST gives generated outputs resembling reconstructed images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重建结果包含了地面真实图像的大致形状，但图像中缺少清晰的高频特征。任何包含文本或复杂设计图案的内容在模型输出中都呈现模糊状态。输入任何包含Fashion-MNIST类别的短文本会生成类似于重建图像的输出。
- en: '![](../Images/28aa56efd802c265552db30458c473bb.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28aa56efd802c265552db30458c473bb.png)'
- en: 'Generated images “dress” from CVAE Fashion-MNIST. Source: Author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从CVAE Fashion-MNIST生成的图像“服饰”。来源：作者
- en: The generated images have an MSE of 11 and a SSIM of 0.76\. These constitute
    good generations signifying that in simple, small images, CVAEs can generate quality
    images. GANs and DDPMs will produce higher quality images with complex features,
    but CVAEs can handle simple cases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像的MSE为11，SSIM为0.76。这些构成了较好的生成结果，表明在简单、小型图像中，CVAEs可以生成高质量的图像。GANs和DDPMs将在具有复杂特征的图像中生成更高质量的图像，但CVAEs能够处理简单的情况。
- en: Long Text to Image using CLIP and COCO
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长文本到图像生成，使用CLIP和COCO
- en: When scaling up to image generation to text of any length, more robust methods
    would be needed besides regular expression matching. To do this, Open AI’s CLIP
    is used to convert text into a high dimensional embedding vector. The embedding
    model is used in its ViT-B/32 configuration, which outputs embeddings of length
    512\. A limitation of the CLIP model is that it has a maximum token length of
    77, with studies showing an even smaller effective length of 20 [5]. Thus, in
    instances where the input text contains multiple sentences, the text is split
    up by sentence and passed through the CLIP encoder. The resulting embeddings are
    averaged together to create the final output embedding.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当扩展到生成任何长度的文本图像时，除了常规的表达式匹配方法外，还需要更强大的方法。为此，使用了OpenAI的CLIP将文本转换为高维嵌入向量。该嵌入模型采用ViT-B/32配置，输出长度为512的嵌入向量。CLIP模型的一个限制是它的最大标记长度为77，研究表明有效长度甚至小于20
    [5]。因此，在输入文本包含多个句子的情况下，文本会按句子拆分并通过CLIP编码器处理。然后将得到的嵌入向量进行平均，以创建最终的输出嵌入。
- en: 'A long text model requires far more complicated training data than Fashion-MNIST,
    so COCO dataset was used. COCO dataset has annotations (that are not completely
    robust but that will be discussed later) that can be passed into CLIP to get embeddings.
    However, COCO images are of size 640x480, meaning that even with cropping transforms,
    a larger network is needed. Adding and concatenating conditional inputs architectures
    are both tested for long text to image generation, but the concatenating approach
    is shown here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本模型需要比Fashion-MNIST更复杂的训练数据，因此使用了COCO数据集。COCO数据集有标注（这些标注并不完全健全，但稍后会讨论），可以将这些标注传递给CLIP以获得嵌入向量。然而，COCO图像的大小为640x480，这意味着即使使用裁剪转换，也需要一个更大的网络。对于长文本到图像的生成，测试了添加和连接条件输入架构，但此处展示的是连接方法：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Another major point of investigation was image generation and reconstruction
    on images of different sizes. Specifically, modifying COCO images to be of size
    64x64, 128x128, and 256x256\. After training the network, reconstruction results
    should first be tested.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要的研究点是对不同大小图像的生成与重建。具体来说，将COCO图像修改为64x64、128x128和256x256的大小。训练网络后，应该首先测试重建结果。
- en: '![](../Images/b1c38dec6034d3615eacc75be851c892.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c38dec6034d3615eacc75be851c892.png)'
- en: 'CVAE reconstruction on COCO with different image sizes. Source: Author'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同图像尺寸对COCO进行CVAE重建。来源：作者
- en: All image sizes lead to reconstructed background with some feature outlines
    and correct colors. However, as image size increases, more features are able to
    be recovered. This makes sense as although it will take a lot longer to train
    a model with a larger image size, there is more information that can be captured
    and learned by the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像尺寸的重建结果都包含了背景的基本形状、某些特征的轮廓和正确的颜色。然而，随着图像尺寸的增大，更多的特征能够被恢复。这是有道理的，虽然训练一个大尺寸图像的模型需要更长时间，但可以捕捉到更多的信息并被模型学习。
- en: With image generation, it is extremely difficult to generate high quality images.
    Most images have backgrounds to some degree and blurred features in the image.
    This would be expected for image generation from a CVAE. This occurs in both concatenation
    and addition for the conditional input, but the concatenated approach performs
    better. This is likely because concatenated conditional inputs will not interfere
    with important features and ensures information is preserved distinctly. Conditions
    can be ignored if they are irrelevant. However, additive conditional inputs can
    interfere with existing features and completely mess up the network when updating
    weights during backpropagation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像生成中，生成高质量图像极其困难。大多数图像都有一定程度的背景和图像中的模糊特征。这是从CVAE生成图像时的预期情况。这种情况出现在条件输入的拼接和加法中，但拼接方法表现得更好。这可能是因为拼接的条件输入不会干扰重要特征，并确保信息能够清晰地保留下来。如果条件不相关，可以忽略。然而，加法条件输入可能会干扰现有特征，并在反向传播过程中完全破坏网络的权重更新。
- en: '![](../Images/fcd471faa0f0229c3e1e220ed0f71cae.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcd471faa0f0229c3e1e220ed0f71cae.png)'
- en: 'Generated images by CVAE on COCO. Source: Author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在COCO上通过CVAE生成的图像。来源：作者
- en: All of the COCO generated images have a far lower SSIM of about 0.4 compared
    to the SSIM on Fashion-MNIST. MSE is proportional to image size, so it is difficult
    to quanity differences. FID for COCO image generations are in the 200s for further
    proof that COCO CVAE generated images are not robust.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通过COCO生成的图像的SSIM值大约为0.4，远低于Fashion-MNIST上的SSIM值。MSE与图像大小成正比，因此很难量化差异。COCO图像生成的FID值在200左右，这进一步证明了COCO
    CVAE生成的图像不够鲁棒。
- en: Limitations of CVAEs for Image Generation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CVAEs在图像生成中的局限性
- en: The biggest limitation in trying to use CVAEs for image generation is, well,
    the CVAE. The amount of information that can be contained and reconstructed/generated
    is extremely dependent on the size of the latent space. A latent space that is
    too small won’t capture any meaningful information and is proportional to the
    size of the output image. A 28x28 image needs a far smaller latent space than
    a 64x64 image (as it proportionally squares from image size). However, a latent
    space bigger than the actual image adds unnecessary info and at that point just
    create a 1-to-1 mapping. For the COCO dataset, a latent space of at least 512
    is needed to capture some features. And while CVAEs are generative models, a convolutional
    encoder and decoder is a rather rudimentary network. The training style of a GAN
    or the complex denoising process of a DDPM allows for far more complicated image
    generation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CVAEs进行图像生成的最大限制是，嗯，就是CVAEs本身。所能包含和重建/生成的信息量极度依赖于潜在空间的大小。潜在空间太小无法捕获任何有意义的信息，且与输出图像的大小成正比。28x28的图像需要比64x64图像小得多的潜在空间（因为它按图像大小的平方比例变化）。然而，潜在空间比实际图像大则会增加不必要的信息，这时只是创建一个一对一的映射。对于COCO数据集，至少需要512维的潜在空间来捕获一些特征。虽然CVAEs是生成模型，但卷积编码器和解码器是相当基础的网络。GAN的训练方式或DDPM的复杂去噪过程则能生成更为复杂的图像。
- en: Another major limitation in image generation is the dataset trained on. Although
    the COCO dataset has annotations, the annotations are not extensively detailed.
    In order to train complex generative models, a different dataset should be used
    for training. COCO does not provide locations or excess information for background
    details. A complex feature vector from the CLIP encoder can’t be effectively utilized
    to a CVAE on COCO.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成中的另一个主要限制是所使用的训练数据集。尽管COCO数据集有注释，但这些注释并不十分详细。为了训练复杂的生成模型，应使用不同的数据集进行训练。COCO没有提供背景细节的位置信息或其他额外信息。来自CLIP编码器的复杂特征向量无法在COCO上有效地用于CVAE。
- en: Although CVAEs and image generation on COCO have their limitations, it creates
    a workable image generation model. More code and details can be provided just
    reach out!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CVAEs和在COCO上的图像生成存在局限性，但它创造了一个可行的图像生成模型。如果需要更多代码和细节，欢迎随时联系！
- en: References
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Kingma, Diederik P, et. al. “Auto-encoding variational bayes.” *arXiv:1312.6114*
    (2013).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Kingma, Diederik P, 等人。“自编码变分贝叶斯。”*arXiv:1312.6114*（2013年）。'
- en: '[2] Sohn, Kihyuk, et. al. “Learning Structured Output Representation using
    Deep Conditional Generative Models.” *NeurIPS Proceedings* (2015).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sohn, Kihyuk, 等人。“使用深度条件生成模型学习结构化输出表示。”*NeurIPS会议论文集*（2015年）。'
- en: '[3] Nilsson, J., et. al. “Understanding ssim.” *arXiv:2102.12037* (2020).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Nilsson, J., 等人。“理解SSIM。”*arXiv:2102.12037*（2020年）。'
- en: '[4] Xiao, Han, et. al. “Fashion-mnist: a novel image dataset for benchmarking
    machine learning algorithms.” *arXiv:2403.15378* (2024) (MIT license).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Xiao, Han 等. “Fashion-mnist: 用于基准测试机器学习算法的全新图像数据集。” *arXiv:2403.15378*
    （2024年）（MIT许可证）。'
- en: '[5] Zhang, B., et. al. “Long-clip: Unlocking the long-text capability of clip.”
    *arXiv:2403.15378* (2024).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Zhang, B. 等. “Long-clip: 解锁clip的长文本能力。” *arXiv:2403.15378* （2024年）。'
- en: A reference to my group project partners Jake Hession (Deloitte Consultant),
    Ashley Hong (Google SWE), and Julian Kuppel (Quant)!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我的小组项目伙伴：Jake Hession（德勤顾问）、Ashley Hong（谷歌软件工程师）和Julian Kuppel（量化分析师）！
