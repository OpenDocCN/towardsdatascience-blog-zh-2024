- en: Multi-Head Attention — Formally Explained and Defined
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11](https://towardsdatascience.com/multi-head-attention-formally-explained-and-defined-89dc70ce84bd?source=collection_archive---------9-----------------------#2024-06-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive and detailed formalisation of multi-head attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[![Jean
    Meunier-Pion](../Images/2d97f6d450ad143cbcb75a701204cc72.png)](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    [Jean Meunier-Pion](https://medium.com/@jmpion?source=post_page---byline--89dc70ce84bd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89dc70ce84bd--------------------------------)
    ·9 min read·Jun 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06dd5a838c54bd29e7e3bcf3512bced8.png)'
  prefs: []
  type: TYPE_IMG
- en: Robot with multiple heads, paying attention — Image by author (AI-generated,
    Microsoft Copilot)
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention plays a crucial role in transformers, which have revolutionized
    Natural Language Processing (NLP). Understanding this mechanism is a necessary
    step to getting a clearer picture of current state-of-the-art language models.
  prefs: []
  type: TYPE_NORMAL
- en: Even though this concept was introduced several years ago and has been widely
    used and discussed since then, ambiguous notations along with a lack of formal
    definitions have prevented newcomers from quickly demystifying the multi-head
    attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, the goal is to offer a comprehensive and unambiguous formalization
    of multi-head attention, to make this mechanism easily understandable.
  prefs: []
  type: TYPE_NORMAL
- en: Since, to better understand new concepts, it is crucial to use them by yourself,
    this article comes with several exercises/questions *(along with solutions)* to
    precisely get what happens with multi-head attention.
  prefs: []
  type: TYPE_NORMAL
- en: '**Disclaimer:** Before starting with the definitions and explanations of multi-head
    attention, note that the lack of LaTeX support forced me to turn the equations
    into images to display the different mathematical objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Input
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
