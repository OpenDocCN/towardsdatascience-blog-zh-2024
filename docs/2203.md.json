["```py\n# Import required libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\n\n# Create dataset from dictionary\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\ndf = pd.DataFrame(dataset_dict)\n\n# Prepare data: encode categorical variables\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\ndf['Play'] = (df['Play'] == 'Yes').astype(int)\n\n# Rearrange columns\ncolumn_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']\ndf = df[column_order]\n\n# Split data into features and target\nX, y = df.drop(columns='Play'), df['Play']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Scale numerical features\nscaler = StandardScaler()\nX_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])\nX_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])\n\n# Print results\nprint(\"Training set:\")\nprint(pd.concat([X_train, y_train], axis=1), '\\n')\nprint(\"Test set:\")\nprint(pd.concat([X_test, y_test], axis=1))\n```", "```py\n# Initialize weights (including bias) to 0.1\ninitial_weights = np.full(X_train_np.shape[1], 0.1)\n\n# Create and display DataFrame for initial weights\nprint(f\"Initial Weights: {initial_weights}\")\n```", "```py\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef calculate_probabilities(X, weights):\n    z = np.dot(X, weights)\n    return sigmoid(z)\n\ndef calculate_log_loss(probabilities, y):\n    return -y * np.log(probabilities) - (1 - y) * np.log(1 - probabilities)\n\ndef create_output_dataframe(X, y, weights):\n    probabilities = calculate_probabilities(X, weights)\n    log_losses = calculate_log_loss(probabilities, y)\n\n    df = pd.DataFrame({\n        'Probability': probabilities,\n        'Label': y,\n        'Log Loss': log_losses\n    })\n\n    return df\n\ndef calculate_average_log_loss(X, y, weights):\n    probabilities = calculate_probabilities(X, weights)\n    log_losses = calculate_log_loss(probabilities, y)\n    return np.mean(log_losses)\n\n# Convert X_train and y_train to numpy arrays for easier computation\nX_train_np = X_train.to_numpy()\ny_train_np = y_train.to_numpy()\n\n# Add a column of 1s to X_train_np for the bias term\nX_train_np = np.column_stack((np.ones(X_train_np.shape[0]), X_train_np))\n\n# Create and display DataFrame for initial weights\ninitial_df = create_output_dataframe(X_train_np, y_train_np, initial_weights)\nprint(initial_df.to_string(index=False, float_format=lambda x: f\"{x:.6f}\"))\nprint(f\"\\nAverage Log Loss: {calculate_average_log_loss(X_train_np, y_train_np, initial_weights):.6f}\")\n```", "```py\ndef gradient_descent_step(X, y, weights, learning_rate):\n    m = len(y)\n    probabilities = calculate_probabilities(X, weights)\n    gradient = np.dot(X.T, (probabilities - y)) / m\n    new_weights = weights - learning_rate * gradient  # Create new array for updated weights\n    return new_weights\n\n# Perform one step of gradient descent (one of the simplest optimization algorithm)\nlearning_rate = 0.1\nupdated_weights = gradient_descent_step(X_train_np, y_train_np, initial_weights, learning_rate)\n\n# Print initial and updated weights\nprint(\"\\nInitial weights:\")\nfor feature, weight in zip(['Bias'] + list(X_train.columns), initial_weights):\n    print(f\"{feature:11}: {weight:.2f}\")\n\nprint(\"\\nUpdated weights after one iteration:\")\nfor feature, weight in zip(['Bias'] + list(X_train.columns), updated_weights):\n    print(f\"{feature:11}: {weight:.2f}\")\n```", "```py\n# With sklearn, you can get the final weights (coefficients)\n# and final bias (intercepts) easily.\n# The result is almost the same as doing it manually above.\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(penalty=None, solver='saga')\nlr_clf.fit(X_train, y_train)\n\ncoefficients = lr_clf.coef_\nintercept = lr_clf.intercept_\n\ny_train_prob = lr_clf.predict_proba(X_train)[:, 1]\nloss = -np.mean(y_train * np.log(y_train_prob) + (1 - y_train) * np.log(1 - y_train_prob))\n\nprint(f\"Weights & Bias Final: {coefficients[0].round(2)}, {round(intercept[0],2)}\")\nprint(\"Loss Final:\", loss.round(3))\n```", "```py\n# Calculate prediction probability\npredicted_probs = lr_clf.predict_proba(X_test)[:, 1]\n\nz_values = np.log(predicted_probs / (1 - predicted_probs))\n\nresult_df = pd.DataFrame({\n    'ID': X_test.index,\n    'Z-Values': z_values.round(3),\n    'Probabilities': predicted_probs.round(3)\n}).set_index('ID')\n\nprint(result_df)\n\n# Make predictions\ny_pred = lr_clf.predict(X_test)\nprint(y_pred)\n```", "```py\nresult_df = pd.DataFrame({\n    'ID': X_test.index,\n    'Label': y_test,\n    'Probabilities': predicted_probs.round(2),\n    'Prediction': y_pred,\n}).set_index('ID')\n\nprint(result_df)\n```", "```py\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nregs = [None, 'l1', 'l2']\ncoeff_dict = {}\n\nfor reg in regs:\n    lr_clf = LogisticRegression(penalty=reg, solver='saga')\n    lr_clf.fit(X_train, y_train)\n    coefficients = lr_clf.coef_\n    intercept = lr_clf.intercept_\n    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]\n    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))\n    predictions = lr_clf.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n\n    coeff_dict[reg] = {\n        'Coefficients': coefficients,\n        'Intercept': intercept,\n        'Loss': loss,\n        'Accuracy': accuracy\n    }\n\nfor reg, vals in coeff_dict.items():\n    print(f\"{reg}: Coeff: {vals['Coefficients'][0].round(2)}, Intercept: {vals['Intercept'].round(2)}, Loss: {vals['Loss'].round(3)}, Accuracy: {vals['Accuracy'].round(3)}\")\n```", "```py\n# List of regularization strengths to try for L1\nstrengths = [0.001, 0.01, 0.1, 1, 10, 100]\n\ncoeff_dict = {}\n\nfor strength in strengths:\n    lr_clf = LogisticRegression(penalty='l1', C=strength, solver='saga')\n    lr_clf.fit(X_train, y_train)\n\n    coefficients = lr_clf.coef_\n    intercept = lr_clf.intercept_\n\n    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]\n    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))\n    predictions = lr_clf.predict(X_test)\n\n    accuracy = accuracy_score(y_test, predictions)\n\n    coeff_dict[f'L1_{strength}'] = {\n        'Coefficients': coefficients[0].round(2),\n        'Intercept': round(intercept[0],2),\n        'Loss': round(loss,3),\n        'Accuracy': round(accuracy*100,2)\n    }\n\nprint(pd.DataFrame(coeff_dict).T)\n```", "```py\n# List of regularization strengths to try for L2\nstrengths = [0.001, 0.01, 0.1, 1, 10, 100]\n\ncoeff_dict = {}\n\nfor strength in strengths:\n    lr_clf = LogisticRegression(penalty='l2', C=strength, solver='saga')\n    lr_clf.fit(X_train, y_train)\n\n    coefficients = lr_clf.coef_\n    intercept = lr_clf.intercept_\n\n    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]\n    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))\n    predictions = lr_clf.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n\n    coeff_dict[f'L2_{strength}'] = {\n        'Coefficients': coefficients[0].round(2),\n        'Intercept': round(intercept[0],2),\n        'Loss': round(loss,3),\n        'Accuracy': round(accuracy*100,2)\n    }\n\nprint(pd.DataFrame(coeff_dict).T)\n```", "```py\n# Import required libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\n# Load the dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']\n}\ndf = pd.DataFrame(dataset_dict)\n\n# Prepare data: encode categorical variables\ndf = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\ndf['Play'] = (df['Play'] == 'Yes').astype(int)\n\n# Split data into training and testing sets\nX, y = df.drop(columns='Play'), df['Play']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Scale numerical features\nscaler = StandardScaler()\nfloat_cols = X_train.select_dtypes(include=['float64']).columns\nX_train[float_cols] = scaler.fit_transform(X_train[float_cols])\nX_test[float_cols] = scaler.transform(X_test[float_cols])\n\n# Train the model\nlr_clf = LogisticRegression(penalty='l2', C=1, solver='saga')\nlr_clf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = lr_clf.predict(X_test)\n\n# Evaluate the model\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n```"]