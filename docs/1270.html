<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>PyTorch Native FP8 Data Types</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>PyTorch Native FP8 Data Types</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-native-fp8-fedc06f1c9f7?source=collection_archive---------2-----------------------#2024-05-21">https://towardsdatascience.com/pytorch-native-fp8-fedc06f1c9f7?source=collection_archive---------2-----------------------#2024-05-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6ca2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Accelerating PyTorch Training Workloads with FP8 — Part 2</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--fedc06f1c9f7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fedc06f1c9f7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--fedc06f1c9f7--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fedc06f1c9f7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/0ae1023a0975006e7a589c7b4963d87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Az34i3-79te2Mctz"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@alexandrelion?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alex Lion</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6b78" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the presence of AI-based applications becomes more and more ubiquitous in our daily lives, the challenge of optimizing their runtime performance increases. Reducing the number of bits that are used to represent floating-point types is a common technique that can accelerate AI applications and reduce their memory footprint. And indeed, many modern-day AI hardware accelerators include dedicated support for 8-bit floating point representations. In a <a class="af nb" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a>, we discussed the potential (and risks) of training with FP8 and demonstrated it in practice on an H100-based training instance using PyTorch and <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine" rel="noopener ugc nofollow" target="_blank">Transformer Engine</a> (TE), a dedicated library for accelerating Transformer models on NVIDIA GPUs. Naturally, it was only a matter of time until PyTorch introduced native support for FP8 data types. In this post we will review the current capabilities and demonstrate their use on another FP8-supporting AI chip, the <a class="af nb" href="https://www.nvidia.com/en-us/data-center/l4/" rel="noopener ugc nofollow" target="_blank">NVIDIA L4 GPU</a>. More specifically, we will run our experiments on a Google Cloud <a class="af nb" href="https://cloud.google.com/compute/docs/gpus#l4-gpus" rel="noopener ugc nofollow" target="_blank">g2-standard-16</a> VM (with a single L4 GPU), a dedicated <a class="af nb" href="https://cloud.google.com/deep-learning-vm/docs/release-notes" rel="noopener ugc nofollow" target="_blank">deep learning VM image</a>, and PyTorch 2.3.0.</p><p id="c39a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Importantly, as of the time of this writing the PyTorch-native FP8 support is <em class="ny">highly</em> experimental. Its use is <em class="ny">not</em> recommended for the faint-of-heart or <em class="ny">fault-intolerant</em>. This post is intended primarily for early adopters — anybody who (like us) is obsessed with AI model performance optimization and the potential goodness of this new technology. Keep in mind that the APIs we refer may undergo revision by the time you read this post.</p><p id="a909" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our focus will be on the potential impact that using FP8 can have on the runtime performance of AI applications. To learn about the algorithmic implications, we refer the reader to dedicated tutorials on the topic (such as <a class="af nb" href="https://arxiv.org/pdf/2209.05433" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af nb" href="https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/" rel="noopener ugc nofollow" target="_blank">here</a>).</p><p id="5c9b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Many thanks to <a class="af nb" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a> for his contributions to this post.</p><h1 id="76ab" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">PyTorch Native Float8 Types</h1><p id="0619" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">As of version 2.2, PyTorch includes “<a class="af nb" href="https://pytorch.org/docs/stable/tensors.html#id13" rel="noopener ugc nofollow" target="_blank">limited support</a>” for the <code class="cx pa pb pc pd b">torch.float8_e4m3fn</code> and <code class="cx pa pb pc pd b">torch.float8_e5m2</code> data types (with 3 and 2 mantissa bits, respectively) both of which are implementations of types specified in the <a class="af nb" href="https://arxiv.org/pdf/2209.05433" rel="noopener ugc nofollow" target="_blank">FP8 Formats for Deep Learning</a> paper. In the snippet of code below we display the properties and dynamic range of the new types compared to the legacy floating bit types:</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="e6c8" class="ph oa fq pd b bg pi pj l pk pl">import torch<br/>from tabulate import tabulate<br/> <br/>f32_type = torch.float32<br/>bf16_type = torch.bfloat16<br/>e4m3_type = torch.float8_e4m3fn<br/>e5m2_type = torch.float8_e5m2<br/><br/># collect finfo for each type<br/>table = []<br/>for dtype in [f32_type, bf16_type, e4m3_type, e5m2_type]:<br/>    numbits = 32 if dtype == f32_type else 16 if dtype == bf16_type else 8<br/>    info = torch.finfo(dtype)<br/>    table.append([info.dtype, numbits, info.max, <br/>                  info.min, info.smallest_normal, info.eps])<br/><br/>headers = ['data type', 'bits', 'max', 'min', 'smallest normal', 'eps']<br/>print(tabulate(table, headers=headers))<br/> <br/>'''<br/>Output:<br/><br/>data type      bits          max           min  smallest normal          eps<br/>-------------  ----  -----------  ------------  ---------------  -----------<br/>float32          32  3.40282e+38  -3.40282e+38      1.17549e-38  1.19209e-07<br/>bfloat16         16  3.38953e+38  -3.38953e+38      1.17549e-38    0.0078125<br/>float8_e4m3fn     8          448          -448         0.015625        0.125<br/>float8_e5m2       8        57344        -57344      6.10352e-05         0.25<br/>'''</span></pre><p id="96e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can create FP8 tensors by specifying the <em class="ny">dtype</em> in the tensor initialization function as demonstrated below:</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="082b" class="ph oa fq pd b bg pi pj l pk pl">device="cuda"<br/>e4m3 = torch.tensor(1., device=device, dtype=e4m3_type)<br/>e5m2 = torch.tensor(1., device=device, dtype=e5m2_type)</span></pre><p id="e776" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also cast legacy types to FP8. In the code block below we generate a random tensor of floats and compare the results of casting them into four different floating-point types:</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="592f" class="ph oa fq pd b bg pi pj l pk pl">x = torch.randn(2, 2, device=device, dtype=f32_type)<br/>x_bf16 = x.to(bf16_type)<br/>x_e4m3 = x.to(e4m3_type)<br/>x_e5m2 = x.to(e5m2_type)<br/><br/>print(tabulate([[‘float32’, *x.cpu().flatten().tolist()],<br/>                [‘bfloat16’, *x_bf16.cpu().flatten().tolist()],<br/>                [‘float8_e4m3fn’, *x_e4m3.cpu().flatten().tolist()],<br/>                [‘float8_e5m2’, *x_e5m2.cpu().flatten().tolist()]],<br/>               headers=[‘data type’, ‘x1’, ‘x2’, ‘x3’, ‘x4’]))<br/><br/>'''<br/>The sample output demonstrates the dynamic range of the different types:<br/><br/>data type                  x1              x2              x3              x4<br/>-------------  --------------  --------------  --------------  --------------<br/>float32        2.073093891143  -0.78251332044  -0.47084918620  -1.32557279110<br/>bfloat16       2.078125        -0.78125        -0.4707031      -1.328125<br/>float8_e4m3fn  2.0             -0.8125         -0.46875        -1.375<br/>float8_e5m2    2.0             -0.75           -0.5            -1.25<br/>-------------  --------------  --------------  --------------  --------------<br/>'''</span></pre><p id="2eff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although creating FP8 tensors is easy enough, you may quickly find that performing some basic arithmetic operations on FP8 tensors is not supported (in PyTorch 2.3.0, as of the time of this writing). The one (arguably most important) exception is FP8 matrix multiplication, which is supported via the dedicated torch._scaled_mm function. Demonstrated in the code block below, this function receives two FP8 tensors (of identical type) and their associated scaling factors, as well as an optional bias tensor:</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="1e94" class="ph oa fq pd b bg pi pj l pk pl">output, output_amax = torch._scaled_mm(<br/>        torch.randn(16,16, device=device).to(e4m3_type),<br/>        torch.randn(16,16, device=device).to(e4m3_type).t(),<br/>        bias=torch.randn(16, device=device).to(bf16_type),<br/>        out_dtype=e4m3_type,<br/>        scale_a=torch.tensor(1.0, device=device),<br/>        scale_b=torch.tensor(1.0, device=device)<br/>    )</span></pre><p id="8fde" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To get a better feel for the current API capabilities and usage modes, you can take a look at the <a class="af nb" href="https://github.com/pytorch/pytorch/blob/v2.3.0/test/test_matmul_cuda.py" rel="noopener ugc nofollow" target="_blank">API test script</a> in the PyTorch repository.</p><p id="7dc6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Contrary to the FP8 support in the <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine" rel="noopener ugc nofollow" target="_blank">Transformer Engine</a> library that we demonstrated in our <a class="af nb" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a>, the PyTorch natives enable the explicit definition and use of FP8 data types. This provides advanced developers with much greater flexibility in designing and implementing custom FP8 algorithms. However, as discussed in our previous post, successful FP8 ML model training often requires some creative acrobatics; many users will desire a high-level API that automatically applies battle-tested scaling and type conversion schemes to their existing AI model training algorithms. While not (as of the time of this writing) part of the official PyTorch library, such functionality is offered via the <a class="af nb" href="https://github.com/pytorch-labs/float8_experimental/tree/cb55df259cfb22a856ca92107a778343edea5fc7" rel="noopener ugc nofollow" target="_blank">float8_experimental library</a>.</p><h1 id="310b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Training with in Native PyTorch with FP8</h1><p id="9b4d" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">In this section, we will demonstrate the use of the <a class="af nb" href="https://github.com/pytorch-labs/float8_experimental/tree/cb55df259cfb22a856ca92107a778343edea5fc7" rel="noopener ugc nofollow" target="_blank">float8_experimental library</a> on a simple <a class="af nb" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT-Huge) backed classification model with 632 million parameters (using version 1.0.3 of the popular <a class="af nb" href="https://pypi.org/project/timm/" rel="noopener ugc nofollow" target="_blank">timm</a> Python package). Please see the documentation for <a class="af nb" href="https://github.com/pytorch-labs/float8_experimental/tree/cb55df259cfb22a856ca92107a778343edea5fc7?tab=readme-ov-file#installation" rel="noopener ugc nofollow" target="_blank">instructions</a> on installing the <a class="af nb" href="https://github.com/pytorch-labs/float8_experimental/tree/cb55df259cfb22a856ca92107a778343edea5fc7" rel="noopener ugc nofollow" target="_blank">float8_experimental library</a>. We set the ViT backbone to use <em class="ny">average global pooling </em>to avoid some kinks in the current offering (e.g., see <a class="af nb" href="https://github.com/pytorch/pytorch/issues/123761" rel="noopener ugc nofollow" target="_blank">here</a>). In the code block below, we demonstrate FP8 training with the <a class="af nb" href="https://github.com/pytorch-labs/float8_experimental/tree/cb55df259cfb22a856ca92107a778343edea5fc7?tab=readme-ov-file#float8-linear-with-delayed-scaling" rel="noopener ugc nofollow" target="_blank">delayed scaling strategy</a> on a randomly generated dataset. We include controls for toggling the floating point type, using <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> mode, and setting the batch size.</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="e794" class="ph oa fq pd b bg pi pj l pk pl">import torch<br/>from timm.models.vision_transformer import VisionTransformer<br/>from torch.utils.data import Dataset, DataLoader<br/>import os<br/>import time<br/><br/>#float8 imports<br/>from float8_experimental import config<br/>from float8_experimental.float8_linear import Float8Linear<br/>from float8_experimental.float8_linear_utils import (<br/>    swap_linear_with_float8_linear,<br/>    sync_float8_amax_and_scale_history<br/>)<br/><br/>#float8 configuration (see documentation)<br/>config.enable_amax_init = False<br/>config.enable_pre_and_post_forward = False<br/><br/># model configuration controls:<br/>fp8_type = True # toggle to change floating-point precision<br/>compile_model = True # toggle to enable model compilation<br/>batch_size = 32 if fp8_type else 16 # control batch size<br/> <br/>device = torch.device('cuda')<br/> <br/># use random data<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, 256, 256], dtype=torch.float32)<br/>        label = torch.tensor(data=[index % 1024], dtype=torch.int64)<br/>        return rand_image, label<br/><br/># get data loader<br/>def get_data(batch_size):<br/>    ds = FakeDataset()<br/>    return DataLoader(<br/>           ds,<br/>           batch_size=batch_size, <br/>           num_workers=os.cpu_count(),<br/>           pin_memory=True<br/>         )<br/><br/># define the timm model<br/>def get_model():<br/>    model = VisionTransformer(<br/>        class_token=False,<br/>        global_pool="avg",<br/>        img_size=256,<br/>        embed_dim=1280,<br/>        num_classes=1024,<br/>        depth=32,<br/>        num_heads=16<br/>    )<br/>    if fp8_type:<br/>        swap_linear_with_float8_linear(model, Float8Linear)<br/>    return model<br/><br/># define the training step<br/>def train_step(inputs, label, model, optimizer, criterion):<br/>    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):<br/>        outputs = model(inputs)<br/>        loss = criterion(outputs, label)<br/>    optimizer.zero_grad(set_to_none=True)<br/>    loss.backward()<br/>    if fp8_type:<br/>        sync_float8_amax_and_scale_history(model)<br/>    optimizer.step()<br/><br/><br/>model = get_model()<br/>optimizer = torch.optim.Adam(model.parameters())<br/>criterion = torch.nn.CrossEntropyLoss()<br/>train_loader = get_data(batch_size)<br/> <br/># copy the model to the GPU<br/>model = model.to(device)<br/>if compile_model:<br/>    # compile model<br/>    model = torch.compile(model)<br/>model.train()<br/> <br/>t0 = time.perf_counter()<br/>summ = 0<br/>count = 0<br/> <br/>for step, data in enumerate(train_loader):<br/>    # copy data to GPU<br/>    inputs = data[0].to(device=device, non_blocking=True)<br/>    label = data[1].squeeze(-1).to(device=device, non_blocking=True)<br/> <br/>    # train step<br/>    train_step(inputs, label, model, optimizer, criterion)<br/> <br/>    # capture step time<br/>    batch_time = time.perf_counter() - t0<br/>    if step &gt; 10:  # skip first steps<br/>        summ += batch_time<br/>        count += 1<br/>    t0 = time.perf_counter()<br/>    if step &gt; 50:<br/>        break<br/><br/>print(f'average step time: {summ / count}')</span></pre><p id="fb14" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The first thing we note is that the use of the lower precision data type frees up GPU memory which enables us to double the batch size. The table below summarizes the performance results (as measured by the average step time) when training with a variety of configuration settings. As suggested in the documentation, the <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> FP8 experiment was run using a nightly version of PyTorch (specifically version torch-2.4.0.dev20240520+cu121).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pm"><img src="../Images/6914ffc764ae0e4cf074631ed38b15d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlHxX-RrSaB4_Z69yH2alw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Experiment Results (By Author)</figcaption></figure><p id="dbb2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the results demonstrate, the use of FP8 linear layers increases the performance of our toy model by 47%(!!) over our baseline experiment, but <em class="ny">only</em> when it is combined with the use of <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a>. Naturally, the results will vary based on the definition and size of the model.</p><h1 id="3141" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Comparison to Transformer Engine</h1><p id="b5d1" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">For the sake of comparison, we implement the same training sequence using the <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine" rel="noopener ugc nofollow" target="_blank">Transformer Engine (TE)</a> library (version 1.6). Although TE includes its own optimized <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine/blob/67bc399d7ba7e49bf540746c1ef6a7e43eaed8f7/transformer_engine/pytorch/transformer.py#L70" rel="noopener ugc nofollow" target="_blank">TransformerLayer</a> (as demonstrated in our <a class="af nb" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a>), we manually overwrite the <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear" rel="noopener ugc nofollow" target="_blank">torch.nn.Linear</a> layer with the <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine/blob/release_v0.12/transformer_engine/pytorch/module/linear.py#L442" rel="noopener ugc nofollow" target="_blank">TE Linear</a> layer in order to limit our comparative evaluation to just the FP8 linear support. In the code block below, we implement a simple linear layer swapping utility (use at your own risk!!) and apply it to our ViT model. We also include the training step function required for FP8 training using TE:</p><pre class="ml mm mn mo mp pe pd pf bp pg bb bk"><span id="3e82" class="ph oa fq pd b bg pi pj l pk pl">import transformer_engine.pytorch as te<br/><br/># swap all linear layers with te.Linear<br/>def simple_swap(model):<br/>    for submodule_name, submodule in model.named_modules():<br/>        if isinstance(submodule, torch.nn.Linear):<br/>            print(submodule_name)<br/>            path_in_state_dict = submodule_name.split('.')<br/>            current_module = model<br/>            <br/>            # traverse to leaf module<br/>            leaf_path = path_in_state_dict[:-1]<br/>            leaf_name = path_in_state_dict[-1]<br/>            for child_name in leaf_path:<br/>                current_module = getattr(current_module, child_name)<br/>            <br/>            # perform a swap<br/>            old_leaf = getattr(current_module, leaf_name)<br/>            new_leaf = te.Linear(old_leaf.in_features, <br/>                                 old_leaf.out_features, <br/>                                 old_leaf.bias is not None)<br/>            setattr(current_module, leaf_name, new_leaf)<br/><br/>def get_model():<br/>    model = VisionTransformer(<br/>        class_token=False,<br/>        global_pool="avg",<br/>        img_size=256,<br/>        embed_dim=1280,<br/>        num_classes=1024,<br/>        depth=32,<br/>        num_heads=16<br/>    )<br/>    simple_swap(model)<br/>    return model<br/> <br/> <br/>def train_step(inputs, label, model, optimizer, criterion):<br/>    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):<br/>        with te.fp8_autocast(enabled=True):<br/>            outputs = model(inputs)<br/>        loss = criterion(outputs, label)<br/>    optimizer.zero_grad(set_to_none=True)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="9ca1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results of the TE experiments are captured below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pm"><img src="../Images/d76a672e9da89ed29140d37f80038e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDfqIIwT9OiVEG7jeEApvg.png"/></div></div></figure><p id="bc25" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the uncompiled TE FP8 model performs significantly better than our previous FP8 model, the compiled PyTorch FP8 model still provides the best results. Importantly, as of the time of this writing, TE FP8 modules do not support <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">model compilation</a>. Thus, applying <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> will result in “partial compilation”, i.e. it will include multiple graph breaks (every time FP8 is used).</p><p id="d998" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We intentionally limited our tests to just the linear layers of our toy model. Unsurprisingly, applying the full power of TE to our model, as demonstrated in our <a class="af nb" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a>, would have resulted in a 72% boost (compared to our baseline experiment).</p><p id="9d57" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a more detailed comparison between the TE and PyTorch-native FP8 operators, covering a wide range of matrix sizes, we recommend following <a class="af nb" href="https://github.com/pytorch/pytorch/issues/123761" rel="noopener ugc nofollow" target="_blank">this github issue</a>.</p><h1 id="6185" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusions</h1><p id="d4c9" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Although still in its early days with clear room for improvement both in terms of API coverage and performance, we have succeeded in demonstrating some of the potential advantages of the PyTorch native FP8 support. First, the ability to explicitly declare and operate on FP8 tensors will enable developers much greater freedom in customizing FP8-based algorithms. Second, the built-in support for JIT-compilation facilitates greater potential for runtime optimization. A third advantage (not demonstrated here) is the ability to support a greater range of FP8-supporting devices. This is contrary to TE which is developed by NVIDIA and heavily tailored to their GPUs.</p><h1 id="d8f0" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Summary</h1><p id="8f6d" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The ever-increasing size of AI models necessitates advanced techniques and algorithms for both reducing memory footprint and boosting runtime performance. Using the FP8 data type on dedicated HW accelerators offers the ability to achieve both. Although our focus has been on model training, the implications are no less important on model inference, where the time that it takes to load a large model into memory and run it, can have a decisive impact on a user’s experience.</p><p id="4ef9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The newly defined PyTorch-native FP8 data types and operators that we experimented with in this post, are certain to facilitate and accelerate the adoption of this important technology. We look forward to seeing how this native support evolves and matures.</p><p id="ef5b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For more tools and techniques for AI model optimization, be sure to check out some of our <a class="af nb" href="https://chaimrand.medium.com/" rel="noopener">other posts</a>.</p></div></div></div></div>    
</body>
</html>