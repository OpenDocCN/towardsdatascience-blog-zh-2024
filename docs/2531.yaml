- en: Fine-Tuning BERT for Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A hackable example with Python code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    ·6 min read·Oct 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Although today’s 100B+ parameter transformer models are state-of-the-art in
    AI, there’s still much we can accomplish with smaller (< 1B parameter) models.
    In this article, I will walk through one such example, fine-tuning BERT (110M
    parameters) to classify phishing URLs. I’ll start by covering key concepts and
    then share example Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c8e635ce4bf650d83e3b25350eac527.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Canva.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**Fine-tuning**](/fine-tuning-large-language-models-llms-23473d763b91) **involves**
    **adapting a pre-trained model to a particular use case through additional training**.'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained models are developed via unsupervised learning, which precludes
    the need for large-scale labeled datasets. Fine-tuned models can then exploit
    pre-trained model representations to significantly **reduce training costs** and
    **improve model performance** compared to training from scratch [1].
  prefs: []
  type: TYPE_NORMAL
