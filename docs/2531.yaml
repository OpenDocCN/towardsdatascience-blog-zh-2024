- en: Fine-Tuning BERT for Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调BERT进行文本分类
- en: 原文：[https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-a01f89b179fc?source=collection_archive---------1-----------------------#2024-10-17)
- en: A hackable example with Python code
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个可以修改的示例，带有Python代码
- en: '[](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--a01f89b179fc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    ·6 min read·Oct 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a01f89b179fc--------------------------------)
    ·6分钟阅读·2024年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Although today’s 100B+ parameter transformer models are state-of-the-art in
    AI, there’s still much we can accomplish with smaller (< 1B parameter) models.
    In this article, I will walk through one such example, fine-tuning BERT (110M
    parameters) to classify phishing URLs. I’ll start by covering key concepts and
    then share example Python code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天的100B+参数变换器模型在人工智能领域处于最前沿，但我们仍然可以通过较小的（<1B参数）模型取得很大进展。在本文中，我将通过一个例子，演示如何微调BERT（110M参数）来分类钓鱼URL。我将首先介绍一些关键概念，然后分享示例Python代码。
- en: '![](../Images/7c8e635ce4bf650d83e3b25350eac527.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8e635ce4bf650d83e3b25350eac527.png)'
- en: Image from Canva.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Canva。
- en: '**Fine-tuning**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**微调**'
- en: '[**Fine-tuning**](/fine-tuning-large-language-models-llms-23473d763b91) **involves**
    **adapting a pre-trained model to a particular use case through additional training**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[**微调**](/fine-tuning-large-language-models-llms-23473d763b91) **涉及** **通过额外的训练将预训练模型调整到特定的使用案例**。'
- en: Pre-trained models are developed via unsupervised learning, which precludes
    the need for large-scale labeled datasets. Fine-tuned models can then exploit
    pre-trained model representations to significantly **reduce training costs** and
    **improve model performance** compared to training from scratch [1].
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型通过无监督学习开发，这避免了对大规模标签数据集的需求。与从头开始训练相比，微调后的模型可以利用预训练模型的表示，显著**降低训练成本**并**提高模型性能**[1]。
