<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05">https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7785" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Learn how to build Llama 3.2-Vision locally in a chat-like mode, and explore its Multimodal skills on a Colab notebook</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lihi Gur Arie, PhD" class="l ep by dd de cx" src="../Images/7a1eb30725a95159401c3672fa5f43ab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M0YTyQAxsWWqI8MLBi2xrA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------" rel="noopener follow">Lihi Gur Arie, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ea91d922aefd2e7597a663d07051b804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDZ0saKEG_WeDVLCbF5waw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Annotated image by author. Original image by <a class="af nc" href="https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><h1 id="2905" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="1607" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The integration of vision capabilities with Large Language Models (LLMs) is revolutionizing the computer vision field through multimodal LLMs (MLLM). These models combine text and visual inputs, showing impressive abilities in image understanding and reasoning. While these models were previously accessible only via APIs, recent open source options now allow for local execution, making them more appealing for production environments.</p><p id="0e4f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this tutorial, we will learn how to chat with our images using the open source Llama 3.2-Vision model, and you’ll be amazed by its OCR, image understanding, and reasoning capabilities. All the code is conveniently provided in a handy Colab notebook.</p><p id="64c1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="pa">If you don’t have a paid Medium account, you can read for free </em><a class="af nc" rel="noopener" target="_blank" href="/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675"><em class="pa">here</em></a><em class="pa">.</em></p><h1 id="e966" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk"><strong class="al">Llama 3.2-Vision</strong></h1><p id="7d6c" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Background</strong></p><p id="fc4b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Llama, short for “Large Language Model Meta AI” is a series of advanced LLMs developed by Meta. Their latest, Llama 3.2, was introduced with advanced vision capabilities. The vision variant comes in two sizes: 11B and 90B parameters, enabling inference on edge devices. With a context window of up to 128k tokens and support for high resolution images up to 1120x1120 pixels, Llama 3.2 can process complex visual and textual information.</p><p id="c7a6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Architecture</strong></p><p id="965f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The Llama series of models are decoder-only Transformers. Llama 3.2-Vision is built on top of a pre-trained Llama 3.1 text-only model. It utilizes a standard, dense auto-regressive Transformer architecture, that does not deviate significantly from its predecessors, Llama and Llama 2.</p><p id="d3d3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To support visual tasks, Llama 3.2 extracts image representation vectors using a pre-trained vision encoder (ViT-H/14), and integrates these representations into the frozen language model using a vision adapter. The adapter consists of a series of cross-attention layers that allow the model to focus on specific parts of the image that correspond to the text being processed [1].</p><p id="d4e1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The adapter is trained on text-image pairs to align image representations with language representations. During adapter training, the parameters of the image encoder are updated, while the language model parameters remain frozen to preserve existing language capabilities.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xqoPM6-ltd-t_O0v0eyXg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Llama 3.2-Vision architecture. The vision module (green) is integrated into the fixed language model (pink). Image was created by author.</figcaption></figure><p id="eae1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This design allows Llama 3.2 to excel in multimodal tasks while maintaining its strong text-only performance. The resulting model demonstrates impressive capabilities in tasks that require both image and language understanding, and allowing users to interactively communicate with their visual inputs.</p><h1 id="05b5" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Lets code!</h1><p id="80ba" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">With our understanding of Llama 3.2’s architecture in place, we can dive into the practical implementation. But first, we need do some preparations.</p><h2 id="4399" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk"><strong class="al">Preparations</strong></h2><p id="f73e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Before running Llama 3.2 — Vision 11B on Google Colab, we need to make some preparations:</p><ol class=""><li id="29e4" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pt pu pv bk">GPU setup:</li></ol><ul class=""><li id="eacc" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw pu pv bk">A high-end GPU with at least 22GB VRAM is recommended for efficient inference [2].</li><li id="56ac" class="nz oa fq ob b go px od oe gr py og oh oi pz ok ol om qa oo op oq qb os ot ou pw pu pv bk">For Google Colab users: Navigate to ‘Runtime’ &gt; ‘Change runtime type’ &gt; ‘A100 GPU’. Note that high-end GPU’s may not be available for free Colab users.</li></ul><p id="2aa0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">2. Model Permissions:</p><ul class=""><li id="9971" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw pu pv bk">Request Access to Llama 3.2 Models <a class="af nc" href="https://www.llama.com/llama-downloads/" rel="noopener ugc nofollow" target="_blank">here</a>.</li></ul><p id="ac97" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">3. Hugging Face Setup:</p><ul class=""><li id="1b6a" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw pu pv bk">Create a Hugging Face account if you don’t have on already <a class="af nc" href="https://huggingface.co/join" rel="noopener ugc nofollow" target="_blank">here</a>.</li><li id="9e41" class="nz oa fq ob b go px od oe gr py og oh oi pz ok ol om qa oo op oq qb os ot ou pw pu pv bk">Generate an access token from your Hugging Face account if you don’t have one, <a class="af nc" href="https://huggingface.co/settings/tokens" rel="noopener ugc nofollow" target="_blank">here</a>.</li><li id="43e2" class="nz oa fq ob b go px od oe gr py og oh oi pz ok ol om qa oo op oq qb os ot ou pw pu pv bk">For Google Colab users, set up the Hugging Face token as a secret environmental variable named ‘HF_TOKEN’ in google Colab Secrets.</li></ul><p id="ae24" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">4. Install the required libraries.</p><p id="3e69" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Loading The</strong> <strong class="ob fr">Model</strong></p><p id="7601" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Once we’ve set up the environment and acquired the necessary permissions, we will use the Hugging Face Transformers library to instantiate the model and its associated processor. The processor is responsible for preparing inputs for the model and formatting its outputs.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="98c9" class="qg ne fq qd b bg qh qi l qj qk">model_id = "meta-llama/Llama-3.2-11B-Vision-Instruct"<br/><br/>model = MllamaForConditionalGeneration.from_pretrained(<br/>    model_id,<br/>    torch_dtype=torch.bfloat16,<br/>    device_map="auto")<br/><br/>processor = AutoProcessor.from_pretrained(model_id)</span></pre><p id="97bd" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Expected Chat Template</strong></p><p id="991d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Chat templates maintain context through conversation history by storing exchanges between the “user” (us) and the “assistant” (the AI model). The conversation history is structured as a list of dictionaries called <code class="cx ql qm qn qd b">messages</code>, where each dictionary represents a single conversational turn, including both user and model responses. User turns can include image-text or text-only inputs, with <code class="cx ql qm qn qd b">{"type": "image"}</code> indicating an image input.</p><p id="22b3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For example, after a few chat iterations, the <code class="cx ql qm qn qd b">messages</code> list might look like this:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="9698" class="qg ne fq qd b bg qh qi l qj qk">messages = [<br/>    {"role": "user",      "content": [{"type": "image"}, {"type": "text", "text": prompt1}]},<br/>    {"role": "assistant", "content": [{"type": "text", "text": generated_texts1}]},<br/>    {"role": "user",      "content": [{"type": "text", "text": prompt2}]},<br/>    {"role": "assistant", "content": [{"type": "text", "text": generated_texts2}]},<br/>    {"role": "user",      "content": [{"type": "text", "text": prompt3}]},<br/>    {"role": "assistant", "content": [{"type": "text", "text": generated_texts3}]}<br/>]</span></pre><p id="dbd8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This list of messages is later passed to the <code class="cx ql qm qn qd b">apply_chat_template()</code> method to convert the conversation into a single tokenizable string in the format that the model expects.</p><p id="4d00" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Main function</strong></p><p id="a435" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For this tutorial I provided a <code class="cx ql qm qn qd b">chat_with_mllm</code> function that enables dynamic conversation with the Llama 3.2 MLLM. This function handles image loading, pre-processes both images and the text inputs, generates model responses, and manages the conversation history to enable chat-mode interactions.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="6103" class="qg ne fq qd b bg qh qi l qj qk">def chat_with_mllm (model, processor, prompt, images_path=[],do_sample=False, temperature=0.1, show_image=False, max_new_tokens=512, messages=[], images=[]):<br/><br/>    # Ensure list:<br/>    if not isinstance(images_path, list):<br/>        images_path =  [images_path]<br/><br/>    # Load images <br/>    if len (images)==0 and len (images_path)&gt;0:<br/>            for image_path in tqdm (images_path):<br/>                image = load_image(image_path)<br/>                images.append (image)<br/>                if show_image:<br/>                    display ( image )<br/><br/>    # If starting a new conversation about an image<br/>    if len (messages)==0:<br/>        messages = [{"role": "user", "content": [{"type": "image"}, {"type": "text", "text": prompt}]}]<br/><br/>    # If continuing conversation on the image<br/>    else:<br/>        messages.append ({"role": "user", "content": [{"type": "text", "text": prompt}]})<br/><br/>    # process input data<br/>    text = processor.apply_chat_template(messages, add_generation_prompt=True)<br/>    inputs = processor(images=images, text=text, return_tensors="pt", ).to(model.device)<br/><br/>    # Generate response<br/>    generation_args = {"max_new_tokens": max_new_tokens, "do_sample": True}<br/>    if do_sample:<br/>        generation_args["temperature"] = temperature<br/>    generate_ids = model.generate(**inputs,**generation_args)<br/>    generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:-1]<br/>    generated_texts = processor.decode(generate_ids[0], clean_up_tokenization_spaces=False)<br/><br/>    # Append the model's response to the conversation history<br/>    messages.append ({"role": "assistant", "content": [  {"type": "text", "text": generated_texts}]})<br/><br/>    return generated_texts, messages, images</span></pre><h2 id="9b97" class="pc ne fq bf nf pd pe pf ni pg ph pi nl oi pj pk pl om pm pn po oq pp pq pr ps bk">Chat with Llama</h2><ol class=""><li id="af4f" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou pt pu pv bk"><strong class="ob fr">Butterfly Image Example</strong></li></ol><p id="6c88" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In our our first example, we’ll chat with Llama 3.2 about an image of a hatching butterfly. Since Llama 3.2-Vision does not support prompting with system prompts when using images, we will append instructions directly to the user prompt to guide the model’s responses. By setting <code class="cx ql qm qn qd b">do_sample=True</code> and <code class="cx ql qm qn qd b">temperature=0.2</code> , we enable slight randomness while maintaining response coherence. For fixed answer, you can set <code class="cx ql qm qn qd b">do_sample==False</code> . The <code class="cx ql qm qn qd b">messages</code> parameter, which holds the chat history, is initially empty, as in the <code class="cx ql qm qn qd b">images</code> parameter.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="928a" class="qg ne fq qd b bg qh qi l qj qk">instructions = "Respond concisely in one sentence."<br/>prompt = instructions + "Describe the image."<br/><br/>response, messages,images= chat_with_mllm ( model, processor, prompt,<br/>                                             images_path=[img_path],<br/>                                             do_sample=True,<br/>                                             temperature=0.2,<br/>                                             show_image=True,<br/>                                             messages=[],<br/>                                             images=[])<br/><br/># Output:  "The image depicts a butterfly emerging from its chrysalis, <br/>#           with a row of chrysalises hanging from a branch above it."</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/547d3652bb89e47a8039ddfee8ff7c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KPV1vztrp5clHxjJW4rEAQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by <a class="af nc" href="https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><p id="c477" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As we can see, the output is accurate and concise, demonstrating that the model effectively understood the image.</p><p id="a53a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For the next chat iteration, we’ll pass a new prompt along with the chat history (<code class="cx ql qm qn qd b">messages</code>) and the image file (<code class="cx ql qm qn qd b">images</code>). The new prompt is designed to assess the reasoning ability of Llama 3.2:</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="0730" class="qg ne fq qd b bg qh qi l qj qk">prompt = instructions + "What would happen to the chrysalis in the near future?"<br/>response, messages, images= chat_with_mllm ( model, processor, prompt,<br/>                                             images_path=[img_path,],<br/>                                             do_sample=True,<br/>                                             temperature=0.2,<br/>                                             show_image=False,<br/>                                             messages=messages,<br/>                                             images=images)<br/><br/># Output: "The chrysalis will eventually hatch into a butterfly."</span></pre><p id="cee7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We continued this chat in the provided Colab notebook and obtained the following conversation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qp"><img src="../Images/a72d924ad8dd280b870dadf9ab7c7224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*QYVq6trOmlJkuJO2YB10-Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="e177" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The conversation highlights the model’s image understanding ability by accurately describing the scene. It also demonstrates its reasoning skills by logically connecting information to correctly conclude what will happen to the chrysalis and explaining why some are brown while others are green.</p><p id="fe20" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">2. Meme Image Example</strong></p><p id="15d1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this example, I will show the model a meme I created myself, to assess Llama’s OCR capabilities and determine whether it understands my sense of humor.</p><pre class="mm mn mo mp mq qc qd qe bp qf bb bk"><span id="cbe5" class="qg ne fq qd b bg qh qi l qj qk">instructions = "You are a computer vision engineer with sense of humor."<br/>prompt = instructions + "Can you explain this meme to me?"<br/><br/><br/>response, messages,images= chat_with_mllm ( model, processor, prompt,<br/>                                             images_path=[img_path,],<br/>                                             do_sample=True,<br/>                                             temperature=0.5,<br/>                                             show_image=True,<br/>                                             messages=[],<br/>                                             images=[])</span></pre><p id="02aa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This is the input meme:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/e196e4c1075cc3f596b4cfa4160b6658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28-OAdEcIaet81CXXrhDRA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Meme by author. Original bear image by <a class="af nc" href="https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA" rel="noopener ugc nofollow" target="_blank">Hans-Jurgen Mager</a>.</figcaption></figure><p id="b4c9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And this is the model’s response:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/92233ad5cc21df183820c16f4e063aae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ea95GfUxmmyOwVQYoiITAg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="4876" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As we can see, the model demonstrates great OCR abilities, and understands the meaning of the text in the image. As for its sense of humor — what do you think, did it get it? Did you get it? Maybe I should work on my sense of humor too!</p><h1 id="5c1d" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Concluding Remarks</h1><p id="2632" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this tutorial, we learned how to build the Llama 3.2-Vision model locally and manage conversation history for chat-like interactions, enhancing user engagement. We explored Llama 3.2’s zero-shot abilities and were impressed by its scene understanding, reasoning and OCR skills.</p><p id="fc12" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Advanced techniques can be applied to Llama 3.2, such as fine-tuning on unique data, or using retrieval-augmented generation (RAG) to ground predictions and reduce hallucinations.</p><p id="d275" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Overall, this tutorial provides insight into the rapidly evolving field of Multimodal LLMs and their powerful capabilities for various applications.</p><h1 id="1896" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Thank you for reading!</h1><p id="1202" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Congratulations on making it all the way here. Click 👍x50 to show your appreciation and raise the algorithm self esteem 🤓</p><p id="cf31" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Want to learn more?</strong></p><ul class=""><li id="67ab" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw pu pv bk"><a class="af nc" href="https://medium.com/@lihigurarie" rel="noopener"><strong class="ob fr">Explore</strong></a> additional articles I’ve written</li><li id="f48c" class="nz oa fq ob b go px od oe gr py og oh oi pz ok ol om qa oo op oq qb os ot ou pw pu pv bk"><a class="af nc" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"><strong class="ob fr">Subscribe</strong></a><strong class="ob fr"> </strong>to get notified when I publish articles</li><li id="7274" class="nz oa fq ob b go px od oe gr py og oh oi pz ok ol om qa oo op oq qb os ot ou pw pu pv bk">Follow me on <a class="af nc" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"><strong class="ob fr">Linkedin</strong></a></li></ul><h1 id="5bb2" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Full Code as Colab notebook:</h1><figure class="mm mn mo mp mq mr"><div class="qs io l ed"><div class="qt qu l"/></div></figure><h1 id="d195" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><p id="6767" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">[0] Code on Colab Notebook: <a class="af nc" href="https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa" rel="noopener ugc nofollow" target="_blank">link</a></p><p id="ee09" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[1] <a class="af nc" href="https://arxiv.org/pdf/2407.21783" rel="noopener ugc nofollow" target="_blank">The Llama 3 Herd of Models</a></p><p id="b822" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[2] <a class="af nc" href="https://llamaimodel.com/requirements-3-2/" rel="noopener ugc nofollow" target="_blank">Llama 3.2 11B Vision Requirements</a></p></div></div></div></div>    
</body>
</html>