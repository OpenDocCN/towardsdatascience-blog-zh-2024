<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The AQLM Quantization Algorithm, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The AQLM Quantization Algorithm, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13">https://towardsdatascience.com/the-aqlm-quantization-algorithm-explained-8cf33e4a783e?source=collection_archive---------3-----------------------#2024-03-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Pierre Lienhart" class="l ep by dd de cx" src="../Images/d7e5267b3d1aef443da43494d83587f4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mrPOEXbvnaWaAXaecd7x9Q.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@plienhar?source=post_page---byline--8cf33e4a783e--------------------------------" rel="noopener follow">Pierre Lienhart</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8cf33e4a783e--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">2</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="bd37" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">There is a new quantization algorithm in town! The <strong class="lz fr"><em class="mv">Additive Quantization of Language Models (AQLM)</em></strong> [1] quantization procedure was released in early February 2024 and has already been integrated to <a class="af mw" href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.AqlmConfig" rel="noopener ugc nofollow" target="_blank">HuggingFace Transformers</a> (as of version <a class="af mw" href="https://github.com/huggingface/transformers/releases/tag/v4.38.0" rel="noopener ugc nofollow" target="_blank">4.38.0</a>–21/02/2024) and <a class="af mw" href="https://huggingface.co/docs/peft/developer_guides/quantization" rel="noopener ugc nofollow" target="_blank">HuggingFace PEFT</a> (as of version <a class="af mw" href="https://github.com/huggingface/peft/releases/tag/v0.9.0" rel="noopener ugc nofollow" target="_blank">0.9.0</a>–28/02/2024). This means that checkpoints quantized using AQLM can be loaded using these libraries and HuggingFace Transformers can be used to quantize compatible checkpoints using AQLM.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div role="button" tabindex="0" class="ng nh ed ni bh nj"><div class="mx my mz"><img src="../Images/51e1af2cecf7b74e06333ee31bf733ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uUmiDL9FDfCzZDK9"/></div></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Photo by <a class="af mw" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JJ Ying</a> on <a class="af mw" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8c5b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In this blog post, we will examine the key results presented in the AQLM paper [1] and provide a detailed overview of the key concepts behind this new quantization technique.</p><p id="83cf" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In this article, we will first review the key results presented in the AQLM paper. Next, we will examine the motivations for quantizing large language models for inference. We will then dive into the details of Multi-Codebook Quantization (MCQ), a technique uniquely leveraged by AQLM for weight quantization. After breaking down the memory footprint of AQLM models and examining key quantization parameters, we will explain the AQLM quantization procedure step-by-step. Finally, we will discuss the concept of Pareto efficiency as it relates to model quantization, providing perspective on how AQLM pushes the boundaries of Pareto-optimal quantization.</p><h1 id="f53e" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">AQLM Performance</h1><p id="4180" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Existing weight-only quantization algorithms could technically quantize model weights down to the 2-bit range. However, they failed at effectively preserving model accuracy. AQLM is a new weight-only post-training quantization (PTQ) algorithm that sets a new state-of-the-art for the 2 bit-per-parameter range. It also provides smaller benchmark improvements compared to existing methods for the 3-bit and 4-bit ranges (Table 1). Specifically, AQLM outperforms popular algorithms like GPTQ [2] as well as more recent but lesser known methods such as QuIP [3] and QuIP# [4]. AQLM authors also claim that their quantization algorithm pushes the Pareto frontier of the tradeoff between model accuracy and memory footprint below 3 bits per parameter for the first time.</p><p id="512a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The table below summarizes the performance of AQLM when compressing the Llama-2–70B model to 4-bit, 3-bit, and 2-bit per parameter. Performance is measured by perplexity on the WikiText2 [5] and C4 [6]. datasets (lower is better) as well as zero-shot accuracy on the WinoGrande [7] and HellaSwag [8] benchmarks (higher is better). For comparison, the performance of QuIP#, the top competing method, is shown for 4-bit and 2-bit compression. Since the <a class="af mw" href="https://github.com/Cornell-RelaxML/quip-sharp" rel="noopener ugc nofollow" target="_blank">available QuIP# implementation</a> does not support 3-bit compression, SpQR [9] is included as the comparison method for AQLM at 3 bits.</p><figure class="na nb nc nd ne nf"><div class="ot ic l ed"><div class="ou ov l"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Table 1 —AQLM vs. top competitor on Llama-2–70B compressed at 2, 3 and 4 bits per parameter</figcaption></figure><p id="0714" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">While quantization can sometimes reduce inference latency compared to FP16, this is not guaranteed. In benchmarks, AQLM-quantized models showed moderate latency improvements, with speedups ranging from 1.2x to 2x in most cases, and up to 3.05x in the best case. However, latency reduction was not the focus of AQLM’s designers. Their priority was maximizing accuracy within a target model size, rather than optimizing for speed. Consequently, the latency gains from AQLM quantization are noticeable but not as dramatic as the improvements from other existing quantization algorithms.</p><p id="2776" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Nevertheless, AQLM marks an important step towards making large language models more accessible on consumer hardware and mobile devices. For example, when quantizing a 7B model from 16-bit half precision formats like FP16 (16 bits or 2 bytes per parameter) down to just 2 bits per parameter (0.25 bytes per parameter), the memory footprint is reduced by a factor of 8x — decreasing from 14GB down to only 1.75GB.</p><h1 id="0029" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Why and what do we quantize?</h1><p id="05c5" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">PTQ methods fall into two categories: those that quantize just the model weights, and those that quantize both weights and activations. AQLM falls into the first category, only quantizing weights. Model weights are static by definition, so they can be quantized offline before deployment and even distributed on platforms such as the <a class="af mw" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">HuggingFace Model Hub</a>. Activations encompass everything else, including the key-value (KV) cache, and are only known at runtime during inference.</p><p id="84da" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The first checkpoints quantized (mostly to 2 bits) using AQLM have started to appear on <a class="af mw" href="https://huggingface.co/collections/ISTA-DASLab/aqlm-65e8dc75b908c7d73ec35598" rel="noopener ugc nofollow" target="_blank">the HF Hub</a>. However, <a class="af mw" href="https://huggingface.co/TheBloke" rel="noopener ugc nofollow" target="_blank">TheBloke</a>, a popular model quantizer, has not yet included this quantization technique in his set of quantization methods.</p><p id="d42e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">When quantizing LLMs weights, not all the weights are actually quantized. Only the parameters that make up the bulk of the parameter count, like the large projection matrices of both the attention and feed-forward layers, are typically quantized. Other parameters are usually kept in native precision.</p><p id="27fa" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">When opting for weight-only quantization, efficient mixed precision kernels for matrix multiplications are usually not available. As a result, quantized weights are dequantized at runtime after being fetched from memory. Depending on the overhead of dequantization, the latency reductions from lower data transfer can be partially preserved or completely offset.</p><p id="1327" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">There are four main benefits associated with the reduced weight memory footprint of quantized models for LLM inference:</p><p id="10a2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By reducing the weight’s memory footprint, quantizing large language model weights for inference provides four main benefits:</p><ul class=""><li id="ea80" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk">Reduced hardware requirements for model serving: A quantized model can be served using less expensive GPUs or even made accessible on consumer devices or mobile platforms.</li><li id="de56" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">Increased space for the KV cache to enable larger batch sizes and/or sequence lengths.</li><li id="5564" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">Faster decoding latency. As the decoding process is memory bandwidth bound, less data movement from reduced weight sizes directly improves this, unless offset by dequantization overhead.</li><li id="68ea" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk">A higher compute-to-memory access ratio (through reduced data movement), known as arithmetic intensity. This allows for fuller utilization of available compute resources during decoding.</li></ul><h1 id="bdeb" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">What is Multi-Codebook Quantization (MCQ)?</h1><p id="7a44" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk"><strong class="lz fr">AQLM applies Multi-Codebook Quantization (MCQ) to compress the weights of LLMs.</strong> Originally, MCQ was developed to enable efficient nearest neighbor search on vector databases. It works by splitting each vector of the database into <strong class="lz fr">subgroups</strong> (sub-vectors), which are in turn approximated using learned vectors named <strong class="lz fr">codewords</strong>. A <strong class="lz fr">codebook</strong> is a set of such codewords. This allows similarity computations to be performed efficiently using the finite set of codewords instead of the full vector database.</p><p id="db69" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In AQLM, the vectors that are quantized correspond to the rows of the weight matrices. That is, AQLM quantizes the output channels of each weight matrix using MCQ.</p><p id="77cb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Note:</strong> It should be noted that AQLM uses the <em class="mv">W.X</em> notation convention (<em class="mv">W</em> and <em class="mv">X</em> are the weight and activation matrices respectively), whereas some other quantization papers use the reverse <em class="mv">X.W</em> convention. This means the output channels that AQLM quantizes correspond to the rows of the weight matrix, while in <em class="mv">X.W</em> notation, they would be the columns.</p><p id="1406" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Each row of the weight matrix</strong> of shape <em class="mv">(d_out, d_in)</em> is divided into sub-vectors called <strong class="lz fr">groups</strong> of size <em class="mv">(1, g)</em>. <strong class="lz fr">Assuming the codebooks have already been learned</strong>, AQLM approximates each group as <strong class="lz fr">the sum of <em class="mv">M </em>same-size</strong> <strong class="lz fr">codewords</strong> that are stored at native precision. Each codeword belongs to a different codebook, each codebook containing <em class="mv">2^B</em> codewords. To reconstruct a group using the learned codebooks, we actually only need to store the index of each constituent codeword in its codebook. This index can be represented as a <em class="mv">2^B</em>-dimensional one-hot vector called a <strong class="lz fr">code</strong>. So each group is represented by <em class="mv">M</em> one-hot code vectors of size <em class="mv">2^B</em>. Storing such a one-hot vector requires <em class="mv">B</em> bits. Therefore, the total memory footprint to store the compressed representation of each group is <em class="mv">M</em> x <em class="mv">B</em> bits.</p><p id="b144" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The process of building the quantized representation in AQLM is summarized in Figure 1. It should be noted that before splitting each output channel into groups, the output channels are scaled by a learned scaling factor.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pe"><img src="../Images/cd0c7859ad86e7a3a36f044856401918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UwoN-suJ-z-DtH0DTVN58g.png"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Figure 1 — Multi-codebook encoding of a parameter group (d_in=9, d_out=4, g=3, M=3, B=2) — Figure by author</figcaption></figure><p id="206e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">As mentioned previously, at inference time, the matrix multiplication with activations <em class="mv">X</em> uses <strong class="lz fr">dequantized</strong>, native-precision parameters rather than the quantized code vectors. As shown in Figure 2, the dequantization process works by decompressing the code vectors back into one-hot index vectors to retrieve the corresponding codewords from each codebook. These codewords are summed together, then scaled to reproduce the original, half-precision weight values for computation.</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pe"><img src="../Images/4bc3250e1d3dc7b30b166f427707413c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZ2VEa6GWvA6pv7BCot-bA.png"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Figure 2 — Decoding of a parameter group from codebook indices (codes) (d_in=9, d_out=4, g=3, M=3, B=2) — Figure by author</figcaption></figure><h1 id="6610" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Memory footprint of AQLM-quantized models</h1><p id="7281" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Most importantly, what is the achieved average number of bits per parameter using AQLM? To store an AQLM-quantized weight matrix, the following information needs to be stored:</p><ul class=""><li id="dd09" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ow ox oy bk"><em class="mv">M</em> codebooks, each containing <em class="mv">2^B</em> codewords stored at native 16-bit precision. Each codeword has size <em class="mv">(1, g)</em>.</li><li id="9b38" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><em class="mv">d_out</em> scaling factors, each stored as a 16-bit float</li><li id="27d9" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu ow ox oy bk"><em class="mv">M</em> code vectors of <em class="mv">B</em> bits each to encode each group, of which there are total <em class="mv">d_out</em> x <em class="mv">d_in/g</em>.</li></ul><p id="6da2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Therefore, the average number of bits per parameter can be calculated with the following formula:</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pf"><img src="../Images/b1bc29895cb8a23c9f56a168f22a0a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*m6SIH0dh6QYjWjCkWZ7QAA.png"/></div></figure><p id="7fee" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It should be noted that the formula above calculates the average bits per parameter for a single weight matrix, i.e. a single layer, not the entire model.</p><p id="6030" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Let’s look at each term’s contribution for different configurations (Table 2) taking Llama-2–70B feed-forward layer as an example :</p><p id="b78b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To understand how each term contributes for different configurations, let’s examine a specific example: the feed-forward layer of the Llama-2–70B model (<em class="mv">d_in=8 192</em> and <em class="mv">d_out=28 672</em>). Table 2 shows the breakdown of each term’s contribution across different configurations for this layer.</p><figure class="na nb nc nd ne nf"><div class="ot ic l ed"><div class="ou ov l"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Table 2 — Decomposed average bits per parameter. Scenario A: g=8 ; M=1 ; B = 16 (2 bits) — Scenario B: g=8 ; M=2 ; B = 12 (3 bits) — Scenario C: g=8 ; M=2 ; B = 16 (4 bits) — Scenario D: g=32 ; M=6 ; B = 16 (3.85 bits)</figcaption></figure><p id="1bbb" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The scaling factor terms are always negligible in their contribution. The average number of bits per parameter is primarily dictated by the codes encoding each group. The codebooks term generally has a small contribution, unless both <em class="mv">B</em> and <em class="mv">g</em> are set to relatively high values (as in Scenario D).</p><h1 id="5a60" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Key AQLM quantization parameters</h1><p id="9fdc" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">The group size <em class="mv">g</em>, number of codebooks <em class="mv">M</em>, and codebook size <em class="mv">B</em> are hyperparameters in AQLM’s quantization process. Assuming the code terms dominate the average bits per parameter, we can approximate the total as <em class="mv">B.M/g</em>. This means multiple combinations of <em class="mv">g</em>, <em class="mv">M</em>, and <em class="mv">B</em> can satisfy the same overall bit budget. To select the optimal configuration, we need to examine how these parameters impact model performance.</p><p id="dc4a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Note:</strong> The names of AQLM-quantized models follow a <code class="cx pg ph pi pj b">XBit-MxB</code> naming scheme such as <code class="cx pg ph pi pj b">ISTA-DASLab/gemma-2b-AQLM-2Bit-1x16-hf</code> for the 2-bit quantized version of Gemma-2B using one codebook with 65 536 (2¹⁶) codewords. Knowing the total bit budget, <em class="mv">M</em> and <em class="mv">B</em>, we can easily derive <em class="mv">g</em>.</p><p id="356d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Regarding latency, the higher the number of codewords, the slower, i.e. the lower the latency speedup. For example, matrix-vector multiplication of the 2-bit 1x16 (65 536 codewords total) Llama-7B model on GPU (Nvidia RTX 3090) shows a x1.31 speedup compared to the FP16 model, whereas the same size 2x8 (512 codewords total) model achieves a x1.57 speedup.</p><p id="93bd" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">However, decreasing the number of codewords negatively impacts model accuracy. As an example, the paper demonstrates that the 1x16 Llama-7B model (2-bit range) achieves a perplexity score of 6.29 on WikiText2 [5], while the 2x8 variant of the same model scores 7.98 on the same dataset. In comparison, the FP16 version scores 5.12.</p><p id="1a1d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now, considering a fixed total bit budget (e.g. 2 bits) and codebook size <em class="mv">B</em> (e.g. B=8), there are multiple valid (<em class="mv">M, g</em>) pairs that satisfy the budget constraint. For instance, with <em class="mv">B=8</em>, the pairs (1, 4), (2, 8), …, (8, 32), etc. are valid configurations. The paper demonstrates that within a given budget, larger (M, g) values correlate with lower perplexity, i.e. reduced quantization errors, although with diminishing returns. This reveals a latency-accuracy tradeoff — higher M improves accuracy but also increases latency.</p><p id="1e5b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Note: </strong>For many quantization methods, the average bits per parameter is dictated by the precision used to store parameters, such as INT8, INT4, INT3, etc. This only allows a few discrete average bits sizes. In contrast, AQLM provides much more flexibility — by adjusting the <em class="mv">g</em>, <em class="mv">M</em>, and <em class="mv">B</em> hyperparameters, a wider range of average bits can be achieved with finer granularity (as shown in Table 3).</p><figure class="na nb nc nd ne nf"><div class="ot ic l ed"><div class="ou ov l"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Table 3 — Average number of bits per parameter for Llama-2–70B feed-forward layer quantized using different (<em class="pk">B, M, g</em>) values</figcaption></figure><p id="7d17" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><strong class="lz fr">Note: </strong>Leaving model accuracy aside, it is likely that not all configurations are equally efficient. For instance, if the value of <em class="mv">B</em> is not a multiple of 8, then each stored code does not utilize all the bits across the bytes needed to represent it</p><h1 id="866e" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">The AQLM quantization procedure</h1><p id="5d2f" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">In the previous section, we assumed the codebooks and codes were already learned in order to demonstrate how AQLM builds a compressed representation. <strong class="lz fr">In practice, quantizing a model with AQLM involves learning these codebooks.</strong> Once the codebooks have been learned, compressing a weight matrix using the process described above is straightforward.</p><p id="a80b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">For an input half-precision weight matrix <em class="mv">W</em>, the AQLM quantization process learns: <em class="mv">M</em> codebooks <em class="mv">C</em>, <em class="mv">d_out</em> scaling factors <em class="mv">s</em>, and for each group, <em class="mv">M</em> code vectors <em class="mv">b</em> . These are learned by minimizing the following loss function:</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pl"><img src="../Images/b8ca1b6206df67ddbc9a861a64561692.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*wjUn93oxrlQXjCtf75XyKQ.png"/></div></figure><p id="dff0" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To learn the codebooks and the codes, <strong class="lz fr">calibration data </strong>(i.e. training data) is required. The authors use a few hundred 4096-length sequences from the RedPajama-v1 dataset [10] as calibration data. Performance is measured by evaluating perplexity on the WikiText2 [5] and C4 [6] datasets, which serve as validation sets.</p><p id="7ff5" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Looking at technicalities of this particular training would take us too far into the peculiarities of codebook learning. We will just cover the AQLM training (and therefore quantization) procedure main steps.</p><p id="4c4d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The AQLM algorithm actually applies to each Transformer decoder block. For a given decoder block, quantization is a two-step process:</p><ol class=""><li id="fac8" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pm ox oy bk">Codebooks, scaling factors and codes are learned for each linear layer in the block. In each case, the loss function minimization occurs in two stages: 1. The codes are learned first using the initialized codebooks and scaling factors. The codebooks here are fixed, initialized with a residual k-means approach. 2. With the codes learned from the first stage remaining fixed, the codebooks and scaling factors are then updated starting from their initialized values.</li><li id="df7c" class="lx ly fq lz b ma oz mc md me pa mg mh mi pb mk ml mm pc mo mp mq pd ms mt mu pm ox oy bk">After quantizing each linear layer in a decoder block, the block’s codebooks, scaling factors, and non-quantized parameters (like normalization layer scales/biases) undergo further fine-tuning. The codes remain frozen at this stage. This fine-tuning uses input and output activations recorded before quantization and allows joint optimization of the parameters across layers. Optimizing jointly accounts for interactions between quantization errors across layers, which is important at very low bitrates where quantization errors are relatively larger.</li></ol><h1 id="3ca6" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Pareto optimality</h1><p id="d53a" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">The AQLM authors claim to have pushed the Pareto frontier for the tradeoff between model accuracy (measured by perplexity for example) and memory footprint below 3 bits per weight for the first time. While an important achievement, what does this milestone represent?</p><p id="a5c9" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><a class="af mw" href="https://en.wikipedia.org/wiki/Pareto_efficiency" rel="noopener ugc nofollow" target="_blank">Pareto optimality</a> refers to an efficient state where one metric cannot be improved without negatively impacting another metric. For example, consider a system described by two desirable characteristics. A Pareto-optimal state is one where there exists no modification that could improve one characteristic without worsening the other. Conversely, if a change could positively affect one characteristic at no cost to the other, that would be considered Pareto-inefficient, as a more optimal state is possible. The Pareto frontier plots all such Pareto-optimal states.</p><p id="352c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">When applied to model quantization, each model variant (quantized or full-precision) represents a state described by its accuracy and memory footprint. The Pareto frontier comprises the set of (usually quantized) models with the optimal tradeoff between accuracy and size. On this frontier, there exists no way to further compress model size without losing accuracy, or improve accuracy without increasing memory requirements.</p><p id="a072" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">For example, the paper shows Llama-2–13B quantized using AQLM to 2 bits per weight achieves 5.65 perplexity, while 4-bit AQLM quantization of Llama-2–7B achieves 5.21 perplexity. Both occupy ~1.7GB, but the 2-bit model has worse accuracy. Therefore at this footprint, the 4-bit model is more efficient — higher accuracy for the same 1.7GB size.</p><p id="d03c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">How is that possible? These Pareto efficiency limitations stem from the difficulty quantization techniques face in avoiding substantial accuracy losses at extremely low bit-per-parameter values.</p><p id="66a4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">If we assume all quantization techniques could perfectly preserve model accuracy, then each time a new technique achieves higher compression, the Pareto frontier would simply shift to include only models quantized using that latest technique (Figure 3).</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pe"><img src="../Images/96d66c13b2221c4493224787716f313e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FW_yqZfUXa8bzvKmcYhZSw.png"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Figure 3 — Perfect quantization methods — Figure by author</figcaption></figure><p id="6a2a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">However, because quantization leads to losses in model accuracy, achieving higher compression does not necessarily mean reaching the Pareto frontier if the accuracy loss is too great compared to other existing techniques (Figure 4).</p><figure class="na nb nc nd ne nf mx my paragraph-image"><div class="mx my pe"><img src="../Images/1da66cf1d9d414e7df9efa94e3f1468e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWI3XNxSG7IzLapg2bXYcQ.png"/></div><figcaption class="nl nm nn mx my no np bf b bg z dx">Figure 4 — Imperfect quantization methods — Figure by author</figcaption></figure><p id="e333" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Pushing the Pareto frontier below 3 bits per weight means that existing sub-3-bit quantized models were not Pareto optimal — for a given model memory footprint, accuracy was not maximized. The authors determine 2.5 bits as the optimal rate for the Llama-2 family with AQLM. In other words, Llama-2 models that are quantized to use an average of 2.5 bits per parameter using AQLM sit on the Pareto frontier.</p><h1 id="3ca7" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Conclusion</h1><p id="95be" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">In this post, we introduced AQLM, a new quantization algorithm that applies Multi-Codebook Quantization (MCQ) to large language models for the first time. AQLM sets a new state-of-the-art for model compression in the 2-bit per parameter range and achieves Pareto optimality with sub-3-bit models for the first time.</p><p id="0b01" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">With its groundbreaking compression rates and maintenance of accuracy, AQLM represents a major step forward in deploying large language models efficiently and making large language models more accessible to consumer hardware and mobile devices.</p><p id="cf9d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">AQLM is already supported by the HuggingFace Transformers and PEFT libraries, making it easy for developers to leverage AQLM’s advantages!</p></div></div></div><div class="ab cb pn po pp pq" role="separator"><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2787" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[1]: V. Egiazarian et al., <a class="af mw" href="https://arxiv.org/abs/2401.06118" rel="noopener ugc nofollow" target="_blank">Extreme Compression of Large Language Models via Additive Quantization</a> (2024), arXiv preprint arXiv:2401.06118</p><p id="148a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[2]: E. Frantar et al., <a class="af mw" href="https://arxiv.org/abs/2210.17323" rel="noopener ugc nofollow" target="_blank">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> (2022), ICLR 2023</p><p id="53a8" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[3]: J. Chee et al., <a class="af mw" href="https://arxiv.org/abs/2307.13304" rel="noopener ugc nofollow" target="_blank">QuIP: 2-Bit Quantization of Large Language Models With Guarantees</a> (2023), NeurIPS 2023 spotlight</p><p id="a850" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[4]: A. Tseng et al., <a class="af mw" href="https://arxiv.org/abs/2402.04396" rel="noopener ugc nofollow" target="_blank">QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks</a> (2024), arXiv preprint arXiv:2402.04396</p><p id="03e2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[5]: S. Merity et al., <a class="af mw" href="https://arxiv.org/abs/1609.07843" rel="noopener ugc nofollow" target="_blank">Pointer Sentinel Mixture Models</a> (2016), ICLR 2017 Poster</p><p id="890d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[6]: C. Raffel et al., <a class="af mw" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (2019), JMLR 2020</p><p id="3d40" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[7]: K. Sagaguchi et al., <a class="af mw" href="https://arxiv.org/abs/1907.10641" rel="noopener ugc nofollow" target="_blank">WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a> (2021), ACM 2021</p><p id="29e4" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[8]: R. Zellers et al., <a class="af mw" href="https://arxiv.org/abs/1905.07830" rel="noopener ugc nofollow" target="_blank">HellaSwag: Can a Machine Really Finish Your Sentence?</a> (2019), ACL 2019</p><p id="73d1" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[9]: T. Dettmers et al., <a class="af mw" href="https://arxiv.org/abs/2306.03078" rel="noopener ugc nofollow" target="_blank">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</a> (2023), arXiv preprint arXiv:2306.03078</p><p id="2c4d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">[10]: Together Computer, <a class="af mw" href="https://github.com/togethercomputer/RedPajama-Data" rel="noopener ugc nofollow" target="_blank">RedPajama: an Open Dataset for Training Large Language Models</a> (2023), <a class="af mw" href="https://github.com/togethercomputer/RedPajama-Data" rel="noopener ugc nofollow" target="_blank">https://github.com/togethercomputer/RedPajama-Data</a></p></div></div></div></div>    
</body>
</html>