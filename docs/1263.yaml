- en: Why LLMs are not Good for Coding — Part II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20](https://towardsdatascience.com/llms-coding-software-development-artificial-intelligence-68f195bb2ad3?source=collection_archive---------1-----------------------#2024-05-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Large Language Models for Coding Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page---byline--68f195bb2ad3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--68f195bb2ad3--------------------------------)
    ·6 min read·May 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/591bff2e771516cb8f20a5bc9481bda2.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made image.
  prefs: []
  type: TYPE_NORMAL
- en: 'After publishing the first article of this series, “[Why LLMs are Not Good
    for Coding](https://medium.com/towards-data-science/llms-coding-chatgpt-python-artificial-intelligence-4ea7a7bbdd93),”
    I received several comments on social media, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: “I am using ChatGPT for coding and it works perfectly fine.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “You are wrong. Large Language Models are useful coding assistants.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I was surprised by these reactions since the purpose of this article series
    is not to discourage anyone from using Large Language Models (LLMs) for coding
    but to **identify the key areas that need improvement to transform LLMs into more
    effective coding assistants.**
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs such as ChatGPT can be helpful in some instances, **they often produce
    code that could be syntactically correct but suboptimal** or even incorrect in
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous article, we discussed how the tokenizer, the complexity of context
    windows when applied to code, and the nature of the training itself can influence
    the performance of these models in coding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this second article, we will explore in more depth the type of training
    these models undergo to be used for coding tasks, as well as another reason why
    **LLMs are not inherently proficient at coding “out of the box”: the challenge
    of staying up-to-date**…'
  prefs: []
  type: TYPE_NORMAL
