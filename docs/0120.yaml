- en: Exploring “Small” Vision-Language Models with TinyGPT-V
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/exploring-small-vision-language-models-with-tinygpt-v-499d37a1456d?source=collection_archive---------6-----------------------#2024-01-12](https://towardsdatascience.com/exploring-small-vision-language-models-with-tinygpt-v-499d37a1456d?source=collection_archive---------6-----------------------#2024-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: TinyGPT-V is a “small” vision-language model that can run on a single GPU
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@scottcampit?source=post_page---byline--499d37a1456d--------------------------------)[![Scott
    Campit, Ph.D.](../Images/66a9bc8111e05b8ff2992092a0eb27e9.png)](https://medium.com/@scottcampit?source=post_page---byline--499d37a1456d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--499d37a1456d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--499d37a1456d--------------------------------)
    [Scott Campit, Ph.D.](https://medium.com/@scottcampit?source=post_page---byline--499d37a1456d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--499d37a1456d--------------------------------)
    ·8 min read·Jan 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI technologies are continuing to become embedded in our everyday lives. One
    application of AI includes going multi-modal, such as integrating language with
    vision models. These vision-language models can be applied towards tasks such
    as video captioning, semantic searching, and many other problems.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: This week, I’m going to shed a spotlight towards a recent vision-language model
    called TinyGPT-V ([Arxiv](https://arxiv.org/abs/2312.16862) | [GitHub](https://github.com/DLYuanGod/TinyGPT-V)).
    What makes this multimodal language model interesting is that it is very “small”
    for a large language model, and can be deployed on a single GPU with as little
    as 8GB of GPU or CPU for inference. This is significant for maximizing the speed,
    efficiency, and costs of AI models in the wild.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: I would like to note that I’m not an author or in anyway affiliated with the
    authors of the model. However, as a researcher and practitioner, I thought it
    was an intriguing development in AI that is worth examining, especially since
    having more efficient models will unlock many more applications. Let’s dive in!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The Problem: Vision-Language Models are Useful But Resource Intensive'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ca5a8701ea7b8c3639e3763e992109c5.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jp Valery](https://unsplash.com/@jpvalery?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal models, such as [vision-language models](https://huggingface.co/blog/vision_language_pretraining),
    are achieving record performance in human-aligned responses. As these models continue
    to improve, we could see companies begin to apply these technologies in real-world
    scenarios and applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: However, many AI models, especially multi-modal models, require substantial
    computational resources for both model training and inference. This physical constraint
    of time, hardware resources, and capital is a bottleneck for researchers and practitioners.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Further, these constrains currently prevent multi-modal models from being deployed
    in certain application interfaces, such as edge-devices. Research and development
    towards quantized (smaller) and high performance models is needed to address these
    challenges.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'TinyGPT-V: “Small” Vision Language Models'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/1bf020bc7c57efbc6dd573e8c313f49b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Photo by [Céline Haeberly](https://unsplash.com/@celinehaeberly?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: TinyGPT-V is a 2.8B parameter vision-language model that can be trained on a
    24GB GPU and uses 8GB of GPU or CPU for inference. This is significant, because
    other state-of-the-art “smaller” vision-language models, such as [LLaVA1.5](https://huggingface.co/liuhaotian/llava-v1.5-13b),
    are still relatively “big” (7B and 13B parameters).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: When benchmarking against other larger vision-language models, TinyGPT-V achieves
    similar performance on multiple tasks. Together, this work contributes towards
    a movement to make AI models more efficient by reducing their computational needs
    while retaining performance. Balancing these two objectives will enable vision-language
    models to be served directly on devices, which will offer better user experiences
    including reduced latency and more robustness.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Related Work and Adjacent Technologies Applied in the TinyGPT-V architecture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not-So-Large Foundation Vision-Language Models (VLMs)
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VLMs learn the relationship between images/videos and text, which can be applied
    for many common tasks such as searching for objects within a photo (semantic search),
    asking questions and receiving answers on videos (VQA), and many more tasks. **LLaVA1.5**
    and [**MiniGPT-4**](https://minigpt-4.github.io/) are two multi-modal large language
    models that are state-of-the-art as of January 2024, and are relatively smaller
    than similar VL foundation models. However, these VLMs still requires significant
    GPU usage and training hours. For example, the authors describe the training resources
    for LLaVA-v1.5 13B parameter model, which uses eight A100 GPUs with 80GB RAM for
    25.5 hours of training. This is a barrier towards individuals and institutions
    that wish to study, develop, and apply these models in the wild.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'TinyGPT-V is one of the latest VLMs that aims to address this issue. It uses
    two separate foundation models for the vision and language components: the [**EVA**](https://arxiv.org/abs/2211.07636)
    encoder was used as the vision component, while [**Phi-2**](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)
    was used as the language model. Briefly, EVA scales up to a 1B parameter vision
    transformer model that is pre-trained to reconstruct masked image-text features.
    Phi-2 is a 2.7B parameter language model that was trained on curated synthetic
    and web datasets. The authors were able to merge these two models and quantize
    them to have a total parameter size of 2.8B.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Shown below is the performance of TinyGPT-V compared to other VLMs with various
    visual language tasks. Notably, TinyGPT-V performs similarly to [**BLIP-2**](https://arxiv.org/abs/2301.12597),
    likely due to the pre-trained Q-Former module that was taken from BLIP-2\. Further,
    it appears that [**InstructBLIP**](https://arxiv.org/abs/2305.06500) achieved
    better performance compared to TinyGPT-V, although it is noted that the smallest
    InstructBLIP model is trained with 4B parameters. Depending on the application,
    this trade-off may be worth it to a practitioner, and additional analyses would
    need to be conducted to explain for this difference.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The following datasets the model is trained with include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[**GQA**](https://cs.stanford.edu/people/dorarad/gqa/about.html): Real-world
    visual reasoning and compositional QA'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**VSR**](https://paperswithcode.com/dataset/vsr): text-image pairs in english
    with spatial relationships'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**IconQA**](https://iconqa.github.io/): visual understanding and reasoning
    with icon images'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**VizWiz**](https://www.vizwiz.com/): visual queries derived from a photo
    taken by a visually impaired individual with a smartphone and supplemented with
    10 answers.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**HM**](https://hatefulmemeschallenge.com/): a multimodal collection designed
    to detect hateful content in memes.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/712ba98abc73a68df1461e3f4618fbef.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: TinyGPT-V benchmark performance against similar state-of-the-art “smaller” vision
    language models (adapted from Figure 1 of [Yuan et al., 2023](https://arxiv.org/abs/2312.16862)).
    Note that we should assume that the authors denote their model as “TinyGPT-4”.
    It’s performance is comparable to BLIP-2, which is ~3.1B parameters. InstructBLIP
    has better performance across different tasks, but is notably ~4B parameters.
    This is much bigger than TinyGPT-V, which is ~2.1B parameters in size.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modal alignment of visual and language features
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VLM training consists of several objective functions to optimize for to a) expand
    the utility of VLMs, b) increase VLM general performance, and c) mitigate the
    risk of catastrophic forgetting. In addition to different objective functions,
    there are several model architectures or methods to learn and merge the joint
    representation of vision and language features. We will discuss the relevant layers
    for training TinyGPT-V, which are shown below as blocks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70636cbaddfef36fe846e263bc823d27.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: TinyGPT-V training schemes, adapted from Figure 2 ([Yuan et al., 2023](https://arxiv.org/abs/2312.16862)).
    Stage 1 was a warm-up pre-training stage. The second stage is a pre-training stage
    to train the LoRA module. The third training stage aims to instruction-tune the
    model. Finally, the fourth training stage aims to fine-tune the model for various
    multi-modal tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Q-Former** described in BLIP-2 paper was used to learn the joint representation
    from the aligned image-text data. The Q-Former method optimizes for three objectives
    to learn the vision-language representation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '*Image-Text Matching:* Learn fine-grained alignment between the image and text
    representation'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Image-Text Contrastive Learning:* Align the image and text representation
    to maximize the mutual information gained'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Image-Grounded Text Generation:* Train the model to generate text, given input
    images'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the Q-former layer, they employed a pre-trained linear projection
    layer from MiniGPT-4 (Vicuna 7B) in order to accelerate learning. Then they apply
    a linear projection layer to embed these features into the Phi-2 language model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training smaller large-scale language models from different modalities presented
    significant challenges. During their training process, they found that the model
    outputs were susceptible to NaN or INF values. Much of this was attributed to
    the vanishing gradient problem, as the model had a limited number of trainable
    parameters. To address these issues, they applied several normalization procedures
    in the Phi-2 model to ensure that the data is in an adequate representation for
    model training.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: There are three normalization techniques that are applied throughout the Phi-2
    model with minor adjustments from their vanilla implementation. They updated the
    **LayerNorm** mechanism that is applied within each hidden layer by including
    a small number for numerical stability. Further they implemented **RMSNorm** as
    a post-normalization procedure after each Multi-Head Attention Layer. Finally,
    they incorporated a **Query-Key Normalization** procedure, which they determined
    as being important in low-resource learning scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Efficient Fine-Tuning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-tuning models is essential to achieve better performance on downstream
    tasks or domain areas that are not covered in pre-training. This is an essential
    step to provide huge performance gains compared to out-of-the-box foundation models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: One intuitive way to fine-tune a model is to update all pre-trained parameters
    with the new task or domain area in mind. However, there are issues with this
    way of fine-tuning large language models, as it requires a full copy of the fine-tuned
    model for each task. **Parameter Efficient Fine-Tuning (PEFT)** is an active area
    of research in the AI community, where a smaller number of task-specific parameters
    are updated while most of the foundation model parameters are frozen.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-Rank Adaptation** ([**LoRA**](https://github.com/microsoft/LoRA)) is
    a specific PEFT method that was used to fine-tune TinyGPT-V. At a high-level,
    LoRA freezes the pre-trained model weights, and injects trainable rank decomposition
    matrices into each layer of a transformer, which reduces the number of trainable
    parameters for downstream tasks. Shown below is how the LoRA module was applied
    to the TinyGPT-V model.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7170418f71365ad34f801255abf522a0.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Adapted from Figure 3 ([Yuan et al., 2023](https://arxiv.org/abs/2312.16862)).
    Low-Rank Adaptation (LoRA) was applied to fine-tune TinyGPT-V. Panel c) hows how
    LoRA was implemented in TinyGPT-V. Panel d) shows the query-key normalization
    method described in the previous section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions and parting thoughts
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8d6eb5e4f2fac6192e6a2b70598621f8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mourizal Zativa](https://unsplash.com/@mourimoto?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: TinyGPT-V contributes to a body of research for making multi-modal large language
    models more efficient. Innovations in multiple areas, such as PEFT, quantization
    methods, and model architectures will be essential to getting models as small
    as possible while not sacrificing too much performance. As was observed in the
    pre-print, TinyGPT-V achieves a similar performance to other smaller VLMs. It
    matches BLIP-2 performance (smallest model is 3.1B parameters), and while it falls
    short of InstructBLIP’s performance on similar benchmarks, it is still smaller
    in size (TinyGPT-V is 2.8B parameters versus InstructBLIP’s 4B).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: For future directions, there are certainly aspects that could be explored to
    improve TinyGPT’s performance. For instance, other PEFT methods could have been
    applied for fine-tuning. From the pre-print, it is unclear if these model architecture
    decisions were purely based on empirical performance, or if it was a matter of
    convenience for implementation. This should be studied further.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Finally, at the time of this writing the pre-trained model and the model fine-tuned
    for instruction learning are available, while the multi-task model is currently
    a test version on GitHub. As developers and users use the model, further improvements
    could shed insights into additional strengths and weaknesses with TinyGPT-V. But
    altogether, I thought this was a useful study for designing more efficient VLMs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this breakdown of TinyGPT-V useful for your own applications!
    If you want to chat more about AI or if you’re in the Bay Area and just want to
    grab some coffee, please feel free to reach out on [LinkedIn](https://www.linkedin.com/in/scottcampit/).
    Otherwise, you can also catch me on [torchstack.ai](https://www.torchstack.ai/),
    where we offer custom AI solutions to customers and businesses.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得这份关于TinyGPT-V的分析对你的应用有帮助！如果你想更多地聊聊AI，或者如果你在湾区并且想喝杯咖啡，欢迎通过[LinkedIn](https://www.linkedin.com/in/scottcampit/)联系我。否则，你也可以通过[torchstack.ai](https://www.torchstack.ai/)找到我，我们在这里为客户和企业提供定制化的AI解决方案。
