- en: End-to-End Data Engineering System on Real Data with Kafka, Spark, Airflow,
    Postgres, and Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19](https://towardsdatascience.com/end-to-end-data-engineering-system-on-real-data-with-kafka-spark-airflow-postgres-and-docker-a70e18df4090?source=collection_archive---------0-----------------------#2024-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--a70e18df4090--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a70e18df4090--------------------------------)
    ·16 min read·Jan 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This article is part of a project that’s split into two main phases. The first
    phase focuses on building a data pipeline. This involves getting data from an
    API and storing it in a PostgreSQL database. In the second phase, we’ll develop
    an application that uses a language model to interact with this database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideal for those new to data systems or language model applications, this project
    is structured into two segments:'
  prefs: []
  type: TYPE_NORMAL
- en: This initial article guides you through constructing a data pipeline utilizing
    **Kafka** for streaming, **Airflow** for orchestration, **Spark** for data transformation,
    and **PostgreSQL** for storage. To set-up and run these tools we will use **Docker.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second article, which will come later, will delve into creating agents using
    tools like LangChain to communicate with external databases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This first part project is ideal for beginners in data engineering, as well
    as for data scientists and machine learning engineers looking to deepen their
    knowledge of the entire data handling process. Using these data engineering tools
    firsthand is beneficial. It helps in refining the creation and expansion of machine
    learning models, ensuring they perform effectively in practical settings.
  prefs: []
  type: TYPE_NORMAL
- en: This article focuses more on practical application rather than theoretical aspects
    of the tools discussed. For detailed understanding of how these tools work internally,
    there are many excellent resources available online.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s break down the data pipeline process step-by-step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Streaming: Initially, data is streamed from the API into a Kafka topic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Processing: A Spark job then takes over, consuming the data from the Kafka
    topic and transferring it to a PostgreSQL database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scheduling with Airflow: Both the streaming task and the Spark job are orchestrated
    using Airflow. While in a real-world scenario, the Kafka producer would constantly
    listen to the API, for demonstration purposes, we’ll schedule the Kafka streaming
    task to run daily. Once the streaming is complete, the Spark job processes the
    data, making it ready for use by the LLM application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of these tools will be built and run using docker, and more specifically
    [docker-compose](https://docs.docker.com/compose/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97b5e1b0b1332bc0aef250d1fdb62728.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the data pipeline. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a blueprint of our pipeline, let’s dive into the technical
    details !
  prefs: []
  type: TYPE_NORMAL
- en: Local setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First you can clone the Github repo on your local machine using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the overall structure of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `airflow` directory contains a custom Dockerfile for setting up airflow
    and a `[dags](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html)`
    directory to create and schedule the tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `data` directory contains the *last_processed.json file* which is crucial
    for the Kafka streaming task. Further details on its role will be provided in
    the Kafka section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker-compose-airflow.yaml`file defines all the services required to run
    airflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `docker-compose.yaml` file specifies the Kafka services and includes a docker-proxy.
    This proxy is essential for executing Spark jobs through a docker-operator in
    Airflow, a concept that will be elaborated on later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `spark` directory contains a custom Dockerfile for spark setup.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`src` contains the python modules needed to run the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To set up your local development environment, start by installing the required
    Python packages. The only essential package is psycopg2-binary. You have the option
    to install just this package or all the packages listed in the `requirements.txt`
    file. To install all packages, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next let’s dive step by step into the project details.
  prefs: []
  type: TYPE_NORMAL
- en: About the API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The API is [RappelConso](https://api.gouv.fr/les-api/api-rappel-conso) from
    the French public services. It gives access to data relating to recalls of products
    declared by professionals in France. The data is in French and it contains initially
    **31** columns (or fields). Some of the most important are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*reference_fiche (reference sheet):* Unique identifier of the recalled product.
    It will act as the primary key of our Postgres database later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*categorie_de_produit (Product category):* For instance food, electrical appliance,
    tools, transport means, etc …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*sous_categorie_de_produit (Product sub-category):* For instance we can have
    meat, dairy products, cereals as sub-categories for the food category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*motif_de_rappel (Reason for recall*): Self explanatory and one of the most
    important fields.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*date_de_publication* which translates to the publication date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*risques_encourus_par_le_consommateur* which contains the risks that the consumer
    may encounter when using the product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also several fields that correspond to different links, such as link
    to product image, link to the distributers list, etc..
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see some examples and query manually the dataset records using this
    [link](https://data.economie.gouv.fr/explore/dataset/rappelconso0/api/?disjunctive.categorie_de_produit=&sort=date_de_publication).
  prefs: []
  type: TYPE_NORMAL
- en: 'We refined the data columns in a few key ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Columns like `ndeg_de_version` and `rappelguid`, which were part of a versioning
    system, have been removed as they aren’t needed for our project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We combined columns that deal with consumer risks — `risques_encourus_par_le_consommateur`
    and `description_complementaire_du_risque` — for a clearer overview of product
    risks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `date_debut_fin_de_commercialisation` column, which indicates the marketing
    period, has been divided into two separate columns. This split allows for easier
    queries about the start or end of a product’s marketing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ve removed accents from all columns except for links, reference numbers,
    and dates. This is important because some text processing tools struggle with
    accented characters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a detailed look at these changes, check out our transformation script at
    `src/kafka_client/transformations.py`. The updated list of columns is available
    in`src/constants.py` under `DB_FIELDS`.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To avoid sending all the data from the API each time we run the streaming task,
    we define a local json file that contains the last publication date of the latest
    streaming. Then we will use this date as the starting date for our new streaming
    task.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example, suppose that the latest recalled product has a publication
    date of **22 november 2023\.** If we make the hypothesis that all of the recalled
    products infos before this date are already persisted in our Postgres database,
    We can now stream the data starting from the 22 november. Note that there is an
    overlap because we may have a scenario where we didn’t handle all of the data
    of the 22nd of November.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file is saved in `./data/last_processed.json` and has this format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By default the file is an empty json which means that our first streaming task
    will process all of the API records which are 10 000 approximately.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in a production setting this approach of storing the last processed
    date in a local file is not viable and other approaches involving an external
    database or an object storage service may be more suitable.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the kafka streaming can be found on `./src/kafka_client/kafka_stream_data.py`and
    it involves primarily querying the data from the API, making the transformations,
    removing potential duplicates, updating the last publication date and serving
    the data using the kafka producer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to run the kafka service defined the docker-compose defined
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The key highlights from this file are:'
  prefs: []
  type: TYPE_NORMAL
- en: The **kafka** service uses a base image `bitnami/kafka`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We configure the service with only one **broker** which is enough for our small
    project. A Kafka broker is responsible for receiving messages from producers (which
    are the sources of data), storing these messages, and delivering them to consumers
    (which are the sinks or end-users of the data). The broker listens to port 9092
    for internal communication within the cluster and port 9094 for external communication,
    allowing clients outside the Docker network to connect to the Kafka broker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the **volumes** part, we map the local directory `kafka` to the docker container
    directory `/*bitnami/kafka*`to ensure data persistence and a possible inspection
    of Kafka’s data from the host system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We set-up the service **kafka-ui** that uses the docker image `provectuslabs/kafka-ui:latest`
    . This provides a user interface to interact with the Kafka cluster. This is especially
    useful for monitoring and managing Kafka topics and messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure communication between **kafka** and **airflow** which will be run
    as an external service, we will use an external network **airflow-kafka***.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before running the kafka service, let’s create the airflow-kafka network using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now everything is set to finally start our kafka service
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After the services start, visit the kafka-ui at [http://localhost:8800/](http://localhost:8000/).
    Normally you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/972914181dc72291fbbb4e232bdb72d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the Kafka UI. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Next we will create our topic that will contain the API messages. Click on Topics
    on the left and then Add a topic at the top left. Our topic will be called **rappel_conso**
    and since we have only one broker we set the **replication factor** to **1**.
    We will also set the **partitions** number to **1** since we will have only one
    consumer thread at a time so we won’t need any parallelism. Finally, we can set
    the time to retain data to a small number like one hour since we will run the
    spark job right after the kafka streaming task, so we won’t need to retain the
    data for a long time in the kafka topic.
  prefs: []
  type: TYPE_NORMAL
- en: Postgres set-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before setting-up our spark and airflow configurations, let’s create the Postgres
    database that will persist our API data. I used the **pgadmin 4** tool for this
    task, however any other Postgres development platform can do the job.
  prefs: []
  type: TYPE_NORMAL
- en: To install postgres and pgadmin, visit this link [https://www.postgresql.org/download/](https://www.postgresql.org/download/)
    and get the packages following your operating system. Then when installing postgres,
    you need to setup a password that we will need later to connect to the database
    from the spark environment. You can also leave the port at 5432.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your installation has succeeded, you can start pgadmin and you should observe
    something like this window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e80944d60d82b2e7a4b068ab4281ebb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of pgAdmin interface. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have a lot of columns for the table we want to create, we chose to
    create the table and add its columns with a script using **psycopg2,** a PostgreSQL
    database adapter for Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can run the script with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the script I saved the postgres password as environment variable
    and name it *POSTGRES_PASSWORD.* So if you use another method to access the password
    you need to modify the script accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Set-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having set-up our Postgres database, let’s delve into the details of the spark
    job. The goal is to stream the data from the Kafka topic *rappel_conso* to the
    Postgres table *rappel_conso_table.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the key highlights and functionalities of the spark job:'
  prefs: []
  type: TYPE_NORMAL
- en: First we create the Spark session
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 2\. The `create_initial_dataframe` function ingests streaming data from the
    Kafka topic using Spark's structured streaming.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Once the data is ingested, `create_final_dataframe`transforms it. It applies
    a schema (defined by the columns **DB_FIELDS**) to the incoming JSON data, ensuring
    that the data is structured and ready for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 4\. The `start_streaming`function reads existing data from the database, compares
    it with the incoming stream, and appends new records.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The complete code for the Spark job is in the file `src/spark_pgsql/spark_streaming.py`.
    We will use the Airflow DockerOperator to run this job, as explained in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the process of creating the Docker image we need to run our
    Spark job. Here’s the Dockerfile for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In this Dockerfile, we start with the `bitnami/spark` image as our base. It's
    a ready-to-use Spark image. We then install `py4j`, a tool needed for Spark to
    work with Python.
  prefs: []
  type: TYPE_NORMAL
- en: The environment variables `POSTGRES_DOCKER_USER` and `POSTGRES_PASSWORD` are
    set up for connecting to a PostgreSQL database. Since our database is on the host
    machine, we use `host.docker.internal` as the user. This allows our Docker container
    to access services on the host, in this case, the PostgreSQL database. The password
    for PostgreSQL is passed as a build argument, so it's not hard-coded into the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that this approach, especially passing the database password
    at build time, might not be secure for production environments. It could potentially
    expose sensitive information. In such cases, more secure methods like Docker BuildKit
    should be considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s build the Docker image for Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This command will build the image `rappel-conso/spark:latest`. This image includes
    everything needed to run our Spark job and will be used by Airflow’s DockerOperator
    to execute the job. Remember to replace `$POSTGRES_PASSWORD` with your actual
    PostgreSQL password when running this command.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As said earlier, Apache Airflow serves as the orchestration tool in the data
    pipeline. It is responsible for scheduling and managing the workflow of the tasks,
    ensuring they are executed in a specified order and under defined conditions.
    In our system, Airflow is used to automate the data flow from streaming with Kafka
    to processing with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at the Directed Acyclic Graph (DAG) that will outline the
    sequence and dependencies of tasks, enabling Airflow to manage their execution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here are the key elements from this configuration
  prefs: []
  type: TYPE_NORMAL
- en: The tasks are set to execute daily.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first task is the **Kafka Stream Task.** It is **i**mplemented using the
    **PythonOperator** to run the Kafka streaming function. This task streams data
    from the *RappelConso* API into a Kafka topic, initiating the data processing
    workflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The downstream task is the **Spark Stream Task.** It uses the **DockerOperator**
    for execution. It runs a Docker container with our custom Spark image, tasked
    with processing the data received from Kafka.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tasks are arranged sequentially, where the Kafka streaming task precedes
    the Spark processing task. This order is crucial to ensure that data is first
    streamed and loaded into Kafka before being processed by Spark.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About the DockerOperator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using docker operator allow us to run docker-containers that correspond to our
    tasks. The main advantage of this approach is easier package management, better
    isolation and enhanced testability. We will demonstrate the use of this operator
    with the spark streaming task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key details about the docker operator for the spark streaming
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the image `rappel-conso/spark:latest` specified in the *Spark Set-up*
    section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The command will run the Spark submit command inside the container, specifying
    the master as local, including necessary packages for PostgreSQL and Kafka integration,
    and pointing to the `spark_streaming.py` script that contains the logic for the
    Spark job.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**docker_url** represents the url of the host running the docker daemon. The
    natural solution is to set it as `unix://var/run/docker.sock`and to mount the
    `var/run/docker.sock` in the airflow docker container. One problem we had with
    this approach is a permission error to use the socket file inside the airflow
    container. A common workaround, changing permissions with `chmod 777 var/run/docker.sock`,
    poses significant security risks. To circumvent this, we implemented a more secure
    solution using `bobrik/socat` as a docker-proxy. This proxy, defined in a Docker
    Compose service, listens on TCP port 2375 and forwards requests to the Docker
    socket:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the DockerOperator, we can access the host docker `/var/run/docker.sock`
    via the`tcp://docker-proxy:2375` url, as described [here](https://medium.com/@benjcabalonajr_56579/using-docker-operator-on-airflow-running-inside-a-docker-container-7df5286daaa5)
    and [here](https://stackoverflow.com/a/70100729).
  prefs: []
  type: TYPE_NORMAL
- en: Finally we set the network mode to **airflow-kafka.** This allows us to use
    the same network as the proxy and the docker running kafka. This is crucial since
    the spark job will consume the data from the kafka topic so we must ensure that
    both containers are able to communicate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After defining the logic of our DAG, let’s understand now the airflow services
    configuration in the `docker-compose-airflow.yaml` file.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow Configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The compose file for airflow was adapted from the official apache airflow docker-compose
    file. You can have a look at the original file by visiting this [link](https://airflow.apache.org/docs/apache-airflow/2.7.3/docker-compose.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: As pointed out by this [article](https://datatalks.club/blog/how-to-setup-lightweight-local-version-for-airflow.html),
    this proposed version of airflow is highly resource-intensive mainly because the
    core-executor is set to **CeleryExecutor** that is more adapted for distributed
    and large-scale data processing tasks. Since we have a small workload, using a
    single-noded **LocalExecutor** is enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an overview of the changes we made on the docker-compose configuration
    of airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: We set the environment variable AIRFLOW__CORE__EXECUTOR to **LocalExecutor**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We removed the services **airflow-worker** and **flower** because they only
    work for the Celery executor. We also removed the **redis** caching service since
    it works as a backend for celery. We also won’t use the **airflow-triggerer**
    so we remove it too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We replaced the base image `${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}` for
    the remaining services, mainly the **scheduler** and the **webserver**, by a custom
    image that we will build when running the docker-compose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We mounted the necessary volumes that are needed by airflow. AIRFLOW_PROJ_DIR
    designates the airflow project directory that we will define later. We also set
    the network as **airflow-kafka** to be able to communicate with the kafka boostrap
    servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to create some environment variables that will be used by docker-compose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Where `AIRFLOW_UID` represents the User ID in Airflow containers and `AIRFLOW_PROJ_DIR`
    represents the airflow project directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now everything is set-up to run your airflow service. You can start it with
    this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Then to access the airflow user interface you can visit this url `http://localhost:8080`
    .
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6580d6371fbac338bc4f5ecc512f9269.png)'
  prefs: []
  type: TYPE_IMG
- en: Sign-in window on Airflow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the username and password are **airflow** for both. After signing
    in, you will see a list of Dags that come with airflow. Look for the dag of our
    project **kafka_spark_dag** and click on it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdec2476e37b4256db6b0be23b8286af.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the task window in airflow. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start the task by clicking on the button next to **DAG: kafka_spark_dag.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can check the status of your tasks in the Graph tab. A task is done
    when it turns green. So, when everything is finished, it should look something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b068e823ce2776a714df06f36dcbecfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the `rappel_conso_table` is filled with data, use the following
    SQL query in the pgAdmin Query Tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: When I ran this in January 2024, the query returned a total of 10022 rows. Your
    results should be around this number as well.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article has successfully demonstrated the steps to build a basic yet functional
    data engineering pipeline using Kafka, Airflow, Spark, PostgreSQL, and Docker.
    Aimed primarily at beginners and those new to the field of data engineering, it
    provides a hands-on approach to understanding and implementing key concepts in
    data streaming, processing, and storage.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this guide, we’ve covered each component of the pipeline in detail,
    from setting up Kafka for data streaming to using Airflow for task orchestration,
    and from processing data with Spark to storing it in PostgreSQL. The use of Docker
    throughout the project simplifies the setup and ensures consistency across different
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that while this setup is ideal for learning and small-scale
    projects, scaling it for production use would require additional considerations,
    especially in terms of security and performance optimization. Future enhancements
    could include integrating more advanced data processing techniques, exploring
    real-time analytics, or even expanding the pipeline to incorporate more complex
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this project serves as a practical starting point for those looking
    to get their hands dirty with data engineering. It lays the groundwork for understanding
    the basics, providing a solid foundation for further exploration in the field.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we’ll explore how to effectively use the data stored in
    our PostgreSQL database. We’ll introduce agents powered by Large Language Models
    (LLMs) and a variety of tools that enable us to interact with the database using
    natural language queries. So, stay tuned !
  prefs: []
  type: TYPE_NORMAL
- en: To reach out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
