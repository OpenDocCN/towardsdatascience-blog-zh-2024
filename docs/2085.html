<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The MMD-Critic Method, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The MMD-Critic Method, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-mmd-critic-method-explained-c6a77f2dbf18?source=collection_archive---------7-----------------------#2024-08-27">https://towardsdatascience.com/the-mmd-critic-method-explained-c6a77f2dbf18?source=collection_archive---------7-----------------------#2024-08-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="08d7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A powerful yet under-the-radar method for data summarization and explainable AI</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@physboom?source=post_page---byline--c6a77f2dbf18--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Chak" class="l ep by dd de cx" src="../Images/88881eb5a7c8f08c15555bc8c3c613d3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*mIH-jXvH_AeExbLqQxTJxw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c6a77f2dbf18--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@physboom?source=post_page---byline--c6a77f2dbf18--------------------------------" rel="noopener follow">Matthew Chak</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c6a77f2dbf18--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="f2d3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Despite being a powerful tool for data summarization, the MMD-Critic method has a surprising lack of both usage and “coverage”. Perhaps this is because simpler and more established methods for data summarization exist (e.g. K-medoids, see [1] or, more simply, the <a class="af ne" href="https://en.wikipedia.org/wiki/K-medoids" rel="noopener ugc nofollow" target="_blank">Wikipedia page</a>), or perhaps this is because no Python package for the method existed (before now). Regardless, the results presented in the <a class="af ne" href="https://papers.nips.cc/paper_files/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html" rel="noopener ugc nofollow" target="_blank">original paper</a> [2] warrant more use than MMD-Critic has currently. As such, I’ll explain the MMD-Critic method here with as much clarity as possible. I’ve also published an <a class="af ne" href="https://pypi.org/project/mmd-critic/" rel="noopener ugc nofollow" target="_blank">open-source Python package</a> with an implementation of the technique so you can use it easily.</p><h1 id="b6df" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Prototypes and Criticisms</h1><p id="c079" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">Before jumping into the MMD-Critic method itself, it’s worth discussing what exactly we’re trying to accomplish. Ultimately, we wish to take a dataset and find examples that are representative of the data (<strong class="mk fr">prototypes</strong>), as well as edge-case examples that may confound our machine learning models (<strong class="mk fr">criticisms</strong>).</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh oi"><img src="../Images/71c22de7c81736578e26d81da3a21ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uOb-XEpcfFalVENs.jpg"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Prototypes and criticisms for the MNIST dataset, taken from [2].</figcaption></figure><p id="453c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">There are many reasons why this may be useful:</p><ul class=""><li id="a868" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd oz pa pb bk">We can get a very nice summarized view of our dataset by seeing both stereotypical and atypical examples</li><li id="8ca1" class="mi mj fq mk b go pc mm mn gr pd mp mq mr pe mt mu mv pf mx my mz pg nb nc nd oz pa pb bk">We can test models on the criticisms to see how they handle edge cases (this is, for obvious reasons, very important)</li><li id="7a96" class="mi mj fq mk b go pc mm mn gr pd mp mq mr pe mt mu mv pf mx my mz pg nb nc nd oz pa pb bk">Though perhaps not as useful, we can use prototypes to create a naturally explainable K-means-esque algorithm wherein the closest prototype to the new data point is used to label it. Then explanations are simple since we just show the user the most similar data point.</li><li id="df20" class="mi mj fq mk b go pc mm mn gr pd mp mq mr pe mt mu mv pf mx my mz pg nb nc nd oz pa pb bk">More</li></ul><p id="6430" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You can see section 6.3 in <a class="af ne" href="https://f0nzie.github.io/interpretable_ml-rsuite/proto.html#examples-5" rel="noopener ugc nofollow" target="_blank">this book</a> for more info on the applications of this (and for a decent explanation of MMD-Critic as well), but it suffices to say that finding these examples is useful for a wide variety of reasons. MMD-Critic allows us to do this.</p><h1 id="7188" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Maximal Mean Discrepancy</h1><p id="4b52" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">I unfortunately cannot claim to have a hyper-rigorous understanding of Maximal Mean Discrepancy (MMD), as such an understanding would require a strong background in functional analysis. If you have such a background, you can find the paper that introduced the measure <a class="af ne" href="https://jmlr.csail.mit.edu/papers/volume13/gretton12a/gretton12a.pdf" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="ac1b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In simple terms though, MMD is a way to determine the difference between two probability distributions. Formally, for two probability distributions <em class="ph">P </em>and <em class="ph">Q</em>, we define the MMD of the two as</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pi"><img src="../Images/806f69f4f5fb7bdfddc1e2b7e198f203.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*66iZ7AgaKzwRPAl6iopfYQ.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The formula for the MMD of two distributions P, Q</figcaption></figure><p id="cba2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here, <em class="ph">F</em> is any <strong class="mk fr">function space</strong> — that is, any set of functions with the same domain and codomain. Note also that the notation <em class="ph">x~P </em>means that we are treating <em class="ph">x</em> as if it’s a random variable drawn from the distribution <em class="ph">P</em> — that is, <em class="ph">x</em> is <strong class="mk fr">described by</strong> <em class="ph">P</em>. This formula thus finds the highest difference in the expected values of <em class="ph">X</em> and <em class="ph">Y</em> when they are transformed by some function from our space <em class="ph">F</em>.</p><p id="1690" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This may be a little hard to wrap your head around, but here’s an example. Suppose that <em class="ph">X</em> is <code class="cx pj pk pl pm b">Uniform(0, 1)</code> (i.e. a distribution that is equivalent to picking a random number from 0 to 1), and <em class="ph">Y</em> is <code class="cx pj pk pl pm b">Uniform(-1, 1)</code> . Let’s also let <em class="ph">F</em> be a fairly simple family containing three functions — <em class="ph">f(x) = 0</em>, <em class="ph">f(x) = x</em>, and <em class="ph">f(x) = x²</em>. Iterating over each function in our space, we get:</p><ol class=""><li id="d5ed" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pn pa pb bk">In the <em class="ph">f(x)</em> <em class="ph">= 0</em> case, E[f(x)] when <em class="ph">x ~ P</em> is 0 since no matter what <em class="ph">x</em> we choose, <em class="ph">f(x)</em> will be 0. The same holds for when <em class="ph">x ~ Q</em>. Thus, we get a mean discrepancy of 0</li><li id="3b57" class="mi mj fq mk b go pc mm mn gr pd mp mq mr pe mt mu mv pf mx my mz pg nb nc nd pn pa pb bk">In the <em class="ph">f(x) = x</em> case, we have E[f(x)] = 0.5 for the <em class="ph">P</em> case and 0 for the <em class="ph">Q</em> case, so our mean discrepancy is 0.5</li><li id="0a4d" class="mi mj fq mk b go pc mm mn gr pd mp mq mr pe mt mu mv pf mx my mz pg nb nc nd pn pa pb bk">In the <em class="ph">f(x) = x² </em>case, we note that</li></ol><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh po"><img src="../Images/ea72888c6295343ec4a233c883265a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*ztYgWoJXPgkfULnTAUK--w.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Formula for the expected value of a random variable <em class="pp">x</em> transformed by a function f</figcaption></figure><p id="5e72" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">thus in the P case, we get</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pq"><img src="../Images/f96988df2e03471d79f8585d36fc3596.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*AEtBFpFxaIIo0bYyP-ZzxA.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Expected value of f(x) under the distribution P</figcaption></figure><p id="50c5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and in the Q case, we get</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pr"><img src="../Images/25a4e021d3c2c5b36bcaaa3c774de110.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*a8S82-aVsE32X_dlQy16wQ.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Expected value of f(x) under the distribution Q</figcaption></figure><p id="1902" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">thus our discrepancy in this case is also 0. The supremum over our function space is thus 0.5, so that’s our MMD.</p><p id="4a9f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You may now notice a few problems with our MMD. It seems highly dependent on our choice of function space and also appears highly expensive (or even impossible) to compute for a large or infinite function space. Not only that, but it also requires us to know our distributions <em class="ph">P</em> and <em class="ph">Q</em>, which is not realistic.</p><p id="01e3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The latter problem is easily solvable, as we can rewrite our MMD metric to use estimates of P and Q based on our dataset:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh ps"><img src="../Images/2140a40093f3dd0d76850321ceb07100.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*ZQXVr7lIUkFZuRMc8IkCaQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">MMD using estimates of P and Q</figcaption></figure><p id="cdf5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here, our <em class="ph">x</em>’s are our samples from the dataset drawing from <em class="ph">P</em>, and the <em class="ph">y</em>’s are the samples drawn from <em class="ph">Q</em>.</p><p id="f1e5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The first two problems are solvable with a bit of extra math. Without going into too much detail, it turns out that if <em class="ph">F</em> is something called a <strong class="mk fr">Reproducing Kernel Hilbert Space</strong> (RKHS), we know what function is going to give us our MMD in advance. Namely, it’s the following function, called the <strong class="mk fr">witness function</strong>:</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pt"><img src="../Images/e46b5d340b5f3db6a69efe75e2b8cc47.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*cYxW3caOMwwxLVtVzXtCTA.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Our optimal f(x) in an RKHS</figcaption></figure><p id="0f63" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">where <em class="ph">k</em> is the <strong class="mk fr">kernel</strong> (inner product) associated with the RKHS<strong class="mk fr"><em class="ph">¹</em></strong>. Intuitively, this function “witnesses” the discrepancy between <em class="ph">P</em> and <em class="ph">Q</em> at the point <em class="ph">x</em>.</p><p id="c18b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We thus only need to choose a sufficiently expressive RKHS/kernel — usually, the RBF kernel is used which has the kernel function</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pu"><img src="../Images/f5ea568dd5266a59359ef3961d188f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*AhHmGD4ZCWkOlxiDxA55Fw.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The RBF kernel, where sigma is a hyperparameter</figcaption></figure><p id="44e2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This generally gets fairly intuitive results. Here, for instance, is the plot of the witness function with the RBF kernel when estimated (in the same way as mentioned before — that is, replacing expectations with a sum) on two datasets drawn from <code class="cx pj pk pl pm b">Uniform(-0.5, 0.5)</code> and <code class="cx pj pk pl pm b">Uniform(-1, 1)</code> :</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh pv"><img src="../Images/fd302a4cc896b43064206d874d7409da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*4Lc_CKM5jTYmQPJ8zlLtog.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Values of the witness function at different points for two uniform distributions</figcaption></figure><p id="83cc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The code for generating the above graph is here:</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="2fc4" class="pz ng fq pm b bg qa qb l qc qd">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>def rbf(v1, v2, sigma=0.5):<br/>  return np.exp(-(v2 - v1) ** 2/(2 * sigma**0.5))<br/><br/>def comp_wit_fn(x, d1, d2):<br/>  return 1/len(d1) * sum([rbf(x, dp) for dp in d1]) - 1/len(d2) * sum([rbf(x, dp) for dp in d2])<br/><br/>low1, high1 = -0.5, 0.5  # Range for the first uniform distribution<br/>low2, high2 = -1, 1  # Range for the second uniform distribution<br/><br/># Generate data for the uniform distributions<br/>data1 = np.random.uniform(low1, high1, 10000)<br/>data2 = np.random.uniform(low2, high2, 10000)<br/><br/># Generate a range of x values for which to compute comp_wit_fn<br/>x_values = np.linspace(min(low1 * 2, low2 * 2), max(high1 * 2, high2 * 2), 100)<br/><br/>comp_wit_values = [comp_wit_fn(x, data1, data2) for x in x_values]<br/>sns.kdeplot(data1, label=f'Uniform({low1}, {high1})', color='blue', fill=True)<br/>sns.kdeplot(data2, label=f'Uniform({low2}, {high2})', color='red', fill=True)<br/>plt.plot(x_values, comp_wit_values, label='Witness Function', color='green')<br/><br/>plt.xlabel('Value')<br/>plt.ylabel('Density / Wit Fn')<br/>plt.legend()<br/>plt.show()</span></pre><h1 id="6dd8" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">The MMD-Critic Method, Finally</h1><p id="7750" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The idea behind MMD-Critic is now fairly simple — if we want to find <em class="ph">k</em> prototypes, we need to find the set of prototypes that best matches the distribution of the original dataset given by their squared MMD. In other words, we wish to find a subset <em class="ph">P </em>of cardinality <em class="ph">k</em> of our dataset that minimizes <em class="ph">MMD²(F, X, P)</em>. Without going into too much detail about why, the square MMD is given by</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qe"><img src="../Images/0dff52d10b914ee9e6082e9b47cb6292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*9wIp2qy3GAb4AJhsrBc7Gg.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The square MMD metric, with X ~ P, Y ~ Q, and k the kernel for our RKHS F</figcaption></figure><p id="7436" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">After finding these prototypes, we then select the points where the hypothetical distribution of our prototypes is most different from our dataset distribution as criticisms. As we’ve seen before, the difference between two distributions at a point can be measured by our witness function, so we just find points that maximize its absolute value in the context of <em class="ph">X</em> and <em class="ph">P</em>. In other words, we define our criticism “score” as</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qf"><img src="../Images/d19b02d21f693952776695457da054ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*txxiaLw7s58nIiL31wCmeA.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The “score” for a criticism c</figcaption></figure><p id="52e4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Or, in the more usable approximate form,</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qg"><img src="../Images/2a763ae077e95c0041dba13b5c4fab86.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*fTDdpn5PlZJwMB3hQ96K1w.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">The approximated S(c) for a criticism c</figcaption></figure><p id="51a1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Then, to find our desired amount of criticisms, say <em class="ph">m</em> of them, we simply wish to find the set <em class="ph">C</em> of size <em class="ph">m</em> that maximizes</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qh"><img src="../Images/8eacec3442f5cc088cb6704f587a7190.png" data-original-src="https://miro.medium.com/v2/resize:fit:142/format:webp/1*gWYJVJnjZzZMtJbNspzIZA.png"/></div></figure><p id="4205" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To promote picking more varied criticisms, the paper also suggests adding a regularizer term that encourages selected criticisms to be as far apart as possible. The suggested regularizer in the paper is the log determinant regularizer, though this is not required. I won’t go into much detail here since it’s not critical, but the paper suggests reading [6]<strong class="mk fr"><em class="ph">²</em></strong>.</p><p id="480a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can thus implement an <strong class="mk fr"><em class="ph">extremely </em>naive</strong> MMD-Critic without criticism regularization as follows (<strong class="mk fr">do NOT use this</strong>):</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="0a33" class="pz ng fq pm b bg qa qb l qc qd">import math<br/>import itertools<br/><br/>def euc_distance(p1, p2):<br/>    return math.sqrt(sum((x - y) ** 2 for x, y in zip(p1, p2)))<br/><br/>def rbf(v1, v2, sigma=0.5):<br/>  return math.exp(-euc_distance(v1, v2) ** 2/(2 * sigma**0.5))<br/><br/>def mmd_sq(X, Y, sigma=0.5):<br/>  sm_xx = 0<br/>  for x in X:<br/>    for x2 in X:<br/>      sm_xx += rbf(x, x2, sigma)<br/><br/>  sm_xy = 0<br/>  for x in X:<br/>    for y in Y:<br/>      sm_xy += rbf(x, y, sigma)<br/><br/>  sm_yy = 0<br/>  for y in Y:<br/>    for y2 in Y:<br/>      sm_yy += rbf(y, y2, sigma)<br/><br/>  return 1/(len(X) ** 2) * sm_xx \<br/>          - 2/(len(X) * len(Y)) * sm_xy \<br/>          + 1/(len(Y) ** 2) * sm_yy<br/><br/>def select_protos(X, n, sigma=0.5):<br/>  min_score, min_sub = math.inf, None<br/>  for subset in itertools.combinations(X, n):<br/>    new_mmd = mmd_sq(X, subset, sigma)<br/>    if new_mmd &lt; min_score:<br/>      min_score = new_mmd<br/>      min_sub = subset<br/>  return min_sub<br/><br/>def criticism_score(criticism, prototypes, X, sigma=0.5):<br/>  return abs(1/len(X) * sum([rbf(criticism, x, sigma) for x in X])\<br/>             - 1/len(prototypes) * sum([rbf(criticism, p, sigma) for p in prototypes]))<br/>  <br/>def select_criticisms(X, P, n, sigma=0.5):<br/>  candidates = [c for c in X if c not in P]<br/>  max_score, crits = -math.inf, []<br/>  for subset in itertools.combinations(candidates, n):<br/>    new_score = sum([criticism_score(c, P, X, sigma) for c in subset])<br/>    if new_score &gt; max_score:<br/>      max_score = new_score<br/>      crits = subset<br/><br/>  return crits</span></pre><h1 id="64cb" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Optimizing MMD-Critic</h1><p id="82d3" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">The above implementation is so impractical that, when I ran it, I failed to find 5 prototypes in a dataset with 25 points in a reasonable time. This is because our MMD calculation is <em class="ph">O(max(|X|, |Y|)²)</em>, and iterating over every length-n subset is <em class="ph">O(C(|X|, n)) </em>(where C is the choose function), which gives us a horrendous runtime complexity.</p><p id="6cf2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Disregarding using more efficient computation methods (e.g. using pure numpy/numexpr/matrix calculations instead of loops/whatever) and caching repeated calculations, there are a few optimizations we can make on the theoretical level. Firstly, the most obvious slowdown we have is looping over the <em class="ph">C(|X|, n)</em> subsets in our prototype and criticism methods. Instead of that, we can use an approximation that loops <em class="ph">n</em> times, greedily selecting the best prototype each time. This allows us to change our prototype selection code to</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="9e50" class="pz ng fq pm b bg qa qb l qc qd">def select_protos(X, n, sigma=0.5):<br/>  protos = []<br/>  for _ in range(n):<br/>    min_score, min_proto = math.inf, None<br/>    for cand in X:<br/>      if cand in protos:<br/>        continue<br/>      new_score = mmd_sq(X, protos + [cand], sigma)<br/>      if new_score &lt; min_score:<br/>        min_score = new_score<br/>        min_proto = cand<br/>    protos.append(min_proto)<br/>  return protos</span></pre><p id="fbff" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and similar for the criticisms.</p><p id="22c9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">There’s one other important lemma that makes this problem much more optimizable. It turns out that by changing our prototype selection into a minimization problem and adding a regularization term to the cost, we can compute the cost function very efficiently with matrix operations. I won’t go into much detail here, but you can check out the original paper for details.</p><h1 id="2f02" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Playing With the <code class="cx pj pk pl pm b">MMD-Critic </code>Package</h1><p id="caa6" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">Now that we understand the MMD-Critic method, we can finally play with it! You can install it by running</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="be9c" class="pz ng fq pm b bg qa qb l qc qd">pip install mmd-critic</span></pre><p id="8fc0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The implementation in the package itself is much faster than the one presented here, so don’t worry.</p><p id="a18f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can run a fairly simple example using blobs as such:</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="415f" class="pz ng fq pm b bg qa qb l qc qd">from sklearn.datasets import make_blobs<br/>from mmd_critic import MMDCritic<br/>from mmd_critic.kernels import RBFKernel<br/><br/>n_samples = 50  # Total number of samples<br/>centers = 4       # Number of clusters<br/>cluster_std = 1 # Standard deviation of the clusters<br/><br/>X, _ = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, n_features=2, random_state=42)<br/>X = X.tolist()<br/><br/># MMD critic with the kernel used for the prototypes being an RBF with sigma=1,<br/># for the criticisms one with sigma=0.025<br/>critic = MMDCritic(X, RBFKernel(1), RBFKernel(0.025))<br/>protos, _ = critic.select_prototypes(centers)<br/>criticisms, _ = critic.select_criticisms(10, protos)</span></pre><p id="8177" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Then plotting the points and criticisms gets us</p><figure class="oj ok ol om on oo og oh paragraph-image"><div class="og oh qi"><img src="../Images/ad6879f3f0b021cff94f57e71b28503f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*YIHNIkIpdABflz9Ds9En6A.png"/></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Plotting the found prototypes (green) and criticisms (red)</figcaption></figure><p id="5807" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You’ll notice that I provided the option to use a separate kernel for prototype and criticism selection. This is because I’ve found that results for criticisms especially can be <em class="ph">extremely</em> sensitive to the sigma hyperparameter. This is an unfortunate limitation of the MMD Critic method and kernel methods in general. Overall, I’ve found good results using a large sigma for prototypes and a smaller one for criticisms.</p><p id="6f5a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can also, of course, use a more complicated dataset. Here, for instance, is the method used on MNIST<strong class="mk fr"><em class="ph">³</em></strong>:</p><pre class="oj ok ol om on pw pm px bp py bb bk"><span id="f45d" class="pz ng fq pm b bg qa qb l qc qd">from sklearn.datasets import fetch_openml<br/>import numpy as np<br/>from mmd_critic import MMDCritic<br/>from mmd_critic.kernels import RBFKernel<br/><br/># Load MNIST data<br/>mnist = fetch_openml('mnist_784', version=1)<br/>images = (mnist['data'].astype(np.float32)).to_numpy() / 255.0<br/>labels = mnist['target'].astype(np.int64)<br/><br/><br/>critic = MMDCritic(images[:15000], RBFKernel(2.5), RBFKernel(0.025))<br/>protos, _ = critic.select_prototypes(40)<br/>criticisms, _ = critic.select_criticisms(40, protos)</span></pre><p id="fa36" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">which gets us the following prototypes</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qj"><img src="../Images/6b5e036ce193ebc12d8db963cca7fcfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9S_6oE9ZPD0vqfPuwcTYPw.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Prototypes found by MMD critic for MNIST. MNIST is free for commercial use under the GPL-3.0 License.</figcaption></figure><p id="509f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and criticisms</p><figure class="oj ok ol om on oo og oh paragraph-image"><div role="button" tabindex="0" class="op oq ed or bh os"><div class="og oh qj"><img src="../Images/37ca1ea60129a1dc22e293b08e4442e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lxxEhXK53kQSvt3tPNlujQ.png"/></div></div><figcaption class="ou ov ow og oh ox oy bf b bg z dx">Criticisms found by the MMD Critic method</figcaption></figure><p id="0c8b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Pretty neat, huh?</p><h1 id="b899" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusions</h1><p id="2d0f" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">And that’s about it for the MMD-Critic method. It is quite simple at the core, and it is nice to use save for having to fiddle with the Sigma hyperparameter. I hope that the newly released Python package gives it more use.</p><p id="460f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="ph">Please contact mchak@calpoly.edu for any inquiries. All images by author unless stated otherwise.</em></p></div></div></div><div class="ab cb qk ql qm qn" role="separator"><span class="qo by bm qp qq qr"/><span class="qo by bm qp qq qr"/><span class="qo by bm qp qq"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="cb94" class="qs ng fq bf nh qt qu qv nk qw qx qy nn mr qz ra rb mv rc rd re mz rf rg rh ri bk">Footnotes</h2><p id="d8c8" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">[1] You may be familiar with RKHSs and kernels if you’ve ever studied SVMs and the kernel trick — the kernels used there are just inner products in <em class="ph">some</em> RKHS. The most common is the RBF kernel, for which the associated RKHS of functions is an infinite-dimensional set of smooth functions.</p><p id="7e45" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[2] I have not read this source beyond a brief skim. It seems mostly irrelevant, and the log determinant regularizer is fairly simple to implement. If you want to read it though, go for it.</p><p id="4f64" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[3] For legal reasons, you can find a repository with the MNIST dataset <a class="af ne" href="https://github.com/sharmaroshan/MNIST-Dataset/tree/master" rel="noopener ugc nofollow" target="_blank">here</a>. It is free for commercial use under the GPL-3.0 License.</p><h2 id="4de5" class="qs ng fq bf nh qt qu qv nk qw qx qy nn mr qz ra rb mv rc rd re mz rf rg rh ri bk">References</h2><p id="9597" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">[1] <a class="af ne" href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316801" rel="noopener ugc nofollow" target="_blank">https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316801</a><br/>[2]<a class="af ne" href="https://papers.nips.cc/paper_files/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html" rel="noopener ugc nofollow" target="_blank">https://proceedings.neurips.cc/paper_files/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf</a><br/>[3] <a class="af ne" href="https://f0nzie.github.io/interpretable_ml-rsuite/proto.html#examples-5" rel="noopener ugc nofollow" target="_blank">https://f0nzie.github.io/interpretable_ml-rsuite/proto.html#examples-5</a><br/>[4] <a class="af ne" href="https://jmlr.csail.mit.edu/papers/volume13/gretton12a/gretton12a.pdf" rel="noopener ugc nofollow" target="_blank">https://jmlr.csail.mit.edu/papers/volume13/gretton12a/gretton12a.pdf</a><br/>[5] <a class="af ne" href="https://www.stat.cmu.edu/~ryantibs/journalclub/mmd.pdf" rel="noopener ugc nofollow" target="_blank">https://www.stat.cmu.edu/~ryantibs/journalclub/mmd.pdf</a><br/>[6] <a class="af ne" href="https://jmlr.org/papers/volume9/krause08a/krause08a.pdf" rel="noopener ugc nofollow" target="_blank">https://jmlr.org/papers/volume9/krause08a/krause08a.pdf</a></p></div></div></div></div>    
</body>
</html>