- en: 'Paper Walkthrough: Vision Transformer (ViT)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13](https://towardsdatascience.com/paper-walkthrough-vision-transformer-vit-c5dcf76f1a7a?source=collection_archive---------2-----------------------#2024-08-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring Vision Transformer (ViT) through PyTorch Implementation from Scratch.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Muhammad
    Ardi](../Images/b59b3752bc33df0166eea834bbdb122f.png)](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    [Muhammad Ardi](https://medium.com/@muhammad_ardi?source=post_page---byline--c5dcf76f1a7a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c5dcf76f1a7a--------------------------------)
    ·19 min read·Aug 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00cee3371ad23d4932c8e123ee815157.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [v2osk](https://unsplash.com/@v2osk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vision Transformer — or commonly abbreviated as ViT — can be perceived as a
    breakthrough in the field of computer vision. When it comes to vision-related
    tasks, it is commonly addressed using CNN-based models which so far always perform
    better than any other type of neural networks. It wasn’t until 2020, when a paper
    titled “*An Image is Worth 16×16 Words: Transformers for Image Recognition at
    Scale*” written by Dosovitskiy et al. [1] was published, which offers better capability
    than CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: A single convolution layer in CNN works by extracting features using kernels.
    Since the size of a kernel is relatively small as compared to the input image,
    hence it can only capture information contained within that small region. In other
    words, we can simply say that it focuses on extracting local features. To understand
    the global context of an image, a stack of multiple convolution layers is required.
    This problem is addressed by ViT as it directly captures global information from
    the initial layer. Thus, stacking multiple layers in ViT results in even more
    comprehensive information extraction.
  prefs: []
  type: TYPE_NORMAL
