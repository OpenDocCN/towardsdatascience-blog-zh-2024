<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Beyond the Blind Zone</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Beyond the Blind Zone</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-the-blind-zone-706ba4b171c5?source=collection_archive---------10-----------------------#2024-04-05">https://towardsdatascience.com/beyond-the-blind-zone-706ba4b171c5?source=collection_archive---------10-----------------------#2024-04-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="89bf" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Inpainting radar gaps with deep learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@frasertheking?source=post_page---byline--706ba4b171c5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Fraser King" class="l ep by dd de cx" src="../Images/7d24cfc8432660d0a09c0acabd79ebb6.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9KaPKk_S0eXbf000YevLvA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--706ba4b171c5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@frasertheking?source=post_page---byline--706ba4b171c5--------------------------------" rel="noopener follow">Fraser King</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--706ba4b171c5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3fdf81845653284f63fa771d0ef57350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rADKbgXmGjWkY-LfuiUiFA.gif"/></div></div></figure><h1 id="02b7" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Overview</h1><p id="2b62" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">In this post, we review high-level details from our recent work on image inpainting of radar blind zones. We discuss the main science problems, inpainting techniques, model architecture decisions, fidelity metrics, uncertainties, and finish with an analysis of model explainability (XAI), in the hope that this information can help others when planning future, similar projects. This work was recently published in the American Meteorologic Society’s Artificial Intelligence for Earth Sciences (AIES) <a class="af op" href="https://doi.org/10.1175/AIES-D-23-0063.1" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1175/AIES-D-23-0063.1</a>, which we recommend readers view for additional project details.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="83bb" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Motivation</h1><p id="8c34" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Radar observations are powerful sources of information for tasks like precipitation forecasting. These systems are used by millions of people daily to help plan their lives, and their predictive accuracy has large economic impacts to agriculture, tourism, and the outdoor recreation industry. But how do these systems work? In a nutshell, these predictive models relate precipitation rates to power backscatter measurements from a radar signal interacting with falling hydrometeors in the atmosphere (Fig. 1). With a large enough reference dataset, we can take the information produced by the radar profile (along with some atmospheric state variables) and back-out an estimate of surface precipitation.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/428f33fcb7a69a74219ec8f0319157f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFwWo_YRxjHsuSX2-GTVMQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 1:</strong> a) Reflectivity profile showing the power backscattered by the cloud; b) Vertical profile of the snowfall rate estimates in the cloud; and c) surface snowfall rates extrapolated down from the lowest precipitating cloud layer shown in b). Image retrieved from King et al., 2020 (<a class="af op" href="https://doi.org/10.1029/2019EA000776" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1029/2019EA000776</a>).</figcaption></figure><p id="6d1c" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Unlike vertical-pointing surface radar instruments which are stationary, satellites are unrestricted in space and can provide a much richer, comprehensive view of global precipitation patterns due to their orbit. However, unlike surface radars, satellite instruments exhibit a unique type of measurement problem since they point down towards the Earth: a radar blind zone. As implied by its name, the blind zone is a portion of the radar profile that the satellite cannot directly observe. As the downward pointed radar signal reaches the Earth’s surface, the backscatter from the ground produces an attenuated signal that becomes saturated with noise (and therefore unobserved). A comparison between surface and spaceborne radars (and the corresponding blind zone) is illustrated in Fig. 2.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/430ec51b523cca2df63b7702bdcee143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcWQedWtnXL5UXV2bL5Tbw.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 2:</strong> Multi-panel of coincident vertical reflectivity profiles from a surface radar, CloudSat and the Global Precipitation Measurement mission, along with their respective radar blind zones. Image retrieved from Kidd et al., 2021 (<a class="af op" href="https://doi.org/10.3390/rs13091708" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3390/rs13091708</a>).</figcaption></figure><p id="15b1" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">While the size of the blind zone region can vary, it is a common issue on active spaceborne systems (e.g. GPM, CloudSat) that will persist as a source of uncertainty in future Earth observing missions (e.g. EarthCARE and AOS). Additionally, although this region is just a small subset of the full profile (only about 4% of CloudSat’s vertical extent, for instance), this 0–2 km of the atmosphere can contain large quantities of precipitating cloud (Lamer et al., 2020). Therefore, by masking this region, we are potentially missing a large quantity of snow (or overestimating snow in the case of virga), further increasing the uncertainty of an already uncertain estimate of surface snow accumulation.</p><blockquote class="pp pq pr"><p id="4375" class="nt nu ps nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk"><em class="fq">… the blind zone leads to reflectivity being underestimated by up to 1 dB, the number of events being altered by +/- 5% and the precipitation amount being underestimated by 9 to 11 percentage points (Maahn et al., 2014).</em></p></blockquote><p id="d6c6" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk"><strong class="nv fr">What can we do about this?</strong></p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="186c" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Our Solution</h1><p id="0beb" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">At its core, the ideas behind image inpainting have been around for decades for use in things like image restoration (e.g. removing a scratch on a family photo), or object removal (removing a tourist in a picture from your last vacation) (Guillemot and Le Meur, 2013). Originally, this type of repair was a costly endeavour that was done by hand by a trained artist (Fig. 2), but as has become increasingly clear in recent years, computers are quite skilled at this task too (with much shorter training times compared to a human)!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/8fd0c522bdefa88df8d4a0bfc274bba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jR4WpXDCQ_tkueJ7UJvzeg.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 3:</strong> Painting restoration by hand by a trained professional. Image courtesy of Ana Alba.</figcaption></figure><p id="0d73" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">While the first iterations of these techniques like structure-based linear interpolation, texture-synthesis-based Efros interpolation, and diffusion-based Laplace or Navier-Stokes interpolation can work well in certain contexts, they often fell short when dealing with large gaps. The differences between these techniques are shown in Fig. 4 where an elephant mask is inpainted in the center of the image. These techniques rely heavily on information/patterns from the edges of the targeted inpainting region and are often unable to effectively make use information from the scene’s global context when making intelligent predictions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/cfee9ef65e81acff2d979d9d7a73d2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcjZlc29Pb4lirXOLPmbOQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 4:</strong> Object removal application a) mask and inpainting results with methods from different categories; b) anisotropic diffusion, c) exemplar-base; d) patch sparse representation, e) hybrid with one global energy minimization; and f) patch offsets. Image retrieved from Guillemot and Le Meur, 2013 (<a class="af op" href="https://doi.org/10.1109/MSP.2013.2273004" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/MSP.2013.2273004</a>).</figcaption></figure><p id="0097" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">However, the field of computer vision has flourished in recent decades, primarily driven by advances in computing power and the development of newer, more efficient machine learning techniques. Generative approaches using Generative Adversarial Networks (GANs) for instance have been very popular recently with OpenAI’s <a class="af op" href="https://openai.com/dall-e-2" rel="noopener ugc nofollow" target="_blank">DALL-E</a>, or Stability.ai’s <a class="af op" href="https://stability.ai/blog/stable-diffusion-public-release" rel="noopener ugc nofollow" target="_blank">Stable Diffusion</a>, producing incredibly realistic images based on a simple text prompt. There has also been some work previously attempting to use similar methods for inpainting, but there exists a trade-off between realism and fidelity/stability for inpainting taskings (Lugmayr et al., 2017; Geiss and Hardin, 2021).</p><p id="34d2" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">For instance, while you might generate a region of an image that looks great to the human eye, the actual pixel values are not necessarily correct if compared to a reference image and may vary quite a bit based on the provided random noise/seed (Fig. 5). This is not unexpected, though, as these techniques are not necessarily constructed with such constraints in mind and exist for other purposes.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/f6a99a28b7fd4d2291878e4d597054fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQqcWF-jXFOsaqUl6X_CJQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 5:</strong> A set of examples from the <em class="pw">Denoising Diffusion Probabilistic Models (DDPM) project for image inpainting introduced in </em>Lugmayr et al., 2017 (<a class="af op" href="https://doi.org/10.1109/CVPR52688.2022.01117" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/CVPR52688.2022.01117</a>).</figcaption></figure><p id="2040" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Instead, we focus on another machine learning architecture in this work: the U-Net. U-Nets are a class of Convolutional Neural Networks (CNNs), that ingest information in the form of an image (typically) and produce an output that is of the same dimensions. Often used for image segmentation, the encoder-decoder architecture of the U-Net allow the model to learn both local and global features of an image (context that is often quite valuable for correctly interpreting an image’s content during inpainting). We will use this architecture to teach the model to learn about latent features in aloft cloud to predict near surface reflectivity data in the aforementioned radar blind zone.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2a19" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Data</h1><p id="dbb9" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">The data used in this project comes from primary two sources:</p><ol class=""><li id="d253" class="nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo px py pz bk"><a class="af op" href="https://www.arm.gov/capabilities/science-data-products/vaps/kazrarsclcloudsat" rel="noopener ugc nofollow" target="_blank"><strong class="nv fr">ARM KaZR CloudSat-aligned reflectivity</strong></a></li><li id="a5d1" class="nt nu fq nv b go qa nx ny gr qb oa ob oc qc oe of og qd oi oj ok qe om on oo px py pz bk"><a class="af op" href="https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5" rel="noopener ugc nofollow" target="_blank"><strong class="nv fr">ERA-5</strong></a></li></ol><p id="657f" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Both datasets are publicly available (governed by the <a class="af op" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution 4.0 International (CC BY 4.0)</a>) and are collocated at two Arctic locations in northern Alaska, USA at North Slope and Oliktok Point (Fig. 6). Since we are focused on snowfall, we limit our observations to cold periods below 2 degrees C. Further, data is split in contiguous training/validation/testing chunks to avoid overfitting from autocorrelation as shown in Fig. 6.c. We are using radar reflectivity data from the ARM KaZR, along with temperature, specific humidity, u-component wind, and v-component wind from ERA-5.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/d4585b57b89a6d6cfbaff626021b8056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xQAmmuBLDlBrm17YBvGRQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 6:</strong> a) Study site locations; b) surface MET temperature timeseries at NSA and OLI; and c) data splitting methods used for model training. Image by author.</figcaption></figure></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9f28" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Our Model</h1><p id="3707" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">What type of U-Net should be used for this task? We experimented with a variety of different U-Nets in this project, including the UNet++ model from Zhou et al., 2018 and the 3Net+ from Huang et al., 2020. But let’s talk about the model architecture for a moment. For instance, why use these methods over a just a traditional U-Net? First let’s review how U-Nets work.</p><p id="4a81" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">U-Nets are primarily thought of in three parts, an encoder path, a bottleneck layer, and a decoder path. The encoder is responsible for taking your initially high-resolution image, and through a series of convolutions and pooling steps, trades spatial information for rich feature information (learning about the latent context of your image). Depending on how deep your network is, these latent features are encoded most densely in the lowest dimensionality bottleneck layer at the base of the U. At this point, the image array may be only a fraction of the size of the original, and while you’ve lost most of the spatial information, the model has identified a set of embeddings that represent what it sees as key elements. Then, in the decoder path, the reverse occurs. The low-resolution, feature-rich arrays in the bottleneck layer are downsampled until the feature information has been transformed back into spatial information (resulting in a final image that has the same dimensions as the original).</p><p id="f1f6" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">One of the key differences between a U-Net, U-Net++ and 3Net+, is how each variation handles skip connections (Fig. 7). Skip connections allow these models to “skip” some data directly across the encoder path to be used in the decoder, which helps in conveying low-level feature information during decoding and produces a more stable model that converges in meaningful ways. In a vanilla U-Net for example, these connections simply concatenate feature maps from the contracting encoder path to the corresponding level in the decoding expansive path.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/d375e4f7f7a8acf030ae34c5433dfb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMNhVx2b7OL_elnTH_RG-Q.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 7:</strong> Comparisons between a) U-Net; b) U-Net++; and c) 3Net+ model architectures. Image retrieved from Huang et al., 2020 (<a class="af op" href="https://doi.org/10.48550/arXiv.2004.08790" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2004.08790</a>).</figcaption></figure><p id="e974" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">The UNet++ introduces a series of nested and dense skip pathways that attempt to address the issue of the unknown depth of the optimal architecture in a traditional U-Net. Instead of just having direct connections from the encoder to the decoder, the UNet++ has multiple skip pathways. For a given level in the encoder, there are skip connections to all subsequent levels in the decoder, creating a dense set of connections. These nested skip connections are designed to capture and fuse features at various semantic levels more effectively than the vanilla U-Net, however this comes at the cost of a larger model (more parameters) and increased training times.</p><p id="b8fc" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">The 3Net+ builds on ideas of both previous techniques and is the architecture used in our final model (Fig. 8). This method breaks the skip connections into a series of across-skip connections (similar to the vanilla U-Net), inter-skip connections, and intra-skip connections. These inter- and intra- skip connections make full use of multi-scale features in the scene by passing information in a manner which incorporates low-level details with high-level semantics from feature maps in full scales, but with fewer parameters to the U-Net++ model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/bef457d14bf4a1db66331064820d8da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1oN4YQypMH6lNC1rH_HVg.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 8:</strong> a) 128x128 chunks of input KaZR and ERA-5 variables; and b) our model’s 3Net+ architecture with deep supervision layers. Image by author.</figcaption></figure><p id="ffb3" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Further, our model makes use of deep supervision to learn hierarchical representations from the full-scale aggregated feature maps at each level of the decoder. This helps the model learn to correctly position clouds within the blind zone by examining the wider context of the scene. In the remaining sections we will compare the skill of this 3Net+ model trained on just reflectivity (herein denoted as 3+_1), another version trained on reflectivity and ERA5 data (3+_5) and two linear inpainting techniques using repeating extrapolation (REP) and marching average (MAR) methods. For additional details on how these are implemented, please refer to our AIES paper.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a281" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Fidelity</h1><p id="4573" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">To comprehensively evaluate model performance in blind zone reconstructive accuracy, we will first examine a handful of common cases, followed by a more general statistical analysis of the full test dataset. Note that all results shown from here on out are strictly taken from the unseen test set of observations.</p><h2 id="36c7" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Case Studies</h2><p id="fc99" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Examples of inpainted blind zone reflectivity values (taken from both NSA and OLI) for REP, MAR, 3+ 1 and 3+ 5 models, along with the corresponding target KaZR VAP product (i.e. the ground truth) in the far left column are shown below in Figs. 9/10.</p><p id="a87e" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">This first set of examples highlights cases of near surface reflectivity gradients and cloud gaps that are common at both locations. The black dashed line indicates the 1.2 km blind zone threshold (values below this are masked and reconstructed by each model), and the shaded regions indicate areas of high uncertainty in the inpainted U-Net predictions (more on this later in the Monte Carlo Dropout section).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/67beb050c487cf93946cc99aa5f3e2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pcS978dmEMBweoKmTpLKUA.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 9:</strong> Examples a)-e) of inpainted blind zone reflectivity values (taken from both NSA and OLI) for REP, MAR, 3+_1 and 3+_5 models, along with the corresponding target KaZR VAP product (i.e. the ground truth) in the far left column. This set of examples highlights cases of near surface reflectivity gradients and cloud gaps. Image by author.</figcaption></figure><p id="e481" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">We find that the linear models (REP and MAR) perform well for deep, homogeneous systems, but fall short in more complex cases. Further, due to their reliance on blind zone threshold reflectivities, reflectivity gradients (vertical and horizontal) are missed by REP and MAR (and usually captured using the U-Nets). Finally, shallow, Arctic mixed-phase clouds can also be resolved using U-Nets, along with cloud gaps and cases of virga (Fig. 10), which is exciting as this has substantial implications to surface snowfall quantities.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/bbc422502db91e68dc004abfda2a21c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhv5-f1x7RYDgZtyT-mf-Q.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 10:</strong> The same as Fig. 9, now focusing on shallow snowfall and virga cases for each model’s inpainted predictions. Image by author.</figcaption></figure><p id="98f5" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">The most challenging cases to accurately model are those with sparse reflectivity profiles and distant cloud. For instance, consider Fig. 11 showing two similar cases at NSA occurring in different years. To the human eye, both locations have very similar cloud structures, however one has a shallow near surface cloud in the blind zone while the other does not. The linear inpainting techniques are clearly well outside of their comfort zone here and will always produce the same “no cloud” output as we see in a). However, the U-Net models are still able to resolve cloud presence in cases such as this, with the 3+_5 model using the additional context from ERA-5 to better understand that the atmospheric conditions in this case likely result in blind zone cloud.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/a94a0fd2e2e36458dcbefc8f0f7d5dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xw2rqjZpkD-73vfzOztLOg.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 11:</strong> a) Example of a case where a cloud aloft (with mean reflectivity near -20 dBZ) has no near surface activity in the blind zone; and b) a similar cloud structure to a), except in this case there does exist a near surface band of reflectivity in a shallow cloud that is correctly identified using the 3+_5 model. Image by author.</figcaption></figure><h2 id="50b8" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Robustness</h2><p id="230e" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">As we can see in Fig. 12 a), the U-Net PSD curves are closer to observations compared to the linear methods. Further, the nonlinear reconstructions produce a more realistic lowest cloud echo height (shown in the probability density plots of b), suggesting that we are better capturing cloud positions. Finally, the entire blind zone structure is summarized in c)-g), where the linear methods are able to capture general macro-scale trends in reflectivity but fall short of capturing the fine-scale variability. We do note that the U-Net models have a slight “cold” bias towards -60 dBZ due to their tendency to produce “safer” estimates closer to “no cloud” over much rarer, high intensity snowfall events.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/c3d1eb566f9d18bfd765ffbd0f64e748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n47SrZDBSU3dibYxOjprvw.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 12:</strong> Cloud structure metrics including a) vertical power spectral density curves; b) lowest reflectivity echo layer probability density plot; and c) 2d reflectivity histograms. Image by author.</figcaption></figure><p id="79c4" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Further, improving our predictions of surface snowfall and virga cases would have substantial hydrologic implications and reduce our uncertainty in snow accumulation quantities. So, we performed checks to see how well three cases were reconstructed with our model using probability of detection (POD) and false alarm rate (FAR):</p><ol class=""><li id="c985" class="nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo px py pz bk">Blind zone cloud presence (i.e. was any cloud detected)</li><li id="4308" class="nt nu fq nv b go qa nx ny gr qb oa ob oc qc oe of og qd oi oj ok qe om on oo px py pz bk">Shallow snowfall (i.e. was there snowfall at the surface but not at the blind zone threshold)</li><li id="dee9" class="nt nu fq nv b go qa nx ny gr qb oa ob oc qc oe of og qd oi oj ok qe om on oo px py pz bk">Virga (i.e. was there snowfall detected at the blind zone threshold but not at the surface)</li></ol><p id="ac17" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">The critical success index (CSI) for each of these metrics is shown in the performance diagram below (Fig. 13), with the 3+_5 model performing the best overall. Shallow snowfall cases were typically the hardest to reconstruct, as we saw these cases can be quite tricky to get right (Fig. 11).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/6c899e8750bee385130f6461048b4685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rt4KYdqFs4QJUdI464DNXQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 13:</strong> Cloud, shallow snowfall and virga detection performance diagram for each model. Image by author.</figcaption></figure></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b0cb" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Explainability</h1><p id="fc20" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">To add further trust in the model’s decision-making process, we also performed a series of <a class="af op" rel="noopener" target="_blank" href="/what-is-explainable-ai-xai-afc56938d513#:~:text=Explainable%20AI%20refers%20to%20methods,be%20understood%20by%20human%20experts.">eXplainable Artificial Intelligence (XAI)</a> tests (i.e., thinking towards a mechanistic interpretation of model behaviour). These tests were aimed at connecting model behaviour to logical physical processes to both inspire confidence in the model and provide additional insights for potentially enhancing future retrievals. If we can learn about previously unknown connections in the data, that can be very valuable! Separately, each individual XAI method gives a slightly different “local” explination of the decision-making process, and it is therefore useful to incorporate multiple tests to derive a more robust understanding.</p><h2 id="d884" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Feature Maps</h2><p id="9eb1" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">The first, most basic, test we considered were feature/activation maps. By examining the values of the reLU activator in different levels of the encoder path of Fig. 8.b, we get a rough idea of where the model is looking in an image for a given input. As shown in Fig.14 below, the e1 encoder layer of the 3+_5 model typically looked along cloud edges, at reflectivity gradients and directly along the blind zone threshold.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/2fb9255e81b9dfed726a26212cbc6758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0802EhLhm86BOcwhPHaEQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 14:</strong> a) Example reflectivity input to the 3+_5 model; and b) the set of 32 feature maps produced by the e1 encoder layer of the model. Image by author.</figcaption></figure><h2 id="936b" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Drop Channel Importance</h2><p id="6a35" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">One of the biggest questions in this project was whether ERA-5 was providing useful context to the model. If we can use a more simplistic U-Net that only relies on reflectivity for instance (i.e. the 3+_1 model), then we should do so, as this would be more computationally efficient. However, if the additional atmospheric state variables from ERA-5 provide the model with useful context for inpainting complex systems, then the use of this more complex model may be warranted.</p><p id="20bc" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Since we only have a handful of inputs in this case (1 mandatory (radar) and 4 supplementary (temperature, humidity, u-wind, and v-wind), we can do an exhaustive search of the input space to evaluate their marginal contributions to accuracy. More formally, this drop-channel approach uses the below formula (Eq. 1/ Eq. 2) to calculate marginal contributions of importance from the provided inputs. Note that this technique does not consider potential nonlinear interactions between inputs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/eaec56ae5cf66e13cceea7a0f26cfe60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kaW9Iri6Doq9ySw7_J06aw.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Eq. 1 / Eq. 2:</strong> Used in calculating marginal importance from N inputs. Image by author.</figcaption></figure><p id="21fc" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">If we perform a series of this test runs (25 epochs) and examine changes in the validation loss, we can gain some rough insight into which inputs are most useful. The results of these tests are shown below in Fig. 15, where we note a trend of decreasing validation loss as we add more ERA-5 inputs (suggesting that none of the inputs are wholly unimportant). Further, the marginal contributions to validation loss suggest that the wind data is the most influential overall. We believe that this importance may stem from that fact that in the upper troposphere, wind patterns can hint at mesoscale atmospheric dynamics, such as the presence of high or low-pressure systems, fronts, and jet streams (which are of course linked to cloud/hydrometeor formation).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/c184566fd3f2516c6f00d0a71c52e881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jnRUXZoXmRVDCiFEfvUAfg.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 15:</strong> Input channel combination contributions to inpainting performance in 3Net+ models. Image by author.</figcaption></figure><h2 id="2873" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Saliency Maps</h2><p id="2a22" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Finally, we also examined saliency maps for a handful of cases (Fig. 16), to further compare the differences in importance between the 3+_5 and 3+_1 models. These pixel attribution vanilla gradient saliency maps are inspired by the work of Simonyan et al., 2014, and provide additional insight into areas the model identifies as crucial contributors of information for inpainting accuracy using a given input. These saliency maps are generated by running an image through the network and subsequently extracting the gradients of the output based on the input across all channels. While simplistic, this method is particularly useful for visualizing which parts of the observed image are most valuable in inpainting the blind zone reflectivity values, allowing for direct plotting of the activation gradients.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/ab1f00f285d909031eb129497ba5ef34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuKrsN62j32O5-287bqFZA.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 16:</strong> Vanilla gradient saliency maps for both the 3+_1 and 3+_5 models, along with the corresponding ERA5 atmospheric state variables for a handful of cases at NSA. Image by author.</figcaption></figure><p id="bb65" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">For multilayer clouds that intersect with the blind zone cut-off (e.g. Fig. 16.a), both models focus on the top of the cloud and the 1.2 km boundary threshold, as these systems often continue to extend down to the surface with similar reflectivity intensities. Both models also typically focus on and around cloud gaps in deeper systems (e.g. Fig. 16.b), however, a distinct halo of importance towards the tropopause is noticeable in the 3+_5 model. This recurring feature is likely incorporating upper troposphere wind and humidity data into predictions of near surface reflectivity. Interestingly, the 3+_1 model does not solely focus on the areas of high reflectivity in the scene but also on the regions around the cloud.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9478" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Applications</h1><p id="a659" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">The motivating goal of this work is to eventually apply a surface trained U-Net to spaceborne observations. While additional work needs to be completed on resolution-matching between these two systems, we performed early tests against coincident CloudSat-CPR observations near NSA. The idea here being that both systems (while not perfectly overlapped), will be observing similar portions of the same storm system. We consider a handful of examples and include one below for a shallow cumuliform snowfall case.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/21fe4f2052e4c25ceb85aa247b6fffc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utGVOYkyfjZV7VCgqRbKug.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 17:</strong> a) Map depicting a CloudSat overpass granule path (red), 128-step site-coincident CloudSat footprint near NSA, and white point showing the closest observed profile to the site (black circle represents a 50 km radius around the site); b) CloudSat reflectivity profile of a shallow cumuliform system, with the region between the two dashed lines representing the blue (coincident) portion of the overpass in a), and the white shaded region showing the satellite blind zone; c) Vertical NSA KaZR reflectivity profile, with the corresponding overpass period from b) shown in the region between the two dashed lines; d) A closeup of the KaZR CloudSat coincident profile from c); e) The CloudSat-NSA coincident profile from b); and f-h) The REP, MAR and 3+_1 inpainted blind zone scenes, respectively, when provided with e) as input. Image by author.</figcaption></figure><p id="54b7" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">In this example, we noted that both the spaceborne and surface radars observed a shallow cloud about 3 km tall (however CloudSat misses the increased reflectivity gradient below the blind zone due to surface clutter). When this region is reconstructed using both the traditional techniques and our U-Net, we find the U-Net is the only method that can accuratly represent the band of increased reflectivity around 1 km. More formally, if we look at the structure between the nearest CloudSat observation to the site (white dashed line) to each of the closest reconstructed regions, Pearson correlations are substantially improved using the U-Net (r_MAR=0.13 to r_3+_1=0.74).</p><p id="174d" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">While these examples don’t comprise a comprehensive anaylsis allowing us to say something about general performance, they do indicate that we see skill consistent with what we have previous noted when looking at the simulated surface blind zone. Further application work to spaceborne instruments is ongoing.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="db8a" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Final Notes</h1><p id="956a" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Before we finish up this already long post, I wanted to highlight a few of the other features we built into the model and provide some training code examples for those interested in developing their own inpainting model.</p><h2 id="3303" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Monte Carlo Dropout</h2><p id="d70c" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Unlike traditional Bayesian methods, we do not directly produce a physically-based uncertainty estimate using a U-Net. To get a rough idea of model confidence and stability, we decided to introduce a dropout at inference layer to the model based on the work of Gal and Ghahramani, 2016 which allows us to generate a distribution of inpainted predictions for each test case. These distributions allow us to produce confidence intervals for each inpainted pixel, and further refine our estimates to regions that model is more certain of when inpainting. An example of this is shown below in Fig. 17.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/6f708001b2ccb77735357449f51301ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uVdp2fkRu4LSY7WFc6T5WA.gif"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Figure 18:</strong> Monte Carlo dropout example outputs (n=50 iterations). Image by author.</figcaption></figure><p id="e8cf" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">We typically use N=50 iterations per case, and as we can see above, the areas with the highest uncertainty are typically cloud edges and cloud gaps, as the model often hallucinates when positioning these features.</p><h2 id="fed7" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Training Statistics</h2><p id="1b74" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Model training for this project was completed on two sets of hardware, including a Linux-based GPU computing cluster on Microsoft Azure, and a high-performance desktop running Windows 11 (additional system details in Table 1). An extensive Bayesian hyperparameter sweep was also performed over the course of 2 days. Further, batch normalization is applied along with early stopping (n=20), dropout and L2 regularization (ridge regression) to help mitigate overfitting during the training process. Learning rate decay is also applied at two epochs (450 and 475), allowing the model to more easily settle into a local loss minima near the end of the training phase. All training runs and hyperparameter sweeps are saved online using the<a class="af op" href="https://wandb.ai" rel="noopener ugc nofollow" target="_blank"> Weights &amp; Biases cloud storage option</a>, to monitor model learning rates and stability over time.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/2fa4de49f4643bec06b12735d6f575ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dPLBjS_A5-5uIZXd3gC7IQ.png"/></div></div><figcaption class="pe pf pg mj mk ph pi bf b bg z dx"><strong class="bf mz">Table 1:</strong> Summary details of hardware used for model training. Image by author.</figcaption></figure><h2 id="bebb" class="qi my fq bf mz qj qk ql nc qm qn qo nf oc qp qq qr og qs qt qu ok qv qw qx qy bk">Example Code</h2><p id="0fe6" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">A link to the GitHub is here: <a class="af op" href="https://github.com/frasertheking/blindzone_inpainting" rel="noopener ugc nofollow" target="_blank">https://github.com/frasertheking/blindzone_inpainting</a></p><p id="d254" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">However, I wanted to provide an overview of the actual 3Net+ implementation (with variable depth) in Tensorflow below for those interested in playing around with it.</p><pre class="mm mn mo mp mq rh ri rj bp rk bb bk"><span id="b3f8" class="rl my fq ri b bg rm rn l ro rp">def conv_block(x, kernels, kernel_size=(3, 3), strides=(1, 1), padding='same', is_bn=True, is_relu=True, n=2, l2_reg=1e-4):<br/>    for _ in range(1, n+1):<br/>        x = k.layers.Conv2D(filters=kernels, kernel_size=kernel_size,<br/>                            padding=padding, strides=strides,<br/>                            kernel_regularizer=tf.keras.regularizers.l2(l2_reg),<br/>                            kernel_initializer=k.initializers.he_normal(seed=42))(x)<br/>        if is_bn:<br/>            x = k.layers.BatchNormalization()(x)<br/>        if is_relu:<br/>            x = k.activations.relu(x)<br/>    return x<br/><br/>def unet3plus(input_shape, output_channels, config, depth=4, training=False, clm=False):<br/><br/>    """ Prep """<br/>    interp = config['interpolation']<br/>    input_layer = k.layers.Input(shape=input_shape, name="input_layer")<br/>    xpre = preprocess(input_layer, output_channels)<br/><br/>    """ Encoder """<br/>    encoders = []<br/>    for i in range(depth+1):<br/>        if i == 0:<br/>            e = conv_block(xpre, config['filters']*(2**i), kernel_size=(config['kernel_size'], config['kernel_size']), l2_reg=config['l2_reg'])<br/>        else:<br/>            e = k.layers.MaxPool2D(pool_size=(2, 2))(encoders[i-1])<br/>            e = k.layers.Dropout(config['dropout'])(e, training=True)<br/>            e = conv_block(e, config['filters']*(2**i), kernel_size=(config['kernel_size'], config['kernel_size']), l2_reg=config['l2_reg'])<br/>        encoders.append(e)<br/><br/>    """ Middle """<br/>    cat_channels = config['filters']<br/>    cat_blocks = depth+1<br/>    upsample_channels = cat_blocks * cat_channels<br/><br/>    """ Decoder """<br/>    decoders = []<br/>    for d in reversed(range(depth+1)):<br/>        if d == 0 :<br/>            continue<br/>        loc_dec = []<br/>        decoder_pos = len(decoders)<br/>        for e in range(len(encoders)):<br/>            if d &gt; e+1:<br/>                e_d = k.layers.MaxPool2D(pool_size=(2**(d-e-1), 2**(d-e-1)))(encoders[e])<br/>                e_d = k.layers.Dropout(config['dropout'])(e_d, training=True)<br/>                e_d = conv_block(e_d, cat_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, l2_reg=config['l2_reg'])<br/>            elif d == e+1:<br/>                e_d = conv_block(encoders[e], cat_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, l2_reg=config['l2_reg'])<br/>            elif e+1 == len(encoders):<br/>                e_d = k.layers.UpSampling2D(size=(2**(e+1-d), 2**(e+1-d)), interpolation=interp)(encoders[e])<br/>                e_d = k.layers.Dropout(config['dropout'])(e_d, training=True)<br/>                e_d = conv_block(e_d, cat_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, l2_reg=config['l2_reg'])<br/>            else:<br/>                e_d = k.layers.UpSampling2D(size=(2**(e+1-d), 2**(e+1-d)), interpolation=interp)(decoders[decoder_pos-1])<br/>                e_d = k.layers.Dropout(config['dropout'])(e_d, training=True)<br/>                e_d = conv_block(e_d, cat_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, l2_reg=config['l2_reg'])<br/>                decoder_pos -= 1<br/>            loc_dec.append(e_d)<br/>        de = k.layers.concatenate(loc_dec)<br/>        de = conv_block(de, upsample_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, l2_reg=config['l2_reg'])<br/>        decoders.append(de)<br/><br/>    """ Final """<br/>    d1 = decoders[len(decoders)-1]<br/>    d1 = conv_block(d1, output_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, is_bn=False, is_relu=False, l2_reg=config['l2_reg'])<br/>    outputs = [d1]<br/><br/>    """ Deep Supervision """<br/>    if training:<br/>        for i in reversed(range(len(decoders))):<br/>            if i == 0:<br/>                e = conv_block(encoders[len(encoders)-1], output_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, is_bn=False, is_relu=False, l2_reg=config['l2_reg'])<br/>                e = k.layers.UpSampling2D(size=(2**(len(decoders)-i), 2**(len(decoders)-i)), interpolation=interp)(e)<br/>                outputs.append(e)<br/>            else:<br/>                d = conv_block(decoders[i - 1], output_channels, kernel_size=(config['kernel_size'], config['kernel_size']), n=1, is_bn=False, is_relu=False, l2_reg=config['l2_reg'])<br/>                d = k.layers.UpSampling2D(size=(2**(len(decoders)-i), 2**(len(decoders)-i)), interpolation=interp)(d)<br/>                outputs.append(d)<br/><br/>    if training:<br/>        for i in range(len(outputs)):<br/>            if i == 0:<br/>                continue<br/>            d_e = outputs[i]<br/>                d_e = k.layers.concatenate([out1, out2, out3])<br/>            outputs[i] = merge_output(input_layer, k.activations.linear(d_e), output_channels)<br/><br/>    return tf.keras.Model(inputs=input_layer, outputs=outputs, name='UNet3Plus')</span></pre></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9d4b" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Future</h1><p id="a858" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">I know this was a long post, and we covered a lot of stuff, but I want to give a quick summary of everything we’ve covered for those who have made this far (or those who skipped to the end).</p><p id="49e1" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">The satellite radar blind zone is an ongoing problem on satellite-based Earth observing precipitation missions, with critical implications to global water-energy budget calculations. In order to overcome common issues with traditional linear inpainting methods for filling in this region, we opted to use a nonlinear, deeply supervised U-Net for radar blind zone inpainting. The U-Net outperforms linear techniques across nearly all metrics and can even reconstruct complex cloud structures like multilayer clouds, cloud gaps and shallow cloud. Further, using a variety of XAI techniques, we saw that information directly at the blind zone threshold and along the tropopause (especially wind information) was found to be very useful in the model’s decision-making process. While we don’t suggest that these models fully replace current physically-based solutions, we believe they offer a unique new perspective that can be used to supplement other retrievals in future missions.</p><p id="0423" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">We are currently working on a follow-up project to this with direct applications to CloudSat-CPR observations.</p></div></div></div><div class="ab cb oq or os ot" role="separator"><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow ox"/><span class="ou by bm ov ow"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e181" class="mx my fq bf mz na oy gq nc nd oz gt nf ng pa ni nj nk pb nm nn no pc nq nr ns bk">Referernces</h1><p id="46c2" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Gal, Y., &amp; Ghahramani, Z. (2016). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning (arXiv:1506.02142). arXiv. <a class="af op" href="https://doi.org/10.48550/arXiv.1506.02142" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1506.02142</a></p><p id="27d2" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Geiss, A., &amp; Hardin, J. C. (2021). Inpainting radar missing data regions with deep learning. Atmospheric Measurement Techniques, 14(12), 7729–7747. <a class="af op" href="https://doi.org/10.5194/amt-14-7729-2021" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.5194/amt-14-7729-2021</a></p><p id="02d1" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Guillemot, C., &amp; Le Meur, O. (2014). Image Inpainting: Overview and Recent Advances. <em class="ps">IEEE Signal Processing Magazine</em>, <em class="ps">31</em>(1), 127–144. <a class="af op" href="https://doi.org/10.1109/MSP.2013.2273004" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/MSP.2013.2273004</a></p><p id="107f" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.-W., &amp; Wu, J. (2020). UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation (arXiv:2004.08790). arXiv. <a class="af op" href="https://doi.org/10.48550/arXiv.2004.08790" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2004.08790</a></p><p id="412b" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Kidd, C., Graham, E., Smyth, T., &amp; Gill, M. (2021). Assessing the Impact of Light/Shallow Precipitation Retrievals from Satellite-Based Observations Using Surface Radar and Micro Rain Radar Observations. Remote Sensing, 13(9), Article 9. <a class="af op" href="https://doi.org/10.3390/rs13091708" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.3390/rs13091708</a></p><p id="2548" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">King, F., &amp; Fletcher, C. G. (2020). Using CloudSat-CPR Retrievals to Estimate Snow Accumulation in the Canadian Arctic. Earth and Space Science, 7(2), e2019EA000776. <a class="af op" href="https://doi.org/10.1029/2019EA000776" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1029/2019EA000776</a></p><p id="17bd" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Lamer, K., Kollias, P., Battaglia, A., &amp; Preval, S. (2020). Mind the gap — Part 1: Accurately locating warm marine boundary layer clouds and precipitation using spaceborne radars. Atmospheric Measurement Techniques, 13(5), 2363–2379. <a class="af op" href="https://doi.org/10.5194/amt-13-2363-2020" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.5194/amt-13-2363-2020</a></p><p id="e995" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., &amp; Van Gool, L. (2022). RePaint: Inpainting using Denoising Diffusion Probabilistic Models. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 11451–11461. <a class="af op" href="https://doi.org/10.1109/CVPR52688.2022.01117" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1109/CVPR52688.2022.01117</a></p><p id="655e" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Maahn, M., Burgard, C., Crewell, S., Gorodetskaya, I. V., Kneifel, S., Lhermitte, S., Van Tricht, K., &amp; van Lipzig, N. P. M. (2014). How does the spaceborne radar blind zone affect derived surface snowfall statistics in polar regions? Journal of Geophysical Research: Atmospheres, 119(24), 13,604–13,620. <a class="af op" href="https://doi.org/10.1002/2014JD022079" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1002/2014JD022079</a></p><p id="8763" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Simonyan, K., Vedaldi, A., &amp; Zisserman, A. (2014). Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps (arXiv:1312.6034). arXiv. <a class="af op" href="https://doi.org/10.48550/arXiv.1312.6034" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1312.6034</a></p><p id="0e17" class="pw-post-body-paragraph nt nu fq nv b go pj nx ny gr pk oa ob oc pl oe of og pm oi oj ok pn om on oo fj bk">Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., &amp; Liang, J. (2018). UNet++: A Nested U-Net Architecture for Medical Image Segmentation (arXiv:1807.10165). arXiv. <a class="af op" href="https://doi.org/10.48550/arXiv.1807.10165" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1807.10165</a></p></div></div></div></div>    
</body>
</html>