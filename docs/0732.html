<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>DIGITOUR: Automatic Digital Tours for Real-Estate Properties üè†</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>DIGITOUR: Automatic Digital Tours for Real-Estate Properties üè†</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18">https://towardsdatascience.com/digitour-automatic-digital-tours-for-real-estate-properties-b0a6d2f7d638?source=collection_archive---------10-----------------------#2024-03-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3bf4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An Automated Pipeline for Creating 3D Experiences from Equirectangular Images</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Prateek Chhikara" class="l ep by dd de cx" src="../Images/4cabb40cbab34038c0f762b45d58bbba.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F6Bov2ToQ_oaiAcr8eYWpg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@prateekchhikara?source=post_page---byline--b0a6d2f7d638--------------------------------" rel="noopener follow">Prateek Chhikara</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b0a6d2f7d638--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Mar 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="9d90" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">1. Introduction</h1><p id="f400" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The demand for online real-estate tools has increased drastically due to the ease of accessibility to the Internet, especially in countries like India. There are many online real-estate platforms for owners, developers, and real-estate brokers to post properties for buying and renting purposes. Daily, these platforms receive 8,000 to 9,000 new listings. Until now, the users on these platforms view images, snapshots, or videos, which may not build the desired confidence to decide and finalize the deal. To overcome this challenge and to enhance user experience, virtual tours are a potential solution.</p><p id="2b1b" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Virtual tours are images linked together, allowing viewers to experience a particular location remotely (<em class="of">a frequently used example is Google Street View</em> [1]). Recently, the demand for virtual tours has increased as they provide better interaction with users/customers, especially in businesses like real-estate, hotels, restaurants, universities, schools, etc [2]. Broadly, there are three categories of virtual tours:</p><ol class=""><li id="6a08" class="ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz og oh oi bk"><em class="of">2D video tours</em>,</li><li id="d588" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><em class="of">360‚ó¶ video-based virtual tours</em>, and</li><li id="c12b" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><em class="of">360‚ó¶ static image-based virtual tours</em>.</li></ol><p id="e33e" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Compared to 2D video tours and 360‚ó¶ video-based tours, the static equirectangular image-based virtual tours provide more immersion and interactivity, thus leading to better decision-making and avoiding unnecessary visits.</p><blockquote class="oo op oq"><p id="f6a6" class="ne nf of ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">üöÄ Before delving deep into the proposed work, I would like to mention that this work is published and available at the<em class="fq"> </em><a class="af or" href="https://dl.acm.org/doi/proceedings/10.1145/3570991" rel="noopener ugc nofollow" target="_blank"><strong class="ng fr">International Conference on Data Science &amp; Management of Data (CODS-COMAD)<em class="fq">‚Äî 2023</em></strong></a>. ‚ú®‚ú®‚ú®</p></blockquote><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot ou"><img src="../Images/3e998bf0108bec1443f8275915922668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JF6vUNUZs7Kim8Y5z5Ah6A.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 1:</strong> An example of Equirectangular image clicked at the entry of a house showing the living room. (Source: Image by the author)</figcaption></figure><p id="2ed9" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Typically, the pipeline for creating a virtual tour consists of the following components:</p><ol class=""><li id="4119" class="ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz og oh oi bk"><strong class="ng fr">Equirectangular Image Capture:</strong> An equirectangular image represents a spherical object as a 2D image (<em class="of">as shown in </em><strong class="ng fr"><em class="of">Figure 1</em></strong>). It is a spherical panorama incorporating 180¬∞ vertical and 360¬∞ horizontal viewing angles. A simple example can be the projection of Earth (<em class="of">a spherical-shaped object</em>) on a 2D map. These images are clicked using 360‚ó¶ cameras such as <a class="af or" href="https://theta360.com/en/etc/technology.html" rel="noopener ugc nofollow" target="_blank">Ricoh-Theta</a>,<a class="af or" href="https://www.insta360.com/product/insta360-onex2" rel="noopener ugc nofollow" target="_blank"> Insta3602</a>, etc.</li><li id="c82a" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Connecting Equirectangular Images:</strong> For any location, we will have multiple equirectangular images. To illustrate, in real estate properties, we typically have equilateral images for bedrooms, halls, kitchens, dining rooms, etc. It is essential to build navigation between images to have a complete ‚Äú<em class="of">walkthrough</em>‚Äù experience. Moreover, there can be multiple routes from one position to other positions. For instance, we can go from the hall to the kitchen, bedroom, balcony, etc. Therefore, it is crucial to connect all the equirectangular images (<em class="of">please refer to </em><strong class="ng fr"><em class="of">Figure 2</em></strong><em class="of"> for an example</em>). Generally, it is done manually, which is both costly and time-consuming [3].</li><li id="9df2" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Publishing Virtual Tour: </strong>Once we have formed connections between equirectangular images, we can publish the final virtual tour on the cloud or in an application.</li></ol><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot pl"><img src="../Images/fc4ab6e0c944df1cdf01073844a0a7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvjJw8h3rqkBq-aTe-L57A.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 2:</strong> Example property floor plan (a) and it‚Äôs connections (b) to make a digital tour. The numbers represent the position of tags and equirectangular images. (Source: Image by the author)</figcaption></figure><p id="159a" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">While automating the above pipeline, one of the significant challenges is the manual annotation for connecting equirectangular images. Generally, it takes more than 20 minutes to create a virtual tour. We propose an end-to-end pipeline for real-estate equirectangular images (<em class="of">called </em><strong class="ng fr"><em class="of">DIGITOUR</em></strong>) to overcome the challenge of creating automated digital tours. The <strong class="ng fr">DIGITOUR</strong> pipeline consists of the following components:</p><ol class=""><li id="2676" class="ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz og oh oi bk"><strong class="ng fr">Colored tag placement and clicking 360¬∞ images:</strong> We propose novel paper tags that are bi-colored and numbered, facilitating better learning of downstream computer vision tasks (<em class="of">i.e., tag recognition and digit recognition</em>) and automatic stitching of equirectangular images.</li><li id="c3f5" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Map equirectangular images to cube map projection:</strong> We used the publicly available Python library <a class="af or" href="https://github.com/bhautikj/vrProjector" rel="noopener ugc nofollow" target="_blank">vrProjector3</a> to map equirectangular images to their cube map projections (<em class="of">corresponding to six cube faces</em>).</li><li id="f3d8" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Tag Detection:</strong> For each cube face, we propose colored tag detection in an image using YOLOv5 [4] architecture.</li><li id="3080" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Digit Recognition:</strong> We propose to perform digit recognition using a light-weight custom MobileNet [5] model.</li></ol><p id="daec" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Finally, we connect all the equirectangular images using the detected tags.</p></div></div><div class="pa"><div class="ab cb"><div class="ll pm lm pn ln po cf pp cg pq ci bh"><figure class="ov ow ox oy oz pa ps pt paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot pr"><img src="../Images/b2924b4a19f2b908403e11e142d69682.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Ot_8EFbk5b2tJWq2uacO9Q.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 3:</strong> End-to-end pipeline for proposed approach DIGITOUR. (Source: Image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3019" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">2. Proposed Pipeline: DIGITOUR</h1><p id="7f92" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">The complete pipeline for producing digital tours (<em class="of">referred to as </em><strong class="ng fr"><em class="of">DIGITOUR</em></strong><em class="of"> and shown in </em><strong class="ng fr"><em class="of">Figure 3</em></strong>) is as follows.</p><h2 id="d0ba" class="pu mj fq bf mk pv pw px mn py pz qa mq nn qb qc qd nr qe qf qg nv qh qi qj qk bk">2.1 Tag Placement and Image Capturing</h2><p id="f02d" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">While creating a digital tour for any real-estate property, it is essential to click 360‚ó¶ images from different property locations such as bedroom, living room, kitchen, etc., then automatically stitching them together to have a ‚Äú<em class="of">walkthrough</em>‚Äù experience without being physically present at the location. Therefore, to connect multiple equirectangular images, we propose placing paper tags on the floor covering each location of the property, and placing the camera (<em class="of">in our case, we used Ricoh-Theta</em>) in the middle of the scene to capture the whole site (<em class="of">front, back, left, right and bottom</em>).</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot ql"><img src="../Images/f77f0cce2bfed149b2703ed694c0e1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjKukLpeFfa8X7_mImLezQ.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 4:</strong> Proposed bi-colored tag format and color scheme for each digit with their corresponding HSV values. (Source: Image by the author)</figcaption></figure><p id="931b" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Moreover, we ensure that the scene is clear of all noisy elements such as dim lighting and ‚Äò<em class="of">unwanted</em>‚Äô artifacts for better model training and inference. As shown in <strong class="ng fr">Figure 4</strong>, we have standardized the tags with dimensions of 6‚Äù √ó 6‚Äù with two properties:</p><ol class=""><li id="c7bb" class="ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz og oh oi bk">they are numbered which will help the photographer place tags in sequence and</li><li id="b0af" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk">they are bi-colored to formulate the digit recognition problem as classification task and facilitate better learning of downstream computer vision tasks (<em class="of">i.e. tag detection and digit recognition</em>).</li></ol><p id="47f5" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Please note that different colors are assigned to each digit (<em class="of">from 0 to 9</em>) using the HSV color scheme and leading digit of a tag has a black circle to distinguish it from the trailing digit as shown in <strong class="ng fr">Figure 4</strong>. The intuition behind standardizing the paper tags is that it allows to train tag detection and digit recognition models, which are invariant to distortions, tag placement angle, reflection from lighting sources, blur conditions, and camera quality.</p><h2 id="9268" class="pu mj fq bf mk pv pw px mn py pz qa mq nn qb qc qd nr qe qf qg nv qh qi qj qk bk">2.2 Mapping Equirectangular Image to Cubemap Projection</h2><p id="fa2c" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">An equirectangular image consists of a single image whose width and height correlate as 2 : 1 (<em class="of">as shown in </em><strong class="ng fr"><em class="of">Figure 1</em></strong>). In our case, images are clicked using a Ricoh-Theta camera having dimensions 4096 √ó 2048 √ó 3. Typically, each point in an equirectangular image corresponds to a point in a sphere, and the images are stretched in the ‚Äò<em class="of">latitude</em>‚Äô direction. Since the contents of an equirectangular image are distorted, it becomes challenging to detect tags and recognize digits directly from it. For example, in <strong class="ng fr">Figure 1</strong>, the tag is stretched at the middle-bottom of the image. Therefore, it is necessary to map the image to a less-distorted projection and switch back to the original equirectangular image to build the digital tour.</p><p id="1a55" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">In this work, we propose to use cube map projection, which is a set of six images representing six faces of a cube. Here, every point in the spherical coordinate space corresponds to a point in the face of the cube. As shown in <strong class="ng fr">Figure 5</strong>, we map the equirectangular image to six faces (<em class="of">left, right, front, back, top and bottom</em>) of a cube having dimensions 1024 √ó 1024 √ó 3 using python library vrProjector.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qm"><img src="../Images/7ca251fffbf74d5b614c9a69cb6ddb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSu9pTzkcCrpKPj18831VA.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 5:</strong> Conversion of an equirectangular image to its corresponding six faces cubemap projection. (Source: Image by the author)</figcaption></figure><h2 id="9255" class="pu mj fq bf mk pv pw px mn py pz qa mq nn qb qc qd nr qe qf qg nv qh qi qj qk bk">2.3 Tag Detection</h2><p id="e61e" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Once we get the six images corresponding to the faces of a cube, we detect the location of tags placed in each image. For tag detection, we have used the state-of-the-art YOLOv5 model. We initialized the network with COCO weights followed by training on our dataset. As shown in <strong class="ng fr">Figure 6</strong>, the model takes an image as input and returns the detected tag along with coordinates of the bounding box and confidence of the prediction. The model is trained on our dataset for 100 epochs with a batch size of 32.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qn"><img src="../Images/912ab51117c479af66d5c4fb733164b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bUF19AOyCZikHwBtQRY9Uw.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 6:</strong> Tag detection using Yolov5. (Source: Image by the author)</figcaption></figure><h2 id="c360" class="pu mj fq bf mk pv pw px mn py pz qa mq nn qb qc qd nr qe qf qg nv qh qi qj qk bk">2.4 Digit Recognition</h2><p id="5a73" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">For the detected tags, we need to recognize the digits from the tag. In a real-world environment, the detected tags might have incorrect orientation, poor luminosity, reflection from the bulbs in the room, etc. Due to these reasons, it is challenging to use Optical Character Recognition (OCR) engines to have good digit recognition performance. Therefore, we have used a custom MobileNet model initialized on Imagenet weights, which uses color information in tags for digit recognition. In the proposed architecture, we have replaced the final classification block of the original MobileNet with the dropout layer and dense layer with 20 nodes representing our tags from 1 to 20. <strong class="ng fr">Figure 7</strong> illustrates the proposed architecture. For training the model, we have used Adam as an optimizer with a learning rate of 0.001 and a discounting factor (ùúå) to be 0.1. We have used categorical cross-entropy as a loss function and set the batch size to 64 and the number of epochs to 50.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qo"><img src="../Images/ae153326a3cc38d15f5ad27f5b12681c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZGDk8-u5OovjXakngIQYA.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 7:</strong> Digit recognition using custom MobileNet model. (Source: Image by the author)</figcaption></figure><h2 id="7d42" class="pu mj fq bf mk pv pw px mn py pz qa mq nn qb qc qd nr qe qf qg nv qh qi qj qk bk">2.5 Mapping tag coordinates to the original 360‚ó¶ Image and Virutal Tour Creation</h2><p id="b876" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">Once we have detected the tags and recognized the digits we use the python library vrProjector to map the cube map coordinates back to the original equirectangular image. An example output is shown in <strong class="ng fr">Figure 8</strong>. For each equirectangular image, the detected tags form the nodes of a graph with an edge between them. In the subsequent equirectangular images of a property, the graph gets populated with more nodes, as more tags are detected. Finally, we connect multiple equirectangular images in sequence based on recognized digits written on them and the resulting graph is the<br/>virtual tour as shown in <strong class="ng fr">Figure 2(b)</strong>.</p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qp"><img src="../Images/52f56a6ef982f9405afa11390ba0029d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xy6KX1VMXdU8oF1A2PQvzg.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 8:</strong> Mapping tags to original equirectangular image. (Source: Image by the author)</figcaption></figure><h1 id="b3f6" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">4. Datasets</h1><p id="cec3" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">We have collected data by placing tags and clicking equirectangular images using Ricoh-Theta camera for several residential properties in Gurugram, India (<em class="of">Tier 1 city</em>). While collecting images we made sure that certain conditions were met such as all doors were opened, lights were turned on, ‚Äò<em class="of">unwanted</em>‚Äô objects were removed and the tags were placed covering each area of the property. Following these instructions, average number of equirectangular images clicked per residential property was 7 or 8. Finally, we have validated our approach on the following generated datasets (<em class="of">based on background color of the tags</em>).</p><ol class=""><li id="799e" class="ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz og oh oi bk"><strong class="ng fr">Green Colored Tags: </strong>We have kept the background color of these tags (<em class="of">numbered 1 to 20</em>) to be green. We have collected 1572 equirectangular images from 212 properties. Once we convert these equirectangular images to cubemap projection, we get 9432 images (<em class="of">corresponding to cube faces</em>). Since not all of the cube faces have tags (<em class="of">for e.g. top face</em>), we get 1503 images with atleast one tag.</li><li id="fd98" class="ne nf fq ng b go oj ni nj gr ok nl nm nn ol np nq nr om nt nu nv on nx ny nz og oh oi bk"><strong class="ng fr">Proposed Bi-colored Tags (see Figure 4):</strong> For these tags, we have collected 2654 equirectangular images from 350 properties. Finally, we got 2896 images (<em class="of">corresponding to cube faces</em>) with atleast one tag.</li></ol><p id="f7c9" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">Finally, we label the tags present in cube map projection images using <a class="af or" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">LabelImg</a> which is an open-source tool for labeling images in several formats such as Pascal VOC and YOLO. For all the experiments, we reserved 20% of data for testing and the remaining for training.</p><h1 id="8336" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">5. End-to-end Evaluation</h1><p id="abb7" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">For any input image, we first detect the tags and finally recognize the digits written on the tags. From this we were able to identify the true positives (<em class="of">tags detected and read correctly</em>), false positives (<em class="of">tags detected but read incorrectly</em>) and false negatives (<em class="of">tags not detected</em>). The obtained mAP, Precision, Recall and f1-score at 0.5 IoU threshold are 88.12, 93.83, 97.89 and 95.81 respectively. Please note that all metrics are averaged (<em class="of">weighted</em>) over all the 20 classes. If all tags across all equirectangular images of a property are detected and read correctly, we receive a 100% accurate virtual tour since all nodes of the graph are detected and connected with their appropriate edges. In our experiments, we were able to accurately generate 100% accurate virtual tour for 94.55% of the properties. The inaccuracies were due to the presence of colorful artifacts that were falsely detected as tags; and bad lightning conditions.</p><p id="9bbf" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk"><strong class="ng fr">Figure 9</strong> demonstrates the performance of Yolov5 model for tag detection based on green colored and bi-colored tags. Further, experiments and comparison of models on digit recognition is shown in <strong class="ng fr">Figure 10.</strong></p><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qq"><img src="../Images/1864fd9df7d19c37b16d666c372ad94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5i3qrjNhC9fnW4RsAJqLA.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 9:</strong> Tag detection performance (%). (Source: Image by the author)</figcaption></figure><figure class="ov ow ox oy oz pa os ot paragraph-image"><div role="button" tabindex="0" class="pb pc ed pd bh pe"><div class="os ot qr"><img src="../Images/79c7b22749f4cc268950294dc6459f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HoUzGWWobfDFlfIR7yPNmQ.png"/></div></div><figcaption class="pg ph pi os ot pj pk bf b bg z dx"><strong class="bf mk">Figure 10: </strong>Comparison of different state-of-the-art models on bi-colored tags dataset for digit recognition task. (Source: Image by the author)</figcaption></figure><h1 id="c619" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">5. Conclusion</h1><p id="bc4d" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">We propose an end-to-end pipeline (DIGITOUR) for automatically generating digital tours for real-estate properties. For any such property, we first place the proposed bi-colored paper tags covering each area of the property. Then, we click equirectangular images, followed by mapping these images to less distorted cubemap images. Once we get the six images corresponding to cube faces, we detect the location of tags using the YOLOv5 model, followed by digit recognition using the MobileNet model. The next step is to map the detected coordinates along with recognized digits to the original equirectangular images. Finally, we stitch together all the equirectangular images to build a virtual tour. We have validated our pipeline on a real-world dataset and shown that the end-to-end pipeline performance is 88.12 and 95.81 in terms of mAP and f1-score at 0.5 IoU threshold averaged (weighted) over all classes.</p><p id="018c" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">If you find our work beneficial and utilize it in your projects, we kindly request that you cite it. üòä</p><pre class="ov ow ox oy oz qs qt qu bp qv bb bk"><span id="5786" class="qw mj fq qt b bg qx qy l qz ra">@inproceedings{chhikara2023digitour,<br/>  title={Digitour: Automatic digital tours for real-estate properties},<br/>  author={Chhikara, Prateek and Kuhar, Harshul and Goyal, Anil and Sharma, Chirag},<br/>  booktitle={Proceedings of the 6th Joint International Conference on Data Science \&amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},<br/>  pages={223--227},<br/>  year={2023}<br/>}</span></pre><h1 id="262a" class="mi mj fq bf mk ml mm gq mn mo mp gt mq mr ms mt mu mv mw mx my mz na nb nc nd bk">References</h1><p id="87b1" class="pw-post-body-paragraph ne nf fq ng b go nh ni nj gr nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fj bk">[1] Dragomir Anguelov, Carole Dulong, Daniel Filip, Christian Frueh, St√©phane Lafon, Richard Lyon, Abhijit Ogale, Luc Vincent, and Josh Weaver. 2010. Google street view: Capturing the world at street level. Computer 43, 6 (2010), 32‚Äì38.</p><p id="4d94" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">[2] Mohamad Zaidi Sulaiman, Mohd Nasiruddin Abdul Aziz, Mohd Haidar Abu Bakar, Nur Akma Halili, and Muhammad Asri Azuddin. 2020. Matterport: virtual tour as a new marketing approach in real estate business during pandemic COVID-19. In International Conference of Innovation in Media and Visual Design (IMDES 2020). Atlantis Press, 221‚Äì226.</p><p id="d931" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">[3] Chinu Subudhi. 2021. Cutting-Edge 360-Degree Virtual Tours. <a class="af or" href="https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours" rel="noopener ugc nofollow" target="_blank">https://www.mindtree.com/insights/resources/cutting-edge-360-degree-virtual-tours</a></p><p id="3e71" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">[4] Glenn Jocher, Ayush Chaurasia, Alex Stoken, Jirka Borovec, NanoCode012, Yonghye Kwon, TaoXie, Jiacong Fang, imyhxy, Kalen Michael, Lorna, Abhiram V, Diego Montes, Jebastin Nadar, Laughing, tkianai, yxNONG, Piotr Skalski, Zhiqiang Wang, Adam Hogan, Cristi Fati, Lorenzo Mammana, AlexWang1900, Deep Patel, Ding Yiwei, Felix You, Jan Hajek, Laurentiu Diaconu, and Mai Thanh Minh. 2022. ultralytics/yolov5: v6.1 ‚Äî TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference.</p><p id="375a" class="pw-post-body-paragraph ne nf fq ng b go oa ni nj gr ob nl nm nn oc np nq nr od nt nu nv oe nx ny nz fj bk">[5] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and LiangChieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4510‚Äì4520.</p></div></div></div></div>    
</body>
</html>