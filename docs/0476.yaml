- en: 6 Step Framework to Manage Reputational & Ethical Risks of Generative AI in
    Your Product
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/6-step-framework-to-manage-ethical-risks-of-generative-ai-in-your-product-7ed46783d282?source=collection_archive---------7-----------------------#2024-02-19](https://towardsdatascience.com/6-step-framework-to-manage-ethical-risks-of-generative-ai-in-your-product-7ed46783d282?source=collection_archive---------7-----------------------#2024-02-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A pragmatic guide to understand AI risks and build trust with users through
    responsible AI development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarthakh330?source=post_page---byline--7ed46783d282--------------------------------)[![Sarthak
    Handa](../Images/0c75ba0f085fdb22a221705450047c40.png)](https://medium.com/@sarthakh330?source=post_page---byline--7ed46783d282--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7ed46783d282--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7ed46783d282--------------------------------)
    [Sarthak Handa](https://medium.com/@sarthakh330?source=post_page---byline--7ed46783d282--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7ed46783d282--------------------------------)
    ·9 min read·Feb 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0471e3425a437220dcefcbd1449eb78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Dalle-3'
  prefs: []
  type: TYPE_NORMAL
- en: In the fast-paced technology landscape, product teams feel the relentless pressure
    to rush innovative AI offerings to market. However, prioritizing speed over ethical
    and safety considerations can severely backfire. When AI systems breach social
    norms and values, organizations bear the risk of facing long-lasting reputational
    damage and losing trust with users.
  prefs: []
  type: TYPE_NORMAL
- en: Companies are caught in a real-world “*prisoner’s dilemma*” of the AI era —
    while collective adherence to ethical development is ideal, it’s often more tempting
    for companies to secure a first-mover advantage by compromising on these crucial
    standards. This tension is exemplified by the infamous **Facebook-Cambridge Analytica**
    scandal, where taking ethical shortcuts led to grave, long-lasting reputational
    damage. As AI becomes more entrenched in our daily lives, the stakes of this ethical
    gamble only heightens, with each player in the industry weighing immediate gains
    against the longer-term value of trust and responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: With the rise in AI adoption across industries since 2018, high-profile incidents
    of ethical violations have sharply increased. Mapping out these risks and mitigation
    strategies is now pivotal. This article probes the key source of AI risks, examines
    a few high-profile fallouts, and lays out a 6 steps framework for building products
    that counter these threats.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the Risks of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***1\. Privacy Intrusion & Consent Issue***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Risk:** Arguably the most prevalent AI pitfall is the infringement of privacy
    and copyright laws. This entails two distinct types of failures: failure to secure
    consent for the use of data, and using the data for purposes beyond the scope
    for which the consent was given.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example: A**rtists and writers have filed class action lawsuits against **OpenAI**
    and **MidJourney** for training models with their work without consent. Similarly,
    **Getty Images** is suing **Stability AI** for using its data for model training.
    Privacy breaches can also occur even when consent is given but the data is used
    for unintended purposes. For example, **Google DeepMind** used the data of 1.6
    million patients at the **Royal London NHS Foundation Trust** to build a new healthcare
    application. Although there was an implied consent that the data could be used
    to improve patients’ health, the Trust and DeepMind did not clearly communicate
    to patients that their information is being used to build an app.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30/?source=post_page-----7ed46783d282--------------------------------)
    [## Artists take new shot at Stability, Midjourney in updated copyright lawsuit'
  prefs: []
  type: TYPE_NORMAL
- en: A group of visual artists has filed an amended copyright lawsuit against Stability
    AI, Midjourney and other companies…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.reuters.com](https://www.reuters.com/legal/litigation/artists-take-new-shot-stability-midjourney-updated-copyright-lawsuit-2023-11-30/?source=post_page-----7ed46783d282--------------------------------)
    [](https://www.wired.co.uk/article/google-deepmind-nhs-health-data?source=post_page-----7ed46783d282--------------------------------)
    [## Why Google consuming DeepMind Health is scaring privacy experts
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind Health has run into data protection problems before and it being moved
    fully into Google raises more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.wired.co.uk](https://www.wired.co.uk/article/google-deepmind-nhs-health-data?source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '***2\. Algorithmic Bias***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Risk**: This risk involves AI systems making biased predictions, which systematically
    disadvantages or excludes certain group of people based on characteristics like
    race, gender, or socio-economic background. Such biases can have significant societal
    impact, especially when the AI is used to make critical decisions impacting the
    lives of the individuals.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** There was backlash against Apple in 2019, when allegations surfaced
    that men received higher credit limits for **Apple Credit Cards** than women,
    despite women having better credit scores in some cases. Other notable instances
    include AI-driven recruitment software and criminal justice applications, such
    as the **COMPAS** tool, which have been criticized for displaying racial and gender
    bias.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.cnn.com/2019/11/12/business/apple-card-gender-bias/index.html?source=post_page-----7ed46783d282--------------------------------)
    [## Apple Card is accused of gender bias. Here''s how that can happen | CNN Business'
  prefs: []
  type: TYPE_NORMAL
- en: Some Apple Card customers say the credit card's issuer, Goldman Sachs, is giving
    women far lower credit limits, even if…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.cnn.com](https://www.cnn.com/2019/11/12/business/apple-card-gender-bias/index.html?source=post_page-----7ed46783d282--------------------------------)
    [](https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html?source=post_page-----7ed46783d282--------------------------------)
    [## Opinion | When an Algorithm Helps Send You to Prison (Published 2017)
  prefs: []
  type: TYPE_NORMAL
- en: Giving a computer program responsibility over sentences doesn't eliminate bias.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.nytimes.com](https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html?source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '***3\. Accuracy Risk & Explainability Gap***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Risk:** Significant risks arise when AI systems that are used for making
    high-stakes decisions provide inaccurate results or fail to offer a clear rationale
    for their output. The ‘black box’ nature of the AI makes it difficult for users
    to understand and verify its results, which obscures accountability and leads
    to a loss of trust.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example:** **IBM Watson for Oncology** was designed to provide clinicians
    with personalized recommendations for cancer treatment. However, reports surfaced
    that Watson gave unsafe and incorrect treatment suggestions, which led to a loss
    of trust in technology and damaged IBM’s reputation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/?source=post_page-----7ed46783d282--------------------------------)
    [## IBM''s Watson supercomputer recommended ''unsafe and incorrect'' cancer treatments,
    internal documents…'
  prefs: []
  type: TYPE_NORMAL
- en: Slide decks presented last summer by an IBM Watson Health executive largely
    blame the problems on the training of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.statnews.com](https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/?source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '***4\. Security Risk***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Risk**: A significant societal risks emerge from the use of AI to create
    deepfakes, which involves generating highly convincing images or video that makes
    it appear as though individuals are saying or doing things they never actually
    did. Deepfakes have been used to create deceptive and realistic media that can
    be used for committing fraud, spreading misinformation, or damaging reputations.
    Moreover, AI can be weaponized for cyberattacks and social engineering, like tailoring
    phishing campaigns, which can introduce extensive security vulnerabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: In the political domain, use of deepfakes to fabricate speeches
    or actions could sway public opinion during critical events like elections. During
    the **Russia-Ukraine conflict**, a deepfake showing Ukrainian president appearing
    to tell his soldiers to lay down arms and surrender was circulated, an act that
    could have demoralized troops and provided time-sensitive tactical advantage to
    Russia. In the cybersecurity realm, AI-powered voice imitation was used to impersonate
    a CEO’s voice convincingly, leading to a fraudulent transfer of funds.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.npr.org/2022/03/16/1087062648/deepfake-video-zelenskyy-experts-war-manipulation-ukraine-russia?source=post_page-----7ed46783d282--------------------------------)
    [## Deepfake video of Zelenskyy could be ''tip of the iceberg'' in info war, experts
    warn'
  prefs: []
  type: TYPE_NORMAL
- en: A fake video of the Ukrainian president claiming defeat spread on social media
    on Wednesday.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.npr.org](https://www.npr.org/2022/03/16/1087062648/deepfake-video-zelenskyy-experts-war-manipulation-ukraine-russia?source=post_page-----7ed46783d282--------------------------------)
    [](https://www.trendmicro.com/vinfo/mx/security/news/cyber-attacks/unusual-ceo-fraud-via-deepfake-audio-steals-us-243-000-from-u-k-company?source=post_page-----7ed46783d282--------------------------------)
    [## Unusual CEO Fraud via Deepfake Audio Steals US$243,000 From UK Company
  prefs: []
  type: TYPE_NORMAL
- en: An unusual case of CEO fraud used a deepfake audio, an artificial intelligence
    (AI)-generated audio, and was reported…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.trendmicro.com](https://www.trendmicro.com/vinfo/mx/security/news/cyber-attacks/unusual-ceo-fraud-via-deepfake-audio-steals-us-243-000-from-u-k-company?source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**6 Step Framework for Mitigating AI Risks**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing AI risks requires a thoughtful approach throughout the entire product
    life-cycle. Below is a six step framework, organized by different stages of AI
    development, that organizations can adopt to ensure the responsible use of AI
    technology in their products.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef48c5b206b4ec5173e6f452dc24bc0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '***1\. Pre-Development: Ethical Groundwork and Design Principles***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before a single line of code is written, product teams should lay out the groundwork.
    Prioritize early engagement with a broad set of stakeholders, including users,
    technical experts, ethicists, legal professionals, and members of communities
    who may be impacted by the AI application. The goal is to identify both the overt
    and subtle risks associated with the product’s use case. Use these insights to
    chalk out the set of ethical guidelines and product capabilities that needs to
    be embedded into the product prior to its launch to preemptively address the identified
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: '***2\. Development: Data Consent, Integrity, Diversity***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is the bedrock of AI and also the most significant source of AI risks.
    It is critical to ensure that all data procured for model training are ethically
    sourced and comes with consent for its intended use. For example, **Adobe** trained
    its image generation model (**Firefly**) with proprietary data which allows it
    to provide legal protection to users against copyright lawsuits.
  prefs: []
  type: TYPE_NORMAL
- en: Further, Personally Identifiable Information (PII) should be removed from sensitive
    datasets used for training models to prevent potential harm. Access to such datasets
    should be appropriately gated and tracked to protect privacy. It’s equally important
    to ensure that the datasets represent the diversity of user base and the breadth
    of usage scenarios to mitigate bias and fairness risks. Companies like **Runway**
    have trained their text-to-image models with synthetic datasets containing AI-generated
    images of people from different ethnicities, genders, professions, and ages to
    ensure that their AI models exhibit diversity in the content they create.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.nasdaq.com/articles/how-adobes-copyright-protection-will-make-it-an-ai-leader?source=post_page-----7ed46783d282--------------------------------)
    [## How Adobe''s Copyright Protection Will Make it an AI Leader'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial intelligence (AI) is fundamentally reshaping many industries. AI
    will have many benefits, but it's also…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.nasdaq.com](https://www.nasdaq.com/articles/how-adobes-copyright-protection-will-make-it-an-ai-leader?source=post_page-----7ed46783d282--------------------------------)
    [](https://research.runwayml.com/publications/mitigating-stereotypical-biases-in-text-to-image-generative-systems?source=post_page-----7ed46783d282--------------------------------)
    [## Mitigating stereotypical biases in text to image generative systems | Runway
    Research
  prefs: []
  type: TYPE_NORMAL
- en: Reimagining creativity with artificial intelligence.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: research.runwayml.com](https://research.runwayml.com/publications/mitigating-stereotypical-biases-in-text-to-image-generative-systems?source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '***3\. Development: Robustness Testing and Implementing Guardrails***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The testing phase is pivotal in determining AI’s readiness for a public release.
    This involves comparing AI’s output against the curated set of verified results.
    An effective testing uses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance Metrics** aligned with user objectives and business values,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation Data** representing users from different demographics and covering
    a range of usage scenarios, including edge-cases'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to performance testing, it is also critical to implement guardrails
    that prevents AI from producing harmful results. For instance, **ImageFX**, **Google**’s
    Image generation service, proactively blocks users from generating content that
    could be deemed inappropriate or used to spread misinformation. Similarly, **Anthropic**
    has proactively set guardrails and measures to avoid misuse of its AI services
    in 2024 elections.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.google/technology/ai/google-imagen-2/?source=post_page-----7ed46783d282--------------------------------#:~:text=Our%20responsible%20approach%20to%20building%20Imagen%202)
    [## New and better ways to create images with Imagen 2'
  prefs: []
  type: TYPE_NORMAL
- en: We're rolling out Imagen 2, a major update to our image generation technology.
    Try it out today in Bard, Image FX…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'blog.google](https://blog.google/technology/ai/google-imagen-2/?source=post_page-----7ed46783d282--------------------------------#:~:text=Our%20responsible%20approach%20to%20building%20Imagen%202)
    [](https://www.linkedin.com/posts/anthropicresearch_preparing-for-global-elections-in-2024-activity-7164707718656643073-VcaW?utm_source=share&utm_medium=member_desktop&source=post_page-----7ed46783d282--------------------------------)
    [## Anthropic on LinkedIn: Preparing for global elections in 2024'
  prefs: []
  type: TYPE_NORMAL
- en: With high-stakes elections taking place around the world in 2024, we're shedding
    light on some of the work that our…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.linkedin.com](https://www.linkedin.com/posts/anthropicresearch_preparing-for-global-elections-in-2024-activity-7164707718656643073-VcaW?utm_source=share&utm_medium=member_desktop&source=post_page-----7ed46783d282--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '***4\. Development: Explainability & Empowerment***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In critical industry use cases where building trust is pivotal, it’s important
    for the AI to enable humans in an assistive role. This can be achieved by:'
  prefs: []
  type: TYPE_NORMAL
- en: Providing citations for the sources of the AI’s insights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlighting the uncertainty or confidence-level of the AI’s prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offering users the option to opt-out of using the AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating application workflows that ensure human oversight and prevent some
    tasks from being fully automated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***5\. Deployment: Progressive Roll Out & Transparency***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you transition the AI systems from development to real-world deployment,
    adopting a phased roll-out strategy is crucial for assessing risks and gathering
    feedback in a controlled setting. It’s also important to clearly communicate the
    AI’s intended use case, capabilities, and limitations to users and stakeholders.
    Transparency at this stage helps manage expectations and mitigates reputational
    risks associated with unexpected failures of the AI system.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI**, for example, demonstrated this approach with **Sora**, its latest
    text-to-video service, by initially making the service available to only a select
    group of red teamers and creative professionals. It has been upfront about Sora’s
    capabilities as well as its current limitations, such as challenges in generating
    video involving complex physical interactions. This level of disclosure ensures
    users understand where the technology excels and where it might fail, thereby
    managing expectations, earning users’ trust, and facilitating responsible adoption
    of the AI technology.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://openai.com/sora?source=post_page-----7ed46783d282--------------------------------#safety)
    [## Sora: Creating video from text'
  prefs: []
  type: TYPE_NORMAL
- en: Sora is an AI model that can create realistic and imaginative scenes from text
    instructions. Read technical report All…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: openai.com](https://openai.com/sora?source=post_page-----7ed46783d282--------------------------------#safety)
  prefs: []
  type: TYPE_NORMAL
- en: '***6\. Deployment: Monitoring, Feedback, and Adaptation***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After an AI system goes live, the work isn’t over. Now comes the task of keeping
    a close watch on how the AI behaves in the wild and tuning it based on what you
    find. Create an ongoing mechanism to track performance drifts and continually
    test and train the model on fresh data to avoid degradation in the AI performance.
    Make it easy for users to flag issues and use these insights to adapt AI and constantly
    update guardrails to meet high ethical standards. This will ensure that the AI
    systems remain reliable, trustworthy, and in step with the dynamic world they
    operate in.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion:**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As AI becomes further entrenched in our lives, managing ethical risks proactively
    is no longer optional — it is imperative. By embedding ethical considerations
    in every step of bringing AI products to life, companies not only mitigate risks
    but also build foundational trust with their users. The era of “*move fast and
    break things*” cannot be applied when dealing with technologies that are exponentially
    more powerful than anything we have seen before. There are no shortcuts when it
    comes to managing risks that can have far-reaching societal impacts.
  prefs: []
  type: TYPE_NORMAL
- en: AI product builders have a duty to society to move more intentionally and purposefully,
    making trust their true North Star. The future success and continued progress
    of AI hinges on getting the ethics right today.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks for reading! If these insights resonate with you or spark new thoughts,
    let’s continue the conversation. Share your perspectives in the comments below
    or connect with me on* [***LinkedIn***](https://www.linkedin.com/)*.*'
  prefs: []
  type: TYPE_NORMAL
