<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Set up a local LLM on CPU with chat UI in 15 minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Set up a local LLM on CPU with chat UI in 15 minutes</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06">https://towardsdatascience.com/set-up-a-local-llm-on-cpu-with-chat-ui-in-15-minutes-4cdc741408df?source=collection_archive---------1-----------------------#2024-02-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="83a0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post shows how to easily run an LLM locally and how to set up a ChatGPT-like GUI in 4 easy steps.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Kasper Groes Albin Ludvigsen" class="l ep by dd de cx" src="../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*d9Qyepa_yxVv9p34aZzifQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://kaspergroesludvigsen.medium.com/?source=post_page---byline--4cdc741408df--------------------------------" rel="noopener follow">Kasper Groes Albin Ludvigsen</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4cdc741408df--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4af6944f035f6558d066e724d2f69d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBfAwQTU38-iJlsRSBfslA@2x.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by Liudmila Shuvalova on Unsplash</figcaption></figure><p id="1037" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thanks to the global open source community, it is now easier than ever to run performant large language models (LLM) on consumer laptops or CPU-based servers and easily interact with them through well-designed graphical user interfaces.</p><p id="ec40" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is particularly valuable to all the organizations who are not allowed or not willing to use services that requires sending data to a third party.</p><p id="5abe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This tutorial shows how to set up a local LLM with a neat ChatGPT-like UI in four easy steps. If you have the prerequisite software installed, it will take you no more than 15 minutes of work (excluding the computer processing time used in some of the steps).</p><p id="c648" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This tutorial assumes you have the following installed on your machine:</p><ul class=""><li id="bb3b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><a class="af ob" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a></li><li id="e2c8" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">Docker</li><li id="8452" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">React</li><li id="4101" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx ny nz oa bk">Python and common packages including transformers</li></ul><p id="33e0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s get going.</p><h1 id="e540" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Step 1 — Decide which Huggingface LLM to use</h1><p id="3ec9" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The first step is to decide what LLM you want to run locally. Maybe you already have an idea. Otherwise, for English, the <a class="af ob" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">instruct version</a> of Mistral 7b seems to be the go-to choice. For Danish, I recommend Munin-NeuralBeagle although its known to over-generate tokens (perhaps because it’s a merge of a model that was not instruction fine tuned). For other Scandinavian languages, see <a class="af ob" href="https://scandeval.com/" rel="noopener ugc nofollow" target="_blank">ScandEval’s</a> evaluation of Scandinavian generative models.</p><p id="5d3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once you’ve decided which LLM to use, copy the Huggingface “path” to the model. For Mistral 7b it would be “mistralai/Mistral-7B-v0.1". You’ll need it in the next step.</p><div class="pi pj pk pl pm pn"><a rel="noopener follow" target="_blank" href="/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e?source=post_page-----4cdc741408df--------------------------------"><div class="po ab ig"><div class="pp ab co cb pq pr"><h2 class="bf fr hw z io ps iq ir pt it iv fp bk">How to make a PyTorch Transformer for time series forecasting</h2><div class="pu l"><h3 class="bf b hw z io ps iq ir pt it iv dx">This post will show you how to transform a time series Transformer architecture diagram into PyTorch code step by step.</h3></div><div class="pv l"><p class="bf b dy z io ps iq ir pt it iv dx">towardsdatascience.com</p></div></div><div class="pw l"><div class="px l py pz qa pw qb lr pn"/></div></div></a></div><h1 id="90e1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Step 2 – Quantize the LLM</h1><p id="76b1" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Next step is to quantize your chosen model unless you selected a model that was already quantized. If your model’s name ends in GGUF or GPTQ it’s already quantized.</p><p id="a9a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Quantization is a technique that converts the weights of a model (its learned parameters) to a smaller data type than the original, eg from fp16 to int4. This makes the model take up less memory and also makes it faster to run inference which is a nice feature if you’re running on CPU.</p><p id="8b9a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The script <code class="cx qc qd qe qf b">quantize.py</code>in my repo <a class="af ob" href="https://github.com/KasperGroesLudvigsen/local_llm" rel="noopener ugc nofollow" target="_blank">local_llm</a> is adapated from <a class="af ob" href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing" rel="noopener ugc nofollow" target="_blank">Maxime Labonne’s fantastic Colab notebook</a> (see his <a class="af ob" href="https://github.com/mlabonne/llm-course" rel="noopener ugc nofollow" target="_blank">LLM course</a> for other great LLM resources). You can use his notebook or my script. The method’s been tested on Mistral and Mistral-like models.</p><p id="0095" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To quantize, first clone my repo:</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="c02d" class="qj oi fq qf b bg qk ql l qm qn">git clone https://github.com/KasperGroesLudvigsen/local_llm.git<br/></span></pre><p id="553e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, change the <code class="cx qc qd qe qf b">MODEL_ID </code>variable in the <code class="cx qc qd qe qf b">quantize.py</code> file to reflect your model of choice. This is where you need the Huggingface “path” that you copied in the first step. So if you wanna use Mistral 7b:</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="e154" class="qj oi fq qf b bg qk ql l qm qn">MODEL_ID = "mistralai/Mistral-7B-v0.1"</span></pre><p id="6c7a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, in your terminal, run the script:</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="ebbf" class="qj oi fq qf b bg qk ql l qm qn">python quantize.py</span></pre><p id="a15d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This will take some time. While the quantization process runs, you can proceed to the next step.</p><p id="f237" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The script will produce a directory that contains the model files for the model you selected as well as the quantized version of the model which has the file extension “.gguf”.</p><div class="pi pj pk pl pm pn"><a rel="noopener follow" target="_blank" href="/chatgpts-energy-use-per-query-9383b8654487?source=post_page-----4cdc741408df--------------------------------"><div class="po ab ig"><div class="pp ab co cb pq pr"><h2 class="bf fr hw z io ps iq ir pt it iv fp bk">ChatGPT’s energy use per query</h2><div class="pu l"><h3 class="bf b hw z io ps iq ir pt it iv dx">How much electricity does ChatGPT use to answer one question?</h3></div><div class="pv l"><p class="bf b dy z io ps iq ir pt it iv dx">towardsdatascience.com</p></div></div><div class="pw l"><div class="qo l py pz qa pw qb lr pn"/></div></div></a></div><h1 id="a8ec" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Step 3: Build and run Ollama version of model</h1><p id="c7e0" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">We will run the model with <a class="af ob" href="https://ollama.ai/" rel="noopener ugc nofollow" target="_blank">Ollama</a>. Ollama is a software framework that neatly wraps a model into an API. Ollama also integrates easily with various front ends as we’ll see in the next step.</p><p id="4612" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To build an Ollama image of the model, you need a so-called model file which is a plain text file that configures the Ollama image. If you’re acquainted with Dockerfiles, Ollama’s model files will look familiar.</p><p id="a393" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the example below, we first specify which LLM to use. We’re assuming that there is a folder in your repo called <code class="cx qc qd qe qf b">mistral7b</code> and that the folder contains a model called <code class="cx qc qd qe qf b">quantized.gguf</code>. Then we specify the model’s context window to 8,000 – Mistral 7b’s max context size. In the Modelfile, you can also specify which prompt template to use, and you can specify stop tokens.</p><p id="14b5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Save the model file, eg as Modelfile.txt.</p><p id="f733" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For more configuration options, see <a class="af ob" href="https://github.com/ollama/ollama/blob/main/docs/modelfile.md" rel="noopener ugc nofollow" target="_blank">Ollama’s GitHub.</a></p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="ce48" class="qj oi fq qf b bg qk ql l qm qn">FROM ./mistral7b/quantized.gguf<br/><br/>PARAMETER num_ctx 8000<br/><br/>TEMPLATE """&lt;|im_start|&gt;system {{ .System }}&lt;|im_end|&gt;&lt;|im_start|&gt;user {{ .Prompt }}&lt;|im_end|&gt;&lt;|im_start|&gt;assistant&lt;|im_end|&gt;"""<br/><br/>PARAMETER stop &lt;|im_end|&gt;<br/>PARAMETER stop &lt;|im_start|&gt;user<br/>PARAMETER stop &lt;|end|&gt;</span></pre><p id="77b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that you have made the Modelfile, build an Ollama image from the Modelfile by running this from your terminal. This will also take a few moments:</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="2ee0" class="qj oi fq qf b bg qk ql l qm qn">ollama create choose-a-model-name -f &lt;location of the file e.g. ./Modelfile&gt;'</span></pre><p id="9ece" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When the “create” process is done, start the Ollama server by running this command. This will expose all your Ollama model(s) in a way that the GUI can interact with them.</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="5bfe" class="qj oi fq qf b bg qk ql l qm qn">ollama serve</span></pre><div class="pi pj pk pl pm pn"><a rel="noopener follow" target="_blank" href="/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----4cdc741408df--------------------------------"><div class="po ab ig"><div class="pp ab co cb pq pr"><h2 class="bf fr hw z io ps iq ir pt it iv fp bk">How to estimate and reduce the carbon footprint of machine learning models</h2><div class="pu l"><h3 class="bf b hw z io ps iq ir pt it iv dx">Two ways to easily estimate the carbon footprint of machine learning models and 17 ideas for how you might reduce it</h3></div><div class="pv l"><p class="bf b dy z io ps iq ir pt it iv dx">towardsdatascience.com</p></div></div><div class="pw l"><div class="qp l py pz qa pw qb lr pn"/></div></div></a></div><h1 id="b612" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Step 4 – Set up chat UI for Ollama</h1><p id="9c2d" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The next step is to set up a GUI to interact with the LLM. Several options exist for this. In this tutorial, we’ll use “Chatbot Ollama” – a very neat GUI that has a ChatGPT feel to it. “Ollama WebUI” is a similar option. You can also setup your own chat <a class="af ob" href="https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps" rel="noopener ugc nofollow" target="_blank">GUI with Streamlit</a>.</p><p id="15fc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By running the two commands below, you’ll first clone the Chatbot Ollama GitHub repo, and then install React dependencies:</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="52b7" class="qj oi fq qf b bg qk ql l qm qn">git clone https://github.com/ivanfioravanti/chatbot-ollama.git<br/>npm ci</span></pre><p id="365d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next step is to build a Docker image from the Dockerfile. If you’re on Linux, you need to change the OLLAMA_HOST environment variable in the Dockerfile from <code class="cx qc qd qe qf b">hhtp://host.docker.internal:11434</code>to <code class="cx qc qd qe qf b">http://localhost:11434</code> .</p><p id="b6c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, build the Docker image and run a container from it by executing these commands from a terminal. You need to stand in the root of the project.</p><pre class="mm mn mo mp mq qg qf qh bp qi bb bk"><span id="8650" class="qj oi fq qf b bg qk ql l qm qn">docker build -t chatbot-ollama .<br/><br/>docker run -p 3000:3000 chatbot-ollama</span></pre><p id="152d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The GUI is now running inside a Docker container on your local computer. In the terminal, you’ll see the address at which the GUI is available (eg. “http://localhost:3000")</p><p id="f7d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Visit that address in your browser, and you should now be able to chat with the LLM through the Ollama Chat UI.</p><h1 id="9a42" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Conclusion</h1><p id="38d5" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">This concludes this brief tutorial on how to easily set up chat UI that let’s you interact with an LLM that’s running on your local machine. Easy, right? Only four steps were required:</p><ol class=""><li id="4d23" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq nz oa bk">Select a model on Huggingface</li><li id="b682" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx qq nz oa bk">(Optional) Quantize the model</li><li id="58d1" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx qq nz oa bk">Wrap model in Ollama image</li><li id="c9c1" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx qq nz oa bk">Build and run a Docker container that wraps the GUI</li></ol><p id="d48d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Remember, it’s all made possible because open source is awesome 👏</p><p id="c99f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GitHub repo for this article: <a class="af ob" href="https://github.com/KasperGroesLudvigsen/local_llm" rel="noopener ugc nofollow" target="_blank">https://github.com/KasperGroesLudvigsen/local_llm</a></p></div></div></div><div class="ab cb qr qs qt qu" role="separator"><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx qy"/><span class="qv by bm qw qx"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="53a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That’s it! I hope you enjoyed the story. Let me know what you think!</p><p id="6e04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Get the benefits of Medium and support my writing by signing up for a Medium membership <a class="af ob" href="https://kaspergroesludvigsen.medium.com/membership" rel="noopener">HERE</a>.</p><p id="2916" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Follow me for more on AI and sustainability and <a class="af ob" href="https://kaspergroesludvigsen.medium.com/subscribe" rel="noopener">subscribe</a> to get my stories via email when I publish.</p><p id="745e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I also sometimes write about <a class="af ob" rel="noopener" target="_blank" href="/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e">time series forecasting</a>.</p><p id="285e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And feel free to connect on <a class="af ob" href="https://www.linkedin.com/in/kaspergroesludvigsen" rel="noopener ugc nofollow" target="_blank">LinkedIn</a>.</p></div></div></div></div>    
</body>
</html>