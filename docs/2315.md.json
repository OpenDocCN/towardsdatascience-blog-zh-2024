["```py\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n\n-Steps-\n1\\. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [{entity_types}]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"{tuple_delimiter}<entity_name>{tuple_delimiter}<entity_type>{tuple_delimiter}<entity_description>\n\n2\\. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"{tuple_delimiter}<source_entity>{tuple_delimiter}<target_entity>{tuple_delimiter}<relationship_description>{tuple_delimiter}<relationship_strength>)\n\n3\\. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2\\. Use **{record_delimiter}** as the list delimiter.\n\n4\\. When finished, output {completion_delimiter}\n\n<Multishot Examples>\n\n-Real Data-\n######################\nEntity_types: {entity_types}\nText: {input_text}\n######################\nOutput:\n```", "```py\nclass GraphExtractor:\n    def __init__(self, graph_db) -> None:\n        self.tuple_delimiter = \"<|>\"\n        self.record_delimiter = \"##\"\n        self.completion_delimiter = \"<|COMPLETE|>\"\n        self.entity_types = [\"organization\", \"person\", \"geo\", \"event\"]\n\n        self.graph_extraction_system = prompts.GRAPH_EXTRACTION_SYSTEM.format(\n            entity_types=\", \".join(self.entity_types),\n            record_delimiter=self.record_delimiter,\n            tuple_delimiter=self.tuple_delimiter,\n            completion_delimiter=self.completion_delimiter,\n        )\n\n        self.llm = LLMSession(system_message=self.graph_extraction_system,\n                              model_name=\"gemini-1.5-pro-001\")\n\n    def __call__(self, text_input: str, max_extr_rounds: int = 5) -> None:\n\n        input_prompt = self._construct_extractor_input(input_text=text_input)\n\n        print(\"+++++ Init Graph Extraction +++++\")\n\n        init_extr_result = self.llm.generate_chat(\n            client_query_string=input_prompt, temperature=0, top_p=0)\n        print(f\"Init result: {init_extr_result}\")\n\n        for round_i in range(max_extr_rounds):\n\n            print(f\"+++++ Contd. Graph Extraction round {round_i} +++++\")\n\n            round_response = self.llm.generate_chat(\n                client_query_string=prompts.CONTINUE_PROMPT, temperature=0, top_p=0)\n            init_extr_result += round_response or \"\"\n\n            print(f\"Round response: {round_response}\")\n\n            if round_i >= max_extr_rounds - 1:\n                break\n\n            completion_check = self.llm.generate_chat(\n                client_query_string=prompts.LOOP_PROMPT, temperature=0, top_p=0)\n\n            if \"YES\" not in completion_check:\n                print(\n                    f\"+++++ Complete with completion check after round {round_i} +++++\")\n                break\n```", "```py\n@dataclass\nclass EdgeData:\n    source_uid: str\n    target_uid: str \n    description: str\n    edge_uid: str | None = None\n    document_id: str | None = None\n\n@dataclass\nclass NodeData:\n    node_uid: str\n    node_title: str\n    node_type: str\n    node_description: str\n    node_degree: int\n    document_id: str \n    community_id: int | None = None # community id based on source document \n    edges_to: list[str] = field(default_factory=list)\n    edges_from: list[str] = field(default_factory=list)  # in case of directed graph\n    embedding: list[float] = field(default_factory=list)  # text embedding representing node e.g. combination of title & description\n\n@dataclass\nclass CommunityData:\n    title: str # title of comm, None if not yet computed\n    community_nodes: set[str] = field(default_factory=set) # list of node_uid belonging to community\n    summary: str | None = None # description of comm, None if not yet computed\n    document_id: str | None = None # identifier for source knowlede base document for this entity\n    community_uid: str | None = None # community identifier\n    community_embedding: Tuple[float, ...] = field(default_factory=tuple) # text embedding representing community\n    rating: int | None = None\n    rating_explanation: str | None = None\n    findings: list[dict] | None = None\n```", "```py\nfrom graph2nosql.graph2nosql.graph2nosql import NoSQLKnowledgeGraph\nfrom graph2nosql.databases.firestore_kg import FirestoreKG\nfrom graph2nosql.datamodel import data_model\n\nfskg = FirestoreKG(\n        gcp_project_id=project_id,\n        gcp_credential_file=firestore_credential_file,\n        firestore_db_id=database_id,\n        node_collection_id=node_coll_id,\n        edges_collection_id=edges_coll_id,\n        community_collection_id=community_coll_id)\n\nnode_data = data_model.NodeData(\n        node_uid=entity_name,\n        node_title=entity_name,\n        node_type=entity_type,\n        node_description=entity_description,\n        document_id=str(source_doc_id),\n        node_degree=0)\n\nfskg.add_node(node_uid=entity_name,node_data=node_data)\n```", "```py\n# clean graph off all nodes without any edges\nfskg.clean_zerodegree_nodes()\n\n# generate communities based on cleaned graph\ncomms = kg.get_louvain_communities()\n```", "```py\nYou are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.\n\n# Goal\nWrite a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community's key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.\n\n# Report Structure\n\nThe report should include the following sections:\n\n- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.\n- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.\n- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.\n- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.\n- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.\n```", "```py\ndef async_generate_comm_report(self, comm_members: set[str]) -> data_model.CommunityData:\n\n        llm = LLMSession(system_message=prompts.COMMUNITY_REPORT_SYSTEM,\n                         model_name=\"gemini-1.5-flash-001\")\n\n        response_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                    \"title\": {\n                        \"type\": \"string\"\n                    },\n                \"summary\": {\n                        \"type\": \"string\"\n                        },\n                \"rating\": {\n                        \"type\": \"int\"\n                        },\n                \"rating_explanation\": {\n                        \"type\": \"string\"\n                        },\n                \"findings\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"summary\": {\n                                    \"type\": \"string\"\n                                },\n                                \"explanation\": {\n                                    \"type\": \"string\"\n                                }\n                            },\n                            # Ensure both fields are present in each finding\n                            \"required\": [\"summary\", \"explanation\"]\n                        }\n                        }\n            },\n            # List required fields at the top level\n            \"required\": [\"title\", \"summary\", \"rating\", \"rating_explanation\", \"findings\"]\n        }\n\n        comm_report  = llm.generate(client_query_string=prompts.COMMUNITY_REPORT_QUERY.format(\n            entities=comm_nodes,\n            relationships=comm_edges,\n            response_mime_type=\"application/json\",\n            response_schema=response_schema\n        ))\n\ncomm_data = data_model.CommunityData(title=comm_report_dict[\"title\"],                                              summary=comm_report_dict[\"summary\"],                                                rating=comm_report_dict[\"rating\"],             rating_explanation=comm_report_dict[\"rating_explanation\"],               findings=comm_report_dict[\"findings\"],\ncommunity_nodes=comm_members)\n\n return comm_data\n```", "```py\ndef generate_response(client_query: str, community_report: dict):\n\n    llm = LLMSession(\n        system_message=MAP_SYSTEM_PROMPT,\n        model_name=\"gemini-1.5-pro-001\"\n    )\n\n    response_schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"response\": {\n                \"type\": \"string\",\n                \"description\": \"The response to the user question as raw string.\",\n            },\n            \"score\": {\n                \"type\": \"number\",\n                \"description\": \"The relevance score of the given community report context towards answering the user question [0.0, 10.0]\",\n            },\n        },\n        \"required\": [\"response\", \"score\"],\n    }\n\n    query_prompt = MAP_QUERY_PROMPT.format(\n        context_community_report=community_report, user_question=client_query)\n\n    response = llm.generate(client_query_string=query_prompt,\n                 response_schema=response_schema,\n                 response_mime_type=\"application/json\")\n\n    return response\n```", "```py\n---Role---\nYou are an expert agent answering questions based on context that is organized as a knowledge graph.\nYou will be provided with exactly one community report extracted from that same knowledge graph.\n\n---Goal---\nGenerate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the given community report.\n\nYou should use the data provided in the community description below as the only context for generating the response.\nIf you don't know the answer or if the input community description does not contain sufficient information to provide an answer respond \"The user question cannot be answered based on the given community context.\".\n\nYour response should always contain following elements:\n- Query based response: A comprehensive and truthful response to the given user query, solely based on the provided context.\n- Importance Score: An integer score between 0-10 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.\n\nThe response should be JSON formatted as follows:\n{{\"response\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}}\n\n---Context Community Report---\n{context_community_report}\n\n---User Question---\n{user_question}\n\n---JSON Response---\nThe json response formatted as follows:\n{{\"response\": \"Description of point 1 [Data: Reports (report ids)]\", \"score\": score_value}}\n\nresponse: \n```", "```py\nclass KGraphGlobalQuery:\n    def __init__(self) -> None:\n        # initialized with info on mq, knowledge graph, shared nosql state\n        pass\n\n    @observe()\n    def __call__(self, user_query: str) -> str:\n\n        # orchestration method taking natural language user query to produce and return final answer to client\n        comm_report_list = self._get_comm_reports()\n\n        # pair user query with existing community reports\n        query_msg_list = self._context_builder(\n            user_query=user_query, comm_report_list=comm_report_list)\n\n        # send pairs to pubsub queue for work scheduling\n        for msg in query_msg_list:\n            self._send_to_mq(message=msg)\n        print(\"int response request sent to mq\")\n\n        # periodically query shared state to check for processing compeltion & get intermediate responses\n        intermediate_response_list = self._check_shared_state(\n            user_query=user_query)\n\n        # based on helpfulness build final context\n        sorted_final_responses = self._filter_and_sort_responses(intermediate_response_list=intermediate_response_list)\n\n        # get full community reports for the selected communities\n        comm_report_list = self._get_communities_reports(sorted_final_responses)\n\n        # generate & return final response based on final context community repors and nodes.\n        final_response_system = prompts.GLOBAL_SEARCH_REDUCE_SYSTEM.format(\n            response_type=\"Detailled and wholistic in academic style analysis of the given information in at least 8-10 sentences across 2-3 paragraphs.\")\n\n        llm = LLMSession(\n            system_message=final_response_system,\n            model_name=\"gemini-1.5-pro-001\"\n        )\n\n        final_query_string = prompts.GLOBAL_SEARCH_REDUCE_QUERY.format(\n            report_data=comm_report_list,\n            user_query=user_query\n        )\n        final_response = llm.generate(client_query_string=final_query_string)\n        return final_response\n```"]