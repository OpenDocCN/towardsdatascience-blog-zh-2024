<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The Proof of Learning in Machine Learning/AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The Proof of Learning in Machine Learning/AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17">https://towardsdatascience.com/the-proof-of-learning-in-machine-learning-ai-4faae3c85fe6?source=collection_archive---------0-----------------------#2024-05-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d4c1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Before any mathematical development, we must first understand the foundation of learning and how it is closely linked to the concept of error</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Rômulo Pauliv" class="l ep by dd de cx" src="../Images/2a2d648d3ac84cf300bc7113b839c4b5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*PH4l7Hf-eajtpu6C"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@romulo_pauliv?source=post_page---byline--4faae3c85fe6--------------------------------" rel="noopener follow">Rômulo Pauliv</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4faae3c85fe6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="a9b3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">The Hypothetical Cook</h1><p id="7059" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Imagine that, on any given day, you decide to replicate a delicacy you ate at a renowned restaurant. You remember the taste of this delicacy perfectly. Based on this, you search for the recipe online and attempt to reproduce it at home.</p><p id="fe8a" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Let’s denote the taste of the delicacy you ate at the restaurant as <em class="og">T</em>, which will represent the expected taste, your <em class="og">target</em>. Based on the recipe you found online, you hope to achieve this goal, i.e., the taste <em class="og">T</em>.</p><p id="f94f" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To reproduce this recipe, you follow all the indicated steps, use all the ingredients, the necessary temperature, the cooking time, etc. Let’s denote all these methods and ingredients as <em class="og">X</em>.</p><p id="d403" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">After completing the entire process, you taste the dish. At this moment, you judge whether it is similar to the expected taste <em class="og">T</em>. You notice that it is saltier or sweeter than expected. The taste of the delicacy you reproduced at home will be represented by <em class="og">Y</em>.</p><p id="54b6" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, upon realizing that the taste is different from the target <em class="og">T</em>, you assign a quantitative measure of how different it is from the target taste based on taste <em class="og">Y</em>. In other words, you could have added more salt or less salt, more seasoning or less seasoning.</p><p id="dd9b" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The difference between <em class="og">T</em> and <em class="og">Y</em> can be defined as the error <em class="og">E</em>. The distinction between <em class="og">T </em>and <em class="og">Y</em> is made by the memory of your palate. Therefore, your palate performs a specific function at this moment, which we can define as <em class="og">P(Y) = E</em>. In other words, when experiencing taste <em class="og">Y</em>, the palate assigns the error <em class="og">E</em> based on the target taste <em class="og">T</em>.</p><p id="5899" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Having this quantitative measure of error <em class="og">E</em>, we can reproduce this recipe every day so that with each passing day, the error <em class="og">E </em>decreases. In other words, the distance between the target taste <em class="og">T</em> and the taste <em class="og">Y</em> decreases until <em class="og">T = Y</em>.</p><p id="b861" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Based on this hypothetical scenario, we can define error as the judgment that disagrees with the observed reality, where there is always a function that performs the action of judging. Therefore, in the above case, taste and memory created this judging function.</p><p id="75b7" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The act of learning, in this specific case, is characterized by the ability to reduce error. In other words, it is the ability to interact in different ways with the reproduced object in order to decrease the output of the judging function.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ad92" class="mj mk fq bf ml mm op gq mo mp oq gt mr ms or mu mv mw os my mz na ot nc nd ne bk">The Cook’s Expertise</h1><p id="c748" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Returning to the hypothetical case, we have the ingredients and methods <em class="og">X </em>as indicated by the recipe. All the ingredients and equipment are the same as those used by the restaurant; therefore, the outcome depends solely on your ability to manipulate them correctly to achieve the target taste <em class="og">T</em>.</p><p id="cac3" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">In other words, you manipulate <em class="og">X</em> to obtain <em class="og">Y</em>. Therefore, we can define that you are essentially a function that transforms <em class="og">X</em> into <em class="og">Y</em>, denoted as <em class="og">f(X) = Y</em>.</p><p id="1248" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The function <em class="og">f(X)</em>, which represents the act of manipulating the ingredients, also depends on how your brain functions. In other words, if you have had culinary experiences, you will find it easier to transform <em class="og">X</em> into <em class="og">Y</em>.</p><p id="639d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Let’s define <em class="og">W</em> as the weights of your neurons or your neural capacity to manipulate <em class="og">X</em>. If <em class="og">W</em> is already pre-adjusted based on culinary experiences, it will be easier to transform <em class="og">X</em> into <em class="og">Y</em>. Otherwise, we will need to adjust <em class="og">W</em> until we can transform <em class="og">X</em> into <em class="og">Y</em>.</p><p id="f5bd" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, we know that <em class="og">f(X) = Y</em> also depends on <em class="og">W</em>, i.e., we can represent it linearly where <em class="og">f(X) = WX</em>.</p><p id="f3b7" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Thus, our goal is to discover how we can modify <em class="og">W</em> until the generated <em class="og">Y</em> is very close or equal to <em class="og">T</em>. In other words, how can we adjust <em class="og">W</em> until the error <em class="og">E</em> significantly decreases or becomes zero.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4b8a" class="mj mk fq bf ml mm op gq mo mp oq gt mr ms or mu mv mw os my mz na ot nc nd ne bk">The Cost Function</h1><p id="9843" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The function that evaluates the difference between the result and the expected outcome is the <em class="og">cost function</em>. The function that converts the ingredients and culinary methods into the delicacy is our model, which can be an artificial neural network or other machine learning models.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/81c8069c15e37623b48e52f0d4d47639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxktvs3dOrnK4MjsnQflEg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (1)</figcaption></figure><p id="d35d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">In equation (1), the definition of the cost function <em class="og">E</em>, which depends on the <em class="og">n</em> weights <em class="og">w</em>. In other words, it is a function that indicates the error based on the values of <em class="og">w</em>. In a specific case where all <em class="og">n</em> weights <em class="og">w</em> are not adjusted, the value of the error <em class="og">E</em> will be large. Conversely, in a case where the weights are properly adjusted, the value of the error <em class="og">E</em> will be small or zero.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/a9881fc9e2264c7e7090d4622f42596f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdA5YuhuIiYD3O7PAMR6vQ.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (2)</figcaption></figure><p id="2fb2" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, our objective is to find the values of the <em class="og">n</em> weights <em class="og">w</em> such that the condition above is true.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="91db" class="mj mk fq bf ml mm op gq mo mp oq gt mr ms or mu mv mw os my mz na ot nc nd ne bk">The Gradient</h1><p id="58f3" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">To facilitate understanding of how we will do this, we will define the following function:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/3f31937eb0c958d9a052d0c932592352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQuTRkj0CL0OjpP423BxHA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Img. (1)</figcaption></figure><p id="7ba8" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, we intuitively know that when <em class="og">x = 0</em> and <em class="og">y = 0</em>, <em class="og">f(x, y) = 0</em>. However, we want an algorithm that, given random <em class="og">x</em> and <em class="og">y</em> values, adjusts the values of <em class="og">x</em> and <em class="og">y</em> until the function <em class="og">f(x, y)</em> equals zero.</p><p id="d8cd" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To achieve this, we can use the gradient of the function. In vector calculus, the gradient is a vector that indicates the direction and magnitude in which, by displacement from the specified point, we obtain the greatest possible increase in the value of a quantity.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/6b0a4bc945f0bc3b6c5593ea6afea079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oXwLY5vaIjuMDtnfH9Lwg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (3)</figcaption></figure><p id="a96d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">That is, by applying the gradient to the function <em class="og">f(x, y)</em>, we obtain a vector, as shown in the equation (3), which informs how to increment the values of <em class="og">x</em> and <em class="og">y</em> so that the value of <em class="og">f(x, y)</em> grows. However, our goal is to find the values of <em class="og">x</em> and <em class="og">y</em> necessary for the function <em class="og">f(x, y) = 0</em>. Therefore, we can use the negative gradient.</p><p id="15bb" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Below is a representation in two dimensions of the function <em class="og">f(x, y)</em> where the coloring shows the value of <em class="og">z</em>. Using the negative gradient, we see the vectors pointing to the minimum of the function.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/a0bb5dd854ce8c7cd8056e5bc0dcf2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWlzqo87OoenIZp5m9tSIg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Img. (2)</figcaption></figure><p id="b717" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Based on this, we can develop a method to update <em class="og">x</em> and <em class="og">y</em> using the gradient field of the function <em class="og">f(x, y)</em> to find the necessary values for <em class="og">f(x, y) = 0</em>.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="fcdc" class="mj mk fq bf ml mm op gq mo mp oq gt mr ms or mu mv mw os my mz na ot nc nd ne bk">The Proof of Learning</h1><p id="a373" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">We will define a simple function <em class="og">f(x)</em> for algorithm testing. Our intention is to find the minimum of this function. To do this, we can apply the gradient of <em class="og">f(x)</em>.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/2e5c69f07244b0fc525671c391f4667c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ENUQ2nDLHKtQnrsHeprvg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (4)</figcaption></figure><p id="7015" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Above, we have the gradient of the function <em class="og">f(x)</em>. We will not delve into defining the concept of derivative in this article, but we recommend <a class="af pn" href="https://www.britannica.com/science/derivative-mathematics" rel="noopener ugc nofollow" target="_blank">reading about its definition</a> and why we can represent it in this way.</p><p id="a3a9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Knowing that <em class="og">h</em> tends to zero, we can represent the gradient of <em class="og">f(x)</em> as follows:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/aacc5bc9b490877922185777eb5704ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yT9xvGSDcxrVLT3gPzIHew.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (5)</figcaption></figure><p id="0462" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Based on this, we can replace <em class="og">h</em> with the following term:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/38547a0b1525f66f6c154596eff08895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDI2BERccVZlZm0k6RTsag.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (6)</figcaption></figure><p id="1c66" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">We define the element <em class="og">alpha</em> to maintain the necessity of the term <em class="og">h</em>, where <em class="og">alpha</em> must be strictly positive and always tend to zero, identical to the term <em class="og">h</em>. Substituting the new relationship into the definition of derivative, we have:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/d2855f453f451dc2a29c66fe68e26d12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpvMran4KsWxXnoY1beqkw.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (7)</figcaption></figure><p id="9956" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Now we have a valuable relationship for our proof. We know that any element squared will be positive. From this concept comes the need to replace <em class="og">h</em> with minus <em class="og">alpha</em> times the <em class="og">gradient of f(x)</em>.</p><p id="1b2c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">So:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/eda81fa6ccc6d5f3476ae70aaf7b74fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GD01qasWM9l1de1LpTfgtg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (8)</figcaption></figure><p id="5de9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, we can judge that the condition in (8) is true as long as <em class="og">alpha</em> is always a positive value.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/3252475d5891fd3c42050d356336ae5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xvF5HCSxWp-OgekXCQgeA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (9)</figcaption></figure><p id="dc93" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">That is, the value of <em class="og">f(x)</em> being subtracted by a strictly positive value will always be less than the original value of <em class="og">f(x)</em>. Therefore, we can replace it with the following relationship using eq. (7) and (9):</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/9196a08c8585b7dabac2f3681905d4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YVC8j2lDyFDc5Mm0zxAhg.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (10)</figcaption></figure><p id="d17d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, we have a proven relationship on how to update the values of <em class="og">x </em>so that the function <em class="og">f(x)</em> is at least smaller than its previous state.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/38dc51d4143b3e80cc9a4c707e7278e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqeoQMW5tZmUMHEyDkItkA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (11)</figcaption></figure><p id="78b5" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">So, we know how to decrease the current <em class="og">x</em> to satisfy the inequality (11):</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/f860f57ff92a6314dc8800177c2adb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nk7oIJwoveblDHvfaMXDpA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (12)</figcaption></figure><p id="0498" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To confirm the validity of this relationship, we can apply this methodology to the function <em class="og">f(x, y)</em> in img. (1) whose behavior we know. So:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/d07ea17cab43755bdebf0265bf359b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDo5mA3pC37KNpd7Pfo7EA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (13)</figcaption></figure><p id="9076" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Applying this algorithm to the function <em class="og">f(x, y)</em> numerous times, we expect to see the value of the function decrease until it reaches the minimum. To do this, we conducted a simulation where, in addition, we applied noise to the assignment of the updated <em class="og">x</em> and <em class="og">y</em> to visualize the decrease in the value of <em class="og">f(x, y)</em>.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/54af915ff42ea099c2429f8cca2a7043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8uquBFrpxq26VaqkdZom5g.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Img. (3)</figcaption></figure><p id="4a85" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Notice that when the value of <em class="og">alpha</em> tends to zero, we observe the values of <em class="og">x</em> and <em class="og">y</em> tending to the minimum of the function. When this is not true, for example, at <em class="og">alpha = 0.6</em>, we observe a certain difficulty in finding the minimum of the function <em class="og">f(x, y)</em>.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="beb7" class="mj mk fq bf ml mm op gq mo mp oq gt mr ms or mu mv mw os my mz na ot nc nd ne bk">Gradient Descent</h1><p id="1ab6" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This algorithm is known as “Gradient Descent” or “Method of Steepest Descent,” being an optimization method to find the minimum of a function where each step is taken in the direction of the negative gradient. This method does not guarantee that the global minimum of the function will be found, but rather a local minimum.</p><p id="f5b0" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Discussions about finding the global minimum could be developed in another article, but here, we have mathematically demonstrated how the gradient can be used for this purpose.</p><p id="79d5" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Now, applying it to the cost function <em class="og">E</em> that depends on the <em class="og">n</em> weights <em class="og">w</em>, we have:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/efddc76046b63efd822e311b8e49e13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_FJQJkQEYcJHQOTLlPRBQ.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (14)</figcaption></figure><p id="800b" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To update all elements of <em class="og">W</em> based on gradient descent, we have:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/a714a82bbf8aa6a8fecdbcb25f225b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAx9hNvXl-0KSJMfMUnPqQ.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (15)</figcaption></figure><p id="6a36" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">And for any <em class="og">n</em>th element 𝑤 of the vector <em class="og">W</em>, we have:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/3e9670c17c0ae010568107356bcca08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMCkcbGLV_f7Eg9DxUxgqw.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Source: The Author. Eq. (16)</figcaption></figure><p id="1e60" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Therefore, we have our <em class="og">theoretical learning algorithm</em>. Logically, this is not applied to the hypothetical idea of the cook, but rather to numerous machine learning algorithms that we know today.</p><h1 id="eca8" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Conclusion</h1><p id="fbae" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Based on what we have seen, we can conclude the demonstration and the mathematical proof of the theoretical learning algorithm. Such a structure is applied to numerous learning methods such as AdaGrad, Adam, and Stochastic Gradient Descent (SGD).</p><p id="b6a3" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This method does not guarantee finding the <em class="og">n</em>-weight values <em class="og">w</em> where the <em class="og">cost function</em> yields a result of zero or very close to it. However, it assures us that a local minimum of the cost function will be found.</p><p id="947c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To address the issue of local minima, there are several more robust methods, such as SGD and Adam, which are commonly used in deep learning.</p><p id="2fe1" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Nevertheless, understanding the structure and the mathematical proof of the theoretical learning algorithm based on gradient descent will facilitate the comprehension of more complex algorithms.</p><h2 id="f5b2" class="po mk fq bf ml pp pq pr mo ps pt pu mr no pv pw px ns py pz qa nw qb qc qd qe bk">References</h2><p id="b5aa" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Carreira-Perpinan, M. A., &amp; Hinton, G. E. (2005). On contrastive divergence learning. In R. G. Cowell &amp; Z. Ghahramani (Eds.), Artificial Intelligence and Statistics, 2005. (pp. 33–41). Fort Lauderdale, FL: Society for Artificial Intelligence and Statistics.</p><p id="2932" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">García Cabello, J. Mathematical Neural Networks. Axioms 2022, 11, 80.</p><p id="0c10" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Geoffrey E. Hinton, Simon Osindero, Yee-Whye Teh. A Fast Learning Algorithm for Deep Belief Nets. Neural Computation 18, 1527–1554. Massachusetts Institute of Technology</p><p id="da33" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">LeCun, Y., Bottou, L., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324.</p></div></div></div></div>    
</body>
</html>