<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>What’s The Story With HNSW?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>What’s The Story With HNSW?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-the-story-with-hnsw-d1402c37a44e?source=collection_archive---------0-----------------------#2024-02-25">https://towardsdatascience.com/whats-the-story-with-hnsw-d1402c37a44e?source=collection_archive---------0-----------------------#2024-02-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="41da" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring the path to fast nearest neighbour search with Hierarchical Navigable Small Worlds</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ryan.mcdermott.000?source=post_page---byline--d1402c37a44e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan McDermott" class="l ep by dd de cx" src="../Images/90b50c1ae0cbfb0f636543488955d1da.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*jZa-4CCCm3JHcWv0zEW9UQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d1402c37a44e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ryan.mcdermott.000?source=post_page---byline--d1402c37a44e--------------------------------" rel="noopener follow">Ryan McDermott</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d1402c37a44e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d307bf1a8ec4a248af507dda79584af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxRVHq_GCgQGK8XAIW9MhA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by DALL·E 2 with the prompt “A bright abstract expressionist painting of a layered network of dots connected by lines.”</figcaption></figure><p id="fd95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hierarchical Navigable Small World (HNSW) has become popular as one of the best performing approaches for approximate nearest neighbour search. HNSW is a little complex though, and descriptions often lack a complete and intuitive explanation. This post takes a journey through the history of the HNSW idea to help explain what “hierarchical navigable small world” actually means and why it’s effective.</p><h1 id="7f32" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Contents</h1><ul class=""><li id="5382" class="nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx oz pa pb bk">Approximate Nearest Neighbour Search</li><li id="e8a6" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Small Worlds</li><li id="8c39" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Navigable Small Worlds</li><li id="c1a9" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Hierarchical Navigable Small Worlds</li><li id="0f37" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Summary</li><li id="3011" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">Appendix<br/>- Improved search<br/>- HNSW search &amp; insertion<br/>- Improved insertion</li><li id="dd86" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oz pa pb bk">References</li></ul><h1 id="9d38" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Approximate Nearest Neighbour Search</h1><p id="17bd" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">A common application of machine learning is <em class="ph">nearest neighbour search</em>, which means finding the most similar items* to a target — for example, to recommend items that are similar to a user’s preferences, or to search for items that are similar to a user’s search query.</p><p id="55c7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The simple method is to calculate the similarity of every item to the target and return the closest ones. However, if there are a large number of items (maybe millions), this will be slow.</p><p id="b90a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Instead, we can use a structure called an <em class="ph">index</em> to make things much faster.</p><p id="bd43" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There is a trade-off, however. Unlike the simple method, indexes only give approximate results: we may not retrieve all of the nearest neighbours (i.e. recall may be less than 100%).</p><p id="fa98" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are several different types of index (e.g. locality sensitive hashing; inverted file index), but HNSW has proven particularly effective on various datasets, achieving high speeds while keeping high recall.</p><p id="a0af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ph">*Typically, items are represented as </em><a class="af pi" href="https://www.pinecone.io/learn/vector-embeddings/" rel="noopener ugc nofollow" target="_blank">embeddings</a><em class="ph">, which are vectors produced by a machine learning model; the similarity between items corresponds to the </em>distance<em class="ph"> between the embeddings. This post will usually talk of vectors and distances, though in general HNSW can handle any kind of items with some measure of similarity.</em></p><h1 id="73f3" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Small Worlds</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/28b5f840ff812ef970754df771ce985f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*806zic3iZT3VYtgGRXhW6Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Illustration of the small-world experiment.</em></figcaption></figure><p id="a28f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Small worlds were famously studied in Stanley Milgram’s small-world experiment <a class="af pi" href="https://snap.stanford.edu/class/cs224w-readings/travers69smallworld.pdf" rel="noopener ugc nofollow" target="_blank">[1]</a>.</p><p id="f543" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Participants were given a letter containing the address and other basic details of a randomly chosen target individual, along with the experiment’s instructions. In the unlikely event that they personally knew the target, they were instructed to send them the letter; otherwise, they were told to think of someone they knew who was more likely to know the target, and send the letter on to them.</p><p id="24e3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The surprising conclusion was that the letters were typically only sent around six times before reaching the target, demonstrating the famous idea of “six degrees of separation” — any two people can usually be connected by a small chain of friends.</p><p id="05de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the mathematical field of graph theory, a <a class="af pi" href="https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)" rel="noopener ugc nofollow" target="_blank">graph</a> is a set of points, some of which are connected. We can think of a social network as a graph, with people as points and friendships as connections. The small-world experiment found that most pairs of points in this graph are connected by short paths that have a small number of steps. (This is described technically as the graph having a low <em class="ph">diameter</em>.)</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d9564c64321a802d7ff200d5f4e5c06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b2nQwR4IhOGq-Q7l"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Illustration of a small world. Most connections (grey) are local, but there are also long-range connections (green), which create short paths between points, such as the three step path between points A and B indicated with arrows.</em></figcaption></figure><p id="0e4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Having short paths is not that surprising in itself: most graphs have this property, including graphs created by just connecting random pairs of points. But social networks are not connected randomly, they are highly <em class="ph">local</em>: friends tend to live close to each other, and if you know two people, it’s quite likely they know each other too. (This is described technically as the graph having a high <em class="ph">clustering coefficient</em>.) The surprising thing about the small-world experiment is that two distant points are only separated by a short path despite connections typically being short-range.</p><p id="4a67" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In cases like these when a graph has lots of local connections, but also has short paths, we say the graph is a <strong class="ne fr">small world</strong>.</p><p id="7c3f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another good example of a small world is the global airport network. Airports in the same region are highly connected to one another, but it’s possible to make a long journey in only a few stops by making use of major hub airports. For example, a journey from Manchester, UK to Osaka, Japan typically starts with a local flight from Manchester to London, then a long distance flight from London to Tokyo, and finally another local flight from Tokyo to Osaka. Long-range hubs are a common way of achieving the small world property.</p><p id="470a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A final interesting example of graphs with the small world property is biological neural networks such as the human brain.</p><h1 id="62e2" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk"><em class="pj">Navigable</em> Small Worlds</h1><p id="b730" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In small world graphs, we can quickly reach a target in a few steps. This suggests a promising idea for nearest neighbour search: perhaps if we create connections between our vectors in such a way that it forms a small world graph, we can quickly find the vectors near a target by starting from an arbitrary “entry point” vector and then navigating through the graph towards the target.</p><p id="18aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This possibility was explored by Kleinberg [<a class="af pi" href="https://www.stat.berkeley.edu/users/aldous/Networks/swn-1.pdf" rel="noopener ugc nofollow" target="_blank">2</a>]. He noted that the existence of short paths wasn’t the only interesting thing about Miller’s experiment: it was also surprising that people were able to <em class="ph">find</em> these short paths, without using any global knowledge about the graph. Rather, the people were following a simple <a class="af pi" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank">greedy algorithm</a>. At each step, they examined each of their immediate connections, and sent it to the one they thought was closest to the target. We can use a similar algorithm to search a graph that connects vectors.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/1bbc1f0218a57d4865c256ca4ad22f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iG5pORh_ZifU3X1A"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Illustration of the greedy search algorithm. We are searching for the vector that is nearest the target X. Starting at the entry point E, we check the distance to X of each vector connected to E (indicated by the arrows from E), and go to the closest one (indicated by the red arrow from E). We repeat this procedure at successive vectors until we reach Y. As Y has no connections that are closer to X than Y itself, we stop and return Y.</em></figcaption></figure><p id="94f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Kleinberg wanted to know whether this greedy algorithm would always find a short path. He ran simple simulations of small worlds in which all of the points were connected to their immediate neighbours, with additional longer connections created between random points. He discovered that the greedy algorithm would only find a short path in specific conditions, depending on the lengths of the long-range connections.</p><p id="e520" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If the long-range connections were too long (as was the case when they connected pairs of points in completely random locations), the greedy algorithm could follow a long-range connection to quickly reach the rough area of the target, but after that the long-range connections were of no use, and the path had to step through the local connections to get closer. On the other hand, if the long-range connections were too short, it would simply take too many steps to reach the area of the target.</p><p id="893a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If, however, the lengths of the long-range connections were just right (to be precise, if they were uniformly distributed, so that all lengths were equally likely), the greedy algorithm would typically reach the neighbourhood of the target in an especially small number of steps (to be more specific, a number proportional to <em class="ph">log(n)</em>, where <em class="ph">n</em> is the number of points in the graph).</p><p id="17a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In cases like this where the greedy algorithm can find the target in a small number of steps, we say the small world is a <strong class="ne fr">navigable</strong> small world (NSW).</p><p id="1ceb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An NSW sounds like an ideal index for our vectors, but for vectors in a complex, high-dimensional space, it’s not clear how to actually build one. Fortunately, Malkov et al. [<a class="af pi" href="https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059" rel="noopener ugc nofollow" target="_blank">3</a>] discovered a method: we insert one randomly chosen vector at a time to the graph, and connect it to a small number <em class="ph">m</em> of nearest neighbours that were already inserted.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pk"><img src="../Images/c9cab3b1ade02d4f2cef6f39970e4546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YJU5RZbhvK4N8F94BG85RQ.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Illustration of building an NSW. Vectors are inserted in a random order and connected to the nearest m = 2 inserted vectors. Note how the first vectors to be inserted form long-range connections while later vectors form local connections.</em></figcaption></figure><p id="5d16" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This method is remarkably simple and requires no global understanding of how the vectors are distributed in space. It’s also very efficient, as we can use the graph built so far to perform the nearest neighbour search for inserting each vector.</p><p id="90b7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Experiments confirmed that this method produces an NSW. Because the vectors inserted early on are randomly chosen, they tend to be quite far apart. They therefore form the long-range connections needed for a small world. It’s not so obvious why the small world is navigable, but as we insert more vectors, the connections will get gradually shorter, so it’s plausible that the distribution of connection lengths will be fairly even, as required.</p><h1 id="f841" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Hierarchical Navigable Small Worlds</h1><p id="e858" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Navigable small worlds can work well for approximate nearest neighbours search, but further analysis revealed areas for improvement, leading Markov et al. [<a class="af pi" href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf" rel="noopener ugc nofollow" target="_blank">4</a>] to propose HNSW.</p><p id="b985" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A typical path through an NSW from the entry point towards the target went through two phases: a “zoom-out” phase, in which connection lengths increase from short to long, and a “zoom-in” phase, in which the reverse happens.</p><p id="c7ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The first simple improvement is to use a long-range hub (such as the first inserted vector) as the entry point. This way, we can skip the zoom-out phase and go straight to the zoom-in phase.</p><p id="b7d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Secondly, although the search paths were short (with a number of steps proportional to <em class="ph">log(n)</em>), the whole search procedure wasn’t so fast. At each vector along the path, the greedy algorithm must examine each of the connected vectors, calculating their distance to the target in order to choose the closest one. While most of the locally connected vectors had only a few connections, most long-range hubs had many connections (again, a number proportional to <em class="ph">log(n)</em>); this makes sense as these vectors were usually inserted early on in the building process and had many opportunities to connect to other vectors. As a result, the total number of calculations during a search was quite large (proportional to <em class="ph">log(n)²</em>).</p><p id="cfb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To improve this, we need to limit the number of connections checked at each hub. This leads to the main idea of HNSW: explicitly distinguishing between short-range and long-range connections. In the initial stage of a search, we will only consider the long-range connections between hubs. Once the greedy search has found a hub near the target, we then switch to using the short-range connections.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/45591095e3d1714a360f9685f2d4b2c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9yTgcnoAGjU5URxB"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="pj">Illustration of a search through an HNSW. We are searching for the vector nearest the target X. Long-range connections and hubs are green; short-range connections are grey. Arrows show the search path. Starting at the entry point E1, we perform a greedy search among the long-range connections, reaching E2, which is the nearest long-range hub to X. From there we continue the greedy search among the short-range connections, ending at Y, the nearest vector to X.</em></figcaption></figure><p id="8349" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the number of hubs is relatively small, they should have few connections to check. We can also explicitly impose a maximum number of long-range and short-range connections of each vector when we build the index. This results in a fast search time (proportional to <em class="ph">log(n)</em>).</p><p id="6335" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The idea of separate short and long connections can be generalised to include several intermediate levels of connection lengths. We can visualise this as a <strong class="ne fr">hierarchy</strong> of layers of connected vectors, with each layer only using a fraction of the vectors in the layer below.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pl ln pm lo pn cf po cg pp ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr pq pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/fa8d7c3a0bb33530bedb9691c64c77de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ysNNz4OqpBHjncxfjaRJyQ.png"/></div></figure><figure class="lb mr pq pr ps pt pu paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/274baca27e99378be90705c826d71310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gkDCSMc7vbI2LfaOFLVNvA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx pv ed pw px"><strong class="bf oa"><em class="pj">Left</em></strong><em class="pj">: illustration of an HNSW with three levels of connection length — short connections are grey, longer connections are green, and the longest connections are red. </em>E<em class="pj"> is the entry point. </em><strong class="bf oa"><em class="pj">Right</em></strong><em class="pj">: visualising the HNSW as a stack of three layers. Dotted lines indicate the location of the same vector in the layer below.</em></figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0cee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The best number of layers (and other parameters like the maximum number of connections of each vector) can be found by experiment; there are also heuristics suggested in the HNSW paper.</p><p id="b359" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Incidentally, HNSW also generalises another data structure called a <a class="af pi" href="https://en.wikipedia.org/wiki/Skip_list" rel="noopener ugc nofollow" target="_blank">skip list</a>, which enables fast searching of sorted one-dimensional values (rather than multi-dimensional vectors).</p><p id="8179" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building an HNSW uses similar ideas to NSW. Vectors are inserted one at a time, and long-range connections are created through connecting random vectors — although in HNSW, these vectors are randomly chosen throughout the whole building process (while in NSW they were the first vectors in the random order of insertion).</p><p id="387f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To be precise, whenever a new vector is inserted, we first use a random function to choose the highest layer in which it will appear. All vectors appear in the bottom layer; a fraction of those also appear in the first layer up; a fraction of those also appear in the second layer up; and so on.</p><p id="9707" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Similar to NSW, we then connect the inserted vector to its <em class="ph">m</em> nearest neighbours in each layer that it appears; we can search for these neighbours efficiently using the index built so far. As the vectors become more sparse in higher layers, the connections typically become longer.</p><h1 id="643c" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Summary</h1><p id="bff2" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">This completes the discussion of the main ideas leading to HNSW. To summarise:</p><p id="4a23" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A small world is a graph that connects local points but also has short paths between distant points. This can be achieved through hubs with long-range connections.</p><p id="3624" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building these long-range connections in the right way results in a small world that is navigable, meaning a greedy algorithm can quickly find the short paths. This enables fast nearest neighbour search.</p><p id="414d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One such method for building the connections is to insert vectors in a random order and connect them to their nearest neighbours. However, this leads to long-range hubs with many connections, and a slower search time.</p><p id="0063" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To avoid this, a better method is to separately build connections of different lengths by choosing random vectors to use as hubs. This gives us the HNSW index, which significantly increases the speed of nearest neighbour search.</p><h1 id="96cd" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Appendix</h1><p id="23b0" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The post above provides an overview of the HNSW index and the ideas behind it. This appendix discusses additional interesting details of the HNSW algorithms for readers seeking a complete understanding. See the references for further details and pseudocode.</p><h2 id="3350" class="py nz fq bf oa pz qa qb od qc qd qe og nl qf qg qh np qi qj qk nt ql qm qn qo bk">Improved search</h2><p id="6b43" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Navigable small world methods only give approximate results for nearest neighbour search. Sometimes, the greedy search algorithm stops before finding the nearest vector to the target. This happens when the search path encounters a “false local optimum”, meaning the vector’s immediate connections are all further from the target, although there is a closer vector somewhere else in the graph.</p><p id="b24e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Things can be improved by performing several independent searches from different entry points, the results of which can give us several good <em class="ph">candidates </em>for the nearest neighbour. We then calculate the distance of all of the candidates to the target, and return the closest one.</p><p id="72fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we want to find more than one (say <em class="ph">k</em>) nearest neighbours, we can first expand the set of candidates by adding all of their immediate connections, before calculating the distances to the target and returning the closest <em class="ph">k</em>.</p><p id="edb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This simple method for finding candidates has some shortcomings. Each greedy search path is still at risk of ending at a false local optimum; this could be improved by exploring beyond the immediate connections of each vector. Also, a search path may encounter several vectors towards the end which are close to the target, but aren’t chosen as candidates (because they aren’t the final vector in the path or one of its immediate connections).</p><p id="d4d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Rather than following several greedy paths independently, a more effective approach is to follow a <em class="ph">set</em> of vectors, updating the whole set in a greedy fashion.</p><p id="654d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To be precise, we will maintain a set containing the closest vectors to the target encountered so far, along with their distances to the target. This set holds a maximum of <em class="ph">ef</em> vectors, where <em class="ph">ef</em> is the desired number of candidates. Initially, the set contains the entry points. We then proceed by a greedy process, evaluating each vector in the set by checking its connections.</p><p id="98d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All vectors in the set are initially marked as “unevaluated”. At each step, we evaluate the closest unevaluated vector to the target (and mark it as “evaluated”). Evaluating the vector means checking each of its connected vectors by calculating that vector’s distance to the target, and inserting it into the set (marked as “unevaluated”) if it’s closer than some of the vectors there (pushing the furthest vector out of the set if it’s at maximum capacity). (We also keep track of the vectors for which we’ve already calculated the distance, to avoid repeating work.)</p><p id="7df8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The process ends when all of the vectors in the set have been evaluated and no new vectors have been inserted. The final set is returned as the candidates, from which we can take the closest vector or <em class="ph">k</em> closest vectors to the target.</p><p id="f741" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">(Note that for <em class="ph">ef = 1</em>, this algorithm is simply the basic greedy search algorithm.)</p><h2 id="0c0a" class="py nz fq bf oa pz qa qb od qc qd qe og nl qf qg qh np qi qj qk nt ql qm qn qo bk">HNSW search &amp; insertion</h2><p id="3eb3" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">The above describes a search algorithm for an NSW, or a single layer of an HNSW.</p><p id="b9a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To search the whole HNSW structure, the suggested approach is to use basic greedy search for the nearest neighbour in each layer from the top until we reach the layer of interest, at which point we use the layer search algorithm with several candidates.</p><p id="6764" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For performing a k-nearest neighbours search (including <em class="ph">k = 1</em>) on the completed index, this means using basic greedy search until we reach the bottom layer, at which point we use the layer search algorithm with <em class="ph">ef = efSearch</em> candidates. <em class="ph">efSearch</em> is a parameter to be tuned; higher <em class="ph">efSearch</em> is slower but more accurate.</p><p id="d5e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For inserting a vector into HNSW, we use basic greedy search until the first layer in which the new vector appears. Here, we search for the <em class="ph">m</em> nearest neighbours using layer search with <em class="ph">ef = efConstruction</em> candidates. We also use the candidates as the entry points for continuing the process in the next layer down.</p><h2 id="b40b" class="py nz fq bf oa pz qa qb od qc qd qe og nl qf qg qh np qi qj qk nt ql qm qn qo bk">Improved insertion</h2><p id="2846" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">NSW introduced a simple method of building the graph in which each inserted vector is connected to its <em class="ph">m</em> nearest neighbours. While this method for choosing connections also works for HNSW, a modified approach was introduced which significantly improves the performance of the resulting index.</p><p id="3060" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As usual, we start by finding <em class="ph">efConstruction</em> candidate vectors. We then go through these candidates in order of increasing distance from the inserted vector and connect them. However, if a candidate is closer to one of the newly connected candidates than it is to the inserted vector, we skip over it without connecting. We stop when <em class="ph">m</em> candidates have been connected.</p><p id="c305" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The idea is that we can already reach the candidate from the inserted vector through a newly connected candidate, so it’s a waste to also add a direct connection; it’s better to connect a more distant point. This increases the diversity of connections in the graph, and helps connect nearby clusters of vectors.</p><h1 id="8af3" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">References</h1><p id="f41a" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">[1] J. Travers and S. Milgram, <a class="af pi" href="https://snap.stanford.edu/class/cs224w-readings/travers69smallworld.pdf" rel="noopener ugc nofollow" target="_blank">An Experimental Study of the Small World Problem</a> (1969), Sociometry</p><p id="88f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] J. Kleinberg, <a class="af pi" href="https://www.stat.berkeley.edu/users/aldous/Networks/swn-1.pdf" rel="noopener ugc nofollow" target="_blank">The Small-World Phenomenon: An Algorithmic Perspective</a> (2000), Proceedings of the thirty-second annual ACM symposium on Theory of Computing</p><p id="cc60" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Y. Malkov, A. Ponomarenko, A. Logvinov and V. Krylov, <a class="af pi" href="https://publications.hse.ru/mirror/pubs/share/folder/x5p6h7thif/direct/128296059" rel="noopener ugc nofollow" target="_blank">Approximate nearest neighbor algorithm based on navigable small world graphs</a> (2014), Information Systems, vol. 45<br/><em class="ph">(There are several similar papers; this one is the most recent and complete, and includes the more advanced k-nearest neighbours search algorithm.)</em></p><p id="7aaa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] Y. Malkov and D. Yashunin, <a class="af pi" href="https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf" rel="noopener ugc nofollow" target="_blank">Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</a> (2016), IEEE Transactions on Pattern Analysis and Machine Intelligence</p><p id="239a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All images are created by the author and free to use with a citation.</p></div></div></div></div>    
</body>
</html>