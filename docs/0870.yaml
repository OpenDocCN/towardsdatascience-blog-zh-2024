- en: 'Deploying Large Language Models: vLLM and Quantization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05](https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step-by-step guide on how to accelerate large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Ayoola
    Olafenwa](../Images/86914a74cbf83e711887fab896e318a9.png)](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    [Ayoola Olafenwa](https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------)
    ·9 min read·Apr 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/399881e6f19fe28227d9982dd335ab20.png)'
  prefs: []
  type: TYPE_IMG
- en: '[source](https://unsplash.com/photos/a-computer-chip-with-the-letter-a-on-top-of-it-eGGFZ5X2LnA)'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment of Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We live in an amazing time of Large Language Models like ChatGPT, GPT-4, and
    Claude that can perform multiple amazing tasks. In practically every field, ranging
    from education, healthcare to arts and business, Large Language Models are being
    used to facilitate efficiency in delivering services. Over the past year, many
    brilliant open-source Large Language Models, such as Llama, Mistral, Falcon, and
    Gemma, have been released. These open-source LLMs are available for everyone to
    use, but deploying them can be very challenging as they can be very slow and require
    a lot of GPU compute power to run for real-time deployment. Different tools and
    approaches have been created to simplify the deployment of Large Language Models.
  prefs: []
  type: TYPE_NORMAL
- en: Many deployment tools have been created for serving LLMs with faster inference,
    such as vLLM, c2translate, TensorRT-LLM, and llama.cpp. Quantization techniques
    are also used to optimize GPUs for loading very large Language Models. In this
    article, I will explain how to deploy Large Language Models with vLLM and quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency and Throughput**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some of the major factors that affect the speed performance of a Large Language
    Model are GPU hardware requirements and model size. The larger the size of the
    model, the more GPU compute power is required to run it. Common benchmark metrics
    used in measuring the speed performance of a Large Language Model are ***Latency***
    and ***Throughput***.
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency:** This is the time required for a Large Language Model to generate
    a response. It is usually measured in seconds or milliseconds.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput:** This is the number of tokens generated per second or millisecond
    from a Large Language Model.'
  prefs: []
  type: TYPE_NORMAL
- en: Install Required Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below are the two required packages for running a Large Language Model: Hugging
    Face ***transformers*** and ***accelerate***.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: What is Phi-2?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Phi-2*** is a state-of-the-art foundation model from Microsoft with 2.7
    billion parameters. It was pre-trained with a variety of data sources, ranging
    from code to textbooks. Learn more about ***Phi-2*** from [here](https://huggingface.co/microsoft/phi-2).'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking LLM Latency and Throughput with Hugging Face Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generated Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step By Step Code Breakdown**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 6–10:** Loaded ***Phi-2*** model and tokenized the prompt “***Generate
    a python code that accepts a list of numbers and returns the sum.***”'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 12- 18:** Generated a response from the model and obtained the ***latency***
    by calculating the time required to generate the response.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 21–23:** Obtained the total length of tokens in the response generated,
    divided it by the ***latency*** and calculated the ***throughput***.'
  prefs: []
  type: TYPE_NORMAL
- en: This model was run on an A1000 (16GB GPU), and it achieves a ***latency*** of
    ***2.7 seconds*** and a throughput of ***32 tokens/second.***
  prefs: []
  type: TYPE_NORMAL
- en: Deployment of A Large Language Model with vLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: vLLM is an open source LLM library for serving Large Language Models at low
    ***latency*** and high ***throughput***.
  prefs: []
  type: TYPE_NORMAL
- en: How vLLM works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transformer is the building block of Large Language Models. The transformer
    network uses a mechanism called the ***attention mechanism***, which is used by
    the network to study and understand the context of words. The ***attention mechanism***
    is made up of a bunch of mathematical calculations of matrices known as attention
    keys and values. The memory used by the interaction of these attention keys and
    values affects the speed of the model. vLLM introduced a new attention mechanism
    called ***PagedAttention*** that efficiently manages the allocation of memory
    for the transformer’s attention keys and values during the generation of tokens.
    The memory efficiency of vLLM has proven very useful in running Large Language
    Models at low latency and high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: This is a high-level explanation of how vLLM works. To learn more in-depth technical
    details, visit the vLLM documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub | Documentation | Paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Install vLLM**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Run Phi-2 with vLLM**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generated Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step By Step Code Breakdown**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 1–3:** Imported required packages from vLLM for running ***Phi-2***.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 5–8:** Loaded ***Phi-2*** with vLLM, defined the prompt and set important
    parameters for running the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 10–16:** Generated the model’s response using ***llm.generate*** and
    computed the ***latency***.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 19–21:** Obtained the length of total tokens generated from the response,
    divided the length of tokens by the latency to get the ***throughput***.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 23–24:** Obtained the generated text.'
  prefs: []
  type: TYPE_NORMAL
- en: I ran ***Phi-2*** with vLLM on the same prompt, ***“Generate a python code that
    accepts a list of numbers and returns the sum.”*** On the same GPU, an A1000 (16GB
    GPU), vLLM produces a ***latency*** of ***1.2 seconds*** and a ***throughput***
    of ***63 tokens/second***, compared to Hugging Face transformers’ ***latency***
    of ***2.85 seconds*** and a ***throughput*** of ***32 tokens/second.*** Running
    a Large Language Model with vLLM produces the same accurate result as using Hugging
    Face, with much lower latency and higher throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The metrics (latency and throughput) I obtained for vLLM are estimated
    benchmarks for vLLM performance. The model generation speed depends on many factors,
    such as the length of the input prompt and the size of the GPU. According to the
    official vLLM report, running an LLM model on a powerful GPU like the A100 in
    a production setting with vLLM achieves **24x higher throughput** than Hugging
    Face Transformers.'
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Latency and Throughput in Real Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The way I calculated the latency and throughput for running Phi-2 is experimental,
    and I did this to explain how vLLM accelerates a Large Language Model’s performance.
    In the real-world use case of LLMs, such as a chat-based system where the model
    outputs a token as it is generated, measuring the latency and throughput is more
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: A chat-based system is based on streaming output tokens. Some of the major factors
    that affect the LLM metrics are ***Time to First Token*** (the time required for
    a model to generate the first token), ***Time Per Output Token*** (the time spent
    per output token generated), ***the input sequence length, the expected output,
    the total expected output tokens***, and ***the model size***. In a chat-based
    system, the latency is usually a combination of ***Time to First Token*** and
    ***Time Per Output Token*** multiplied by the total expected output tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The longer the input sequence length passed into a model, the slower the response.
    Some of the approaches used in running LLMs in real-time involve batching users’
    input requests or prompts to perform inference on the requests concurrently, which
    helps in improving the throughput. Generally, using a powerful GPU and serving
    LLMs with efficient tools like vLLM improves both the latency and throughput in
    real-time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Run the vLLM deployment on Google Colab**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: colab.research.google.com](https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Quantization of Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantization is the conversion of a machine learning model from a higher precision
    to a lower precision by shrinking the model’s weights into smaller bits, usually
    ***8-bit*** or ***4-bit***. Deployment tools like vLLM are very useful for inference
    serving of Large Language Models at very low latency and high throughput. We are
    able to run ***Phi-2*** with Hugging Face and vLLM conveniently on the T4 GPU
    on Google Colab because it is a smaller LLM with ***2.7 billion parameters***.
    For example, a 7-billion-parameter model like ***Mistral 7B*** cannot be run on
    Colab with either Hugging Face or vLLM. Quantization is best for managing GPU
    hardware requirements for Large Language Models. When GPU availability is limited
    and we need to run a very large Language Model, quantization is the best approach
    to load LLMs on constrained devices.
  prefs: []
  type: TYPE_NORMAL
- en: BitsandBytes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a python library built with custom quantization functions for shrinking
    model’s weights into lower bits(***8-bit*** and ***4-bit***).
  prefs: []
  type: TYPE_NORMAL
- en: '**Install BitsandBytes**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Quantization of Mistral 7B Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '***Mistral 7B***, a 7-billion-parameter model from MistralAI, is one of the
    best state-of-the-art open-source Large Language Models. I will go through a step-by-step
    process of running ***Mistral 7B*** with different quantization techniques that
    can be run on the T4 GPU on Google Colab.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization with 8bit Precision**: This is the conversion of a machine learning
    model’s weight into 8-bit precision. ***BitsandBytes*** has been integrated with
    Hugging Face transformers to load a language model using the same Hugging Face
    code, but with minor modifications for quantization.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 1:** Imported the needed packages for running model, including the ***BitsandBytesConfig***
    library.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 3–4:** Defined the quantization config and set the parameter ***load_in_8bit***
    to true for loading the model’s weights in ***8-bit*** precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 7–9:** Passed the quantization config into the function for loading
    the model, set the parameter ***device_map*** for ***bitsandbytes*** to automatically
    allocate appropriate GPU memory for loading the model. Finally loaded the tokenizer
    weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization with 4bit Precision**: This is the conversion of a machine learning
    model’s weight into ***4-bi***t precision.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for loading ***Mistral 7B*** in 4-bit precision is similar to that
    of ***8-bit*** precision except for a few changes:'
  prefs: []
  type: TYPE_NORMAL
- en: changed ***load_in_8bit*** to ***load_in_4bit***.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new parameter ***bnb_4bit_compute_dtype*** is introduced into the ***BitsandBytesConfig***
    to perform the model’s computation in ***bfloat16***. ***bfloat16*** is computation
    data type for loading model’s weights for faster inference. It can be used with
    both **4-bit** and ***8-bit*** precisions***.*** If it is in ***8-bit*** you just
    need to change the parameter from ***bnb_4bit_compute_dtype*** to ***bnb_8bit_compute_dtype.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NF4(4-bit Normal Float) and **Double Quantization**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**NF4 (4-bit Normal Float)** from QLoRA is an optimal quantization approach
    that yields better results than the standard 4-bit quantization. It is integrated
    with double quantization, where quantization occurs twice; quantized weights from
    the first stage of quantization are passed into the next stage of quantization,
    yielding optimal float range values for the model’s weights. According to the
    report from the QLoRA paper, ***NF4 with double quantization*** does not suffer
    from a drop in accuracy performance. Read more in-depth technical details about
    NF4 and Double Quantization from the QLoRA paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)
    [## QLoRA: Efficient Finetuning of Quantized LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: We present QLoRA, an efficient finetuning approach that reduces memory usage
    enough to finetune a 65B parameter model…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Line 4–9:** Extra parameters were set the ***BitsandBytesConfig:***'
  prefs: []
  type: TYPE_NORMAL
- en: '***load_4bit:*** loading model in 4-bit precision is set to true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***bnb_4bit_quant_type:*** The type of quantization is set to nf4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***bnb_4bit_use_double_quant:*** Double quantization is set to True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***bnb_4_bit_compute_dtype:*** ***bfloat16*** computation data type is used
    for faster inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Line 11–13:** Loaded the model’s weights and tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Full Code for Model Quantization**'
  prefs: []
  type: TYPE_NORMAL
- en: Generated Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Quantization is a very good approach for optimizing the running of very Large
    Language Models on smaller GPUs and can be applied to any model, such as Llama
    70B, Falcon 40B, and mpt-30b. According to reports from the [LLM.int8 paper](https://arxiv.org/abs/2208.07339),
    very Large Language Models suffer less from accuracy drops when quantized compared
    to smaller ones. Quantization is best applied to very Large Language Models and
    does not work well for smaller models because of the loss in accuracy performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Run Mixtral 7B Quantization on Google Colab**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
    [## Google Colaboratory'
  prefs: []
  type: TYPE_NORMAL
- en: Edit description
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: colab.research.google.com](https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&source=post_page-----becfe17396a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, I provided a step-by-step approach to measuring the speed performance
    of a Large Language Model, explained how vLLM works, and how it can be used to
    improve the latency and throughput of a Large Language Model. Finally, I explained
    quantization and how it is used to load Large Language Models on small-scale GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reach to me via:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: [olafenwaayoola@gmail.com](https://mail.google.com/mail/u/0/#inbox)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linkedin: [https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/](https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [## vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub | Documentation | Paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: blog.vllm.ai](https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------)
    [](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [## Making LLMs even more accessible with bitsandbytes, 4-bit quantization and
    QLoRA
  prefs: []
  type: TYPE_NORMAL
- en: We're on a journey to advance and democratize artificial intelligence through
    open source and open science.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: huggingface.co](https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [## Understanding performance benchmarks for LLM inference
  prefs: []
  type: TYPE_NORMAL
- en: This guide helps you interpret LLM performance metrics to make direct comparisons
    on latency, throughput, and cost.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.baseten.co](https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------)
    [](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)
    [## What are Quantized LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: Discover the power of quantized LLMs! Learn how model quantization reduces size,
    enables efficient hardware usage, and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.tensorops.ai](https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
