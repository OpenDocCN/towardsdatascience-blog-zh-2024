<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Unimodals to Multimodality: DIY Techniques for Building Foundational Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Unimodals to Multimodality: DIY Techniques for Building Foundational Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25">https://towardsdatascience.com/from-open-source-unimodal-to-multimodal-diy-techniques-for-building-foundational-models-e1df92276379?source=collection_archive---------10-----------------------#2024-06-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a255" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A comprehensive tutorial: Using advanced techniques like prompt adaptation and adapters to transform open-source unimodal models into multimodal ones, including all variants of LLaMA-Adapters, LLaVa, MiniGPT-4, and more.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Elahe Aghapour" class="l ep by dd de cx" src="../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DfA3l4L2kLpNaOAUK9Rb4g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--e1df92276379--------------------------------" rel="noopener follow">Elahe Aghapour</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e1df92276379--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6dd6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Authors:</strong> <span class="ia"><span class="ia" aria-hidden="false"><a class="ne ib nf" href="https://medium.com/u/75214fb27311?source=post_page---user_mention--e1df92276379--------------------------------" rel="noopener" target="_blank">Elahe Aghapour</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="ne ib nf" href="https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--e1df92276379--------------------------------" rel="noopener" target="_blank">Salar Rahili</a></span></span></p><p id="510f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">INTRODUCTION</em></strong></p><p id="edf3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">With recent advancements in large language models (LLMs), AI has become the spotlight of technology. We’re now more eager than ever to reach AGI-level intelligence. Yet, achieving a human-like understanding of our surroundings involves much more than just mastering language and text comprehension. Humans use their five senses to interact with the world and act based on these interactions to achieve goals. This highlights that the next step for us is to develop large models that incorporate multimodal inputs and outputs, bringing us closer to human-like capabilities. However, we face two main obstacles. First, we need a multimodal labeled dataset, which is not as accessible as text data. Second, we are already pushing the limits of compute capacity for training models with textual data. Increasing this capacity to include other modalities, especially high-dimensional ones like images and videos, is incredibly challenging.</p><p id="563c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These limitations have been a barrier for many AI researchers aiming to create capable multimodal models. So far, only a few well-established companies like Google, Meta, and OpenAI have managed to train such models. However, none of these prominent models are open source, and only a few APIs are available for public use. This has forced researchers, especially in academia, to find ways to build multimodal models without massive compute capabilities, relying instead on open-sourced pre-trained models, which are mostly single modal.</p><p id="c1a2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this blog, we focus on successful, low-effort approaches to creating multi-modal models. Our criteria are centered on projects where the compute costs remain a few thousand dollars, assuming this is within the budget a typical lab can afford.</p><p id="1568" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">1- Parameter-Efficient Fine-Tuning (PEFT)</em></strong></p><p id="b51a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Before we dive into the proposed approaches for integrating and aligning two pre-trained models, we need to discuss the mechanics of fine-tuning a large model with limited compute power. Therefore, we’ll start by exploring Parameter-Efficient Fine-Tuning (PEFT) and then describe how these methods can be further used to align pre-trained models and build open-source multimodal models.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni nj"><img src="../Images/f23534ce80c10c51bc3988dae122296c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L3GUB89V4sIRqqbN"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 1. Different PEFT methods (image from <a class="af oa" href="https://arxiv.org/pdf/2302.08106" rel="noopener ugc nofollow" target="_blank">paper</a> ).</figcaption></figure><p id="cab4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As model sizes continue to grow, the need for efficient fine-tuning methods becomes more critical. Fine-tuning all parameters in a large-scale pre-trained model is often impractical due to the substantial computational resources and time required. Parameter-efficient fine-tuning (PEFT) addresses this challenge by freezing the model’s parameters and only training the injected modules with a small number of parameters. Hence, only one copy of the large Transformer is stored with learned task specific lightweight PEFT modules, yielding a very small overhead for each additional task. This approach not only reduces resource demands but also accelerates the adaptation of models to new tasks, making it a practical and effective strategy in the era of ever-expanding models. PEFT approaches are very commonly used in LLMs and giant vision models and can be mainly divided into three categories as shown in Fig. 1: Among several methods that have been proposed, three have gotten significant attention from the community.</p><p id="1356" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">1- adapters: An adapter is essentially a small module, typically consisting of a downsample layer, nonlinearity, and an upsample layer with a skip connection to preserve the original input. This module is inserted into a pretrained model, with only the adapters being trained during fine-tuning.</p><p id="af50" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">2- LoRA injects trainable low-rank decomposition matrices into the model to approximate weight updates, significantly reducing the number of trainable parameters for downstream tasks. For a pre-trained weight matrix W of dimensions d×k, LoRA represents its update with a low-rank decomposition: W+ΔW=W+DU<br/>where D​ has dimensions d×r and U has dimensions r×k. These matrices D and U are the tunable parameters. LoRA can be applied to the attention matrices and/or the feedforward module for efficient finetuning.</p><p id="078b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">3- P*-tuning (prefix-tuning, prompt tuning) typically prepend a set of learnable prefix vectors or tokens to the input embedding, and only these so-called “soft prompts” are trained when fine-tuning on downstream tasks. The philosophy behind this approach is to assist the pre-trained models in understanding downstream tasks with the guidance of a sequence of extra “virtual tokens” information. Soft prompts are sequences of vectors that do not correspond to actual tokens in the vocabulary. Instead, they serve as intermediary representations that guide the model’s behavior to accomplish specific tasks, despite having no direct linguistic connection to the task itself.</p><p id="9c3f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">Evaluating PEFT Techniques: Strengths and Limitations:</em></strong></p><p id="cb1a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Adapters</strong> add a small number of parameters (3–4% of the total parameters) which makes them more efficient than full fine-tuning but less than prompt tuning or LoRA. However, they are capable of capturing complex task-specific information effectively due to the additional neural network layers and often achieve high performance on specific tasks by learning detailed task-specific features. On the downside, this approach makes the model deeper, which can complicate the optimization process and lead to longer training times.</p><p id="db5a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">LoRa</strong> adds only a small fraction of parameters (0.1% to 3%), making it highly efficient and scalable with very large models, making it suitable for adapting state-of-the-art LLMs and VLMs. However, LoRA’s adaptation is constrained to what can be expressed within the low-rank structure. While efficient, LoRA might be less flexible compared to adapters in capturing certain types of task-specific information.</p><p id="336a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">P*- tuning</strong> is extremely parameter-efficient (often requiring less than 0.1%), as it only requires learning additional prompt tokens while keeping the original model parameters unchanged, thereby preserving the model’s generalization capabilities. However, it may not be able to capture complex task-specific information as effectively as other methods.</p><p id="6d25" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So far, we’ve reviewed new methods to fine-tune a large model with minimal compute power. This capability opens the door for us to combine two large models, each with billions of parameters, and fine-tune only a few million parameters to make them work together properly. This alignment allows one or both models to generate embeddings that are understandable by the other. Next, we’ll discuss three main approaches that demonstrate successful implementations of such a training regime.</p><p id="74a7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">2.1 Prompt adaptation:</em></strong></p><figure class="nk nl nm nn no np nh ni paragraph-image"><div class="nh ni ob"><img src="../Images/8747e0b6294956f4a19d280589019020.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/0*4K0hokDf5YM1gJyI"/></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 2. Early fusion of visual prompts and late fusion of adaptation prompts (image from <a class="af oa" href="https://arxiv.org/pdf/2304.15010" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><p id="0a27" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2303.16199" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter</strong></a> presents a lightweight adaptation method to efficiently fine-tune the <a class="af oa" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> model into an instruction-following model. This is achieved by freezing the pre-trained <a class="af oa" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> 7B model and introducing a set of learnable adaptation prompts (1.2M parameters) into the topmost transformer layers. To avoid the initial instability and effectiveness issues caused by randomly initialized prompts, the adaptation prompts are zero-initialized. Additionally, a learnable zero-initialized gating factor is introduced to adaptively control the importance of the adaptation prompts.<br/>Furthermore, <a class="af oa" href="https://arxiv.org/pdf/2303.16199" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter</strong></a> extends to multi-modal tasks by integrating visual information using a pre-trained visual encoder such as CLIP. Given an image as visual context, the global visual features are acquired through multi-scale feature aggregation and then projected into the dimension of the LLM’s adaptation prompt via a learnable projection network. The resulting overall image token is repeated K times, and element-wisely added to the K-length adaptation prompts at all L inserted transformer layers. Fine-tuning with <a class="af oa" href="https://arxiv.org/pdf/2303.16199" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter</strong></a> takes less than one hour on 8 A100 GPUs. A similar approach is used in <a class="af oa" href="https://arxiv.org/pdf/2401.10446" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">RobustGER</strong></a>, where LLMs are fine-tuned to perform denoising for generative error correction (GER) in automatic speech recognition. This process takes 1.5–4.5 hours of training on a single NVIDIA A40 GPU.</p><p id="e5f5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2304.15010" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter V2</strong></a> focuses on instruction-following vision models that can also generalize well on open-ended visual instructions. To achieve this goal, three key improvements are presented over the original <a class="af oa" href="https://arxiv.org/pdf/2303.16199" rel="noopener ugc nofollow" target="_blank">LLaMA-Adapter</a>. First, it introduces more learnable parameters (14M) by unfreezing all the normalization layers in <a class="af oa" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> and adding a learnable bias and scale factor to all linear layers in the transformer, which distributes the instruction-following capability across the entire model. Second, visual tokens are fed into the early layers of the language model, while the adaptation prompts are added to the top layers. This improves the integration of visual knowledge without disrupting the model’s instruction-following abilities (see Fig. 2). Third, a joint training paradigm for both image-text captioning data and language-only instruction data is employed. The visual projection layers are trained for image-text captioning data while the late adaptation prompts and the unfrozen norms are trained from the instruction-following data. Additionally, expert models like captioning and OCR systems are integrated during inference, enhancing image understanding without additional training costs. We weren’t able to find specific details on GPU requirements and the time needed for training. However, based on information from GitHub, it takes approximately 100 hours on a single A100 GPU.</p><p id="24c4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">2.2 Intermediate Module Training:</em></strong></p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni oc"><img src="../Images/45e1e925a345f38d407df0488f1ce2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XnEYeNUXXeu3g9kU"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 3. Overview of how intermediate module training is working (image by authors)</figcaption></figure><p id="a631" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To create a multi-modal model, two or more unimodal foundation models can be connected through a learnable projection module. This module maps features from one modality to another, enabling the integration of different data types. For instance, a vision encoder can be connected to a large language model (LLM) via a projection module. Hence, as illustrated in Fig. 3, the LLM’s input consists of a sequence of projected image features and text. The training process typically involves two stages:</p><ol class=""><li id="a928" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd od oe of bk"><strong class="mk fr">Pretraining</strong>: The projection module is pretrained on a large dataset of paired examples to achieve cross-modality alignment.</li><li id="ddda" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk"><strong class="mk fr">Fine-tuning</strong>: The projection module (with one or more unimodal models) is fine-tuned for specific downstream tasks, such as instruction-following tasks.</li></ol><p id="b0df" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a>, aligns a frozen visual encoder, <a class="af oa" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf" rel="noopener ugc nofollow" target="_blank">ViT-G/14</a>, with a frozen LLM, <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a>, using one projection layer. For visual encoder, the same pretrained visual perception component of <a class="af oa" href="https://arxiv.org/pdf/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> is utilized which consists of a <a class="af oa" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.pdf" rel="noopener ugc nofollow" target="_blank">ViT-G/14</a> and Q-former network. <a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a> adds a single learnable projection layer where its output is considered as a soft prompt for the LLM in the following format:<br/>“###Human: &lt;Img&gt;&lt;ImageFeatureFromProjectionLayer&gt;&lt;/Img&gt; TextTokens. ###Assistant:”. <br/>Training the projection layer involves two stages. First, pretrain the projection layer on a large dataset of aligned image-text pairs to acquire vision-language knowledge. Then, fine-tune the linear projection layer with a smaller, high-quality dataset. In both stages, all other parameters are frozen. As a result, <a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a> is capable of producing more natural and reliable language outputs. <a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a> requires training approximately 10 hours on 4 A100 GPUs.</p><p id="3c5d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Tuning the LLMs to follow instructions using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks. To explore this idea in the multimodality field, <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVA</strong></a> connects LLM <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a>, with a vision encoder, <a class="af oa" href="https://arxiv.org/pdf/2103.00020" rel="noopener ugc nofollow" target="_blank">ViT-L/14</a>, using a single linear layer for vision-language instruction following tasks. In the first stage, the projection layer is trained on a large image-text pairs dataset while the visual encoder and LLM weights are kept frozen. This stage creates a compatible visual tokenizer for the frozen LLM. In the second stage, the pre-trained projection layer and LLM weights are fine-tuned using a high-quality generated dataset of language-image instruction-following data. This stage enhances the model’s ability to follow multimodal instructions and perform specific tasks. <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVA</strong></a> uses 8× A100s GPUs. The pretraining takes 4 hours, and the fine-tuning takes 4–8 hours depending on the specific task dataset. It showcases commendable proficiency in visual reasoning capabilities, although it falls short on academic benchmarks requiring short-form answers.</p><p id="ad44" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To improve the performance of <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVA</strong></a>, in <a class="af oa" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVa-1.5</strong></a>:</p><ol class=""><li id="bc11" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd od oe of bk">A two-layer MLP is added to connect the LLM to the vision encoder.</li><li id="d4fa" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk">The input image resolution has been scaled up using CLIP-ViT-L-336px, allowing for better detail perception.</li><li id="60f6" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk">The <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a> model has been scaled up to 13B parameters.</li><li id="68f7" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk">Academic-task-oriented VQA data has been added, along with specific response formatting prompts to indicate the desired output format. When prompting for short-form answers, the prompt “Answer the question using a single word or phrase.” is appended to the VQA question.</li></ol><p id="6427" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The training finishes in ∼1 day on a single 8-A100 GPU and achieves state-of-the-art results on a wide range of benchmarks.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni ol"><img src="../Images/5d98bc0d070476023387b41945ac6d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Zp4VTw46bWkLGk0pNudRw.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 4. Architecture of Video-ChatGPT (image from <a class="af oa" href="https://arxiv.org/pdf/2306.05424" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><p id="2620" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2306.05424" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Video-ChatGPT</strong></a> focuses on creating a video-based conversational agent. Given the limited availability of video-caption pairs and the substantial resources required for training from scratch, it uses the pretrained image-based visual encoder, CLIP <a class="af oa" href="https://arxiv.org/pdf/2103.00020" rel="noopener ugc nofollow" target="_blank">ViT-L/14</a> for video tasks and connect it with pretrained LLM <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a> through a learnable linear projection model. <a class="af oa" href="https://arxiv.org/pdf/2103.00020" rel="noopener ugc nofollow" target="_blank">ViT-L/14</a> encodes images, so for a given video sample with T frames, it generates T frame-level embeddings with dimensions h*w*D. As illustrated in Fig. 4. The process of obtaining video-level features involves two key steps:</p><ul class=""><li id="7aaf" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd om oe of bk"><strong class="mk fr">Spatial Video Features</strong>: These are obtained by average-pooling frame-level features across the temporal dimension to achieve an <em class="ng">h*w*D</em> dimension.</li><li id="af5d" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd om oe of bk"><strong class="mk fr">Temporal Video Features</strong>: These are obtained by average-pooling across the spatial dimension to achieve <em class="ng">T*D</em> dimensions.</li></ul><p id="431e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">These temporal and spatial features are concatenated to form video-level features, which are then projected into the textual embedding space by a learnable linear layer. The model is trained on their curated, high-quality dataset of video-text pairs, and the training of the linear projection layer takes around 3 hours on 8 A100 40GB GPUs. This approach allows <a class="af oa" href="https://arxiv.org/pdf/2306.05424" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Video-ChatGPT</strong></a> to generate detailed and coherent conversations about video content.</p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni on"><img src="../Images/c9d5e22d243e4c0ea39f2e47e0d76609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvAwoONNvUxCUTayuhE6bg.png"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 5. llustration of PandaGPT (image from <a class="af oa" href="https://arxiv.org/pdf/2305.16355" rel="noopener ugc nofollow" target="_blank">paper</a>)</figcaption></figure><p id="c470" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2305.16355" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">PandaGPT</strong></a>, while not connecting unimodal models, introduces the first general-purpose model capable of instruction-following by integrating the pretrained LLM <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a> with the multimodal encoder <a class="af oa" href="https://arxiv.org/pdf/2305.05665" rel="noopener ugc nofollow" target="_blank">ImageBind</a> through a linear projection layer (see Fig. 5). The linear projection layer is trained, and <a class="af oa" href="https://lmsys.org/blog/2023-03-30-vicuna/" rel="noopener ugc nofollow" target="_blank">Vicuna</a>’s attention modules are fine-tuned using LoRA on 8×A100 40G GPUs for 7 hours, leveraging only image-language (multi-turn conversation) instruction-following data. Despite being trained exclusively on image-text pairs, <a class="af oa" href="https://arxiv.org/pdf/2305.16355" rel="noopener ugc nofollow" target="_blank">PandaGPT</a> exhibits emergent, zero-shot, cross-modal capabilities across multiple modalities by leveraging the binding property across six modalities (image/video, text, audio, depth, thermal, and IMU) inherited from the frozen <a class="af oa" href="https://arxiv.org/pdf/2305.05665" rel="noopener ugc nofollow" target="_blank">ImageBind</a> encoders. This enables PandaGPT to excel in tasks such as image/video-grounded question answering, image/video-inspired creative writing, visual and auditory reasoning, and more.</p><p id="df24" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">2.3 Adapter Mixture:</em></strong></p><figure class="nk nl nm nn no np nh ni paragraph-image"><div role="button" tabindex="0" class="nq nr ed ns bh nt"><div class="nh ni oo"><img src="../Images/9b16151683a668fa33079bfa3b6cdb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*32DA93caZhNdpnfA"/></div></div><figcaption class="nv nw nx nh ni ny nz bf b bg z dx">Fig. 6. The overview of the Mixture-of-Modality Adaptation (MMA) (image by authors)</figcaption></figure><p id="2235" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Cheap&amp;Quick</strong></a> adopts lightweight adapters to integrate large language models (LLMs) and vision models for vision-language tasks. The paper proposes a Mixture-of-Modality Adapters (MMA), designed to facilitate switching between single- and multi-modal instructions without compromising performance. A learnable token <em class="ng">t</em> is proposed as the modality selector token. This token indicates the input features’ modality (i.e., unimodal or multimodal input) and informs the router module on how to combine the output of the learned adapters, as illustrated in Fig 6. The adapter is formulated as : <br/>Z′=Z+s⋅router(f(Z),g(Z); t)<br/>where Z is the input features, either unimodal or concatenated multimodal (image-text) features. Modules <em class="ng">f</em> and <em class="ng">g</em> share a common <a class="af oa" href="https://arxiv.org/pdf/2302.08106" rel="noopener ugc nofollow" target="_blank">unimodal adapter</a> architecture. s is a scaling factor, and the router(⋅) function determines the routing path based on the modality token <em class="ng">t</em>.<br/>To demonstrate the effectiveness of MMA, the authors connected <a class="af oa" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> and <a class="af oa" href="https://arxiv.org/pdf/2103.00020" rel="noopener ugc nofollow" target="_blank">CLIP-ViT</a> with a single linear layer and inserted MMA into both ViT and <a class="af oa" href="https://arxiv.org/pdf/2302.13971" rel="noopener ugc nofollow" target="_blank">LLaMA</a> before the multi-head attention modules. The adapters and projection layer (only 3.8M parameters) were trained with a mixture of text-only and text-image data on 8 A100 GPUs for 1.4 hours. This approach showed a significant reduction in training costs while maintaining high performance on vision-language tasks.</p><p id="97af" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">2.4 A Modality as Grounding Without Training</em><br/></strong>Up to this point, we have discussed papers that connect unimodal models to create a multimodal model. However, advancing toward AGI requires a multimodal model capable of handling data from different modalities for diverse tasks, ranging from calculus to generating images based on descriptions.</p><p id="f49c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Recently, many papers have explored integrating pre-trained multi-modal models via language prompting into a unified model capable of handling various tasks across different modalities without additional training. In this approach, language serves as an intermediary for models to exchange information. Through prompt engineering (e.g., <a class="af oa" href="https://arxiv.org/pdf/2303.04671" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Visual ChatGPT</strong></a>,<strong class="mk fr"> </strong><a class="af oa" href="https://arxiv.org/pdf/2303.11381" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MM-REACT</strong></a>) or fine-tuning (e.g., <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Toolformer</strong></a>,<strong class="mk fr"> </strong><a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">GPT4Tools</strong></a>), LLMs can invoke specialized foundation models to handle various modality-specific tasks. While this topic is beyond the scope of our current blog post, you can refer to these papers for more detailed information.</p><p id="ce00" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In another similar work, <a class="af oa" href="https://arxiv.org/pdf/2205.02655" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MAGIC</strong></a> proposes a novel, training-free, plug-and-play framework and uses image embedding, through pre-trained CLIP, as the grounding foundation. This framework connects GPT-2 with CLIP to perform image-grounded text generation (e.g., image captioning) in a zero-shot manner. By incorporating the similarity between the image embeddings from CLIP and the top-k generated tokens from a pre-trained LLM at each time step into the decoding inference, the model effectively leverages visual information to guide text generation. Without any additional training, this approach demonstrates the capability to generate visually grounded stories given both an image and a text prompt.</p><p id="1852" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="ng">3. High quality Curated data:</em></strong></p><p id="b3a7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We have discussed various methods of aligning different modalities up to this point; however, it is important to remember that having curated, high-quality data in such training regimes is equally crucial. For instance, detailed and accurate instructions and responses significantly enhance the zero-shot performance of large language models on interactive natural language tasks. In the field of interactive vision-language tasks, the availability of high-quality data is often limited, prompting researchers to develop innovative methods for generating such data.</p><p id="2fa3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a> proposes a two-stage method to curate a detailed, instruction-following image description dataset:<br/>1-<strong class="mk fr">Data Generation</strong>: The pre-trained model from the first stage of training is used to generate detailed descriptions. For a given image, a carefully crafted prompt is used to enable the pre-trained model to produce detailed and informative image descriptions in multiple steps,</p><p id="f13a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">2- <strong class="mk fr">Post-Processing and Filtering</strong>: The generated image descriptions contain noisy or incoherent descriptions. In order to fix these issues, ChatGPT is employed to refine the generated descriptions according to specific post-processing requirements and standards, guided by a designed prompt. The refined dataset is then manually verified to ensure the correctness and quality of the image-text pairs.</p><p id="e3d4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVA</strong></a> proposes a method to generate multimodal instruction-following data by querying ChatGPT/GPT-4 based on widely available image-text pair data. They designed a prompt that consists of an image caption, bounding boxes to localize objects in the scene, and a few examples for in-context learning. This method leverages the existing rich dataset and the capabilities of ChatGPT/GPT-4 to produce highly detailed and accurate multimodal data.</p><p id="d22a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2306.05424" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Video-ChatGPT</strong></a> utilized two approaches for generating high-quality video instruction data.<br/>1- <strong class="mk fr">Human-Assisted Annotation</strong>: Expert annotators enrich given video-caption pairs by adding comprehensive details to the captions<br/>2- <strong class="mk fr">Semi-Automatic Annotation</strong>: This involves a multi-step process leveraging several pretrained models:</p><ol class=""><li id="b1af" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd od oe of bk">Pretrained <a class="af oa" href="https://arxiv.org/pdf/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> and <a class="af oa" href="https://arxiv.org/pdf/2212.00280" rel="noopener ugc nofollow" target="_blank">GRiT</a> models are used for analyzing key frames in the videos. <a class="af oa" href="https://arxiv.org/pdf/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> generates frame-level captions, while <a class="af oa" href="https://arxiv.org/pdf/2212.00280" rel="noopener ugc nofollow" target="_blank">GRiT</a> provides detailed descriptions of scene objects. Additionally, the pretrained <a class="af oa" href="https://arxiv.org/pdf/2303.05657" rel="noopener ugc nofollow" target="_blank">Tag2Text</a> model generates tags for each key frame of the video.</li><li id="0702" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk">A specialized filtering mechanism is employed to remove any captions from <a class="af oa" href="https://arxiv.org/pdf/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> or <a class="af oa" href="https://arxiv.org/pdf/2212.00280" rel="noopener ugc nofollow" target="_blank">GRiT</a> that do not match the <a class="af oa" href="https://arxiv.org/pdf/2303.05657" rel="noopener ugc nofollow" target="_blank">Tag2Text</a> frame-level tags.</li><li id="e2d5" class="mi mj fq mk b go og mm mn gr oh mp mq mr oi mt mu mv oj mx my mz ok nb nc nd od oe of bk">The GPT-3.5 model is used to merge the filtered captions and generate a singular, coherent video-level caption.</li></ol><p id="c7f7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af oa" href="https://arxiv.org/pdf/2306.05425" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MIMIC-IT</strong></a> generated a dataset of 2.8 million multimodal instruction-response pairs, aimed at enhancing Vision-Language Models (VLMs) in perception, reasoning, and planning. To demonstrate the importance of high-quality data, they fine-tuned <a class="af oa" href="https://arxiv.org/pdf/2308.01390" rel="noopener ugc nofollow" target="_blank">OpenFlamingo</a> using the MIMIC-IT dataset on 8 A100 GPUs over 3 epochs in one day. The resulting model outperforms <a class="af oa" href="https://arxiv.org/pdf/2308.01390" rel="noopener ugc nofollow" target="_blank">OpenFlamingo</a>, demonstrating superior in-context and zero-shot learning capabilities.</p><blockquote class="op"><p id="50e8" class="oq or fq bf os ot ou ov ow ox oy nd dx">The opinions expressed in this blog post are solely our own and do not reflect those of our employer.</p></blockquote><p id="cc0c" class="pw-post-body-paragraph mi mj fq mk b go oz mm mn gr pa mp mq mr pb mt mu mv pc mx my mz pd nb nc nd fj bk"><strong class="mk fr"><em class="ng">References:</em></strong></p><p id="df04" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[1] <a class="af oa" href="https://arxiv.org/pdf/2303.16199" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter</strong></a>: Zhang, Renrui, et al. “Llama-adapter: Efficient fine-tuning of language models with zero-init attention.” (2023).<br/>[2] <a class="af oa" href="https://arxiv.org/pdf/2304.15010" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaMA-Adapter V2</strong></a>: Gao, Peng, et al. “Llama-adapter v2: Parameter-efficient visual instruction model.” (2023).<br/>[3] <a class="af oa" href="https://arxiv.org/pdf/2304.10592" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MiniGPT-4</strong></a>: Zhu, Deyao, et al. “Minigpt-4: Enhancing vision-language understanding with advanced large language models.” (2023).<br/>[4] <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/6dcf277ea32ce3288914faf369fe6de0-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVA</strong></a>: Liu, Haotian, et al. “Visual instruction tuning.” (2024).<br/>[5] <a class="af oa" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Improved_Baselines_with_Visual_Instruction_Tuning_CVPR_2024_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">LLaVa-1.5</strong></a>: Liu, Haotian, et al. “Improved baselines with visual instruction tuning.” (2024).<br/>[6] <a class="af oa" href="https://arxiv.org/pdf/2306.05424" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Video-ChatGPT</strong></a>: Maaz, Muhammad, et al. “Video-chatgpt: Towards detailed video understanding via large vision and language models.” (2023).<br/>[7] <a class="af oa" href="https://arxiv.org/pdf/2305.16355" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">PandaGPT</strong></a>: Su, Yixuan, et al. “Pandagpt: One model to instruction-follow them all.” (2023).<br/>[8] <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/5e84e4413268b713f0d4a1b23a9dae57-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Cheap&amp;Quick</strong></a>: Luo, Gen, et al. “Cheap and quick: Efficient vision-language instruction tuning for large language models.” (2024).<br/>[9] <a class="af oa" href="https://arxiv.org/pdf/2401.10446" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">RobustGER</strong></a>: Hu, Yuchen, et al. “Large Language Models are Efficient Learners of Noise-Robust Speech Recognition.” (2024).<br/>[10] <a class="af oa" href="https://arxiv.org/pdf/2205.02655" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MAGIC</strong></a>: Su, Yixuan, et al. “Language models can see: Plugging visual controls in text generation.” (2022).<br/>[11] <a class="af oa" href="https://arxiv.org/pdf/2303.04671" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Visual ChatGPT</strong></a>: Wu, Chenfei, et al. “Visual chatgpt: Talking, drawing and editing with visual foundation models.” (2023).<br/>[12] <a class="af oa" href="https://arxiv.org/pdf/2303.11381" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MM-REACT</strong></a>: Yang, Zhengyuan, et al. “Mm-react: Prompting chatgpt for multimodal reasoning and action.” (2023).<br/>[13] <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f658a906-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">Toolformer</strong></a>: Schick, Timo, et al. “Toolformer: Language models can teach themselves to use tools.” (2024).<br/>[14] <a class="af oa" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e393677793767624f2821cec8bdd02f1-Paper-Conference.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">GPT4Tools</strong></a>: Yang, Rui, et al. “Gpt4tools: Teaching large language model to use tools via self-instruction.” (2024).<br/>[15] <a class="af oa" href="https://arxiv.org/pdf/2306.05425" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">MIMIC-IT</strong></a>: Li, Bo, et al. “Mimic-it: Multi-modal in-context instruction tuning.” (2023).<br/>[16] He, Junxian, et al. “Towards a unified view of parameter-efficient transfer learning.” (2021).</p></div></div></div></div>    
</body>
</html>