# 超越RAG：在语义世界中的精确过滤

> 原文：[https://towardsdatascience.com/beyond-rag-precision-filtering-in-a-semantic-world-333d332c2d45?source=collection_archive---------3-----------------------#2024-11-12](https://towardsdatascience.com/beyond-rag-precision-filtering-in-a-semantic-world-333d332c2d45?source=collection_archive---------3-----------------------#2024-11-12)

![](../Images/91efb2bc336e0e2f88a30a0012054ae2.png)

图片由[Nathan Dumlao](https://unsplash.com/@nate_dumlao?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 通过使用传统的机器学习方法来缩小大语言模型（LLM）响应中的差距，从而对齐期望与现实。

[](https://medium.com/@dkulikm?source=post_page---byline--333d332c2d45--------------------------------)[![Daniel Kulik](../Images/f48c2034f1381c195d7798e99f30f8d7.png)](https://medium.com/@dkulikm?source=post_page---byline--333d332c2d45--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--333d332c2d45--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--333d332c2d45--------------------------------) [Daniel Kulik](https://medium.com/@dkulikm?source=post_page---byline--333d332c2d45--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--333d332c2d45--------------------------------) ·阅读时长7分钟·2024年11月12日

--

很早我们就意识到，大语言模型（LLMs）仅知道其训练数据中的内容。与它们玩耍很有趣，没错，但它们一直以来都容易产生幻觉。将这种“原始”形式的产品用于商业化，可以说——简直是愚蠢透顶（指的是LLM，不是你…可能）。为了解决幻觉问题以及对未见过/私有数据的知识缺乏，有两条主要途径可供选择。训练一个基于你私人数据的定制LLM（也就是费力的办法），或者使用检索增强生成（也就是我们基本上都选择的办法）。

RAG是现在在自然语言处理（NLP）和生成型AI领域广泛使用的一个缩写。它已经发展并引领了许多不同的新形式和方法，例如GraphRAG，偏离了大多数人最初使用的简单方法。两年前的我可能会将原始文档解析成一个简单的RAG，然后在检索时，将这些可能的（很可能是）无用的上下文提供给LLM，希望它能够理解并利用这些信息更好地回答用户的问题。 ***哇，真是“无知是福”；而且，别评判：我们都做过这种事。*** 我们很快意识到，“*垃圾进，垃圾出*”，因为我们最初的概念验证……嗯……表现得并不好。于是，开源社区投入了大量的精力，给我们提供了更多合理的方式来实现商业上可行的应用。这些方法包括例如：重新排序、语义路由、防护措施、更好的文档解析、将用户的问题重新对齐以检索更相关的文档、上下文压缩，等等。除此之外，我们所有人还提高了传统NLP技能，并为团队编写了指导方针，确保存储在数据库中的解析文档既整洁又易于阅读。

在处理一个大约有16个步骤的检索系统时，始终有一个问题不断出现。**我存储的上下文真的能回答这个问题吗？** 或者换句话说，我更喜欢的表述是：**这个问题真的属于存储的上下文吗？** 虽然这两个问题看起来相似，但它们的区别在于，第一个问题是局部化的（例如，10个检索到的文档），而第二个问题是全局化的（针对整个文档数据库的主题/话题空间）。你可以把它们看作一个是精细化筛选，而另一个是更广泛的筛选。我相信你现在可能在想，这一切的意义何在？“*我对我检索到的文档做余弦相似度阈值筛选，一切正常，为什么你要把事情搞得这么复杂？*” 好吧，我编造了最后那句话，我知道你不会那么刻薄。

为了凸显我的过度复杂化，这里有一个例子。假设用户问：“*谁是第一个登上月球的人？*”现在，假设我们忘记LLM可以直接回答这个问题，我们期望RAG为这个问题提供上下文……但是，我们的所有文档都是关于时尚品牌产品的！这个例子有点傻，没错，但在生产环境中，我们很多人都看过用户总是提出一些与我们拥有的文档不相关的问题。“*是的，但我的前提告诉LLM忽略不属于某个话题类别的问题。余弦相似度会过滤掉这些问题的弱上下文*”或者“*我使用了护栏或语义路由来处理这个问题*。”当然，同意，这些方法有效，但所有这些选项要么是在下游做得太晚（比如前两个例子），要么没有完全针对这个问题量身定制（比如最后两个例子）。我们真正需要的是一种快速的分类方法，能够在检索文档之前快速告诉你问题是否适合由文档提供上下文……即使在检索之前。如果你猜到这是什么意思，那么你就是经典机器学习团队的一员了 ;) 是的，没错，就是经典的离群值检测！

> 离群值检测与自然语言处理（NLP）结合？显然有人有太多的空闲时间来玩这个。

在构建生产级RAG系统时，有几件事情我们需要确保：效率（响应通常需要多长时间）、准确性（响应是否正确且相关）和可重复性（有时被忽视，但非常重要，检查一下缓存库）。那么，离群值检测方法（OD）如何帮助这些呢？让我们快速集思广益。如果OD看到一个问题并立即说“否，这是一个离群值”（我在这里拟人化），那么很多后续步骤可以被跳过，从而使得这个过程更加高效。假设OD现在说“是，安全”，好吧，稍微增加一些开销，我们就能更有把握地知道问题和存储文档的主题空间是否一致。至于可重复性，幸运的是，我们又是幸运的，因为经典的机器学习方法通常是可重复的，所以至少这个额外的步骤不会突然开始道歉并带我们进入重复和误解的恶性循环（我在看你，ChatGPT）。

> 哇，这部分有点冗长，抱歉，不过终于我现在可以开始展示一些有趣的内容了。

[Muzlin](https://pypi.org/project/muzlin/)，一个Python库，是我积极参与的项目，专门为这些类型的语义过滤任务开发，通过使用简单的机器学习来适应生产环境。怀疑吗？好吧，来吧，让我们快速看看它能为我们做些什么。

我们将使用的数据集是来自BEIR（[Scifact](https://huggingface.co/datasets/BeIR/scifact)，CC BY-SA 4.0）的5.18K行数据集。为了创建一个向量库，我们将使用科学的*声明*列。

所以，数据已经加载了（虽然是一个小数据集，但嘿，这只是一个演示！），接下来的步骤是编码它。有很多方法可以做到这一点，例如分词、向量嵌入、图节点-实体关系等，但对于这个简单的示例，我们就使用向量嵌入。Muzlin 内置了对所有流行品牌（苹果、微软、谷歌、OpenAI）的支持，嗯，我是说它们的关联嵌入模型，但你懂的。我们就选择，嗯，HuggingFace，因为你知道，它是免费的，而我当前的 POC 预算是…就像省吃俭用一样。

太棒了！如果你能相信的话，我们已经走了一半了。难道只有我觉得，很多这些 LLM 库让你写了额外的 1000 行代码，依赖了成千上万的库，结果在老板要求演示时就崩溃了？不只是我吧？对吧？不管怎样，废话少说，我们真的只剩下两个步骤，就能让我们的过滤器启动并运行。第一个步骤是使用一个异常检测方法来评估嵌入向量。这可以构建一个无监督模型，给出当前或新嵌入中任何给定向量的可能性值。

不开玩笑，就这样了。你的模型已经完成了。Muzlin 完全兼容 Sklearn，并经过 Pydantic 校验。而且，MLFlow 也完全集成了数据记录功能。上面的示例并没有使用它，因此该结果会自动在你的本地目录中生成一个 joblib 模型。很酷吧？目前只有 PyOD 模型支持这种类型的 OD，但谁知道未来会怎样呢。

> 该死的丹尼尔，你怎么让这一切变得这么简单。敢打赌你一直在引导我，然后从这里开始一切都将一落千丈。

对于上面的回应，s..u..r..e 这个梗现在确实有点过时了。但除此之外，别开玩笑，最后一步就要来了，和之前的所有步骤一样，轻松得很。

好的，好吧，这是最长的脚本，但看……大部分内容只是为了玩玩而已。但让我们分析一下这里发生了什么。首先，OutlierDetector 类现在期待一个模型。我发誓这不是 bug，这是一项功能！在生产环境中，你不希望每次都在现场训练模型来进行推理，而且通常训练和推理是在不同的计算实例上进行，特别是在云计算上。所以，OutlierDetector 类为此提供了支持，让你加载一个已经训练好的模型，以便可以随时进行推理。YOLO。现在你要做的就是编码一个用户的问题，然后用 OD 模型进行预测，*嘿，瞧瞧，这里，我们找到了一个小小的异常值。*

那么，当用户的问题是一个异常值时，这意味着什么呢？很酷的事情，这一切都由你来决定。存储的文档很可能没有任何上下文能够以有意义的方式回答这个问题。你可以选择将其重新引导，要么告诉测试团队的 Kyle 停止胡闹，要么更严肃一点，节省代币并设置一个默认响应，例如“抱歉，Dave，我恐怕做不到” （哦，HAL 9000，你真幽默，也请不要太空我）。

总结一下，集成更好（哈哈，给数学爱好者的数学笑话）。但实际上，经典的机器学习已经存在了很久，并且在生产环境中更具可靠性。我相信未来更多的工具应该融入这种理念，尤其是在我们所有人共同参与的生成性 AI 过山车旅程中，（顺便说一下，这趟旅程花费的代币实在太多）。通过使用异常检测，偏离主题的查询可以快速被重新引导，从而节省计算和生成成本。作为额外的福利，我甚至提供了一个选项，可以用 GraphRAGs 来实现这一点，太棒了——极客们团结起来！前进吧，享受那些开源开发者为了让我们免费使用而付出了太多失眠的工具。祝你旅途愉快，记得享受乐趣！
