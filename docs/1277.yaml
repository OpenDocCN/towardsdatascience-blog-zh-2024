- en: 'Profiling CUDA Using Nsight Systems: A Numba Example'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Nsight Systems进行CUDA性能分析：一个Numba示例
- en: 原文：[https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22](https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22](https://towardsdatascience.com/profiling-cuda-using-nsight-systems-a-numba-example-fc65003f8c52?source=collection_archive---------4-----------------------#2024-05-22)
- en: Learn about profiling by inspecting concurrent and parallel Numba CUDA code
    in Nsight Systems
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过检查并发和并行的Numba CUDA代码，在Nsight Systems中了解性能分析
- en: '[](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)[![Carlos
    Costa, Ph.D.](../Images/fc5e03e455f11b963086355fe0ccc077.png)](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)
    [Carlos Costa, Ph.D.](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)[![Carlos
    Costa, Ph.D.](../Images/fc5e03e455f11b963086355fe0ccc077.png)](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)
    [Carlos Costa, Ph.D.](https://medium.com/@cdacostaf?source=post_page---byline--fc65003f8c52--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)
    ·14 min read·May 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fc65003f8c52--------------------------------)
    ·14分钟阅读·2024年5月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'Optimization is a crucial part of writing high performance code, no matter
    if you are writing a web server or computational fluid dynamics simulation software.
    Profiling allows you to make informed decisions regarding your code. In a sense,
    optimization without profiling is like flying blind: mostly fine for seasoned
    professionals with expert knowledge and fine-tuned intuition, but a recipe for
    disaster for almost everyone else.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 优化是编写高性能代码的重要部分，无论你是在编写一个网络服务器，还是在开发计算流体力学模拟软件。性能分析使你能够对代码做出明智的决策。从某种意义上说，没有性能分析的优化就像盲目飞行：对于经验丰富的专业人员来说，这种方式大多可以应付，因为他们拥有专业知识和精确的直觉，但对于大多数人来说，这是灾难的配方。
- en: '![](../Images/d423edacc4ce187efb9a0e90523049ca.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d423edacc4ce187efb9a0e90523049ca.png)'
- en: Photo by [Rafa Sanfilippo](https://unsplash.com/@rafasanfilippo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Rafa Sanfilippo](https://unsplash.com/@rafasanfilippo?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In This Tutorial
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本教程内容
- en: Following my initial series CUDA by Numba Examples (see parts [1](https://colab.research.google.com/drive/1h0Savk8HSIgraT61burXQwbEUDMz4HT6?usp=sharing),
    [2](https://colab.research.google.com/drive/1GkGLDexnYUnl2ilmwNxAlWAH6Eo5ZK2f?usp=sharing),
    [3](https://colab.research.google.com/drive/1iRUQUiHUVdl3jlKzKucxQHQdDPElPb3M?usp=sharing),
    and [4](https://colab.research.google.com/drive/1Eq1Xyuq8hJ440ma_9OBrEVdGm3bIyigt?usp=sharing)),
    we will study a comparison between unoptimized, single-stream code and a slightly
    better version which uses stream concurrency and other optimizations. We will
    learn, from the ground-up, how to use [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems)
    to profile and analyze CUDA code. All the code in this tutorial can also be found
    in the repo [cako/profiling-cuda-nsight-systems](https://github.com/cako/profiling-cuda-nsight-systems/).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的系列《CUDA by Numba 示例》（参见第[1部分](https://colab.research.google.com/drive/1h0Savk8HSIgraT61burXQwbEUDMz4HT6?usp=sharing)，[2部分](https://colab.research.google.com/drive/1GkGLDexnYUnl2ilmwNxAlWAH6Eo5ZK2f?usp=sharing)，[3部分](https://colab.research.google.com/drive/1iRUQUiHUVdl3jlKzKucxQHQdDPElPb3M?usp=sharing)，和[4部分](https://colab.research.google.com/drive/1Eq1Xyuq8hJ440ma_9OBrEVdGm3bIyigt?usp=sharing)）之后，我们将学习未优化的单流代码与使用流并发和其他优化的略好版本之间的对比。我们将从零开始学习如何使用[NVIDIA
    Nsight Systems](https://developer.nvidia.com/nsight-systems)对CUDA代码进行分析和性能分析。本教程中的所有代码也可以在仓库[cako/profiling-cuda-nsight-systems](https://github.com/cako/profiling-cuda-nsight-systems/)中找到。
- en: Nsight Systems
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Nsight Systems
- en: NVIDIA recommends as best practice to follow the [APOD framework](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess-parallelize-optimize-deploy)
    (*Assess, Parallelize, Optimize, Deploy*). There are a variety of proprietary,
    open-source, free, and commercial software for different types of assessments
    and profiling. Veteran Python users may be familiar with basic profilers such
    as `cProfile`, `[line_profiler](https://kernprof.readthedocs.io/en/latest/)`,
    `memory_profiler` (unfortunately, unmaintaned as of 2024) and more advanced tools
    like [PyInstrument](https://pyinstrument.readthedocs.io/en/latest/) and [Memray](https://bloomberg.github.io/memray/).
    These profilers target specific aspects of the "host" such as CPU and RAM usage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 推荐遵循 [APOD 框架](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess-parallelize-optimize-deploy)（*评估、并行化、优化、部署*）作为最佳实践。市面上有多种专有、开源、免费的以及商业化的软件，用于不同类型的评估和分析。资深的
    Python 用户可能熟悉一些基本的分析器，比如 `cProfile`、`[line_profiler](https://kernprof.readthedocs.io/en/latest/)`、`memory_profiler`（遗憾的是，从
    2024 年开始不再维护）以及更先进的工具，如 [PyInstrument](https://pyinstrument.readthedocs.io/en/latest/)
    和 [Memray](https://bloomberg.github.io/memray/)。这些分析器主要针对 "主机"（如 CPU 和内存）的不同方面。
- en: However, profiling “device” (e.g., GPU) code — and its interactions with the
    host — requires specialized tools provided by the device vendor. For NVIDIA GPUs,
    Nsight Systems, Nsight Compute, Nsight Graphics are available for profiling different
    aspects of computation. In this tutorial we will focus on using Nsight Systems,
    which is a system-wide profiler. We will use it to profile Python code which interacts
    with the GPU via Numba CUDA.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分析 "设备"（例如 GPU）代码及其与主机的交互，需要设备供应商提供的专业工具。对于 NVIDIA 的 GPU，可以使用 Nsight Systems、Nsight
    Compute 和 Nsight Graphics 来分析计算的不同方面。在本教程中，我们将重点使用 Nsight Systems，它是一个系统级的分析工具。我们将使用它来分析通过
    Numba CUDA 与 GPU 交互的 Python 代码。
- en: To get started, you will need Nsight Systems CLI and GUI. The CLI can be installed
    separately and will be used to profile the code in a GPGPU-capable system. The
    full version includes both CLI and GUI. Note that both versions could be installed
    in a system without a GPU. Grab the version(s) you need from the [NVIDIA website](https://developer.nvidia.com/nsight-systems/get-started).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，你需要安装 Nsight Systems 的 CLI 和 GUI。CLI 可以单独安装，并用于在支持 GPGPU 的系统上分析代码。完整版包括
    CLI 和 GUI。请注意，即使系统没有 GPU，也可以安装这两个版本。你可以从 [NVIDIA 网站](https://developer.nvidia.com/nsight-systems/get-started)下载所需的版本。
- en: To make it easier to visualize code sections in the GUI, NVIDIA also provides
    the Python `pip` and `conda`-installable library `[nvtx](https://nvtx.readthedocs.io/en/latest/)`
    which we will use to annotate sections of our code. More on this later.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更方便地在图形界面中可视化代码部分，NVIDIA 还提供了可以通过 Python 的 `pip` 和 `conda` 安装的库 `[nvtx](https://nvtx.readthedocs.io/en/latest/)`，我们将使用它来标注代码的各个部分。稍后会详细介绍。
- en: 'Setting Everything Up: A Simple Example'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置一切：一个简单的例子
- en: 'In this section we will set our development and profiling environment up. Below
    are two very simple Python scripts: `kernels.py` and `run_v1.py`. The former will
    contain all CUDA kernels, and the latter will serve as the entry point to run
    the example. In this example we are following the "reduce" pattern introduced
    in article [*CUDA by Numba Examples Part 3: Streams and Events*](/cuda-by-numba-examples-7652412af1ee)
    to compute the sum of an array.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将设置开发和分析环境。以下是两个非常简单的 Python 脚本：`kernels.py` 和 `run_v1.py`。前者将包含所有 CUDA
    内核，后者将作为运行示例的入口点。在这个例子中，我们遵循文章 [*CUDA by Numba Examples Part 3: Streams and Events*](/cuda-by-numba-examples-7652412af1ee)
    中介绍的 "reduce" 模式来计算一个数组的总和。'
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is a simple script that can just be run with:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的脚本，可以通过以下命令运行：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also run this code through our profiler, which just entails calling `nsys`
    with some options before the call to our script:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会通过分析器运行此代码，这只需要在调用脚本之前使用一些选项调用 `nsys`：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can consult the [Nsight CLI docs](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
    for all the available options to the `nsys` CLI. For this tutorial we will always
    use the ones above. Let''s dissect this command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考 [Nsight CLI 文档](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)，了解
    `nsys` CLI 所有可用的选项。在本教程中，我们将始终使用上述选项。现在我们来剖析一下这个命令：
- en: '`profile` puts `nsys` in profile mode. There are many other modes like `export`
    and `launch`.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`profile` 将 `nsys` 设置为分析模式。还有其他多种模式，如 `export` 和 `launch`。'
- en: '`--trace cuda,osrt,nvtx` ensures we "listen" to all CUDA calls (`cuda`), OS
    runtime library calls (`osrt`) and `nvtx` annotations (none in this example).
    There are many more trace options such as `cublas`, `cudnn`, `mpi`,`dx11` and
    several others. Check the [docs](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)
    for all options.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--trace cuda,osrt,nvtx` 确保我们“监听”所有CUDA调用（`cuda`）、操作系统运行时库调用（`osrt`）和 `nvtx`
    注释（此示例中没有）。还有许多其他追踪选项，如 `cublas`、`cudnn`、`mpi`、`dx11` 等。有关所有选项，请查阅[文档](https://docs.nvidia.com/nsight-systems/UserGuide/index.html)。'
- en: '`--gpu-metrics-device=all` records GPU metrics for all GPUs, including [Tensor
    Core](https://www.nvidia.com/en-us/data-center/tensor-cores/) usage.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--gpu-metrics-device=all` 记录所有GPU的GPU指标，包括[Tensor Core](https://www.nvidia.com/en-us/data-center/tensor-cores/)的使用情况。'
- en: '`--cuda-memory-usage` tracks GPU memory usage of kernels. It may significantly
    slow down execution and requires `--trace=cuda`. We use it because our scripts
    our pretty fast anyways.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--cuda-memory-usage` 跟踪内核的GPU内存使用情况。它可能显著减慢执行速度，并且需要 `--trace=cuda`。我们使用它是因为我们的脚本反正运行得很快。'
- en: Navigating the Nsight Systems GUI
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 浏览Nsight Systems图形界面
- en: 'If the command exited successfully, we will have a `profile_run_v1.nsys-rep`
    in the current folder. We will open this file by launching the Nsight Systems
    GUI, `File > Open`. The initial view is slightly confusing. So we will start by
    decluttering: resize the `Events View` port to the bottom, and minimize `CPU`,
    `GPU` and `Processes` under the `Timeline View` port. Now expand only `Processes
    > python > CUDA HW`. See Figures 1a and 1b.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功退出，我们将在当前文件夹中看到一个 `profile_run_v1.nsys-rep` 文件。我们将通过启动Nsight Systems图形界面，点击
    `File > Open` 打开此文件。初始视图可能有些混乱。因此，我们将首先进行清理：将 `Events View` 窗口调整到底部，最小化 `CPU`、`GPU`
    和 `Processes` 下的 `Timeline View` 窗口。现在只展开 `Processes > python > CUDA HW`。请参见图1a和图1b。
- en: '![](../Images/66267f5fd85f7b4f42ce705216edc596.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66267f5fd85f7b4f42ce705216edc596.png)'
- en: 'Figure 1a: Opening an nsys report and decluttering the interface. Credits:
    Own work. CC BY-SA 4.0.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1a：打开nsys报告并清理界面。图片来源：原创作品。CC BY-SA 4.0。
- en: '![](../Images/a0df7779681087a99fe5f56d8a325ed4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0df7779681087a99fe5f56d8a325ed4.png)'
- en: 'Figure 1b: nsys report showing host-to-device memory operations (green), device-to-host
    memory operations (red) and CUDA kernels (blue). Credits: Own work. CC BY-SA 4.0.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1b：nsys报告显示主机到设备内存操作（绿色）、设备到主机内存操作（红色）和CUDA内核（蓝色）。图片来源：原创作品。CC BY-SA 4.0。
- en: First up, let’s find our kernels. On the `CUDA HW` line, you will find green
    and red blobs, and very small slivers of light blue (see Figure 1b). If you hover
    over those you will see tooltips saying, "CUDA Memory operations in progress"
    for red and green, and "CUDA Kernel Running (89.7%)" for the light blues. These
    are going to be the bread and butter of our profiling. On this line, we will be
    able to tell when and how memory is being transferred (red and green) and when
    and how our kernels are running (light blue).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们找到我们的内核。在 `CUDA HW` 行中，你会看到绿色和红色的块，以及非常小的浅蓝色条形（见图1b）。如果你将鼠标悬停在这些位置，你会看到工具提示，红色和绿色显示“CUDA内存操作进行中”，浅蓝色显示“CUDA内核正在运行（89.7%）”。这些将是我们分析的重点。在这一行中，我们将能够看到内存何时以及如何被传输（红色和绿色），以及我们的内核何时以及如何运行（浅蓝色）。
- en: Let’s dig in a little bit more on our kernels. You should see three very small
    blue slivers, each representing a kernel call. We will zoom into the region by
    clicking and dragging the mouse from just before the start of the first kernel
    call to just after the end of the last one, and then pressing Shift + Z. See Figure
    2.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步分析一下我们的内核。你应该能看到三个非常小的蓝色条形，每个代表一个内核调用。我们将通过点击并拖动鼠标，从第一个内核调用开始前一直拖到最后一个内核调用结束后，然后按下
    Shift + Z 来放大该区域。请参见图2。
- en: '![](../Images/6032ed5cc1bbb9414714374c4665e801.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6032ed5cc1bbb9414714374c4665e801.png)'
- en: 'Figure 2: Navigating an nsys report and zooming into an area of interest. Credits:
    Own work. CC BY-SA 4.0.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：浏览nsys报告并放大感兴趣区域。图片来源：原创作品。CC BY-SA 4.0。
- en: Now that we have found our kernels, let’s see some metrics. We open the `GPU
    > GPU Metrics` tabs for that. In this panel, can find "Warp Occupancy" (beige)
    for compute kernels. One way to optimize CUDA code is to ensure that the warp
    occupancy is as close to 100% as possible for as long as possible. This means
    that our GPU is not idling. We notice that this is happening for the first and
    last kernels but not the middle kernel. That is expected as the middle kernel
    launches a single thread. One final thing to note in this section is the `GPU
    > GPU Metrics > SMs Active > Tensor Active / FP16 Active` line. This line will
    show whether the tensor cores are being used. In this case you should verify that
    they are not.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了我们的内核，接下来让我们看看一些指标。我们打开`GPU > GPU Metrics`标签页。 在这个面板中，可以找到计算内核的“Warp占用率”（米色）。优化CUDA代码的一种方法是确保warp占用率尽可能长时间接近100%。这意味着我们的GPU不会空闲。我们注意到这是第一个和最后一个内核的情况，但中间的内核则没有。这是预期中的情况，因为中间的内核启动了一个线程。最后要注意的是`GPU
    > GPU Metrics > SMs Active > Tensor Active / FP16 Active`这一行。这一行将显示是否正在使用张量核心。在这种情况下，你应该确认它们没有被使用。
- en: 'Now let’s briefly look at the Events View. Right click `Processes > python
    > CUDA HW` and click "Show in Events View". Then sort the events by descending
    duration. In Figure 3, we see that the slowest events are two pageable memory
    transfers. We have seen in [*CUDA by Numba Examples Part 3: Streams and Events*](/cuda-by-numba-examples-7652412af1ee)
    that pageable memory transfers can be suboptimal, and we should prefer page-locked
    or "pinned" memory transfers. If we have slow memory transfers due to use of pageable
    memory, the Events View can be a great location to identify where these slow transfers
    can be found. Pro tip: you can isolate memory transfers by right clicking `Processes
    > python > CUDA HW > XX% Memory` instead.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们简要查看一下事件视图。右键点击`Processes > python > CUDA HW`并选择“在事件视图中显示”。然后按持续时间降序排列事件。在图3中，我们看到最慢的事件是两个可分页内存传输。我们在[*CUDA
    by Numba Examples Part 3: Streams and Events*](/cuda-by-numba-examples-7652412af1ee)中看到，可分页内存传输可能效率较低，我们应该更倾向于使用页面锁定或“固定”内存传输。如果由于使用了可分页内存而导致内存传输变慢，事件视图是一个非常好的地方来识别这些慢速传输的位置。小贴士：你可以通过右键点击`Processes
    > python > CUDA HW > XX% Memory`来隔离内存传输。'
- en: '![](../Images/1c760b926c35e13ae7463edf08b32a3f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c760b926c35e13ae7463edf08b32a3f.png)'
- en: 'Figure 3\. Events View in Nsight Systems showing a pageable (non-pinned) memory
    transfer. Credits: Own work. CC BY-SA 4.0.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. Nsight Systems中的事件视图显示了一个可分页（非固定）内存传输。图片来源：原创作品。CC BY-SA 4.0。
- en: In this section we learned how to profile a Python program which uses CUDA,
    and how to visualize basic information of this program in the Nsight Systems GUI.
    We also noticed that in this simple program, we are using pageable instead of
    pinned memory, that one of our kernels is not occupying all warps, that the GPU
    is idle for quite some time between kernels being run and that we are not using
    tensor cores.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何分析使用CUDA的Python程序，并且如何在Nsight Systems GUI中可视化该程序的基本信息。我们还注意到，在这个简单的程序中，我们使用的是可分页内存而不是固定内存，某个内核并没有占满所有warp，GPU在运行内核之间有相当长的空闲时间，而且我们没有使用张量核心。
- en: Annotating with NVTX
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NVTX进行注解
- en: 'In this section we will learn how to improve our profiling experience by annotation
    sections in Nsight Systems with NVTX. NVTX allows us to mark different regions
    of the code. It can mark ranges and instantaneous events. For a deeper look, check
    the [docs](https://nvtx.readthedocs.io/en/latest/index.html). Below we create
    `run_v2.py`, which, in addition to annotating `run_v1.py`, also changes this line:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何通过在Nsight Systems中使用NVTX进行注解来改进我们的性能分析体验。NVTX允许我们标记代码中的不同区域。它可以标记范围和瞬时事件。要深入了解，请查看[文档](https://nvtx.readthedocs.io/en/latest/index.html)。以下是我们创建的`run_v2.py`，除了注解`run_v1.py`外，还修改了这一行：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'to these:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 改为：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Therefore, in addition to the annotations, we are now using a pinned memory.
    If you want to learn more about the different types of memories that CUDA supports,
    see the [CUDA C++ Programming Guide](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/).
    It is of relevance that this is not the only way to pin an array in Numba. A previously
    created Numpy array can also be created with a context, as explained in the [Numba
    documentation](https://numba.readthedocs.io/en/stable/cuda/memory.html#pinned-memory).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了注释之外，我们现在还使用了固定内存。如果你想了解更多关于CUDA支持的不同类型内存的信息，请参见[CUDA C++ 编程指南](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)。需要注意的是，这并不是在Numba中固定数组的唯一方式。一个先前创建的Numpy数组也可以使用上下文创建，详情请参见[Numba
    文档](https://numba.readthedocs.io/en/stable/cuda/memory.html#pinned-memory)。
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Comparing the two files, you can see it’s as simple as wrapping some GPU kernel
    calls with
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这两个文件，你可以看到，只需用一些GPU内核调用进行封装就能实现。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Pro tip: you can also annotate functions by placing the `@nvtx.annotate` decorator
    above their definition, automatically annotate everything by calling your script
    with `python -m nvtx run_v2.py`, or apply the autoannotator selectively in you
    code by enabling or disabling `nvtx.Profile()`. See the [docs](https://nvtx.readthedocs.io/en/latest/index.html)!'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 专业提示：你还可以通过在函数定义上方添加`@nvtx.annotate`装饰器来注释函数，或者通过使用`python -m nvtx run_v2.py`命令自动注释所有内容，或者在代码中选择性地启用或禁用`nvtx.Profile()`来应用自动注释器。详细信息请参见[文档](https://nvtx.readthedocs.io/en/latest/index.html)！
- en: Let’s run this new script and open the results in Nsight Systems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这个新脚本，并在 Nsight Systems 中查看结果。
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Again, we start by minimizing everything, leaving only `Processes > python >
    CUDA HW` open. See Figure 4\. Notice that we now have a new line, `NVTX`. On this
    line in the timeline window we should see different colored blocks corresponding
    to the annotation regions that we created in the code. These are `Compilation`,
    `H2D Memory`, `Kernels` and `D2H Memory`. Some of these my be too small to read,
    but will be legible if you zoom into the region.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们从最小化所有内容开始，只保留`Processes > python > CUDA HW`打开。参见图 4\. 注意我们现在有了一条新线，`NVTX`。在时间轴窗口的这一行，我们应该能看到不同颜色的块，这些块对应于我们在代码中创建的注释区域。这些区域包括`Compilation`、`H2D
    Memory`、`Kernels`和`D2H Memory`。其中一些可能太小无法阅读，但如果你放大区域，它们将变得清晰可见。
- en: '![](../Images/5d39f2812209ec8b24bb77d6c9c10a41.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d39f2812209ec8b24bb77d6c9c10a41.png)'
- en: 'Figure 4\. Example of NVTX annotations and an Events View with pinned memory.
    Credits: Own work. CC BY-SA 4.0.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. NVTX 注释示例和带有固定内存的事件视图。图片来源：自作，CC BY-SA 4.0。
- en: The profiler confirms that this memory is pinned, ensuring that our code is
    truly using pinned memory. In addition, `H2D Memory` and `D2H Memory` are now
    taking less than half of the time that they were taking before. Generally we can
    expect better performance using pinned memory or prefetched mapped arrays (not
    supported by Numba).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析器确认该内存已被固定，确保我们的代码确实在使用固定内存。此外，`H2D Memory`和`D2H Memory`现在的时间不到之前的一半。一般来说，使用固定内存或预取的映射数组（Numba不支持）通常会获得更好的性能。
- en: Stream Concurrency
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流并发性
- en: Now we will investigate whether we can improve this code by introducing streams.
    The idea is that while memory transfers are occurring, the GPU can start processing
    the data. This allows a level of concurrency, which hopefully will ensure that
    we are occupying our warps as fully as possible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将调查是否可以通过引入流来改进此代码。其思想是，在进行内存传输的同时，GPU可以开始处理数据。这允许一定程度的并发性，理论上将确保我们尽可能充分地利用warps。
- en: '![](../Images/1bbde1985c738ae6d4cc600f1aa61ad7.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1bbde1985c738ae6d4cc600f1aa61ad7.png)'
- en: 'Figure 5\. Using different streams may allow for concurrent execution. Credits:
    [Zhang et al. 2021](https://www.mdpi.com/1045598) (CC BY 4.0).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 使用不同流可能允许并发执行。图片来源：[Zhang et al. 2021](https://www.mdpi.com/1045598)（CC
    BY 4.0）。
- en: In the code below we will split the processing of our array into roughly equal
    parts. Each part will run in a separate stream, including transferring data and
    computing the sum of the array. Then, we synchronize all streams and sum their
    partial sums. At this point we can then launch normalization kernels for each
    stream independently.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将把数组的处理分成大致相等的部分。每个部分将在单独的流中运行，包括数据传输和数组求和。然后，我们同步所有流并求和它们的部分和。此时，我们可以为每个流独立启动归一化内核。
- en: 'We want to answer a few questions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想回答以下几个问题：
- en: Will the code below truly create concurrency? Could we be introducing a bug?
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码真的会创建并发吗？我们会引入一个bug吗？
- en: Is it faster than the code which uses a single stream?
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它比使用单一流的代码更快吗？
- en: Is the warp occupancy better?
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: warp占用率更好吗？
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's run the code and collect results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行代码并收集结果。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The program ran and yielded the correct answer. But when we open the profiling
    file (see Figure 6), we notice that there are two streams instead of 4! And one
    is basically completely idle! What’s going on here?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行并得出了正确的答案。但是当我们打开分析文件时（见图6），我们注意到只有两个流，而不是四个！而且其中一个几乎完全闲置！这是怎么回事？
- en: '![](../Images/0fb44b9158a16628a582cda7e564b1b3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fb44b9158a16628a582cda7e564b1b3.png)'
- en: 'Figure 6\. Example of buggy multi-stream code. Credits: Own work. CC BY-SA
    4.0.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6. 错误的多流代码示例。来源：自主创作。CC BY-SA 4.0。
- en: There is a bug in the creation of the streams. By doing
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在流的创建过程中存在一个错误。通过以下操作
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: we are actually creating a single stream and repeating it `nstreams` times.
    So why are we seeing two streams instead of one? The fact that one is not doing
    much computation should be an indicator that there is a stream that we are not
    using. This stream is the default stream, which we are not using at all in out
    code since all GPU interactions are given a stream, the stream we created.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上是创建了一个流，并将其重复`nstreams`次。那么为什么我们看到的是两个流，而不是一个流呢？事实上，其中一个没有做多少计算应该是一个信号，表明有一个流我们并没有使用。这个流是默认流，我们在代码中完全没有使用它，因为所有GPU交互都指定了一个流，即我们创建的流。
- en: 'We can fix this bug with:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式修复此错误：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code above will also ensure they are really different streams, so it would
    have caught the bug had we had it in the code. It does so by checking the stream
    pointer value.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码还将确保它们真的是不同的流，因此，如果代码中存在错误，它也能被捕捉到。它通过检查流指针值来实现这一点。
- en: Now we can run the fixed code with 1 stream and 8 streams for comparison. See
    Figures 7 and 8, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以分别用1个流和8个流运行修复后的代码进行比较。分别见图7和图8。
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/7da8e8b11bc79e7ca948e97a21759fb7.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7da8e8b11bc79e7ca948e97a21759fb7.png)'
- en: 'Figure 7\. Example of single stream code. Credits: Own work. CC BY-SA 4.0.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 单流代码示例。来源：自主创作。CC BY-SA 4.0。
- en: '![](../Images/2da5d025d8a9e310ce1b4eb55ddfd113.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2da5d025d8a9e310ce1b4eb55ddfd113.png)'
- en: 'Figure 7\. Example of correct multi-stream code. Credits: Own work. CC BY-SA
    4.0.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 正确的多流代码示例。来源：自主创作。CC BY-SA 4.0。
- en: Again, both give correct results. By opening the one with 8 streams we see that
    yes, the bug has been fixed (Figure 7). Indeed, we now see 9 streams (8 created
    + default). In addition, we see that they are working at the same time! So we
    have achieved concurrency!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，两者都给出了正确的结果。通过打开带有8个流的那个文件，我们看到，确实，错误已经被修复（见图7）。实际上，现在我们看到了9个流（8个创建的流+默认流）。此外，我们还看到它们同时工作！因此，我们实现了并发！
- en: Unfortunately, if we dig a bit deeper we notice that the concurrent code is
    not necessarily faster. On my machine the critical section of both versions, from
    start of memory transfer to the last GPU-CPU copy takes around 160 ms.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，如果我们再深入一点，就会发现并发代码不一定更快。在我的机器上，无论是哪个版本的代码，从内存传输开始到最后的GPU-CPU拷贝，都大约需要160毫秒。
- en: A likely culprit is the warp occupancy. We notice that the warp occupancy is
    significantly better in the single-stream version. The gains we are getting in
    this example in compute are likely being lost by not occupying our GPU as efficiently.
    This is likely related to the structure of the code which (artificially) calls
    way too many kernels. In addition, if all threads are filled by a single stream,
    there is no gain in concurrency, since other streams have to be idle until resources
    free up.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的罪魁祸首是warp占用率。我们注意到，在单流版本中，warp占用率显著更好。在这个例子中，我们在计算上获得的增益可能因为没有高效利用GPU而被浪费掉。这可能与代码结构有关，该结构（人为地）调用了太多的内核。此外，如果所有线程都由一个流填充，则并发性没有增益，因为其他流必须等到资源释放后才能开始工作。
- en: This example is important because it shows that our preconceived notions of
    performance are just hypotheses. They need to be verified.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子很重要，因为它表明我们对于性能的先入之见仅仅是假设，必须经过验证。
- en: At this point of APOD, we have assessed, parallelized (both through threads
    and concurrency) and so the next step would be to deploy. We also noticed a slight
    performance regression with concurrency, so for this example, a single-stream
    version would likely be the one deployed. In production, the next step would be
    to follow the next piece of code which is best suited for parallelization and
    restarting APOD.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 APOD 的这个阶段，我们已经完成了评估和并行化（通过线程和并发），因此下一步将是部署。我们还注意到并发导致了轻微的性能回退，因此在这个例子中，单流版本可能是最适合部署的。在生产环境中，下一步将是找到最适合并行化的代码段并重新启动
    APOD。
- en: Conclusion
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article we saw how to set up, use and interpret results from profiling
    Python code in NVIDIA Nsight Systems. C and C++ code can be analyzed very similarly,
    and indeed most of the material out there uses C and C++ examples.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了如何设置、使用以及解读在 NVIDIA Nsight Systems 中对 Python 代码的性能分析结果。C 和 C++ 代码的分析方法非常相似，实际上大多数相关资料都使用了
    C 和 C++ 示例。
- en: We also show how profiling can allow us to catch bugs and performance test our
    programs, ensuring that the features we introduce truly are improving performance,
    and if they are not, why.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了性能分析如何帮助我们捕捉 bug 和对程序进行性能测试，确保我们引入的新功能确实能够提高性能，如果没有提高，也能找出原因。
