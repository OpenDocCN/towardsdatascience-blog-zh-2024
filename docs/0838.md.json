["```py\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\ndef owl_classifier(X_train, T, R, kernel, gamma):\n  n = len(T)\n  pi = np.zeroes(n) #Initialize pi as a vector of zeroes\n  probs = LogisticRegression().fit(X_train, T).predict_proba(X_train)#This is a n*unique(T) matrix that gives every person the probability of getting each treatment\n  for t in np.unique(T):\n    pi += probs[,t]*(T==t) #Every data point is assigned the probability of getting the treatment that it got, given the covariates\n  clf = svm.SVC(kernel = kernel, gamma = gamma) # initialize an svm classifier, the parameters need to be found by cross validation\n  clf.fit(X_train, T, sample_weight = R/pi) # fit the classifier with the treatments as labels and R/pi as sample weights \n```", "```py\n# This code block creates the data for the simulation\nimport numpy as np\n\nn_train = 500 # I purposely chose a small training set to simulate a medical trial\nn_col = 50 # This is the number of features\nn_test = 1000\nX_train = np.random.uniform(low = -1, high = 1, size = (n_train, n_col))\nT = np.random.randint(3, size = n_train) # Treatments given at random uniformly\nR_mean = (X_train[:,0]+X_train[:,1])*(T==0) + (X_train[:,0]-X_train[:,1])*(T==1) + (X_train[:,1]-X_train[:,0])*(T==2)\nR = np.random.normal(loc = R_mean, scale = .1) # The stanadard deviation can be tweaked\nX_test = np.random.uniform(low = -1 , high = 1, size = (n_test, n_col))\n\n# The optimal classifier can be deduced from the design of R\noptimal_classifier = (1-(X_test[:,0] >0)*(X_test[:,1]>0))*((X_test[:,0] > X_test[:,1]) + 2*(X_test[:,1] > X_test[:,0]))\n```", "```py\n# Code for the plot \nimport seaborn as sns\n\nkernel = 'rbf'\ngamma = 1/X_train.shape[1] \n# gamma is a hyperparameter that has to be found by cross validation but this is a good place to start\nD = owl_classifier(X_train, T, R, kernel, gamma)\nprediction = D.predict(X_test)\nsns.scatterplot(x = X_test[:,0], y = X_test[:,1], c = prediction )\n```"]