- en: Fine-tune Google Gemma with Unsloth and Distilled DPO on Your Computer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的电脑上使用Unsloth和蒸馏DPO微调Google Gemma
- en: 原文：[https://towardsdatascience.com/fine-tune-google-gemma-with-unsloth-and-distilled-dpo-on-your-computer-ca1ce8828122?source=collection_archive---------5-----------------------#2024-03-18](https://towardsdatascience.com/fine-tune-google-gemma-with-unsloth-and-distilled-dpo-on-your-computer-ca1ce8828122?source=collection_archive---------5-----------------------#2024-03-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-google-gemma-with-unsloth-and-distilled-dpo-on-your-computer-ca1ce8828122?source=collection_archive---------5-----------------------#2024-03-18](https://towardsdatascience.com/fine-tune-google-gemma-with-unsloth-and-distilled-dpo-on-your-computer-ca1ce8828122?source=collection_archive---------5-----------------------#2024-03-18)
- en: Following Hugging Face’s Zephyr recipe
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遵循Hugging Face的Zephyr配方
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--ca1ce8828122--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)
    ·8 min read·Mar 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ca1ce8828122--------------------------------)
    ·阅读时间8分钟·2024年3月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b55099aa531dd1323734916dffb4496b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b55099aa531dd1323734916dffb4496b.png)'
- en: Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E生成
- en: Finding good training hyperparameters for new LLMs is always difficult and time-consuming.
    With [Zephyr Gemma 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1),
    Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used
    a combination of distilled supervised fine-tuning and DPO similar to what they
    did for their [original Zephyr based on Mistral 7B](https://medium.com/towards-data-science/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7).
    However, training Gemma with DPO on consumer hardware is challenging due to its
    memory consumption.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为新LLM模型寻找合适的训练超参数一直是一个既困难又耗时的任务。通过[Zephyr Gemma 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1)，Hugging
    Face似乎找到了一个良好的微调Gemma的配方。他们采用了蒸馏监督式微调与DPO的组合，类似于他们为[原版Zephyr（基于Mistral 7B）](https://medium.com/towards-data-science/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7)所做的。然而，由于内存消耗问题，在消费者硬件上用DPO训练Gemma仍然具有挑战性。
- en: In this article, I first review the recipe used by Hugging Face to train Zephyr
    Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing
    various optimizations for fast and memory-efficient training. The method presented
    in this article has a peak memory consumption of 19 GB of VRAM and a total training
    time of only 8 hours. In other words, DPO training for Gemma is possible on consumer
    hardware.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文首先回顾了Hugging Face用于训练Zephyr Gemma 7B的配方。接着，我展示了如何结合使用这个配方与Unsloth，一个实现了各种优化的框架，用于快速且内存高效的训练。本文中介绍的方法的峰值内存消耗为19GB
    VRAM，训练总时长仅为8小时。换句话说，在消费者硬件上进行Gemma的DPO训练是可行的。
- en: A Closer Look at Zephyr Gemma
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入了解Zephyr Gemma
- en: Supervised Fine-tuning (SFT)
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督式微调（SFT）
- en: 'DPO must use for reference a model trained with supervised fine-tuning (SFT)
    on an instruction dataset. Hugging Face also released this SFT model:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DPO必须参考一个已经通过在指令数据集上进行监督式微调（SFT）训练的模型。Hugging Face也发布了这个SFT模型：
- en: '[HuggingFaceH4/zephyr-7b-gemma-sft-v0.1](https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-sft-v0.1)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFaceH4/zephyr-7b-gemma-sft-v0.1](https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-sft-v0.1)'
