- en: Building an LLMOPs Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-an-llmops-pipeline-08d367b36d64?source=collection_archive---------4-----------------------#2024-01-18](https://towardsdatascience.com/building-an-llmops-pipeline-08d367b36d64?source=collection_archive---------4-----------------------#2024-01-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Utilize SageMaker Pipelines, JumpStart, and Clarify to Fine-Tune and Evaluate
    a Llama 7B Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--08d367b36d64--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--08d367b36d64--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--08d367b36d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--08d367b36d64--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--08d367b36d64--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--08d367b36d64--------------------------------)
    ·10 min read·Jan 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f943f8d3c72ddb5cd475429d4efef2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/black-and-gray-metal-pipe-4CNNH2KEjhc)
    by [Sigmund](https://unsplash.com/@sigmund)
  prefs: []
  type: TYPE_NORMAL
- en: 2023 was the year that witnessed the rise of various Large Language Models (LLMs)
    in the Generative AI space. LLMs have incredible power and potential, but productionizing
    them has been a consistent challenge for users. An especially prevalent problem
    is what LLM should one use? Even more specifically, how can one evaluate an LLM
    for accuracy? This is especially challenging when there’s a large number of models
    to choose from, different datasets for fine-tuning/RAG, and a variety of prompt
    engineering/tuning techniques to consider.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem we need to establish [DevOps](https://aws.amazon.com/devops/what-is-devops/#:~:text=DevOps%20is%20the%20combination%20of,development%20and%20infrastructure%20management%20processes.)
    best practices for LLMs. Having a workflow or pipeline that can help one evaluate
    different models, datasets, and prompts. This field is starting to get known as
    [LLMOPs/FMOPs](https://aws.amazon.com/blogs/machine-learning/fmops-llmops-operationalize-generative-ai-and-differences-with-mlops/).
    Some of the parameters that can be considered in LLMOPs are shown below, in a
    (extremely) simplified flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09d38bebfc8fd0563afadb93fe8d73dd.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM Evaluation Consideration (By Author)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll try to tackle this problem by building a pipeline that
    fine-tunes, deploys, and evaluates a [Llama 7B model](https://huggingface.co/meta-llama/Llama-2-7b).
    You can also scale this example, by…
  prefs: []
  type: TYPE_NORMAL
