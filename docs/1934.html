<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Create Stronger Decision Trees with bootstrapping and genetic algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Create Stronger Decision Trees with bootstrapping and genetic algorithms</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09">https://towardsdatascience.com/create-stronger-decision-trees-with-bootstrapping-and-genetic-algorithms-1ae633a993c9?source=collection_archive---------1-----------------------#2024-08-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b86a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A technique to better allow decision trees to be used as interpretable models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--1ae633a993c9--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1ae633a993c9--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">24 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">1</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="06fb" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">While decision trees can often be effective as interpretable models (they are quite comprehensible), they rely on a greedy approach to construction that can result in sub-optimal trees. In this article, we show how to generate classification decision trees of the same size that may be generated by a standard algorithm, but that can have significantly better performance.</p><p id="ff5d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This article continues a series of articles on interpretable AI that also includes discussions of <a class="af ni" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a>, <a class="af ni" href="https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223" rel="noopener">AdditiveDecisionTrees</a>, and <a class="af ni" rel="noopener" target="_blank" href="/prism-rules-in-python-14d2cfd801a3">PRISM Rules</a>.</p><h1 id="3356" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Motivation</h1><p id="3ffc" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">It’s often useful in machine learning to use interpretable models for prediction problems. Interpretable models provide at least two major advantages over black-box models. First, with interpretable models, we understand why the specific predictions are made as they are. And second, we can determine if the model is safe for use on future (unseen) data. Interpretable models are often preferred over black-box models, for example, in high-stakes or highly-regulated environments where there is too much risk in using black-box models.</p><p id="abba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Decision trees, at least when constrained to reasonable sizes, are quite comprehensible and are excellent interpretable models when they are sufficiently accurate. However, it is not always the case that they achieve sufficient accuracy and decision trees can often be fairly weak, particularly compared to stronger models for tabular data such as CatBoost, XGBoost, and LGBM (which are themselves boosted ensembles of decision trees).</p><p id="0fb0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, where decision trees are sufficiently accurate, this accuracy is often achieved by allowing the trees to grow to large sizes, thereby eliminating any interpretability. Where a decision tree has a depth of, say, 6, it has 2⁶ (64) leaf nodes, so effectively 64 rules (though the rules overlap, so the cognitive load to understand these isn’t necessarily as large as with 64 completely distinct rules) and each rule has 6 conditions (many of which are often irrelevant or misleading). Consequently, a tree this size could probably not be considered interpretable — though may be borderline depending on the audience. Certainly anything much larger would not be interpretable by any audience.</p><p id="973e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, any reasonably small tree, such as with a depth of 3 or 4, could be considered quite manageable for most purposes. In fact, shallow decision trees are likely as interpretable as any other model.</p><p id="4a5f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given how effective decision trees can be as interpretable models (even if high accuracy and interpretability isn’t always realized in practice), and the small number of other options for interpretable ML, it’s natural that much of the research into interpretable ML (including this article) relates to making decision trees that are more effective as interpretable models. This comes down to making them more accurate at smaller sizes.</p><h1 id="ba52" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Interpretable models as proxy models</h1><p id="2562" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">As well as creating interpretable models, it’s also often useful in machine learning to use interpretable models as something called <em class="ok">proxy models</em>.</p><p id="af66" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, we can create, for some prediction problem, possibly a CatBoost or neural network model that appears to perform well. But the model will be (if CatBoost, neural network, or most other modern model types) inscrutable: we won’t understand its predictions. That is, testing the model, we can determine if it’s sufficiently accurate, but will not be able to determine <em class="ok">why </em>it is making the predictions it is.</p><p id="f20b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given this, it may or may not be workable to put the model into production. What we can do, though, is create a tool to try to estimate (and explain in a clear way) why the model is making the predictions it is. One technique for this is to create what’s called a <em class="ok">proxy model</em>.</p><p id="baca" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can create a simpler, interpretable model, such as a Decision Tree, rule list, GAM, or ikNN, to predict the behavior of the black-box model. That is, the proxy model predicts what the black-box model will predict. Decision Trees can be very useful for this purpose.</p><p id="b491" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the proxy model can be made sufficiently accurate (it estimates well what the black-box model will predict) but also interpretable, it provides some insight into the behavior of the black-box model, albeit only approximately: it can help explain why the black-box model makes the predictions it does, though may not be fully accurate and may not be able to predict the black-box’s behavior on unusual future data. Nevertheless, where only an approximate explanation is necessary, proxy models can be quite useful to help understand black-box models.</p><p id="9750" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For the remainder of this article, we assume we are creating an interpretable model to be used as the actual model, though creating a proxy model to approximate another model would work in the same way, and is also an important application of creating more accurate small decision trees.</p><h1 id="922f" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">The greedy approach used by a standard decision tree</h1><p id="3bc6" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Normally when constructing a decision tree, we start at the root node and identify the best initial split, creating two child nodes, which are then themselves split in two, and so on until some stopping condition is met.</p><p id="f0a4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Each node in a decision tree, during training, represents some portion of the training data. The root node covers the full training set. This will have two child nodes that each represent some subset of the training data (such that the two subsets do not overlap, and cover the full set of training data from their parent node).</p><p id="f9a3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The set of rows covered by each internal node are split into two subsets of rows (typically not of the same sizes) based on some condition relating to one of the features. In the figure below, the root node splits on feature A &gt; 10.4, so the left node will represent all rows in the training data where feature A &lt; 10.4 and the right node will represent all rows in the training data where feature A ≥ 10.4.</p><p id="08da" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To find the split condition at each internal node, this process selects one feature and one split point that will maximize what’s known as the <em class="ok">information gain, </em>which relates to the consistency in the target values. That is: assuming a classification problem, we start (for the root node) with the full dataset. The target column will include some proportion of each target class. We try to split the dataset into two subsets such that the two subsets are each as consistent with respect to the target classes as possible.</p><figure class="oo op oq or os ot ol om paragraph-image"><div role="button" tabindex="0" class="ou ov ed ow bh ox"><div class="ol om on"><img src="../Images/19fcd3415e736cd4ccf334ac30f7503f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBv8cV1GART9Imex7Du9vg.png"/></div></div></figure><p id="159f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, in the full dataset we may have 1000 rows, with 300 rows for class A, 300 for class B, and 400 for class C. We may split these into two subsets such that the two subsets have:</p><ul class=""><li id="82be" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh oz pa pb bk">left subset: 160 class A, 130 class B, 210 class C</li><li id="dc88" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">right subset: 140 class A, 170 class B, 190 class C</li></ul><p id="38e9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here we have the proportions of the three classes almost the same in the two child nodes as in the full dataset, so there is no (or almost no) information gain. This would be a poor choice of split.</p><p id="04b8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">On the other hand, if we split the data such that we have:</p><ul class=""><li id="e28b" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh oz pa pb bk">left subset: 300 class A, 5 class B, 3 class C</li><li id="59b3" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">right subset: 0 class A, 295 class B, 397 class C</li></ul><p id="7cf1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this case, we have far more consistency in terms of the target in the two child nodes than in the full dataset. The left child has almost only class A, and the right child has only classes B and C. So, this is a very good split, with high information gain.</p><p id="a83e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The best split would be then be selected, as possibly the second example here, or, if possible, a split resulting in even higher information gain (with even more consistency in the target classes in the two child nodes).</p><p id="3136" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The process is then repeated in each of these child nodes. In the figure above we see the left child node is then split on feature B &gt; 20.1 and then its left child node is split on feature F &gt; 93.3.</p><p id="798c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is generally a reasonable approach to constructing trees, but in no way guarantees finding the best tree that’s possible. Each decision is made in isolation, considering only the data covered by that node and not the tree as a whole.</p><p id="5a53" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Further, with standard decision trees, the selection of the feature and threshold at each node is a one-time decision (that is, it’s a greedy algorithm): decision trees are limited to the choices made for split points once these splits are selected. While the trees can (at lower levels) compensate for poor modeling choices higher in the tree, this will usually result in extra nodes, or in splits that are harder to understand, so will reduce interpretability, and may not fully mitigate the effects of the choices of split points above.</p><p id="c4ae" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Though the greedy approach used by Decision Trees is often quite sub-optimal, it does allow trees to be constructed very quickly. Historically, this was more important given lower-powered computer systems (evaluating every possible split point in every feature at each node is actually a substantial amount of work, even if very fast on modern hardware). And, in a modern context, the speed allowed with a greedy algorithm can also be very useful, as it allows quickly constructing many trees in models based on large ensembles of decision trees.</p><p id="8e61" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, to create a single decision tree that is both accurate and interpretable (of a reasonably small size), using a greedy algorithm is very limiting. It is often possible to construct a decision tree of a limited size that can achieve both a good level of accuracy, and a substantially higher level of accuracy than would be found with a greedy approach.</p><h1 id="b336" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Genetic Algorithms</h1><p id="2bf7" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Before looking at decision trees specifically, we’ll go over quickly genetic algorithms generally. They are used broadly in computer science and are often very effective at developing solutions to problems. They work by generating many potential solutions to a give problem and finding the best through trial and error, though in a guided, efficient way, simulating real-world genetic processes.</p><p id="ceda" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Genetic algorithms typically proceed by starting with a number of candidate solutions to a problem (usually created randomly), then iterating many times, with each round selecting the strongest candidates, removing the others, and creating a new set of candidate solutions based on the best (so far) existing solutions. This may be done either by mutating (randomly modifying) an existing solution or by combining two or more into a new solution, simulating reproduction as seen in real-world evolutionary processes.</p><p id="c4a0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this way, over time, a set of progressively stronger candidates tends to emerge. Not every new solution created is any stronger than the previously-created solutions, but at each step, some fraction likely will be, even if only slightly.</p><p id="f0cf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">During this process, it’s also possible to regularly generate completely new random solutions. Although these will not have had the benefit of being mutations or combinations of strong solutions (also initially created randomly), they may nevertheless, by chance, be as strong as some more evolved-solutions. Though this is increasingly less likely, as the candidates that are developed through the genetic process (and selected as among the best solutions thus far) become increasingly evolved and well-fit to the problem.</p><p id="23f8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Applied to the construction of decision trees, genetic algorithms create a set of candidate decision trees, select the best of these, mutate and combine these (with some new trees possibly doing both: deriving new offspring from multiple existing trees and mutating these offspring at the same time). These steps may be repeated any number of times.</p><p id="5bb4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Each time a new tree is generated from one or more existing trees, the new tree will be quite similar to the previous tree(s), but slightly different. Usually most internal nodes will be the same, but one (or a small number of) internal nodes will be modified: changing either the feature and threshold, or simply the threshold. Modifications may also include adding, removing, or rearranging the existing internal nodes. The predictions in the leaf nodes must also be re-calculated whenever internal nodes are modified.</p><p id="a03b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This process can be slow, requiring many iterations before substantial improvements in accuracy are seen, but in the case covered in this article (creating interpretable decision trees), we can assume all decision trees are reasonably small (by necessity for interpretability), likely with a maximum depth of about 2 to 5. This allows progress to be made substantially faster than where we attempt to evolve large decision trees.</p><h1 id="6152" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Other Approaches to Creating Stronger Decision Trees</h1><p id="b42b" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">There have been, over time, a number of proposals for genetic algorithms for decision trees. The solution covered in this article has the benefit of providing python code on github, but is far from the first and many other solutions may work better for your projects. There are several other projects on github as well to apply genetic algorithms to constructing decision trees, which may be worth investigating as well. But the solution presented here is straightforward and effective, and worth looking at where interpretable ML is useful.</p><p id="71e4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Besides genetic algorithms, other work seeking to make Decision Trees more accurate and interpretable (accurate at a constrained size) include <a class="af ni" href="https://arxiv.org/abs/1904.12847" rel="noopener ugc nofollow" target="_blank">Optimal Sparce Decision Trees</a>, oblique decision trees, oblivious decision trees, and <a class="af ni" href="https://github.com/Brett-Kennedy/AdditiveDecisionTree" rel="noopener ugc nofollow" target="_blank">AdditiveDecisionTrees</a>. The last of these I’ve covered in another Medium article, and will hopefully cover the others in subsequent articles.</p><p id="40c8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, there is a body of work related to creating interpretable rules including <a class="af ni" href="https://github.com/csinva/imodels" rel="noopener ugc nofollow" target="_blank">imodels</a> and <a class="af ni" href="https://github.com/Brett-Kennedy/PRISM-Rules" rel="noopener ugc nofollow" target="_blank">PRISM-Rules</a>. While rules are not quite equivalent to decision trees, they may often be used in a similar way and offer similar levels of accuracy and interpretability. And, trees can always be trivially converted to rules.</p><p id="f37d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Some tools such as <a class="af ni" href="https://github.com/cod3licious/autofeat" rel="noopener ugc nofollow" target="_blank">autofeat</a>, <a class="af ni" href="https://github.com/Brett-Kennedy/ArithmeticFeatures" rel="noopener ugc nofollow" target="_blank">ArithmeticFeatures</a>, <a class="af ni" href="https://github.com/Brett-Kennedy/FormulaFeatures" rel="noopener ugc nofollow" target="_blank">FormulaFeatures</a>, and <a class="af ni" href="https://github.com/Brett-Kennedy/RotationFeatures" rel="noopener ugc nofollow" target="_blank">RotationFeatures </a>may also be combined with standard or GeneticDecisionTrees to create models that are more accurate still. These take the approach of creating more powerful features so that fewer nodes within a tree are necessary to achieve a high level of accuracy: there is some loss in interpretability as the features are more complex, but the trees are often substantially smaller, resulting in an overall gain (sometimes a very large gain) in interpretability.</p><h1 id="2d8c" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Implementation Details</h1><p id="3296" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Decision Trees can be fairly sensitive to the data used for training. Decision Trees are notoriously unstable, often resulting in different internal representations with even small changes in the training data. This may not affect their accuracy significantly, but can make it questionable how well they capture the true function between the features and target.</p><p id="8f26" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The tendency to high variance (variability based on small changes in the training data) also often leads to overfitting. But with the GeneticDecisionTree, we take advantage of this to generate random candidate models.</p><p id="0932" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Under the hood, GeneticDecisionTree generates a set of scikit-learn decision trees, which are then converted into another data structure used internally by GeneticDecisionTrees (which makes the subsequent mutation and combination operations simpler). To create these scikit-learn decision trees, we simply fit them using different bootstrap samples of the original training data (along with varying the random seeds used).</p><p id="a866" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We also vary the size of the samples, allowing for further diversity. The sample sizes are based on a logarithmic distribution, so we are effectively selecting a random order of magnitude for the sample size. Given this, smaller sizes are more common than larger, but occasionally larger sizes are used as well. This is limited to a minimum of 128 rows and a maximum of two times the full training set size. For example, if the dataset has 100,000 rows, we allow sample sizes between 128 and 200,000, uniformly sampling a random value between log(128) and log(200,000), then taking the exponential of this random value as the sample size.</p><p id="5084" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The algorithm starts by creating a small set of decision trees generated in this way. It then iterates a specified number of times (five by default). Each iteration:</p><ul class=""><li id="674c" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh oz pa pb bk">It randomly mutates the top-scored trees created so far (those best fit to the training data). In the first iteration, this uses the full set of trees created prior to iterating. From each top-performing tree, a large number of mutations are created.</li><li id="3f12" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">It combines pairs of the top-scored trees created so far. This is done in an exhaustive manner over all pairs of the top performing trees that can be combined (details below).</li><li id="7de3" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">It generates additional random trees using scikit-learn and random bootstrap samples (less of these are generated each iteration, as it becomes more difficult to compete with the models that have experienced mutating and/or combining).</li><li id="28f9" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">It selects the top-performing trees before looping back for the next iteration. The others are discarded.</li></ul><p id="a1a9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Each iteration, a significant number of new trees are generated. Each is evaluated on the training data to determine the strongest of these, so that the next iteration starts with only a small number of well-performing trees and each iteration tends to improve on the previous.</p><p id="da25" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the end, after executing the specified number of iterations, the single top performing tree is selected and is used for prediction.</p><p id="a8db" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As indicated, standard decision trees are constructed in a purely greedy manner, considering only the information gain for each possible split at each internal node. With Genetic Decision Trees, on the other hand, the construction of each new tree may be partially or entirely random (the construction done by scikit-learn is largely non-random, but is based on random samples; the mutations are purely random; the combinations are purely deterministic). But the important decisions made during fitting (selecting the best models generated so far) relate to the fit of the tree as a whole to the available training data. This tends to generate a final result that fits the training better than a greedy approach allows.</p><p id="01c2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Despite the utility of the genetic process, an interesting finding is that: even while not performing mutations or combinations each iteration (with each iteration simply generating random decision trees), GeneticDecisionTrees tend to be more accurate than standard decision trees of the same (small) size.</p><p id="d163" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The mutate and combine operations are configurable and may be set to False to allow faster execution times — in this case, we simply generate a set of random decision trees and select the one that best fits the training data.</p><p id="4a07" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is as we would expect: simply by trying many sets of possible choices for the internal nodes in a decision tree, some will perform better than the single tree that is constructed in the normal greedy fashion.</p><p id="c80c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is, though, a very interesting finding. And also very practical. It means: even without the genetic processes, simply trying many potential small decision trees to fit a training set, we can almost always find one that fits the data better than a small decision tree of the same size grown in a greedy manner. Often substantially better. This can, in fact, be a more practical approach to constructing near-optimal decision trees than specifically seeking to create the optimal tree, at least for the small sizes of trees appropriate for interpretable models.</p><p id="d6b1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where mutations and combinations are enabled, though, generally after one or two iterations, the majority of the top-scored candidate decision trees (the trees that fit the training data the best) will be based on mutating and/or combining other strong models. That is, enabling mutating and combining does tend to generate stronger models.</p><p id="e5df" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Assuming we create a decision tree of a limited size, there is a limit to how strong the model can be — there is (though in practice it may not be actually found), some tree that can be created that best matches the training data. For example, with seven internal nodes (a root, two child nodes, and four grandchild nodes), there are only seven decisions to be made in fitting the tree: the feature and threshold used in each of these seven internal nodes.</p><p id="cff8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Although a standard decision tree is not likely to find the ideal set of seven internal nodes, a random process (especially if accompanied by random mutations and combinations) can approach this ideal fairly quickly. Though still unlikely to reach the ideal set of internal nodes, it can come close.</p><h1 id="127b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Exhaustive Tests for the Ideal Tree</h1><p id="cce2" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">An alternative method to create a near-optimal decision tree is to create and test trees using each possible set of features and thresholds: an exhaustive search of the possible small trees.</p><p id="3d98" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With even a very small tree (for example, seven internal nodes), however, this is intractable. With, for example, ten features, there are 10⁷ choices just for the features in each node (assuming features can appear any number of times in the tree). There are, then, an enormous number of choices for the thresholds for each node.</p><p id="012a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It would be possible to select the thresholds using information gain (at each node holding the feature constant and picking the threshold that maximizes information gain). With just ten features, this may be feasible, but the number of combinations to select the feature for each node still quickly explodes given more features. At 20 features, 20⁷ choices is over a billion.</p><p id="2e22" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using some randomness and a genetic process to some extent can improve this, but a fully exhaustive search is, in almost all cases, infeasible.</p><h1 id="3b65" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Execution Time</h1><p id="8b51" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The algorithm presented here is far from exhaustive, but does result in an accurate decision tree even at a small size.</p><p id="c6e1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The gain in accuracy, though, does come at the cost of time and this implementation has had only moderate performance optimizations (it does allow internally executing operations in parallel, for example) and is far slower than standard scikit-learn decision trees, particularly when executing over many iterations.</p><p id="102c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, it is reasonably efficient and testing has found using just 3 to 5 iterations is usually sufficient to realize substantial improvements for classification as compared to scikit-learn decision trees. For most practical applications, the performance is quite reasonable.</p><p id="5fd6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For most datasets, fitting is still only about 1 to 5 minutes, depending on the size of the data (both the number of rows and number of columns are relevant) and the parameters specified. This is quite slow compared to training standard decision trees, which is often under a second. Nevertheless, a few minutes can often be well-warranted to generate an interpretable model, particularly when creating an accurate, interpretable model can often be quite challenging.</p><p id="ada5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where desired, limiting the number of iterations to only 1 or 2 can reduce the training time and can often still achieve strong results. As would likely be expected, there are diminishing returns over time using additional iterations, and some increase in the chance of overfitting. Using the verbose setting, it is possible to see the progress of the fitting process and determine when the gains appear to have plateaued.</p><p id="95bc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Disabling mutations and/or combinations, though, is the most significant means to reduce execution time. Mutations and combinations allow the tool to generate variations on existing strong trees, and are often quite useful (they produce trees different than would likely be produced by scikit-learn), but are slower processes than simply generating random trees based on bootstrap samples of the training data: a large fraction of mutations are of low accuracy (even though a small fraction can be higher accuracy than would be found otherwise), while those produced based on random samples will all be at least viable trees.</p><p id="57de" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">That is, with mutations, we may need to produce and evaluate a large number before very strong ones emerge. This is less true of combinations, though, which are very often stronger than either original tree.</p><h1 id="3250" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Generating Random Decision Trees</h1><p id="8cfa" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">As suggested, it may be reasonable in some cases to disable mutations and combinations and instead generate only a series of random trees based on random bootstrap samples. This approach could not be considered a genetic algorithm — it simply produces a large number of small decision trees and selects the best-performing of these. Where sufficient accuracy can be achieved in this way, though, this may be all that’s necessary, and it can allow faster training times.</p><p id="212d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s also possible to start with this as a baseline and then test if additional improvements can be found by enabling mutations and/or combinations. Where these are used, the model should be set to execute at least a few iterations, to give it a chance to progressively improve over the randomly-produced trees.</p><p id="dc7e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We should highlight here as well, the similarity of this approach (creating many similar but random trees, not using any genetic process) to creating a RandomForest — RandomForests are also based on a set of decision trees, each trained on a random bootstrap sample. However, RandomForests will use all decision trees created and will combine their predictions, while GeneticDecisionTree retains only the single, strongest of these decision trees.</p><h1 id="2575" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Mutating</h1><p id="35dc" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">We’ll now describe in more detail specifically how the mutating and combining processes work with GeneticDecisionTree.</p><p id="c1ad" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The mutating process currently supported by GeneticDecisionTree is quite simple. It allows only modifying the thresholds used by internal nodes, keeping the features used in all nodes the same.</p><p id="0137" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">During mutation, a well-performing tree is selected and a new copy of that tree is created, which will be the same other than the threshold used in one internal node. The internal node to be modified is selected randomly. The higher in the tree it is, and the more different the new threshold is from the previous threshold, the more effectively different from the original tree the new tree will be.</p><p id="e119" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is surprisingly effective and can often substantially change the training data that is used in the two child nodes below it (and consequently the two sub-trees below the selected node).</p><p id="602a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Prior to mutation, the trees each start with the thresholds assigned by scikit-learn, selected based purely on information gain (not considering the tree as a whole). Even keeping the remainder of the tree the same, modifying these thresholds can effectively induce quite different trees, which often perform preferably. Though the majority of mutated trees do not improve over the original tree, an improvement can usually be identified by trying a moderate number of variations on each tree.</p><p id="5b47" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Future versions may also allow rotating nodes within the tree, but testing to date has found this not as effective as simply modifying the thresholds for a single internal node. However, more research will be done on other mutations that may prove effective and efficient.</p><h1 id="bbb5" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Combining</h1><p id="69ec" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The other form of modification currently supported is combining two well-performing decision trees. To do this, we take the top twenty trees found during the previous iteration and attempt to combine each pair of these. A combination is possible if the two trees use the same feature in their root nodes.</p><p id="b515" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, assume Tree 1 and Tree 2 (the two trees in the top row in the figure below) are among the top-performing trees found so far.</p><p id="521a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The figure shows four trees in all: Tree 1, Tree 2, and the two trees created from these. The internal nodes are shown as circles and the leaf nodes as squares.</p><p id="2a32" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Tree 1 has a split in its root node on Feature A &gt; 10.4 and Tree 2 has a split in its root node on Feature A&gt; 10.8. We can, then, combine the two trees: both use Feature A in their root node.</p><figure class="oo op oq or os ot ol om paragraph-image"><div role="button" tabindex="0" class="ou ov ed ow bh ox"><div class="ol om ph"><img src="../Images/99637b11c5a6f9461a86b4393a899905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wA8AbxL9wwHcAJdSHG1Cg.png"/></div></div></figure><p id="2e37" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then create two new trees. In both new trees, the split in the root node is taken as the average of that in the two original trees, so in this example, both new trees (shown in the bottom row of the figure) will have Feature A &gt; 10.6 in their root nodes.</p><p id="a04a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first new tree will have Tree 1’s left sub-tree (the left sub-tree under Tree 1’s root node, drawn in blue) and Tree 2’s right sub tree (drawn in pink). The other new tree will have Tree 2’s left sub-tree (purple) and Tree 1’s right sub-tree (green).</p><p id="ce51" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, Tree 1 and Tree 2 both have only 3 levels of internal nodes. In other examples, the subtrees may be somewhat larger, but if so, likely only one or two additional layers deep. The idea is the same regardless of the size or shapes of the subtrees.</p><p id="5637" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Combining in this way effectively takes, other than the root, half of one tree and half of another, with the idea that:</p><ul class=""><li id="f90d" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh oz pa pb bk">If both trees are strong, then (though not necessarily) likely the common choice of feature in the root node is strong. Further, a split point between those selected by both may be preferable. In the above example we used 10.6, which is halfway between the 10.4 and 10.8 used by the parent trees.</li><li id="9b27" class="mm mn fq mo b go pc mq mr gr pd mt mu mv pe mx my mz pf nb nc nd pg nf ng nh oz pa pb bk">While both trees are strong, neither may be optimal. The difference, if there is one, is in the two subtrees. It could be that Tree 1 has both the stronger left sub-tree and the stronger right sub-tree, in which case it is not possible to beat Tree 1 by combining with Tree 2. Similarly if Tree 2 has both the stronger left and right sub-trees. But, if Tree 1 has the stronger left sub-tree and Tree 2 the stronger right sub-tree, then creating a new tree to take advantage of this will produce a tree stronger than either Tree 1 or Tree 2. Similarly for the converse.</li></ul><p id="23b2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There are other ways we could conceivably combine two trees, and other tools to generate decision trees through genetic algorithms use other methods to combine trees. But simply taking a subtree from one tree and another subtree from another tree is a very straightforward approach and appealing in this way.</p><p id="36d7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Future versions will allow combining using nodes other than the root, though the effects are smaller in these cases — we’re then keeping the bulk of one tree and replacing a smaller portion from other tree, so producing a new tree less distinct from the original. This is, though, still a valuable form of combination and will be supported in the future.</p><h1 id="e1aa" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Overfitting</h1><p id="8f93" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Decision Trees commonly overfit and GeneticDecisionTrees may as well. Like most models, GeneticDecisionTree attempts to fit to the training data as well as is possible, which may cause it to generalize poorly compared to other decision trees of the same size.</p><p id="0b58" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, overfitting is limited as the tree sizes are generally quite small, and the trees cannot grow beyond the specified maximum depth. Each candidate decision tree produced will have equal complexity (or nearly equal — some paths may not extend to the full maximum depth allowed, so some trees may be slightly smaller than others), so are roughly equally likely to overfit.</p><p id="1fbe" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As with any model, though, it’s recommended to tune GeneticDecisionTrees to find the model that appears to work best with your data.</p><h1 id="63a9" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Regression</h1><p id="fbad" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">GeneticDecisionTrees support both classification and regression, but are much more appropriate for classification. In general, regression functions are very difficult to model with shallow decision trees, as it’s necessary to predict a continuous numeric value and each leaf node predicts only a single value.</p><p id="5f45" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, a tree with eight leaf nodes can predict only eight unique values. This is often quite sufficient for classification problems (assuming the number of distinct target classes is under eight) but can produce only very approximate predictions with regression. With regression problems, even with simple functions, generally very deep trees are necessary to produce accurate results. Going deeper into the trees, the trees are able to fine-tune the predictions more and more precisely.</p><p id="a242" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using a small tree with regression is viable only where the data has only a small number of distinct values in the target column, or where the values are in a small number of clusters, with the range of each being fairly small.</p><p id="0e19" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">GeneticDecisionTrees can work setting the maximum depth to a very high level, allowing accurate models, often substantially higher than standard decision trees, but the trees will not, then, be interpretable. And the accuracy, while often strong, will still likely not be competitive with strong models such as XGBoost, LGBM, or CatBoost. Given this, GeneticDecisionTrees for regression (or any attempts to create accurate shallow decision trees for regression), is typically infeasible.</p><h1 id="7d84" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Installation</h1><p id="0acd" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The github page for GeneticDecisionTrees is: <a class="af ni" href="https://github.com/Brett-Kennedy/GeneticDecisionTree" rel="noopener ugc nofollow" target="_blank">https://github.com/Brett-Kennedy/GeneticDecisionTree</a></p><p id="3b6c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To install, you can simply download the single <a class="af ni" href="https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/genetic_decision_tree.py" rel="noopener ugc nofollow" target="_blank">genetic_decision_tree.py</a> file and import it into your projects.</p><p id="611f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The github page also includes some example notebooks, but it should be sufficient to go through the <a class="af ni" href="https://github.com/Brett-Kennedy/GeneticDecisionTree/blob/main/Examples/Simple_Examples.ipynb" rel="noopener ugc nofollow" target="_blank">Simple Examples</a> notebook to see how to use the tool and some examples of the APIs. The github page also documents the APIs, but these are relatively simple, providing a similar, though smaller, signature than scikit-learn’s DecisionTreeClassifier.</p><h1 id="5bf9" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Examples</h1><p id="1c99" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The following example is taken from the Simple_Examples notebook provided on the github page. This loads a dataset, does a train-test split, fits a GeneticDecisionTree, creates predictions, and outputs the accuracy, here using the F1 macro score.</p><pre class="oo op oq or os pi pj pk bp pl bb bk"><span id="2aa7" class="pm nk fq pj b bg pn po l pp pq">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import f1_score<br/>from sklearn.datasets import load_wine<br/>from genetic_decision_tree import GeneticDecisionTree<br/><br/>data = load_wine()<br/>df = pd.DataFrame(data.data)<br/>df.columns = data.feature_names<br/>y_true = data.target<br/>X_train, X_test, y_train, y_test = train_test_split(df, y_true, test_size=0.3, random_state=42)<br/>gdt = GeneticDecisionTree(max_depth=2, max_iterations=5, allow_mutate=True, allow_combine=True, verbose=True)<br/>gdt.fit(X_train, y_train)<br/>y_pred = gdt.predict(X_test)<br/>print("Genetic DT:", f1_score(y_test, y_pred, average='macro'))</span></pre><p id="eac2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">GeneticDecisionTree is a single class used for both classification and regression. It infers from the target data the data type and handles the distinctions between regression and classification internally. As indicated, it is much better suited for classification, but is straight-forward to use for regression where desired as well.</p><p id="785a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Similar to scikit-learn’s decision tree, GeneticDecisionTree provides an export_tree() API. Used with the wine dataset, using a depth of 2, GeneticDecisionTree was able to achieve an F1 macro score on a hold-out test set of 0.97, compared to 0.88 for the scikit-learn decision tree. The tree produced by GeneticDecisionTree is:</p><pre class="oo op oq or os pi pj pk bp pl bb bk"><span id="ce56" class="pm nk fq pj b bg pn po l pp pq">IF flavanoids &lt; 1.4000<br/>| IF color_intensity &lt; 3.7250<br/>| | 1<br/>| ELSE color_intensity &gt; 3.7250<br/>| | 2<br/>ELSE flavanoids &gt; 1.4000<br/>| IF proline &lt; 724.5000<br/>| | 1<br/>| ELSE proline &gt; 724.5000<br/>| | 0</span></pre><h1 id="6699" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Classification Test</h1><p id="e097" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">The github page provides an extensive test of GeneticDecisionTrees. This tests with a large number of test sets from OpenML and for each creates a standard (scikit-learn) Decision Tree and four GeneticDecisionTrees: each combination of allowing mutations and allowing combinations (supporting neither, mutations only, combinations only, and both). In all cases, a max depth of 4 was used.</p><p id="4026" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In almost all cases, at least one, and usually all four, variations of the GeneticDecisionTree strongly out-perform the standard decision tree. These tests used F1 macro scores to compare the models. A subset of this is shown here:</p><figure class="oo op oq or os ot ol om paragraph-image"><div role="button" tabindex="0" class="ou ov ed ow bh ox"><div class="ol om pr"><img src="../Images/ee6908333111df374720c0fae4725d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_FSnqkhg-lceF0Kn.png"/></div></div></figure><p id="1959" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In most cases, enabling either mutations or combinations, or both, improves over simply producing random decision trees.</p><p id="edf2" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given the large number of cases tested, running this notebook is quite slow. It is also not a definitive evaluation: it uses only a limited set of test files, uses only default parameters other than max_depth, and tests only the F1 macro scores. It does, however, demonstrate the GeneticDecisionTrees can be effective and interpretable models in many cases.</p><h1 id="51ff" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusions</h1><p id="4c22" class="pw-post-body-paragraph mm mn fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">There are a number of cases where it is preferable to use an interpretable model (or a black-box model along with an interpretable proxy model for explainability) and in these cases, a shallow decision tree can often be among the best choices. However, standard decision trees can be generated in a sub-optimal way, which can result in lower accuracy, particularly for trees where we limit the size.</p><p id="68f6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The simple process demonstrated here of generating many decision trees based on random samples of the training data and identifying the tree that fits the training data best can provide a significant advantage over this.</p><p id="4a68" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In fact, the largest finding was that generating a set of decision trees based on different random samples can be almost as affective as the genetic methods included here. This finding, though, may not continue to hold as strongly as further mutations and combinations are added to the codebase in future versions, or where large numbers of iterations are executed.</p><p id="53a4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Beyond generating many trees, allowing a genetic process, where the training executes over several iterations, each time mutating and combining the best-performing trees that have been discovered to date, can often further improve this.</p><p id="42b6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The techniques demonstrated here are easy to replicate and enhance to suit your needs. It is also possible to simply use the GeneticDecisionTree class provided on github.</p><p id="b900" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where it makes sense to use decision trees for a classification project, it likely also makes sense to try GeneticDecisionTrees. They will almost always work as well, and often substantially better, albeit with some increase in fitting time.</p><p id="8832" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images by the author</p></div></div></div></div>    
</body>
</html>