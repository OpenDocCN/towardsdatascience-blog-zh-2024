- en: 'Towards Generalization on Graphs: From Invariance to Causality'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图谱的泛化探索：从不变性到因果性
- en: 原文：[https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18](https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18](https://towardsdatascience.com/towards-generalization-on-graphs-from-invariance-to-causality-c81a174ac37b?source=collection_archive---------6-----------------------#2024-07-18)
- en: '*This blog post shares recent papers on out-of-distribution generalization
    on graph-structured data*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*这篇博客分享了关于图结构数据的分布外泛化的最新论文*'
- en: '[](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)[![Qitian
    Wu](../Images/363e62ead857be0af76f9654c19f2b8c.png)](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)
    [Qitian Wu](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)[![Qitian
    Wu](../Images/363e62ead857be0af76f9654c19f2b8c.png)](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)
    [Qitian Wu](https://medium.com/@qitianwu228?source=post_page---byline--c81a174ac37b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)
    ·14 min read·Jul 18, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--c81a174ac37b--------------------------------)
    ·阅读时间：14分钟·2024年7月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb13a4c4ace10b85ad4a680e17242b0f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb13a4c4ace10b85ad4a680e17242b0f.png)'
- en: Image generated by GPT-4
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由GPT-4生成
- en: '*This blog post introduces recent advances in out-of-distribution generalization
    on graphs, an important yet under-explored problem in machine learning. We will
    first introduce the problem formulation and typical scenarios involving distribution
    shifts on graphs. Then we present an overview of three recently published papers
    (where I am the author):*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇博客介绍了图上的分布外泛化的最新进展，这是机器学习中一个重要但尚未深入探索的问题。我们将首先介绍问题的表述以及涉及图上分布变化的典型场景。然后，我们将概述三篇最近发布的论文（我是作者之一）：*'
- en: '[Handling Distribution Shifts on Graphs: An Invariance Perspective](https://arxiv.org/pdf/2202.02466),
    ICLR2022.'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[处理图上的分布变化：一种不变性视角](https://arxiv.org/pdf/2202.02466)，ICLR2022。'
- en: ''
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Graph Out-of-Distribution Generalization via Causal Intervention](https://arxiv.org/pdf/2402.11494),
    WWW2024.'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[图谱的分布外泛化通过因果干预](https://arxiv.org/pdf/2402.11494)，WWW2024。'
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Learning Divergence Fields for Shift-Robust Graph Representations](https://arxiv.org/pdf/2406.04963),
    ICML2024.'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习分歧场以获得抗偏移的图表示](https://arxiv.org/pdf/2406.04963)，ICML2024。'
- en: '*These works focus on generalization on graphs through the lens of invariance
    principle and causal intervention. Moreover, we will compare these methods and
    discuss potential future directions in this area.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些工作聚焦于通过不变性原理和因果干预的视角来研究图上的泛化问题。此外，我们将比较这些方法并讨论该领域未来可能的发展方向。*'
- en: Graph machine learning remains a popular research direction, especially with
    the wave of AI4Science driving increasingly diverse applications of graph data.
    Unlike general image and text data, graphs stand as a mathematical abstraction
    that describes the attributes of entities and their interactions within a system.
    In this regard, graphs can not only represent real-world physical systems of different
    scales (such as molecules, protein interactions, social networks, etc.), but also
    describe certain abstract topological relationships (such as scene graphs, industrial
    processes, chains of thought, etc.).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图机器学习仍然是一个热门的研究方向，尤其是在AI4Science浪潮的推动下，图数据的应用领域越来越广泛。与普通的图像和文本数据不同，图是一种数学抽象，描述了实体的属性以及它们在系统中的相互作用。在这方面，图不仅可以表示不同规模的现实世界物理系统（如分子、蛋白质相互作用、社交网络等），还可以描述某些抽象的拓扑关系（如场景图、工业过程、思维链等）。
- en: How to build universal foundation models for graph data is a research question
    that has recently garnered significant attention. Despite the powerful representation
    capabilities demonstrated by existing methods such as Graph Neural Networks (GNNs)
    and Graph Transformers, the generalization of machine learning models on graph-structured
    data remains an underexplored open problem [1, 2, 3]. On the one hand, the non-Euclidean
    space and geometric structures involved in graph data significantly increase the
    difficulty of modeling, making it challenging for existing methods aimed at enhancing
    model generalization to succeed [4, 5, 6]. On the other hand, the distribution
    shift in graph data, i.e., the difference in distribution between training and
    testing data, arises from more complex guiding factors (such as topological structures)
    and external context, making this problem even more challenging to study [7, 8].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如何构建通用的图数据基础模型是一个最近受到广泛关注的研究问题。尽管现有方法如图神经网络（GNNs）和图变换器在表示能力上表现强大，但图结构数据上机器学习模型的泛化能力仍然是一个尚未深入探索的开放问题[1,
    2, 3]。一方面，图数据中涉及的非欧几里得空间和几何结构显著增加了建模的难度，使得现有的旨在增强模型泛化能力的方法难以成功[4, 5, 6]。另一方面，图数据中的分布转移，即训练数据和测试数据之间的分布差异，源于更为复杂的引导因素（如拓扑结构）和外部环境，使得这个问题更加难以研究[7,
    8]。
- en: '![](../Images/ba1c3e0d24e963ef3fdbe2526ec318ce.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba1c3e0d24e963ef3fdbe2526ec318ce.png)'
- en: The generalization challenge aims at handling distribution shifts from training
    to testing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化挑战旨在处理从训练到测试的分布转移。
- en: Problem and Motivation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与动机
- en: Distribution Shifts in An Open World
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放世界中的分布转移
- en: The issue of generalization is crucial because models in real-world scenarios
    often need to interact with an open, dynamic, and complex environment. In practical
    situations, due to limited observation and resources, training data cannot encompass
    all possible environments, and the model cannot foresee all potential future circumstances
    during the training process. At the testing stage, however, the model is likely
    to encounter samples that are not aligned with the training distribution. *The
    key focus of the out-of-distribution generalization (OOD) problem targets how
    machine learning models perform on test data outside the training distribution.*
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化问题至关重要，因为在现实场景中，模型通常需要与一个开放、动态且复杂的环境进行交互。在实际情况下，由于观察和资源的限制，训练数据无法涵盖所有可能的环境，模型也无法在训练过程中预见到所有潜在的未来情形。然而，在测试阶段，模型很可能会遇到与训练分布不一致的样本。*分布外泛化（OOD）问题的关键焦点是如何使机器学习模型在测试数据上表现良好，即使这些测试数据超出了训练分布。*
- en: '![](../Images/22a358ef7dbb045f049ced6606b78efc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22a358ef7dbb045f049ced6606b78efc.png)'
- en: 'Typical scenarios involving distribution shifts on graphs require machine learning
    models to generalize from limited training data to new test distributions. Images
    from Medium blogs: [Temporal Graph Networks](/temporal-graph-networks-ab8f327f2efe)
    and [Advective Diffusion Transformers](https://medium.com/towards-data-science/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及图数据分布转移的典型场景要求机器学习模型从有限的训练数据泛化到新的测试分布。来自Medium博客的图片：[时序图网络](/temporal-graph-networks-ab8f327f2efe)
    和 [对流扩散变换器](https://medium.com/towards-data-science/topological-generalisation-with-advective-diffusion-transformers-70f263a5fec7)
- en: In this setting, since the test data/distribution is strictly unseen/unknown
    during the training process, structural assumptions about the data generation
    are necessarily required as a premise. Conversely, without any data assumptions,
    out-of-distribution generalization is impossible (no-free lunch theorem). Therefore,
    it is important to clarify upfront that the research goal of the OOD problem is
    *not* to eliminate all assumptions *but* to 1) maximize the model’s generalization
    ability under reasonable assumptions, and 2) properly add/reduce assumptions to
    ensure the model’s capability to handle certain distribution shifts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于测试数据/分布在训练过程中是严格未见/未知的，因此关于数据生成的结构假设是必要的前提。相反，如果没有任何数据假设，分布外泛化将是不可能的（无免费午餐定理）。因此，必须事先明确指出，OOD问题的研究目标*不是*消除所有假设，*而是*
    1）在合理的假设下最大化模型的泛化能力，2）适当增加/减少假设，以确保模型能够处理某些分布转移。
- en: Out-of-Distribution Generalization on Graphs
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图上的分布外泛化
- en: 'The general out-of-distribution (OOD) problem can be simply described as:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的分布外（OOD）问题可以简单地描述为：
- en: How to design effective machine learning methods when p(x,y|train)≠p(x,y|test)?
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当 p(x,y|train)≠p(x,y|test) 时，如何设计有效的机器学习方法？
- en: Here, we follow the commonly used setting in the literature, assuming that the
    data distribution is controlled by an underlying environment. Thus, under a given
    environment e, the data generation can be written as (x,y)∼p(x,y|e). Then for
    the OOD problem, training and test data can be assumed to be generated from different
    environments. Consequently, the problem can be further elaborated as
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遵循文献中常用的设定，假设数据分布由潜在的环境控制。因此，在给定环境 e 下，数据生成可以写作 (x,y)∼p(x,y|e)。对于OOD问题，训练和测试数据可以假定来自不同的环境。因此，这个问题可以进一步阐述为
- en: How to learn a predictor model f such that it performs (equally) well across
    all environments e∈E?
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何学习一个预测模型 f，使其在所有环境 e∈E 中表现（同样）良好？
- en: 'Specifically, for graph-structured data, the input data also contains structural
    information. In this regard, depending on the form in which graph structures exist,
    the problem can be further categorized into two types: node-level tasks and graph-level
    tasks. The following figure presents the formulation of the OOD problem under
    the two types of tasks.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于图结构数据，输入数据还包含结构信息。在这方面，根据图结构存在的形式，问题可以进一步分为两类：节点级任务和图级任务。下图展示了在这两种任务类型下OOD问题的公式化。
- en: '![](../Images/41cb47be812bc54b61c448f24c059046.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41cb47be812bc54b61c448f24c059046.png)'
- en: The formulation of OOD generalization on graphs, where we further distinguish
    between graph-level and node-level tasks which vary in the form of graph structures.
    Specifically, for node-level tasks, due to the inter-dependence introduced by
    the graph structures among node instances, [5] proposes to divide a whole graph
    into node-centered ego-graphs that can be considered as independent inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的OOD泛化形式，其中我们进一步区分了图级任务和节点级任务，这些任务在图结构的形式上有所不同。具体来说，对于节点级任务，由于图结构引入了节点实例之间的相互依赖关系，[5]
    提出将整个图分割为以节点为中心的自我图（ego-graphs），这些自我图可以被视为独立的输入。
- en: As previously mentioned, the OOD problem requires certain assumptions about
    data generation which pave the way for building generalizable machine learning
    methods. Below, we will specifically introduce two classes of methods that utilize
    the invariance principle and causal intervention, respectively, to achieve out-of-distribution
    generalization on graphs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，OOD问题需要关于数据生成的某些假设，这些假设为构建具有泛化能力的机器学习方法铺平了道路。接下来，我们将具体介绍两类方法，分别利用不变性原理和因果干预来实现图上的分布外泛化。
- en: Generalization by Invariance Principle
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过不变性原理的泛化
- en: Learning methods based on the invariance principle, often referred to as *invariant
    learning* [9, 10, 11], aim to design new learning algorithms that guide machine
    learning models to leverage the *invariant relations* in data. Invariant relations
    particularly refer to the predictive relations from input x and label y that universally
    hold across all environments. Therefore, when a predictor model f (e.g., a neural
    network) successfully learns such invariant relations, it can generalize across
    data from different environments. On the contrary, if the model learns *spurious
    correlations*, which particularly refer to the predictive relations from x and
    y that hold only in some environments, then excessively improving training accuracy
    would mislead the predictor to overfit the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于不变性原理的学习方法，通常被称为*不变学习* [9, 10, 11]，旨在设计新的学习算法，引导机器学习模型利用数据中的*不变关系*。不变关系特别指的是从输入
    x 和标签 y 中获得的预测关系，这些关系在所有环境中普遍适用。因此，当预测模型 f（例如神经网络）成功学习到这些不变关系时，它能够跨不同环境中的数据进行泛化。相反，如果模型学习到的是*虚假的相关性*，即仅在某些环境中成立的从
    x 和 y 获得的预测关系，那么过度提高训练准确性将误导预测模型过拟合数据。
- en: 'In light of the above illustration, we notice that invariant learning relies
    on the *invariant assumption* in data generation, i.e., there exists a predictive
    relation between x and y that remains invariant across different environments.
    Mathematically, this can be formulated as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述说明，我们注意到不变学习依赖于数据生成中的*不变假设*，即在不同环境中，x 和 y 之间存在一个保持不变的预测关系。数学上，这可以被公式化为：
- en: There exists a mapping c such that z=c(x) satisfies p(y|z,e)=p(y|z), ∀e∈E.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 存在一个映射 c，使得 z=c(x) 满足 p(y|z,e)=p(y|z)，∀e∈E。
- en: 'In this regard, we naturally have two follow-up questions: i) how can the invariant
    assumption be defined on graphs? and ii) is this a reasonable assumption for common
    graph data?'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，我们自然有两个后续问题：i) 如何在图上定义不变假设？ii) 对于常见的图数据，这个假设是否合理？
- en: 'We next introduce the recent paper [5], *Wu et al., “*[*Handling Distribution
    Shifts on Graphs: An Invariance Perspective*](https://arxiv.org/pdf/2202.02466)*”
    (ICLR2022)*. This paper proposes applying the invariance principle to out-of-distribution
    generalization on graphs and poses the invariance assumption for graph data.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍最近的论文[5]，*吴等人，"*[*处理图上的分布变化：一种不变性视角*](https://arxiv.org/pdf/2202.02466)*”（ICLR2022）*。该论文提出将不变性原则应用于图上的分布外泛化，并提出了图数据的不变假设。
- en: Invariant Assumption on Graphs
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图上的不变假设
- en: Inspired by the Weisfeiler-Lehman algorithm for graph isomorphism testing, [5]
    considers ego-graphs centered on each node and characterizes the contributions
    of all the nodes’ features within the ego-graph to the label of the central node.
    The latter is specifically decomposed into *invariant features* and *spurious
    features*. This definition accommodates the topological structures and also allows
    enough flexibility. The following figure illustrates the invariant assumption
    as defined in [5] and provides an example of a citation network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 受图同构测试中的Weisfeiler-Lehman算法启发，[5]考虑了以每个节点为中心的自我图，并描述了所有节点特征在自我图中对中心节点标签的贡献。后者被具体分解为*不变特征*和*虚假特征*。这一定义兼顾了拓扑结构，同时也允许足够的灵活性。下图展示了[5]中定义的不变假设，并提供了一个引用网络的示例。
- en: '![](../Images/d61ef3d9b14b6f635324bc8bca0152a8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d61ef3d9b14b6f635324bc8bca0152a8.png)'
- en: The invariant assumption on graphs (left) and an example of a citation network
    (right). In the citation network, each node represents a paper, and the label
    y to be predicted is the research field of the paper. The node features x include
    the paper’s published venue (x1) and its citation index (x2), with the environment
    (e) being the publication time. In this example, x1 is an invariant feature because
    its relationship with y is independent of the environment. Conversely, x2 is a
    spurious feature; although it is strongly correlated with y, this correlation
    changes over time. Therefore, in this case, an ideal predictor should utilize
    the information in x1 to achieve generalization across different environments.
    Image from [the paper](https://arxiv.org/pdf/2202.02466).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的不变假设（左）和引用网络的示例（右）。在引用网络中，每个节点代表一篇论文，待预测的标签y是论文的研究领域。节点特征x包括论文的发表场所（x1）和引用指数（x2），环境（e）是发表时间。在这个例子中，x1是一个不变特征，因为它与y的关系与环境无关。相反，x2是一个虚假特征；尽管它与y有很强的相关性，但这种相关性会随时间变化。因此，在这种情况下，一个理想的预测器应该利用x1中的信息，以实现跨不同环境的泛化。图像来自[论文](https://arxiv.org/pdf/2202.02466)。
- en: 'Proposed Method: Explore-to-Extrapolate Risk Minimization'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提出的方法：探索-外推风险最小化
- en: Under the invariance assumption, a natural approach is to regularize the loss
    difference across environments to facilitate learning invariant relations. However,
    real-world data typically lack environment labels, i.e., the correspondence between
    each instance and its environment is unknown, making it impossible to directly
    compute differences in loss across different environments. To address this challenge,
    [5] proposes Exploration-Extrapolation Risk Minimization (EERM), which involves
    introducing K context generators to augment and diversify the input data, thereby
    simulating input data from different environments. Through theoretical analysis,
    [5] proves that the new learning objective can guarantee an optimal solution for
    the formulated out-of-distribution generalization problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在不变假设下，一种自然的方法是对不同环境下的损失差异进行正则化，以促进学习不变关系。然而，现实世界中的数据通常缺乏环境标签，即每个实例与其环境之间的对应关系未知，这使得无法直接计算不同环境间的损失差异。为了解决这个挑战，[5]提出了探索-外推风险最小化（EERM），该方法引入了K个上下文生成器来扩展和多样化输入数据，从而模拟来自不同环境的输入数据。通过理论分析，[5]证明了新的学习目标可以保证在所提出的分布外泛化问题上得到最优解。
- en: '![](../Images/af67d937affec052f992998fa894ac9d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af67d937affec052f992998fa894ac9d.png)'
- en: Explore-to-Extrapolate Risk Minimization (EERM) proposed by [5], where the inner
    objective is to maximize the “diversity” of data generated by K context generators
    and the outer objective involves computing the mean and variance of losses using
    data from the K generated (virtual) environments for training the predictor. Image
    from [the paper](https://arxiv.org/pdf/2202.02466).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[探索-外推风险最小化（EERM）](https://arxiv.org/pdf/2202.02466)由[5]提出，其中内在目标是最大化由K个上下文生成器生成的数据的“多样性”，外在目标则涉及使用来自K个生成的（虚拟）环境的数据来计算损失的均值和方差，用于训练预测模型。'
- en: Apart from generating (virtual) environments, another recent study [12] proposes
    inferring latent environments from observed data and introduces an additional
    model for environment inference, iteratively optimizing it alongside the predictor
    during training. Meanwhile, [13] approaches OOD generalization with data augmentation,
    using the invariance principle to guide the data augmentation process that preserves
    invariant features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了生成（虚拟）环境外，最近的另一项研究[12]提出从观察数据推断潜在环境，并引入了一个额外的环境推断模型，在训练过程中与预测器一起迭代优化。同时，[13]通过数据增强处理分布外的泛化，使用不变性原理来指导数据增强过程，从而保留不变的特征。
- en: Generalization by Causal Intervention
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 因果干预下的泛化
- en: Invariant learning requires assuming the existence of invariant relations in
    data that can be learned. This to some extent limits the applicability of such
    methods, as the model can only generalize reliably on test data that shares certain
    invariance with training data. For out-of-distribution test data that violates
    this condition, the model’s generalization performance remains unknown.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 不变学习要求假设数据中存在可以学习的不变关系。这在一定程度上限制了此类方法的适用性，因为模型只能在与训练数据共享某些不变性的测试数据上可靠地泛化。对于违反此条件的分布外测试数据，模型的泛化性能仍然未知。
- en: Next, we introduce another approach proposed by recent work [14], *Wu et al.,
    “*[*Graph Out-of-Distribution Generalization via Causal Intervention*](https://arxiv.org/pdf/2402.11494)*”
    (WWW2024)*. This paper aims to tackle out-of-distribution generalization through
    the lens of causal intervention. Unlike invariant learning, this approach does
    not rely on the invariant assumption in data generation. Instead, it guides the
    model to learn causality from x to y through the learning algorithm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍最近的工作[14]提出的另一种方法，*Wu等人，“*[*通过因果干预实现图的分布外泛化*](https://arxiv.org/pdf/2402.11494)*”
    (WWW2024)*。本文旨在通过因果干预的视角解决分布外泛化问题。与不变学习不同，该方法不依赖于数据生成中的不变假设。相反，它通过学习算法引导模型从x到y学习因果关系。
- en: A Causal Perspective for Graph Learning
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图学习的因果视角
- en: Firstly, let us consider the causal dependency among variables typically induced
    by machine learning models such as graph neural networks. We have the input G
    (e.g., ego-graphs centered on each node in a graph), the label Y, and the environment
    E influencing the data distribution. After training with the standard supervised
    learning objective (e.g., empirical risk minimization or equivalently, maximum
    likelihood estimation), their dependencies are illustrated in the diagram below.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑由机器学习模型（如图神经网络）通常引起的变量之间的因果依赖关系。我们有输入G（例如，图中每个节点中心的自我图），标签Y，以及影响数据分布的环境E。在使用标准监督学习目标（例如，经验风险最小化或等效的最大似然估计）进行训练后，它们之间的依赖关系在下图中展示。
- en: '![](../Images/b8939081e9596e8c38629ad55d204ae7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8939081e9596e8c38629ad55d204ae7.png)'
- en: 'In the causal graph, there are three dependence paths: i) from G to Y, induced
    by the predictor; ii) from E to G, given by definition of data generation; iii)
    from E to Y, led by the model training.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在因果图中，有三条依赖路径：i) 由预测器引起的从G到Y的路径；ii) 由数据生成定义给出的从E到G的路径；iii) 由模型训练引导的从E到Y的路径。
- en: The causal graph above reveals the limitation of traditional training methods,
    specifically their inability to achieve out-of-distribution generalization. Here,
    both the input G and the label Y are outcomes of the environment E, suggesting
    that they are correlated due to this *confounder*. During training, the model
    continuously fits the training data, causing the predictor f to learn the spurious
    correlation between inputs and labels specific to a particular environment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的因果图揭示了传统训练方法的局限性，特别是它们无法实现分布外的泛化。在这里，输入G和标签Y是环境E的结果，暗示它们由于这个*混杂因素*而相关。在训练过程中，模型不断拟合训练数据，导致预测器f学习到输入和标签之间的虚假关联，这些关联是特定于某一环境的。
- en: '[14] introduces an example of a social network to illustrate this learning
    process. Suppose we need to predict the interests of users (nodes) in a social
    network, where notice that user interests are significantly influenced by factors
    such as age and social circles. Therefore, if a predictor is trained on data from
    a university social network, it might easily predict a user’s interest in “basketball”
    because within a university environment, there is a higher proportion of users
    interested in basketball due to the environment itself. However, this predictive
    relation may not hold when the model is transferred to LinkedIn’s social network,
    where user ages and interests are more diverse. This example highlights that an
    ideal model needs to learn the causal relations between inputs and labels to generalize
    across different environments.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[14]通过一个社交网络示例来说明这一学习过程。假设我们需要预测社交网络中用户（节点）的兴趣，其中注意到用户兴趣受到年龄和社交圈等因素的显著影响。因此，如果一个预测模型在大学社交网络的数据上训练，它可能很容易预测用户对“篮球”的兴趣，因为在大学环境中，由于环境本身的影响，用户对篮球的兴趣比例较高。然而，当模型转移到LinkedIn社交网络时，这种预测关系可能不再成立，因为LinkedIn的用户年龄和兴趣更加多样化。这个例子突显了一个理想的模型需要学习输入和标签之间的因果关系，从而能够在不同的环境中进行泛化。'
- en: To this end, a common approach is *causal intervention*, which involves cutting
    off the dependence path between E and G in the causal graph. This is achieved
    by disrupting how the environment influences the inputs and labels, thereby guiding
    the model to learn causality. The diagram below illustrates this approach. In
    causal inference terminology [15], such interventions, aimed at removing dependence
    paths to a specific variable, can be represented using the do-operator. Therefore,
    if we aim to enforce cutting off the dependence path between E and G during training,
    it effectively means replacing the traditional optimization objective p(Y|G) (the
    likelihood of observed data) with p(Y|do(G)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，一种常见的方法是*因果干预*，即切断因果图中E与G之间的依赖路径。这是通过干扰环境如何影响输入和标签，从而引导模型学习因果关系来实现的。下图展示了这种方法。在因果推断术语[15]中，旨在移除到特定变量的依赖路径的干预，可以使用do-算子表示。因此，如果我们在训练过程中旨在强制切断E与G之间的依赖路径，这实际上意味着将传统的优化目标p(Y|G)（观察数据的似然）替换为p(Y|do(G))。
- en: '![](../Images/ab6865bfb02c094f00308f3fc481fe28.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab6865bfb02c094f00308f3fc481fe28.png)'
- en: The learning objective based on causal intervention. As one step further, utilizing
    the backdoor adjustment from causal inference [15], we can derive the explicit
    form of the objective from the causal graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于因果干预的学习目标。进一步地，利用因果推断中的反向调整[15]，我们可以从因果图中推导出目标的显式形式。
- en: However, computing this learning objective requires observed environment information
    in data, specifically the correspondence between each sample G and its environment
    E. In practice, however, environments are often unobservable.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算这个学习目标需要数据中观察到的环境信息，具体来说是每个样本G与其环境E之间的对应关系。然而，在实际中，环境往往是不可观察的。
- en: 'Proposed Method: Variational Context Adjustment'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提出的方法：变分上下文调整
- en: To make the above approach feasible, [14] derives a variational lower bound
    for the causal intervention objective, using a data-driven approach that infers
    the latent environments from data to address the issue of unobservable environments.
    Particularly, [14] introduces a variational distribution q(E|G), resulting in
    a surrogate learning objective depicted in the following figure.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使上述方法可行，[14]推导了因果干预目标的变分下界，采用了一种数据驱动的方法，通过从数据中推断潜在环境来解决不可观察环境的问题。特别地，[14]引入了变分分布q(E|G)，从而得出了下图所示的替代学习目标。
- en: '![](../Images/0e36ba25a7486a54d6723529fb018953.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e36ba25a7486a54d6723529fb018953.png)'
- en: The variational lower bound of the original causal intervention objective and
    the specific instantiations of three terms in the final learning objective proposed
    by [14]. Image from [the paper](https://arxiv.org/pdf/2402.11494).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 原始因果干预目标的变分下界以及[14]中提出的最终学习目标中三个项的具体实例。图像来自[论文](https://arxiv.org/pdf/2402.11494)。
- en: The new learning objective is comprised of three components. [14] instantiates
    them as an environment inference model, a GNN predictor, and a (non-parametric)
    prior distribution of the environment. The first two models contain trainable
    parameters and are jointly optimized during training.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新的学习目标由三个部分组成。[14]将其具体化为一个环境推断模型，一个GNN预测器，和一个（非参数的）环境先验分布。前两个模型包含可训练参数，并在训练过程中共同优化。
- en: To validate the effectiveness of the proposed method, [14] applies the model
    to various real-world graph datasets with distribution shifts. Specifically, because
    the proposed method CaNet does not depend on specific backbone models, [14] uses
    GCN and GAT as the backbone, respectively, and compares the model with state-of-the-art
    OOD methods (including the previously-introduced approach EERM). The table below
    shows some of the experimental results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所提方法的有效性，[14]将该模型应用于多个具有分布偏移的真实世界图数据集。具体而言，由于所提方法CaNet不依赖于特定的主干模型，[14]分别使用GCN和GAT作为主干，并与最先进的OOD方法（包括先前介绍的EERM方法）进行比较。下表显示了部分实验结果。
- en: '![](../Images/74193aab006dc46f879205a144029993.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74193aab006dc46f879205a144029993.png)'
- en: Experimental results of testing Accuracy (resp. ROC-AUC) on Arxiv (resp. Twitch),
    where the distribution shifts are introduced by splitting the data according to
    publication years (resp. subgraphs).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在Arxiv（或Twitch）上测试准确率（或ROC-AUC）的实验结果，其中分布偏移通过根据发表年份（或子图）对数据进行划分引入。
- en: Implicit Assumptions in Causal Intervention
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因果干预中的隐性假设
- en: 'So far, we have introduced the method of causal intervention that shows competitiveness
    for out-of-distribution generalization on graphs. As mentioned earlier in this
    blog, achieving guaranteed generalization requires necessary assumptions about
    how the data is generated. This triggers a natural inquiry: *What assumptions
    does causal intervention require for generalization?* Unlike invariant learning,
    causal intervention does not start from explicit assumptions but instead relies
    on implicit assumptions during modeling and analysis:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了因果干预方法，它在图结构的分布外推理中展现了竞争力。正如之前在本博客中提到的，确保泛化能力需要关于数据如何生成的必要假设。这引发了一个自然的问题：*因果干预在泛化中需要什么假设？*
    与不变学习不同，因果干预并不是从明确假设开始，而是依赖于建模和分析过程中的隐性假设：
- en: There exists only one confounding factor (the environment) between the inputs
    and the labels.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在输入和标签之间仅存在一个混杂因素（环境）。
- en: This assumption simplifies the analysis of the real system to some extent but
    introduces approximation errors. For more complex scenarios, there remains significant
    exploration space in the future.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设在一定程度上简化了实际系统的分析，但也引入了近似误差。对于更复杂的场景，未来仍有大量的探索空间。
- en: Generalization with Implicit Graph Structures
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用隐式图结构进行泛化
- en: In the previous discussion, we assumed that the structural information of input
    data is observed and complete. For more general graph data, structural information
    may be partially observed or even completely unknown. Such data is referred to
    as implicit graph structures. Moreover, distribution shifts on graphs may involve
    underlying structures that impact data distribution, posing unresolved challenges
    in characterizing the influence of geometry on data distribution.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的讨论中，我们假设输入数据的结构信息是可观察和完整的。对于更一般的图数据，结构信息可能是部分可观察的，甚至完全未知。此类数据称为隐式图结构。此外，图上的分布偏移可能涉及影响数据分布的潜在结构，从而带来了在表征几何对数据分布影响时未解决的挑战。
- en: To address this, recent work [16], *Wu et al., “*[*Learning Divergence Fields
    for Shift-Robust Graph Representations*](https://arxiv.org/pdf/2406.04963)*” (ICML2024)*,
    leverages the inherent connection between continuous diffusion equations and message
    passing mechanisms, integrating the causal intervention approach introduced earlier.
    This design aims to develop a learning method that is applicable for both explicit
    and implicit graph structures where distribution shifts pose the generalization
    challenge.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，最近的研究[16]，*吴等，“*[*为抗偏移鲁棒图表示学习发散场*](https://arxiv.org/pdf/2406.04963)*”（ICML2024）*，利用了连续扩散方程与消息传递机制之间的固有联系，整合了前述的因果干预方法。该设计旨在开发一种适用于显式和隐式图结构的学习方法，在这些结构中，分布偏移会构成泛化挑战。
- en: From Message Passing to Diffusion Equations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从消息传递到扩散方程
- en: Message Passing mechanism serves as a foundational design in modern graph neural
    networks and graph Transformers, propagating information from other nodes in each
    layer to update the representation of the central node. Essentially, if we view
    the layers of a neural network as discretized approximations of continuous time,
    then message passing can be seen as a discrete form of diffusion process on graphs
    [17, 18]. The following diagram illustrates their analogy. (We refer the readers
    interested in more details along this line to [recent blogs by Prof. Michael Bronstein
    et al.](https://michael-bronstein.medium.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递机制作为现代图神经网络和图 Transformer 的基础设计，通过每一层将信息从其他节点传播到中心节点，从而更新其表示。本质上，如果我们将神经网络的层视为连续时间的离散化近似，那么消息传递可以被看作图上的扩散过程的离散形式[17,
    18]。下图说明了它们的类比。（有兴趣深入了解此方面的读者可以参考[Prof. Michael Bronstein 等人的最新博客](https://michael-bronstein.medium.com/graph-neural-networks-as-neural-diffusion-pdes-8571b8c0c774)）。
- en: '![](../Images/18aaa6a57b7e1d69b6bfa3900007361f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18aaa6a57b7e1d69b6bfa3900007361f.png)'
- en: 'Message passing (the inter-layer updates in GNNs and Transformers) can be viewed
    as discrete iterations of a continuous diffusion equation through the analogy:
    nodes in the graph are mapped to locations on a manifold, node embeddings are
    represented by heat signals, layer-wise updates of embeddings correspond to changes
    in heat signals over time, and interactions between nodes in each layer are reflected
    by interactions between positions on the manifold.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 消息传递（GNNs 和 Transformers 中的层间更新）可以通过以下类比看作是连续扩散方程的离散迭代：图中的节点被映射到流形上的位置，节点嵌入由热信号表示，嵌入的层次更新对应于热信号随时间的变化，每层中节点之间的交互反映了流形上位置之间的交互。
- en: Particularly, the diffusivity (denoted by d_u) in the diffusion equation controls
    the interactions between nodes during the diffusion process. When adopting local
    or global diffusion forms, the discrete iterations of the diffusion equation respectively
    lead to the layer-wise update formulas of Graph Neural Networks [18] and Transformers
    [19].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，扩散方程中的扩散率（记作 d_u）控制了扩散过程中节点之间的交互。当采用局部或全局扩散形式时，扩散方程的离散迭代分别导致了图神经网络[18]和
    Transformer[19]的层更新公式。
- en: However, the deterministic diffusivity cannot model the multi-faceted effects
    and uncertainties in interactions between instances. Therefore, [16] proposes
    defining the diffusivity as a random sample from a probability distribution. The
    corresponding diffusion equation will yield a stochastic trajectory (as shown
    in the figure below).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确定性扩散率无法模拟实例之间交互中的多方面效应和不确定性。因此，[16] 提出了将扩散率定义为概率分布中的随机样本。相应的扩散方程将产生一个随机轨迹（如下图所示）。
- en: '![](../Images/5a55342e17812d86519433c30936b880.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a55342e17812d86519433c30936b880.png)'
- en: After defining the diffusivity d_u as a random variable, the divergence field
    of the diffusion equation at each time (i.e., the change in node embeddings at
    the current layer) will become stochastic. This enables modeling the uncertainty
    in interactions between nodes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在将扩散率 d_u 定义为随机变量之后，扩散方程在每个时间点的散度场（即当前层中节点嵌入的变化）将变为随机的。这使得能够对节点之间的交互中的不确定性进行建模。
- en: Even so, if the traditional supervised learning objective is directly applied
    for training, the model described above can not generalize well with distribution
    shifts. This issue is echoed by the causal perspective of graph learning discussed
    earlier. Specifically, in the diffusion models considered here, the input x (such
    as a graph) and the output y (such as node labels in the graph) are associated
    by diffusivity. The diffusivity can be seen as an embodiment of the environment
    specific to the dataset, determining the interdependencies among instances. Therefore,
    the model trained on limited training data tends to learn specific interdependent
    patterns specific to the training set, making it unable to generalize to new test
    data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，如果直接应用传统的监督学习目标进行训练，上述模型在分布变化的情况下不能很好地泛化。这个问题与之前讨论的图学习的因果视角相呼应。具体而言，在此处考虑的扩散模型中，输入
    x（例如图）和输出 y（例如图中的节点标签）通过扩散率关联。扩散率可以看作是特定数据集环境的体现，决定了实例之间的相互依赖关系。因此，训练数据有限的模型往往会学习到特定于训练集的相互依赖模式，从而无法在新的测试数据上泛化。
- en: Causality-guided Divergence Field Learning
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 因果引导的散度场学习
- en: To address this challenge, we once again employ causal intervention to eliminate
    the dependency between the diffusivity d and the input x during training. Unlike
    previous work [14] where the mapping from input to output was given by a predictor,
    here the dependence path from x to y involves a multi-step diffusion process (corresponding
    to multiple layers of updates in GNNs/Transformers). Therefore, causal intervention
    is needed at each step of the diffusion process. However, since the diffusivity
    is an abstract notion for modeling and cannot be directly observed (similar to
    the environment discussed earlier), [16] extends the variational approach used
    in [14] to derive a variational lower bound for the learning objective pertaining
    to the diffusion process. This serves as an approximate objective for causal intervention
    at each step of the diffusion process.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，我们再次采用因果干预，在训练过程中消除扩散性d与输入x之间的依赖关系。与之前的工作[14]不同，其中输入到输出的映射是由预测器给出的，这里从x到y的依赖路径涉及一个多步骤的扩散过程（对应于GNNs/Transformers中的多层更新）。因此，每一步的扩散过程中都需要因果干预。然而，由于扩散性是一个抽象的建模概念，无法直接观察（类似于前面讨论的环境），[16]扩展了[14]中使用的变分方法，推导出扩散过程学习目标的变分下界。这作为每一步扩散过程中的因果干预的近似目标。
- en: '![](../Images/adc9aad2a68adae832796aeea7619a98.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adc9aad2a68adae832796aeea7619a98.png)'
- en: The learning approach proposed in [16] estimates the diffusivity for each step
    of the diffusion model and applies causal intervention. This approach guides the
    model to learn stable causal relations from inputs to outputs, thereby enhancing
    its ability to generalize under distribution shifts. Image from [the paper](https://arxiv.org/pdf/2406.04963).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[16]中提出的学习方法估计了扩散模型每一步的扩散性，并应用因果干预。该方法引导模型学习从输入到输出的稳定因果关系，从而增强其在分布偏移下的泛化能力。图片来自[论文](https://arxiv.org/pdf/2406.04963)。'
- en: 'As an implementation of the aforementioned method, [16] introduces three specific
    model designs:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作为上述方法的实现，[16]引入了三种特定的模型设计：
- en: '**GLIND-GCN**: Considers the diffusivity as a constant matrix instantiated
    by the normalized graph adjacency matrix;'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GLIND-GCN**：将扩散性视为通过标准化图邻接矩阵实例化的常量矩阵；'
- en: '**GLIND-GAT**: Considers the diffusivity as a time-dependent matrix implemented
    by graph attention networks;'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GLIND-GAT**：将扩散性视为通过图注意力网络实现的时间依赖矩阵；'
- en: '**GLIND-Trans**: Considers the diffusivity as a time-dependent matrix implemented
    by global all-pair attention networks.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GLIND-Trans**：将扩散性视为通过全局所有对注意力网络实现的时间依赖矩阵。'
- en: Particularly, for GLIND-Trans, to address the quadratic complexity issue in
    global attention computations, [16] further adopts the linear attention function
    design from DIFFormer [19]. (We also refer the readers interested in how to achieve
    linear complexity for all-pair attentions to this [Blog](https://medium.com/towards-data-science/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，对于**GLIND-Trans**，为了解决全局注意力计算中的二次复杂度问题，[16]进一步采用了DIFFormer [19]中的线性注意力函数设计。（我们还建议对如何实现所有对注意力的线性复杂度感兴趣的读者参阅此[博客](https://medium.com/towards-data-science/how-to-build-graph-transformers-with-o-n-complexity-d507e103d30a)）。
- en: The table below presents partial experimental results in scenarios involving
    implicit structures.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了涉及隐式结构场景中的部分实验结果。
- en: '![](../Images/79cdd9decfaf711c8f75a1ee1572cb71.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79cdd9decfaf711c8f75a1ee1572cb71.png)'
- en: Experimental results of testing Accuracy on CIFAR and STL, where the original
    datasets contain no structural information and we use k-nearest-neighbor to construct
    graphs. Furthermore, for CIFAR and STL, we introduce distribution shifts by adding
    rotation angles (that change the similarity function for k-nearest-neighbor) and
    using different k, respectively.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在CIFAR和STL上的测试准确率实验结果，其中原始数据集不包含结构信息，我们使用k近邻方法构建图。此外，对于CIFAR和STL，我们通过添加旋转角度（改变k近邻的相似度函数）和使用不同的k，分别引入了分布偏移。
- en: Summary and Discussion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与讨论
- en: This blog briefly introduces recent advances in out-of-distribution (OOD) generalization,
    focusing primarily on three published papers [5, 14, 16]. These works approach
    the problem from the perspectives of invariant learning and causal intervention,
    proposing methods applicable to both explicit and implicit graph structures. As
    mentioned earlier, we note that OOD problems require assumptions about the data
    generation as a prerequisite for effective solutions. Based on this, future research
    could focus on refining existing methods or analyzing the limits of generalization
    under the well-established assumptions. It could also explore how to achieve generalization
    under other assumption conditions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本文简要介绍了分布外（OOD）泛化的最新进展，主要聚焦于三篇已发表的论文[5, 14, 16]。这些工作从不变学习和因果干预的角度处理该问题，提出了适用于显式和隐式图结构的方法。如前所述，我们指出，OOD问题需要在数据生成假设的前提下才能有效解决。基于此，未来的研究可以集中于完善现有方法或分析在已建立的假设条件下的泛化极限，也可以探索在其他假设条件下如何实现泛化。
- en: Another challenge closely related to OOD generalization is *Out-of-Distribution
    Detection* [20, 21, 22]. Unlike OOD generalization, OOD detection aims to investigate
    how to equip models during training to recognize out-of-distribution samples appearing
    during the testing phase. Future research could also focus on extending the methods
    in this blog to OOD detection or exploring the intersection of these two problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与OOD泛化密切相关的挑战是*分布外检测* [20, 21, 22]。与OOD泛化不同，OOD检测的目的是研究如何在训练过程中使模型具备识别测试阶段出现的分布外样本的能力。未来的研究还可以集中在将博客中提到的方法扩展到OOD检测，或者探索这两个问题的交集。
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Garg et al., Generalization and Representational Limits of Graph Neural
    Networks, ICLR 2020.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Garg等人，图神经网络的泛化与表示限制，ICLR 2020。'
- en: '[2] Koh et al., WILDS: A Benchmark of in-the-Wild Distribution Shifts, ICML
    2021'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Koh等人，WILDS：野外分布变化基准，ICML 2021。'
- en: '[3] Morris et al., Position: Future Directions in the Theory of Graph Machine
    Learning, ICML 2024.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Morris等人，Position：图机器学习理论的未来方向，ICML 2024。'
- en: '[4] Zhu et al., Shift-Robust GNNs: Overcoming the Limitations of Localized
    Graph Training Data, NeurIPS 2021.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Zhu等人，抗变化的GNN：克服局部图训练数据的局限性，NeurIPS 2021。'
- en: '[5] Wu et al., Handling Distribution Shifts on Graphs: An Invariance Perspective,
    ICLR 2022.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Wu等人，在图上处理分布变化：一种不变性视角，ICLR 2022。'
- en: '[6] Li et al., OOD-GNN: Out-of-Distribution Generalized Graph Neural Network,
    TKDE 2022.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Li等人，OOD-GNN：分布外广义图神经网络，TKDE 2022。'
- en: '[7] Yehudai et al., From Local Structures to Size Generalization in Graph Neural
    Networks, ICML 2021.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Yehudai等人，从局部结构到图神经网络中的规模泛化，ICML 2021。'
- en: '[8] Li et al., Size Generalization of Graph Neural Networks on Biological Data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Li等人，生物数据上的图神经网络规模泛化：'
- en: Insights and Practices from the Spectral Perspective, Arxiv 2024.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 来自谱学视角的洞察与实践，Arxiv 2024。
- en: '[9] Arjovsky, et al., Invariant Risk Minimization, Arxiv 2019.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Arjovsky等人，不变风险最小化，Arxiv 2019。'
- en: '[10] Rojas-Carulla, et al., Invariant Models for Causal Transfer Learning,
    JMLR 2018.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Rojas-Carulla等人，因果迁移学习的不变模型，JMLR 2018。'
- en: '[11] Krueger et al., Out-of-Distribution Generalization via Risk Extrapolation,
    ICML 2021.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Krueger等人，通过风险外推进行分布外泛化，ICML 2021。'
- en: '[12] Yang et al., Learning Substructure Invariance for Out-of-Distribution
    Molecular Representations, NeurIPS 2022.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Yang等人，学习分布外分子表示的子结构不变性，NeurIPS 2022。'
- en: '[13] Sui et al., Unleashing the Power of Graph Data Augmentation on Covariate
    Distribution Shift, NeurIPS 2023.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Sui等人，释放图数据增强在协变量分布变化中的力量，NeurIPS 2023。'
- en: '[14] Wu et al., Graph Out-of-Distribution Generalization via Causal Intervention,
    WWW 2024.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Wu等人，通过因果干预进行图的分布外泛化，WWW 2024。'
- en: '[15] Pearl et al., Causal Inference in Statistics: A Primer, 2016.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Pearl等人，统计学中的因果推断：入门，2016。'
- en: '[16] Wu et al., Learning Divergence Fields for Shift-Robust Graph Representations,
    ICML 2024.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] Wu等人，学习用于应对变化的图表示的发散场，ICML 2024。'
- en: '[17] Freidlin et al., Diffusion Processes on Graphs and the Averaging Principle,
    The Annals of probability 1993.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Freidlin等人，图上的扩散过程与平均化原理，概率年鉴 1993。'
- en: '[18] Chamberlain et al., GRAND: Graph Neural Diffusion, ICML 2021.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[18] Chamberlain等人，GRAND：图神经扩散，ICML 2021。'
- en: '[19] Wu et al., DIFFormer: Scalable (Graph) Transformers Induced by'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[19] Wu等人，DIFFormer：由能量约束扩散生成的可扩展（图）变换器，ICLR 2023。'
- en: Energy Constrained Diffusion, ICLR 2023.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 能量约束扩散，ICLR 2023。
- en: '[20] Wu et al., Energy-based Out-of-Distribution Detection for Graph Neural
    Networks, ICLR 2023.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[20] Wu 等人，基于能量的图神经网络图外分布检测，ICLR 2023。'
- en: '[21] Liu et al., GOOD-D: On Unsupervised Graph Out-Of-Distribution Detection,
    WSDM 2023.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[21] Liu 等人，GOOD-D：无监督图外分布检测，WSDM 2023。'
- en: '[22] Bao et al., Graph Out-of-Distribution Detection Goes Neighborhood Shaping,
    ICML 2024.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[22] Bao 等人，图神经网络的图外分布检测通过邻域塑形，ICML 2024。'
