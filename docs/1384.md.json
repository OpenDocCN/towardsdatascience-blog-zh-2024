["```py\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\n\n# from https://github.com/kyegomez/zeta\nclass SimpleRMSNorm(nn.Module):\n   \"\"\"\n   SimpleRMSNorm\n\n   Args:\n       dim (int): dimension of the embedding\n\n   Usage:\n   We can use SimpleRMSNorm as a layer in a neural network as follows:\n       >>> x = torch.randn(1, 10, 512)\n       >>> simple_rms_norm = SimpleRMSNorm(dim=512)\n       >>> simple_rms_norm(x).shape\n       torch.Size([1, 10, 512])\n\n   \"\"\"\n\n   def __init__(self, dim):\n       super().__init__()\n       self.scale = dim**-0.5\n\n   def forward(self, x):\n       \"\"\"Forward method of SimpleRMSNorm\"\"\"\n       return F.normalize(x, dim=-1) * self.scale\n\ndef activation_quant(x: Tensor):\n   \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n\n   Args:\n       x (Tensor): _description_\n\n   Returns:\n       _type_: _description_\n   \"\"\"\n   scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n   y = (x * scale).round().clamp_(-128, 127) / scale\n   return y\n\ndef weight_quant(w: Tensor):\n   scale = w.abs().mean()\n   e = w.mean()\n   u = (w - e).sign() * scale\n   return u\n\nclass BitLinear(nn.Linear):\n   \"\"\"\n   Custom linear layer with bit quantization.\n\n   Args:\n       dim (int): The input dimension of the layer.\n       training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n       *args: Variable length argument list.\n       **kwargs: Arbitrary keyword arguments.\n\n   Attributes:\n       dim (int): The input dimension of the layer.\n\n   \"\"\"\n\n   def forward(self, x: Tensor) -> Tensor:\n       \"\"\"\n       Forward pass of the BitLinear layer.\n\n       Args:\n           x (Tensor): The input tensor.\n\n       Returns:\n           Tensor: The output tensor.\n       \"\"\"\n       w = self.weight\n       x_norm = SimpleRMSNorm(self.in_features)(x)\n\n       # STE using detach\n       # the gradient of sign() or round() is typically zero\n       # so to train the model we need to do the following trick\n       # this trick leads to \"w\" high precision weights update \n       # while we are doing \"fake\" quantisation during the forward pass\n       x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n       w_quant = w + (weight_quant(w) - w).detach()\n       y = F.linear(x_quant, w_quant)\n       return y\n```", "```py\n def forward(self, x: Tensor) -> Tensor:\n       \"\"\"\n       Forward pass of the BitLinear layer.\n\n       Args:\n           x (Tensor): The input tensor.\n\n       Returns:\n           Tensor: The output tensor.\n       \"\"\"\n       w = self.weight\n       #x_norm = SimpleRMSNorm(self.in_features)(x)\n\n       # STE using detach\n       #x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n       x_quant = x\n       w_quant = w + (weight_quant(w) - w).detach()\n       y = F.linear(x_quant, w_quant)\n       return y\n```", "```py\nfrom peft.tuners.lora.layer import LoraLayer\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nclass BitLoraLayer(LoraLayer):\n   def update_layer(\n       self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights, use_rslora, use_dora: bool = False\n   ):\n       if r <= 0:\n           raise ValueError(f\"`r` should be a positive integer value but the value passed is {r}\")\n\n       self.r[adapter_name] = r\n       self.lora_alpha[adapter_name] = lora_alpha\n       if lora_dropout > 0.0:\n           lora_dropout_layer = nn.Dropout(p=lora_dropout)\n       else:\n           lora_dropout_layer = nn.Identity()\n\n       self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))\n       # Actual trainable parameters\n       # The only update of the original method is here\n       self.lora_A[adapter_name] = BitLinear(self.in_features, r, bias=False)\n       self.lora_B[adapter_name] = BitLinear(r, self.out_features, bias=False)\n\n       if use_rslora:\n           self.scaling[adapter_name] = lora_alpha / math.sqrt(r)\n       else:\n           self.scaling[adapter_name] = lora_alpha / r\n\n       if isinstance(init_lora_weights, str) and init_lora_weights.startswith(\"pissa\"):\n           self.pissa_init(adapter_name, init_lora_weights)\n       elif init_lora_weights == \"loftq\":\n           self.loftq_init(adapter_name)\n       elif init_lora_weights:\n           self.reset_lora_parameters(adapter_name, init_lora_weights)\n\n       # check weight and qweight (for GPTQ)\n       for weight_name in (\"weight\", \"qweight\"):\n           weight = getattr(self.get_base_layer(), weight_name, None)\n           if weight is not None:\n               # the layer is already completely initialized, this is an update\n               if weight.dtype.is_floating_point or weight.dtype.is_complex:\n                   self.to(weight.device, dtype=weight.dtype)\n               else:\n                   self.to(weight.device)\n               break\n\n       if use_dora:\n           self.dora_init(adapter_name)\n           self.use_dora[adapter_name] = True\n       else:\n           self.use_dora[adapter_name] = False\n\n       self.set_adapter(self.active_adapters)\n```", "```py\nimport importlib\n\noriginal = importlib.import_module(\"peft\")\noriginal.tuners.lora.layer.LoraLayer.update_layer = (\n    BitLoraLayer.update_layer\n)\n```", "```py\ndef weight_quant(w: Tensor):\n   scale = w.abs().mean()\n   adjustment = 1e-4 + scale / 2\n   w_quant = w / adjustment\n   return torch.clip(input=torch.round(w_quant), min=-1, max=1)*scale\n```", "```py\ntensor([[-9.4658e-03,  1.8515e-02,  2.4467e-02,  ...,  8.9979e-03]])\n```", "```py\ntensor([[-0.0098,  0.0098,  0.0098,  ...,  0.0098]])\n```"]