- en: Evaluations with Chat Formats
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用聊天格式的评估
- en: 原文：[https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21](https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21](https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21)
- en: Applying chat templates to generative LM evaluation tests
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将聊天模板应用于生成式语言模型的评估测试
- en: '[](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[![Daniel
    Furman](../Images/f7a1b4c6239ede8bb01e50f167931719.png)](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    [Daniel Furman](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[![Daniel
    Furman](../Images/f7a1b4c6239ede8bb01e50f167931719.png)](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    [Daniel Furman](https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    ·7 min read·Feb 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------)
    ·7分钟阅读·2024年2月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8e714bc5984165c0816edc5f138bd2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e714bc5984165c0816edc5f138bd2f.png)'
- en: Photo by [Google DeepMind](https://unsplash.com/@googledeepmind) on [Unsplash](https://unsplash.com/photos/a-close-up-of-a-metal-structure-made-of-wood-and-metal-pyET8SQTc0A)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Google DeepMind](https://unsplash.com/@googledeepmind) 提供，来源于 [Unsplash](https://unsplash.com/photos/a-close-up-of-a-metal-structure-made-of-wood-and-metal-pyET8SQTc0A)
- en: “**Building solid evals should be the starting point** for any LLM-based system
    or product (as well as conventional machine learning systems).” — Eugene Yan,
    [link](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “**构建扎实的评估应该是任何基于 LLM 的系统或产品的起点**（以及传统的机器学习系统）。” — Eugene Yan, [链接](https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals)
- en: '**tl;dr**'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简要总结**'
- en: Chat models are typically fine-tuned on datasets formatted with a prompt template.
    These chat templates are programmed recipes that convert a chat conversation into
    a single string. At prediction time, it’s standard to match an LLM’s expected
    chat format — not doing so is oft-noted as causing performance degradations [1].
    However, do we in fact see these degradations on evaluation benchmarks?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模型通常在使用提示模板格式的数据集上进行微调。这些聊天模板是编程好的“食谱”，能够将一次聊天对话转化为一个字符串。在预测时，通常需要匹配大语言模型（LLM）期望的聊天格式——如果不这么做，通常会被指出会导致性能下降
    [1]。但是，实际上我们是否在评估基准上看到了这些性能下降？
- en: '**NB**: This blog post is intended for readers with basic familiarity with
    Python programming and neural language modeling.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本博客适合具有 Python 编程和神经语言建模基础知识的读者。'
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'If you’ve built on top of OpenAI’s chat API, the following code will be recognizable.
    Under the hood, this input is transformed into one tokenizable string via the
    [ChatML](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml)
    format:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在 OpenAI 的聊天 API 上构建过应用，下面的代码将是你熟悉的。底层，这些输入会通过 [ChatML](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml)
    格式转换成一个可分词的字符串：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It turns out there’s a wide variety of chat templates across the LLM research
    community. Take an open-source model like `Mixtral-8x7B-Instruct-v0.1`*.* It’s
    format looks wildly different from `gpt-3.5-turbo` above:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，LLM 研究社区中有各种各样的聊天模板。以开源模型 `Mixtral-8x7B-Instruct-v0.1`*.* 为例，它的格式与上面提到的
    `gpt-3.5-turbo` 看起来截然不同：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Why bother with chat templates? Well, it’s strongly advised to match the expected
    chat template at prediction time (for instance, see the info on “Instruction format”
    at the [repo](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for
    `Mixtral-8x7B-Instruct-v0.1`). And, with proprietary chat models like `gpt-3.5-turbo`,
    chat templates are often applied behind the scenes of an endpoint whether you
    like it or not!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要使用聊天模板呢？其实，强烈建议在预测时匹配期望的聊天模板（例如，参见[仓库](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)中的“指令格式”信息，针对`Mixtral-8x7B-Instruct-v0.1`）。而且，对于像`gpt-3.5-turbo`这样的专有聊天模型，聊天模板通常在端点背后自动应用，无论你是否喜欢！
- en: But how do we know whether chat formatting is indeed improving our performance?
    Enter LM evals.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们怎么知道聊天格式是否真的在提高我们的性能呢？这就是语言模型评估的作用。
- en: LM evals
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型评估
- en: 'Evaluations are used to measure an AI/ML model’s performance, and they can
    take many shapes and sizes. Evals include two core components: a dataset curated
    for a specific task and associated metric(s) measuring the modeling performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 评估用于衡量AI/ML模型的性能，它们可以有许多不同的形式和大小。评估包括两个核心组件：针对特定任务策划的数据集和与之相关的衡量模型性能的指标。
- en: Generative LM evals carry some additional nuances. For example, different frameworks
    measure text generation performance in different ways — even varying for the same
    eval ([reference](https://huggingface.co/blog/evaluating-mmlu-leaderboard)). When
    comparing scores across studies, it’s therefore very important to confirm that
    the results were computed with the same code and config to avoid any errant analysis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性语言模型评估包含一些额外的细节。例如，不同的框架以不同方式衡量文本生成性能——即使是相同的评估也会有所不同（[参考](https://huggingface.co/blog/evaluating-mmlu-leaderboard)）。因此，在跨研究比较分数时，非常重要的一点是要确认结果是使用相同的代码和配置计算的，以避免错误分析。
- en: 'The superb Instruction-Following Evaluation ([IFEval](https://arxiv.org/abs/2311.07911))
    [2] is used for our testing here. This eval includes 541 prompts that measures
    a language model’s ability to follow verifiable natural language instructions.
    Examples of these verifiable instructions include:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 超级的指令遵循评估（[IFEval](https://arxiv.org/abs/2311.07911)）[2]在这里用于我们的测试。该评估包括541个提示，用来衡量语言模型遵循可验证自然语言指令的能力。这些可验证指令的示例包括：
- en: “Write 450 to 500 words”, “your entire output should be in JSON output”, “include
    a title, and put it into two square brackets such as [[ title ]]”
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “写450到500字”，“你的所有输出应该是JSON格式”，“包括一个标题，并将其放入两个方括号中，例如[[ title ]]”
- en: 'For a given response and a verifiable instruction, we examine whether the instruction
    has been followed or not with the following four metrics:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的响应和可验证指令，我们使用以下四个指标来检查该指令是否已被遵循：
- en: '1\. **Prompt-level strict-accuracy**: The percentage of prompts that all verifiable
    instructions in each prompt are followed.'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. **提示级严格准确度**：每个提示中所有可验证指令都被遵循的百分比。
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2\. **Inst-level strict-accuracy**: The percentage of verifiable instructions
    that are followed.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. **指令级严格准确度**：可验证的指令中被遵循的百分比。
- en: ''
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '3\. **Prompt-level loose-accuracy**: Prompt-level accuracy computed with the
    loose criterion.'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. **提示级宽松准确度**：使用宽松标准计算的提示级准确度。
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '4\. **Inst-level loose-accuracy**: Instruction-level accuracy computed with
    a loose criterion.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. **指令级宽松准确度**：使用宽松标准计算的指令级准确度。
- en: The average of these four metrics was computed here (Table 1), primarily to
    use a single metric that captures the most diverse signal available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个指标的平均值在此计算（表格1），主要目的是使用一个捕捉最广泛信号的单一指标。
- en: IFEval is an ideal test for exploring the impacts of chat templates, since the
    test is specifically designed to measure instruction-following capabilities on
    chat data. Another interesting line of questioning is whether chat templating
    positively impacts evals that aren’t as well suited for chat data — a topic left
    for future research.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: IFEval是探索聊天模板影响的理想测试，因为该测试专门设计用来衡量在聊天数据上的指令遵循能力。另一个有趣的问题是，聊天模板是否对那些不太适合聊天数据的评估产生积极影响——这是一个留待未来研究的话题。
- en: Chat templates for IFEval
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IFEval的聊天模板
- en: 'Eleuther.AI’s [lm-eval](https://github.com/EleutherAI/lm-evaluation-harness)
    is the de facto open-source package for LM evaluation. Since chat templating for
    more models is an oft-requested addition to the library, it was easy to sync up
    with other developers wanting to work on this feature in the 🤗 model class specifically.
    At present, development is underway at the `add-chat-templating` branch ([link](https://github.com/EleutherAI/lm-evaluation-harness/tree/add-chat-templating)),
    spurred by issues #1098 ([link](https://github.com/EleutherAI/lm-evaluation-harness/issues/1098#issuecomment-1947116099))
    and #1209 ([link](https://github.com/EleutherAI/lm-evaluation-harness/issues/1209#issuecomment-1879966071)).
    When using this branch, we can apply chat formats to an eval as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Eleuther.AI的[lm-eval](https://github.com/EleutherAI/lm-evaluation-harness)是事实上的开源语言模型评估工具包。由于更多模型的聊天模板功能是用户常请求的新增功能，因此我们很容易与其他开发者协作，专注于在🤗模型类中实现这一功能。目前，开发工作正在`add-chat-templating`分支中进行（[链接](https://github.com/EleutherAI/lm-evaluation-harness/tree/add-chat-templating)），由问题#1098（[链接](https://github.com/EleutherAI/lm-evaluation-harness/issues/1098#issuecomment-1947116099)）和#1209（[链接](https://github.com/EleutherAI/lm-evaluation-harness/issues/1209#issuecomment-1879966071)）推动。当使用此分支时，我们可以按如下方式将聊天格式应用于评估：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The newly introduced triggers `use_chat_template` and `system_prompt` appear
    to the right of `model_args` and control how the chat template is applied. In
    the branch’s current experimental form, the code prints the first prompt before
    and after applying the chat template. Here’s what that looks like for the above
    code block:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 新引入的触发器`use_chat_template`和`system_prompt`出现在`model_args`的右侧，用于控制聊天模板的应用方式。在当前分支的实验性版本中，代码在应用聊天模板前后打印第一个提示。这是上述代码块的效果：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output has taken on the desired chat template!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 输出已采用所需的聊天模板！
- en: 'We are now ready to A/B test the influence of chat templates on the IFEval.
    A handful of popular LLMs were selected for our experiment— each with its own
    unique chat template. On the larger end we have the 70B parameter `Llama-2–70b-chat`,
    two variants of the same 47B parameter model, `Mixtral-8x7B-Instruct-v0.1` and
    `Nous-Hermes-2-Mixtral-8x7B-DPO`, as well as the 34B parameter `Nous-Hermes-2-Yi-34B`.
    On the smaller end we have three 7B parameter models: `Mistral-Instruct-7B-v0.2`,
    `Zephyr-7b-beta`, and `Starling-LM-7B-alpha`. As for the system prompt, a simple
    “You are a helpful assistant.” was used for compatible models. More details about
    each of these seven models are included below [3].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备进行A/B测试，评估聊天模板对IFEval的影响。我们为实验选择了一些流行的LLM，每个模型都有自己独特的聊天模板。在较大的模型方面，我们选择了70B参数的`Llama-2–70b-chat`，两种47B参数模型的变体，`Mixtral-8x7B-Instruct-v0.1`和`Nous-Hermes-2-Mixtral-8x7B-DPO`，以及34B参数的`Nous-Hermes-2-Yi-34B`。在较小的模型方面，我们有三个7B参数的模型：`Mistral-Instruct-7B-v0.2`、`Zephyr-7b-beta`和`Starling-LM-7B-alpha`。至于系统提示，兼容模型使用了简单的提示“你是一个有帮助的助手。”更多关于这七个模型的详细信息请参见下文[3]。
- en: 'And, without further delay, our results:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，毫不拖延，我们的结果：
- en: '![](../Images/89bcb4f71233c0c5252237b46374b882.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89bcb4f71233c0c5252237b46374b882.png)'
- en: '**Table 1**: Results from the A/B test on IFEval, sorted by model size descending
    ([link](https://docs.google.com/spreadsheets/d/1Tawz9IHH2B-_XWj-JjeVGmu-og60lgSSpMywrGxcj6Q/edit?usp=sharing)).
    See the “Additional Notes” section below for more details, such as links to the
    run logs. As per reproducibility, the experiments were executed with models in
    half precision bfloat16, a workstation equipped with 2x H100 80 GB SXM5 chips,
    and a fork of the lm-eval package at hash [0c0c314c0df4c10f35bf7c17dc80f745f8027e9b](https://github.com/EleutherAI/lm-evaluation-harness/tree/0c0c314c0df4c10f35bf7c17dc80f745f8027e9b).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1**：来自IFEval的A/B测试结果，按模型大小降序排列（[链接](https://docs.google.com/spreadsheets/d/1Tawz9IHH2B-_XWj-JjeVGmu-og60lgSSpMywrGxcj6Q/edit?usp=sharing)）。有关更多详细信息，请参见下面的“附加说明”部分，例如运行日志的链接。为了保证可重复性，实验在半精度bfloat16模型上执行，工作站配置了2个H100
    80GB SXM5芯片，并使用了`lm-eval`包的分支，哈希值为[0c0c314c0df4c10f35bf7c17dc80f745f8027e9b](https://github.com/EleutherAI/lm-evaluation-harness/tree/0c0c314c0df4c10f35bf7c17dc80f745f8027e9b)。'
- en: 🔥 Chat templates caused serious shakeup to IFEval scoring! `Nous-Hermes-2-Mixtral-8x7B-DPO`
    clocked in as the most performant model tested here, with an average score of
    ~63%. In contrast, `Zephyr-7b-beta` was the worst performing model yet had the
    largest boost from chat templating — a whopping +39%! As a reference, the IFEval
    paper reported `gpt-4` (Nov 2023) at an average score of ~81% and `PaLM 2S`(Aug
    2023) at ~51% [2].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 聊天模板对 IFEval 评分产生了重大影响！`Nous-Hermes-2-Mixtral-8x7B-DPO` 作为测试中表现最好的模型，平均得分约为
    63%。相比之下，`Zephyr-7b-beta` 是表现最差的模型，但却从聊天模板中获得了最大的提升——惊人的 +39%！作为参考，IFEval 论文中报告的
    `gpt-4`（2023年11月）平均得分约为 81%，`PaLM 2S`（2023年8月）为约 51% [2]。
- en: 'In sum, these results point to a couple key insights:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这些结果揭示了几个关键的洞察：
- en: Chat templating has a positive impact on instruction-following for open-source
    LLMs, the extent to which varies by model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天模板对开源 LLM 的指令跟随有积极影响，其影响程度因模型而异。
- en: Open-source LLMs are less equipped at following natural language instructions
    than SOA proprietary models like `gpt-4`.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开源 LLM 在遵循自然语言指令方面不如 SOA 专有模型，如 `gpt-4`。
- en: Conclusion
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Chat templates caused a significant uplift in IFEval scores across the board
    in our experiment, as proven over a variety of formats and models. However, I
    don’t necessarily expect these effects to generalize to all LM evals. To further
    explore the impacts of chat templating on benchmarks, next steps include experimentation
    with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天模板在我们的实验中显著提升了 IFEval 的评分，这在各种格式和模型中都得到了证明。然而，我并不一定期待这些效果能普遍适用于所有 LM 评估。为了进一步探讨聊天模板对基准的影响，下一步包括进行以下实验：
- en: more instruction-following evals similar to IFEval
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多类似 IFEval 的指令跟随评估
- en: general-purpose evals such as those in 🤗’ [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般用途评估，例如 🤗 的[开放LLM排行榜](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
- en: in-context retrieval evals like “[Needle in a Haystack](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)”
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文检索评估，例如“[Needle in a Haystack](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2)”
- en: and much, much more!
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有更多，更多内容！
- en: Zooming out to a thirty thousand foot level, it’s a great time to research LM
    evals — for one, because stronger LLMs require a new generation of tests to effectively
    evaluate them. Whether you create your own or build on top of existing ones, researching
    evals is an impactful way to contribute to the open science community.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从三万英尺的高度来看，现在是进行 LM 评估研究的好时机——首先，因为更强大的 LLM 需要新一代测试来有效评估它们。无论是创建自己的评估方法，还是在现有方法的基础上进行改进，研究评估是为开放科学社区做出贡献的一种重要方式。
- en: Citations
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: '[1] Matthew Carrigan (2023), [Chat Templates: An End to the Silent Performance
    Killer](https://huggingface.co/blog/chat-templates), Hugging Face.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Matthew Carrigan（2023），[聊天模板：终结沉默的性能杀手](https://huggingface.co/blog/chat-templates)，Hugging
    Face。'
- en: '[2] Zhou et al. (2023), [Instruction-Following Evaluation for Large Language
    Models](https://arxiv.org/pdf/2311.07911.pdf), arXiv.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Zhou 等人（2023），[大规模语言模型的指令跟随评估](https://arxiv.org/pdf/2311.07911.pdf)，arXiv。'
- en: '**Dataset licensing**: The IFEval dataset used herein is publicly available
    to all without restriction ([Apache-2.0 license](https://github.com/google-research/google-research/tree/master#Apache-2.0-1-ov-file)).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集许可**：此处使用的 IFEval 数据集对所有人公开，无限制使用（[Apache-2.0 许可证](https://github.com/google-research/google-research/tree/master#Apache-2.0-1-ov-file)）。'
- en: '[3] Models used here, from largest to smallest (all permissively licensed for
    research use).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 此处使用的模型，按大小排列（所有模型均已获得研究使用的宽松许可）。'
- en: '`Llama-2–70b-chat` ([link](http://meta-llama/Llama-2-70b-chat-hf)) — Meta'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Llama-2–70b-chat`（[链接](http://meta-llama/Llama-2-70b-chat-hf)）— Meta'
- en: '`Mixtral-8x7B-Instruct-v0.1` ([link](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1))
    — Mistral.AI'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mixtral-8x7B-Instruct-v0.1`（[链接](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)）—
    Mistral.AI'
- en: '`Nous-Hermes-2-Mixtral-8x7B-DPO` ([link](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO))
    — Nous-Research'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nous-Hermes-2-Mixtral-8x7B-DPO`（[链接](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)）—
    Nous-Research'
- en: '`Nous-Hermes-2-Yi-34B` ([link](http://NousResearch/Nous-Hermes-2-Yi-34B)) —
    Nous-Research'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Nous-Hermes-2-Yi-34B`（[链接](http://NousResearch/Nous-Hermes-2-Yi-34B)）— Nous-Research'
- en: '`Starling-LM-7B-alpha` ([link](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha))
    — Berkeley NEST'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Starling-LM-7B-alpha`（[链接](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha)）—
    Berkeley NEST'
- en: '`Zephyr-7B-beta` ([link](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta))
    — Hugging Face'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Zephyr-7B-beta`（[链接](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)）—
    Hugging Face'
- en: '`Mistral-7B-Instruct-v0.2` ([link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2))
    — Mistral.AI'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mistral-7B-Instruct-v0.2` ([链接](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2))
    — Mistral.AI'
- en: Additional Notes
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他注意事项
- en: See the notebooks [here](https://github.com/daniel-furman/evals-with-chat-formats)
    for the code used to run the experiments.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看用于运行实验的代码，可以在[这里](https://github.com/daniel-furman/evals-with-chat-formats)找到。
- en: To audit the results, see outputs for each run [here](https://github.com/daniel-furman/evals-with-chat-formats/tree/main/assets/IFEval_results)
    as well as Zeno logs [here](https://hub.zenoml.com/project/79b4684d-0f4e-48f4-b739-ba4e0bd63ee8/IFEval-chat-templating-experiments-run-1)
    and [here](https://hub.zenoml.com/project/548fcb7a-52cf-4c60-aaf7-13d6b03343fd/IFEval-chat-templating-experiments-run-2)
    (models were ran in 2 total batches). Note that the Zeno logs don’t yet capture
    the application of chat templates to the prompts — this is a “to do” item in development
    backlog.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要审计结果，请查看每次运行的输出，[这里](https://github.com/daniel-furman/evals-with-chat-formats/tree/main/assets/IFEval_results)，以及Zeno日志，[这里](https://hub.zenoml.com/project/79b4684d-0f4e-48f4-b739-ba4e0bd63ee8/IFEval-chat-templating-experiments-run-1)和[这里](https://hub.zenoml.com/project/548fcb7a-52cf-4c60-aaf7-13d6b03343fd/IFEval-chat-templating-experiments-run-2)（模型在2个批次中运行）。请注意，Zeno日志尚未捕捉到聊天模板应用于提示的过程——这是开发待办事项中的一项内容。
- en: For compute, RunPod ([link](https://www.runpod.io/)) was used for access to
    workstations with Nvidia GPU chips — in particular, a cluster with 2x H100 80
    GB SXM5 chips. In total, the experiment included 14 runs of the IFEval, which
    accumulated ~6 hrs of cluster uptime.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算方面，使用了RunPod ([链接](https://www.runpod.io/)) 访问带有Nvidia GPU芯片的工作站——特别是一个拥有2个H100
    80 GB SXM5芯片的集群。总的来说，实验包括了14次IFEval的运行，总共积累了约6小时的集群运行时间。
- en: Confidence intervals were taken to estimate statistical uncertainty in our results
    (the bootstrap resampling method was used). These 95% confidence intervals ranged
    from roughly +/- 2.75% to 4.25% — small relative to the measured effects of chat
    templating.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过置信区间估计我们的结果中的统计不确定性（使用了自助法重抽样方法）。这些95%的置信区间大约在+/- 2.75%到4.25%之间——相对于聊天模板应用的测量效果来说，这个范围较小。
