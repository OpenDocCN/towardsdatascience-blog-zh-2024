- en: Streamline Your Prompts to Decrease LLM Costs and Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/streamline-your-prompts-to-decrease-llm-costs-and-latency-29591dd0e9e4?source=collection_archive---------5-----------------------#2024-05-24](https://towardsdatascience.com/streamline-your-prompts-to-decrease-llm-costs-and-latency-29591dd0e9e4?source=collection_archive---------5-----------------------#2024-05-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover 5 techniques to optimize token usage without sacrificing accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@janekmajewski?source=post_page---byline--29591dd0e9e4--------------------------------)[![Jan
    Majewski](../Images/2d06418ffe9f14cb558ebaec7f871cf0.png)](https://medium.com/@janekmajewski?source=post_page---byline--29591dd0e9e4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29591dd0e9e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29591dd0e9e4--------------------------------)
    [Jan Majewski](https://medium.com/@janekmajewski?source=post_page---byline--29591dd0e9e4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29591dd0e9e4--------------------------------)
    ·8 min read·May 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7acbcaeb3816dbcac95c78644fc0bad3.png)'
  prefs: []
  type: TYPE_IMG
- en: image generated by author with GPT-4o
  prefs: []
  type: TYPE_NORMAL
- en: High costs and latency are one of the key obstacles when launching LLM Apps
    in production, both strongly related to prompt size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The versatility of Large Language Models led many to believe that they can be
    treated as a silver bullet solution for pretty much any task. When you combine
    LLMs with access to tools including RAG and API calling and give them detailed
    instructions they can often perform on a near-human level.
  prefs: []
  type: TYPE_NORMAL
- en: The key problem with this all-in-one approach might quickly lead to **exploding
    prompt sizes, which result in infeasible costs and latency for each call.**
  prefs: []
  type: TYPE_NORMAL
- en: If you use LLMs to optimize a high-value activity such as coding, keeping costs
    down is not the top priority — you can wait half a minute for code you would normally
    write for half an hour. However, when using LLMs in customer-facing apps, where
    you expect thousands of chats, costs and latency can make or break your solution.
  prefs: []
  type: TYPE_NORMAL
- en: This article is the first from a series, where I will share insights from building
    an LLM-powered Real Estate Search Assistant ‘*Mieszko*’ for [Resider.pl](http://www.Resider.pl)…
  prefs: []
  type: TYPE_NORMAL
