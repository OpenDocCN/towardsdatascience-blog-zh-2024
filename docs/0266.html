<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fraud Detection with Generative Adversarial Nets (GANs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fraud Detection with Generative Adversarial Nets (GANs)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29">https://towardsdatascience.com/fraud-detection-with-generative-adversarial-nets-gans-26bea360870d?source=collection_archive---------5-----------------------#2024-01-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2182" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Application of GANs for data augmentation to adjust an imbalanced dataset</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Michio Suginoo" class="l ep by dd de cx" src="../Images/15e4a70d17d163889cc902bf4409931a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kx7NRQ9KN0OWP2BRK-obxA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://deeporigami.medium.com/?source=post_page---byline--26bea360870d--------------------------------" rel="noopener follow">Michio Suginoo</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--26bea360870d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/b701f889a034bd2b3b6ca7bfd994721c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nzC1oUDOJGi_-EeG"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Brett Jordan</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3287" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">“Generative Adversarial Nets</a>” (GANs) demonstrated outstanding performance in generating realistic synthetic data which are indistinguishable from the real data in the past. Unfortunately, GANs caught the public’s attention because of its unethical applications, <a class="af nc" href="https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/" rel="noopener ugc nofollow" target="_blank">deepfakes</a> (Knight, 2018).</p><p id="12a9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This article illustrates a case with a good motive in the application of GANs in the context of fraud detection.</p><p id="928a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fraud detection is an application of binary classification prediction. Fraud cases, which account for only a small fraction of the transaction universe, constitute a minority class that makes the dataset highly imbalanced. In general, the resulting model tends to be biased towards the majority class and tends to underfit to the minority class. Thus, the less balanced the dataset, the poorer the performance of the classification predictor would be.</p><p id="8b02" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">My motive here is to use GANs as a data augmentation tool in an attempt to address this classical problem of fraud detection associated with the imbalanced dataset. More specifically, GANs can generate realistic synthetic data of the minority fraud class and transform the imbalanced dataset perfectly balanced.</p><p id="13eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And, I am hoping that this sophisticated algorithm could materially contribute to the performance of fraud detection. In other words, my initial expectation is: the better sophisticated algorithm, the better performance.</p><p id="f6b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A relevant question is if the use of GANs will guarantee a promising improvement in the performance of fraud detection and satisfy my motive. Let’s see.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d24e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al"><em class="pd">Introduction</em></strong></h1><p id="2f84" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">In principle, fraud detection is an application of binary classification algorithm: to classify each transaction whether it is a fraud case or not.</p><p id="7b99" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fraud cases account for only a small fraction of the transaction universe. In general, fraud cases constitute the minority class, thus, make the dataset highly imbalanced.</p><p id="56b6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The fewer fraud cases, the more sound the transaction system would be.</p><p id="17bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Very simple and intuitive.</p><p id="c9f1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Paradoxically, that sound condition was one of the primary reasons that made fraud detection challenging in the past, if not impossible. It is simply because it was difficult for a classification algorithm to learn the probability distribution of the minority class of fraud.</p><p id="1e8e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In general, the more balanced the dataset, the better the performance of the classification predictor. In other words, the less balanced (or the more imbalanced) the dataset, the poorer the performance of classifier.</p><p id="651b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This paints the classical problem of fraud detection: a binary classification application with highly imbalanced dataset.</p><p id="1c0b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this setting, we can use Generative Adversarial Nets (GANs) as a data augmentation tool to generate realistic synthetic data of the minority fraud class to transform the entire dataset more balanced in an attempt to improve the performance of the classifier model of fraud detection.</p><p id="b6b4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This article is divided into the following sections:</p><ul class=""><li id="75c7" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">Section 1: Algorithm Overview: Bi-level Optimization Architecture of GANs</li><li id="1b92" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Section 2: Fraud Dataset</li><li id="5485" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Section 3: Python Code breakdown of GANs for data augmentation</li><li id="da0c" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Section 4: Fraud Detection Overview (Benchmark Scenario vs GANs Scenario)</li><li id="c32e" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Section 5: Conclusion</li></ul><p id="b7d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Overall, I will primarily focus on the topic of GANs (both the algorithm and the code). For the remaining topics of the model development other than GANs, such as data preprocessing and classifier algorithm, I will only outline the process and refrain from going into their details. In this context, this article assumes that the readers have a basic knowledge about the binary classifier algorithm (especially, Ensemble Classifier that I selected for fraud detection) as well as general understanding of data cleaning and preprocessing.</p><p id="0522" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the detailed code, the readers are welcome to access the following link: <a class="af nc" href="https://github.com/deeporigami/Portfolio/blob/6538fcaad1bf58c5f63d6320ca477fa867edb1df/GAN_FraudDetection_Medium_2.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/deeporigami/Portfolio/blob/6538fcaad1bf58c5f63d6320ca477fa867edb1df/GAN_FraudDetection_Medium_2.ipynb</a></p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="97ec" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al"><em class="pd">Section 1: Algorithm Overview: Bi-level Optimization Architecture of GANs</em></strong></h1><p id="b522" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">GANs is a special type of generative algorithm. As its name suggests, Generative Adversarial Nets (GANs) is composed of two neural networks: the generative network (the generator) and the adversarial network (the discriminator). GANs pits these two agents against each other to engage in a competition, where the generator attempts to generate realistic synthetic data and the discriminator to distinguish the synthetic data from the real data.</p><p id="da07" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The original GANs was introduced in a seminal paper: “<a class="af nc" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">Generative Adversarial Nets</a>” (Goodfellow, et al., Generative Adversarial Nets, 2014). The co-authors of the original GANs portrayed GANs with a counterfeiter-police analogy: an iterative game, where the generator acts as a counterfeiter and the discriminator plays the role of the police to detect the counterfeit that the generator forged.</p><p id="3c71" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The original GANs was innovative in a sense that it addressed and overcame conventional difficulties in training deep generative algorithm in the past. And as its core, it was designed with bi-level optimization framework with an equilibrium seeking objective setting (vs maximum likelihood oriented objective setting).</p><p id="1c38" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Ever since, many variant architectures of GANs have been explored. As a precaution, this article refers solely to the prototype architecture of the original GANs.</p><p id="10fa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">Generator and Discriminator</em></strong></p><p id="66b5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Repeatedly, in the architecture of GANs, the two neural networks — the generator and the discriminator — compete against each other. In this context, the competition takes place through the iteration of forward propagation and backward propagation (according to the general framework of neural networks).</p><p id="62df" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On one hand, it is straight-forward that the discriminator is a binary classifier by design: it classifies whether each sample is real (label: 1) or fake/synthetic (label:0). And the discriminator is fed with both the real samples and the synthetic samples during the forward propagation. Then, during the backpropagation, it learns to detect the synthetic data from the mixed data feed.</p><p id="00e8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On the other hand, the generator is a noise distribution by design. The generator is fed with the real samples during the forward propagation. Then, during the backward propagation, the generator learns the probability distribution of the real data in order to better simulate its synthetic samples.</p><p id="5275" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And these two agents are trained alternately via “bi-level optimization” framework.</p><p id="05e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">Bi-level Training Mechanism (bi-level optimization method)</em></strong></p><p id="07c3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the original GAN paper, in order to train these two agents that pursue their diametrically opposite objectives, the co-authors designed a “bi-level optimization (training)” architecture, in which one internal training block (training of the discriminator) is nested within another high-level training block (training of the generator).</p><p id="f1ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The image below illustrates the structure of “bi-level optimization” in the nested training loops. The discriminator is trained within the nested inner loop, while the generator is trained in the main loop at the higher level.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycLOYA4e0aGva5nwJtA-aQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="3daf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And GANs trains these two agents alternately in this bi-level training architecture (Goodfellow, et al., Generative Adversarial Nets, 2014, p. 3). In other words, while training one agent during the alternation, we need to freeze the learning process of the other agent (Goodfellow I. , 2015, p. 3).</p><p id="9e61" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">Mini-Max Optimization Objective</em></strong></p><p id="8473" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In addition to the “bi-level optimization” mechanism which enables the alternate training of these two agents, another unique feature that differentiates GANs from the conventional prototype of neural network is its mini-max optimization objective. Simply put, in contrast to the conventional maximum seeking approach (such as maximum-likelihood) , GANs pursues an equilibrium-seeking optimization objective.</p><p id="1395" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">What is an equilibrium-seeking optimization objective?</p><p id="3151" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s break it down.</p><p id="a642" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GANs’ two agents have two diametrically opposite objectives. While the discriminator, as a binary classifier, aims at maximizing the probability of correctly classifying the mixture of the real samples and the synthetic samples, the generator’s objective is to minimize the probability that the discriminator correctly classifies the synthetic data: simply because the generator needs to fool the discriminator.</p><p id="8725" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this context, the co-authors of the original GANs called the overall objective a “<strong class="nf fr"><em class="pr">minimax game</em></strong>”. (Goodfellow, et al., 2014, p. 3)</p><p id="cbc4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Overall, the ultimate <strong class="nf fr"><em class="pr">mini-max optimization objective</em></strong> of GANs is not to search for the global maximum/minimum of either of these objective functions. Instead, it is set to seek an equilibrium point which can be interpreted as:</p><ul class=""><li id="157b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">“a saddle point that is a local maximum for the classifier and a local minimum for the generator” (Goodfellow I. , 2015, p. 2)</li><li id="8d7b" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">where neither of agents can improve their performance any longer.</li><li id="86c1" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">where the synthetic data that the generator learned to create has become realistic enough to fool the discriminator.</li></ul><p id="8f15" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And the equilibrium point could be conceptually represented by the probability of random guessing, 0.5 (50%), for the discriminator: D(z) =&gt; 0.5 .</p><p id="e7dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s transcribe the conceptual framework of GANs’ minimax optimization in terms of their objective functions.</p><p id="56d2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The objective of the discriminator is to maximize the objective function in the following image:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/5b3ace53f4fcc19374406302839ec188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3JYgvyy45xQmVdtzpd2Cw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="b8f0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to resolve a potential saturation issue, they converted the second term of the original log-likelihood objective function for the generator as follows and recommended to maximize the converted version as the generator’s objective:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/1047088989096cf0cee24f1e93409952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zatXfLgggP6-EzW3PEZw2w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="34fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Overall, the architecture of GANs’ “bi-level optimization” can be translated in to the following algorithm.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pv"><img src="../Images/1c7d25fd63b2c1395701db01c688802a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*iXAVkRmooCTZEM5HI_petg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="c98e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For more details about the algorithmic design of GANs, please read another article of mine: <a class="af nc" rel="noopener" target="_blank" href="/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02">Mini-Max Optimization Design of Generative Adversarial Nets</a> .</p><p id="e300" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let’s move on to the actual coding with a dataset.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cb81" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to highlight GANs algorithm, I will primarily focus on the code of GANs here and only outline the rest of the process.</p><h1 id="4ad6" class="oh oi fq bf oj ok pw gq om on px gt op oq py os ot ou pz ow ox oy qa pa pb pc bk">Section 2: Fraud Dataset</h1><p id="aa68" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">For fraud detection, I selected the following dataset of credit card transactions from Kaggle: <a class="af nc" href="https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud</a></p><p id="6d96" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Data License: <a class="af nc" href="https://opendatacommons.org/licenses/dbcl/1-0/" rel="noopener ugc nofollow" target="_blank">Database Contents License (DbCL) v1.0</a></p><p id="69ff" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is a summary of the dataset.</p><p id="10cb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The dataset contains 284,807 transactions. In the dataset, we have only 492 fraud cases (including 29 duplicated cases).</p><p id="112c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since the fraud class accounts for only 0.172% of all transactions, it constitutes an extremely small minority class. This dataset is an appropriate one for illustrating the classical problem of fraud detection associated with the imbalanced dataset.</p><p id="190a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It has the following 30 features:</p><ul class=""><li id="2631" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">V1, V2, … V28: 28 principal components obtained by PCA. The source of the data is not disclosed for the privacy protection purpose.</li><li id="7fbe" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">‘Time’: the seconds elapsed between each transaction and the first transaction of the dataset.</li><li id="6bdf" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">‘Amount’: the amount of the transaction.</li></ul><p id="3121" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The label is set as ‘Class’.</p><ul class=""><li id="a8d7" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">‘Class’: 1 in case of fraud; and 0 otherwise.</li></ul><h2 id="89b5" class="qb oi fq bf oj qc qd qe om qf qg qh op nm qi qj qk nq ql qm qn nu qo qp qq qr bk"><strong class="al"><em class="pd">Data Preprocessing: Feature Selection</em></strong></h2><p id="6bef" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Since the dataset has already been pretty much, if not perfectly, cleaned, I only had to do few things for the data cleaning: elimination of duplicated data and removal of outliers.</p><p id="4bfd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thereafter, given 30 features in the dataset, I decided to run the feature selection to reduce the number of the features by eliminating less important features before the training process. I selected the built-in <a class="af nc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr"><em class="pr">feature importance score</em></strong></a><strong class="nf fr"><em class="pr"> </em></strong>of the scikit-learn Random Forest Classifier to estimate the scores of all the 30 features.</p><p id="09fe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following chart displays the summary of the result. If interested in the detailed process, please visit my code listed above.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/2ce0c2fdd9ede86e0b09d75dea23a6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F3hAXLhbU5aTO8Yzd2noBA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="8179" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Based on the results displayed in the bar chart above, I made my subjective judgement to select the top 6 features for the analysis and remove all the remaining insignificant features from the model building process.</p><p id="d8ae" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the selected top 6 important features.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qs"><img src="../Images/69b736742bf84f1f6d3608082ba86018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPentHMtD8Hg_NgEY4FcjA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="bdd6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the model building purpose going forward, I focused on these 6 selected features. After the data preprocessing, we have the working dataframe, df, of the following shape:</p><ul class=""><li id="42a1" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">df.shape = (282513, 7)</li></ul><p id="024c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Hopefully, the feature selection would reduce the complexity of the resulting model and stabilize its performance, while retaining critical information for optimizing a binary classifier.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8168" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al"><em class="pd">Scenario 3: </em></strong>Code breakdown of GANs for data augmentation</h1><p id="8dc7" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Finally, it’s time for us to use GANs for data augmentation.</p><p id="6dd3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">So how many synthetic data do we need to create?</em></strong></p><p id="6c04" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First of all, our interest for the data augmentation is only for the model training. Since the test dataset is out-of-sample data, we want to preserve the original form of the test dataset. Secondly, because our intention is to transform the imbalanced dataset perfectly, we do not want to augment the majority class of non-fraud cases.</p><p id="b5cf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Simply put, we want to augment only the train dataset of the minority fraud class, nothing else.</p><p id="2b6c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let’s split the working dataframe into the train dataset and the test dataset in 80/20 ratio, using a stratified data split method.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="45d3" class="qx oi fq qu b bg qy qz l ra rb"># Separate features and target variable<br/>X = df.drop('Class', axis=1)<br/>y = df['Class']<br/><br/># Splitting data into train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)<br/><br/># Combine the features and the label for the train dataset <br/>train_df = pd.concat([X_train, y_train], axis=1)</span></pre><p id="d5bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a result, the shape of the train dataset is as follows:</p><ul class=""><li id="5120" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">train_df.shape = (226010, 7)</li></ul><p id="813c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s see the composition (the fraud cases and the non-fraud cases) of the train dataset.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="2097" class="qx oi fq qu b bg qy qz l ra rb"># Load the dataset (fraud and non-fraud data)<br/>fraud_data = train_df[train_df['Class'] == 1].drop('Class', axis=1).values<br/>non_fraud_data = train_df[train_df['Class'] == 0].drop('Class', axis=1).values<br/><br/># Calculate the number of synthetic fraud samples to generate<br/>num_real_fraud = len(fraud_data)<br/>num_synthetic_samples = len(non_fraud_data) - num_real_fraud<br/>print("# of non-fraud: ", len(non_fraud_data))<br/>print("# of Real Fraud:", num_real_fraud)<br/>print("# of Synthetic Fraud required:", num_synthetic_samples)<br/><br/># of non-fraud:  225632<br/># of Real Fraud: 378<br/># of Synthetic Fraud required: 225254</span></pre><p id="c441" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This tells us that the train dataset (226,010) is comprised of 225,632 non-fraud data and 378 fraud data. In other words, the difference between them is 225,254. This number is the number of the synthetic fraud data (<em class="pr">num_synthetic_samples</em>) that we need to augment in order to perfectly match the numbers of these two classes within the train dataset: as a reminder, we do preserve the original test dataset.</p><p id="d186" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">Next, let’s code GANs.</em></strong></p><p id="2b73" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First, let’s create custom functions to determine the two agents: the discriminator and the generator.</p><p id="2abf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the generator, I create a noise distribution function, <em class="pr">build_generator()</em>, which requires two parameters: <em class="pr">latent_dim</em> (the dimension of the noise) as the shape of its input; and the shape of its output, <em class="pr">output_dim</em>, which corresponds to the number of the features.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="678b" class="qx oi fq qu b bg qy qz l ra rb"># Define the generator network<br/>def build_generator(latent_dim, output_dim):<br/>    model = Sequential()<br/>    model.add(Dense(64, input_shape=(latent_dim,)))<br/>    model.add(Dense(128, activation='sigmoid'))<br/>    model.add(Dense(output_dim, activation='sigmoid'))<br/>    return model</span></pre><p id="90b1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the discriminator, I create a custom function <em class="pr">build_discriminator() </em>that takes <em class="pr">input_dim</em>, which corresponds to the number of the features.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="826f" class="qx oi fq qu b bg qy qz l ra rb"># Define the discriminator network<br/>def build_discriminator(input_dim):<br/>    model = Sequential()<br/>    model.add(Input(input_dim))<br/>    model.add(Dense(128, activation='sigmoid'))<br/>    model.add(Dense(1, activation='sigmoid'))<br/>    return model</span></pre><p id="e79e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then, we can call these function to create the generator and the discriminator. Here, for the generator I arbitrarily set <em class="pr">latent_dim</em> to be 32: you can try other value here, if you like.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="1c3b" class="qx oi fq qu b bg qy qz l ra rb"># Dimensionality of the input noise for the generator<br/>latent_dim = 32<br/><br/># Build generator and discriminator models<br/>generator = build_generator(latent_dim, fraud_data.shape[1])<br/>discriminator = build_discriminator(fraud_data.shape[1])</span></pre><p id="8333" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At this stage, we need to compile the discriminator, which is going to be nested in the main (higher) optimization loop later. And we can compile the discriminator with the following argument setting.</p><ul class=""><li id="d6fa" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">the loss function of the discriminator: the generic cross-entropy loss function for a binary classifier</li><li id="908b" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">the evaluation metrics: precision and recall.</li></ul><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="72b4" class="qx oi fq qu b bg qy qz l ra rb"># Compile the discriminator model<br/>from keras.metrics import Precision, Recall<br/>discriminator.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy',  metrics=[Precision(), Recall()])</span></pre><p id="1fab" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the generator, we will compile it when we construct the main (upper) optimization loop.</p><p id="baf4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At this stage, we can define the custom objective function for the generator as follows. Remember, the recommended objective was to maximize the following formula:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/1047088989096cf0cee24f1e93409952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zatXfLgggP6-EzW3PEZw2w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="8495" class="qx oi fq qu b bg qy qz l ra rb">def generator_loss_log_d(y_true, y_pred):<br/>    return - K.mean(K.log(y_pred + K.epsilon()))</span></pre><p id="d627" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Above, the negative sign is required, since the loss function by default is designed to be minimized.</p><p id="cff0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Then, we can construct the main (upper) loop, <em class="pr">build_GANs(generator, discriminator),</em> of the bi-level optimization architecture. In this main loop, we compile the generator implicitly. In this context, we need to use the custom objective function of the generator, <em class="pr">generator_loss_log_d</em>, when we compile the main loop.</p><p id="dee3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As aforementioned, we need to freeze the discriminator when we train the generator.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="edd4" class="qx oi fq qu b bg qy qz l ra rb"># Build and compile the GANs upper optimization loop combining generator and discriminator<br/>def build_gan(generator, discriminator):<br/>    discriminator.trainable = False<br/>    model = Sequential()<br/>    model.add(generator)<br/>    model.add(discriminator)<br/>    model.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss=generator_loss_log_d)<br/><br/>    return model<br/><br/># Call the upper loop function<br/>gan = build_gan(generator, discriminator)</span></pre><p id="02d0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At the last line above, <em class="pr">gan</em> calls <em class="pr">build_gan()</em> in order to implement the batch training below, using <a class="af nc" href="https://keras.io/api/models/model_training_apis/" rel="noopener ugc nofollow" target="_blank">Keras’ model<em class="pr">.train_on_batch()</em> method</a><em class="pr">.</em></p><p id="27b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a reminder, while we train the discriminator, we need to freeze the training of the generator; and while we train the generator, we need to freeze the training of the discriminator.</p><p id="6b2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the batch training code incorporating the alternating training process of these two agents under the bi-level optimization framework.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="7bcc" class="qx oi fq qu b bg qy qz l ra rb"># Set hyperparameters<br/>epochs = 10000<br/>batch_size = 32<br/><br/># Training loop for the GANs<br/>for epoch in range(epochs):<br/>    # Train discriminator (freeze generator)<br/>    discriminator.trainable = True<br/>    generator.trainable = False<br/><br/>    # Random sampling from the real fraud data<br/>    real_fraud_samples = fraud_data[np.random.randint(0, num_real_fraud, batch_size)]<br/><br/>    # Generate fake fraud samples using the generator<br/>    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))<br/>    fake_fraud_samples = generator.predict(noise)<br/><br/>    # Create labels for real and fake fraud samples<br/>    real_labels = np.ones((batch_size, 1))<br/>    fake_labels = np.zeros((batch_size, 1))<br/><br/>    # Train the discriminator on real and fake fraud samples<br/>    d_loss_real = discriminator.train_on_batch(real_fraud_samples, real_labels)<br/>    d_loss_fake = discriminator.train_on_batch(fake_fraud_samples, fake_labels)<br/>    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)<br/><br/>    # Train generator (freeze discriminator)<br/>    discriminator.trainable = False<br/>    generator.trainable = True<br/><br/>    # Generate synthetic fraud samples and create labels for training the generator<br/>    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))<br/>    valid_labels = np.ones((batch_size, 1))<br/><br/>    # Train the generator to generate samples that "fool" the discriminator<br/>    g_loss = gan.train_on_batch(noise, valid_labels)<br/><br/>    # Print the progress<br/>    if epoch % 100 == 0:<br/>        print(f"Epoch: {epoch} - D Loss: {d_loss} - G Loss: {g_loss}")</span></pre><p id="4215" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here, I have a quick question for you.</p><p id="e0eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below we have an excerpt associated with the generator training from the code above.</p><p id="9636" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Can you explain what this code is doing?</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="27be" class="qx oi fq qu b bg qy qz l ra rb"># Generate synthetic fraud samples and create labels for training the generator<br/>    noise = np.random.normal(0, 1, size=(batch_size, latent_dim))<br/>    valid_labels = np.ones((batch_size, 1))</span></pre><p id="763e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the first line, <em class="pr">noise</em> generates the synthetic data. In the second line, <em class="pr">valid_labels</em> assigns the label of the synthetic data.</p><p id="2411" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Why do we need to label it with 1, which is supposed to be the label for the real data? Didn’t you find the code counter-intuitive?</p><p id="618e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Ladies and gentlemen, welcome to the world of counterfeiters.</p><p id="7699" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is the labeling magic that trains the generator to create samples that can fool the discriminator.</p><p id="a41e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let’s use the trained generator to create the synthetic data for the minority fraud class.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="bc0e" class="qx oi fq qu b bg qy qz l ra rb"># After training, use the generator to create synthetic fraud data<br/>noise = np.random.normal(0, 1, size=(num_synthetic_samples, latent_dim))<br/>synthetic_fraud_data = generator.predict(noise)<br/><br/># Convert the result to a Pandas DataFrame format<br/>fake_df = pd.DataFrame(synthetic_fraud_data, columns=features.to_list())</span></pre><p id="a35a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, the synthetic data is created.</p><p id="9d59" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next section, we can combine this synthetic fraud data with the original train dataset to make the entire train dataset perfectly balanced. I hope that the perfectly balanced training dataset would improve the performance of the fraud detection classification model.</p><h1 id="6f1c" class="oh oi fq bf oj ok pw gq om on px gt op oq py os ot ou pz ow ox oy qa pa pb pc bk">Section 4: Fraud Detection Overview (with and without GANs data augmentation)</h1><p id="1786" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Repeatedly, the use of GANs in this project is exclusively for data augmentation, but not for classification.</p><p id="79a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">First of all, we would need the benchmark model as the basis of the comparison in order for us to evaluate the improvement made by the GANs based data augmentation on the performance of the fraud detection model.</p><p id="975c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a binary classifier algorithm, I selected Ensemble Method for building the fraud detection model. As the benchmark scenario, I developed a fraud detection model only with the original imbalanced dataset: thus, without data augmentation. Then, for the second scenario with data augmentation by GANs, I can train the same algorithm with the perfectly balanced train dataset, which contains the synthetic fraud data created by GANs.</p><ul class=""><li id="f97f" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">Benchmark Scenario: Ensemble Classifier without data augmentation</li><li id="6892" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">GANs Scenario: Ensemble Classifier with data augmentation by GANs</li></ul><p id="8c13" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">Benchmark Scenario: Ensemble without data augmentation</em></strong></p><p id="89da" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, let’s define the benchmark scenario (without data augmentation). I decided to select Ensemble Classifier: voting method as the meta learner with the following 3 base learners.</p><ul class=""><li id="9ee0" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">Gradient Boosting</li><li id="fb5f" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Decision Tree</li><li id="1be4" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Random Forest</li></ul><p id="ce4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since the original dataset is highly imbalanced, rather than accuracy I shall select evaluation metrics from the following 3 options: precision, recall, and F1-Score.</p><p id="64ba" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following custom function, <em class="pr">ensemble_training(X_train, y_train)</em>, defines the training and validation process.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="b400" class="qx oi fq qu b bg qy qz l ra rb">def ensemble_training(X_train, y_train):</span></pre><pre class="rc qt qu rd re ay rf bk"><span id="8c34" class="qb oi fq qu b hw rg rh l im rb">  # Initialize base learners<br/>  gradient_boosting = GradientBoostingClassifier(random_state=42)<br/>  decision_tree = DecisionTreeClassifier(random_state=42)<br/>  random_forest = RandomForestClassifier(random_state=42)</span><span id="8f83" class="qb oi fq qu b hw ri rh l im rb">  # Define the base models<br/>  base_models = {<br/>    'RandomForest': random_forest,<br/>    'DecisionTree': decision_tree,<br/>    'GradientBoosting': gradient_boosting<br/>  }</span><span id="403a" class="qb oi fq qu b hw ri rh l im rb">  # Initialize the meta learner<br/>  meta_learner = VotingClassifier(estimators=[(name, model) for name, model in base_models.items()], voting='soft')</span><span id="be44" class="qb oi fq qu b hw ri rh l im rb">  # Lists to store training and validation metrics<br/>  train_f1_scores = []<br/>  val_f1_scores = []</span><span id="1846" class="qb oi fq qu b hw ri rh l im rb">  # Splitting the train set further into training and validation sets<br/>  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=y_train)</span><span id="bc19" class="qb oi fq qu b hw ri rh l im rb">  # Training and validation<br/>  for model_name, model in base_models.items():<br/>    model.fit(X_train, y_train)</span><span id="a320" class="qb oi fq qu b hw ri rh l im rb">    # Training metrics<br/>    train_predictions = model.predict(X_train)<br/>    train_f1 = f1_score(y_train, train_predictions)<br/>    train_f1_scores.append(train_f1)</span><span id="653e" class="qb oi fq qu b hw ri rh l im rb">    # Validation metrics using the validation set<br/>    val_predictions = model.predict(X_val)<br/>    val_f1 = f1_score(y_val, val_predictions)<br/>    val_f1_scores.append(val_f1)</span><span id="c534" class="qb oi fq qu b hw ri rh l im rb">  # Training the meta learner on the entire training set<br/>  meta_learner.fit(X_train, y_train)</span><span id="b57c" class="qb oi fq qu b hw ri rh l im rb">  return meta_learner, train_f1_scores, val_f1_scores, base_models</span></pre><p id="f8db" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next function block, <em class="pr">ensemble_evaluations(meta_learner, X_train, y_train, X_test, y_test)</em>, calculates the performance evaluation metrics at the meta learner level.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="f87d" class="qx oi fq qu b bg qy qz l ra rb">def ensemble_evaluations(meta_learner,X_train, y_train, X_test, y_test):<br/># Metrics for the ensemble model on both traininGANsd test datasets<br/>  ensemble_train_predictions = meta_learner.predict(X_train)<br/>  ensemble_test_predictions = meta_learner.predict(X_test)</span></pre><pre class="rc qt qu rd re ay rf bk"><span id="4052" class="qb oi fq qu b hw rg rh l im rb">  # Calculating metrics for the ensemble model<br/>  ensemble_train_f1 = f1_score(y_train, ensemble_train_predictions)<br/>  ensemble_test_f1 = f1_score(y_test, ensemble_test_predictions)</span><span id="e42e" class="qb oi fq qu b hw ri rh l im rb">  # Calculate precision and recall for both training and test datasets<br/>  precision_train = precision_score(y_train, ensemble_train_predictions)<br/>  recall_train = recall_score(y_train, ensemble_train_predictions)</span><span id="ac2d" class="qb oi fq qu b hw ri rh l im rb">  precision_test = precision_score(y_test, ensemble_test_predictions)<br/>  recall_test = recall_score(y_test, ensemble_test_predictions)</span><span id="4c1e" class="qb oi fq qu b hw ri rh l im rb">  # Output precision, recall, and f1 score for both training and test datasets<br/>  print("Ensemble Model Metrics:")<br/>  print(f"Training Precision: {precision_train:.4f}, Recall: {recall_train:.4f}, F1-score: {ensemble_train_f1:.4f}")<br/>  print(f"Test Precision: {precision_test:.4f}, Recall: {recall_test:.4f}, F1-score: {ensemble_test_f1:.4f}")</span><span id="dc0c" class="qb oi fq qu b hw ri rh l im rb">  return ensemble_train_predictions, ensemble_test_predictions, ensemble_train_f1, ensemble_test_f1, precision_train, recall_train, precision_test, recall_test</span></pre><p id="4a31" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below, let’s look at the performance of the benchmark Ensemble Classifier.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="27e5" class="qx oi fq qu b bg qy qz l ra rb">Training Precision: 0.9811, Recall: 0.9603, F1-score: 0.9706<br/>Test Precision: 0.9351, Recall: 0.7579, F1-score: 0.8372</span></pre><p id="3768" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At the meta-learner level, the benchmark model generated F1-Score at a reasonable level of 0.8372.</p><p id="76c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Next, let’s move on to the scenario with data augmentation using GANs . We want to see if the performance of the scenario with GAN can outperform the benchmark scenario.</p><p id="8df8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="pr">GANs Scenario: Fraud Detection with data augmentation by GANs</em></strong></p><p id="1543" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we have constructed a perfectly balanced dataset by combining the original imbalanced train dataset (both non-fraud and fraud cases), <em class="pr">train_df</em>, and the synthetic fraud dataset generated by GANs, <em class="pr">fake_df</em>. Here, we will preserve the test dataset as original by not involving it in this process.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="009e" class="qx oi fq qu b bg qy qz l ra rb">wdf = pd.concat([train_df, fake_df], axis=0)</span></pre><p id="19f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We will train the same ensemble method with the mixed balanced dataset to see if it will outperform the benchmark model.</p><p id="3830" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, we need to split the mixed balanced dataset into the features and the label.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="db83" class="qx oi fq qu b bg qy qz l ra rb">X_mixed = wdf[wdf.columns.drop("Class")]<br/>y_mixed = wdf["Class"]</span></pre><p id="d413" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Remember, when I ran the benchmark scenario earlier, I already defined the necessary custom function blocks to train and evaluate the ensemble classifier. I can use those custom functions here as well to train the same Ensemble algorithm with the combined balanced data.</p><p id="292d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can pass the features and the label (X_mixed, y_mixed) into the custom Ensemble Classifier function ensemble_training().</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="c039" class="qx oi fq qu b bg qy qz l ra rb">meta_learner_GANs, train_f1_scores_GANs, val_f1_scores_GANs, base_models_GANs=ensemble_training(X_mixed, y_mixed)</span></pre><p id="5ddd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Finally, we can evaluate the model with the test dataset.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="c181" class="qx oi fq qu b bg qy qz l ra rb">ensemble_evaluations(meta_learner_GANs, X_mixed, y_mixed, X_test, y_test)</span></pre><p id="feb3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the result.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="b5d7" class="qx oi fq qu b bg qy qz l ra rb">Ensemble Model Metrics:<br/>Training Precision: 1.0000, Recall: 0.9999, F1-score: 0.9999<br/>Test Precision: 0.9714, Recall: 0.7158, F1-score: 0.8242</span></pre></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="213c" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al"><em class="pd">Conclusion</em></strong></h1><p id="2c4c" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Finally, we can assess whether the data augmentation by GANs improved the performance of the classifier, as I expected.</p><p id="140c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s compare the evaluation metrics between the benchmark scenario and GANs scenario.</p><p id="3524" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the result from the benchmark scenario.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="36cb" class="qx oi fq qu b bg qy qz l ra rb"># The Benchmark Scenrio without data augmentation by GANs<br/>Training Precision: 0.9811, Recall: 0.9603, F1-score: 0.9706<br/>Test Precision: 0.9351, Recall: 0.7579, F1-score: 0.8372</span></pre><p id="5cc5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the result from GANs scenario.</p><pre class="mm mn mo mp mq qt qu qv bp qw bb bk"><span id="4faa" class="qx oi fq qu b bg qy qz l ra rb">Training Precision: 1.0000, Recall: 0.9999, F1-score: 0.9999<br/>Test Precision: 0.9714, Recall: 0.7158, F1-score: 0.8242</span></pre><p id="1aa4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When we review the evaluation results on the training dataset, clearly GANs scenario outperformed the benchmark scenario over all the three evaluation metrics.</p><p id="2e65" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Nevertheless, when we focus on the results on the out-of-sample test data, GANs scenario outperformed the benchmark scenario only for precision (Benchmark: 0.935 vs GANs Scenario: 0.9714): it failed do so for recall and F1-Score (Benchmark: 0.7579; 0.8372 vs GANs Scenario: 0.7158; 0.8242).</p><ul class=""><li id="7b61" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk">A higher precision means that the model’s prediction of fraud cases did include less proportion of non-fraud cases than the benchmark scenario.</li><li id="e6c5" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">A lower recall means that the model failed to detect certain varieties of the actual fraud cases.</li></ul><p id="db2d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">These two comparisons indicate: while the data augmentation by GANs was successful in simulating the realistic fraud data within the training dataset, it has failed to capture the diversity of the actual fraud cases included in the out-of-sample test dataset.</p><p id="29af" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">GANs was too good in simulating the particular probability distribution of the train data. Ironically, as a result, the use of GANs as the data augmentation tool, accounting for overfitting to the train data, resulted in a poor generalization of the resulting fraud detection (classification) model.</p><p id="1433" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Paradoxically, this particular example made a counter-intuitive case that a better sophisticated algorithm might not necessarily guarantee a better performance when compared with simpler conventional algorithms.</p><p id="7f3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In addition, we could also take into account of another unintended consequence, wasteful carbon footprint: adding energy demanding algorithms into your model development could increase the carbon footprint in the use of the machine learning in our daily life. This case could illustrate an example of an unnecessarily wasteful case which wasted energy unnecessarily without delivering a better performance.</p><p id="b607" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here I leave you some links regarding energy consumption of machine learning.</p><ul class=""><li id="5b3d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pj pk pl bk"><a class="af nc" href="https://spectrum.ieee.org/ai-energy-consumption" rel="noopener ugc nofollow" target="_blank">https://spectrum.ieee.org/ai-energy-consumption</a></li><li id="66e2" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk"><a class="af nc" href="https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2542435123003653%3Fshowall%3Dtrue" rel="noopener ugc nofollow" target="_blank">https://www.cell.com/joule/fulltext/S2542-4351(23)00365-3?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2542435123003653%3Fshowall%3Dtrue</a></li></ul><p id="d77f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Today, we have many variants of GANs. In the future article, I would like to explore other variants of GANs to see if any variant can capture a wider diversity of the original samples so that it can improve the performance of a fraud detector.</p><p id="8013" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thanks for reading.</p><p id="a6b1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Michio Suginoo</p><h1 id="841a" class="oh oi fq bf oj ok pw gq om on px gt op oq py os ot ou pz ow ox oy qa pa pb pc bk">REFERENCE</h1><ul class=""><li id="e540" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pj pk pl bk">Borji, A. (2018, 10 24). <em class="pr">Pros and Cons of GAN Evaluation Measures.</em> Retrieved from ArXvi: <a class="af nc" href="https://arxiv.org/abs/1802.03446" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.03446</a></li><li id="0793" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Goodfellow, I. (2015, 5 21). <em class="pr">On distinguishability criteria for estimating generative models.</em> Retrieved from ArXiv: <a class="af nc" href="https://arxiv.org/abs/1412.6515" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6515</a></li><li id="96a4" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozairy, S., . . . Bengioz, Y. (2014, 6 10). <em class="pr">Generative Adversarial Nets.</em> Retrieved from arXiv: <a class="af nc" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.2661</a></li><li id="251c" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozairy, S., . . . Bengioz, Y. (2014, 6 10). <em class="pr">Generative Adversarial Networks.</em> Retrieved from arXiv: <a class="af nc" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.2661</a></li><li id="5c87" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Knight, W. (2018, 8 17). <em class="pr">Fake America great again.</em> Retrieved from MIT Technology Review: <a class="af nc" href="https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/" rel="noopener ugc nofollow" target="_blank">https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/</a></li><li id="b7b0" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Suginoo, M. (2024, 1 13). <em class="pr">Mini-Max Optimization Design of Generative Adversarial Networks (GAN).</em> Retrieved from Towards Data Science: <a class="af nc" rel="noopener" target="_blank" href="/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02">https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02</a></li></ul></div></div></div></div>    
</body>
</html>