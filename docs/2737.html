<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AdaBoost Classifier, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AdaBoost Classifier, Explained: A Visual Guide with Code Examples</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10">https://towardsdatascience.com/adaboost-classifier-explained-a-visual-guide-with-code-examples-fc0f25326d7b?source=collection_archive---------1-----------------------#2024-11-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b235" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">ENSEMBLE LEARNING</h2><div/><div><h2 id="dd1a" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx"><strong class="al">Putting the weight where weak learners need it most</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--fc0f25326d7b--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fc0f25326d7b--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">1</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mo mp mq mr ms mt"><a rel="noopener follow" target="_blank" href="/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=post_page-----fc0f25326d7b--------------------------------"><div class="mu ab il"><div class="mv ab co cb mw mx"><h2 class="bf ga ib z it my iv iw mz iy ja fz bk">Random Forest, Explained: A Visual Guide with Code Examples</h2><div class="na l"><h3 class="bf b ib z it my iv iw mz iy ja dx">Making tree-mendous predictions with random trees</h3></div><div class="gq l"><p class="bf b dy z it my iv iw mz iy ja dx">towardsdatascience.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng lw mt"/></div></div></a></div><p id="bbee" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Everyone makes mistakes — even the simplest <a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">decision trees</a> in machine learning. Instead of ignoring them, AdaBoost (Adaptive Boosting) algorithm does something different: it learns (or <em class="oe">adapts</em>) from these mistakes to get better.</p><p id="ac77" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Unlike <a class="af od" rel="noopener" target="_blank" href="/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c">Random Fores</a>t, which makes many trees at once, AdaBoost starts with a single, simple tree and identifies the instances it misclassifies. It then builds new trees to fix those errors, learning from its mistakes and getting better with each step.</p><p id="30c4" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Here, we’ll illustrate exactly how AdaBoost makes its predictions, building strength by combining targeted weak learners just like a workout routine that turns focused exercises into full-body power.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og oh"><img src="../Images/38e85c5674011760cafd45ee4d7c85c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="b8a1" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Definition</h1><p id="db06" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">AdaBoost is an ensemble machine learning model that creates a sequence of weighted decision trees, typically using shallow trees (often just single-level “stumps”). Each tree is trained on the entire dataset, but with adaptive sample weights that give more importance to previously misclassified examples.</p><p id="9bc1" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For classification tasks, AdaBoost combines the trees through a weighted voting system, where better-performing trees get more influence in the final decision.</p><p id="67b7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The model’s strength comes from its adaptive learning process — while each simple tree might be a “weak learner” that performs only slightly better than random guessing, the weighted combination of trees creates a “strong learner” that <strong class="nj ga">progressively focuses on and corrects mistakes</strong>.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/5860f3e88183ae0db9f76a78c52525eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckqRCN3_pPKReooegdFgig.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">AdaBoost is part of the boosting family of algorithms because it builds trees one at a time. Each new tree tries to fix the mistakes made by the previous trees. It then uses a weighted vote to combine their answers and make its final prediction.</figcaption></figure><h1 id="1adf" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Dataset Used</h1><p id="bd4c" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Throughout this article, we’ll focus on the classic golf dataset as an example for classification.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/05586d1dcea17f8a18206b58019181ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0_DqZWXc5OM--Zxp3_uuw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Columns: ‘Outlook (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)</figcaption></figure><pre class="oi oj ok ol om qa qb qc bp qd bb bk"><span id="3b41" class="qe oz fq qb b bg qf qg l qh qi">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/># Create and prepare dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/># Prepare data<br/>df = pd.DataFrame(dataset_dict)<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Prepare features and target<br/>X,y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)Main Mechanism</span></pre><h1 id="0209" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Main Mechanism</h1><p id="0108" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Here’s how AdaBoost works:</p><ol class=""><li id="1e72" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qj qk ql bk"><strong class="nj ga">Initialize Weights:</strong> Assign equal weight to each training example.</li><li id="422e" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Iterative Learning:</strong> In each step, a simple decision tree is trained and its performance is checked. Misclassified examples get more weight, making them a priority for the next tree. Correctly classified examples stay the same, and all weights are adjusted to add up to 1.</li><li id="c167" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Build Weak Learners:</strong> Each new, simple tree targets the mistakes of the previous ones, creating a sequence of specialized weak learners.</li><li id="dc58" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><strong class="nj ga">Final Prediction:</strong> Combine all trees through weighted voting, where each tree’s vote is based on its importance value, giving more influence to more accurate trees.</li></ol><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/0e77ad190e2d8b5466b6debd4ecff4cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HImcqWdiiQnr84PBGWjf5Q.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">An AdaBoost Classifier makes predictions by using many simple decision trees (usually 50–100). Each tree, called a “stump,” focuses on one important feature, like temperature or humidity. The final prediction is made by combining all the trees’ votes, each weighted by how important that tree is (“alpha”).</figcaption></figure><h1 id="9bed" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Training Steps</h1><p id="1005" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Here, we’ll follow the SAMME (Stagewise Additive Modeling using a Multi-class Exponential loss function) algorithm, the standard approach in scikit-learn that handles both binary and multi-class classification.</p><p id="82bc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1.1. Decide the weak learner to be used. A one-level decision tree (or “stump”) is the default choice.<br/>1.2. Decide how many weak learner (in this case the number of trees) you want to build (the default is 50 trees).</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/9e914ecac19ae6b4d06dffb719872b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHe5z8F3Bi44Ymvv7WZw3Q.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">We begin with depth-1 decision trees (stumps) as our weak learners. Each stump makes just one split, and we’ll train 50 of them sequentially, adjusting weights along the way.</figcaption></figure><p id="53f2" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1.3. Start by giving each training example equal weight: <br/>· Each sample gets weight = 1/<em class="oe">N</em> (<em class="oe">N</em> is total number of samples)<br/>· All weights together sum to 1</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/a10a476d7edac1311aefbcf5e3b50dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfpe2J9VFxbBYETnY6ej2A.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">All data points start with equal weights (0.0714), with the total weight adding up to 1. This ensures every example is equally important when training begins.</figcaption></figure><h2 id="2cf8" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">For the First Tree</h2><p id="f2f8" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">2.1. Build a decision stump while considering sample weights</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/68a18d476059d011467b9290fe6574e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zc40sPBgtzp4sXvtxf1PCg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Before making the first split, the algorithm examines all data points with their weights to find the best splitting point. These weights influence how important each example is in making the split decision.</figcaption></figure><p id="bcf3" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">a. Calculate initial weighted Gini impurity for the root node</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/f8ad9f1f34d4f6cf8aa4ffa1096cec83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcwRF413IqEM0pDm02S9NQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The algorithm calculates the Gini impurity score at the root node, but now considers the weights of all data points.</figcaption></figure><p id="e302" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. For each feature: <br/>· Sort data by feature values (exactly like in <a class="af od" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree</a> classifier)</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/b62db5393a1ed6bdb676b65a3b9207dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KxeePluunbaamlyQDqPkg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">For each feature, the algorithm sorts the data and identifies potential split points, exactly like the standard Decision Tree.</figcaption></figure><p id="2537" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">· For each possible split point: <br/>·· Split samples into left and right groups <br/>·· Calculate weighted Gini impurity for both groups<br/>·· Calculate weighted Gini impurity reduction for this split</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/a825f39a5d40c52a1a8255495e984c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1OU5gEAtcqchsxZXNSPP6w.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The algorithm calculates weighted Gini impurity for each potential split and compares it to the parent node. For feature “sunny” with split point 0.5, this impurity reduction (0.066) shows how much this split improves the data separation.</figcaption></figure><p id="40ca" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">c. Pick the split that gives the largest Gini impurity reduction</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/5a4349476737e871ffda440a8e995b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3ZHYOmlPNixtXyasyLRdA.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">After checking all possible splits across features, the column ‘overcast’ (with split point 0.5) gives the highest impurity reduction of 0.102. This means it’s the most effective way to separate the classes, making it the best choice for the first split.</figcaption></figure><p id="1915" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. Create a simple one-split tree using this decision</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/e9cf90d6e8fe547e552f0af06cc220b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUogq6G1LLau6twn-Hkkgw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Using the best split point found, the algorithm divides the data into two groups, each keeping their original weights. This simple decision tree is purposely kept small and imperfect, making it just slightly better than random guessing.</figcaption></figure><p id="ba12" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.2. Evaluate how good this tree is <br/>a. Use the tree to predict the label of the training set. <br/>b. Add up the weights of all <strong class="nj ga">misclassified samples</strong> to get error rate</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/d3dcfbdd4971811d91c3ba1fb5d763c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCPNBt4i3TAwxGhsJE-Qyw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The first weak learner makes predictions <strong class="bf pa">on the training data</strong>, and we check where it made mistakes (marked with X). The error rate of 0.357 shows this simple tree gets some predictions wrong, which is expected and will help guide the next steps of training.</figcaption></figure><p id="ecb7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">c. Calculate tree importance (<em class="oe">α</em>) using: <br/><em class="oe">α</em> = learning_rate × log((1-error)/error)</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/e4a210b84e6605415fb8f6268044a9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngaJSgTGiV_R-drSpZS7Sw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Using the error rate, we calculate the tree’s influence score (α = 0.5878). Higher scores mean more accurate trees, and this tree earned moderate importance for its decent performance.</figcaption></figure><p id="3a9d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.3. Update sample weights <br/>a. Keep the original weights for correctly classified samples<br/>b. Multiply the weights of misclassified samples by e^(<em class="oe">α</em>). <br/>c. Divide each weight by the sum of all weights. This normalization ensures all weights still sum to 1 while maintaining their relative proportions.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/b4490670bf0192e1521f0f479a4c1873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLD4LOAPh5-CrJL-TNmyEw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Cases where the tree made mistakes (marked with X) get higher weights for the next round. After increasing these weights, all weights are normalized to sum to 1, ensuring misclassified examples get more attention in the next tree.</figcaption></figure><h2 id="a48b" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">For the Second Tree</h2><p id="8afc" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">2.1. Build a new stump, but now using the updated weights <br/>a. Calculate new weighted Gini impurity for root node: <br/>· Will be different because misclassified samples now have bigger weights <br/>· Correctly classified samples now have smaller weights</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/7e820aa41f2204a8607d14793adc59d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tHvdku0KKWNC6fXgpz6whg.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Using the updated weights (where misclassified examples now have higher importance), the algorithm calculates the weighted Gini impurity at the root node. This begins the process of building the second decision tree.</figcaption></figure><p id="719d" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">b. For each feature: <br/>· Same process as before, but the weights have changed<br/>c. Pick the split with best weighted Gini impurity reduction <br/>· Often completely different from the first tree’s split<br/>· Focuses on samples the first tree got wrong</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/8380dccedc96e8aad94425ca3ab24b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qW6ey3u87YuGozFWm6bosQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">With updated weights, different split points show different effectiveness. Notice that “overcast” is no longer the best split — the algorithm now finds temperature (84.0) gives the highest impurity reduction, showing how weight changes affect split selection.</figcaption></figure><p id="cd0f" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">d. Create the second stump</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/f1d85397a88dd3805b964813e8a5fd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0F0u95NJjp6tCOF51EH8yQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Using temperature ≤ 84.0 as the split point, the algorithm assigns YES/NO to each leaf based on which class has more total weight in that group, not just by counting examples. This weighted voting helps correct the previous tree’s mistakes.</figcaption></figure><p id="51f5" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2.2. Evaluate this new tree <br/>a. Calculate error rate with current weights <br/>b. Calculate its importance (<em class="oe">α</em>) using the same formula as before<br/>2.3. Update weights again — Same process: increase weights for mistakes then normalize.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/0773fcc8ffd93058f55eebccc00b9c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T2UO2T8PSgJ61uId3Li60A.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The second tree achieves a lower error rate (0.222) and higher importance score (α = 1.253) than the first tree. Like before, misclassified examples get higher weights for the next round.</figcaption></figure><h2 id="429a" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">For the Third Tree onwards</h2><p id="6e12" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Repeat Step 2.1–2.3 for all remaining trees.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/d19ad8eddbd4588dac4781b288276203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V0WyZpspm4Rg65jYxVNEeA.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The algorithm builds 50 simple decision trees sequentially, each with its own importance score (α). Each tree learns from previous mistakes by focusing on different aspects of the data, creating a strong combined model. Notice how some trees (like Tree 2) get higher importance scores when they perform better.</figcaption></figure><p id="6ba0" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj ga">Step 3: Final Ensemble<br/></strong>3.1. Keep all trees and their importance scores</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/4f0e5ae526c79b2e46766e4e85bfdb13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0VFjJSLP1mP8gDmChpjag.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">The 50 simple decision trees work together as a team, each with its own importance score (α). When making predictions, trees with higher α values (like Tree 2 with 1.253) have more influence on the final decision than trees with lower scores.</figcaption></figure><pre class="oi oj ok ol om qa qb qc bp qd bb bk"><span id="0274" class="qe oz fq qb b bg qf qg l qh qi">from sklearn.tree import plot_tree<br/>from sklearn.ensemble import AdaBoostClassifier<br/>from sklearn.tree import plot_tree<br/>import matplotlib.pyplot as plt<br/><br/># Train AdaBoost<br/>np.random.seed(42)  # For reproducibility<br/>clf = AdaBoostClassifier(algorithm='SAMME', n_estimators=50, random_state=42)<br/>clf.fit(X_train, y_train)<br/><br/># Create visualizations for trees 1, 2, and 50<br/>trees_to_show = [0, 1, 49]<br/>feature_names = X_train.columns.tolist()<br/>class_names = ['No', 'Yes']<br/><br/># Set up the plot<br/>fig, axes = plt.subplots(1, 3, figsize=(14,4), dpi=300)<br/>fig.suptitle('Decision Stumps from AdaBoost', fontsize=16)<br/><br/># Plot each tree<br/>for idx, tree_idx in enumerate(trees_to_show):<br/>    plot_tree(clf.estimators_[tree_idx],<br/>              feature_names=feature_names,<br/>              class_names=class_names,<br/>              filled=True,<br/>              rounded=True,<br/>              ax=axes[idx],<br/>              fontsize=12)  # Increased font size<br/>    axes[idx].set_title(f'Tree {tree_idx + 1}', fontsize=12)<br/><br/>plt.tight_layout(rect=[0, 0.03, 1, 0.95])</span></pre><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og rh"><img src="../Images/c3c0699734afabfd3e5c095b1571e9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4bJQUMN1h4Ylh23xPvO_w.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">Each node shows its ‘value’ parameter as [weight_NO, weight_YES], which represents the weighted proportion of each class at that node. These weights come from the sample weights we calculated during training.</figcaption></figure><h2 id="b45a" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Testing Step</h2><p id="9fba" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">For predicting: <br/>a. Get each tree’s prediction<br/>b. Multiply each by its importance score (<em class="oe">α</em>) <br/>c. Add them all up <br/>d. The class with higher total weight will be the final prediction</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/c9a49e61ccbf0b28a73a1ace1a9fd700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z61xGdKzgtZ64deolN8elw.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">When predicting for new data, each tree makes its prediction and multiplies it by its importance score (α). The final decision comes from adding up all weighted votes — here, the NO class gets a higher total score (23.315 vs 15.440), so the model predicts NO for this unseen example.</figcaption></figure><h2 id="5267" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Evaluation Step</h2><p id="01c9" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">After building all the trees, we can evaluate the test set.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/0e4cb581d6d058a44ad7f63b4a7dd494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5IRLz_4CvkT1EMI34crqQ.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">By iteratively training and weighting weak learners to focus on misclassified examples, AdaBoost creates a strong classifier that achieves high accuracy — typically better than single decision trees or simpler models!</figcaption></figure><pre class="oi oj ok ol om qa qb qc bp qd bb bk"><span id="92c6" class="qe oz fq qb b bg qf qg l qh qi"># Get predictions<br/>y_pred = clf.predict(X_test)<br/><br/># Create DataFrame with actual and predicted values<br/>results_df = pd.DataFrame({<br/>    'Actual': y_test,<br/>    'Predicted': y_pred<br/>})<br/>print(results_df) # Display results DataFrame<br/><br/># Calculate and display accuracy<br/>from sklearn.metrics import accuracy_score<br/>accuracy = accuracy_score(y_test, y_pred)<br/>print(f"\nModel Accuracy: {accuracy:.4f}")</span></pre><h1 id="5328" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Key Parameters</h1><p id="46f2" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Here are the key parameters for AdaBoost, particularly in <code class="cx ri rj rk qb b">scikit-learn</code>:</p><p id="768e" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx ri rj rk qb b">estimator</code>: This is the base model that AdaBoost uses to build its final solution. The 3 most common weak learners are:<br/><strong class="nj ga">a. Decision Tree with depth 1 (Decision Stump)</strong>: This is the default and most popular choice. Because it only has one split, it is considered a very weak learner that is just a bit better than random guessing, exactly what is needed for boosting process.<br/><strong class="nj ga">b. Logistic Regression</strong>: Logistic regression (especially with high-penalty) can also be used here even though it is not really a weak learner. It could be useful for data that has linear relationship.<br/><strong class="nj ga">c. Decision Trees with small depth (e.g., depth 2 or 3)</strong>: These are slightly more complex than decision stumps. They’re still fairly simple, but can handle slightly more complex patterns than the decision stump.</p><figure class="oi oj ok ol om on of og paragraph-image"><div role="button" tabindex="0" class="oo op ed oq bh or"><div class="of og pz"><img src="../Images/72829a3081a99f628be3a36d2be8ad47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuHumEJDnKZ9fEm6crckFA.png"/></div></div><figcaption class="ot ou ov of og ow ox bf b bg z dx">AdaBoost’s base models can be simple decision stumps (depth=1), small trees (depth 2–3), or penalized linear models. Each type is kept simple to avoid overfitting while offering different ways to capture patterns.</figcaption></figure><p id="2923" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx ri rj rk qb b">n_estimators</code>: The number of weak learners to combine, typically around 50–100. Using more than 100 rarely helps.</p><p id="e357" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><code class="cx ri rj rk qb b">learning_rate</code>: Controls how much each classifier affects the final result. Common starting values are 0.1, 0.5, or 1.0. Lower numbers (like 0.1) and a bit higher <code class="cx ri rj rk qb b">n_estimator</code> usually work better.</p><h2 id="8070" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Key differences from Random Forest</h2><p id="fd53" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">As both Random Forest and AdaBoost works with multiple trees, it is easy to confuse the parameters involved. The key difference is that Random Forest combines many trees <strong class="nj ga">independently</strong> (bagging) while AdaBoost builds trees <strong class="nj ga">one after another</strong> to fix mistakes (boosting). Here are some other details about their differences:</p><ol class=""><li id="9a04" class="nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qj qk ql bk">No <code class="cx ri rj rk qb b">bootstrap</code> parameter because AdaBoost uses all data but with changing weights</li><li id="21ba" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">No <code class="cx ri rj rk qb b">oob_score</code> because AdaBoost doesn't use bootstrap sampling</li><li id="db46" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk"><code class="cx ri rj rk qb b">learning_rate</code> becomes crucial (not present in Random Forest)</li><li id="9dc3" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">Tree depth is typically kept very shallow (usually just stumps) unlike Random Forest’s deeper trees</li><li id="bf7d" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc qj qk ql bk">The focus shifts from parallel independent trees to sequential dependent trees, making parameters like <code class="cx ri rj rk qb b">n_jobs</code> less relevant</li></ol><h1 id="82ad" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Pros &amp; Cons</h1><h2 id="8f0a" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Pros:</h2><ul class=""><li id="768f" class="nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc rl qk ql bk"><strong class="nj ga">Adaptive Learning:</strong> AdaBoost gets better by giving more weight to mistakes it made. Each new tree pays more attention to the hard cases it got wrong.</li><li id="d85e" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc rl qk ql bk"><strong class="nj ga">Resists Overfitting:</strong> Even though it keeps adding more trees one by one, AdaBoost usually doesn’t get too focused on training data. This is because it uses weighted voting, so no single tree can control the final answer too much.</li><li id="f8e0" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc rl qk ql bk"><strong class="nj ga">Built-in Feature Selection:</strong> AdaBoost naturally finds which features matter most. Each simple tree picks the most useful feature for that round, which means it automatically selects important features as it trains.</li></ul><h2 id="3abf" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Cons:</h2><ul class=""><li id="bddf" class="nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc rl qk ql bk"><strong class="nj ga">Sensitive to Noise:</strong> Because it gives more weight to mistakes, AdaBoost can have trouble with messy or wrong data. If some training examples have wrong labels, it might focus too much on these bad examples, making the whole model worse.</li><li id="0b37" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc rl qk ql bk"><strong class="nj ga">Must Be Sequential:</strong> Unlike Random Forest which can train many trees at once, AdaBoost must train one tree at a time because each new tree needs to know how the previous trees did. This makes it slower to train.</li><li id="260b" class="nh ni fq nj b gt qm nl nm gw qn no np nq qo ns nt nu qp nw nx ny qq oa ob oc rl qk ql bk"><strong class="nj ga">Learning Rate Sensitivity:</strong> While it has fewer settings to tune than Random Forest, the learning rate really affects how well it works. If it’s too high, it might learn the training data too exactly. If it’s too low, it needs many more trees to work well.</li></ul><h1 id="f1b8" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">Final Remarks</h1><p id="9f03" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">AdaBoost is a key boosting algorithm that many newer methods learned from. Its main idea — getting better by focusing on mistakes — has helped shape many modern machine learning tools. While other methods try to be perfect from the start, AdaBoost tries to show that sometimes the best way to solve a problem is to learn from your errors and keep improving.</p><p id="02d7" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">AdaBoost also works best in binary classification problems and when your data is clean. While Random Forest might be better for more general tasks (like predicting numbers) or messy data, AdaBoost can give really good results when used in the right way. The fact that people still use it after so many years shows just how well the core idea works!</p><h1 id="e2db" class="oy oz fq bf pa pb pc gv pd pe pf gy pg ph pi pj pk pl pm pn po pp pq pr ps pt bk">🌟 AdaBoost Classifier Code Summarized</h1><pre class="oi oj ok ol om qa qb qc bp qd bb bk"><span id="7e2a" class="qe oz fq qb b bg qf qg l qh qi">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.ensemble import AdaBoostClassifier<br/>from sklearn.tree import DecisionTreeClassifier<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', <br/>                'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy',<br/>                'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast',<br/>                'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0,<br/>                   72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0,<br/>                   88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0,<br/>                 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0,<br/>                 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True,<br/>             True, False, True, True, False, False, True, False, True, True, False,<br/>             True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes',<br/>             'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split features and target<br/>X, y = df.drop('Play', axis=1), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Train AdaBoost<br/>ada = AdaBoostClassifier(<br/>    estimator=DecisionTreeClassifier(max_depth=1), # Create base estimator (decision stump)<br/>    n_estimators=50,        # Typically fewer trees than Random Forest<br/>    learning_rate=1.0,      # Default learning rate<br/>    algorithm='SAMME',      # The only currently available algorithm (will be removed in future scikit-learn updates)<br/>    random_state=42<br/>)<br/>ada.fit(X_train, y_train)<br/><br/># Predict and evaluate<br/>y_pred = ada.predict(X_test)<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre></div></div></div><div class="ab cb rm rn ro rp" role="separator"><span class="rq by bm rr rs rt"/><span class="rq by bm rr rs rt"/><span class="rq by bm rr rs"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b432" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Further Reading</h2><p id="9495" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">For a detailed explanation of the <a class="af od" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" rel="noopener ugc nofollow" target="_blank">AdaBoostClassifier</a> and its implementation in scikit-learn, readers can refer to the official documentation, which provides comprehensive information on its usage and parameters.</p><h2 id="6253" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">Technical Environment</h2><p id="5264" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">This article uses Python 3.7 and scikit-learn 1.6. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="dcdb" class="qr oz fq bf pa qs qt qu pd qv qw qx pg nq qy qz ra nu rb rc rd ny re rf rg fw bk">About the Illustrations</h2><p id="d4cc" class="pw-post-body-paragraph nh ni fq nj b gt pu nl nm gw pv no np nq pw ns nt nu px nw nx ny py oa ob oc fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="9adc" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it ru rv bp rw lw ao"><div class="rx l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ry rz cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ry rz em n ay uj"/></div><div class="sa l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sd hp l"><h2 class="bf ga xg ic it xh iv iw mz iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xi wi wj wk wl lj wm wn uu ii wo wp wq uy uz va ep bm vb ou" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----fc0f25326d7b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xj l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sm dz sn it ab so il ed"><div class="ed sg bx sh si"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sg bx kk sj sk"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sl sk"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div><p id="cb13" class="pw-post-body-paragraph nh ni fq nj b gt nk nl nm gw nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="mo mp mq mr ms"><div role="button" tabindex="0" class="ab bx cp kj it ru rv bp rw lw ao"><div class="rx l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by ry rz cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l ry rz em n ay uj"/></div><div class="sa l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----fc0f25326d7b--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq sd hp l"><h2 class="bf ga xg ic it xh iv iw mz iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk xi wi wj wk wl lj wm wn uu ii wo wp wq uy uz va ep bm vb ou" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fc0f25326d7b--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xj l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sm dz sn it ab so il ed"><div class="ed sg bx sh si"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sg bx kk sj sk"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sl sk"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>