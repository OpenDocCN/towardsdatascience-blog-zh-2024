- en: 'Unleashing the Power of Triton: Mastering GPU Kernel Optimization in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e?source=collection_archive---------7-----------------------#2024-08-13](https://towardsdatascience.com/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e?source=collection_archive---------7-----------------------#2024-08-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accelerating AI/ML Model Training with Custom Operators — Part 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://chaimrand.medium.com/?source=post_page---byline--160a3f52701e--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page---byline--160a3f52701e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--160a3f52701e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--160a3f52701e--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page---byline--160a3f52701e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--160a3f52701e--------------------------------)
    ·10 min read·Aug 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2dd5b644c445a53fd9b98179de44e93.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jas Rolyn](https://unsplash.com/@jasrolyn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: According to Greek mythology, Triton, a god of the sea, would calm or stir the
    sea waters by using his conch shell to control its tides and waves. In one story,
    in particular, Triton is depicted as having used his powers to guide [the Argonauts](https://en.wikipedia.org/wiki/Argonauts)
    through particularly dangerous sea waters. In this post, we similarly call upon
    Triton for navigation through complex journeys, although this time we refer to
    the [Triton language](https://github.com/triton-lang/triton) and compiler for
    writing deep learning (DL) kernels and to our journeys through the world of AI/ML
    development.
  prefs: []
  type: TYPE_NORMAL
- en: This is a sequel to a [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    on the topic of accelerating AI/ML applications with custom operators in which
    we demonstrated the potential for performance optimization by developing custom
    CUDA kernels. One of our intentions was to emphasize the accessibility of custom
    kernel development and the opportunities it provides even for non-expert CUDA
    developers. However, there are challenges to CUDA development that may prove insurmountable
    for some. For one, while many a modern-day AI/ML developer are well-versed in
    Python, they may not feel comfortable developing in C++. Furthermore, tuning a
    CUDA kernel to take full advantage of the GPU’s capabilities requires an intimate
    understanding of the underlying HW architecture and could take a non-trivial amount
    of work. This is particularly true if you want your kernel to run optimally on
    a variety of GPU architectures. Much of the complexity results from CUDA’s “thread-based”
    development model in which the developer is responsible for designing and optimizing
    all elements of the GPU kernel threads, including all details related to the use
    of GPU memory, thread-concurrency, TensorCore scheduling, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The Power of Triton
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Triton library aims to democratize and simplify GPU kernel development in
    two primary ways. First, it provides an API for building custom operators in *Python*
    (rather than C++). Second, it enables kernel development at the *block* level
    (rather than the thread level) thereby abstracting away and automating all issues
    related to optimizing performance *within* CUDA thread blocks. Rather than taking
    the laborious steps of programming the details of the thread invocation, including
    the intricacies related to memory management, scheduling of on-chip acceleration
    engines, thread-synchronization, etc., kernel developers can rely on Triton to
    do it all for them. One important byproduct of the high-level API abstraction
    of Triton’s programming model is that it reduces the burden of needing to tune
    the kernel for multiple different GPU types and architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as is usually the case when up-leveling an API, the Triton programming
    model does have its disadvantages. Some kernels might benefit from the thread-level
    control enabled by CUDA (e.g., they might benefit from the conditional execution
    flow discussed in our [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)).
    Other kernels might require very specialized and delicate treatment to reach peak
    performance and may suffer from the automated result of the Triton compiler. But
    even in cases such as these, where the development of a CUDA kernel may ultimately
    be required, the ability to quickly and easily create a temporary Triton kernel
    could greatly facilitate development and boost productivity.
  prefs: []
  type: TYPE_NORMAL
- en: For more on the motivations behind Triton and on the details of its programming
    model, see the [Triton announcement](https://openai.com/index/triton/), the official
    [Triton documentation](https://triton-lang.org/main/programming-guide/chapter-1/introduction.html#motivations),
    and the original [Triton white-paper](https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to our [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12),
    our intention is to provide a simple demonstration of the opportunity offered
    by Triton. Please do not view this post as a replacement for the official [Triton
    documentation](https://triton-lang.org/main/index.html) or its [associated tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html).
    We will use the same face-detection model as in our [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    as a basis for our demonstration and perform our experiments in the same Google
    Cloud environment — a [g2-standard-16](https://cloud.google.com/compute/docs/gpus#l4-gpus)
    VM (with a single L4 GPU) with a dedicated [deep learning VM image](https://cloud.google.com/deep-learning-vm/docs/release-notes)
    and PyTorch 2.4.0\. As before, we make no effort to optimize our examples and/or
    verify their robustness, durability, or accuracy. It should be noted that although
    we will perform our experiments on a PyTorch model and on an NVIDIA GPU, Triton
    kernel development is supported by additional frameworks and underlying HWs.
  prefs: []
  type: TYPE_NORMAL
- en: Triton as a Component of Torch Compilation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous posts (e.g., [here](/pytorch-model-performance-analysis-and-optimization-10c3c5822869))
    we demonstrated the use of [PyTorch compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)
    and its potential impact on runtime performance. The default compiler used by
    the [torch.compiler](https://pytorch.org/docs/stable/torch.compiler.html#torch-compiler)
    is [TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)
    which relies heavily on Triton kernels for its GPU acceleration. Thus, it seems
    only appropriate that we begin our Triton exploration by assessing the automatic
    Triton-backed optimization afforded by [torch.compile](https://pytorch.org/docs/stable/generated/torch.compile.html).
    The code block below includes the same forward pass of the face detection model
    we introduced in our previous post along with the compiled GIOU loss function.
    For the sake of brevity, we have omitted some of the supporting code. Please refer
    to our [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    for the full implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance results (averaged over multiple runs) are captured below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Recall that the average time of the original loss function (on padded input)
    was 1.844ms. Thus the performance boost resulting from torch compilation is greater
    than 2X(!!).
  prefs: []
  type: TYPE_NORMAL
- en: The Triton kernels automatically generated by torch.compile can actually be
    viewed by setting the `TORCH_LOGS` environment variable, as explained in [this
    PyTorch tutorial](https://pytorch.org/tutorials/recipes/torch_logs.html). In fact,
    some have proposed the use of these kernels as a starting point for Triton development
    (e.g., see [here](https://discuss.pytorch.org/t/choice-of-torch-compile-vs-triton/195604)).
    However, in our experience these kernels can be somewhat difficult to decipher.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will attempt to further improve on the results of PyTorch
    compilation by implementing a GIOU Triton kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Custom Triton Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great place to start your Triton development journey is with the [official
    Triton tutorials](https://triton-lang.org/main/getting-started/tutorials/index.html).
    The tutorials are introduced in incremental order of complexity, with each one
    expanding on one or more of Triton’s unique features. Our GIOU Triton kernel most
    closely resembles the most basic [vector addition](https://triton-lang.org/main/getting-started/tutorials/01-vector-add.html#sphx-glr-getting-started-tutorials-01-vector-add-py)
    example. As in our [CUDA implementation](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12),
    we assign a block to each sample in the input batch, and program it to operate
    on all of the bounding boxes in the sample. Note the use of [tl.load](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12)
    and [tl.store](https://triton-lang.org/main/python-api/generated/triton.language.store.html)
    for reading and writing data from and to memory, as well as the block programs
    use of vectorized arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The results of running with our Triton kernel are captured below. While somewhat
    worse than in our previous experiment, this could be a result of additional optimizations
    performed by torch.compile.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Following the recommendation of PyTorch’s documentation on [the use of Triton
    kernels](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html),
    we further assess the performance of our kernel, this time in combination with
    [PyTorch compilation](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).
    The results (averaged over multiple runs) are slightly better than the auto-compiled
    loss of our first experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When developing [our custom GIOU CUDA kernel](/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12),
    we noted the overhead of converting the input tensors to float32, and the need
    to enhance our kernel to support various input types in order to avoid this conversion.
    In the case of our Triton kernel this can be accomplished quite easily by simply
    removing the conversion operations. The custom kernel will be auto-generated (JIT-compiled)
    with the original types.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our final results are on par with CUDA kernel results that we saw in our [previous
    post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12).
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following table summarizes the results of our experimentation. The results
    were averaged over multiple runs due to some variance that we observed. We have
    included the results of our custom CUDA kernel from our [previous post](https://chaimrand.medium.com/accelerating-ai-ml-model-training-with-custom-operators-163ef2a04b12),
    for reference. Keep in mind that the comparative results are likely to vary greatly
    based on the details of the kernel and the runtime environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e85a9ec9d7a80826abc22c86f922dd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary of Average of Loss Runtimes (by Author)
  prefs: []
  type: TYPE_NORMAL
- en: While our first Triton kernel experiment resulted in reduced performance, compared
    to our custom CUDA operator, by applying compilation and removing the data type
    conversions, we were able to match its speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'These findings are in line with what one might expect from Triton: On the one
    hand, its high-level API abstraction implies a certain loss of control over the
    low-level flow which could result in reduced runtime performance. On the other
    hand, the (relative) simplicity and power of its APIs enable users to close the
    performance gap by implementing features with much greater ease than in CUDA.'
  prefs: []
  type: TYPE_NORMAL
- en: One could make a strong argument that the Triton kernel we chose to evaluate
    is what [the documentation](https://openai.com/index/triton/) would refer to as
    “embarrassingly parallel”, i.e., comprised of element-wise operations, and that
    as such, is a terrible kernel on which to demonstrate the value of Triton. Indeed,
    a more complex program, requiring more sophisticated memory management, scheduling,
    synchronization, etc., may be required to showcase the full power of Triton.
  prefs: []
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Several additional steps are required to complete our task. These include tuning
    our custom kernel and implementing the backward function.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Kernel Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although, Triton abstracts away a lot of the low-level kernel optimization,
    there remain many controls that could greatly impact runtime performance. These
    include the size of each block, the number of thread warps to use (as demonstrated
    in the [softmax tutorial](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py)),
    and how L2 memory is accessed (see the [matrix multiplication tutorial](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)
    for an example of *swizzling*). Triton includes an [autotuning](https://triton-lang.org/main/python-api/generated/triton.autotune.html)
    feature for optimizing the choice of hyper-parameters (as demonstrated in the
    [matrix multiplication tutorial](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)
    and in the [PyTorch Triton example](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html#advanced-usage)).
    Although we have omitted autotuning from our example, it is an essential step
    of Triton kernel development.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Backward Pass Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have limited our example to just the forward pass of the GIOU loss function.
    A full solution would require creating a kernel for the backward pass, as well
    (as demonstrated in the [layer normalization](https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html)
    tutorial). This is usually a bit more complicated than the forward pass. One may
    wonder why the high-level kernel development API exposed by Triton does not address
    this challenge by supporting automatic differentiation. As it turns out, for reasons
    that are beyond the scope of this post (e.g., see [here](https://jax.readthedocs.io/en/latest/pallas/design.html#grad-of-pallas-call)),
    automatic differentiation of custom kernels is extremely difficult to implement.
    Nonetheless, this would be an absolute killer of a feature for Triton and we can
    only hope that this will be supported at some point in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Triton is easily one of the most important and impactful AI/ML libraries of
    the past few years. While it is difficult to assess the amount of innovation and
    progress it has enabled in the field of AI, its footprints can be found everywhere
    — from the core implementation of PyTorch 2 and its dependencies, to the [specialized
    attention layers](https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html#sphx-glr-getting-started-tutorials-06-fused-attention-py)
    within the advanced LLM models that are slowly perforating our every day lives.
  prefs: []
  type: TYPE_NORMAL
- en: Triton’s popularity is owed to its innovative programming model for kernel development.
    Once limited to the domain of CUDA experts, Triton makes creating customized DL
    primitives accessible to every Python developer.
  prefs: []
  type: TYPE_NORMAL
- en: In this post we have only touched the surface of Triton and its capabilities.
    Be sure to check out the Triton’s online [documentation](https://triton-lang.org/main/index.html)
    and other [resources](https://github.com/triton-lang/triton) to learn more.
  prefs: []
  type: TYPE_NORMAL
