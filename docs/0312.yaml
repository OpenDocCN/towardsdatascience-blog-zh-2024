- en: Top Evaluation Metrics for RAG Failures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02](https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/303836d4eb6b35c4a200aa1fd4ef5171.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshoot LLMs and Retrieval Augmented Generation with Retrieval and Response
    Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[![Amber
    Roberts](../Images/ee686891eeedca7f33a63e147cc2c086.png)](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    [Amber Roberts](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    ·7 min read·Feb 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be678bd9a80b7f2b544ca337c3da88a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Root Cause Workflows for LLM RAG Applications (flowchart created
    by author)'
  prefs: []
  type: TYPE_NORMAL
- en: If you have been experimenting with large language models (LLMs) for search
    and retrieval tasks, you have likely come across retrieval augmented generation
    (RAG) as a technique to add relevant contextual information to LLM generated responses.
    By connecting an LLM to private data, RAG can enable a better response by feeding
    relevant data in the context window.
  prefs: []
  type: TYPE_NORMAL
- en: RAG has been shown to be highly effective for complex query answering, knowledge-intensive
    tasks, and enhancing the precision and relevance of responses for AI models, especially
    in situations where standalone training data may fall short.
  prefs: []
  type: TYPE_NORMAL
- en: However, these benefits from RAG can only be reaped if you are continuously
    monitoring your LLM system at common failure points — most notably with response
    and retrieval evaluation metrics. In this piece we will go through the best workflows
    for troubleshooting poor retrieval and response metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Retrieval and Responses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s worth remembering that **RAG works best when required information is readily
    available**. Whether relevant documents are availablefocuses RAG system evaluations
    on two critical aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Retrieval Evaluation:*** To assess the accuracy and relevance of the documents
    that were retrieved'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Response Evaluation:*** Measure the appropriateness of the response generated
    by the system when the context was provided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/0a43ebe03eca8b3e0fdb478c80064940.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Response Evals and Retrieval Evals in an LLM Application (image by
    author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Response Evaluation Metrics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/c69acf88763ec390f93559601d082b09.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Retrieval Evaluation Metrics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/907ac76c31da0365dbf548f1907aa30e.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 2 by author
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting RAG Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s review three potential scenarios to troubleshoot poor LLM performance
    based on the flow diagram.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 1: Good Response, Good Retrieval'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/28c5088db40d603793b39beac7b19aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario everything in the LLM application is acting as expected and
    we have a good response with a good retrieval. We find our response evaluation
    is “correct” and our “Hit = True.” Hit is a binary metric, where “True” means
    the relevant document was retrieved and “False” would mean the relevant document
    was not retrieved. Note that the aggregate statistic for Hit is the Hit rate (percent
    of queries that have relevant context).
  prefs: []
  type: TYPE_NORMAL
- en: For our response evaluations, correctness is an evaluation metric that can be
    done simply with a combination of the **input** (query), **output** (response),
    and **context** as can be seen in *Table 1*. Several of these evaluation criteria
    do not require user labeled ground-truth labels since LLMs can also be used to
    generate labels, scores, and explanations with tools like the [OpenAI function
    calling](https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/),
    below is an example prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1776a24f280136742b49ffb25f859d1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: These [LLM evals](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    can be formatted as numeric, categorical (binary and multi-class) and multi-output
    (multiple scores or labels) — with categorical-binary being the most commonly
    used and numeric being the least commonly used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 2: Bad Response, Bad Retrieval'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/478d15be23dedf0f5031c07c5a2edf9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario we find that the response is incorrect and the relevant content
    was not received. Based on the query we see that the content wasn’t received because
    there is no solution to the query. The LLM cannot predict future purchases no
    matter what documents it is supplied. However, the LLM can generate a better response
    than to hallucinate an answer. Here it would be to experiment with the prompt
    that is generating the response by simply adding a line to the LLM prompt template
    of “*if relevant content is not provided and no conclusive solution is found,
    respond that the answer is unknown.”* In some cases the correct answer is that
    the answer does not exist.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/124665fbc597674323ef4459f78ff639.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Scenario 3: Bad Response, Mixed Retrieval Metrics'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this third scenario, we see an incorrect response with mixed retrieval metrics
    (the relevant document was retrieved, but the LLM hallucinated an answer due to
    being given too much information).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09c89963a3710d41d9c93d128d5c8899.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate an LLM RAG system, you need to both fetch the right context and
    then generate an appropriate answer. Typically, developers will embed a user query
    and use it to search a vector database for relevant chunks (see Figure 3). Retrieval
    performance hinges not only on the returned chunks being semantically similar
    to the query, but on whether those chunks provide enough relevant information
    to generate the correct response to the query. Now, you must configure the parameters
    around your RAG system (type of retrieval, chunk size, and K).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a37b959baffef3f83e5a1bd9958b4b25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: RAG Framework (by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly with our last scenario, we can try editing the prompt template or
    change out the LLM being used to generate responses. Since the relevant content
    is retrieved during the document retrieval process but isn’t being surfaced by
    the LLM, this could be a quick solution. Below is an example of a correct response
    generated from running a revised prompt template (after iterating on prompt variables,
    LLM parameters, and the prompt template itself).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52689ef6524a0041ad99e1a1373f7907.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: When troubleshooting bad responses with mixed performance metrics, we need to
    first figure out which retrieval metrics are underperforming. The easiest way
    of doing this is to implement thresholds and monitors. Once you are alerted to
    a particular underperforming metric you can resolve with specific workflows. Let’s
    take nDCG for example. nDCG is used to measure the effectiveness of your top ranked
    documents and takes into account the position of relevant docs, so if you retrieve
    your relevant document (Hit = ‘True’), you will want to consider implementing
    a reranking technique to get the relevant documents closer to the top ranked search
    results.
  prefs: []
  type: TYPE_NORMAL
- en: For our current scenario we retrieved a relevant document (Hit = ‘True’), and
    that document is in the first position, so let’s try and improve the precision
    (percent relevant documents) up to ‘K’ retrieved documents. Currently our Precision@4
    is 25%, but if we used only the first two relevant documents then Precision@2
    = 50% since half of the documents are relevant. This change leads to the correct
    response from the LLM since it is given less information, but more relevant information
    proportionally.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df1e002e326d5b7f14e8881ce5e0f6cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram by author
  prefs: []
  type: TYPE_NORMAL
- en: Essentially what we were seeing here is a common problem in RAG known as [lost
    in the middle](https://arxiv.org/abs/2307.03172), when your LLM is overwhelmed
    with too much information that is not always relevant and then is unable to give
    the best answer possible. From our diagram, we see that adjusting your chunk size
    is one of the first things many teams do to improve RAG applications but it’s
    not always intuitive. With context overflow and lost in the middle problems, more
    documents isn’t always better, and reranking won’t necessarily improve performance.
    To evaluate which chunk size works best, you need to define an eval benchmark
    and do a sweep over chunk sizes and top-k values. In addition to experimenting
    with chunking strategies, testing out different text extraction techniques and
    embedding methods will also improve overall RAG performance.
  prefs: []
  type: TYPE_NORMAL
- en: Response and Retrieval Evaluation Metrics Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The response and retrieval evaluation metrics and approaches in [this piece](https://arize.com/blog-course/rag-evaluation/)
    offer a comprehensive way to view an LLM RAG system’s performance, guiding developers
    and users in understanding its strengths and limitations. By continually evaluating
    these systems against these metrics, improvements can be made to enhance RAG’s
    ability to provide accurate, relevant, and timely information.
  prefs: []
  type: TYPE_NORMAL
- en: Additional advanced methods for improving RAG include [re-ranking](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b),
    metadata attachments, testing out different embedding models, testing out different
    indexing methods, implementing [HyDE](https://arize.com/blog/hyde-paper-reading-and-discussion/),
    implementing keyword search methods, or implementing Cohere document mode (similar
    to HyDE). Note that while these more advanced methods — like chunking, text extraction,
    embedding model experimentation — may produce more contextually coherent chunks,
    these methods are more resource-intensive. Using RAG along with advanced methods
    can make performance improvements to your LLM system and will continue to do so
    as long as your retrieval and response metrics are properly monitored and maintained.
  prefs: []
  type: TYPE_NORMAL
- en: '*Questions? Please reach out to me here or on* [*LinkedIn*](https://www.linkedin.com/in/amber-roberts42/)*,*
    [*X*](https://twitter.com/astronomeramber)*, or* [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*!*'
  prefs: []
  type: TYPE_NORMAL
