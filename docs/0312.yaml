- en: Top Evaluation Metrics for RAG Failures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG失败的顶级评估指标
- en: 原文：[https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02](https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02](https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02)
- en: '![](../Images/303836d4eb6b35c4a200aa1fd4ef5171.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/303836d4eb6b35c4a200aa1fd4ef5171.png)'
- en: Image created by author using Dall-E 3
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用Dall-E 3创建
- en: Troubleshoot LLMs and Retrieval Augmented Generation with Retrieval and Response
    Metrics
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用检索和响应指标排查LLM和检索增强生成问题
- en: '[](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[![Amber
    Roberts](../Images/ee686891eeedca7f33a63e147cc2c086.png)](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    [Amber Roberts](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[![Amber
    Roberts](../Images/ee686891eeedca7f33a63e147cc2c086.png)](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    [Amber Roberts](https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    ·7 min read·Feb 2, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------)
    ·7分钟阅读·2024年2月2日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/be678bd9a80b7f2b544ca337c3da88a6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be678bd9a80b7f2b544ca337c3da88a6.png)'
- en: 'Figure 1: Root Cause Workflows for LLM RAG Applications (flowchart created
    by author)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM RAG应用的根本原因工作流程（图表由作者创建）
- en: If you have been experimenting with large language models (LLMs) for search
    and retrieval tasks, you have likely come across retrieval augmented generation
    (RAG) as a technique to add relevant contextual information to LLM generated responses.
    By connecting an LLM to private data, RAG can enable a better response by feeding
    relevant data in the context window.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在使用大语言模型（LLM）进行搜索和检索任务，你很可能遇到过检索增强生成（RAG）这一技术，用于向LLM生成的响应中添加相关的上下文信息。通过将LLM与私有数据连接，RAG可以通过在上下文窗口中提供相关数据来增强响应效果。
- en: RAG has been shown to be highly effective for complex query answering, knowledge-intensive
    tasks, and enhancing the precision and relevance of responses for AI models, especially
    in situations where standalone training data may fall short.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RAG已被证明在复杂查询解答、知识密集型任务以及提高AI模型响应的精度和相关性方面非常有效，特别是在单独的训练数据可能不足的情况下。
- en: However, these benefits from RAG can only be reaped if you are continuously
    monitoring your LLM system at common failure points — most notably with response
    and retrieval evaluation metrics. In this piece we will go through the best workflows
    for troubleshooting poor retrieval and response metrics.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，只有在持续监控LLM系统的常见故障点——特别是响应和检索评估指标时，才能获得RAG的这些好处。在本文中，我们将介绍用于排查检索和响应指标不佳的最佳工作流程。
- en: Troubleshooting Retrieval and Responses
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排查检索和响应问题
- en: 'It’s worth remembering that **RAG works best when required information is readily
    available**. Whether relevant documents are availablefocuses RAG system evaluations
    on two critical aspects:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 值得记住的是，**RAG在所需信息易于获取时效果最佳**。相关文档的可用性将RAG系统评估集中在两个关键方面：
- en: '***Retrieval Evaluation:*** To assess the accuracy and relevance of the documents
    that were retrieved'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***检索评估：*** 评估检索到的文档的准确性和相关性'
- en: '***Response Evaluation:*** Measure the appropriateness of the response generated
    by the system when the context was provided'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***响应评估：*** 衡量在提供上下文时，系统生成的响应的适当性'
- en: '![](../Images/0a43ebe03eca8b3e0fdb478c80064940.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a43ebe03eca8b3e0fdb478c80064940.png)'
- en: 'Figure 2: Response Evals and Retrieval Evals in an LLM Application (image by
    author)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM应用中的响应评估与检索评估（图像由作者提供）
- en: 'Table 1: Response Evaluation Metrics'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表1：响应评估指标
- en: '![](../Images/c69acf88763ec390f93559601d082b09.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c69acf88763ec390f93559601d082b09.png)'
- en: Table 1 by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 由作者提供
- en: 'Table 2: Retrieval Evaluation Metrics'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表2：检索评估指标
- en: '![](../Images/907ac76c31da0365dbf548f1907aa30e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/907ac76c31da0365dbf548f1907aa30e.png)'
- en: Table 2 by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表2 由作者提供
- en: Troubleshooting RAG Workflows
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排除RAG工作流程中的问题
- en: Let’s review three potential scenarios to troubleshoot poor LLM performance
    based on the flow diagram.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾三种潜在场景，以根据流程图排除LLM性能不佳的问题。
- en: 'Scenario 1: Good Response, Good Retrieval'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景1：良好响应，良好检索
- en: '![](../Images/28c5088db40d603793b39beac7b19aa0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28c5088db40d603793b39beac7b19aa0.png)'
- en: Diagram by author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图示 由作者提供
- en: In this scenario everything in the LLM application is acting as expected and
    we have a good response with a good retrieval. We find our response evaluation
    is “correct” and our “Hit = True.” Hit is a binary metric, where “True” means
    the relevant document was retrieved and “False” would mean the relevant document
    was not retrieved. Note that the aggregate statistic for Hit is the Hit rate (percent
    of queries that have relevant context).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，LLM应用中的一切都按预期工作，我们得到了一个良好的响应和有效的检索。我们发现我们的响应评估是“正确的”，并且我们的“命中 = 真”。命中是一个二元度量，“真”意味着相关文档已被检索，而“假”则意味着未检索到相关文档。请注意，命中的汇总统计量是命中率（具有相关上下文的查询百分比）。
- en: For our response evaluations, correctness is an evaluation metric that can be
    done simply with a combination of the **input** (query), **output** (response),
    and **context** as can be seen in *Table 1*. Several of these evaluation criteria
    do not require user labeled ground-truth labels since LLMs can also be used to
    generate labels, scores, and explanations with tools like the [OpenAI function
    calling](https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/),
    below is an example prompt template.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的响应评估，正确性是一个评估标准，可以通过简单地结合**输入**（查询）、**输出**（响应）和**上下文**来完成，具体如*表1*所示。多个评估标准不需要用户标注的真实标签，因为大型语言模型（LLM）也可以使用像[OpenAI函数调用](https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/)这样的工具来生成标签、分数和解释，下面是一个示例提示模板。
- en: '![](../Images/1776a24f280136742b49ffb25f859d1e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1776a24f280136742b49ffb25f859d1e.png)'
- en: Image by author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 由作者提供
- en: These [LLM evals](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    can be formatted as numeric, categorical (binary and multi-class) and multi-output
    (multiple scores or labels) — with categorical-binary being the most commonly
    used and numeric being the least commonly used.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些[LLM评估](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)可以格式化为数值型、类别型（二元和多类）和多输出型（多个分数或标签）——其中类别型-二元是最常用的，数值型则是最不常用的。
- en: 'Scenario 2: Bad Response, Bad Retrieval'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景2：错误响应，错误检索
- en: '![](../Images/478d15be23dedf0f5031c07c5a2edf9d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/478d15be23dedf0f5031c07c5a2edf9d.png)'
- en: Diagram by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图示 由作者提供
- en: In this scenario we find that the response is incorrect and the relevant content
    was not received. Based on the query we see that the content wasn’t received because
    there is no solution to the query. The LLM cannot predict future purchases no
    matter what documents it is supplied. However, the LLM can generate a better response
    than to hallucinate an answer. Here it would be to experiment with the prompt
    that is generating the response by simply adding a line to the LLM prompt template
    of “*if relevant content is not provided and no conclusive solution is found,
    respond that the answer is unknown.”* In some cases the correct answer is that
    the answer does not exist.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们发现响应是错误的，且相关内容未被接收。根据查询，我们看到内容没有接收，因为查询没有解决方案。无论提供什么文档，LLM都无法预测未来的购买。然而，LLM可以生成一个比幻觉答案更好的响应。在这里，我们可以通过简单地在LLM提示模板中添加一行“*如果未提供相关内容且未找到结论性解决方案，请回答答案未知*”来进行实验。在某些情况下，正确的答案就是答案不存在。
- en: '![](../Images/124665fbc597674323ef4459f78ff639.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/124665fbc597674323ef4459f78ff639.png)'
- en: Diagram by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图示 由作者提供
- en: 'Scenario 3: Bad Response, Mixed Retrieval Metrics'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景3：错误响应，混合检索指标
- en: In this third scenario, we see an incorrect response with mixed retrieval metrics
    (the relevant document was retrieved, but the LLM hallucinated an answer due to
    being given too much information).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三种场景中，我们看到一个错误的响应和混合检索指标（相关文档已被检索，但LLM由于接收到过多信息而产生了幻觉）。
- en: '![](../Images/09c89963a3710d41d9c93d128d5c8899.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09c89963a3710d41d9c93d128d5c8899.png)'
- en: Diagram by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: To evaluate an LLM RAG system, you need to both fetch the right context and
    then generate an appropriate answer. Typically, developers will embed a user query
    and use it to search a vector database for relevant chunks (see Figure 3). Retrieval
    performance hinges not only on the returned chunks being semantically similar
    to the query, but on whether those chunks provide enough relevant information
    to generate the correct response to the query. Now, you must configure the parameters
    around your RAG system (type of retrieval, chunk size, and K).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估LLM RAG系统，您需要既能获取正确的上下文，又能生成合适的答案。通常，开发人员会嵌入用户查询并用它来搜索向量数据库中的相关片段（见图3）。检索性能不仅依赖于返回的片段与查询在语义上的相似性，还依赖于这些片段是否提供足够的相关信息来生成正确的查询响应。现在，您需要配置RAG系统的参数（检索类型、片段大小和K值）。
- en: '![](../Images/a37b959baffef3f83e5a1bd9958b4b25.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a37b959baffef3f83e5a1bd9958b4b25.png)'
- en: 'Figure 3: RAG Framework (by author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：RAG框架（作者提供）
- en: Similarly with our last scenario, we can try editing the prompt template or
    change out the LLM being used to generate responses. Since the relevant content
    is retrieved during the document retrieval process but isn’t being surfaced by
    the LLM, this could be a quick solution. Below is an example of a correct response
    generated from running a revised prompt template (after iterating on prompt variables,
    LLM parameters, and the prompt template itself).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们上一个场景，我们可以尝试编辑提示模板或更换用于生成响应的LLM。由于在文档检索过程中已检索到相关内容，但LLM没有将其显示出来，这可能是一个快速的解决方案。下面是运行经过修订的提示模板（在调整提示变量、LLM参数和提示模板本身后）生成的正确响应示例。
- en: '![](../Images/52689ef6524a0041ad99e1a1373f7907.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52689ef6524a0041ad99e1a1373f7907.png)'
- en: Diagram by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: When troubleshooting bad responses with mixed performance metrics, we need to
    first figure out which retrieval metrics are underperforming. The easiest way
    of doing this is to implement thresholds and monitors. Once you are alerted to
    a particular underperforming metric you can resolve with specific workflows. Let’s
    take nDCG for example. nDCG is used to measure the effectiveness of your top ranked
    documents and takes into account the position of relevant docs, so if you retrieve
    your relevant document (Hit = ‘True’), you will want to consider implementing
    a reranking technique to get the relevant documents closer to the top ranked search
    results.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在排查混合性能指标的错误响应时，我们首先需要弄清楚哪些检索指标表现不佳。实现这一点最简单的方法是设置阈值和监控器。一旦您收到某个性能不佳的指标提醒，就可以通过特定的工作流程来解决问题。以nDCG为例，nDCG用于衡量排名靠前文档的有效性，并考虑相关文档的位置。因此，如果您检索到相关文档（命中
    = ‘True’），则需要考虑实现重新排序技术，将相关文档靠近排名靠前的搜索结果。
- en: For our current scenario we retrieved a relevant document (Hit = ‘True’), and
    that document is in the first position, so let’s try and improve the precision
    (percent relevant documents) up to ‘K’ retrieved documents. Currently our Precision@4
    is 25%, but if we used only the first two relevant documents then Precision@2
    = 50% since half of the documents are relevant. This change leads to the correct
    response from the LLM since it is given less information, but more relevant information
    proportionally.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的场景中，我们检索到了一份相关文档（命中 = ‘True’），且该文档位于第一位，因此让我们尝试提升精确度（相关文档的百分比），直到检索到‘K’份文档。目前我们的Precision@4是25%，但如果只使用前两份相关文档，则Precision@2
    = 50%，因为一半的文档是相关的。这个变化导致了LLM的正确响应，因为它获取到的信息较少，但比例上却更相关。
- en: '![](../Images/df1e002e326d5b7f14e8881ce5e0f6cf.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df1e002e326d5b7f14e8881ce5e0f6cf.png)'
- en: Diagram by author
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图表由作者提供
- en: Essentially what we were seeing here is a common problem in RAG known as [lost
    in the middle](https://arxiv.org/abs/2307.03172), when your LLM is overwhelmed
    with too much information that is not always relevant and then is unable to give
    the best answer possible. From our diagram, we see that adjusting your chunk size
    is one of the first things many teams do to improve RAG applications but it’s
    not always intuitive. With context overflow and lost in the middle problems, more
    documents isn’t always better, and reranking won’t necessarily improve performance.
    To evaluate which chunk size works best, you need to define an eval benchmark
    and do a sweep over chunk sizes and top-k values. In addition to experimenting
    with chunking strategies, testing out different text extraction techniques and
    embedding methods will also improve overall RAG performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们在这里看到的是RAG中一个常见的问题，称为[中途迷失](https://arxiv.org/abs/2307.03172)，当你的LLM被过多不总是相关的信息压倒时，便无法给出最优的答案。从我们的图示中可以看出，调整分块大小是许多团队在改善RAG应用时首先做的事情，但这并不总是直观的。面对上下文溢出和中途迷失的问题，更多的文档并不总是更好，重排序也不一定能提高性能。为了评估哪种分块大小最有效，你需要定义一个评估基准，并对不同的分块大小和top-k值进行一遍扫查。除了实验不同的分块策略外，测试不同的文本提取技术和嵌入方法也会提升整体RAG性能。
- en: Response and Retrieval Evaluation Metrics Summary
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 响应和检索评估指标总结
- en: The response and retrieval evaluation metrics and approaches in [this piece](https://arize.com/blog-course/rag-evaluation/)
    offer a comprehensive way to view an LLM RAG system’s performance, guiding developers
    and users in understanding its strengths and limitations. By continually evaluating
    these systems against these metrics, improvements can be made to enhance RAG’s
    ability to provide accurate, relevant, and timely information.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[这篇文章](https://arize.com/blog-course/rag-evaluation/)中的响应和检索评估指标与方法提供了一种全面的方式来查看LLM
    RAG系统的表现，指导开发者和用户理解其优点和局限性。通过不断根据这些指标评估这些系统，可以改进RAG的能力，提供准确、相关和及时的信息。'
- en: Additional advanced methods for improving RAG include [re-ranking](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b),
    metadata attachments, testing out different embedding models, testing out different
    indexing methods, implementing [HyDE](https://arize.com/blog/hyde-paper-reading-and-discussion/),
    implementing keyword search methods, or implementing Cohere document mode (similar
    to HyDE). Note that while these more advanced methods — like chunking, text extraction,
    embedding model experimentation — may produce more contextually coherent chunks,
    these methods are more resource-intensive. Using RAG along with advanced methods
    can make performance improvements to your LLM system and will continue to do so
    as long as your retrieval and response metrics are properly monitored and maintained.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 改进RAG的其他高级方法包括[重排序](https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b)、元数据附件、测试不同的嵌入模型、测试不同的索引方法、实施[HyDE](https://arize.com/blog/hyde-paper-reading-and-discussion/)、实施关键词搜索方法，或实施Cohere文档模式（类似于HyDE）。请注意，虽然这些更高级的方法——如分块、文本提取、嵌入模型实验——可能会产生更具上下文一致性的块，但这些方法会更消耗资源。将RAG与高级方法结合使用可以提升LLM系统的性能，并且只要检索和响应指标得到适当监控和维护，性能提升将会持续。
- en: '*Questions? Please reach out to me here or on* [*LinkedIn*](https://www.linkedin.com/in/amber-roberts42/)*,*
    [*X*](https://twitter.com/astronomeramber)*, or* [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*!*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*有问题吗？请通过这里或者在* [*LinkedIn*](https://www.linkedin.com/in/amber-roberts42/)*、*
    [*X*](https://twitter.com/astronomeramber)*，或* [*Slack*](https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg)*与我联系！*'
