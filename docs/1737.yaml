- en: Hacking “Codenames” with GloVe Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GloVe嵌入破解《代号》
- en: 原文：[https://towardsdatascience.com/hacking-codenames-with-glove-embeddings-0cf928af0858?source=collection_archive---------7-----------------------#2024-07-16](https://towardsdatascience.com/hacking-codenames-with-glove-embeddings-0cf928af0858?source=collection_archive---------7-----------------------#2024-07-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hacking-codenames-with-glove-embeddings-0cf928af0858?source=collection_archive---------7-----------------------#2024-07-16](https://towardsdatascience.com/hacking-codenames-with-glove-embeddings-0cf928af0858?source=collection_archive---------7-----------------------#2024-07-16)
- en: Using a GloVe embedding-based algorithm to achieve 100% accuracy on the popular
    party game “Codenames”
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基于GloVe嵌入的算法，在流行派对游戏《代号》中实现100%的准确性。
- en: '[](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)[![Zhiheng
    Jiang](../Images/a83400a64f7ec5f1d579c6fba6da05b9.png)](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)
    [Zhiheng Jiang](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)[![Zhiheng
    Jiang](../Images/a83400a64f7ec5f1d579c6fba6da05b9.png)](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)
    [Zhiheng Jiang](https://jiangzh.medium.com/?source=post_page---byline--0cf928af0858--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)
    ·7 min read·Jul 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学之路](https://towardsdatascience.com/?source=post_page---byline--0cf928af0858--------------------------------)·阅读时间7分钟·2024年7月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '[Codenames](https://codenames.game/) is a popular party game for 2 teams of
    2 players each, where each team consists of a spymaster and an operative. Each
    team is allocated a number of word cards on a game board. During each turn, the
    spymaster provides a word clue, and the number of word cards it corresponds to.
    The operative would then have to guess which words on the game board belong to
    his/her team. The objective is for the spymaster to provide good clues, such that
    the operative can use fewer number of turns to guess all the words, before the
    opponent team. In addition, there will be an “assassin” card, which upon being
    chosen by the operative, causes the team to lose immediately.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[代号](https://codenames.game/)是一个流行的派对游戏，适合两队，每队由两名玩家组成，每队包括一个间谍主人和一个操作员。每队在游戏板上会被分配一定数量的词卡。在每轮游戏中，间谍主人给出一个词线索，并说明该线索对应的词卡数量。操作员需要猜测哪些词卡属于他/她的队伍。目标是间谍主人给出好的线索，使得操作员能够用较少的回合猜出所有的词卡，且在对方队伍之前完成。此外，还有一张“刺客”卡，操作员一旦猜中该卡，队伍将立即输掉游戏。'
- en: In this project, we are going to use a simple word vector algorithm using pre-trained
    word vectors in machine learning to maximize our accuracy in solving the game
    in as few tries as possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用一种简单的词向量算法，利用机器学习中预训练的词向量，尽可能少的尝试次数来最大化我们解游戏的准确性。
- en: 'Below are examples of what the game board looks like:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是游戏板的示例：
- en: '![](../Images/ef91552569e8509704b3ca7648c0890c.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ef91552569e8509704b3ca7648c0890c.png)'
- en: Spymaster View (Image by author)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 间谍主人视图（图片来源：作者）
- en: '![](../Images/b4f8e55702cf738cf5afb1d412926980.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4f8e55702cf738cf5afb1d412926980.png)'
- en: Operative View (Image by author)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 操作员视图（图片来源：作者）
- en: In the card arrangement for the Spymaster, the color of each card represents
    Red Team, Blue Team, Neutral (Beige) and Assassin Card (Black).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在间谍主人的卡片安排中，每张卡片的颜色代表红队、蓝队、中立（米色）和刺客卡（黑色）。
- en: Automating the spymaster and operative
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化间谍主人和操作员
- en: We will be creating an [algorithm](https://github.com/jzh001/codenames) that
    can take on both the roles of spymaster and operative and play the game by itself.
    In a board of 25 cards, there will be 9 “good” cards and 16 “bad” cards (including
    1 “assassin” card).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个[算法](https://github.com/jzh001/codenames)，它可以扮演间谍主人和操作员的角色，自动进行游戏。在一个包含25张卡片的游戏板上，会有9张“好”卡和16张“坏”卡（其中包括1张“刺客”卡）。
- en: Representing meaning with GloVe embeddings
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GloVe嵌入表示意义
- en: In order for the spymaster to give good clues to the operative, our model needs
    to be able to understand the meaning of each word. One popular way to represent
    word meaning is via word embeddings. For this task, we will be using pre-trained
    [GloVe embeddings](https://nlp.stanford.edu/projects/glove/), where each word
    is represented by a 100-dimensional vector.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使间谍头目能够给操作员提供好的线索，我们的模型需要能够理解每个词的含义。表示词义的一种流行方法是通过词嵌入。在这个任务中，我们将使用预训练的 [GloVe
    嵌入](https://nlp.stanford.edu/projects/glove/)，其中每个词由一个 100 维的向量表示。
- en: 'We then score the similarity between two words using cosine similarity, which
    is the dot product of two vectors divided by their magnitudes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用余弦相似度来计算两个词之间的相似性，余弦相似度是两个向量的点积除以它们的模长：
- en: '![](../Images/cdac39603dd26713dca9dfbdad647b5c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdac39603dd26713dca9dfbdad647b5c.png)'
- en: Image by author
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'Operative: Decoding Algorithm'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作员：解码算法
- en: During each turn, the operative receives a clue *c*, and an integer *n* representing
    the number of corresponding words to guess. In other words, the operator has to
    decode a *{c, n}* pair and choose *n* words one at a time without replacement,
    until a wrong word is reached and the turn ends.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每轮中，操作员接收到一个线索 *c*，以及一个表示要猜测的词数的整数 *n*。换句话说，操作员必须解码一个 *{c, n}* 对，并一次选择 *n*
    个词，直到选择到错误词并结束该轮。
- en: 'Our decoder is a straightforward greedy algorithm: simply sort all the remaining
    words on the board based on cosine similarity with the clue word *c*, and pick
    the top *n* words based on similarity score.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器是一个直接贪婪算法：简单地根据与线索词 *c* 的余弦相似度对剩余的所有词进行排序，并根据相似度得分选择前 *n* 个词。
- en: 'Spymaster: Encoding Algorithm'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 间谍头目：编码算法
- en: On each turn, based on the remaining “good” and “bad” words, the spymaster has
    to pick *n* words and decide on a clue *c* to give to the operative. One assumption
    we make here is that the spymaster and operative agree on the decoding strategy
    mentioned above, and hence the operator will pick the optimal *{c, n}* that will
    maximize the number of correct words chosen.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在每轮中，基于剩余的“好”词和“坏”词，间谍头目必须选择 *n* 个词，并决定一个线索 *c* 给予操作员。我们在这里做出的假设是，间谍头目和操作员一致同意上面提到的解码策略，因此操作员会选择最优的
    *{c, n}* 来最大化被选择的正确词的数量。
- en: At this point, we can make an observation that the clue *c* is an information
    bottleneck, because it has to summarize *n* words into a single word *c* for the
    operative to decode. The word vector of the encoded clue lies in the same vector
    space as each of the original word vectors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以观察到线索 *c* 是一个信息瓶颈，因为它必须将 *n* 个词总结为一个单一的词 *c* 供操作员解码。编码后的线索的词向量与每个原始词向量位于同一个向量空间中。
- en: '![](../Images/4fcc5c4d51b44ad6289128c84d3ca814.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fcc5c4d51b44ad6289128c84d3ca814.png)'
- en: Mechanics of encoder-decoder system (Image by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器系统的机制（图片来源：作者）
- en: '**Generating clue candidates**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成线索候选词**'
- en: 'Word embeddings have the property of enabling us to represent composite meanings
    via the addition and subtraction of different word vectors. Given set of “good”
    words *G* and set of “bad” words *B*, we can use this property to obtain an “average”
    meaning of a “good” word, with reference to the “bad” words, by computing a [normalized
    mean](https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:%7E:text=def%20most_similar,self%2C)
    of the vectors, with “good” word vectors being added, and “bad” word vectors being
    subtracted. This average vector enables us to generate clue candidates:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入具有的特性使我们能够通过不同词向量的加法和减法来表示复合意义。给定“好”词集 *G* 和“坏”词集 *B*，我们可以利用这个特性，通过计算[标准化均值](https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#LC690:%7E:text=def%20most_similar,self%2C)来获得“好”词的“平均”意义，参考“坏”词，其中“好”词向量相加，“坏”词向量相减。这个平均向量使我们能够生成线索候选词：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Negative sampling**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**负采样**'
- en: As the number of “bad” words usually exceeds the number of “good” words, we
    perform negative sampling, by randomly sampling an equal number of “bad” words
    compared to the “good” words in our computation of our average word vector. This
    also contributes to more randomness in the clues generated, which improves the
    diversity of clue candidates.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于“坏”词的数量通常超过“好”词的数量，我们进行负采样，通过随机采样与“好”词数量相等的“坏”词来计算我们的平均词向量。这还增加了生成线索时的随机性，从而提高了线索候选词的多样性。
- en: After we find the average word vector, we use the *most_similar()* function
    in Gensim to obtain the top closest words from the entire GloVe vocabulary with
    respect to the average word vector, based on cosine similarity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们找到平均词向量后，我们使用 Gensim 中的 *most_similar()* 函数，根据余弦相似度从整个 GloVe 词汇表中获得与平均词向量最接近的单词。
- en: '**Score function**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**评分函数**'
- en: Now, we have a method to generate candidates for clue *c*, given *n* words.
    However, we still have to decide which candidate *c* to pick, which *n* words
    to choose, and how to determine *n*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一种方法可以根据给定的 *n* 个单词生成线索候选 *c*。然而，我们仍然需要决定选择哪个候选 *c*，选择哪些 *n* 个单词，以及如何确定
    *n*。
- en: Thereafter, we generate all possible combinations of *k, k*-1, …, 1 words from
    the *k “*good” words remaining on the board, as well as their respective clue
    word candidates *c,* working backwards from *k*. To pick the best *{c, n}*,we
    score all the candidates from each possible combination of the remaining “good”
    words, through the decoding algorithm. Then we obtain the maximum number of words
    guessed correct given clue *c, count(c)*, since we know what strategy the operative
    will use.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从剩余的 *k* 个“*好*”单词中，按照 *k, k*-1, …, 1 的顺序生成所有可能的单词组合，并为每个组合生成相应的线索词候选 *c*，从
    *k* 开始向回推。为了选择最佳的 *{c, n}*，我们通过解码算法对剩余的“好”单词的每个可能组合中的所有候选进行评分。然后，我们通过知道操作员将使用的策略，获得给定线索
    *c* 时正确猜出的最大单词数量 count(c)。
- en: '![](../Images/0cd6af4e7a2d8b9295a2106e24ca4ea8.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0cd6af4e7a2d8b9295a2106e24ca4ea8.png)'
- en: Image by author
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Results
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: In each game, 25 words are sampled from [a list of 400 common Codenames words](https://github.com/Gullesnuffs/Codenames/blob/master/wordlist-eng.txt).
    Overall, after 100 experiments, our method chooses the correct word 100% of the
    time, completing the game within an average of 1.98 turns, or 4.55 guesses per
    turn (for 9 correct words), taking at most 2 turns.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在每局游戏中，从 [一份包含400个常见Codenames单词的列表](https://github.com/Gullesnuffs/Codenames/blob/master/wordlist-eng.txt)
    中抽取25个单词。总体而言，经过100次实验，我们的方法能够100%正确选择单词，在平均1.98回合内完成游戏，或者每回合4.55次猜测（对于9个正确单词），最多需要2回合。
- en: '![](../Images/676819238b000e10030980d1d45ef3de.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/676819238b000e10030980d1d45ef3de.png)'
- en: Average number of guesses at each turn as game progresses (Image by author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏进行时每回合的平均猜测次数（图片由作者提供）
- en: In other words, this algorithm takes two turns almost every game, except for
    a few exceptions where it guesses all the words in a single turn.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，除少数例外情况外，这个算法几乎每局游戏都需要两回合，只有在一次回合中猜出所有单词时才会有例外。
- en: Let’s view a sample distribution of word embeddings of the clues and guesses
    we made.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一下我们所做的线索和猜测的词嵌入分布示例。
- en: '![](../Images/a2aae26b63061bcc794fd17b6a7421e1.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2aae26b63061bcc794fd17b6a7421e1.png)'
- en: Scatter plot of word embeddings for 1 game after PCA reduction (Image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1局游戏的词嵌入散点图，经过PCA降维（图片由作者提供）
- en: Although the clues generated do provide some level of semantic summary of the
    words that the operative eventually guessed correctly, these relationships between
    clues and guesses may not be obvious from a human perspective. One way to make
    to clues more interpretable is to cap the maximum number of guesses per turn,
    which generates clues with better semantic approximation of the guesses.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成的线索确实在某种程度上提供了操作员最终正确猜出的单词的语义摘要，但这些线索与猜测之间的关系从人类的角度来看可能并不明显。使线索更易于解释的一种方法是限制每回合最大猜测次数，这样可以生成更接近猜测语义的线索。
- en: Even so, our algorithm promotes a good clustering outcome for each of the words,
    to enable our decoder to get more words correct by providing good clues that lie
    close to the target words.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们的算法仍然促进了每个单词的良好聚类结果，以便我们的解码器能够通过提供接近目标单词的良好线索，帮助它正确地猜出更多的单词。
- en: Conclusion
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, this greedy GloVe-based algorithm performs well as both the spymaster
    and operative in the Codenames game, by offering an effective way to encode and
    decode words via a clue and number.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这种基于贪婪的 GloVe 算法在Codenames游戏中作为间谍首领和操作员都表现得很好，它提供了一种有效的方式通过线索和数字来编码和解码单词。
- en: In our model, the encoder and decoder share a common strategy, which works in
    similar ways as a shared encryption key. A possible limitation of this is that
    the encoder and decoder will not work as well separately, as a human player may
    not be able to interpret the generated clues as effectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，编码器和解码器共享一个共同的策略，这个策略的工作原理与共享加密密钥类似。一个可能的限制是，编码器和解码器分开使用时效果不佳，因为人类玩家可能无法有效地解读生成的线索。
- en: Understanding the mechanics behind word embeddings and vector manipulation is
    a great way to get started in Natural Language Processing. It is interesting to
    see how simple methods can also perform well in such semantic clustering and classification
    tasks. To enhance the gameplay even further, one can consider using adding elements
    of reinforcement learning or training an autoencoder to achieve better results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 理解词嵌入和向量操作背后的机制是开始自然语言处理的一个极好方式。看到简单的方法也能在语义聚类和分类任务中表现出色，确实很有趣。为了进一步提升游戏体验，可以考虑加入强化学习元素或训练自编码器以获得更好的结果。
- en: 'Github Repository: [https://github.com/jzh001/codenames](https://github.com/jzh001/codenames)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'Github 仓库: [https://github.com/jzh001/codenames](https://github.com/jzh001/codenames)'
- en: References
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Koyyalagunta, D., Sun, A., Draelos, R. L., & Rudin, C. (2021). Playing codenames
    with language graphs and word embeddings. *Journal of Artificial Intelligence
    Research*, *71*, 319–346\. [https://doi.org/10.1613/jair.1.12665](https://doi.org/10.1613/jair.1.12665)
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Koyyalagunta, D., Sun, A., Draelos, R. L., & Rudin, C. (2021). 使用语言图和词嵌入玩 Codenames。*人工智能研究期刊*，*71*，319–346\.
    [https://doi.org/10.1613/jair.1.12665](https://doi.org/10.1613/jair.1.12665)
- en: 'Pennington, J., Socher, R., & Manning, C. (2014). Glove: Global vectors for
    word representation. *Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*. [https://doi.org/10.3115/v1/d14-1162](https://doi.org/10.3115/v1/d14-1162)'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pennington, J., Socher, R., & Manning, C. (2014). Glove：用于词表示的全局向量。*2014年自然语言处理经验方法会议（EMNLP）论文集*。
    [https://doi.org/10.3115/v1/d14-1162](https://doi.org/10.3115/v1/d14-1162)
- en: Li, Y., Yan, X., & Shaw, C. (2022). Codenames AI [https://xueweiyan.github.io/codenames-ai-website/](https://xueweiyan.github.io/codenames-ai-website/)
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Li, Y., Yan, X., & Shaw, C. (2022). Codenames AI [https://xueweiyan.github.io/codenames-ai-website/](https://xueweiyan.github.io/codenames-ai-website/)
- en: Friedman, D., & Panigrahi, A. (2021). Algorithms for Codenames [https://www.cs.princeton.edu/~smattw/Teaching/521FA21/FinalProjectReports/FriedmanPanigrahi.pdf](https://www.cs.princeton.edu/~smattw/Teaching/521FA21/FinalProjectReports/FriedmanPanigrahi.pdf)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Friedman, D., & Panigrahi, A. (2021). Codenames 算法 [https://www.cs.princeton.edu/~smattw/Teaching/521FA21/FinalProjectReports/FriedmanPanigrahi.pdf](https://www.cs.princeton.edu/~smattw/Teaching/521FA21/FinalProjectReports/FriedmanPanigrahi.pdf)
- en: 'Jaramillo, C., Charity, M., Canaan , R., & Togelius, J. (2020). Word Autobots:
    Using Transformers for Word Association in the Game Codenames. *Proceedings of
    the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment*,
    *16*(1), 231–237\. [https://doi.org/10.1609/aiide.v16i1.7435](https://doi.org/10.1609/aiide.v16i1.7435)'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jaramillo, C., Charity, M., Canaan, R., & Togelius, J. (2020). 词自机器人：使用变换器进行
    Codenames 游戏中的词关联。*人工智能与互动数字娱乐会议论文集*，*16*(1)，231–237\. [https://doi.org/10.1609/aiide.v16i1.7435](https://doi.org/10.1609/aiide.v16i1.7435)
