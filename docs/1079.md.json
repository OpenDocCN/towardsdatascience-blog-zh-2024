["```py\n# Start the training job with `start_run()`\nwith mlflow.start_run(run_name=\"logging_a_model\") as run:\n  # Previous code...\n  # Train model\n  # Log metrics\n\n  # Calculate predictions for training set\n  predictions = model.predict(X_train_scaled_df)\n\n  # Create Signature\n  # Signature required for model loading later on\n  signature = infer_signature(np.array(X_train_scaled_df), predictions)\n\n  # Model File Name\n  model_file_name = model_name + \"_file\"\n\n  # Log model\n  mlflow.tensorflow.log_model(best_model, model_file_name, signature=signature)\n\n  # Get model URI\n  model_uri = f\"runs:/{run.info.run_id}/{model_file_name}\"\n\n  # Register Model\n  result = mlflow.register_model(model_uri, model_name)\n```", "```py\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient()\n\nmlmodel_names = list(model_dict.keys())\nbest_score = 2\nmetric_name = \"brier\"\nbest_model = {\"model_name\": \"\", \"model_version\": -1}\n\nfor mlmodel in mlmodel_names:\n\n model_versions = client.search_model_versions(filter_string=f\"name = '{mlmodel}'\")\n\n for version in model_versions:\n\n  # Get metric history for Brier score and run ID\n  metric_history = client.get_metric_history(run_id=version.run_id,\n                                             key=metric_name)\n\n  # If score better than best score, save model name and version\n  if metric_history:\n   last_value = metric_history[-1].value\n   if last_value < best_score:\n    best_model[\"model_name\"] = mlmodel\n    best_model[\"model_version\"] = version.version\n    best_score = last_value\n  else:\n   continue\n```", "```py\nmodel_versions = client.search_model_versions(filter_string=f\"name = '{mlmodel}'\")\n```", "```py\nmetric_history = client.get_metric_history(run_id=version.run_id,\n                                         key=metric_name)\n```", "```py\n# Load the best model\nloaded_best_model = mlflow.pyfunc.load_model(f\"models:/{best_model['model_name']}/{best_model['model_version'].version}\")\n\n# Evaluate the best model\nfinal_brier_score = evaluate_model(loaded_best_model, X_test_scaled_df, y_test)\nprint(f\"Best final Brier score: {final_brier_score}\")\n```"]