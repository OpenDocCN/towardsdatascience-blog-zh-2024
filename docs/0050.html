<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Practical Guide to Topic Modeling with Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Practical Guide to Topic Modeling with Latent Dirichlet Allocation (LDA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06">https://towardsdatascience.com/practical-guide-to-topic-modeling-with-lda-05cd6b027bdf?source=collection_archive---------3-----------------------#2024-01-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5921" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Get better results in up to 99% less training time</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Wicaksono Wijono" class="l ep by dd de cx" src="../Images/1de0ffaab559212405648c1cc62a8450.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zdxKZVeUZCKqcikgUl7HNQ@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wwijono?source=post_page---byline--05cd6b027bdf--------------------------------" rel="noopener follow">Wicaksono Wijono</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--05cd6b027bdf--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c48e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Latent Dirichlet Allocation (LDA for short) is a mixed-membership (“soft clustering”) model that’s classically used to infer what a document is talking about. When you read this article, you can easily infer that it’s about machine learning, data science, topic modeling, etc. But when you have a million documents, you can’t possibly read and label each one manually to extract the patterns and trends. You’ll need help from a machine learning model like LDA.</p><p id="82e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">LDA can be useful even if you don’t work with text data.</strong> Text is the <em class="nf">classical </em>use case, but it’s not the only one. If you work at an online shop, you can infer soft categories of products using LDA. In a classification setting, “chocolate” would have to fall under one category such as “snack”, whereas LDA allows “chocolate” to fall under multiple categories such as “snack”, “baking”, “beverage”, and “sauce”. You can also apply LDA on clickstream data to group and categorize pages based on observed user behavior.</p><p id="e5c8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Because LDA is a probabilistic model, it plugs nicely into other probabilistic models like Poisson Factorization. You can embed the items using LDA and then <strong class="ml fr">learn user preferences</strong> using PF. In the context of news articles, this can serve “cold start” recommendations when an article is just published (perhaps for a push notification?), before the news becomes stale.</p><p id="8d37" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">My qualifications? I spent an entire semester focusing on Bayesian inference algorithms and coded up LDA from scratch to understand its inner workings. Then I’ve worked at a news conglomerate to create an LDA pipeline that had to scale up to millions of articles. At this scale, many small choices can be the difference between model runtime of a few days or a year. Safe to say, I know more about LDA than the vast majority of data scientists.</p><p id="6faf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">During all that time, I have never come across a single resource that explains how to use LDA <em class="nf">well</em>, especially at a large scale. This article might be the first. Hopefully it’s useful to you, whoever’s reading. In short:</p><ol class=""><li id="2184" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ng nh ni bk">Tokenize with spaCy instead of NLTK</li><li id="03da" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">Use specifically scikit-learn’s implementation of LDA</li><li id="795c" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">Set learning_mode to “online”</li><li id="3788" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">Know what hyperparameter ranges make sense</li><li id="d41a" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne ng nh ni bk">Select hyperparameters through random search, using validation entropy as the criterion</li></ol><p id="0f6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will assume the reader is familiar with how LDA works and what it does. Many articles already explain it. I’m not going to repeat easily found information.</p><p id="2212" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Disclaimer: the content of this article might be outdated by a year or two as I have not used LDA in a good while, but I believe everything should still be accurate.</p><h1 id="f6ff" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Why Latent Dirichlet Allocation (LDA)?</h1><p id="5d33" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">LDA and its kin (<a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html" rel="noopener ugc nofollow" target="_blank">NMF</a>, <a class="af op" href="https://poismf.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">PF</a>, <a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html" rel="noopener ugc nofollow" target="_blank">truncated SVD</a>, etc.) are simply fancy <a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">PCA</a> modified for count data. (On an unrelated note, have you seen this amazing <a class="af op" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">explanation of PCA</a>?) LDA differs from the others by creating human-interpretable embeddings in the form of topics with these properties:</p><ul class=""><li id="1d2e" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk">Nonnegative. Obviously, counts cannot be negative, but the true importance is that <strong class="ml fr">nonnegativity forces the model to learn parts</strong>. One of my <a class="af op" href="http://belohlavek.inf.upol.cz/vyuka/Lee-Seung-NMF-1999-p.pdf" rel="noopener ugc nofollow" target="_blank">favorite short papers</a> illustrates how nonnegativity forces a model to learn parts of a face, such as the nose, the eyes, the mouth, etc. In contrast, PCA loading vectors are abstract, as you can subtract one part from another.</li></ul><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pa pb ed pc bh pd"><div class="or os ot"><img src="../Images/1922c431362b92d01cc9bd63db51b345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e7x_GxKIt0TM-Ha7nx9SrA.png"/></div></div><figcaption class="pf pg ph or os pi pj bf b bg z dx">Facial parts learned from nonnegative matrix factorization. Source: <a class="af op" href="https://arxiv.org/pdf/1401.5226.pdf" rel="noopener ugc nofollow" target="_blank">Gillis (2014)</a></figcaption></figure><ul class=""><li id="f7f1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk">Sums to 1. The embeddings in LDA are proportions. The model assumes mixed membership because text is complex and is rarely about a single topic.</li><li id="f7a3" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk">Sparse. The embeddings will be mostly zero. Each document is expected to talk about a small handful of topics. Nobody writes a 100-topic article.</li><li id="ec58" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk">Human-interpretable loading vectors. In PCA and other embedding algorithms, it’s not clear what each dimension means. In LDA, you can see the highest probability tokens (“top n words”) to understand the dimension (“topic”).</li></ul><p id="6baf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A common misconception is that LDA is an NLP algorithm. In reality, <strong class="ml fr">you can use LDA on any count data as long as it’s not too sparse</strong>. All LDA does is create a low-dimensional interpretable embedding of counts. You can fit LDA on users’ purchase history or browsing history to infer the different types of shopping habits. I’ve used it that way in the past and it worked surprisingly well. Prof. Blei once mentioned in a seminar that an economics researcher was experimenting with using LDA precisely like that; I felt vindicated.</p><p id="8375" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LDA’s output is often misinterpreted. People treat it as a classification algorithm instead of a mixed-membership model. When LDA says a document is 60% politics and 40% economics, it’s saying that the document is BOTH politics and economics in those proportions. Some people misinterpret it as “the document is classified as politics, but the model’s not too sure”. The model might be <em class="nf">very </em>sure that the document’s about politics AND economics if it’s a long-form article.</p><p id="76cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Alternatives exist, such as <a class="af op" href="https://github.com/ddangelov/Top2Vec" rel="noopener ugc nofollow" target="_blank">top2vec</a>, which is conceptually similar to <a class="af op" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank">word2vec</a>. It’s really cool! However, I’d argue LDA is better than top2vec for several reasons:</p><ul class=""><li id="a5ac" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk">LDA is a multiple-membership model, while top2vec assumes each document only belongs to one topic. top2vec can make sense if your corpus is simple and each document doesn’t stray away from one topic.</li><li id="db01" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk">top2vec uses distances to infer topics, which doesn’t make intuitive sense. The concept of distance is nebulous in higher dimensions because of the<a class="af op" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank"> curse of dimensionality</a>. And what do the distances mean? As an oversimplified example, pretend three topics are on a number line: food — sports — science. If a document talks about food science, it would be smack dab in the middle and it… becomes a sports document? In reality, distances don’t work this way in higher dimensions, but my reservations should be clear.</li></ul><h1 id="c50a" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Tip #1: use spaCy instead of NLTK to tokenize and lemmatize</h1><p id="d0de" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">A corpus needs to be processed before it can be fed into LDA. How? <a class="af op" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank">spaCy</a> is popular in industry while <a class="af op" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">NLTK</a> is popular in academia. They have different strengths and weaknesses. In a work setting, NLTK isn’t really acceptable— don’t use it just because you got comfortable using it in school.</p><p id="94c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">NLTK is notoriously slow. I haven’t run my own comparisons, but <a class="af op" rel="noopener" target="_blank" href="/hands-on-implementation-of-basic-nlp-techniques-nltk-or-spacy-687099e02816">this person</a> reports a 20× speedup in tokenizing using spaCy instead of NLTK.</p><p id="0336" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Surprisingly, it’s not clear if LDA even benefits from stemming or lemmatization. I’ve seen arguments and experiments go both ways. <a class="af op" href="https://www.cs.cornell.edu/~xanda/winlp2017.pdf" rel="noopener ugc nofollow" target="_blank">This paper</a> claims that stemming makes the topics worse. The main reason to lemmatize is to make the topics more interpretable by collapsing lexemes into one token.</p><p id="89b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I’ll provide no opinion on whether you should lemmatize, but if you do decide to lemmatize, spaCy lemmatizes faster and better than NLTK. In NLTK, we need to set up a part-of-speech tagging pipeline and then pass that to the WordNet lemmatizer, which looks up words in a lexical database. spaCy uses word2vec to automatically infer the part of speech for us so it can lemmatize properly — much easier to use and faster, too.</p><p id="3bdf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When using spaCy, make sure to use the word2vec-based <strong class="ml fr">en_core_web_lg</strong> instead of the transformer-based en_core_web_trf language model. The transformer is ever so slightly more accurate (maybe by 1%), but it runs about 15× slower per <a class="af op" href="https://spacy.io/usage/facts-figures#benchmarks-speed" rel="noopener ugc nofollow" target="_blank">spaCy’s speed benchmark</a>. I’ve also observed the 15× difference in my own work. The transformer was way too slow for millions of articles as it’d take multiple months to lemmatize and tokenize everything.</p><h1 id="39e6" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Tip #2: use scikit-learn and do not touch other packages for LDA</h1><p id="9683" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">This is perhaps the most important and most surprising advice: use <a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank">sklearn’s LDA implementation</a>, without exception. The performance difference isn’t even close. Let’s compare it against two popular packages for fitting an LDA model:</p><ul class=""><li id="ba42" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk"><a class="af op" href="https://mimno.github.io/Mallet/topics.html" rel="noopener ugc nofollow" target="_blank">mallet</a> uses collapsed Gibbs sampling, an MCMC algorithm. (If you’d like to learn more about MCMC, check out <a class="af op" href="https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5" rel="noopener">my article</a>.) MCMC is notoriously slow and not scalable. Even worse, Gibbs sampling often gets stuck on a local mode; most NLP problems are highly multimodal. This disqualifies mallet from real-world applications.</li><li id="b6de" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><a class="af op" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank">gensim</a> uses stochastic variational inference (SVI), the Bayesian analog of stochastic gradient descent. As part of LDA’s updating rules, gensim chose to compute the <a class="af op" href="https://en.wikipedia.org/wiki/Digamma_function" rel="noopener ugc nofollow" target="_blank">digamma function</a> exactly, an extremely expensive operation. sklearn chose to approximate it, resulting in a 10–20x speedup. Even worse, <strong class="ml fr">gensim’s implementation of SVI is incorrect </strong>with no function arguments that can fix it. To be precise: if you input the entire corpus in one go, gensim will run SVI just fine; but if you supply a sample at each iteration, gensim’s LDA will never converge.</li></ul><p id="39ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This point about gensim surprised me. It’s a highly popular package (over 3M downloads a month!) especially made for topic modeling — there’s no way it can be worse than sklearn, an all-purpose package? At work, I spent many days troubleshooting it. I dug deep into the source code. And, lo and behold, the source code had an error in its updating equations.</p><p id="16d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I coded LDA trained using SVI from scratch while in school. It ran extremely inefficiently (I’m a data scientist, not an ML engineer!) but it produced the correct output. I know how the model is supposed to update at each iteration. gensim’s implementation is incorrect. The results were so off after just the first iteration, I had to compare manual calculations against gensim’s output to figure out what went wrong. If you sample 100 documents to feed into an iteration of SVI, gensim thinks your entire corpus is 100 documents long, even if you sampled it from a body of a million documents. You can’t tell gensim the size of your corpus in the update() method.</p><p id="dd4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">gensim runs fine if you supply the entire corpus at once. However, at work, I dealt with millions of news articles. There was no way to fit everything in memory. With large corpora, gensim fails entirely.</p><p id="5220" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">sklearn’s version is implemented correctly.</p><h1 id="16b6" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Tip #3: train using the stochastic variational inference (SVI) algorithm</h1><p id="890e" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Since we’ve established that we should not use anything other than sklearn, we’ll refer to <a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html" rel="noopener ugc nofollow" target="_blank">sklearn’s LDA function</a>. We’ll discuss specifically the learning_method argument: “batch” vs “online” (<a class="af op" href="https://medium.com/towards-data-science/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5" rel="noopener">SVI</a>) is analogous to “<a class="af op" href="https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares" rel="noopener ugc nofollow" target="_blank">IRLS</a>” vs “<a class="af op" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">SGD</a>” in linear regression.</p><p id="9463" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Linear regression runs in O(n³). IRLS requires the entire dataset all at once. If we have a million data points, IRLS takes 10¹⁸ units of time. Using SGD, we can sample 1,000 data points in each iteration and run it for 1,000 iterations to approximate the exact IRLS solution, which takes up 10⁹ x 10³ = 10¹² units of time. In this scenario, SGD runs a million times faster! SGD is expected to be imperfect as it merely approximates the optimal IRLS solution, but it usually gets close enough.</p><p id="efe8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With SVI, that intuition goes out the window: <strong class="ml fr">“online” provides a better fit than “batch” AND runs much faster</strong>. It is strictly better. There is no single justification to use “batch”. The <a class="af op" href="https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf" rel="noopener ugc nofollow" target="_blank">SVI paper</a> goes in depth:</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pa pb ed pc bh pd"><div class="or os pk"><img src="../Images/9878749700c20561c2781f9193cfd340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0UfWuYdB-FRAHClJPIFsoA.png"/></div></div><figcaption class="pf pg ph or os pi pj bf b bg z dx">Source: <a class="af op" href="https://arxiv.org/pdf/1206.7051.pdf" rel="noopener ugc nofollow" target="_blank">Hoffman et al. (2013)</a></figcaption></figure><p id="f0d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As a rule of thumb, “online” only requires 10% the training time of “batch” to get equally good results. <strong class="ml fr">To properly use the “online” mode for large corpora, you MUST set total_samples to the total number of documents in your corpus</strong>; otherwise, if your sample size is a small proportion of your corpus, the LDA model will not converge in any reasonable time. You’ll also want to use the partial_fit() method, feeding your data one tiny batch at a time. I’ll talk about the other settings in the next section.</p><h1 id="0e4d" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Tip #4: know the reasonable search space for hyperparameters</h1><p id="d7ee" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Going by sklearn’s arguments, LDA has six tune-able hyperparameters:</p><ul class=""><li id="c0b0" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk"><strong class="ml fr">n_components </strong>(default = 10): the number of topics. Self-explanatory.</li><li id="8a93" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><strong class="ml fr">doc_topic_prior </strong>(default = 1/n_components): the prior for local parameters. Bayesian priors is equivalent regularization is equivalent to padding with fake data. <strong class="ml fr">doc_topic_prior × n_components </strong>is the number of fake words you add to each document. If you’re analyzing tweets, 1–2 fake words might make sense, but 1,000 fake words makes zero sense. If you’re analyzing short stories, 1–2 fake words is virtually zero, while 1,000 fake words can be reasonable. Use your judgment. Values are usually set below 1 unless each document is really long. Make your search space look something like {0.001, 0.01, 0.1, 1}.</li><li id="1ac1" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><strong class="ml fr">topic_word_prior </strong>(default = 1/n_components): the prior for global parameters. Again, Bayesian priors is equivalent regularization is equivalent to padding with fake data. <strong class="ml fr">topic_word_prior × n_components × n_features</strong> is how many fake words are added to the model before any training. n_features is the number of tokens in the model / corpus. If the product is 1,000 and you’re analyzing tweets that average 10 words each, you’re adding 100 fake tweets into the corpus. Use your judgment.</li><li id="b080" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><strong class="ml fr">learning_decay </strong>(default = 0.7): determines how much the step size shrinks with each iteration. A lower value of learning_decay makes the step size shrink more slowly— the model can explore more modes in the multimodal objective function, but it converges more slowly. <strong class="ml fr">You MUST set 0.5 &lt; learning_decay ≤ 1 for LDA to converge</strong> (this is true of any SGD algorithm, which must satisfy the <a class="af op" href="https://www.columbia.edu/~ww2040/8100F16/RM51.pdf" rel="noopener ugc nofollow" target="_blank">Robbins-Monro condition</a>). Interestingly, gensim’s default value is 0.5, which tricks clueless users into training a model that doesn’t converge. Empirically, a value in the 0.7–0.8 range yields the best results.</li><li id="00e4" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><strong class="ml fr">learning_offset </strong>(default = 10): determines the initial step size. A higher value results in a smaller initial step size. From experience, when the batch_size is small relative to the number of documents in the corpus, the model benefits from higher learning_offset, somewhere above 100. You want to take large strides. Searching over {1, 2, 3, 4} is not as effective as searching over {1, 10, 100, 1000}.</li><li id="747d" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk"><strong class="ml fr">batch_size </strong>(default = 128): the number of documents seen at each iteration of SVI. Think of it as an inaccurate compass. The higher the batch_size, the more certain you are of taking a step in the right direction, but the longer it takes to compute. From my experience, 128 is too low as the steps go in the wrong direction too often, making it much harder for the model to converge. I recommend a batch size around 2–10 thousand, which is easily handled by SVI. A higher batch size is almost always better if computation time were no issue. I typically have a fixed number of sampled (with replacement) documents in mind during hyperparameter tuning, such as 500k, and set it to run for 50 iterations of batch_size 10,000 or 250 iterations of batch_size 2,000 to compare which one gets me the most bang for the computation. Then I’ll keep these settings when training for many many more iterations. You will need to supply the partial_fit() method with a random sample of documents of size batch_size.</li></ul><h1 id="c44a" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Tip #5: tune hyperparameters using random search and entropy loss</h1><p id="74db" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk"><strong class="ml fr">In this day and age, random search should be the default algorithm for hyperparameter tuning.</strong> In as few as 60 iterations, random search has &gt;95% probability of finding hyperparameters that are in the best 5% within the search space (<a class="af op" href="https://stats.stackexchange.com/questions/496098/does-random-search-depend-on-the-number-of-dimensions-searched/496125" rel="noopener ugc nofollow" target="_blank">proof</a>). Of course, if your search space completely misses the optimal regions, you’ll never attain good performance.</p><p id="b7a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af op" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">This paper</a> by Bergstra and Bengio illustrates that random search can beat grid search reasonably well. Grid search places too much importance on hyperparameters that don’t matter for the specific use case. If only one of two hyperparameters meaningfully affect the objective, a 3x3 grid only tries three values of that hyperparameter; whereas a 9-point random search should try nine different values of that hyperparameter, giving you more chances to find a great value. Grid search also often skips over narrow regions of good performance.</p><p id="782c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">LDA fitted using SVI has six tune-able hyperparameters (three if you go full-batch). If we want to try as few as three values for each hyperparameter, our grid search will go through 3⁶ = 729 iterations. Going down to 60 using random search to (usually) get better results is a no-brainer.</p><p id="fe70" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Random search should be configured to sample “smartly”. n_components can be sampled from a discrete uniform, but other hyperparameters like doc_topic_prior should be sampled from a lognormal or log-uniform, i.e. rather than {1, 2, 3, 4} it’s smarter to sample evenly along {0.01, 0.1, 1, 10}.</p><p id="cdb3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you want to do slightly better than random search, you can use TPE through the <a class="af op" href="https://github.com/hyperopt/hyperopt/wiki/FMin" rel="noopener ugc nofollow" target="_blank">hyperopt package</a>. Unlike Bayesian Optimization using Gaussian Processes, TPE is designed to work well with a mix of continuous and discrete (n_components) hyperparameters. However, the improvement is so minimal for so much work that it’s not worth doing in most cases.</p><p id="8425" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Okay, now that we have established random search is better than grid search… how do we know which hyperparameter combination performs the best?</p><p id="576f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Topic modeling has a metric specific to it: <a class="af op" href="https://fse.studenttheses.ub.rug.nl/28618/1/s2863685_alfiuddin_hadiat_CCS_thesis.pdf" rel="noopener ugc nofollow" target="_blank">topic coherence</a>. It comes in several flavors such as UMass and UCI. In my experience, coherence is not a good metric in practice as it often cannot be computed on the validation set. When a token does not appear in the validation set, the metric attempts to divide by zero. Topic coherence is useless for hyperparameter tuning.</p><p id="19e2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Traditionally, language models were evaluated using <a class="af op" href="https://en.wikipedia.org/wiki/Perplexity" rel="noopener ugc nofollow" target="_blank">perplexity</a>, defined as 2^entropy. However, this number can be exceedingly large with bad hyperparameters, resulting in numerical overflow errors. sklearn’s LDA has the <a class="af op" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation.score" rel="noopener ugc nofollow" target="_blank">score</a> method, an approximation of what should be proportional to the negative entropy. <strong class="ml fr">Use sklearn’s score</strong>. Higher score is better. (If the score method still runs into overflow issues, you’ll have to create the log-perplexity method yourself.)</p><h1 id="f28b" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">Bonus tip: you can create priors for topics</h1><p id="18a8" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">LDA’s output can be very inconsistent and random. This is the nature of any NLP problem. The objective function is multimodal while SVI LDA only fits to a single mode. Rerunning LDA with the exact same settings can yield different topics.</p><p id="b62c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Sometimes, we need more control over the topics LDA learns. For instance, a business stakeholder might need ten specific topics to be present. You <em class="nf">can</em> try rerunning LDA over and over again until the ten topics show up, but you’ll have better luck playing roulette.</p><p id="0968" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The solution? <strong class="ml fr">Even though the sklearn documentation says topic_word_prior takes a single float, it can accept a matrix!</strong> I dug into the source code and found that sklearn just creates a matrix where all elements are the inputted float value. However, if you supply topic_word_prior with a matrix in the correct dimensions, LDA will use the supplied matrix instead.</p><figure class="ou ov ow ox oy oz or os paragraph-image"><div role="button" tabindex="0" class="pa pb ed pc bh pd"><div class="or os pl"><img src="../Images/90a35101293a873fc4896ac200ba1c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9rrVeoFvlexG6I0eRYpoA.jpeg"/></div></div><figcaption class="pf pg ph or os pi pj bf b bg z dx">A good prior color-codes some words in each document before model training even begins. Source: <a class="af op" href="https://www.rawpixel.com/image/5921950/photo-image-public-domain-hand-kid" rel="noopener ugc nofollow" target="_blank">rawpixel</a></figcaption></figure><p id="f871" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Suppose you need a basketball topic and a golf topic. You can populate the prior of one topic with high probabilities of basketball-related words. Do the same for golf, and then fill the other topic priors with a uniform distribution. When you train the model, LDA becomes <em class="nf">more likely</em> to create these two topics.</p><p id="d25d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note I said <em class="nf">more likely</em>. LDA is fit stochastically. We have no idea where it’ll end up based on the initial settings.</p><p id="bcbf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, we can boost the chances of these topics appearing with a few tweaks in settings: a higher learning_offset and a higher learning_decay that’s run for more iterations (because the model becomes slower to converge). Conversely, low values in these two hyperparameters will immediately erase whatever prior you put in.</p><h1 id="3864" class="no np fq bf nq nr ns gq nt nu nv gt nw nx ny nz oa ob oc od oe of og oh oi oj bk">In closing</h1><p id="eb90" class="pw-post-body-paragraph mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne fj bk">Hopefully this article makes it clear that the 99% training time reduction is not clickbait. Someone who knows little about LDA would reasonably tokenize using NLTK, use gensim’s stochastic variational inference algorithm, and then grid search over an inefficient search space. Switching from NLTK to spaCy gives a speedup of 8–20×, but that’s a separate and relatively small component of the model pipeline. We’ll focus on the model training aspect. Following all the recommendations in this article yields the following improvements:</p><ul class=""><li id="db0b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oq nh ni bk">Someone inexperienced in LDA might use gensim. sklearn’s implementation of the objective function alone cuts down training time by 10–20×. Let’s be conservative and say it gets training time down to 10%.</li><li id="8f66" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk">Alternatively, someone inexperienced in LDA might start in sklearn but use the ‘batch’ mode. Going from full-batch variational inference to stochastic variational inference cuts the time down by a factor of 10×. This also gets us down to 10%.</li><li id="564f" class="mj mk fq ml b go nj mn mo gr nk mq mr ms nl mu mv mw nm my mz na nn nc nd ne oq nh ni bk">We have six hyperparameters to tune. If we want to try 3 different values of each parameter and grid search, it’d take 729 iterations. Random search only needs 60 iterations to perform well, and it will likely outperform grid search. That’s a reduction by a factor of 10×, getting us down to 1% of the original training time.</li></ul><p id="bc83" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Reducing model training time by 100× is not the only outcome. If you follow the tips in this article, the model should yield better topics that make more sense.</p><p id="fcd2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Much of data science is a surface-level understanding of the algorithms and throwing random things at a wall to see what sticks. Specialized knowledge is often labeled as overly pedantic (in a “science” field!). However, a deeper understanding lets us use our tools much more efficiently, and I urge everyone to dig deeper into the tools we choose to use.</p></div></div></div></div>    
</body>
</html>