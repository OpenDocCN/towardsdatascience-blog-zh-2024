<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Hands-On Imitation Learning: From Behavior Cloning to Multi-Modal Imitation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Hands-On Imitation Learning: From Behavior Cloning to Multi-Modal Imitation Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12">https://towardsdatascience.com/hands-on-imitation-learning-from-behavior-cloning-to-multi-modal-imitation-learning-11ec0d37f4a2?source=collection_archive---------10-----------------------#2024-09-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2916" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><strong class="al"><em class="hd">An overview of the most prominent methods in imitation learning while testing on a grid environment</em></strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Yasin Yousif" class="l ep by dd de cx" src="../Images/9c702c021e7e5285bddefe76a144a3e1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*8QJYUEsj9bBmkkWRq4dHig.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@mryasinusif?source=post_page---byline--11ec0d37f4a2--------------------------------" rel="noopener follow">Yasin Yousif</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--11ec0d37f4a2--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/120c495851c2f0539761925dddc91cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z-l4OiKMfHuUb1gY"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Possessed Photography</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1981" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Reinforcement learning is one branch of machine learning concerned with learning by guidance of scalar signals (rewards); in contrast to supervised learning, which needs full labels of the target variable.</p><p id="8a3d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An intuitive example to explain reinforcement learning can be given in terms of a school with two classes having two types of tests repeated continuously. The first class solves the test and gets the full correct answers (supervised learning: SL). The second class solves the test and gets only the grades for each question (reinforcement learning: RL). In the first case, it seems easier for the students to learn the correct answers and memorize them. In the second class, the task is harder because they can learn only by trial and error. However, their learning will be more robust because they don’t only know what is right but also all the wrong answers to avoid.</p><p id="66bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In order to learn efficiently with RL, an accurate reward signal (the grades) should be designed, which is considered a difficult task, especially for real-world applications. For example, a human expert driver knows how to drive, but cannot set rewards for ‘correct driving’ skill, same thing for cooking or painting. This created the need for imitation learning methods (IL). IL is a new branch of RL concerned with learning from mere expert trajectories, without knowing the rewards. Main application areas of IL are in robotics and autonomous driving fields.</p><p id="1baa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the following, we will explore the most famous methods of IL in the literature, ordered by their proposal time from old to new, as shown in the timeline picture below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk nz"><img src="../Images/052ad6e337c649a08caa6e76fb3ecd9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*n9ab37eFDnNOoX11q9uEyQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Timeline of IL methods</figcaption></figure><p id="1ea7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The mathematical formulations will be shown along with nomenclature of the symbols. However, the theoretical derivation is kept to a minimum here; if further depth is needed, the original references can be looked up as cited in the references section at the end. The full code for recreating all the experiments is provided in the accompanying <a class="af nc" href="https://www.github.com/engyasin/ilsurvey" rel="noopener ugc nofollow" target="_blank">github repo</a>.</p><p id="cde6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, buckle up! and let’s dive through imitation learning, from behavior cloning (BC) to information maximization generative adversarial imitation learning (InfoGAIL).</p><h1 id="df93" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Example Environment</h1><p id="ba07" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The environment used in this post is represented as a 15x15 grid. The environment state is illustrated below:</p><ul class=""><li id="191d" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pb pc pd bk">Agent: red color</li><li id="7d66" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">Initial agent location: blue color</li><li id="482f" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">Walls: green color</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/417a0103d83622ac19add6a8c7479bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*G9mHqE7xsMHoMkl-Z8inuw.png"/></div></figure><p id="405b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The goal of the agent is to reach the first row in the shortest possible way through any of the three windows and towards a symmetrical location to its initial position with respect to the vertical axis passing through the middle of the grid. The goal location will not be shown in the state grid.</p><p id="dd42" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So the initial position has 15 possibilities only, and the goal location is changed based on that.</p><h1 id="0f4a" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Action Space</h1><p id="b121" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The action space <em class="pk">A</em> consists of a discrete number from 0 to 4 representing movements in four directions and the stopping action, as illustrated below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/7a40b44e70067c461e30172c23fe1768.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*s-8YtJQcTYgadMefdHeZnA.png"/></div></figure><h1 id="ecf3" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Reward Function</h1><p id="a000" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The ground truth reward here <em class="pk">R</em>(<em class="pk">s</em>,<em class="pk">a</em>) is a function of the current state and action, with a value equal to the displacement distance towards the goal:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="pn po l"/></div></figure><p id="7dac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where 𝑝1​ is the old position and <em class="pk">p</em>2​ is the new position. The agent will always be initialized at the last row, but in a random position each time.</p><h1 id="1d30" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Expert Policy</h1><p id="ccfc" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The expert policy used for all methods (except InfoGAIL) aims to reach the goal in the shortest possible path. This involves three steps:</p><ol class=""><li id="77b1" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pp pc pd bk">Moving towards the nearest window</li><li id="c6dd" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pp pc pd bk">Moving directly towards the goal</li><li id="faa0" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pp pc pd bk">Stopping at the goal location</li></ol><p id="4eda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This behavior is illustrated by a GIF:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/1629324006841fb80098705a78561bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*w4rQPjrF5pJ_c9TI2ZcAdg.gif"/></div></figure><p id="0591" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The expert policy generates demonstration trajectories used by other IL methods. Each trajectory <em class="pk">τ </em>is represented as an ordered sequence of state-action tuples.</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="pr po l"/></div></figure><p id="b111" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where the expert demonstrations set is defined as D={<em class="pk">τ</em>0​,⋯,<em class="pk">τn</em>​}</p><blockquote class="ps pt pu"><p id="280d" class="nd ne pk nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="fq">The expert episodic return was 16.33±6 on average for 30 episodes with a length of 32 steps each.</em></p></blockquote><h1 id="424c" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Forward Reinforcement Learning</h1><p id="968f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">First, we will train a model using the ground truth reward to set some baselines and tune hyperparameters for later use with IL methods.</p><p id="4f47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The implementation of the Forward RL algorithm used in this post is based on Clean RL scripts [12], which provides a readable implementation of RL methods.</p><h1 id="6aa5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Introduction</h1><p id="b113" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">We will test both Proximal Policy Optimization (PPO) [2] and Deep Q-Network (DQN) [1], state-of-the-art on-policy and well-known off-policy RL methods, respectively.</p><p id="65f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following is a summary of the training steps for each method, along with their characteristics:</p><h2 id="409d" class="pv ob fq bf oc pw px py of pz qa qb oi nm qc qd qe nq qf qg qh nu qi qj qk ql bk">On-Policy (PPO)</h2><p id="39c0" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">This method uses the current policy under training and updates its parameters after collecting rollouts for every episode. PPO has two main parts: critic and actor. The actor represents the policy, while the critic provides value estimations for each state with its own updated objective.</p><h2 id="a720" class="pv ob fq bf oc pw px py of pz qa qb oi nm qc qd qe nq qf qg qh nu qi qj qk ql bk">Off-Policy (DQN)</h2><p id="cdf7" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">DQN trains its policy offline by collecting rollouts in a replay buffer using epsilon-greedy exploration. This means that DQN does not take always the best action according to the current policy for every state but rather selects a random action. This enables the exploration of different solutions. An additional target network may be used with less frequently updated version of the policy to make the learning objective more stable.</p><h1 id="3703" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results and Discussion</h1><p id="d4ef" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The following figure shows the episodic return curves for both methods. DQN is in black, while PPO is shown as an orange line.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/3a1db66541085517545021eef57c3ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HNEQjQTjN8Vd6jU2Lpw4qA.png"/></div></div></figure><p id="1b40" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For this simple example:</p><ul class=""><li id="6329" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pb pc pd bk">Both PPO and DQN converge, but with a slight advantage for PPO. Neither method reaches the expert level of 16.6 (PPO comes close with 15.26).</li><li id="0fb2" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">DQN seems slower to converge in terms of interaction steps, known as sample inefficiency compared to PPO.</li><li id="6c3d" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">PPO takes longer training time, possibly due to actor-critic training, updating two networks with different objectives.</li></ul><p id="d99f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The parameters for training both methods are mostly the same. For a closer look at how these curves were generated, check the scripts <code class="cx qn qo qp qq b">ppo.py</code> and <code class="cx qn qo qp qq b">dqn.py</code> in the accompanying repository.</p><h1 id="f8a4" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Behavior Cloning (BC)</h1><p id="93b1" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Behavior Cloning, first proposed in [4], is a direct IL method. It involves supervised learning to map each state to an action based on expert demonstrations D. The objective is defined as:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qr po l"/></div></figure><p id="c30b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <em class="pk">π_bc</em>​ is the trained policy, <em class="pk">π_E</em>​ is the expert policy, and <em class="pk">l</em>(<em class="pk">π_bc</em>​(<em class="pk">s</em>),<em class="pk">π_E</em>​(<em class="pk">s</em>)) is the loss function between the expert and trained policy in response to the same state.</p><p id="1f56" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The difference between BC and supervised learning lies in defining the problem as an interactive environment where actions are taken in response to dynamic states (e.g., a robot moving towards a goal). In contrast, supervised learning involves mapping input to output, like classifying images or predicting temperature. This distinction is explained in [8].</p><p id="876e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this implementation, the full set of initial positions for the agent contains only 15 possibilities. Consequently, there are only 15 trajectories to learn from, which can be memorized by the BC network effectively. To make the problem harder, we clip the size of the training dataset D to half (only 240 state-action pairs out of 480) and repeat this for all IL methods that follow in this post.</p><h1 id="7050" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results</h1><p id="5de0" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">After training the model (as shown in <code class="cx qn qo qp qq b">bc.py</code> script), we get an average episodic return of 11.49 with a standard deviation of 5.24.</p><p id="cf83" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is much less than the forward RL methods before. The following GIF shows the trained BC model in action.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/4d844fec525ada99e1b12814cdde3f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*XBkfQG8F8mwCIGN04aeaDg.gif"/></div></figure><p id="3c6d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the GIF, it’s evident that almost two-thirds of the trajectories have learned to pass through the wall. However, the model gets stuck with the last third, as it cannot infer the true policy from previous examples, especially since it was given only half of the 15 expert trajectories to learn from.</p><h1 id="c2b5" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Maximum Entropy Inverse Reinforcement Learning (MaxENT)</h1><p id="a00d" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">MaxEnt [3] is another method to train a reward model separately (not iteratively), beside Behavior Cloning (BC). Its main idea lies in maximizing the probability of taking expert trajectories based on the current reward function. This can be expressed as:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qs po l"/></div></figure><p id="7b74" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where <em class="pk">N</em> is the trajectory length, and <em class="pk">Z</em> is a normalizing constant of the sum of all possible trajectories returns under the given policy.</p><p id="3375" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From there, the method derives its main objective based on the maximum entropy theorem [3], which states that <em class="pk">the most representative policy fulfilling a given condition is the one with highest entropy H. </em>Therefore, MaxEnt requires an additional objective to maximize the entropy of the policy. This leads to the following formula:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qt po l"/></div></figure><p id="8435" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Which has the derivative:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qu po l"/></div></figure><p id="4714" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where <em class="pk">SVD</em> is the state visitation frequency, which can be calculated with a dynamic programming algorithm given the current policy.</p><p id="7028" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our implementation here of MaxEnt, we skip the training of a new reward, where the dynamic programming algorithm would be slow and lengthy. Instead, we opt to test the main idea of maximizing the entropy by re-training a BC model exactly as in the previous process, but with an added term of the negative entropy of the inferred action distribution to the loss. The entropy should be negative because we wish to maximize it by minimizing the loss.</p><h1 id="3962" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results</h1><p id="5216" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">After adding the negative entropy of the distributions of actions with a weight of 0.5 (choosing the right value is important; otherwise, it may lead to worse learning), we see a slight improvement over the performance of the previous BC model with an average episodic return of 11.56 now (+0.07). The small value of the improvement can be explained by the simple nature of the environment, which contains a limited number of states. If the state space gets bigger, the entropy is expected to have a bigger importance.</p><h1 id="3cba" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Generative Adversarial Imitation Learning (GAIL)</h1><p id="dc7f" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The original work on GAIL [5] was inspired by the concept of Generative Adversarial Networks (GANs), which apply the idea of adversarial training to enhance the generative abilities of a main model. Similarly, in GAIL, the concept is applied to match state-action distributions between trained and expert policies.</p><p id="6e82" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This can be derived as Kullback-Leibler divergence, as shown in the main paper [5]. The paper finally derives the main objective for both models (called generator and discriminator models in GAIL) as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/3d98990eb6f613a4842efbd3f5682a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zwtdIdw2MnHZw17F.png"/></div></div></figure><p id="cf4d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Where <em class="pk">Dt</em>​ is the discriminator, <em class="pk">πθ</em>​ is the generator model (i.e., the policy under training), <em class="pk">πE</em>​ is the expert policy, and <em class="pk">H</em>(<em class="pk">πθ</em>​) is the entropy of the generator model.</p><p id="eb9f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The discriminator acts as a binary classifier, while the generator is the actual policy model being trained.</p><h1 id="6f0d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">The Main Benefit of GAIL</h1><p id="8ef5" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">The main benefit of GAIL over previous methods (and the reason it performs better) lies in its interactive training process. The trained policy learns and explores different states guided by the discriminator’s reward signal.</p><h1 id="bd7b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results</h1><p id="9013" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">After training GAIL for 1.6 million steps, the model converged to a higher level than BC and MaxEnt models. If continued to be trained, even better results can be achieved.</p><p id="fc9a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Specifically, we obtained an average episodic reward of 12.8, which is noteworthy considering that only 50% of demonstrations were provided without any real reward.</p><p id="49ec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This figure shows the training curve for GAIL (with ground truth episodic rewards on the y-axis). It’s worth noting that the rewards coming from log(<em class="pk">D</em>(<em class="pk">s</em>,<em class="pk">a</em>)) will be more chaotic than the ground truth due to GAIL’s adversarial training nature.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/64b2b61d8406584fafe7665aed713417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x1G50X02VUbO12ej93Qbiw.png"/></div></div></figure><h1 id="0584" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Adversarial Inverse Reinforcement Learning (AIRL)</h1><p id="e2cd" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">One remaining problem with GAIL is that the trained reward model (the discriminator) does not actually represent the ground truth reward. Instead, the discriminator is trained as a binary classifier between expert and generator state-action pairs, resulting in an average of its values of 0.5. This means that the discriminator can only be considered a surrogate reward.</p><p id="eedf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To solve this problem, the paper in [6] reformulates the discriminator using the following formula:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qx po l"/></div></figure><p id="6e01" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <em class="pk">fω</em>​(<em class="pk">s</em>,<em class="pk">a</em>) should converge to the actual advantage function. In this example, this value represents how close the agent is to the invisible goal. The ground truth reward can be found by adding another term to include a shaped reward; however, for this experiment, we will restrict ourselves to the advantage function above.</p><h1 id="efe2" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Results</h1><p id="75d1" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">After training the AIRL model with the same parameters as GAIL, we obtained the following training curve:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/fca3ec99819c63a12137e5791e9c6f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2iY1VomoJhAbMSsa1dEScg.png"/></div></div></figure><p id="0ca1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is noted that given the same training steps (1.6 Million steps), AIRL was slower to converge due to the added complexity of training the discriminator. However, now we have a meaningful advantage function, albeit with a performance of only 10.8 episodic reward, which is still good enough.</p><p id="48bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s compare the values of this advantage function and the ground truth reward in response to expert demonstrations. To make these values more comparable, we also normalized the values of the learned advantage function <em class="pk">fω</em>​. From this, we got the following plot:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/5a9cc6f81f11b38b39c4840e9be7e623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EzVtjszdBH1IME1_CuYEOQ.png"/></div></div></figure><p id="c8f6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this figure, there are 15 pulses corresponding to the 15 initial states of the agent. We can see bigger errors in the trained model for the last half of the plot, which is due to the limited use of only half the expert demos in training.</p><p id="fc74" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the first half, we observe a low state when the agent stands still at the goal with zero reward, while it was evaluated as a high value in the trained model. In the second half, there’s a general shift towards lower values.</p><p id="7d70" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Roughly speaking, the learned function approximately follows the ground truth reward and has recovered useful information about it using AIRL.</p><h1 id="f88b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Information Maximization GAIL (InfoGAIL)</h1><p id="a210" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Despite the advancements made by previous methods, an important problem still persists in Imitation Learning (IL): <strong class="nf fr">multi-modal learning</strong>. To apply IL to practical problems, it is necessary to learn from multiple possible expert policies. For instance, when driving or playing football, there is no single “true” way of doing things; experts vary in their methods, and the IL model should be able to learn these variations consistently.</p><p id="5483" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To address this issue, InfoGAIL was developed [7]. Inspired by InfoGAN [11], which conditions the style of outputs generated by GAN using an additional style vector, InfoGAIL builds on the GAIL objective and adds another criterion: maximizing the mutual information between state-action pairs and a new controlling input vector <em class="pk">z</em>. This objective can be derived as:</p><figure class="mm mn mo mp mq mr"><div class="pm ip l ed"><div class="qz po l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Kullback-Leibler divergence,</figcaption></figure><p id="1b2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where estimating the posterior <em class="pk">p</em>(<em class="pk">z</em>∣<em class="pk">s</em>,<em class="pk">a</em>) is approximated with a new model, <em class="pk">Q</em>, which takes (<em class="pk">s</em>,<em class="pk">a</em>) as input and outputs <em class="pk">z</em>.</p><p id="7e09" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The final objective for InfoGAIL can be written as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/b5bc5c1f07a8eea226011e43329d8b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*54goh77d1GVUFQ0x.png"/></div></div></figure><p id="7769" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a result, the policy has an additional input, namely <em class="pk">z</em>, as shown in the following figure:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ra"><img src="../Images/cdb73e4f39a5c4a376c9da575e4f02e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*ElDrv7p0VymnmAbJcAeV0w.png"/></div></figure><p id="58e8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our experiments, we generated new multi-modal expert demos where each expert could enter from one gap only (of the three gaps on the wall), regardless of their goal. The full demo set was used without labels indicating which expert was acting. The <em class="pk">z</em> variable is a one-hot encoding vector representing the expert class with three elements (e.g., <code class="cx qn qo qp qq b">[1 0 0]</code> for the left door). The policy should:</p><ul class=""><li id="ece9" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pb pc pd bk">Learn to move towards the goal</li><li id="0e3d" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">Link randomly generated <em class="pk">z</em> values to different modes of experts (thus passing through different doors)</li><li id="3b03" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pb pc pd bk">The <em class="pk">Q</em> model should be able to detect which mode it is based on the direction of actions in every state</li></ul><p id="33b8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that the discriminator, Q-model, and policy model training graphs are chaotic due to adversarial training.</p><p id="7e2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Fortunately, we were able to learn two modes clearly. However, the third mode was not recognized by either the policy or the Q-model. The following three GIFs show the learned expert modes from InfoGAIL when given different values of <em class="pk">z</em>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/c12ec76451dee278e191b253b631f0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*UfD2KC43yHCOUBprFL2zlA.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">z = [1,0,0]</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/ab355cdb776ac470e7b1e8b5aa1552a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*0pOWapN21ryB4lU4ujCP7g.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">z = [0,1,0]</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/0a66821dfd5078e24edbf1a7557fef59.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*dgA2kHlKdmlby1NtO50Ceg.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">z = [0,0,1]</strong></figcaption></figure><p id="6968" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, the policy was able to converge to an episodic reward of around 10 with 800K training steps. With more training steps, better results can be achieved, even if the experts used in this example are not optimal.</p><h1 id="315f" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Final Overview and Conclusion</h1><p id="ccde" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">As we review our experiments, it’s clear that all IL methods have performed well in terms of episodic reward criteria. The following table summarizes their performance:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rb"><img src="../Images/b240ddb57578ec3271d55c9076130b7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*D_bxwQ1fR4VueRK11KhmlQ.png"/></div></figure><p id="006f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pk">*InfoGAIL results are not comparable as the expert demos were based on multi-modal experts</em></p><p id="e968" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The table shows that GAIL performed the best for this problem, while AIRL was slower due to its new reward formulation, resulting in a lower return. InfoGAIL also learned well but struggled with recognizing all three modes of experts.</p><h1 id="f5ae" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Conclusion</h1><p id="47b0" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">Imitation Learning is a challenging and fascinating field. The methods we’ve explored are suitable for grid simulation environments but may not directly translate to real-world applications. Practical uses of IL are still in its infancy, except for some BC methods. Linking simulations to reality introduces new errors due to differences of their nature.</p><p id="9e4c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Another open challenge in IL is Multi-agent Imitation Learning. Research like MAIRL [9] and MAGAIL [10] have experimented with multi-agent environments but a general theory for learning from multiple expert trajectories remains an open question.</p><p id="5bc3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The attached<a class="af nc" href="http://github.com/engyasin/ilsurvey" rel="noopener ugc nofollow" target="_blank"> repository on GitHub</a> provides a basic approach to implementing these methods, which can be easily extended. The code will be updated in the future. If you’re interested in contributing, please submit an issue or pull request with your modifications. Alternatively, feel free to leave a comment as we’ll follow up with updates.</p><p id="98a3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="pk">Note: Unless otherwise noted, all images are generated by author</em></p><h1 id="372f" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">References</h1><p id="9740" class="pw-post-body-paragraph nd ne fq nf b go ow nh ni gr ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fj bk">[1] Mnih, V. (2013). Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</p><p id="52e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p><p id="b871" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Ziebart, B. D., Maas, A. L., Bagnell, J. A., &amp; Dey, A. K. (2008, July). Maximum entropy inverse reinforcement learning. In Aaai (Vol. 8, pp. 1433–1438).</p><p id="8d3f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Bain, M., &amp; Sammut, C. (1995, July). A Framework for Behavioural Cloning. In Machine Intelligence 15 (pp. 103–129).</p><p id="7c98" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Ho, J., &amp; Ermon, S. (2016). Generative adversarial imitation learning. Advances in neural information processing systems, 29.</p><p id="eda9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[6] Fu, J., Luo, K., &amp; Levine, S. (2017). Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248.</p><p id="c92f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[7] Li, Y., Song, J., &amp; Ermon, S. (2017). Infogail: Interpretable imitation learning from visual demonstrations. Advances in neural information processing systems, 30.</p><p id="1616" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[8] Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., &amp; Peters, J. (2018). An algorithmic perspective on imitation learning. Foundations and Trends® in Robotics, 7(1–2), 1–179.</p><p id="c7f0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[9] Yu, L., Song, J., &amp; Ermon, S. (2019, May). Multi-agent adversarial inverse reinforcement learning. In International Conference on Machine Learning (pp. 7194–7201). PMLR.</p><p id="1f17" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[10] Song, J., Ren, H., Sadigh, D., &amp; Ermon, S. (2018). Multi-agent generative adversarial imitation learning. Advances in neural information processing systems, 31.</p><p id="6ea1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[11] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., &amp; Abbeel, P. (2016). Infogan: Interpretable representation learning by information maximizing generative adversarial nets. Advances in neural information processing systems, 29.</p><p id="c46e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[12] Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., &amp; AraÃšjo, J. G. (2022). Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274), 1–18.</p></div></div></div></div>    
</body>
</html>