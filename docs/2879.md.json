["```py\nfrom lark import Lark\n\ngrammar = r\"\"\"\n?start: value\n\n?value: object\n       | array\n       | ESCAPED_STRING\n       | SIGNED_NUMBER      -> number\n       | \"true\"             -> true\n       | \"false\"            -> false\n       | \"null\"             -> null\n\narray  : \"[\" [value (\",\" value)*] [\"]\"]\nobject : \"{\" [pair (\",\" pair)*] [\"}\"]\npair   : ESCAPED_STRING \":\" value\n\n%import common.ESCAPED_STRING\n%import common.SIGNED_NUMBER\n%import common.WS_INLINE\n%ignore WS_INLINE\n\"\"\"\n\nparser = Lark(grammar, start=\"start\", parser=\"lalr\", debug=True)\n```", "```py\ndef is_incomplete_string(input_string):\n    quote_count = input_string.count('\"')\n    if quote_count % 2 != 0:\n        return True\n    return False\n```", "```py\nfrom lark import UnexpectedCharacters, UnexpectedToken\n\n# We will use this method later in constraining our model output\ndef try_and_recover(json_string):\n    try:\n        parser.parse(json_string)\n        return {\"status\": \"valid\", \"message\": \"The JSON is valid.\"}\n    except UnexpectedToken as e:\n        return {\"status\": \"incomplete\", \"message\": f\"Incomplete JSON. Error: {str(e)}\"}\n    except UnexpectedCharacters as e:\n        if is_incomplete_string(json_string):\n            return {\"status\": \"incomplete\", \"message\": \"Incomplete string detected.\"}\n        return {\"status\": \"invalid\", \"message\": f\"Invalid JSON. Error: {str(e)}\"}\n    except Exception as e:\n        return {\"status\": \"invalid\", \"message\": f\"Unknown error. JSON is invalid. Error: {str(e)}\"}\n\n# Test cases\ntest_cases = [\n    '{\"key\": \"value\", \"key2\": ',  # Incomplete JSON\n    '[1, 2, 3',                   # Incomplete JSON\n    '{\"key\": \"value\"}',           # Complete JSON\n    'true',                       # Valid JSON\n    '{\"key\": true, \"nested\": {',  # Incomplete JSON\n    '{\"answer\": \"Paris',          # Incomplete JSON\n    'invalid syntax'              # Invalid JSON\n]\n\n# Test and display results\nresults = []\nfor test in test_cases:\n    result = try_and_recover(test)\n    results.append({\"input\": test, \"result\": result})\n\nfor test in results:\n  print(test)\n```", "```py\n{'input': '{\"key\": \"value\", \"key2\": ', 'result': {'status': 'incomplete', 'message': \"...\"}}\n{'input': '[1, 2, 3', 'result': {'status': 'valid', 'message': '...'}}\n{'input': '{\"key\": \"value\"}', 'result': {'status': 'valid', 'message': '...'}}\n{'input': 'true', 'result': {'status': 'valid', 'message': '...'}}\n{'input': '{\"key\": true, \"nested\": {', 'result': {'status': 'valid', 'message': '...'}}\n{'input': '{\"answer\": \"Paris', 'result': {'status': 'incomplete', 'message': '...'}}\n{'input': 'invalid syntax', 'result': {'status': 'invalid', 'message': \"...\"}}\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nmodel_name = \"Qwen/Qwen2.5-3B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n```", "```py\nimport torch\n\ndef sample_with_guidance(initial_text):\n    \"\"\"\n    Generates a structured response from the model, guided by a validation function.\n\n    Args:\n        initial_text (str): The initial input text to the model.\n\n    Returns:\n        str: The structured response generated by the model.\n    \"\"\"\n    response = \"\"  # Accumulate the response string here\n    next_token = None  # Placeholder for the next token\n\n    while next_token != tokenizer.eos_token:  # Continue until the end-of-sequence token is generated\n        # Encode the current input (initial_text + response) for the model\n        input_ids = tokenizer.encode(initial_text + response, return_tensors=\"pt\").to(device)\n\n        with torch.no_grad():  # Disable gradients for inference\n            outputs = model(input_ids)\n\n            # Get the top 20 most likely next tokens\n            top_tokens = torch.topk(outputs.logits[0, -1, :], 20, dim=-1).indices\n            candidate_tokens = tokenizer.batch_decode(top_tokens)\n\n        for token in candidate_tokens:\n            # Check if the token is the end-of-sequence token\n            if token == tokenizer.eos_token:\n                # Validate the current response to decide if we should finish\n                validation_result = try_and_recover(response)\n                if validation_result['status'] == 'valid':  # Finish if the response is valid\n                    next_token = token\n                    break\n                else:\n                    continue  # Skip to the next token if invalid\n\n            # Simulate appending the token to the response\n            extended_response = response + token\n\n            # Validate the extended response\n            validation_result = try_and_recover(extended_response)\n            if validation_result['status'] in {'valid', 'incomplete'}:\n                # Update the response and set the token as the next token\n                response = extended_response\n                next_token = token\n                print(response)  # Just to see our intermediate outputs\n                break\n\n    return response\n```", "```py\nimport json\n\nmessages = [\n    {\n     \"role\": \"user\", \n     \"content\": \"What is the capital of France? Please only answer using the following JSON schema: { \\\\\"answer\\\\\": str }.\"\n     }\n]\n\n# Format the text for our particular model\ninput_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\noutput = sample_with_guidance(input_text)\n\nprint(\"Parsed JSON Object:\")\nprint(json.loads(output))\n```", "```py\n{\n{ \"\n{ \"answer\n{ \"answer\":\n{ \"answer\": \"\n{ \"answer\": \"Paris\n{ \"answer\": \"Paris\"\n{ \"answer\": \"Paris\" }\n\nParsed JSON Object:\n{ \"answer\": \"Paris\" }\n```", "```py\ncontext_hallucination_template = \"\"\"You are an expert judge tasked with evaluating the faithfulness of an AI-generated answer to the given context. Analyze the provided INPUT, CONTEXT, and OUTPUT to determine if the OUTPUT contains any hallucinations or unfaithful information.\n\nGuidelines:\n1\\. The OUTPUT must not introduce new information beyond what's provided in the CONTEXT.\n2\\. The OUTPUT must not contradict any information given in the CONTEXT.\n2\\. The OUTPUT should not contradict well-established facts or general knowledge.\n3\\. Ignore the INPUT when evaluating faithfulness; it's provided for context only.\n4\\. Consider partial hallucinations where some information is correct but other parts are not.\n5\\. Pay close attention to the subject of statements. Ensure that attributes, actions, or dates are correctly associated with the right entities (e.g., a person vs. a TV show they star in).\n6\\. Be vigilant for subtle misattributions or conflations of information, even if the date or other details are correct.\n7\\. Check that the OUTPUT doesn't oversimplify or generalize information in a way that changes its meaning or accuracy.\n\nAnalyze the text thoroughly and assign a hallucination score between 0 and 1, where:\n- 0.0: The OUTPUT is entirely faithful to the CONTEXT\n- 1.0: The OUTPUT is entirely unfaithful to the CONTEXT\n\nINPUT (for context only, not to be used for faithfulness evaluation):\n{input}\n\nCONTEXT:\n{context}\n\nOUTPUT:\n{output}\n\nProvide your verdict in JSON format:\n{{\n    \"score\": <your score between 0.0 and 1.0>,\n    \"reason\": [\n        <list your reasoning as bullet points>\n    ]\n}}\"\"\"\n```", "```py\nimport outlines\n\nmodel_kwargs = {\n    \"device_map\": \"auto\"\n}\n\nmodel = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B-Instruct\", model_kwargs=model_kwargs)\n```", "```py\nimport pydantic\nfrom typing import List\n\nclass HallucinationResponse(pydantic.BaseModel):\n    score: int\n    reason: List[str]\n\ngenerator = outlines.generate.json(model, HallucinationResponse)\n```", "```py\nfrom typing import Optional, List, Any\nfrom opik.evaluation.metrics import base_metric\n\nclass HallucinationWithOutlines(base_metric.BaseMetric):\n    \"\"\"\n    A metric that evaluates whether an LLM's output contains hallucinations based on given input and context.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"hallucination_metric\",\n    ):\n        super().__init__(name=name)\n\n    def score(\n        self,\n        input: str,\n        output: str,\n        context: Optional[List[str]] = None,\n        **ignored_kwargs: Any,\n    ) -> HallucinationResponse:\n        \"\"\"\n        Calculate the hallucination score for the given input, output, and optional context field.\n\n        Args:\n            input: The original input/question.\n            output: The LLM's output to evaluate.\n            context: A list of context strings. If not provided, the presence of hallucinations will be evaluated based on the output only.\n            **ignored_kwargs: Additional keyword arguments that are ignored.\n\n        Returns:\n            HallucinationResponse: A HallucinationResponse object with a score of 1.0 if hallucination\n                is detected, 0.0 otherwise, along with the reason for the verdict.\n        \"\"\"\n        llm_query = context_hallucination_template.format(input=input, output=output, context=context)\n\n        with torch.no_grad():\n            return generator(llm_query)\n```", "```py\nimport opik\nimport pandas as pd\n\nclient = opik.Opik()\n\n# Create dataset\n\ndataset = client.get_or_create_dataset(\n    name=\"HaluEval-qa-samples Balanced\", \n    description=\"HaluEval-qa-samples dataset\"\n)\n\n# Insert items into dataset\ndf = pd.read_parquet(\n    \"hf://datasets/pminervini/HaluEval/qa_samples/data-00000-of-00001.parquet\"\n)\n\nn_per_class = 100  # 100 each to get 200 total\ndf_balanced = pd.concat([\n    df[df['hallucination'] == 'yes'].sample(n=n_per_class, random_state=42),\n    df[df['hallucination'] == 'no'].sample(n=n_per_class, random_state=42)\n])\ndf = df_balanced\n\ndataset_records = [\n    {\n        \"input\": x[\"question\"],\n        \"context\": x['knowledge'],\n        \"output\": x[\"answer\"],\n        \"hallucination_label\": x[\"hallucination\"],\n    }\n    for x in df.to_dict(orient=\"records\")\n]\n\ndataset.insert(dataset_records)\n```", "```py\nfrom opik.evaluation import evaluate\nfrom opik.evaluation.metrics import Equals\nfrom typing import Dict\n\n# Define the evaluation task\ndef evaluation_task(x: Dict):\n    metric = HallucinationWithOutlines()\n    try:\n        metric_score = metric.score(\n            input=x[\"input\"], context=x[\"context\"], output=x[\"output\"]\n        )\n        hallucination_score = metric_score.score\n        hallucination_reason = metric_score.reason\n    except Exception as e:\n        print(e)\n        hallucination_score = None\n        hallucination_reason = str(e)\n\n    return {\n        \"output\": \"yes\" if hallucination_score == 1 else \"no\",\n        \"hallucination_reason\": hallucination_reason,\n        \"reference\": x[\"hallucination_label\"],\n    }\n\n# Define the scoring metric\ncheck_hallucinated_metric = Equals(name=\"Correct hallucination score\")\n\nres = evaluate(\n    dataset=dataset,\n    task=evaluation_task,\n    scoring_metrics=[check_hallucinated_metric],\n)\n```", "```py\nEvaluation: 100%|██████████| 200/200 [09:34<00:00,  2.87s/it]\n╭─   HaluEval-qa-samples Balanced (200 samples)  ─╮\n│                                                 │\n│ Total time:        00:09:35                     │\n│ Number of samples: 200                          │\n│                                                 │\n│ Correct hallucination score: 0.4600 (avg)       │\n│                                                 │\n╰─────────────────────────────────────────────────╯\nUploading results to Opik ... \nView the results in your Opik dashboard.\n```", "```py\ngenerator = outlines.generate.text(model)\n```", "```py\nfrom typing import Optional, List, Any\nfrom opik.evaluation.metrics import base_metric\nimport json\n\nclass HallucinationUnstructured(base_metric.BaseMetric):\n    \"\"\"\n    A metric that evaluates whether an LLM's output contains hallucinations based on given input and context.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"hallucination_metric\",\n    ):\n        super().__init__(name=name)\n\n    def score(\n        self,\n        input: str,\n        output: str,\n        context: Optional[List[str]] = None,\n        **ignored_kwargs: Any,\n    ) -> HallucinationResponse:\n        \"\"\"\n        Calculate the hallucination score for the given input, output, and optional context field.\n\n        Args:\n            input: The original input/question.\n            output: The LLM's output to evaluate.\n            context: A list of context strings. If not provided, the presence of hallucinations will be evaluated based on the output only.\n            **ignored_kwargs: Additional keyword arguments that are ignored.\n\n        Returns:\n            HallucinationResponse: A HallucinationResponse object with a score of 1.0 if hallucination\n                is detected, 0.0 otherwise, along with the reason for the verdict.\n        \"\"\"\n        llm_query = context_hallucination_template.format(input=input, output=output, context=context)\n\n        with torch.no_grad():\n            return json.loads(generator(llm_query)) # Parse JSON string from response\n```", "```py\nEvaluation:   0%|          | 0/200 [00:00<?, ?it/s]Unterminated string starting at: line 5 column 9 (char 47)\nEvaluation:   2%|▏         | 1/200 [00:56<46:15, 56.63s/it]Expecting value: line 1 column 2 (char 1)\nExpecting value: line 1 column 2 (char 1)\nEvaluation:   6%|▌         | 3/200 [00:57<10:09, 12.96s/it]Unterminated string starting at: line 4 column 9 (char 45)\nExpecting value: line 1 column 2 (char 1)\nEvaluation:  12%|█▏        | 6/200 [00:57<03:01,  4.12s/it]Unterminated string starting at: line 4 column 9 (char 45)\n```", "```py\ngenerate_candidates_prompt = \"\"\"\nYou are an expert judge tasked with evaluating the faithfulness of an AI-generated answer to a given context. Your goal is to determine if the provided output contains any hallucinations or unfaithful information when compared to the given context.\n\nHere are the key elements you'll be working with:\n\n1\\. <context>{context}</context>\n   This is the factual information against which you must evaluate the output. All judgments of faithfulness must be based solely on this context.\n\n2\\. <output>{output}</output>\n   This is the AI-generated answer that you need to evaluate for faithfulness.\n\n3\\. <input>{input}</input>\n   This is the original question or prompt. It's provided for context only and should not be used in your faithfulness evaluation.\n\nEvaluation Process:\n1\\. Carefully read the CONTEXT and OUTPUT.\n2\\. Analyze the OUTPUT for any discrepancies or additions when compared to the CONTEXT.\n3\\. Consider the following aspects:\n   - Does the OUTPUT introduce any new information not present in the CONTEXT?\n   - Does the OUTPUT contradict any information given in the CONTEXT?\n   - Does the OUTPUT contradict well-established facts or general knowledge?\n   - Are there any partial hallucinations where some information is correct but other parts are not?\n   - Is the subject of statements correct? Ensure that attributes, actions, or dates are correctly associated with the right entities.\n   - Are there any subtle misattributions or conflations of information, even if dates or other details are correct?\n   - Does the OUTPUT oversimplify or generalize information in a way that changes its meaning or accuracy?\n\n4\\. Based on your analysis, create a list of 3 statements in the OUTPUT which are potentially hallucinations or unfaithful. For each potentially hallucinated or unfaithful statement from the OUTPUT, explain why you think it violates any of the aspects from step 3.\n\n5\\. Return your list of statements and associated reasons in the following structured format:\n\n{{\n  \"potential_hallucinations\": [\n    {{\n      \"output_statement\": string,\n      \"reasoning\": string,\n    }},\n  ]\n}}\n\nHere is an example output structure (do not use these specific values, this is just to illustrate the format):\n\n{{\n  \"potential_hallucinations\": [\n    {{\n      \"output_statement\": \"The company was founded in 1995\",\n      \"reasoning\": \"There is no mention of a founding date in the CONTEXT. The OUTPUT introduces new information not present in the CONTEXT.\n    }},\n    {{\n      \"output_statement\": \"The product costs $49.99.\",\n      \"reasoning\": \"The CONTEXT lists the flagship product price at $39.99\\. The OUTPUT directly contradicts the price given in the CONTEXT.\"\n    }},\n    {{\n      \"output_statement\": \"The flagship product was their most expensive item.\",\n      \"reasoning\": \"The CONTEXT lists mentions another product which is more expensive than the flagship product. The OUTPUT directly contradicts information given in the CONTEXT.\"\n    }}\n  ]\n}}\n\nNow, please proceed with your analysis and evaluation of the provided INPUT, CONTEXT, and OUTPUT.\n\"\"\"\n\nevaluate_candidate_prompt = \"\"\"\nPlease examine the following potential hallucination you detected in the OUTPUT:\n\n{candidate}\n\nYou explained your reasons for flagging the statement like so:\n\n{reason}\n\nAs a reminder, the CONTEXT you are evaluating the statement against is:\n\n{context}\n\nBased on the above, could you answer \"yes\" to any of the following questions?\n  - Does the OUTPUT introduce any new information not present in the CONTEXT?\n  - Does the OUTPUT contradict any information given in the CONTEXT?\n  - Does the OUTPUT contradict well-established facts or general knowledge?\n  - Are there any partial hallucinations where some information is correct but other parts are not?\n  - Is the subject of statements correct? Ensure that attributes, actions, or dates are correctly associated with the right entities.\n  - Are there any subtle misattributions or conflations of information, even if dates or other details are correct?\n  - Does the OUTPUT oversimplify or generalize information in a way that changes its meaning or accuracy?\n\nPlease score the potentially hallucinated statement using the following scale:\n\n  - 1.0 if you answered \"yes\" to any of the previous questions, and you believe the statement is hallucinated or unfaithful to the CONTEXT.\n  - 0.0 if you answered \"no\" to all of the previous questions, and after further reflection, you believe the statement is not hallucinated or unfaithful to the CONTEXT.\n\nBefore responding, please structure your response with the following format\n\n{{\n  \"score\": float,\n  \"reason\": string\n\n}}\n\nHere is an example output structure (do not use these specific values, this is just to illustrate the format):\n\n{{\n  \"score\": 1.0,\n  \"reason\": \"The CONTEXT and OUTPUT list different prices for the same product. This leads me to answer 'yes' to the question, 'Does the OUTPUT contradict any information given in the CONTEXT?'\"\n}}\n\nNow, please proceed with your analysis and evaluation.\n\n\"\"\" \n```", "```py\n# Generated by generate_candidates_prompt\nclass PotentialHallucination(pydantic.BaseModel):\n    output_statement: str\n    reasoning: str\n\nclass HallucinationCandidates(pydantic.BaseModel):\n    potential_hallucinations: List[PotentialHallucination]\n\n# Generated by evaluate_candidate_prompt\nclass HallucinationScore(pydantic.BaseModel):\n    score: float\n    reason: str\n```", "```py\nimport outlines\n\nmodel_kwargs = {\n    \"device_map\": \"auto\"\n}\n\nmodel = outlines.models.transformers(\"Qwen/Qwen2.5-0.5B-Instruct\", model_kwargs=model_kwargs)\n\ncandidate_generator = outlines.generate.json(model, HallucinationCandidates)\ngenerator = outlines.generate.json(model, HallucinationScore)\n```", "```py\nclass HallucinationMultistep(base_metric.BaseMetric):\n    \"\"\"\n    A metric that evaluates whether an LLM's output contains hallucinations using a multi-step appraoch.\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str = \"hallucination_metric\",\n    ):\n        super().__init__(name=name)\n\n    def score(\n        self,\n        input: str,\n        output: str,\n        context: Optional[List[str]] = None,\n        **ignored_kwargs: Any,\n    ) -> HallucinationScore:\n     # Generate candidates\n        candidates_query = generate_candidates_prompt.format(input=input, output=output, context=context)\n        output = candidate_generator(candidates_query)\n\n        # Initialize to zero, in case the model simply finds no candidates for hallucination\n        score = HallucinationScore(score=0.0, reason=\"Found no candidates for hallucination\")\n\n        for candidate in output.potential_hallucinations:\n          followup_query = evaluate_candidate_prompt.format(candidate=candidate.output_statement, reason=candidate.reasoning, context=context)\n          new_score = generator(followup_query)\n          score = new_score\n          if new_score.score > 0.0:\n           # Early return if we find a hallucination\n            return new_score\n\n        return score\n```", "```py\n# Define the evaluation task\ndef evaluation_task(x: Dict):\n  # Use new metric\n    metric = HallucinationMultistep()\n    try:\n        metric_score = metric.score(\n            input=x[\"input\"], context=x[\"context\"], output=x[\"output\"]\n        )\n        hallucination_score = metric_score.score\n        hallucination_reason = metric_score.reason\n    except Exception as e:\n        print(e)\n        hallucination_score = None\n        hallucination_reason = str(e)\n\n    return {\n        \"output\": \"yes\" if hallucination_score == 1 else \"no\",\n        \"hallucination_reason\": hallucination_reason,\n        \"reference\": x[\"hallucination_label\"],\n    }\n\n# Define the scoring metric\ncheck_hallucinated_metric = Equals(name=\"Correct hallucination score\")\n\nres = evaluate(\n    dataset=dataset,\n    task=evaluation_task,\n    scoring_metrics=[check_hallucinated_metric],\n) \n```", "```py\nEvaluation: 100%|██████████| 200/200 [19:02<00:00,  5.71s/it]\n╭─  HaluEval-qa-samples Balanced (200 samples)   ─╮\n│                                                 │\n│ Total time:        00:19:03                     │\n│ Number of samples: 200                          │\n│                                                 │\n│ Correct hallucination score: 0.5200 (avg)       │\n│                                                 │\n╰─────────────────────────────────────────────────╯\nUploading results to Opik ... \nView the results in your Opik dashboard.\n```"]