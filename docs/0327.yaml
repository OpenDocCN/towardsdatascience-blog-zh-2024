- en: Visualizing What Batch Normalization Is and Its Advantages
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化批量归一化及其优点
- en: 原文：[https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03](https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03](https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03)
- en: Optimizing your neural network training with batch normalization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化神经网络训练中的批量归一化
- en: '[](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)  '
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    ·5 min read·Feb 3, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    ·阅读时间 5 分钟·2024年2月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8cc4a86665dbb2044eadc71bc16939b9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc4a86665dbb2044eadc71bc16939b9.png)'
- en: Visualizing what batch normalization is and its advantages. Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化批量归一化及其优点。图片来源：作者
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Have you, when conducting deep learning projects, ever encountered a situation
    where the more layers your neural network has, the slower the training becomes?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在进行深度学习项目时，是否遇到过这样的情况：神经网络层数越多，训练速度越慢？
- en: If your answer is YES, then congratulations, it’s time for you to consider using
    batch normalization now.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的回答是“是的”，那么恭喜你，是时候考虑使用批量归一化了。
- en: What is Batch Normalization?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是批量归一化？
- en: 'As the name suggests, batch normalization is a technique where batched training
    data, after activation in the current layer and before moving to the next layer,
    is standardized. Here’s how it works:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，批量归一化是一种技术，批量训练数据在当前层激活后、进入下一层之前会进行标准化。其工作原理如下：
- en: '*The entire dataset is randomly divided into N batches without replacement,
    each with a mini_batch size, for the training.*'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*整个数据集被随机划分为 N 个批次，且不重复，每个批次具有一个 mini_batch 大小，用于训练。*'
- en: '*For the i-th batch, standardize the data distribution within the batch using
    the formula:* `*(Xi - Xmean) / Xstd*`*.*'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*对于第 i 个批次，使用公式标准化批次内的数据分布：* `*(Xi - Xmean) / Xstd*`*。*'
- en: '*Scale and shift the standardized data with* `*γXi + β*` *to allow the neural
    network to undo the effects of standardization if needed.*'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*对标准化后的数据进行缩放和平移，使用公式* `*γXi + β*` *，以便在需要时让神经网络撤销标准化的效果。*'
- en: The steps seem simple, don’t they? So, what are the advantages of batch normalization?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤看起来很简单，是吗？那么，批量归一化有哪些优点呢？
- en: Advantages of Batch Normalization
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化的优点
