- en: Visualizing What Batch Normalization Is and Its Advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03](https://towardsdatascience.com/visualizing-what-batch-normalization-is-and-its-advantages-a49bbcd2fd86?source=collection_archive---------8-----------------------#2024-02-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Optimizing your neural network training with batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Peng
    Qian](../Images/9ce9aeb381ec6b017c1ee5d4714937e2.png)](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    [Peng Qian](https://qtalen.medium.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a49bbcd2fd86--------------------------------)
    ·5 min read·Feb 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cc4a86665dbb2044eadc71bc16939b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing what batch normalization is and its advantages. Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you, when conducting deep learning projects, ever encountered a situation
    where the more layers your neural network has, the slower the training becomes?
  prefs: []
  type: TYPE_NORMAL
- en: If your answer is YES, then congratulations, it’s time for you to consider using
    batch normalization now.
  prefs: []
  type: TYPE_NORMAL
- en: What is Batch Normalization?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the name suggests, batch normalization is a technique where batched training
    data, after activation in the current layer and before moving to the next layer,
    is standardized. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The entire dataset is randomly divided into N batches without replacement,
    each with a mini_batch size, for the training.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*For the i-th batch, standardize the data distribution within the batch using
    the formula:* `*(Xi - Xmean) / Xstd*`*.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Scale and shift the standardized data with* `*γXi + β*` *to allow the neural
    network to undo the effects of standardization if needed.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The steps seem simple, don’t they? So, what are the advantages of batch normalization?
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Batch Normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
