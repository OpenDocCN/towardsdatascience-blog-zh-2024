<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Pushing RL Boundaries: Integrating Foundational Models, e.g. LLMs and VLMs, into Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Pushing RL Boundaries: Integrating Foundational Models, e.g. LLMs and VLMs, into Reinforcement Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17">https://towardsdatascience.com/pushing-boundaries-integrating-foundational-models-e-g-556cfb6d0632?source=collection_archive---------5-----------------------#2024-04-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a255" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">In-Depth Exploration of Integrating Foundational Models such as LLMs and VLMs into RL Training Loop</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Elahe Aghapour" class="l ep by dd de cx" src="../Images/47a2023c566d50d8ecfcafdb69bb9bb7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*DfA3l4L2kLpNaOAUK9Rb4g.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@InfiniteLearningLoop?source=post_page---byline--556cfb6d0632--------------------------------" rel="noopener follow">Elahe Aghapour</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--556cfb6d0632--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6dd6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Authors:</strong> <span class="ia"><span class="ia" aria-hidden="false"><a class="nf ib ng" href="https://medium.com/u/75214fb27311?source=post_page---user_mention--556cfb6d0632--------------------------------" rel="noopener" target="_blank">Elahe Aghapour</a></span></span>, <span class="ia"><span class="ia" aria-hidden="false"><a class="nf ib ng" href="https://medium.com/u/6dff1eb2cc9f?source=post_page---user_mention--556cfb6d0632--------------------------------" rel="noopener" target="_blank">Salar Rahili</a></span></span></p><h2 id="f2ab" class="nh ni fq bf nj nk nl nm nn no np nq nr ms ns nt nu mw nv nw nx na ny nz oa ob bk">Overview:</h2><p id="951e" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">With the rise of the transformer architecture and high-throughput compute, training foundational models has turned into a hot topic recently. This has led to promising efforts to either integrate or train foundational models to enhance the capabilities of reinforcement learning (RL) algorithms, signaling an exciting direction for the field. Here, we’re discussing how foundational models can give reinforcement learning a major boost.</p><p id="099e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before diving into the latest research on how foundational models can give reinforcement learning a major boost, let’s engage in a brainstorming session. Our goal is to pinpoint areas where pre-trained foundational models, particularly Large Language Models (LLMs) or Vision-Language Models (VLMs), could assist us, or how we might train a foundational model from scratch. A useful approach is to examine each element of the reinforcement learning training loop individually, to identify where there might be room for improvement:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi oj"><img src="../Images/d6e65070c5e31ca5151feccf4c58c6b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHQFtiKcbbYrlgg06GQXgA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 1: Overview of foundation models in RL (Image by author)</figcaption></figure><p id="440d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1- </strong>Environment: Given that pre-trained foundational models understand the causal relationships between events, they can be utilized to forecast environmental changes resulting from current actions. Although this concept is intriguing, we’re not yet aware of any specific studies that focus on it. There are two primary reasons holding us back from exploring this idea further for now.</p><ul class=""><li id="60eb" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">While the reinforcement learning training process demands highly accurate predictions for the next step observations, pre-trained LLMs/VLMs haven’t been directly trained on datasets that enable such precise forecasting and thus fall short in this aspect. It’s important to note, as we highlighted in <a class="af pd" rel="noopener" target="_blank" href="/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66">our previous post</a>, that a high-level planner, particularly one used in lifelong learning scenarios, could effectively incorporate a foundational model.</li><li id="2805" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pa pb pc bk">Latency in environment steps is a critical factor that can constrain the RL algorithm, especially when working within a fixed budget for training steps. The presence of a very large model that introduces significant latency can be quite restrictive. Note that while it might be challenging, distillation into a smaller network can be a solution here.</li></ul><p id="dc9c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2- </strong>State (LLM/VLM Based State Generator): While experts often use the terms observation and state interchangeably, there are distinctions between them. A state is a comprehensive representation of the environment, while an observation may only provide partial information. In the standard RL framework, we don’t often discuss the specific transformations that extract and merge useful features from observations, past actions, and any internal knowledge of the environment to produce “state”, the policy input. Such a transformation could be significantly enhanced by employing LLMs/VLMs, which allow us to infuse the “state” with broader knowledge of the world, physics, and history (refer to Fig. 1, highlighted in pink).</p><p id="3a11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3- </strong>Policy (Foundational Policy Model): Integrating foundational models into the policy, the central decision-making component in RL, can be highly beneficial. Although employing such models to generate high-level plans has proven successful, transforming the state into low-level actions has challenges we’ll delve into later. Fortunately, there has been some promising research in this area recently.</p><p id="2aec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">4- </strong>Reward (LLM/VLM Based Reward Generator): Leveraging foundational models to more accurately assess chosen actions within a trajectory has been a primary focus among researchers. This comes as no surprise, given that rewards have traditionally served as the communication channel between humans and agents, setting goals and guiding the agent towards what is desired.</p><ul class=""><li id="2f32" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">Pre-trained foundational models come with a deep knowledge of the world, and injecting this kind of understanding into our decision-making processes can make those decisions more in tune with human desires and more likely to succeed. Moreover, using foundational models to evaluate the agent’s actions can quickly trim down the search space and equip the agent with a head start in understanding, as opposed to starting from scratch.</li><li id="1a79" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pa pb pc bk">Pre-trained foundational models have been trained on internet-scale data generated mostly by humans, which has enabled them to understand worlds similarly to humans. This makes it possible to use foundational models as cost-effective annotators. They can generate labels or assess trajectories or rollouts on a large scale.</li></ul><p id="f9b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1- Foundational models in reward</strong></p><p id="6456" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is challenging to use foundational models to generate low level control actions as low level actions are highly dependent on the setting of the agent and are underrepresented in foundational models’ training dataset. Hence, the foundation model application is generally focused on high level plans rather than low level actions. Reward bridges the gap between high-level planner and low level actions where foundation models can be used. Researchers have adopted various methodologies integrating foundation models for reward assignment. However, the core principle revolves around employing a VLM/LLM to effectively track the progress towards a subgoal or task.</p><p id="be99" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1.a Assigning reward values based on similarity</strong></p><p id="8a40" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Consider the reward value as a signal that indicates whether the agent’s previous action was beneficial in moving towards the goal. A sensible method involves evaluating how closely the previous action aligns with the current objective. To put this approach into practice, as can be seen in Fig. 2, it’s essential to:<br/>- Generate meaningful embeddings of these actions, which can be done through images, videos, or text descriptions of the most recent observation.<br/>- Generate meaningful representations of the current objective.<br/>- Assess the similarity between these representations.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pj"><img src="../Images/22b45a375b7df3fefaee9eb893077dc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mlO7u7OpaEgmnSfv"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 2. Reward values based on similarity (Image by author).</figcaption></figure><p id="50b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s explore the specific mechanics behind the leading research in this area.</p><p id="eda9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Dense and well-shaped reward functions enhance the stability and training speed of the RL agent. Intrinsic rewards address this challenge by rewarding the agent for novel states’ exploration. However, in large environments where most of the unseen states are irrelevant to the downstream task, this approach becomes less effective. <a class="af pd" href="https://arxiv.org/pdf/2302.06692.pdf" rel="noopener ugc nofollow" target="_blank">ELLM</a> uses background knowledge of LLM to shape the exploration. It queries LLM to generate a list of possible goals/subgoals given a list of the agent’s available actions and a text description of the agent current observation, generated by a state captioner. Then, at each time step, the reward is computed by the semantic similarity, cosine similarity, between the LLM generated goal and the description of the agent’s transition.</p><p id="88ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2312.08958.pdf" rel="noopener ugc nofollow" target="_blank">LiFT</a> has a similar framework but also leverages <a class="af pd" href="https://arxiv.org/pdf/2104.08860.pdf" rel="noopener ugc nofollow" target="_blank">CLIP4Clip</a>-style VLMs for reward assignment. <a class="af pd" href="https://arxiv.org/pdf/2104.08860.pdf" rel="noopener ugc nofollow" target="_blank">CLIP4Clip</a> is pre-trained to align videos and corresponding language descriptions through contrastive learning. In <a class="af pd" href="https://arxiv.org/pdf/2312.08958.pdf" rel="noopener ugc nofollow" target="_blank">LiFT</a>, the agent is rewarded based on the alignment score, cosine similarity, between the task instructions and videos of the agent’s corresponding behavior, both encoded by <a class="af pd" href="https://arxiv.org/pdf/2104.08860.pdf" rel="noopener ugc nofollow" target="_blank">CLIP4CLIP</a>.</p><p id="3d58" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2307.09668.pdf" rel="noopener ugc nofollow" target="_blank">UAFM</a> has a similar framework where the main focus is on robotic manipulation tasks, e.g., stacking a set of objects. For reward assignment, they measure the similarity between the agent state image and the task description, both embedded by <a class="af pd" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">CLIP</a>. They finetune <a class="af pd" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">CLIP</a> on a small amount of data from the simulated stacking domain to be more aligned in this use case.</p><p id="1754" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1.b Assigning rewards through reasoning on auxiliary tasks:</strong></p><p id="c4d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In scenarios where the foundational model has the proper understanding of the environment, it becomes feasible to directly pass the observations within a trajectory to the model, LLM/VLM. This evaluation can be done either through straightforward QA sessions based on the observations or by verifying the model’s capability in predicting the goal only by looking at the observation trajectory.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div class="oh oi pk"><img src="../Images/43e8a34aa903d5be475522d623219faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/0*gagBvPSmj0GM-DoJ"/></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 3. Assigning reward through reasoning (Image by author).</figcaption></figure><p id="b651" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2302.04449.pdf" rel="noopener ugc nofollow" target="_blank">Read and Reward</a> integrates the environment’s instruction manual into reward generation through two key components, as can be seen in Fig. 3:</p><ol class=""><li id="a037" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">QA extraction module: it creates a summary of game objectives and features. This LLM-based module, <a class="af pd" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">RoBERTa</a>-large, takes in the game manual and a question, and extracts the corresponding answer from the text. Questions are focused on the game objective, and agent-object interaction, identified by their significance using TF-IDF. For each critical object, a question as: “What happens when the player hits a &lt;object&gt;?” is added to the question set. A summary is then formed with the concatenation of all non-empty question-answer pairs.</li><li id="44b2" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">Reasoning module: During gameplay, a rule-based algorithm detects “hit” events. Following each “hit” event, the LLM based reasoning module is queried with the summary of the environment and a question: “Should you hit a &lt;object of interaction&gt; if you want to win?” where the possible answer is limited to {yes, no}. A “yes” response adds a positive reward, while “no” leads to a negative reward.</li></ol><p id="a350" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2206.09674.pdf" rel="noopener ugc nofollow" target="_blank">EAGER</a> introduces a unique method for creating intrinsic rewards through a specially designed auxiliary task. This approach presents a novel concept where the auxiliary task involves predicting the goal based on the current observation. If the model predicts accurately, this indicates a strong alignment with the intended goal, and thus, a larger intrinsic reward is given based on the prediction confidence level. To achieve this goal, To accomplish this, two modules are employed:</p><ul class=""><li id="7c60" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">Question Generation (QG): This component works by masking all nouns and adjectives in the detailed objective provided by the user.</li><li id="4798" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pa pb pc bk">Question Answering (QA): This is a model trained in a supervised manner, which takes the observation, question masks, and actions, and predicts the masked tokens.</li></ul><p id="fe75" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(P.S. Although this work does not utilize a foundational model, we’ve included it here due to its intriguing approach, which can be easily adapted to any pre-trained LLM)</p><p id="5440" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1.c Generating reward function code</strong></p><p id="c943" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Up to this point, we’ve discussed generating reward values directly for the reinforcement learning algorithms. However, running a large model at every step of the RL loop can significantly slow down the speed of both training and inference. To bypass this bottleneck, one strategy involves utilizing our foundational model to generate the code for the reward function. This allows for the direct generation of reward values at each step, streamlining the process.</p><p id="0d59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the code generation schema to work effectively, two key components are required:<br/>1- A code generator, LLM, which receives a detailed prompt containing all the necessary information to craft the code.<br/>2- A refinement process that evaluates and enhances the code in collaboration with the code generator.<br/>Let’s look at the key contributions for generating reward code:</p><p id="778e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2306.08647.pdf" rel="noopener ugc nofollow" target="_blank">R2R2S</a> generates reward function code through two main components:</p><ol class=""><li id="14a0" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">LLM based motion descriptor: This module uses a pre-defined template to describe robot movements, and leverages Large Language Models (LLMs) to understand the motion. The Motion Descriptor fills in the template, replacing placeholders e.g. “Destination Point Coordinate” with specific details, to describe the desired robot motion within a pre-defined template.</li><li id="4796" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">LLM based reward coder: this component generates the reward function by processing a prompt containing: a motion description, a list of functions with their description that LLM can use to generate the reward function code, an example code of how the response should look like, and constraints and rules the reward function must follow.</li></ol><p id="a30e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2309.11489.pdf" rel="noopener ugc nofollow" target="_blank">Text2Reward</a> develops a method to generate dense reward functions as an executable code within iterative refinement. Given the subgoal of the task, it has two key components:</p><ol class=""><li id="557a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">LLM-based reward coder: generates reward function code. Its prompt consists of: an abstract of observation and available actions, a compact pythonic style environment to represent the configuration of the objects, robot, and callable functions; a background knowledge for reward function design (e.g. “reward function for task X typically includes a term for the distance between object x and y”), and a few-shot examples. They assume access to a pool of instruction, and reward function pairs that top k similar instructions are retrieved as few-shot examples.</li><li id="a16a" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">LLM-Based Refinement: once the reward code is generated, the code is executed to identify the syntax errors and runtime errors. These feedbacks are integrated into subsequent prompts to generate more refined reward functions. Additionally, human feedback is requested based on a task execution video by the current policy.</li></ol><p id="c1db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2312.09238.pdf" rel="noopener ugc nofollow" target="_blank">Auto MC-Reward</a> has a similar algorithm to <a class="af pd" href="https://arxiv.org/pdf/2309.11489.pdf" rel="noopener ugc nofollow" target="_blank">Text2Reward</a>, to generate the reward function code, see Fig. 4. The main difference is in the refinement stage where it has two modules, both LLMs:</p><ol class=""><li id="9b12" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">LLM-Based Reward Critic: It evaluates the code and provides feedback on whether the code is self-consistent and free of syntax and semantic errors.</li><li id="bf98" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">LLM-Based Trajectory Analyser: It reviews the historical information of the interaction between the trained agent and the environment and uses it to guide the modifications of the reward function.</li></ol><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pm"><img src="../Images/9f84ed9097ad74b7b0d223a4572abf62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ueK9Ngvn8rbtwFHOj_VCA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 4. Overview of <a class="af pd" href="https://arxiv.org/pdf/2312.09238.pdf" rel="noopener ugc nofollow" target="_blank">Auto MC-Reward</a> (paper taken from <a class="af pd" href="https://arxiv.org/pdf/2312.09238.pdf" rel="noopener ugc nofollow" target="_blank">Auto MC-Reward</a> paper)</figcaption></figure><p id="6704" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2310.12931.pdf" rel="noopener ugc nofollow" target="_blank">EUREKA</a> generates reward code without the need for task-specific prompting, predefined reward templates, or predefined few-shot examples. To achieve this goal, it has two stages:</p><ol class=""><li id="e248" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">LLM-based code generation: The raw environment code, the task, generic reward design and formatting tips are fed to the LLM as context and LLM returns the executable reward code with a list of its components.</li><li id="cfe3" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">Evolutionary search and refinement: At each iteration, <a class="af pd" href="https://arxiv.org/pdf/2310.12931.pdf" rel="noopener ugc nofollow" target="_blank">EUREKA</a> queries the LLM to generate several i.i.d reward functions. Training an agent with executable reward functions provides feedback on how well the agent is performing. For a detailed and focused analysis of the rewards, the feedback also includes scalar values for each component of the reward function. The LLM takes top-performing reward code along with this detailed feedback to mutate the reward code in-context. In each subsequent iteration, the LLM uses the top reward code as a reference to generate K more i.i.d reward codes. This iterative optimization continues until a specified number of iterations has been reached.</li></ol><p id="90b7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Within these two steps, <a class="af pd" href="https://arxiv.org/pdf/2310.12931.pdf" rel="noopener ugc nofollow" target="_blank">EUREKA</a> is able to generate reward functions that outperform expert human-engineered rewards without any task specific templates.</p><p id="9360" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">1.d. Train a reward model based on preferences (RLAIF)</strong></p><p id="1126" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An alternative method is to use a foundational model to generate data for training a reward function model. The significant successes of Reinforcement Learning with Human Feedback (<a class="af pd" href="https://arxiv.org/pdf/2203.02155.pdf" rel="noopener ugc nofollow" target="_blank">RLHF</a>) have recently drawn increased attention towards employing trained reward functions on a larger scale. The heart of such algorithms is the use of a preference dataset to train a reward model which can subsequently be integrated into reinforcement learning algorithms. Given the high cost associated with generating preference data (e.g., action A is preferable to action B) through human feedback, there’s growing interest in constructing this dataset by obtaining feedback from an AI agent, i.e. VLM/LLM. Training a reward function, using AI-generated data and integrating it within a reinforcement learning algorithm, is known as Reinforcement Learning with AI Feedback (RLAIF).</p><p id="03c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2310.00166.pdf" rel="noopener ugc nofollow" target="_blank">MOTIF</a> requires access to a passive dataset of observations with sufficient coverage. Initially, LLM is queried with a summary of desired behaviors within the environment and a text description of two randomly sampled observations. It then generates the preference, selecting between 1, 2, or 0 (indicating no preference), as seen in Fig. 5. This process constructs a dataset of preferences between observation pairs. Subsequently, this dataset is used to train a reward model employing <a class="af pd" href="https://jmlr.org/papers/volume18/16-634/16-634.pdf" rel="noopener ugc nofollow" target="_blank">preference based RL techniques</a>.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi pn"><img src="../Images/80cc3f30ff6414b84db9a4b52ba2d4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6XZk2I_mefh60HR3A1fEtw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 5. A schematic representation of the three phases of <a class="af pd" href="https://arxiv.org/pdf/2310.00166.pdf" rel="noopener ugc nofollow" target="_blank">MOTIF</a> (image taken from <a class="af pd" href="https://arxiv.org/pdf/2310.00166.pdf" rel="noopener ugc nofollow" target="_blank">MOTIF</a> paper)</figcaption></figure><p id="2691" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">2- Foundation models as Policy</strong></p><p id="ea1d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Achieving the capability to train a foundational policy that not only excels in tasks previously encountered but also possesses the ability to reason about and adapt to new tasks using past learning, is an ambition within the RL community. Such a policy would ideally generalize from past experiences to tackle novel situations and, through environmental feedback, achieve goals previously unseen with human-like adaptability.</p><p id="7919" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, several challenges stand in the way of training such agents. Among these challenges are:</p><ul class=""><li id="5670" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pa pb pc bk">The necessity of managing a very large model, which introduces significant latency into the decision-making process for low-level control actions.</li><li id="da9f" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pa pb pc bk">The requirement to collect a vast amount of interaction data across a wide array of tasks to enable effective learning.</li><li id="ebc5" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pa pb pc bk">Additionally, the process of training a very large network from scratch using RL introduces extra complexities. This is because backpropagation efficiency inherently is weaker in RL compared to supervised training methods .</li></ul><p id="4f1a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Up to now, it’s mostly been teams with substantial resources and top-notch setups who’ve really pushed the envelope in this domain.</p><p id="08c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2301.07608.pdf" rel="noopener ugc nofollow" target="_blank">AdA</a> paved the way for training an RL foundation model within the X.Land 2.0 3D environment. This model achieves human time-scale adaptation on held-out test tasks without any further training. The model’s success is founded on three ingredients:</p><ol class=""><li id="1954" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pl pb pc bk">The core of the <a class="af pd" href="https://arxiv.org/pdf/2301.07608.pdf" rel="noopener ugc nofollow" target="_blank">AdA</a>’s learning mechanism is a Transformer-XL architecture from 23 to 265 million parameters, employed alongside the <a class="af pd" href="https://arxiv.org/pdf/2104.06159.pdf" rel="noopener ugc nofollow" target="_blank">Muesli</a> RL algorithm. Transformer-XL takes in a trajectory of observations, actions, and rewards from time t to T and outputs a sequence of hidden states for each time step. The hidden state is utilized to predict reward, value, and action distribution π. The combination of both long-term and short-term memory is critical for fast adaptation. Long-term memory is achieved through slow gradient updates, whereas short-term memory can be captured within the context length of the transformer. This unique combination allows the model to preserve knowledge across multiple task attempts by retaining memory across trials, even though the environment resets between trials.</li><li id="a140" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">The model benefits from meta-RL training across 1⁰⁴⁰ different partially observable Markov decision processes (POMDPs) tasks. Since <a class="af pd" href="https://arxiv.org/pdf/2206.06614.pdf" rel="noopener ugc nofollow" target="_blank">transformers are meta-learners</a>, no additional meta step is required.</li><li id="ba34" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pl pb pc bk">Given the size and diversity of the task pool, many tasks will either be too easy or too hard to generate a good training signal. To tackle this, they used an automated curriculum to prioritize tasks that are within its capability frontier.</li></ol><p id="faa5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> introduces a method to co-finetune a VLM on both robotic trajectory data and vision-language tasks, resulting in a policy model called <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a>. To enable vision-language models to generate low-level actions, actions are discretized into 256 bins and represented as language tokens.</p><p id="9379" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By representing actions as language tokens, <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> can directly utilize pre-existing VLM architectures without requiring substantial modifications. Hence, VLM input includes robot camera image and textual task description formatted similarly to Vision Question Answering tasks and the output is a series of language tokens that represent the robot’s low-level actions; see Fig. 6.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi po"><img src="../Images/94312d770f343bd4684840e4a373dbc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8doMfC6BbznpQSFy_hWqA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Fig 6. <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> overview (image taken from <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> paper)</figcaption></figure><p id="cdd3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">They noticed that co-finetuning on both types of data with the original web data leads to more generalizable policies. The co-finetuning process equips <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a> with the ability to understand and execute commands that were not explicitly present in its training data, showcasing remarkable adaptability. This approach enabled them to leverage internet-scale pretraining of VLM to generalize to novel tasks through semantic reasoning.</p><p id="612e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">3- Foundation Models as State Representation</strong></p><p id="9040" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In RL, a policy’s understanding of the environment at any given moment comes from its “state” which is essentially how it perceives its surroundings. Looking at the RL block diagram, a reasonable module to inject world knowledge into is the state. If we can enrich observations with general knowledge useful for completing tasks, the policy can pick up new tasks much faster compared to RL agents that begin learning from scratch.</p><p id="e490" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2402.02651.pdf" rel="noopener ugc nofollow" target="_blank">PR2L</a> introduces a novel approach to inject the background knowledge of VLMs from internet scale data into RL.<a class="af pd" href="https://arxiv.org/pdf/2402.02651.pdf" rel="noopener ugc nofollow" target="_blank">PR2L</a> employs generative VLMs which generate language in response to an image and a text input. As VLMs are proficient in understanding and responding to visual and textual inputs, they can provide a rich source of semantic features from observations to be linked to actions.</p><p id="b1f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pd" href="https://arxiv.org/pdf/2402.02651.pdf" rel="noopener ugc nofollow" target="_blank">PR2L</a>, queries a VLM with a task-relevant prompt for each visual observation received by the agent, and receives both the generated textual response and the model’s intermediate representations. They discard the text and use some or all of the models intermediate representation generated for both the visual and text input and the VLM’s generated textual response as “promptable representations”. Due to the variable size of these representations, <a class="af pd" href="https://arxiv.org/pdf/2402.02651.pdf" rel="noopener ugc nofollow" target="_blank">PR2L</a> incorporates an encoder-decoder Transformer layer to embed all the information embedded in promptable representations into a fixed size embedding. This embedding, combined with any available non-visual observation data, is then provided to the policy network, representing the state of the agent. This innovative integration allows the RL agent to leverage the rich semantic understanding and background knowledge of VLMs, facilitating more rapid and informed learning of tasks.</p><blockquote class="pp pq pr"><p id="de8a" class="mj mk ps ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Also Read Our Previous Post: </strong><a class="af pd" rel="noopener" target="_blank" href="/towards-agi-llms-and-foundational-models-roles-in-the-lifelong-learning-revolution-f8e56c17fa66">Towards AGI: LLMs and Foundational Models’ Roles in the Lifelong Learning Revolution</a></p></blockquote><p id="0e31" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">References:</strong></p><p id="31d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] <a class="af pd" href="https://arxiv.org/pdf/2302.06692.pdf" rel="noopener ugc nofollow" target="_blank">ELLM</a>: Du, Yuqing, et al. “Guiding pretraining in reinforcement learning with large language models.” 2023.<br/>[2] <a class="af pd" href="https://arxiv.org/pdf/2309.11489.pdf" rel="noopener ugc nofollow" target="_blank">Text2Reward</a>: Xie, Tianbao, et al. “Text2reward: Automated dense reward function generation for reinforcement learning.” 2023.<br/>[3] <a class="af pd" href="https://arxiv.org/pdf/2306.08647.pdf" rel="noopener ugc nofollow" target="_blank">R2R2S</a>: Yu, Wenhao, et al. “Language to rewards for robotic skill synthesis.” 2023.<br/>[4] <a class="af pd" href="https://arxiv.org/pdf/2310.12931.pdf" rel="noopener ugc nofollow" target="_blank">EUREKA</a>: Ma, Yecheng Jason, et al. “Eureka: Human-level reward design via coding large language models.” 2023.<br/>[5] <a class="af pd" href="https://arxiv.org/pdf/2310.00166.pdf" rel="noopener ugc nofollow" target="_blank">MOTIF</a>: Klissarov, Martin, et al. “Motif: Intrinsic motivation from artificial intelligence feedback.” 2023.<br/>[6] <a class="af pd" href="https://arxiv.org/pdf/2302.04449.pdf" rel="noopener ugc nofollow" target="_blank">Read and Reward</a>: Wu, Yue, et al. “Read and reap the rewards: Learning to play atari with the help of instruction manuals.” 2024.<br/>[7] <a class="af pd" href="https://arxiv.org/pdf/2312.09238.pdf" rel="noopener ugc nofollow" target="_blank">Auto MC-Reward</a>: Li, Hao, et al. “Auto MC-reward: Automated dense reward design with large language models for minecraft.” 2023.<br/>[8] <a class="af pd" href="https://arxiv.org/pdf/2206.09674.pdf" rel="noopener ugc nofollow" target="_blank">EAGER</a>: Carta, Thomas, et al. “Eager: Asking and answering questions for automatic reward shaping in language-guided RL.” 2022.<br/>[9] <a class="af pd" href="https://arxiv.org/pdf/2312.08958.pdf" rel="noopener ugc nofollow" target="_blank">LiFT</a>: Nam, Taewook, et al. “LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers.” 2023.<br/>[10] <a class="af pd" href="https://arxiv.org/pdf/2307.09668.pdf" rel="noopener ugc nofollow" target="_blank">UAFM</a>: Di Palo, Norman, et al. “Towards a unified agent with foundation models.” 2023.<br/>[11] <a class="af pd" href="https://arxiv.org/pdf/2307.15818.pdf" rel="noopener ugc nofollow" target="_blank">RT-2</a>: Brohan, Anthony, et al. “Rt-2: Vision-language-action models transfer web knowledge to robotic control.” 2023.<br/>[12] <a class="af pd" href="https://arxiv.org/pdf/2301.07608.pdf" rel="noopener ugc nofollow" target="_blank">AdA</a>: Team, Adaptive Agent, et al. “Human-timescale adaptation in an open-ended task space.” 2023.<br/>[13] <a class="af pd" href="https://arxiv.org/pdf/2402.02651.pdf" rel="noopener ugc nofollow" target="_blank">PR2L</a>: Chen, William, et al. “Vision-Language Models Provide Promptable Representations for Reinforcement Learning.” 2024.<br/>[14] <a class="af pd" href="https://arxiv.org/pdf/2104.08860.pdf" rel="noopener ugc nofollow" target="_blank">Clip4Clip</a>: Luo, Huaishao, et al. “Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning.” 2022.<br/>[15] <a class="af pd" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">Clip</a>: Radford, Alec, et al. “Learning transferable visual models from natural language supervision.” 2021.<br/>[16] <a class="af pd" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">RoBERTa</a>: Liu, Yinhan, et al. “Roberta: A robustly optimized bert pretraining approach.” 2019.<br/>[17] <a class="af pd" href="https://jmlr.org/papers/volume18/16-634/16-634.pdf" rel="noopener ugc nofollow" target="_blank">Preference based RL</a>: SWirth, Christian, et al. “A survey of preference-based reinforcement learning methods.” 2017.<br/>[18] <a class="af pd" href="https://arxiv.org/pdf/2104.06159.pdf" rel="noopener ugc nofollow" target="_blank">Muesli</a>: Hessel, Matteo, et al. “Muesli: Combining improvements in policy optimization.” 2021.<br/>[19] Melo, Luckeciano C. “<a class="af pd" href="https://arxiv.org/pdf/2206.06614.pdf" rel="noopener ugc nofollow" target="_blank">Transformers are meta-reinforcement learners</a>.” 2022.<br/>[20] <a class="af pd" href="https://arxiv.org/pdf/2203.02155.pdf" rel="noopener ugc nofollow" target="_blank">RLHF</a>: Ouyang, Long, et al. “Training language models to follow instructions with human feedback, 2022.</p></div></div></div></div>    
</body>
</html>