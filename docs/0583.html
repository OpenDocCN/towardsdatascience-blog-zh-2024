<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Visualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557?source=collection_archive---------0-----------------------#2024-03-03">https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557?source=collection_archive---------0-----------------------#2024-03-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="90c5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@markus.stoll?source=post_page---byline--fc2486308557--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Markus Stoll" class="l ep by dd de cx" src="../Images/236ce5901f817a72c6cceb40e0ca2fc5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-8lOzTMVRmfmOj5YkIb9Rw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fc2486308557--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@markus.stoll?source=post_page---byline--fc2486308557--------------------------------" rel="noopener follow">Markus Stoll</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fc2486308557--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ee4f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Retrieval-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries [1]. This workflow does not require costly training or fine-tuning of LLMs on the additional documents. The documents are split into snippets, which are then indexed, often using a compact ML-generated vector representation (embedding). Snippets with similar content will be in proximity to each other in this embedding space.</p><p id="63c3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The RAG application projects the user-provided questions into the embedding space to retrieve relevant document snippets based on their distance to the question. The LLM can use the retrieved information to answer the query and to substantiate its conclusion by presenting the snippets as references.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/ab15fb445c5c48c93aa871fc42c61295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*peWTe1A-MqeROT_Jdof_Cw.gif"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Animation of the iterations of a UMAP [3] dimensionality reduction for Wikipedia Formula One articles in the embedding space with manually labeled clusters — created by the author.</figcaption></figure><p id="c65f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The evaluation of a RAG application is challenging [2]. Different approaches exist: on one hand, there are methods where the answer as ground truth must be provided by the developer; on the other hand, the answer (and the question) can also be generated by another LLM. One of the largest open-source systems for LLM-supported answering is Ragas [4](Retrieval-Augmented Generation Assessment), which provides</p><ul class=""><li id="935b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk">Methods for generating test data based on the documents and</li><li id="0257" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">Evaluations based on different metrics for evaluating retrieval and generation steps one-by-one and end-to-end.</li></ul><p id="197d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, you will learn</p><ul class=""><li id="6f29" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk">How to briefly build a RAG system for Formula One (see the previous article <a class="af og" href="https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f" rel="noopener">Visualize your RAG Data — EDA for Retrieval-Augmented Generation</a> for detailed descriptions)</li><li id="e4d2" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">Generate questions and answers</li><li id="7872" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">Evaluate the RAG system with <a class="af og" href="https://github.com/explodinggradients/ragas" rel="noopener ugc nofollow" target="_blank">Ragas</a></li><li id="71e6" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">Most importantly how to visualize the results with <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight </a>and interpret the results.</li></ul><blockquote class="oh"><p id="cc39" class="oi oj fq bf ok ol om on oo op oq ne dx"><em class="or">The </em><a class="af og" href="https://github.com/Renumics/renumics-rag/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="or">code is available at Github</em></a></p></blockquote><h1 id="944e" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb pc pd pe pf pg ph pi pj pk pl pm pn bk">Get your environment ready</h1><p id="b17a" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">Start a notebook and install the required python packages</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="c187" class="px ot fq pu b bg py pz l qa qb">!pip install langchain langchain-openai chromadb renumics-spotlight<br/>%env OPENAI_API_KEY=&lt;your-api-key&gt;</span></pre><p id="2488" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This tutorial uses the following python packages:</p><ul class=""><li id="9d83" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk"><a class="af og" href="https://github.com/langchain-ai/langchain" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Langchain</strong></a>: A framework to integrate language models and RAG components, making the setup process smoother.</li><li id="1738" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Renumics-Spotlight</strong></a>: A visualization tool to interactively explore unstructured ML datasets.</li><li id="61df" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><a class="af og" href="https://github.com/explodinggradients/ragas" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">Ragas</strong></a>: a framework that helps you evaluate your RAG pipelines</li></ul><p id="df53" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qc">Disclaimer: The author of this article is also one of the developers of Spotlight.</em></p><h1 id="b939" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Prepare documents and embeddings for the dataset</h1><p id="d704" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">You can use your own RAG Application, skip to the next part to learn how to evaluate, extract and visualize.</p><p id="7ed8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Or you can use the RAG application from the <a class="af og" href="https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f" rel="noopener">last article</a> with <a class="af og" href="https://spotlightpublic.blob.core.windows.net/docs-data/rag_demo/docs.zip" rel="noopener ugc nofollow" target="_blank">our prepared dataset of all Formula One articles of Wikipedia</a>. There you can also insert your own Documents into a ‘docs/’ subfolder.</p><blockquote class="qg qh qi"><p id="526a" class="mj mk qc ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This dataset is based on articles from <a class="af og" href="https://www.wikipedia.org/" rel="noopener ugc nofollow" target="_blank">Wikipedia</a> and is licensed under the Creative Commons Attribution-ShareAlike License. The original articles and a list of authors can be found on the respective Wikipedia pages.</p></blockquote><p id="c057" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now you can use Langchain’s <code class="cx qj qk ql pu b">DirectoryLoader</code> to load all files from the docs subdirectory and split the documents in snippets using the <code class="cx qj qk ql pu b">RecursiveCharacterTextSpliter</code>. With <code class="cx qj qk ql pu b">OpenAIEmbeddings</code> you can create embeddings and store them in a <code class="cx qj qk ql pu b">ChromaDB </code>as vector store. For the Chain itself you can use LangChains <code class="cx qj qk ql pu b">ChatOpenAI</code> and a <code class="cx qj qk ql pu b">ChatPromptTemplate</code>.</p><p id="84a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The<a class="af og" href="https://github.com/Renumics/rag-demo/blob/main/notebooks/visualize_rag_tutorial_qs.ipynb" rel="noopener ugc nofollow" target="_blank"> linked code</a> for this article contains all necessary steps and you can find a detailed description of all steps above in <a class="af og" href="https://medium.com/itnext/visualize-your-rag-data-eda-for-retrieval-augmented-generation-0701ee98768f" rel="noopener">the last article</a>.</p><p id="3d91" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One important point is, that you should use a hash function to create ids for snippets in <code class="cx qj qk ql pu b">ChromaDB</code>. This allows to find the embeddings in the db if you only have the document with its content and metadata. This makes it possible to skip documents that already exist in the database.</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="5ab8" class="px ot fq pu b bg py pz l qa qb">import hashlib<br/>import json<br/>from langchain_core.documents import Document<br/><br/>def stable_hash_meta(doc: Document) -&gt; str:<br/>    """<br/>    Stable hash document based on its metadata.<br/>    """<br/>    return hashlib.sha1(json.dumps(doc.metadata, sort_keys=True).encode()).hexdigest()<br/><br/>...<br/>splits = text_splitter.split_documents(docs)<br/>splits_ids = [<br/>    {"doc": split, "id": stable_hash_meta(split.metadata)} for split in splits<br/>]<br/><br/>existing_ids = docs_vectorstore.get()["ids"]<br/>new_splits_ids = [split for split in splits_ids if split["id"] not in existing_ids]<br/><br/>docs_vectorstore.add_documents(<br/>    documents=[split["doc"] for split in new_splits_ids],<br/>    ids=[split["id"] for split in new_splits_ids],<br/>)<br/>docs_vectorstore.persist()</span></pre><h1 id="914c" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Evaluation Questions</h1><p id="9c5b" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">For a common topic like Formula One, one can also use ChatGPT directly to generate general questions. In this article, four methods of question generation are used:</p><ul class=""><li id="fce1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk"><strong class="ml fr">GPT4</strong>: 30 questions were generated using ChatGPT 4 with the following prompt “Write 30 question about Formula one”<br/>– Random Example: “Which Formula 1 team is known for its prancing horse logo?”</li><li id="7356" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">GPT3.5:</strong> Another 199 question were generated with ChatGPT 3.5 with the following prompt “Write 100 question about Formula one” and repeating “Thanks, write another 100 please”<br/>– Example: “”Which driver won the inaugural Formula One World Championship in 1950?”</li><li id="8362" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Ragas_GPT4</strong>: 113 questions were generated using Ragas. Ragas utilizes the documents again and its own embedding model to construct a vector database, which is then used to generate questions with GPT4.<br/>– Example: “Can you tell me more about the performance of the Jordan 198 Formula One car in the 1998 World Championship?”</li><li id="3e3f" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Rags_GPT3.5</strong>: 226 additional questions were generated with Ragas — here we use GPT3.5<br/>– Example: “What incident occurred at the 2014 Belgian Grand Prix that led to Hamilton’s retirement from the race?”</li></ul><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="85d2" class="px ot fq pu b bg py pz l qa qb">from ragas.testset import TestsetGenerator<br/><br/>generator = TestsetGenerator.from_default(<br/>    openai_generator_llm="gpt-3.5-turbo-16k", <br/>    openai_filter_llm="gpt-3.5-turbo-16k"<br/>)<br/><br/>testset_ragas_gpt35 = generator.generate(docs, 100)</span></pre><p id="2502" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The questions and answers were not reviewed or modified in any way. All questions are combined in a single dataframe with the columns <code class="cx qj qk ql pu b">id</code>, <code class="cx qj qk ql pu b">question</code>, <code class="cx qj qk ql pu b">ground_truth</code>, <code class="cx qj qk ql pu b">question_by</code> and <code class="cx qj qk ql pu b">answer</code>.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qm"><img src="../Images/4e3c079da63fa016b279acfea51bef1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_74K0-_SJXyTxq6ovAcWg.png"/></div></div></figure><p id="d0bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, the questions will be posed to the RAG system. For over 500 questions, this can take some time and incur costs. If you ask the questions row-by-row, you can pause and continue the process or recover from a crash without losing the results so far:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="7a2e" class="px ot fq pu b bg py pz l qa qb">for i, row in df_questions_answers.iterrows():<br/>    if row["answer"] is None or pd.isnull(row["answer"]):<br/>        response = rag_chain.invoke(row["question"])<br/><br/>        df_questions_answers.loc[df_questions_answers.index[i], "answer"] = response[<br/>            "answer"<br/>        ]<br/>        df_questions_answers.loc[df_questions_answers.index[i], "source_documents"] = [<br/>            stable_hash_meta(source_document.metadata)<br/>            for source_document in response["source_documents"]<br/>        ]<br/></span></pre><p id="7e65" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Not only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><figure class="ni nj nk nl nm nn qt qu paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qs"><img src="../Images/5fc1f88bf931c7f85b2f884635d5ce03.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*umlKv7Qf9SSLzRslT2r0Qw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="bf07" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, the embeddings for all questions are generated and stored in the dataframe as well. This allows for visualizing them alongside the documents.</p><h1 id="800c" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Evaluation with Ragas</h1><p id="dad8" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk"><a class="af og" href="https://github.com/explodinggradients/ragas" rel="noopener ugc nofollow" target="_blank">Ragas</a> provides metrics for evaluating each component of your RAG pipeline in isolation and end-to-end metrics for overall performance:</p><ol class=""><li id="f8ce" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qv nz oa bk"><strong class="ml fr">Context Precision:</strong> Uses the <code class="cx qj qk ql pu b">question</code> and retrieved <code class="cx qj qk ql pu b">contexts</code> to measure the signal-to-noise ratio.</li><li id="3c8a" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Context Relevancy:</strong> Measures the relevance of the retrieved context to the question, calculated using the <code class="cx qj qk ql pu b">question</code> and <code class="cx qj qk ql pu b">contexts</code>.</li><li id="7ee4" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Context Recall:</strong> Based on the <code class="cx qj qk ql pu b">ground truth</code> and <code class="cx qj qk ql pu b">contexts</code> to check if all relevant information for the answer is retrieved.</li><li id="ec02" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Faithfulness:</strong> Utilizes the <code class="cx qj qk ql pu b">contexts</code> and <code class="cx qj qk ql pu b">answer</code> to measure how factually accurate the generated answer is.</li><li id="2481" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Answer Relevance:</strong> Computed using the <code class="cx qj qk ql pu b">question</code> and <code class="cx qj qk ql pu b">answer</code> to assess the relevance of the generated answer to the question (does not consider factuality).</li><li id="4dab" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Answer Semantic Similarity:</strong> Evaluated using the <code class="cx qj qk ql pu b">ground truth</code> and <code class="cx qj qk ql pu b">answer</code> to assess the semantic resemblance between the generated and the correct answer.</li><li id="975a" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Answer Correctness:</strong> Relies on the <code class="cx qj qk ql pu b">ground truth</code> and <code class="cx qj qk ql pu b">answer</code> to measure the accuracy and alignment of the generated answer with the correct one.</li><li id="4c9e" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne qv nz oa bk"><strong class="ml fr">Aspect Critique:</strong> Involves analyzing the <code class="cx qj qk ql pu b">answer</code> to evaluate submissions based on predefined or custom aspects such as correctness or harmfulness.</li></ol><p id="480c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For now, we focus on the end-to-end metric of answer correctness. The column names and content in the dataframe are copied and adapted to meet the naming and formatting requirements according to the Ragas API:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="5968" class="px ot fq pu b bg py pz l qa qb"># prepare the dataframe for evaluation<br/>df_qa_eval = df_questions_answers.copy()<br/><br/><br/># adapt the ground truth to the ragas naming and format<br/>df_qa_eval.rename(columns={"ground_truth": "ground_truths"}, inplace=True)<br/>df_qa_eval["ground_truths"] = [<br/>    [gt] if not isinstance(gt, list) else gt for gt in df_qa_eval["ground_truths"]<br/>]</span></pre><p id="57dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This again can take some time and even more money than just querying your RAG system. Let’s apply the evaluation row-by-row to be able to recover from a crash without losing the results so far:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="58d7" class="px ot fq pu b bg py pz l qa qb"># evaluate the answer correctness if not already done<br/>fields = ["question", "answer", "contexts", "ground_truths"]<br/>for i, row in df_qa_eval.iterrows():<br/>    if row["answer_correctness"] is None or pd.isnull(row["answer_correctness"]):<br/>        evaluation_result = evaluate(<br/>            Dataset.from_pandas(df_qa_eval.iloc[i : i + 1][fields]),<br/>            [answer_correctness],<br/>        )<br/>        df_qa_eval.loc[i, "answer_correctness"] = evaluation_result[<br/>            "answer_correctness"<br/>        ]<br/></span></pre><p id="301b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Afterwards, you can store the results in the <code class="cx qj qk ql pu b">df_questions_answer</code> dataframe:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="8f8d" class="px ot fq pu b bg py pz l qa qb">df_questions_answers["answer_correctness"] = df_qa_eval["answer_correctness"]</span></pre><h1 id="6a18" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Prepare visualization</h1><p id="ba08" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">To include the document snippets in the visualization, we add references from documents to questions that used the document as a source. Additionally, the count of questions referencing a document is stored:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="8bb5" class="px ot fq pu b bg py pz l qa qb"># Explode 'source_documents' so each document ID is in its own row alongside the question ID<br/>df_questions_exploded = df_qa_eval.explode("source_documents")<br/><br/># Group by exploded 'source_documents' (document IDs) and aggregate<br/>agg = (<br/>    df_questions_exploded.groupby("source_documents")<br/>    .agg(<br/>        num_questions=("id", "count"),  # Count of questions referencing the document<br/>        question_ids=(<br/>            "id",<br/>            lambda x: list(x),<br/>        ),  # List of question IDs referencing the document<br/>    )<br/>    .reset_index()<br/>    .rename(columns={"source_documents": "id"})<br/>)<br/><br/># Merge the aggregated information back into df_documents<br/>df_documents_agg = pd.merge(df_docs, agg, on="id", how="left")<br/><br/># Use apply to replace NaN values with empty lists for 'question_ids'<br/>df_documents_agg["question_ids"] = df_documents_agg["question_ids"].apply(<br/>    lambda x: x if isinstance(x, list) else []<br/>)<br/># Replace NaN values in 'num_questions' with 0<br/>df_documents_agg["num_questions"] = df_documents_agg["num_questions"].fillna(0)</span></pre><p id="cb42" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now concatenate the dataframe of questions with the dataframe of the documents</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="f691" class="px ot fq pu b bg py pz l qa qb">df = pd.concat([df_qa_eval, df_documents_agg], axis=0)</span></pre><p id="92bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, let’s prepare some different UMAP [3] mappings. You could do much the same in the Spotlight GUI later, but doing it upfront can save time.</p><ul class=""><li id="a931" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk">umap_all: UMAP with fit and transform applied on all document and question embeddings</li><li id="6531" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">umap_questions: UMAP with fit applied on questions embeddings only and transform applied on both</li><li id="7947" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk">umap_docs: UMAP with fit applied on document embeddings only and transform applied on both</li></ul><p id="cfd3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We prepare each of the UMAP transformations like this:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="5fa2" class="px ot fq pu b bg py pz l qa qb"><br/>umap = UMAP(n_neighbors=20, min_dist=0.15, metric="cosine", random_state=42).fit<br/>umap_all = umap.transform(df["embedding"].values.tolist())<br/>df["umap"] = umap_all.tolist()<br/></span></pre><p id="1c25" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="29cf" class="px ot fq pu b bg py pz l qa qb">question_embeddings = np.array(df[df["question"].notna()]["embedding"].tolist())<br/>df["nearest_question_dist"] = [  # brute force, could be optimized using ChromaDB<br/>    np.min([np.linalg.norm(np.array(doc_emb) - question_embeddings)])<br/>    for doc_emb in df["embedding"].values<br/>]</span></pre><p id="33f8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This metric can be helpful to find documents that are not referenced by questions.</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><figure class="ni nj nk nl nm nn qt qu paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qw"><img src="../Images/0ec8f60de7d7a00c65c3c4a825bec1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*YTRUXZmd0iX8kyPIdUUnlg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b175" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Visualize results</h1><p id="068a" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">If you skipped the previous steps, you can download the dataframe and load it with:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="b24c" class="px ot fq pu b bg py pz l qa qb">import pandas as pd<br/>df = pd.read_parquet("df_f1_rag_docs_and_questions.parquet")</span></pre><p id="caf2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">and start <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight</a> to visualize it with:</p><pre class="ni nj nk nl nm pt pu pv bp pw bb bk"><span id="2ea7" class="px ot fq pu b bg py pz l qa qb">from renumics import spotlight<br/><br/>spotlight.show(df)<br/>spotlight.show(<br/>    df,<br/>    layout="/home/markus/Downloads/layout_rag_1.json",<br/>    dtype={x: Embedding for x in df.keys() if "umap" in x},<br/>)</span></pre><p id="0923" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It will open a new browser window:</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><figure class="ni nj nk nl nm nn qt qu paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qx"><img src="../Images/6e6da163c2994fe3ee68adcf015fed02.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*IMbva0pP8RAVhoY4dVbjLg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Formula One documents and evaluation questions statistics and similarity maps — created by the author with <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8e6a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On the top left side, you can see a <strong class="ml fr">table of all questions and all document </strong>snippets. You can use the “visible columns” button to control which columns of the dataframe are shown in the table. It is useful to create a filter directly that selects only the questions to be able to turn the questions on and off in the visualizations: Select all questions and and then create a filter using the “Create filter from selected row” button.</p><p id="4b06" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To the right of the table, the <code class="cx qj qk ql pu b">answer correctness</code><strong class="ml fr"> is displayed as a metric </strong>across all questions. Below there are two <strong class="ml fr">histograms</strong>; the left one shows the distribution of <code class="cx qj qk ql pu b">answer correctness</code> divided into the different methods of question generation. The right one shows the distribution of methods of question generation. Here, it is advisable to create a filter for the questions using the filter button to display only the selected rows (the questions) if needed.</p><p id="98ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On the right side, there are <strong class="ml fr">two similarity maps.</strong> The first one uses the <code class="cx qj qk ql pu b">umap_questions</code> column and shows the questions and documents based on the transformation applied only to the questions. It is helpful for viewing the distribution of questions independently from the associated documents because this approach allows analysts to identify patterns or clusters within the questions themselves.</p><p id="e34b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The second similarity map shows the questions and documents based on the transformation applied only to the documents (<code class="cx qj qk ql pu b">umap_docs</code>). It is useful for viewing the questions in the context of their associated documents. A similarity map that simultaneously transforms questions and documents has proven to be less helpful with a larger number of questions, as more or fewer questions get clustered together and tend to be separated from the documents. Therefore, this representation is omitted here.</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><figure class="ni nj nk nl nm nn qt qu paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qx"><img src="../Images/15014bcc7e0dc0ea4f7b99ca395d4993.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*1wZrAj60hiw1T3RVnCuBtA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Formula One evaluation questions statistics and similarity maps — created by the author with <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="46f8" class="qy ot fq bf ou qz ra rb ox rc rd re pa ms rf rg rh mw ri rj rk na rl rm rn ro bk">Document Embedding Similarity Map: Observations</h2><p id="cf84" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">In the similarity map <code class="cx qj qk ql pu b">umap_docs</code>, you can identify areas in the embedding space of the documents that have no neighboring questions. It is even better recognized when selecting <code class="cx qj qk ql pu b">nearest_question_dist</code> for coloring.</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><figure class="ni nj nk nl nm nn qt qu paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rp"><img src="../Images/0d498a101193db93856b2220584aa1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*cMGNPnnBa9Bn7BJ05SzxBw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Similarity map of Formula One documents and questions (highlighted) — created by the author with <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cdf6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Some clusters can be identified, including snippets that contain only headings or tabular data containing only numbers page by page, whose meaning is lost during splitting. Additionally, many Wikipedia-specific text additions that contain no relevant information, such as links to other languages or editing notes, form clusters with no neighboring questions.</p><p id="4927" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Removing the noise in form of Wikipedia-related text is very simple when using the Wikipedia API. It is probably not particularly necessary, as it mainly costs some space — it is not expected that the RAG result will be particularly worsened by it. However, data contained in large tables are hardly captured by the RAG system and it could ne benifical to extract these using advanced pre-processing methods for Table Extraction and to connect them to the RAG system.</p><p id="676b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another point that you can observe in the <code class="cx qj qk ql pu b">umap_docs</code> similarity map is how the questions from different sources are distributed.</p></div></div><div class="nn"><div class="ab cb"><div class="lm qn ln qo lo qp cf qq cg qr ci bh"><div class="ni nj nk nl nm ab ke"><figure class="lb nn rq rr qt qu rs paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><img src="../Images/16c4d18dec8db36dc504497edf4a09a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*IH7z3J4yUmU0C_SruxnDkg.png"/></div></figure><figure class="lb nn rq rr qt qu rs paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><img src="../Images/e6efde6c3a40a1bdf15ae12e744d07f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*K4bADgDmSAr5t4t4r9VImQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx rt ed ru rv">Left: Questions generated from ChatGPT (GPT-3.5 and GPT-4), Right: Questions generated with ragas using GPT-3.5 and GPT-4 — created by the author with <a class="af og" href="https://github.com/Renumics/spotlight" rel="noopener ugc nofollow" target="_blank">Renumics Spotlight</a></figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cf50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The questions that were directly generated by ChatGPT (GPT-3.5, GPT-4) are located in a more confined area in the center, whereas the questions generated with ragas based on the documents cover a larger area.</p><h2 id="bef5" class="qy ot fq bf ou qz ra rb ox rc rd re pa ms rf rg rh mw ri rj rk na rl rm rn ro bk">Answer correctness histogram</h2><p id="1219" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">The histogram can be used as a starting point to get an initial impression of the global statistics of the data. Overall, across all questions, the <code class="cx qj qk ql pu b">answer correctness</code> is 0.45. For the questions created without ragas, it is 0.36, and for questions with ragas, it is 0.52. It was expected that the system would perform better for questions generated by ragas, as these questions are based on the available data, whereas the questions directly generated by ChatGPT could come from all the data with which ChatGPT was trained.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng rw"><img src="../Images/0823d03c0ade2f06c1123e122c0311a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*GsLBsg7uwTrw-AzvO4BHmw.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Histogram of the answer correctness colored by the source of the question - created by the author</figcaption></figure><p id="e4c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A quick, random manual review of some of the questions/answers and ground truth shows that in the interval of<code class="cx qj qk ql pu b">answer correctness</code>0.3–0.4, most questions were still correctly answered according to the ground truth. In the interval 0.2–0.3, many incorrect answers are present. In the interval 0.1–0.2, most answers are incorrect. Notably, almost all questions in this range came from GPT-3.5. The two questions in this interval generated with GPT-4 were answered correctly even though they received an <code class="cx qj qk ql pu b">answer correctness</code> of below 0.2.</p><h2 id="c1ec" class="qy ot fq bf ou qz ra rb ox rc rd re pa ms rf rg rh mw ri rj rk na rl rm rn ro bk">Questions Embedding Similarity Map: Observations</h2><p id="b616" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">The Questions Embedding Similarity Map can be helpful to dig deeper into <code class="cx qj qk ql pu b">answer correctness</code> by examining clusters of similar questions that may cause similar problems.</p><ul class=""><li id="7d90" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Term for driver/process/cars”:</strong> average <code class="cx qj qk ql pu b">answer correctness</code> 0.23: Answers often not precise enough. E.g., Chassis tuning vs. Chassis flexing or brake tuning vs. brake bias adjustment. It is questionable whether these types of questions are suitable for evaluating the system, as it seems very difficult to judge the answers.</li><li id="9493" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Terms for fuel strategy:”</strong> average <code class="cx qj qk ql pu b">answer correctness</code>0.44, similar to the global<code class="cx qj qk ql pu b">answer correctness</code>.</li><li id="b052" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Names of tracks”:</strong> average <code class="cx qj qk ql pu b">answer correctness</code> 0.49, similar to the global <code class="cx qj qk ql pu b">answer correctnes</code>.</li><li id="0fa5" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Who holds the record for…”</strong>: average <code class="cx qj qk ql pu b">answer correctness</code> 0.44, similar to the global <code class="cx qj qk ql pu b">answer correctness</code>.</li><li id="99e0" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Win championship with…”</strong>: average <code class="cx qj qk ql pu b">answer correctnes</code> 0.26 — looks challenging. Questions with many conditions, e.g., “Who is the only driver to win the Formula One World Championship with a British racing license, driving for an Italian team with an American engine.” Extended RAG methods like Multi Query might help improve here.</li><li id="7d12" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne ny nz oa bk"><strong class="ml fr">Cluster “Who is the only driver to win… with a car bearing the number &lt;number&gt;”</strong>: average <code class="cx qj qk ql pu b">answer correctness</code> 0.23 — looks like GPT-3.5 was lazy here, repeating the same question with different numbers, even though most ground truth entries are wrong!</li></ul><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rx"><img src="../Images/fe6a9f7fe12e115d68dc9cf1906b1461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yc03cpSEFlJoZSBPIpMkiQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Similarity map of Formula One questions (highlighted) and documents — created by the author</figcaption></figure><h1 id="3b02" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">Conclusion</h1><p id="ae27" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">In conclusion, utilizing UMAP-based visualizations offers a interesting approach to dig deeper than just analyzing global metrics. The document embedding similarity map gives a good overview, illustrating the clustering of similar documents and their relation to evaluation questions. The question similarity map reveals patterns that allow the differentiation and analysis of questions in conjunction with quality metrics to enable insight generation. Follow the Visualize results section to apply the visualization on your evaluation strategy — what insights will you uncover?</p></div></div></div><div class="ab cb ry rz sa sb" role="separator"><span class="sc by bm sd se sf"/><span class="sc by bm sd se sf"/><span class="sc by bm sd se"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="73d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qc">I am a professional with expertise in creating advanced software solutions for the interactive exploration of unstructured data. I write about unstructured data and use powerful visualization tools to analyze and make informed decisions.</em></p><h1 id="a80a" class="os ot fq bf ou ov ow gq ox oy oz gt pa pb qd pd pe pf qe ph pi pj qf pl pm pn bk">References</h1><p id="969b" class="pw-post-body-paragraph mj mk fq ml b go po mn mo gr pp mq mr ms pq mu mv mw pr my mz na ps nc nd ne fj bk">[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang: <a class="af og" href="https://arxiv.org/abs/2312.10997" rel="noopener ugc nofollow" target="_blank">Retrieval-Augmented Generation for Large Language Models: A Survey</a> (2024), arxiv</p><p id="23f6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Yixuan Tang, Yi Yang: <a class="af og" href="https://arxiv.org/abs/2401.15391" rel="noopener ugc nofollow" target="_blank">MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a> (2021), arXiv</p><p id="6f3e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Leland McInnes, John Healy, James Melville: <a class="af og" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a> (2018), arXiv</p><p id="e144" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert: <a class="af og" href="https://arxiv.org/abs/2309.15217" rel="noopener ugc nofollow" target="_blank">RAGAS: Automated Evaluation of Retrieval Augmented Generation</a> (2023), arXiv</p></div></div></div></div>    
</body>
</html>