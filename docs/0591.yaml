- en: Framework for Optimizing Generative AI to Meet Business Needs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/framework-for-optimizing-generative-ai-to-meet-business-needs-02ac6932d55d?source=collection_archive---------2-----------------------#2024-03-04](https://towardsdatascience.com/framework-for-optimizing-generative-ai-to-meet-business-needs-02ac6932d55d?source=collection_archive---------2-----------------------#2024-03-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The playbook for selecting right optimization strategy guided by clear business
    objectives to better meet the needs of customers.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sarthakh330?source=post_page---byline--02ac6932d55d--------------------------------)[![Sarthak
    Handa](../Images/0c75ba0f085fdb22a221705450047c40.png)](https://medium.com/@sarthakh330?source=post_page---byline--02ac6932d55d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--02ac6932d55d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--02ac6932d55d--------------------------------)
    [Sarthak Handa](https://medium.com/@sarthakh330?source=post_page---byline--02ac6932d55d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--02ac6932d55d--------------------------------)
    ·11 min read·Mar 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/087c9599cd2cdaa90b7b4119bf0d4642.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Dalle3'
  prefs: []
  type: TYPE_NORMAL
- en: Generating human-like text and speech was once only possible in science fiction.
    But the rapid evolution of Large Language Models (LLMs) like GPT-3 and PaLM has
    brought this vision closer to reality, unlocking a range of promising business
    applications from chatbots to content creation.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, the general-purpose foundation models often fail to meet the needs of industry
    use cases. Businesses have different requirements for their generative AI applications
    — from **performance**, **cost**, **latency** to **explainability**. Moreover,
    the nature and quantity of the data available for model training can differ significantly.
    It is therefore important for product teams to outline key business criteria for
    their generative AI application and select the right toolkit of optimization techniques
    to meet these needs.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we outline a framework for identifying and prioritizing strategic
    focus areas for your generative AI application. We will also explore popular optimization
    methods and discuss their unique strengths, ideal applications, and trade-offs
    in meeting the application requirements. With the right optimization strategy
    guided by clear business objectives, companies can develop custom AI solutions
    that balance the priorities critical to their success. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Framework to Assess Business Needs & Constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To tailor the strategy for optimizing LLMs effectively, product teams should
    start by building a deep understanding of the business objectives and the constraints
    within which they’re operating. Assess and prioritize the key dimensions listed
    below for your business use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9208171a2136d7bb407d35336b95456d.png)'
  prefs: []
  type: TYPE_IMG
- en: SourceL Author
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Performance Goal**: Define the measure and level of performance your
    AI needs to achieve. This could be combination of factual accuracy, alignment
    with human values, or other task-specific metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *What are the best dimensions for measuring performance?
    What is the minimum acceptable performance bar? How does performance align with
    user expectations in your industry?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. Latency Targets**: Determine the maximum response time that your application
    can afford without negatively impacting user experience. This could be especially
    important when LLMs are deployed in time-sensitive or resource-constrained scenarios
    (e.g., voice assistant, edge devices).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *How does latency impact user satisfaction and retention?
    What are industry standards for response time?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**3\. Cost Efficiency**: Evaluate the cost of operating AI with the expected
    ROI. Higher initial costs may be justified when they lead to substantial savings,
    revenue growth, or strategic benefits that outweigh investment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *How does the cost of operating LLMs impact your
    budget? How does the ROI compare with the cost of AI deployment?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**4\. Explainability & Trust:** Determine if there is a need to ensure that
    the AI decisions are easily understood by users, which is critical for building
    trust, especially in fields with stringent regulatory demands.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *Is your industry regulated, requiring transparency
    in AI’s decisions? How does explainability affect user trust and adoption?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**5\. External Knowledge**: Assess if your AI needs access to external data
    sources to remain relevant and provide accurate responses.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *Does your AI need real-time data to make decisions?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**6\. Data Availability**: The nature and quantity of data available for training
    your AI could widely impact optimization strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Questions to Consider**: *Do you have access to a large dataset for training,
    or will you need to use synthetic or augmented data? How often will you need to
    update the training data to keep the AI relevant?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Presented below is a table outlining **three distinct use cases** for generative
    AI applications, with a corresponding evaluation of priorities for each dimension
    within the framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68e86d6781466a2dd27403007758c812.png)'
  prefs: []
  type: TYPE_IMG
- en: SourceL Author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the priorities and constrains can vary widely across different
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider a company aiming to develop a *customer support chatbot*
    to ease the workload on human staff. In this scenario, **accuracy performance**
    and **external data integration** are of high priority to deliver responses that
    are not only factually correct but also up-to-date. While **latency** holds some
    significance, users may be willing to tolerate brief delays. Typically, such a
    company will have access to an **extensive archive** of past customer support
    interactions that can be used for training models.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the critical application of AI for *assessing software code quality
    and risk* demands a increased focus on **factual accuracy** and **explainability**
    of the AI’s insights, often due to the potential consequences of errors. **Cost**
    and **latency** are secondary considerations in this context. This use case could
    benefit from **external data integration** in some cases, and it usually faces
    constraints regarding the availability of rich training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: A solid understanding of strategic priorities and constraints associated with
    the use case can help teams develop a tailored strategy for optimizing LLMs to
    meet the unique needs of the users.
  prefs: []
  type: TYPE_NORMAL
- en: Diving Deeper Into LLM Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section delves into the various optimization techniques, highlighting their
    objectives, ideal use-cases, and inherent trade-offs, particularly in the light
    of balancing the business goals discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: '**Techniques Table Breakdown:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2a403e1382512ae0bca1ffd4ed8cd8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Prompt Engineering:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execution Complexity**: Low'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to Use**: For reshaping response and quick improvement without altering
    the model. Start with this technique to maximize a pre-trained model’s effectiveness
    before trying more complex optimization methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What it entails:** Prompt engineering involves crafting the input query to
    a model in a way that elicits the desired output. It requires understanding how
    the model responds to different types of instructions but doesn’t require retraining
    the model or altering its architecture. This method merely optimizes the way the
    existing model accesses and applies its pre-trained knowledge, and does not enhance
    the model’s intrinsic capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: '*“It’s like adjusting the way you ask a question to a knowledgeable friend
    to get the best possible answer.”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples:**'
  prefs: []
  type: TYPE_NORMAL
- en: Asking a language model to “Write a poem in the style of Shakespeare” versus
    “Write a poem” to elicit a response in a specific literary style.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a detailed scenario in prompt for a conversational AI to ensure the
    model understands its role as customer service agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trial & Error:** Designing the most effective prompt requires iterations,
    since relationship between prompt and AI output is not always intuitive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Quality:** The quality of the output is highly dependent on the design
    of the prompt, and there are limitations to the level of improvements that you
    can achieve through this method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2\. Fine-Tuning:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execution Complexity**: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to Use**: Fine-tuning should be considered when you need the model to
    adapt to a specific domain or task that may not be well-covered by the base pre-trained
    model. It is a step towards increasing domain specific accuracy and creating a
    more specialized model that can handle domain specific data and terminology.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What it entails:** Fine-tuning is the process of continuing the training
    of a pre-trained model on a new dataset that is representative of the target task
    or domain. This new dataset consists of input-output pairs that provide examples
    of the desired behavior. During fine-tuning, the model’s weights are updated to
    minimize the loss on this new dataset, effectively adapting the model to the new
    domain.'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Think of it as giving your friend a crash course on a topic you want them
    to become an expert in; showing them multiple examples of questions that may come
    in a test and the sample answers that they are expected to respond with.”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**:'
  prefs: []
  type: TYPE_NORMAL
- en: A general-purpose language model can be fine-tuned on legal documents to improve
    its performance for reviewing such documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An image recognition model can be fine-tuned with medical imaging datasets to
    better identify specific diseases in X-rays or MRIs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Requirement:** Fine-tuning requires a labeled dataset that is relevant
    to the task, which can be resource-intensive to create.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting Risk:** There is a potential risk of the model becoming too specialized
    to the fine-tuning data, which can decrease its ability to generalize to other
    contexts or datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3\. Retrieval-Augmented Generation (RAG):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execution Complexity**: High'
  prefs: []
  type: TYPE_NORMAL
- en: '**When** **to use**: RAG should be considered when there is a need for the
    AI model to access and incorporate external information to generate responses.
    This is especially relevant when the model is expected to provide up-to-date or
    highly specific information that is not contained within its pre-trained knowledge
    base.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What it entails:** RAG combines the generative capabilities of an LLM with
    a retrieval system. The retrieval system queries a database, knowledge base, or
    the internet to find information relevant to the input prompt. The retrieved information
    is then provided to the language model, which incorporates this context to generate
    a richer and more accurate response. By citing the sources used by the RAG system
    to generate responses, generative AI applications can offer enhanced explainability
    to the users.'
  prefs: []
  type: TYPE_NORMAL
- en: In the coming years, this optimization technique is expected to gain widespread
    popularity as an increasing number of products seek to leverage their latest business
    data to tailor experiences for customers.
  prefs: []
  type: TYPE_NORMAL
- en: '*“It’s akin to your friend being able to look up information online to answer
    questions that are outside their immediate expertise. It’s an open book exam.”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**:'
  prefs: []
  type: TYPE_NORMAL
- en: In a RAG-based online chatbot, retriever can pull relevant information from
    a database or the internet to provide up-to-date answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A homework assistant AI could use RAG to fetch the most recent scientific data
    to answer a student’s question about climate change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complex Implementation:** RAG systems require a well-integrated retrieval
    system, which can be challenging to set up and maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality of Information:** The usefulness of the generated response is highly
    dependent on the relevance and accuracy of retrieved information. If the retrieval
    system’s sources are outdated or incorrect, the responses will reflect that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow Response Time:** Retrieving information from external source to generate
    response can add latency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4\. Reinforcement Learning from Human Feedback (RLHF):'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execution Complexity**: Very High'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use**: RLHF should be used when the model’s outputs need to align
    closely with complex human judgments and preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What it entails:** RLHF is a sophisticated reinforcement learning technique
    that refines a model’s behavior by incorporating human evaluations directly into
    the training process. This process typically involves collecting data from human
    operators who rank the outputs from AI on various quality metrics such as relevance,
    helpfulness, tone, etc. These data signals are then used to train a reward model,
    which guides the reinforcement learning process to produce outputs that are more
    closely aligned with human preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: '*“It’s similar to your friend learning from past conversations about what makes
    a discussion enjoyable, and using that knowledge to improve future interactions.”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**:'
  prefs: []
  type: TYPE_NORMAL
- en: A social media platform could use RLHF to train a moderation bot that not only
    identifies inappropriate content but also responds to users in a way that is constructive
    and sensitive to context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A virtual assistant could be fine-tuned using RLHF to provide more personalized
    and context-aware responses to user requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High Complexity:** RLHF involves complex, resource-intensive processes, including
    human feedback gathering, reward modeling, and reinforcement learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality Risk:** There’s a risk of bias in the feedback data, which can lead
    to affect model quality. Ensuring consistent quality of human feedback and aligning
    the reward model with desired outcomes can be difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. Knowledge Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execution Complexity**: Moderate to High'
  prefs: []
  type: TYPE_NORMAL
- en: '**When to use**: Knowledge distillation is used when you need to deploy sophisticated
    models on devices with limited computational power or in applications where response
    time is critical.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What it entails:** It’s a compression technique where a smaller, more efficient
    model (known as the student) is trained to replicate the performance of a larger,
    more complex model (the teacher). The training goes beyond just learning the correct
    answers (hard targets), and involves the student trying to produce similar probabilities
    as the teacher’s predictions (soft targets). This approach enables the student
    model to capture the nuanced patterns and insights that teacher model has learned.'
  prefs: []
  type: TYPE_NORMAL
- en: '*“This is similar to distilling the wisdom of a seasoned expert into a concise
    guidebook that a novice can use to make expert-level decisions without going through
    years of experience.”*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Examples**:'
  prefs: []
  type: TYPE_NORMAL
- en: A large-scale language model could be distilled into a smaller model that runs
    efficiently on smartphones for real-time language translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An image recognition system used in autonomous vehicles can be distilled into
    a light model that can run on vehicle’s onboard computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trade-offs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance vs. Size:** The distilled model may not always match the performance
    of the teacher model, leading to a potential decrease in accuracy or quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training Complexity**: The distillation process is time-consuming and involves
    careful experimentation to ensure the student model learns effectively. It requires
    a deep understanding of models’ architectures and the ability to translate knowledge
    from one to another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s take a look at a real-world example in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Customer Support Chatbot'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s revisit the use case of building customer support chatbot to reduce workload
    on human support staff.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9afcbd5ed94718665a3356efa82af4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Dalle'
  prefs: []
  type: TYPE_NORMAL
- en: 'The requirements/ constraints included:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance**: High Priority (Emphasis on Factual Accuracy)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**External Knowledge**: High Priority'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Latency Targets**: Medium Priority'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cost Efficiency**: Low Priority'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Explainability & Trust**: Medium Priority'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Availability**: Ample (Past Conversations Data)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the clear understanding of business context and priorities, product builders
    can devise the most effective optimization strategy.
  prefs: []
  type: TYPE_NORMAL
- en: '**LLM Optimization Decision Steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt Engineering** should serve as the first step to improve chatbot’s
    initial understanding and response capabilities. However, this alone is unlikely
    to suffice for specialized domain accuracy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fine-Tuning** the model with historic customer conversation data is critical
    for boosting chatbot’s accuracy performance, and making the model adept at handling,
    nuanced industry-specific inquiries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Incorporating **Retrieval-Augmented Generation (RAG)** is vital for providing
    users up-to-date product information and relevant web links.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While a certain degree of latency is tolerable, monitoring and potentially **optimizing
    response times** will still be advisable. Optimization strategies here could include
    caching common queries to speed up responses and using prompt engineering strategically
    to reduce unnecessary external data retrievals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, a combination of strategies is often necessary to meet the specific
    demands of a use case. Flexibility in optimization strategies can be crucial,
    as requirements can change over time, and systems need to balance multiple requirements
    simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Optimizing LLMs for a business use case is both an art and a science, which
    requires a deep understanding of the underlying technology and the objectives
    at hand. As AI continues to evolve, the choice of optimization techniques will
    become increasingly strategic, influencing not only the performance of individual
    applications but also the overall trajectory of AI’s role in society.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you’re optimizing for speed, accuracy, cost, or transparency, the techniques
    discussed above offer a toolkit for enhancing LLMs to meet the demands of tomorrow’s
    generative AI powered business applications. By thoughtfully applying these methods,
    we can create AI that’s not only effective but also responsible and attuned to
    the nuanced needs of users.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks for reading! If these insights resonate with you or spark new thoughts,
    let’s continue the conversation. Share your perspectives in the comments below
    or connect with me on* [***LinkedIn***](https://www.linkedin.com/)*.*'
  prefs: []
  type: TYPE_NORMAL
