- en: Serve Multiple LoRA Adapters with vLLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/serve-multiple-lora-adapters-with-vllm-5323b0425b82?source=collection_archive---------6-----------------------#2024-08-03](https://towardsdatascience.com/serve-multiple-lora-adapters-with-vllm-5323b0425b82?source=collection_archive---------6-----------------------#2024-08-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Without any increase in latency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--5323b0425b82--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--5323b0425b82--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5323b0425b82--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5323b0425b82--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--5323b0425b82--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5323b0425b82--------------------------------)
    ·6 min read·Aug 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49568640e65f99e6f16e6765da72f2a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: With a LoRA adapter, we can specialize a large language model (LLM) for a task
    or a domain. The adapter must be loaded on top of the LLM to be used for inference.
    For some applications, it might be useful to serve users with multiple adapters.
    For instance, one adapter could perform function calling and another could perform
    a very different task, such as classification, translation, or other language
    generation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, to use multiple adapters, a standard inference framework would have
    first to unload the current adapter and then load the new adapter. This unload/load
    sequence can take several seconds which would degrade the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are open source frameworks that can serve multiple adapters
    at the same time without any noticeable time between the use of two different
    adapters. For instance, [vLLM](https://github.com/vllm-project/vllm) (Apache 2.0
    license), one of the most efficient open source inference frameworks, can easily
    run and serve multiple LoRA adapters simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how to use vLLM with multiple LoRA adapters. I
    explain how to use LoRA adapters with offline inference and how to serve several
    adapters to users for online inference. I use Llama 3 for the examples with adapters
    for function calling and chat.
  prefs: []
  type: TYPE_NORMAL
