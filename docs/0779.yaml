- en: How does the Segment-Anything Model’s (SAM’s) decoder work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-decoder-work-0e4ab4732c37?source=collection_archive---------5-----------------------#2024-03-24](https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-decoder-work-0e4ab4732c37?source=collection_archive---------5-----------------------#2024-03-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--0e4ab4732c37--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--0e4ab4732c37--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0e4ab4732c37--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0e4ab4732c37--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--0e4ab4732c37--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0e4ab4732c37--------------------------------)
    ·18 min read·Mar 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b352f48ee952208b5f82ab47be181458.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [dylan nolte](https://unsplash.com/@dylan_nolte?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: A deep dive into how the Segment-Anything model’s decoding procedure, with a
    focus on how its self-attention and cross-attention mechanism works.
  prefs: []
  type: TYPE_NORMAL
- en: This article only focuses on SAM’s decoder. For people interested in SAM’s encoder,
    please see my other article “[How Does the Segment-Anything Model’s (SAM’s) encoder
    work?](https://medium.com/towards-data-science/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b)”.
  prefs: []
  type: TYPE_NORMAL
- en: The Segment-Anything (SAM) model is a 2D interactive segmentation model, or
    guided model. SAM requires user prompts to segment an image. These prompts tell
    the model where to segment. The output of the model is a set of segmentation masks
    at different levels and a confidence score associated with each mask.
  prefs: []
  type: TYPE_NORMAL
- en: A segmentation mask is a 2D binary array with the same size as the input image.
    In this 2D array, an entry at location *(x, y)* has a value 1 if the model thinks
    that the pixel at location *(x, y)* belongs to the segmented area. Otherwise,
    the entry is 0\. Those confidence scores indicate model’s belief on the quality
    of each segmentation, higher score means higher quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network architecture of SAM consists of an encoder and a decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder takes in the image and user prompt inputs to produce image embedding,
    image positional embedding and user prompt embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder takes in the…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
