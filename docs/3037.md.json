["```py\nollama pull llama3.1:8b\n```", "```py\npip install -qU langchain_ollama \n```", "```py\nfrom langchain_ollama import OllamaLLM\n\nllm = OllamaLLM(model=\"llama3.1:8b\")\nllm.invoke(\"How are you?\")\n# I'm just a computer program, so I don't have feelings or emotions \n# like humans do. I'm functioning properly and ready to help with \n# any questions or tasks you may have! How can I assist you today?\n```", "```py\ndef get_llama_prompt(user_message, system_message=\"\"):\n  system_prompt = \"\"\n  if system_message != \"\":\n    system_prompt = (\n      f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system_message}\"\n      f\"<|eot_id|>\"\n    )\n  prompt = (f\"<|begin_of_text|>{system_prompt}\"\n            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n            f\"{user_message}\"\n            f\"<|eot_id|>\"\n            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n           )\n  return prompt   \n\nsystem_prompt = '''\nYou are Rudolph, the spirited reindeer with a glowing red nose, \nbursting with excitement as you prepare to lead Santa's sleigh \nthrough snowy skies. Your joy shines as brightly as your nose, \neager to spread Christmas cheer to the world!\nPlease, answer questions concisely in 1-2 sentences.\n'''\nprompt = get_llama_prompt('How are you?', system_prompt)\nllm.invoke(prompt)\n\n# I'm feeling jolly and bright, ready for a magical night! \n# My shiny red nose is glowing brighter than ever, just perfect \n# for navigating through the starry skies. \n```", "```py\ngenerate_query_system_prompt = '''\nYou are a senior data analyst with more than 10 years of experience writing complex SQL queries. \nThere are two tables in the database with the following schemas. \n\nTable: ecommerce.users \nDescription: customers of the online shop\nFields: \n- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004\n- country (string) - country of residence, for example, \"Netherlands\" or \"United Kingdom\"\n- is_active (integer) - 1 if customer is still active and 0 otherwise\n- age (integer) - customer age in full years, for example, 31 or 72\n\nTable: ecommerce.sessions \nDescription: sessions of usage the online shop\nFields: \n- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004\n- session_id (integer) - unique identifier of session, for example, 106 or 1023\n- action_date (date) - session start date, for example, \"2021-01-03\" or \"2024-12-02\"\n- session_duration (integer) - duration of session in seconds, for example, 125 or 49\n- os (string) - operation system that customer used, for example, \"Windows\" or \"Android\"\n- browser (string) - browser that customer used, for example, \"Chrome\" or \"Safari\"\n- is_fraud (integer) - 1 if session is marked as fraud and 0 otherwise\n- revenue (float) - income in USD (the sum of purchased items), for example, 0.0 or 1506.7\n\nWrite a query in ClickHouse SQL to answer the following question. \nAdd \"format TabSeparatedWithNames\" at the end of the query to get data from ClickHouse database in the right format. \n'''\n```", "```py\nprompt = get_llama_prompt('How many customers made purchase in December 2024?', \n  generate_query_system_prompt)\nllm.invoke(prompt)\n\n# To find out how many customers made a purchase in December 2024, \n# we need to join the `sessions` table with the `users` table on the `user_id`, \n# and then filter by `action_date` for sessions that took place in December 2024\\. \n\n# Here is the query:\n# \n# ```", "```py\n\n# This query first joins the `sessions` table with the `users` table \n# on the `user_id`. Then it filters out sessions where no purchase \n# was made (revenue is greater than 0). Finally, it selects the distinct \n# user IDs of these customers.\n\n# When you run this query in ClickHouse SQL, you can add \"format TabSeparatedWithNames\" \n# at the end to get the result in the right format:\n\n# ```", "```py\n```", "```py\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.tools import tool\n\n@tool\ndef execute_query(comments: str, query: str) -> str:\n  \"\"\"Excutes SQL query.\n\n  Args:\n      comments (str): 1-2 sentences describing the result SQL query \n          and what it does to answer the question,\n      query (str): SQL query\n  \"\"\"\n  pass \n\nchat_llm = ChatOllama(model=\"llama3.1:8b\").bind_tools([execute_query])\nresult = chat_llm.invoke(prompt)\nprint(result.tool_calls)\n\n# [{'name': 'execute_query',\n#   'args': {'comments': 'SQL query returns number of customers who made a purchase in December 2024\\. The query joins the sessions and users tables based on user ID to filter out inactive customers and find those with non-zero revenue in December 2024.',\n#   'query': 'SELECT COUNT(DISTINCT T2.user_id) FROM ecommerce.sessions AS T1 INNER JOIN ecommerce.users AS T2 ON T1.user_id = T2.user_id WHERE YEAR(T1.action_date) = 2024 AND MONTH(T1.action_date) = 12 AND T2.is_active = 1 AND T1.revenue > 0'},\n#   'type': 'tool_call'}]\n```", "```py\n[\n  {\n    \"question\": \"How many customers made purchase in December 2024?\",\n    \"sql_query\": \"select uniqExact(user_id) as customers from ecommerce.sessions where (toStartOfMonth(action_date) = '2024-12-01') and (revenue > 0) format TabSeparatedWithNames\"\n  },\n  {\n    \"question\": \"What was the fraud rate in 2023, expressed as a percentage?\",\n    \"sql_query\": \"select 100*uniqExactIf(user_id, is_fraud = 1)/uniqExact(user_id) as fraud_rate from ecommerce.sessions where (toStartOfYear(action_date) = '2023-01-01') format TabSeparatedWithNames\"\n  },\n  ...\n]\n```", "```py\nimport json\nwith open('golden_set.json', 'r') as f:\n  golden_set = json.loads(f.read())\n\ngolden_df = pd.DataFrame(golden_set) \ngolden_df['id'] = list(range(golden_df.shape[0]))\n```", "```py\ndef generate_query(question):\n  prompt = get_llama_prompt(question, generate_query_system_prompt)\n  result = chat_llm.invoke(prompt)\n  try:\n    generated_query = result.tool_calls[0]['args']['query']\n  except:\n    generated_query = ''\n  return generated_query\n\nimport tqdm\n\ntmp = []\nfor rec in tqdm.tqdm(golden_df.to_dict('records')):\n  generated_query = generate_query(rec['question'])\n  tmp.append(\n    {\n      'id': rec['id'],\n      'generated_query': generated_query\n    }\n  )\n\neval_df = golden_df.merge(pd.DataFrame(tmp))\n```", "```py\nCH_HOST = 'http://localhost:8123' # default address \nimport requests\nimport io\n\ndef get_clickhouse_data(query, host = CH_HOST, connection_timeout = 1500):\n  # pushing model to return data in the format that we want\n  if not 'format tabseparatedwithnames' in query.lower():\n    return \"Database returned the following error:\\n Please, specify the output format.\"\n\n  r = requests.post(host, params = {'query': query}, \n    timeout = connection_timeout)\n  if r.status_code == 200:\n    return r.text\n  else: \n    return 'Database returned the following error:\\n' + r.text\n    # giving feedback to LLM instead of raising exception\n```", "```py\ntmp = []\n\nfor rec in tqdm.tqdm(eval_df.to_dict('records')):\n  golden_output = get_clickhouse_data(rec['sql_query'])\n  generated_output = get_clickhouse_data(rec['generated_query'])\n\n  tmp.append(\n    {\n      'id': rec['id'],\n      'golden_output': golden_output,\n      'generated_output': generated_output\n    }\n  )\n\neval_df = eval_df.merge(pd.DataFrame(tmp))\n```", "```py\ndef is_valid_output(s):\n  if s.startswith('Database returned the following error:'):\n    return 'error'\n  if len(s.strip().split('\\n')) >= 1000:\n    return 'too many rows'\n  return 'ok'\n\neval_df['golden_output_valid'] = eval_df.golden_output.map(is_valid_output)\neval_df['generated_output_valid'] = eval_df.generated_output.map(is_valid_output)\n```", "```py\nfrom langchain_openai import ChatOpenAI\n\naccuracy_system_prompt = '''\nYou are a senior and very diligent QA specialist and your task is to compare data in datasets. \nThey are similar if they are almost identical, or if they convey the same information. \nDisregard if column names specified in the first row have different names or in a different order.\nFocus on comparing the actual information (numbers). If values in datasets are different, then it means that they are not identical.\nAlways execute tool to provide results.\n'''\n\n@tool\ndef compare_datasets(comments: str, score: int) -> str:\n  \"\"\"Stores info about datasets.\n  Args:\n      comments (str): 1-2 sentences about the comparison of datasets,\n      score (int): 0 if dataset provides different values and 1 if it shows identical information\n  \"\"\"\n  pass\n\naccuracy_chat_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature = 0.0)\\\n  .bind_tools([compare_datasets])\n\naccuracy_question_tmp = '''\nHere are the two datasets to compare delimited by ####\nDataset #1: \n####\n{dataset1}\n####\nDataset #2: \n####\n{dataset2}\n####\n'''\n\ndef get_openai_prompt(question, system):\n  messages = [\n    (\"system\", system),\n    (\"human\", question)\n  ]\n  return messages\n```", "```py\nprompt = get_openai_prompt(accuracy_question_tmp.format(\n  dataset1 = 'customers\\n114032\\n', dataset2 = 'customers\\n114031\\n'),\n  accuracy_system_prompt)\n\naccuracy_result = accuracy_chat_llm.invoke(prompt)\naccuracy_result.tool_calls[0]['args']\n# {'comments': 'The datasets contain different customer counts: 114032 in Dataset #1 and 114031 in Dataset #2.',\n#  'score': 0}\n\nprompt = get_openai_prompt(accuracy_question_tmp.format(\n  dataset1 = 'users\\n114032\\n', dataset2 = 'customers\\n114032\\n'),\n  accuracy_system_prompt)\naccuracy_result = accuracy_chat_llm.invoke(prompt)\naccuracy_result.tool_calls[0]['args']\n# {'comments': 'The datasets contain the same numerical value (114032) despite different column names, indicating they convey identical information.',\n#  'score': 1}\n```", "```py\ndef is_answer_accurate(output1, output2):\n  prompt = get_openai_prompt(\n    accuracy_question_tmp.format(dataset1 = output1, dataset2 = output2),\n    accuracy_system_prompt\n  )\n\n  accuracy_result = accuracy_chat_llm.invoke(prompt)\n\n  try:\n    return accuracy_result.tool_calls[0]['args']['score']\n  except:\n    return None\n```", "```py\n def evaluate_sql_agent(generate_query_func, golden_df):\n\n  # generating SQL\n  tmp = []\n  for rec in tqdm.tqdm(golden_df.to_dict('records')):\n    generated_query = generate_query_func(rec['question'])\n    tmp.append(\n      {\n          'id': rec['id'],\n          'generated_query': generated_query\n      }\n    )\n\n  eval_df = golden_df.merge(pd.DataFrame(tmp))\n\n  # executing SQL queries\n  tmp = []\n  for rec in tqdm.tqdm(eval_df.to_dict('records')):\n    golden_output = get_clickhouse_data(rec['sql_query'])\n    generated_output = get_clickhouse_data(rec['generated_query'])\n\n    tmp.append(\n      {\n        'id': rec['id'],\n        'golden_output': golden_output,\n        'generated_output': generated_output\n      }\n    )\n\n  eval_df = eval_df.merge(pd.DataFrame(tmp))\n\n  # checking accuracy\n  eval_df['golden_output_valid'] = eval_df.golden_output.map(is_valid_output)\n  eval_df['generated_output_valid'] = eval_df.generated_output.map(is_valid_output)\n\n  eval_df['correct_output'] = list(map(\n    is_answer_accurate,\n    eval_df['golden_output'],\n    eval_df['generated_output']\n  ))\n\n  eval_df['accuracy'] = list(map(\n    lambda x, y: 'invalid: ' + x if x != 'ok' else ('correct' if y == 1 else 'incorrect'),\n    eval_df.generated_output_valid,\n    eval_df.correct_output\n  ))\n\n  valid_stats_df = (eval_df.groupby('golden_output_valid')[['id']].count().rename(columns = {'id': 'golden set'}).join(\n    eval_df.groupby('generated_output_valid')[['id']].count().rename(columns = {'id': 'generated'}), how = 'outer')).fillna(0).T\n\n  fig1 = px.bar(\n    valid_stats_df.apply(lambda x: 100*x/valid_stats_df.sum(axis = 1)),\n    orientation = 'h', \n    title = '<b>LLM SQL Agent evaluation</b>: query validity',\n    text_auto = '.1f',\n    color_discrete_map = {'ok': '#00b38a', 'error': '#ea324c', 'too many rows': '#f2ac42'},\n    labels = {'index': '', 'variable': 'validity', 'value': 'share of queries, %'}\n  )\n  fig1.show()\n\n  accuracy_stats_df = eval_df.groupby('accuracy')[['id']].count()\n  accuracy_stats_df['share'] = accuracy_stats_df.id*100/accuracy_stats_df.id.sum()\n\n  fig2 = px.bar(\n    accuracy_stats_df[['share']],\n    title = '<b>LLM SQL Agent evaluation</b>: query accuracy',\n    text_auto = '.1f', orientation = 'h',\n    color_discrete_sequence = ['#0077B5'],\n    labels = {'index': '', 'variable': 'accuracy', 'value': 'share of queries, %'}\n  )\n\n  fig2.update_layout(showlegend = False)\n  fig2.show()\n\n  return eval_df\n```", "```py\nreflection_user_query_tmpl = '''\nYou've got the following question: \"{question}\". \nYou've generated the SQL query: \"{query}\".\nHowever, the database returned an error: \"{output}\". \nPlease, revise the query to correct mistake. \n'''\n\ndef generate_query_reflection(question):\n  generated_query = generate_query(question) \n  print('Initial query:', generated_query)\n\n  db_output = get_clickhouse_data(generated_query)\n  is_valid_db_output = is_valid_output(db_output)\n  if is_valid_db_output == 'too many rows':\n    db_output = \"Database unexpectedly returned more than 1000 rows.\"\n\n  if is_valid_db_output == 'ok': \n    return generated_query\n\n  reflection_user_query = reflection_user_query_tmpl.format(\n    question = question,\n    query = generated_query,\n    output = db_output\n  )\n\n  reflection_prompt = get_llama_prompt(reflection_user_query, \n    generate_query_system_prompt) \n  reflection_result = chat_llm.invoke(reflection_prompt)\n\n  try:\n    reflected_query = reflection_result.tool_calls[0]['args']['query']\n  except:\n    reflected_query = ''\n  print('Reflected query:', reflected_query)\n  return reflected_query\n```", "```py\nrefl_eval_df = evaluate_sql_agent(generate_query_reflection, golden_df)\n```", "```py\nfrom langchain_chroma import Chroma\nvector_store = Chroma(embedding_function=embeddings)\n```", "```py\nfrom langchain_openai import OpenAIEmbeddings\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n```", "```py\nwith open('rag_set.json', 'r') as f:\n    rag_set = json.loads(f.read())\nrag_set_df = pd.DataFrame(rag_set)\n\nrag_set_df['formatted_txt'] = list(map(\n    lambda x, y: 'Question: %s; Answer: %s' % (x, y),\n    rag_set_df.question,\n    rag_set_df.sql_query\n))\n\nrag_string_data = '\\n\\n'.join(rag_set_df.formatted_txt)\n```", "```py\nfrom langchain_text_splitters import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter(\n    separator=\"\\n\\n\",\n    chunk_size=1, # to split by character without merging\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False,\n)\n\ntexts = text_splitter.create_documents([rag_string_data])\n```", "```py\ndocument_ids = vector_store.add_documents(documents=texts)\nprint(vector_store._collection.count())\n# 32\n```", "```py\nquestion = 'What was the share of users using Windows yesterday?'\nretrieved_docs = vector_store.similarity_search(question, 3)\ncontext = \"\\n\\n\".join(map(lambda x: x.page_content, retrieved_docs))\nprint(context)\n\n# Question: What was the share of users using Windows the day before yesterday?; \n# Answer: select 100*uniqExactIf(user_id, os = 'Windows')/uniqExact(user_id) as windows_share from ecommerce.sessions where (action_date = today() - 2) format TabSeparatedWithNames\n# Question: What was the share of users using Windows in the last week?; \n# Answer: select 100*uniqExactIf(user_id, os = 'Windows')/uniqExact(user_id) as windows_share from ecommerce.sessions where (action_date >= today() - 7) and (action_date < today()) format TabSeparatedWithNames\n# Question: What was the share of users using Android yesterday?; \n# Answer: select 100*uniqExactIf(user_id, os = 'Android')/uniqExact(user_id) as android_share from ecommerce.sessions where (action_date = today() - 1) format TabSeparatedWithNames\n```", "```py\ngenerate_query_system_prompt_with_examples_tmpl = '''\nYou are a senior data analyst with more than 10 years of experience writing complex SQL queries. \nThere are two tables in the database you're working with with the following schemas. \n\nTable: ecommerce.users \nDescription: customers of the online shop\nFields: \n- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004\n- country (string) - country of residence, for example, \"Netherlands\" or \"United Kingdom\"\n- is_active (integer) - 1 if customer is still active and 0 otherwise\n- age (integer) - customer age in full years, for example, 31 or 72\n\nTable: ecommerce.sessions \nDescription: sessions of usage the online shop\nFields: \n- user_id (integer) - unique identifier of customer, for example, 1000004 or 3000004\n- session_id (integer) - unique identifier of session, for example, 106 or 1023\n- action_date (date) - session start date, for example, \"2021-01-03\" or \"2024-12-02\"\n- session_duration (integer) - duration of session in seconds, for example, 125 or 49\n- os (string) - operation system that customer used, for example, \"Windows\" or \"Android\"\n- browser (string) - browser that customer used, for example, \"Chrome\" or \"Safari\"\n- is_fraud (integer) - 1 if session is marked as fraud and 0 otherwise\n- revenue (float) - income in USD (the sum of purchased items), for example, 0.0 or 1506.7\n\nWrite a query in ClickHouse SQL to answer the following question. \nAdd \"format TabSeparatedWithNames\" at the end of the query to get data from ClickHouse database in the right format. \nAnswer questions following the instructions and providing all the needed information and sharing your reasoning. \n\nExamples of questions and answers: \n{examples}\n'''\n```", "```py\ndef generate_query_rag(question):\n  retrieved_docs = vector_store.similarity_search(question, 3)\n  context = context = \"\\n\\n\".join(map(lambda x: x.page_content, retrieved_docs))\n\n  prompt = get_llama_prompt(question, \n    generate_query_system_prompt_with_examples_tmpl.format(examples = context))\n  result = chat_llm.invoke(prompt)\n\n  try:\n    generated_query = result.tool_calls[0]['args']['query']\n  except:\n    generated_query = ''\n  return generated_query\n```", "```py\nrag_eval_df = evaluate_sql_agent(generate_query_rag, golden_df)\n```", "```py\ndef generate_query_rag_with_reflection(question):\n  generated_query = generate_query_rag(question) \n\n  db_output = get_clickhouse_data(generated_query)\n  is_valid_db_output = is_valid_output(db_output)\n  if is_valid_db_output == 'too many rows':\n      db_output = \"Database unexpectedly returned more than 1000 rows.\"\n\n  if is_valid_db_output == 'ok': \n      return generated_query\n\n  reflection_user_query = reflection_user_query_tmpl.format(\n    question = question,\n    query = generated_query,\n    output = db_output\n  )\n\n  reflection_prompt = get_llama_prompt(reflection_user_query, generate_query_system_prompt) \n  reflection_result = chat_llm.invoke(reflection_prompt)\n\n  try:\n    reflected_query = reflection_result.tool_calls[0]['args']['query']\n  except:\n    reflected_query = ''\n  return reflected_query\n\nrag_refl_eval_df = evaluate_sql_agent(generate_query_rag_with_reflection, \n  golden_df)\n```"]