- en: 8 Practical Prompt Engineering Tips for Better LLM Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01](https://towardsdatascience.com/8-practical-prompt-engineering-tips-for-better-llm-apps-430eef9b0950?source=collection_archive---------4-----------------------#2024-08-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--430eef9b0950--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--430eef9b0950--------------------------------)
    ·9 min read·Aug 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: ⁤Prompt engineering is undoubtedly the most critical skill in developing an
    LLM-native application, as crafting the right prompts can significantly impact
    your application’s performance and reliability.⁤
  prefs: []
  type: TYPE_NORMAL
- en: Over the past two years, I’ve been helping organizations build and deploy dozens
    of LLM applications for real-world use cases. ⁤⁤This experience gave me valuable
    insights into effective prompting techniques. ⁤
  prefs: []
  type: TYPE_NORMAL
- en: This article, informed by the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e),
    presents **eight practical tips** for prompt engineering to level up your LLM
    prompting game.
  prefs: []
  type: TYPE_NORMAL
- en: “LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven
    engineering work.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/605d5e3a9c4a90b2659fc5bd57d9585e.png)'
  prefs: []
  type: TYPE_IMG
- en: (Generated with Midjourney)
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this article, we'll use the “Landing Page Generator” example to demonstrate
    how each tip can be applied to enhance a real-world LLM application.
  prefs: []
  type: TYPE_NORMAL
- en: You can also check out [the full script of Landing Page Generator example](https://gist.github.com/AlmogBaku/5de026a355f9ef8984fa6a5bebd8f51f),
    for a complete lookout.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** This is a simplified, non-production example created to illustrate
    these tips. The code and prompts are intentionally basic to highlight the concepts
    discussed.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Define Clear Cognitive Process Boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start by defining the objective for each agent or prompt. Stick to **one cognitive
    process type per agent**, such as: conceptualizing a landing page, selecting components,
    or generating content for specific sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Having clear boundaries maintains focus and clarity in your LLM interactions,
    aligning with the [*Engineering Techniques*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)
    apex of the [LLM Triangle Principle](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221).
  prefs: []
  type: TYPE_NORMAL
- en: '*“*Each step in our flow is a standalone process that must occur to achieve
    our task.*”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, avoid combining different cognitive processes in the same prompt,
    which might yield suboptimal results. Instead, break these into separate, focused
    agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: By defining *clear boundaries* for each agent, we can ensure that each step
    in our workflow is tailored to a specific mental task. This will **improve the
    quality of outputs** and make it **easier to debug** and refine.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Specify Input/Output Clearly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Define **clear input and output** structures to reflect the objectives and create
    explicit data models. This practice touches on the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e)'
    [*Engineering Techniques*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#3221)
    and [*Contextual Data*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    apexes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These [Pydantic](https://pydantic.dev/) models define the structure of our **input
    and output data** and define clear boundaries and expectations for the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implement Guardrails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Place validations to ensure the quality and moderation of the LLM outputs. [Pydantic](https://pydantic.dev/)
    is excellent for implementing these guardrails, and we can utilize its native
    features for that.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, ensuring the quality of our application by defining two types
    of validators:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using Pydanitc’s** `**Field**` to define simple validations, such as a minimum
    of 2 tone/style attributes, or a minimum of 50 characters in the narrative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using a custom** `**field_validator**`that ensures the generated narrative
    is complying with our content moderation policy (using AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Align with Human Cognitive Processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structure your LLM workflow to mimic human cognitive processes by breaking down
    complex tasks into smaller steps that follow a logical sequence. To do that, follow
    the *SOP (Standard Operating Procedure)* guiding principle of the [LLM Triangle
    Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#be9a).
  prefs: []
  type: TYPE_NORMAL
- en: '*“Without an SOP, even the most powerful LLM will fail to deliver consistently
    high-quality results.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4.1 Capture hidden implicit cognition jumps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our example, we expect the model to return `LandingPageConcept` as a result.
    By asking the model to output certain fields, we guide the LLM similar to how
    a human marketer or designer might approach creating a landing page concept.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `LandingPageConcept` structure encourages the LLM to follow a human-like
    reasoning process, mirroring the *subtle mental leaps* ([implicit cognition "jumps"](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#f8c9))
    that an expert would make instinctively, just as we modeled in our SOP.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Breaking complex processes into multiple steps/agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For complex tasks, break the process down into various steps, each handled
    by a separate LLM call or *"agent"*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7faf32bd345890ba949330bc1e8ad532.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of the multi-agent process code. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This multi-agent approach aligns with how humans tackle complex problems — by
    breaking them into smaller parts.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Leverage Structured Data (YAML)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[YAML](https://en.wikipedia.org/wiki/YAML) is a *popular* human-friendly data
    serialization format. It’s designed to be easily readable by humans while still
    being easy for machines to parse — which makes it classic for LLM usage.'
  prefs: []
  type: TYPE_NORMAL
- en: I found YAML is particularly effective for LLM interactions and yields much
    better results across different models. It focuses the token processing on valuable
    content rather than syntax.
  prefs: []
  type: TYPE_NORMAL
- en: YAML is also much more portable across different LLM providers and allows you
    to maintain a structured output format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we're using few-shot examples to *"show, don't tell"* the expected
    YAML format. This approach is more effective than explicit instructions in prompt
    for the output structure.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Craft Your Contextual Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Carefully consider how to model and present data to the LLM. This tip is central
    to the [*Contextual Data*](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49)
    apex of the [LLM Triangle Principles](/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e#7b49).
  prefs: []
  type: TYPE_NORMAL
- en: '*“Even the most powerful model requires relevant and well-structured contextual
    data to shine.”*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Don’t throw away all the data you have on the model. Instead, inform the model
    with the pieces of information that are relevant to the objective you defined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we're using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/)
    templates to dynamically compose our prompts. This creates focused and relevant
    contexts for each LLM interaction elegantly.
  prefs: []
  type: TYPE_NORMAL
- en: “Data fuels the engine of LLM-native applications. A strategic design of contextual
    data unlocks their true potential.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6.1 Harness the power of few-shot learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Few-shot learning is a must-have technique in prompt engineering. Providing
    the LLM with relevant examples significantly improves its understanding of the
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that in both approaches we discuss below, we **reuse our Pydantic models
    for the few-shots** — this trick ensures consistency between the examples and
    our actual task! Unfortunately, I learned it the hard way.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Examples Few-Shot Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the `few_shots` dictionary in section 5\. In this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Examples are added to the `messages` list as separate user and assistant messages,
    followed by the actual user input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By placing the examples as `messages`, we align with the training methodology
    of instruction models. It allows the model to see multiple “example interactions”
    before processing the user input — helping it understand the expected input-output
    pattern.
  prefs: []
  type: TYPE_NORMAL
- en: As your application grows, you can add more few-shots to cover more use-cases.
    For even more advanced applications, consider implementing [dynamic few-shot](https://arxiv.org/abs/1804.09458)
    selection, where the most relevant examples are chosen based on the current input.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Task-Specific Few-Shot Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This method uses examples directly related to the current task *within the*
    ***prompt*** *itself.* For instance, this prompt template is used for generating
    additional unique selling points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This provides targeted guidance for specific content generation tasks by including
    the examples **directly in the prompt** rather than as separate messages.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. KISS — Keep It Simple, Stupid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While fancy prompt engineering techniques like “Tree of Thoughts” or “Graph
    of Thoughts” are intriguing, especially for research, I found them quite impractical
    and often overkill for production. For real applications, focus on designing a
    proper LLM architecture(aka workflow engineering).
  prefs: []
  type: TYPE_NORMAL
- en: 'This extends to the use of agents in your LLM applications. It''s crucial to
    understand the distinction between standard agents and autonomous agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agents:** “Take me from A → B by doing XYZ.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Autonomous Agents:**“Take me from A → B by doing something, I don’t care
    how.”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While autonomous agents offer flexibility and quicker development, they can
    also introduce unpredictability and debugging challenges. Use autonomous agents
    carefully — only when the benefits *clearly outweigh* the potential loss of control
    and increased complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cd8b67d63da85cbb333002cf95f30b70.png)'
  prefs: []
  type: TYPE_IMG
- en: (Generated with Midjourney)
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Iterate, Iterate, Iterate!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Continuous experimentation is vital to improving your LLM-native applications.
    Don''t be intimidated by the idea of experiments — they can be as small as tweaking
    a prompt. As outlined in ["Building LLM Apps: A Clear Step-by-Step Guide,"](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    it''s crucial to *establish a baseline* andtrack improvements against it.'
  prefs: []
  type: TYPE_NORMAL
- en: Like everything else in “AI,” LLM-native apps require a **research and experimentation
    *mindset*.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another great trick is to try your prompts on a weaker model than the one you
    aim to use in production(such as open-source 8B models) — an “okay” performing
    prompt on a smaller model will perform much better on a larger model.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These eight tips provide a solid foundation for effective prompt engineering
    in LLM-native applications. By applying these tips in your prompts, you'll be
    able to create more reliable, efficient, and scalable LLM-native applications.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the goal isn't to create the most complex system but to build something
    that works in the real world. Keep experimenting, learning, and building — the
    possibilities are endless.
  prefs: []
  type: TYPE_NORMAL
- en: If you find this article helpful, please give it a few **claps** 👏 on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! 🌍
  prefs: []
  type: TYPE_NORMAL
- en: Let's keep the conversation going — feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) 🤝
  prefs: []
  type: TYPE_NORMAL
- en: Special thanks to [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--430eef9b0950--------------------------------)
    and [Yam Peleg](https://medium.com/u/e0607cd7607d?source=post_page---user_mention--430eef9b0950--------------------------------);
    this article is based on insights from our conversations.
  prefs: []
  type: TYPE_NORMAL
