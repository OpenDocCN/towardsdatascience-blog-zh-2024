- en: A Complete Guide to Write your own Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整指南：编写你自己的Transformer
- en: 原文：[https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24](https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24](https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24)
- en: An end-to-end implementation of a Pytorch Transformer, in which we will cover
    key concepts such as self-attention, encoders, decoders, and much more.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是一个端到端的Pytorch Transformer实现，在这个过程中我们将涵盖诸如自注意力（self-attention）、编码器（encoders）、解码器（decoders）等关键概念，以及更多内容。
- en: '[](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)[![Benjamin
    Etienne](../Images/cad8bc2d4b900575e76b7cf9debc9eea.png)](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)
    [Benjamin Etienne](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)[![Benjamin
    Etienne](../Images/cad8bc2d4b900575e76b7cf9debc9eea.png)](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)
    [Benjamin Etienne](https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)
    ·18 min read·Feb 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------)
    ·阅读时间18分钟·2024年2月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fd0bb4ec869d59ea716cca7fc1928d2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd0bb4ec869d59ea716cca7fc1928d2f.png)'
- en: Photo by [Susan Holt Simpson](https://unsplash.com/@shs521?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Susan Holt Simpson](https://unsplash.com/@shs521?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: Writing our own
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们自己编写
- en: 'When I decided to dig deeper into Transformer architectures, I often felt frustrated
    when reading or watching tutorials online as I felt they always missed something
    :'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我决定深入研究Transformer架构时，在阅读或观看在线教程时，我常常感到沮丧，因为我觉得它们总是遗漏了一些内容：
- en: Official tutorials from Tensorflow or Pytorch used their own APIs, thus staying
    high-level and forcing me to have to go in their codebase to see what was under
    the hood. Very time-consuming and not always easy to read 1000s of lines of code.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tensorflow或Pytorch的官方教程使用了它们自己的API，因此保持在高层次，迫使我必须进入它们的代码库去查看底层实现。这样非常耗时，而且阅读成千上万行代码并不总是容易的。
- en: Other tutorials with custom code I found (links at the end of the article) often
    oversimplified use cases and didn’t tackle concepts such as masking of variable-length
    sequence batch handling.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我找到的其他带有自定义代码的教程（文章末尾有链接）通常过于简化了使用场景，并且没有涉及如变长序列批处理的遮蔽等概念。
- en: I therefore decided to write my own Transformer to make sure I understood the
    concepts and be able to use it with any dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我决定自己写一个Transformer，以确保我理解这些概念，并能够在任何数据集上使用它。
- en: During this article, we will therefore follow a methodical approach in which
    we will implement a transformer layer by layer and block by block.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将采取一种系统的方法，逐层逐块地实现一个变换器（Transformer）。
- en: There are obviously a lot of different implementations as well as high-level
    APIs from Pytorch or Tensorflow already available off the shelf, with — I am sure
    — better performance than the model we will build.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，已经有许多不同的实现，以及来自Pytorch或Tensorflow的高级API可以直接使用——我相信它们的性能比我们将要构建的模型更好。
- en: '***“Ok, but why not use the TF/Pytorch implementations then” ?***'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '***“好吧，那为什么不使用TF/Pytorch的实现呢？”***'
- en: The purpose of this article is educational, and I have no pretention in beating
    Pytorch or Tensorflow implementations. I do believe that the theory and the code
    behind transformers is not straightforward, that is why I hope that going through
    this step-by-step tutorial will allow you to have a better grasp over these concepts
    and feel more comfortable when building your own code later.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目的是为了教育目的，我并不打算超过 Pytorch 或 Tensorflow 的实现。我确实相信，Transformer 背后的理论和代码并不简单直接，这也是为什么我希望通过这篇一步步的教程，帮助你更好地掌握这些概念，并在以后构建自己代码时更加得心应手。
- en: 'Another reasons to build your own transformer from scratch is that it will
    allow you to fully understand how to use the above APIs. If we look at the Pytorch
    implementation of the `forward()` method of the Transformer class, you will see
    a lot of obscure keywords like :'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建自己的 Transformer 还有一个原因，那就是它能帮助你完全理解如何使用上述的 API。如果我们查看 Pytorch 中 Transformer
    类的 `forward()` 方法的实现，你会看到很多晦涩的关键词，比如：
- en: '![](../Images/00baf4c1e185c524d03fd1440baf4db9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00baf4c1e185c524d03fd1440baf4db9.png)'
- en: 'source : [Pytorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[Pytorch 文档](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)
- en: If you are already familiar with these keywords, then you can happily skip this
    article.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉这些关键词，那么你可以愉快地跳过本文。
- en: Otherwise, this article will walk you through each of these keywords with the
    underlying concepts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，本文将带你逐步了解这些关键词及其背后的概念。
- en: A very short introduction to Transformers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 简短介绍
- en: If you already heard about ChatGPT or Gemini, then you already met a transformer
    before. Actually, the “T” of ChatGPT stands for Transformer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经听说过 ChatGPT 或 Gemini，那么你已经接触过 Transformer。实际上，ChatGPT 中的“T”就代表 Transformer。
- en: The architecture was first coined in 2017 by Google researchers in the “Attention
    is All you need” paper. It is quite revolutionary as previous models used to do
    sequence-to-sequence learning (machine translation, speech-to-text, etc…) relied
    on RNNs which were computationnally expensive in the sense they had to process
    sequences step by step, whereas Transformers only need to look once at the whole
    sequence, moving the time complexity from O(n) to O(1).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构首次由 Google 研究人员在 2017 年的《Attention is All you need》论文中提出。它具有革命性，因为以前的模型通常依赖于
    RNNs 来进行序列到序列的学习（如机器翻译、语音转文本等），而 RNNs 在计算上是昂贵的，因为它们需要一步步地处理序列，而 Transformer 只需一次性查看整个序列，从而将时间复杂度从
    O(n) 降低到 O(1)。
- en: '![](../Images/7a4b8ca14307d51a11f291b0766a0cc4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a4b8ca14307d51a11f291b0766a0cc4.png)'
- en: '[(Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[(Vaswani 等人, 2017)](https://arxiv.org/abs/1706.03762)'
- en: Applications of transformers are quite large in the domain of NLP, and include
    language translation, question answering, document summarization, text generation,
    etc.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 在 NLP 领域的应用非常广泛，包括语言翻译、问答系统、文档摘要、文本生成等。
- en: 'The overall architecture of a transformer is as below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的整体架构如下：
- en: '![](../Images/481fea3d28cd50a0773fafd6a1ef1366.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/481fea3d28cd50a0773fafd6a1ef1366.png)'
- en: '[source](https://www.tensorflow.org/text/tutorials/transformer)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://www.tensorflow.org/text/tutorials/transformer)'
- en: Multi-head attention
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多头注意力
- en: The first block we will implement is actually the most important part of a Transformer,
    and is called the Multi-head Attention. Let’s see where it sits in the overall
    architecture
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现的第一个模块实际上是 Transformer 中最重要的部分，称为多头注意力机制。让我们来看一下它在整体架构中的位置。
- en: '![](../Images/99b4662ef876d9ab107782fdfebad4b3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99b4662ef876d9ab107782fdfebad4b3.png)'
- en: '[source](https://www.tensorflow.org/text/tutorials/transformer)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[来源](https://www.tensorflow.org/text/tutorials/transformer)'
- en: Attention is a mechanism which is actually not specific to transformers, and
    which was already used in RNN sequence-to-sequence models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制其实并不是 Transformer 所特有的，它早在 RNN 的序列到序列模型中就已经被使用过。
- en: '![](../Images/c24820a0c6d6b508bdf72cfb69d2469f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c24820a0c6d6b508bdf72cfb69d2469f.png)'
- en: 'Attention in a transformer (source: Tensorflow [documentation](https://www.tensorflow.org/text/tutorials/transformer))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 中的注意力机制（来源：Tensorflow [文档](https://www.tensorflow.org/text/tutorials/transformer)）
- en: '![](../Images/6b88cb8f3f37ee9819f5aed4b9578e0d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b88cb8f3f37ee9819f5aed4b9578e0d.png)'
- en: 'Attention in a transformer (source: Tensorflow [documentation](https://www.tensorflow.org/text/tutorials/transformer))'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 中的注意力机制（来源：Tensorflow [文档](https://www.tensorflow.org/text/tutorials/transformer)）
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We need to explain a few concepts here.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里解释几个概念。
- en: 1) Queries, Keys and Values.
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 查询、键和值。
- en: The ***query***is the information you are trying to match,
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***查询***是你试图匹配的信息，'
- en: The ***key***and ***values***are the stored information.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***键***和 ***值*** 是存储的信息。'
- en: 'Think of that as using a dictionary : whenever using a Python dictionary, if
    your query doesn’t match the dictionary keys, you won’t be returned anything.
    But what if we want our dictionary to return a blend of information which are
    quite close ? Like if we had :'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以把它想象成使用字典：每当使用 Python 字典时，如果你的查询与字典的键不匹配，你将不会得到任何返回。但如果我们希望字典返回的是一组相似的信息呢？就像我们有：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is basically what attention is about : looking at different parts of your
    data, and blend them to obtain a synthesis as an answer to your query.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是注意力的作用：观察数据的不同部分，并将它们融合成一个答案来回应你的查询。
- en: The relevant part of the code is this one, where we compute the attention weights
    between the query and the keys
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的相关部分是这一部分，我们计算查询与键之间的注意力权重
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And this one, where we apply the normalized weights to the values :'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们应用标准化权重到值上的情况：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 2) Attention masking and padding
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) 注意力掩码和填充
- en: When attending to parts of a sequential input, we do not want to include useless
    or forbidden information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理顺序输入的各部分时，我们不希望包含无用或禁止的信息。
- en: 'Useless information is for example padding: padding symbols, used to align
    all sequences in a batch to the same sequence size, should be ignored by our model.
    We will come back to that in the last section'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 无用信息例如填充：填充符号用于将批次中的所有序列对齐到相同的序列大小，我们的模型应忽略这些填充。我们将在最后一节回到这一点
- en: Forbidden information is a bit more complex. When being trained, a model learns
    to encode the input sequence, and align targets to the inputs. However, as the
    inference process involves looking at previously emitted tokens to predict the
    next one (think of text generation in ChatGPT), we need to apply the same rules
    during training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 禁止信息稍微复杂一些。在训练过程中，模型学习编码输入序列，并将目标与输入对齐。然而，由于推理过程涉及查看先前发出的标记来预测下一个标记（例如 ChatGPT
    中的文本生成），我们在训练时也需要应用相同的规则。
- en: This is why we apply a *causal mask* to ensure that the targets, at each time
    step, can only see information from the past. Here is the corresponding section
    where the mask is applied (computing the mask is covered at the end)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们应用 *因果掩码* 的原因，以确保每个时间步的目标只能看到来自过去的信息。以下是应用掩码的对应部分（掩码的计算将在最后部分讨论）
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Positional Encoding
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'It corresponds to the following part of the Transformer:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它对应于 Transformer 的以下部分：
- en: '![](../Images/4c54461de6e757740b7df62624b38aab.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c54461de6e757740b7df62624b38aab.png)'
- en: When receiving and treating an input, a transformer has no sense of order as
    it looks at the sequence as a whole, in opposition to what RNNs do. We therefore
    need to add a hint of temporal order so that the transformer can learn dependencies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当接收和处理输入时，Transformer 没有顺序感知，因为它将序列视为整体，这与 RNN 的做法不同。因此，我们需要加入一些时间顺序的提示，以便 Transformer
    能学习依赖关系。
- en: The specific details of how positional encoding works is out of scope for this
    article, but feel free to read the original paper to understand.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码如何工作的具体细节超出了本文的范围，但可以随时阅读原始论文以了解更多。
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Encoders
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: We are getting close to having a full encoder working ! The encoder is the left
    part of the Transformer
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将完成一个完整的编码器！编码器是 Transformer 的左侧部分
- en: '![](../Images/8dea9506d84ec2bd26bf1791cd078aa8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8dea9506d84ec2bd26bf1791cd078aa8.png)'
- en: 'We will add a small part to our code, which is the Feed Forward part :'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向代码中添加一个小部分，即前馈部分：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Putting the pieces together, we get an Encoder module !
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将各部分拼接起来，我们得到了一个编码器模块！
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As shown in the diagram, the Encoder actually contains N Encoder blocks or
    layers, as well as an Embedding layer for our inputs. Let’s therefore create an
    Encoder by adding the Embedding, the Positional Encoding and the Encoder blocks:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，编码器实际上包含 N 个编码器块或层，以及用于输入的嵌入层。因此，我们通过添加嵌入层、位置编码和编码器块来创建一个编码器：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Decoders
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: The decoder part is the part on the left and requires a bit more crafting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器部分是左侧部分，需要更多的构建工作。
- en: '![](../Images/e186299bbdb37043d1147a94c8bdde42.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e186299bbdb37043d1147a94c8bdde42.png)'
- en: 'There is something called *Masked Multi-Head Attention.* Remember what we said
    before about *causal mask* ? Well this happens here. We will use the attention_mask
    parameter of our Multi-head attention module to represent this (more details about
    how we compute the mask at the end) :'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The second attention is called *cross-attention*. It will uses the decoder’s
    query to match with the encoder’s key & values ! Beware : they can have different
    lengths during training, so it is usually a good practice to define clearly the
    expected shapes of inputs as follows :'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And here is the part where we use the encoder’s output, called *memory*, with
    our decoder input :'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Putting the pieces together, we end up with this for the Decoder :'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Padding & Masking
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember the Multi-head attention section where we mentionned excluding certain
    parts of the inputs when doing attention.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, we consider batches of inputs and targets, wherein each instance
    may have a variable length. Consider the following example where we batch 4 words
    : banana, watermelon, pear, blueberry. In order to process them as a single batch,
    we need to align all words to the length of the longest word (watermelon). We
    will therefore add an extra token, PAD, to each word so they all end up with the
    same length as watermelon.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'In the below picture, the upper table represents the raw data, the lower table
    the encoded version:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13b635dfa9a7c67680987264e66197b9.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we want to exclude padding indices from the attention weights
    being calculated. We can therefore compute a mask as follows, both for source
    and target data :'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'What about causal masks now ? Well if we want, at each time step, that the
    model can attend only steps in the past, this means that for each time step T,
    the model can only attend to each step t for t in 1…T. It is a double for loop,
    we can therefore use a matrix to compute that :'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/887c940f4c6dafa57ca2f5a0d50ce1f8.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Case study : a Word-Reverse Transformer'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now build our Transformer by bringing parts together !
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, we will use a very simple dataset to showcase how Transformers
    actually learn.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '***“But why use a Transformer to reverse words ? I already know how to do that
    in Python with word[::-1] !”***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: The objective here is to see whether the Transformer attention mechanism works.
    What we expect is to see attention weights to move from right to left when given
    an input sequence. If so, this means our Transformer has learned a very simple
    grammar, which is just reading from right to left, and could generalize to more
    complex grammars when doing real-life language translation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first begin with our custom Transformer class :'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Performing Inference with Greedy Decoding
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to add a method which will act as the famous `model.predict` of scikit.learn.
    The objective is to ask the model to dynamically output predictions given an input.
    During inference, there is not target : the model starts by outputting a token
    by attending to the output, and uses its own prediction to continue emitting tokens.
    This is why those models are often called auto-regressive models, as they use
    past predictions to predict to next one.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加一个方法，它将充当 scikit.learn 中著名的`model.predict`。目标是让模型根据输入动态输出预测。在推理过程中，没有目标：模型首先通过关注输出生成一个
    token，并使用自身的预测继续生成 token。这就是为什么这些模型通常被称为自回归模型，因为它们使用过去的预测来预测下一个。
- en: The problem with greedy decoding is that it considers the token with the highest
    probability at each step. This can lead to very bad predictions if the first tokens
    are completely wrong. There are other decoding methods, such as Beam search, which
    consider a shortlist of candidate sequences (think of keeping top-k tokens at
    each time step instead of the argmax) and return the sequence with the highest
    total probability.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 贪心解码的问题在于它每一步都选择概率最高的 token。如果第一个 token 完全错误，这可能会导致非常糟糕的预测。还有其他解码方法，如 Beam search，它考虑一个候选序列的简短列表（可以理解为在每个时间步骤保持
    top-k token 而不是 argmax），并返回具有最高总概率的序列。
- en: 'For now, let’s implement greedy decoding and add it to our Transformer model:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现贪心解码并将其添加到我们的 Transformer 模型中：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Creating toy data
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建示例数据
- en: 'We define a small dataset which inverts words, meaning that “helloworld” will
    return “dlrowolleh”:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个小数据集，它会反转单词，这意味着“helloworld”将返回“dlrowolleh”：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will now define training and evaluation steps :'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将定义训练和评估步骤：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And train the model for a couple of epochs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后训练模型若干轮：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/9f7afec6c69db6a3ed76a02e7ce64609.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f7afec6c69db6a3ed76a02e7ce64609.png)'
- en: Visualize attention
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化注意力
- en: 'We define a little function to access the weights of the attention heads :'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个小函数来访问注意力头的权重：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/58dc28e4552c6a895877083d7b02629a.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58dc28e4552c6a895877083d7b02629a.png)'
- en: image from author
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We can see a nice right-to-left pattern, when reading weights from the top.
    Vertical parts at the bottom of the y-axis may surely represent masked weights
    due to padding mask
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一个漂亮的从右到左的模式，当从上方读取权重时。y 轴底部的垂直部分可能表示由于填充掩蔽导致的掩蔽权重。
- en: Testing our model !
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试我们的模型！
- en: 'To test our model with new data, we will define a little `Translator` class
    to help us with the decoding :'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用新数据测试我们的模型，我们将定义一个小的 `Translator` 类来帮助我们进行解码：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You should be able to see the following :'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看到以下内容：
- en: '![](../Images/0a1a3a1b926b437cfc76477fd3c99811.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1a3a1b926b437cfc76477fd3c99811.png)'
- en: 'And if we print the attention head we will observe the following :'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印注意力头，我们将观察到以下内容：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/8481c6a87c9210c958afdf396844f629.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8481c6a87c9210c958afdf396844f629.png)'
- en: image from author
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: We can clearly see that the model attends from right to left when inverting
    our sentence “reversethis” ! (The step 0 actually receives the beginning of sentence
    token).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，当我们倒置句子“reversethis”时，模型是从右到左进行关注的！（步骤 0 实际上接收的是句子的开始 token）。
- en: Conclusion
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: That’s it, you are now able to write Transformer and use it with larger datasets
    to perform machine translation of create you own BERT for example !
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你现在能够编写 Transformer 并将其应用于更大的数据集，进行机器翻译或创建你自己的 BERT 例如！
- en: 'I wanted this tutorial to show you the caveats when writing a Transformer :
    padding and masking are maybe the parts requiring the most attention (pun unintended)
    as they will define the good performance of the model during inference.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这个教程能向你展示在编写 Transformer 时的注意事项：填充和掩蔽可能是最需要关注的部分（无心的双关），因为它们将在推理过程中决定模型的良好性能。
- en: In the following articles, we will look at how to create your own BERT model
    and how to use Equinox, a highly performant library on top of JAX.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的文章中，我们将看看如何创建你自己的 BERT 模型，以及如何使用 Equinox，一个基于 JAX 的高性能库。
- en: Stay tuned !
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请期待！
- en: Useful links
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有用的链接
- en: (+) [“The Annotated Transformer”](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (+) [“注解版 Transformer”](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- en: (+) “[Transformers from scratch](https://peterbloem.nl/blog/transformers)”
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (+) “[从零开始学习 Transformers](https://peterbloem.nl/blog/transformers)”
- en: (+) [“Neural machine translation with a Transformer and Keras”](https://www.tensorflow.org/text/tutorials/transformer)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (+) [“使用 Transformer 和 Keras 的神经机器翻译”](https://www.tensorflow.org/text/tutorials/transformer)
- en: (+) [“The Illustrated Transformer”](https://jalammar.github.io/illustrated-transformer/)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (+) [“图解Transformer”](https://jalammar.github.io/illustrated-transformer/)
- en: (+) [University of Amsterdam Deep Learning Tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: (+) [阿姆斯特丹大学深度学习教程](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html)
- en: (+) [Pytorch tutorial on Transformers](https://pytorch.org/tutorials/beginner/translation_transformer.html)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (+) [Pytorch关于Transformer的教程](https://pytorch.org/tutorials/beginner/translation_transformer.html)
