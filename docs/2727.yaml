- en: Preference Alignment for Everyone!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08](https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Frugal RLHF with multi-adapter PPO on Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    ·26 min read·Nov 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77e9a1dd091dae519aca481072d8bae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by StableDiffusionXL on Amazon Web Services
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: All images, unless otherwise noted, are by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: What is this about and why is it important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the last 2 years, research and practice have delivered plenty of proof
    that preference alignment (PA) is a game changer for boosting Large Language Models
    (LLMs) performance, especially (but not exclusively) for models directly exposed
    to humans. PA uses (human) feedback to align model behavior to what is preferred
    in the environment a model is actually living in, instead of relying solely on
    proxy datasets like other fine-tuning approaches do (as I explain in detailed
    in [this blog post](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)
    on fine-tuning variations). This improvement in model performance, as perceived
    by human users, has been a key factor in making LLMs and other Foundation Models
    (FMs) more accessible and popular, contributing significantly to the current excitement
    around Generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Over time various approaches to PA have been proposed by research and quickly
    adapted by some practitioners. Amongst them, RLHF is (as of Autumn 2024) by far
    the most popular and proven approach.
  prefs: []
  type: TYPE_NORMAL
- en: However, due to challenges around implementation complexity, compute requirements
    or training orchestration, so far the adaptation of PA approaches like RLHF in
    practice is limited to mainly high-skill profile individuals and organizations
    like FM producers. Also, most practical examples and tutorials I found showcasing
    how to master an approach like RLHF are limited or incomplete.
  prefs: []
  type: TYPE_NORMAL
- en: This blog post provides you with a comprehensive introduction into RLHF, discusses
    challenges around the implementation, and suggests RLHF with multi-adapter PPO,
    a light-weight implementation approach tackling some key ones of these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we present an end-to-end (E2E) implementation of this approach in a Jupyter
    notebook, covering data collection, preparation, model training, and deployment.
    We leverage HuggingFace frameworks and Amazon SageMaker to provide a user-friendly
    interface for implementation, orchestration, and compute resources. The blog post
    then guides you through the key sections of this notebook, explaining implementation
    details and the rationale behind each step. This hands-on approach allows readers
    to understand the practical aspects of the process and easily replicate the results.
  prefs: []
  type: TYPE_NORMAL
- en: The principles of RLHF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning from human feedback was one of the major hidden technical
    backbones of the early Generative AI hype, giving the breakthrough achieved with
    great large decoder models like Anthropic Claude or OpenAI’s GPT models an additional
    boost into the direction of user alignment.
  prefs: []
  type: TYPE_NORMAL
- en: The great success of PA for FMs perfectly aligns with the concept of user-centric
    product development, a core and well-established principle of agile product development.
    Iteratively incorporating feedback from actual target users has proven highly
    effective in developing outstanding products. This approach allows developers
    to continually refine and improve their offerings based on real-world user preferences
    and needs, ultimately leading to more successful and user-friendly products.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other fine-tuning approaches like continued pre-training (CPT) or supervised
    fine-tuning (SFT) don’t cover this aspect since:'
  prefs: []
  type: TYPE_NORMAL
- en: the datasets used for these approaches are (labelled or unlabelled) proxies
    for what we think our users like or need (i.e. knowledge or information, language
    style, acronyms or task-specific behaviour like instruction-following, chattiness
    or others), crafted by a few in charge of model training or fine-tuning data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the algorithm(s), training objective(s) and loss function(s) used for these
    approaches (i.e. causal language modeling) are using next-token prediction as
    proxy for higher level metrics (e.g. accuracy, perplexity, …).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, PA is undoubtedly a technique we should employ when aiming to create
    an exceptional experience for our users. This approach can significantly enhance
    the quality, safety and relevance of AI-generated responses, leading to more satisfying
    interactions and improved overall user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: How does RLHF work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Note: This section is an adapted version of the RLHF section in my* [*blog
    post about different fine-tuning variations*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)*.
    For a comprehensive overview about fine-tuning you might want to check it out
    as well.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c206faae0d0b57cb16f4bcbf0ba5631.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Reward model training for RLHF (Source: Lambert et al, 2022)'
  prefs: []
  type: TYPE_NORMAL
- en: 'RLHF works in a two-step process and is illustrated in Figures 13 and 14:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1 (Figure 1): First, a reward model needs to be trained for later usage
    in the actual RL-powered training approach. Therefore, a prompt dataset aligned
    with the objective (e.g. chat/instruct model or domain-specific task objective)
    to optimize is being fed to the model to be fine-tuned, while requesting not only
    one but two or more inference results. These results will be presented to human
    labelers for scoring (1st, 2nd, 3rd, …) based on the optimization objective. There
    are also a few open-sourced preference ranking datasets, among them [“Anthropic/hh-rlhf”](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    (we will use this dataset in the practical part of this blog) which is tailored
    towards red-teaming and the objectives of honesty and harmlessness. After normalizing
    and converting the scores into reward values, a reward model is trained using
    individual sample-reward pairs, where each sample is a single model response.
    The reward model architecture is usually similar to the model to be fine-tuned,
    adapted with a small head eventually projecting the latent space into a reward
    value instead of a probability distribution over tokens. However, the ideal sizing
    of this model in parameters is still subject to research, and different approaches
    have been chosen by model providers in the past. In the practical part of this
    blog, for the reward model we will use the same model architecture compared to
    the model to be fine-tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/762043a6b8c3fb52e183b1886888c937.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Reinforcement learning based model tuning with PPO for RLHF (Source:
    Lambert et al, 2022)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2 (Figure 2): Our new reward model is now used for training the actual
    model. Therefore, another set of prompts is fed through the model to be tuned
    (grey box in illustration), resulting in one response each. Subsequently, these
    responses are fed into the reward model for retrieval of the individual reward.
    Then, Proximal Policy Optimization (PPO), a policy-based RL algorithm, is used
    to gradually adjust the model’s weights in order to maximize the reward allocated
    to the model’s answers. As opposed to Causal Language Modeling (CLM — you can
    find a detailed explanation [here](https://medium.com/towards-data-science/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)),
    instead of gradient descent, this approach leverages gradient ascent (or gradient
    descent over *1 — reward*) since we are now trying to maximize an objective (reward).
    For increased algorithmic stability to prevent too heavy drifts in model behavior
    during training, which can be caused by RL-based approaches like PPO, a prediction
    shift penalty is being added to the reward term, penalizing answers diverging
    too much from the initial language model’s predicted probability distribution
    on the same input prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The way how RLHF is working poses some core challenges to implementing and
    running it at scale, amongst them the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Cost of training the reward model:** Picking the right model architecture
    and size for the reward model is still current state of research. These models
    are usually transformer models similar to the model to be fine-tuned, equipped
    with a modified head delivering reward scores instead of a vocabular probability
    distribution. This means, that independent from the actual choice, most reward
    models are in the billions of parameters. Full parameter training of such a reward
    model is data and compute expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Cost of training cluster:** With the reward model (for the reward values),
    the base model (for the KL prediction shift penalty) and the model actually being
    fine-tuned three models need to be hosted in parallel in the training cluster.
    This leads to massive compute requirements usually only being satisfied by a multi-node
    cluster of multi-GPU instances (in the cloud), leading to hardware and operational
    cost.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Orchestration of training cluster:** The RLHF algorithm requires a combination
    of inference- and training-related operations in every training loop. This needs
    to be orchestrated in a multi-node multi-GPU cluster while keeping communication
    overhead minimal for optimal training throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Training/inference cost in highly specialized setups:** PA shines through
    aligning model performance towards a user group or target domain. Since most professional
    use cases are characterized by specialized domains with heterogenous user groups,
    this leads to an interesting tradeoff: Optimizing for performance will lead in
    training and hosting many specialized models excelling in performance. However,
    optimizing for resource consumption (i.e. cost) will lead to overgeneralization
    of models and decreasing performance.'
  prefs: []
  type: TYPE_NORMAL
- en: RLHF with multi-adapter PPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/43d376a2044c3fc5511ddd397cca4912.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Minimizing GPU footprint of PPO through dynamic multi-adapter loading'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-adapter PPO is a particularly GPU-frugal approach to the second step of
    the RLHF training process. Instead of using full-parameter fine-tuning, it leverages
    parameter-efficient fine-tuning (PEFT) techniques to reduce the infrastructure
    and orchestration footprint drastically. Instead of hosting three distinct models
    (model being fine-tuned, reward model, reference model for KL prediction shift
    penalty) in parallel in the training cluster this approach leverages Low Rank
    Adaptation (LoRA) adapters during the fine-tuning which are dynamically loaded
    and unloaded into the accelerators of the training cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3fd297299c11b108133ded22488394b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: E2E RLHF with multi-adapter PPO for a harmless Q&A bot'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this approach’s goal is ultimately a resource and orchestration frugal
    approach to the second step of RLHF, it has implications on the first step:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reward model choice:** A reward model with the same model architecture as
    the model to be fine-tuned is picked and equipped with a reward classification
    head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward model training approach:** As illustrated in figure 4(2), instead
    of full-parameter reward model training, a reward model LoRA adapter is being
    trained, leading to a much leaner training footprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly to the this, the RLHF fine-tuning of the model being performed in
    the second step is not done in a full-parameter fine-tuning manner. Instead, a
    LoRA adapter is trained. As depicted in figure 4, during a training iteration,
    first the RLHF model adapter is being loaded to generate model responses to the
    prompts of the current training batch (4a). Then, the reward model adapter is
    loaded to calculate the corresponding raw reward values (4b). To complete the
    reward term, the input prompt is fed through the base model for calculation of
    the KL prediction shift penalty. Therefor, all adapters need to be unloaded (4c,
    4d). Finally, the RLHF model adapter is loaded again to perform the weight updates
    for this iteration step (4e).
  prefs: []
  type: TYPE_NORMAL
- en: This approach to RLHF reduces the memory footprint as well as orchestration
    complexity significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Running RLHF with multi-adapter PPO with HuggingFace and Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In what follows we will go through a notebook showcasing RLHF with multi-adapter
    PPO in an E2E fashion. Thereby we use HuggingFace and Amazon SageMaker for an
    especially user-friendly interface towards the implementation, orchestration and
    compute layers. The entire notebook can be found [here](https://github.com/aws-samples/build-language-models-on-aws/tree/main/align-models-with-amazon-sagemaker/rlhf-with-multi-adapter-ppo).
  prefs: []
  type: TYPE_NORMAL
- en: Scenario
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pace model producers nowadays are releasing new models is impressive. Hence,
    I want to keep the scenario we are looking into as generic as possible.
  prefs: []
  type: TYPE_NORMAL
- en: While most of the models published these days have already gone through multiple
    fine-tuning steps like SFT or even PA, since these models are general purpose
    ones they where certainly not performed tailored to your target users or target
    domain. This means that even though we are using a pre-aligned model (e.g. an
    instruction fine-tuned model), for optimising model performance in your domain
    further alignment steps are required.
  prefs: []
  type: TYPE_NORMAL
- en: For this blog we will assume the model should be optimised towards maximising
    the helpfulness while carrying out user-facing single- and multi-turn conversations
    in a Q&A style in the scientific domain. Thus, we will start from a general-purpose
    instruct / Q&A pre-trained FM.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite of being generic we need to choose a model for our endeavour. For this
    blog we will be working with Meta Llama3.1–8b-instruct. This model is the smallest
    fashion of a new collection of multilingual pre-trained and instruction-tuned
    decoder models Meta released in Summer 2024\. More details can be found in the
    [documentation](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1#llama-3.1-instruct)
    in the Meta homepage and in the [model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
    provided by HuggingFace.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92f35b3b8bd00507db12d2a4a26d3815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Llama-3.1–8b-instruct model card on HuggingFace hub'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We start our notebook walkthrough with some prerequisite preparation steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/448c07c74376c62d180935b6d48a716b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Accepting Meta’s licensing agreement through HuggingFace hub'
  prefs: []
  type: TYPE_NORMAL
- en: We will be retrieving the model’s weights from the HuggingFace model hub. To
    be able to do so we need to accept Meta‘s licensing agreement and provide some
    information. This can be submitted directly through the HuggingFace model hub.
  prefs: []
  type: TYPE_NORMAL
- en: Further, for storage of the adapter weights of both the reward model as well
    as the preference-aligned model we will be using private model repositories on
    the HuggingFace model hub. This requires a HuggingFace account. Once logged into
    the HuggingFace platform we need to create two model repositories. For this click
    on the account icon on the top right of the HuggingFace landing page and pick
    “+ New Model” in the menu.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/913dabd67e327e505f481f44bc411501.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Creating model repositories on HuggingFace model hub'
  prefs: []
  type: TYPE_NORMAL
- en: We can then create two private model repositories. Feel free to stick to my
    naming convention or pick a name of choice. If you name your repositories differently
    make sure to also adjust the code in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Once created, we can see the model repositories in our HuggingFace profile.
  prefs: []
  type: TYPE_NORMAL
- en: To authenticate against the HuggingFace model hub when pulling or pushing models
    we need to create an access token, which we will use later in the notebook. For
    this click on the account icon on the top right of the HuggingFace landing page
    and pick „Settings“ in the menu.
  prefs: []
  type: TYPE_NORMAL
- en: In the settings we select the menu item “Access Tokens” and then “+ Create new
    token.”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6444c90a1d12546dd6a9699087935638.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Creating access tokens on HuggingFace hub'
  prefs: []
  type: TYPE_NORMAL
- en: According to the principle of least privileges we want to create a token with
    fine-grained permission configurability. For our purpose read and write access
    to repositories is sufficient — this is why we check all three boxes in this section.
    Then we scroll down and create the token.
  prefs: []
  type: TYPE_NORMAL
- en: Once created the access token appears in plain text. Since the token will only
    be displayed once it makes sense to store it in encrypted format for example in
    a password manager.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are finished with the prerequisites we can move on to the datasets
    we will be using for our endeavor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91ccb2b30be64e7b3eb90d0bb193d5cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Anthropic hh-rlhf dataset on HuggingFace hub'
  prefs: []
  type: TYPE_NORMAL
- en: For training our reward model we will be using the Anthropic/hh-rlhf dataset,
    which is distributed under [MIT license](https://opensource.org/license/mit).
    This is a handcrafted preference dataset Anthropic has open-sourced. It consists
    of chosen and rejected model completions to one and the same prompt input. Further,
    it comes in different fashions, targeting alignment areas like harmlessness, helpfulness
    and more. For our demonstration we will use the ”helpful” subset to preference
    align our Llama model towards helpful answers.
  prefs: []
  type: TYPE_NORMAL
- en: For the actual PA step with PPO and the previously trained reward model we need
    an additional dataset representing the target domain of our model. Since we are
    fine-tuning an instruct model towards helpfulness we need a set of instruction-style
    prompts. [The Stanford Question&Answering dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/),
    distributed under the [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en)**,**
    provides us with question — context — answer pairs across a broad range of different
    areas of expertise. For our experiment we will aim for single-turn open Question&Answering.
    Hence we will use only the “question” feature of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Code repository
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8558c4acfd289147c8bac0697f5e8d7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Code repository'
  prefs: []
  type: TYPE_NORMAL
- en: 'After having looked into the datasets we will use let‘s take a look into the
    directory structure and the files we will use in this demonstration. The directory
    consists of 3 files: config.yaml, a configuration file for running SageMaker jobs
    through the remote decorator and requirements.txt for extending the dependencies
    installed in the training container. Finally, there is the rlhf-multi-adapter-ppo.ipynb
    notebook containing the code for our E2E PA.'
  prefs: []
  type: TYPE_NORMAL
- en: The previously mentioned config.yaml file holds important configurations for
    the training jobs triggered by the remote decorator, e.g. training instance type
    or training image.
  prefs: []
  type: TYPE_NORMAL
- en: Notebook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s open the rlhf-multi-adapter-ppo.ipynb notebook. First, we install
    and import the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing reward model training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously discussed, we will be using the Anthropic/hh-rlhf dataset for
    training our reward model. Therefore, we need to convert the raw dataset into
    the above specified structure, where “input_ids” and “attention_mask” are the
    outputs of input tokenization. This format is specified as interface definition
    by the HuggingFace trl RewardTrainer class and makes the accepted and rejected
    answers easily accessible during reward model training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We login to the HuggingFace hub. Then, we retrieve the “helpful-base” of the
    „Anthropic/hh-rlhf“ dataset. The raw dataset structure looks as follows, we also
    take a look into an example dataset item.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we parse the conversations into an array seperated by conversation turn
    and role.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Based on it’s pre-training process, every model has a specific set of syntax
    and special tokens prompts should be optimized towards — this is the essence of
    prompt engineering and needs to be considered when fine-tuning. For the Meta Llama
    models this can be found in the llama-recipes [GitHub repository](https://github.com/meta-llama/llama-recipes).
    To follow these prompting guidelines for an ideal result we are encoding our dataset
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then we are tokenizing the “chosen” and “rejected” columns. Subsequently we
    remove the plain text columns as we don’t need them any more. The dataset is now
    in the format we were aiming for.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we are uploading the dataset to Amazon S3\. Please adjust the bucket
    path to a path pointing to a bucket in your account.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing PPO dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As previously discussed, we will be using the Stanford Question&Answering Dataset
    (SQuAD) for the actual PA step with PPO. Therefore we need to convert the raw
    dataset into a pre-define structure, where “input_ids“ is the vectorized format
    of the “query“” a padded version of a question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This time we are not pulling the datasets from the HuggingFace hub — instead
    we are cloning them from a GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we parse the conversations into an array separated by conversation turn
    and role. Then we are encoding our dataset according to the Meta Llama prompting
    guidelines for an ideal result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We are padding our training examples to a maximum of 2048 tokens to reduce our
    training memory footprint. This can be adjusted to up to a model’s maximum context
    window. The threshold should be a good compromise between adhering to prompt length
    required by a specific use case or domain and keeping the training memory footprint
    small. Note, that larger input token sizes might require scaling out your compute
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we are uploading the dataset to s3\. Please adjust the bucket path
    to a path pointing to a bucket in your account.
  prefs: []
  type: TYPE_NORMAL
- en: Reward model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the training of the reward model we are defining two helper functions:
    One function counting the trainable parameters of a model to showcase how LoRA
    impacts the trainable parameters and another function to identify all linear modules
    in a model since they will be targeted by LoRA.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The training fuction “train_fn“ is decorated with the remote decorator. This
    allows us to execute it as SageMaker training job. In the decorator we define
    a couple of parameters alongside the ones specified in the config.yaml. These
    parameters can be overwritten by the actual function call when triggering the
    training job.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the training function we first set a seed for determinism. Then we initialize
    an Accelerator object for handling distributed training. This object will orchestrate
    our distributed training in a data parallel manner across 4 ranks (note *nproc_per_node=4*
    in decorator parameters) on a ml.g5.12xlarge instance (note *InstanceType: ml.g5.12xlarge*
    in *config.yaml*).'
  prefs: []
  type: TYPE_NORMAL
- en: We then log into the HuggingFace hub and load and configure the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the next step we are loading the training data from S3 and load them into
    a HuggingFace DatasetDict object. Since this is a demonstration we want to be
    able training with only a subset of the data to save time and resources. For this
    we can configure the range of dataset items to be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are using the HuggingFace bitsandbytes library for quantization. In this
    configuration, bitsandbytes will replace all linear layers of the model with NF4
    layers and the computation as well as storage data type to bfloat16\. Then, the
    model is being loaded from HuggingFace hub in this quantization configuration
    using the flash attention 2 attention implementation for the attention heads for
    further improved memory usage and computational efficiency. We also print out
    all trainable parameters of the model in this state. Then, the model is prepared
    for quantized training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Next, we discover all linear layers of the model to pass them into a LoraConfig
    which specifies some LoRA hyperparameters. Please note, that unlike for traditional
    LLM training the task_type is not “CAUSAL_LM” but ”SEQ_CLS” since we are training
    a reward model and not a text completion model. The configuration is applied to
    the model and the training parameters are printed out again. Please note the difference
    in trainable and total parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We define the RewardConfig holding important training hyperparameters like training
    batch size, training epochs, learning rate and more. We also define a *max_length=512\.*
    Thiswill be the maximum length of prompt+response pairs being used for reward
    adapter training and will be enforced through left-side padding to preserve the
    last conversation turn which marks the key difference between chosen and rejected
    sample. Again, this can be adjusted to up to a model’s maximum context window
    while finding a good compromise between adhering to prompt length required by
    a specific use case or domain and keeping the training memory footprint small.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we initialize the RewardTraining object orchestrating the training
    with this configuration and further training inputs like model, tokenizer and
    datasets. Then we kick off the training. Once the training has finished we push
    the reward model adapter weights to the reward model model repository we have
    created in the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can now kickoff the training itself. Therefor we call the training function
    which kicks off an ephemeral training job in Amazon SageMaker. For this we need
    to pass some parameters to the training function, e.g. the model id, training
    dataset path and some hyperparameters. Note that the hyperparameters used for
    this demonstration can be adjusted as per requirement. For this demonstration
    we work with 100 training and 10 evaluation examples to keep the resource and
    time footprint low. For a real-world use case a full dataset training should be
    considered. Once the training has started the training logs are streamed to the
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Multi-adapter PPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the actual PA step with PPO we are reusing function counting the trainable
    parameters of a model to showcase how LoRA impacts the trainable parameters. Sililarily
    to the reward model training step, the training fuction “train_fn“ is decorated
    with the remote decorator allowing us to execute it as SageMaker training job.
  prefs: []
  type: TYPE_NORMAL
- en: In the training function we first set a seed for determinism. Then we initialize
    an Accelerator object for handling distributed training. As with the reward adapter
    training, this object will handle our distributed training in a data parallel
    manner across 4 ranks on a ml.g5.12xlarge instance.
  prefs: []
  type: TYPE_NORMAL
- en: We then log into the HuggingFace hub and load and configure the tokenizer. In
    the next step we are loading the training data from S3 and load them into a HuggingFace
    DatasetDict object. Since this is a demonstration we want to be able training
    with only a subset of the data to save time and resources. For this we can configure
    the range of dataset items to be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define a LoraConfig which specifies the LoRA hyperparameters. Please
    note, that this time the task_type is “CAUSAL_LM” since we are aiming to fine-tune
    a text completion model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We are using the HuggingFace bitsandbytes library for quantization. In this
    configuration, bitsandbytes will replace all linear layers of the model with NF4
    layers and the computation to bfloat16.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Then, the model is being loaded from HuggingFace hub in this quantization using
    both the specified LoraConfig and BitsAndBytesConfig. Note that this model is
    not wrapped into a simple AutoModelForCausalLM class, instead we are using a AutoModelForCausalLMWithValueHead
    class taking our reward model adapter as input. This is a model class purposely
    built for multi-adapter PPO, orchestrating adapter loading and plugins during
    the actual training loop we will discuss subsequently.For the sake of completeness
    we also print out all trainable parameters of the model in this state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We define the PPOConfig holding important training hyperparameters like training
    batch size, learning rate and more. Further, we initialize the PPOTrainer object
    orchestrating the training with this configuration and further training inputs
    like model, tokenizer and datasets. Note, that the ref_model for the computation
    of the KL divergence is not specified. As previously discussed, in this configuration
    the PPOTrainer uses a reference model with the same architecture as the model
    to be optimized with shared layers. Further, the inference parameters for inference
    to retrieve the text completion based on the query from the training dataset are
    defined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we execute the actual multi-adapter PPO training loop as follows on a
    batch of training data: First, the LoRA adapters we are RLHF fine-tuning are applied
    for inference to retrieve a text completion based on the query from the training
    dataset. The response is decoded into plain text and combined with the query.
    Then, the reward adapters are applied to compute the reward of the the query —
    completion pair in tokenized form. Subsequently, the reward value is used alongside
    the question and response tensors for the optimization step. Note, that in the
    background the Kullback–Leibler-divergence (KL-divergence) between the inference
    logits of the fine-tuned model and base model (prediction shift penalty) is computed
    and included as additional reward signal integrated term used during the optimization
    step. Since this is based on the same input prompt, the KL-divergence acts as
    a measure of how these two probability distributions and hence the models themselves
    differ from each other over training time. This divergence is subtracted from
    the reward term, penalizing divergence from the base model to assure algorithmic
    stability and linguistic consistency. Finally, the adapters we are RLHF fine-tuning
    are applied again for the back propagation.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we kick off the training. Once the training has finished we push the preference-alignged
    model adapter weights to the rlhf model model repository we have created in the
    beginning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can now kickoff the training itself. Therefore we call the training function
    which kicks off an ephemeral training job in Amazon SageMaker. For this we need
    to pass some parameters to the training function, e.g. the model id, training
    dataset path, reward model path and some hyperparameters. Note that the hyperparameters
    used for this demonstration can be adjusted as per requirement. For this demonstration
    we work with 100 training examples to keep the resource and time footprint low.
    For a real-world use case a full dataset training should be considered. Once the
    training has started the training logs are streamed to the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we want to test the tuned model. Therefore we will deploy it to a SageMaker
    endpoint. We start with importing required dependencies as well as setting up
    the SageMaker session and IAM.
  prefs: []
  type: TYPE_NORMAL
- en: For the deployment we are using the [SageMaker — Huggingface integration](https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/)
    with the [TGI containers](https://huggingface.co/docs/text-generation-inference/en/index).
    We define the instance type, image as well as model-related parameters like the
    base model, LoRA adapter, quantization and others.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Then we deploy the model. Once the model has been deployed we can test the model
    inference with a prompt of our choice. Note that we are using the encode_dialogue
    function defined during data preprocessing to optimize the prompt for the Llama
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Cleanup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we cleanup the deployed endpoint and model entity to be responsible
    in resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both reward model adapter training and multi-adapter PPO training were executed
    on an *ml.g5.12xlarge* instance using a dataset of 100 randomly sampled rows from
    the respective training datasets. The average training time was approximately
    400 seconds for each step. As of November 2024, this instance type is priced at
    **$7.09/hour** in the us-east-1 region.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the end-to-end training cost for this RLHF implementation with
    multi-adapter PPO amounts to less than *($7.09 * 400s)/(3600s * 100)* **~ $0.0079
    per individual training sample** for each of the two training steps. This translates
    to **less than $0.015 per 1000 training tokens for the reward model training**
    and **less than $0.0039 per 1000 training tokens** for the multi-adapter PPO step.
  prefs: []
  type: TYPE_NORMAL
- en: For inference, the model is hosted on an *ml.g5.4xlarge* instance. As of November
    2024, this instance type is priced at **$2.03/hour** in the us-east-1 region.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this blog post, we explored RLHF with multi-adapter PPO, a frugal approach
    to preference alignment for large language models. We covered the following key
    points:'
  prefs: []
  type: TYPE_NORMAL
- en: The importance of preference alignment in boosting LLM performance and its role
    in the democratization of AI.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The principles of RLHF and its two-step process involving reward model training
    and PPO-based fine-tuning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Challenges associated with implementing RLHF, including computational resources
    and orchestration complexity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The multi-adapter PPO approach as a solution to reduce infrastructure and orchestration
    footprint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A detailed, end-to-end implementation using HuggingFace frameworks and Amazon
    SageMaker, covering data preprocessing, reward model training, multi-adapter PPO
    training, and model deployment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This frugal approach to RLHF makes preference alignment more accessible to a
    broader range of practitioners, potentially accelerating the development and deployment
    of aligned AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: By reducing computational requirements and simplifying the implementation process,
    multi-adapter PPO opens up new possibilities for fine-tuning language models to
    specific domains or user preferences.
  prefs: []
  type: TYPE_NORMAL
- en: As the field of AI continues to evolve, techniques like this will play a crucial
    role in creating more efficient, effective, and aligned language models. I’d like
    to encourage readers to experiment with this approach, adapt it to their specific
    use cases, and share their success stories in building responsible and user-centric
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '***If you’re interested in learning more about LLM pre-training and alignment,
    I recommend checking out the*** [***AWS SkillBuilder course***](https://explore.skillbuilder.aws/learn/course/external/view/elearning/17556/building-language-models-on-aws)
    ***I recently published with my esteemed colleagues*** [***Anastasia***](https://anastasia-tzeveleka.medium.com/)
    ***and*** [***Gili***](https://medium.com/@gilinachum)***.***'
  prefs: []
  type: TYPE_NORMAL
