- en: Preference Alignment for Everyone!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个人的偏好对齐！
- en: 原文：[https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08](https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08](https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e?source=collection_archive---------2-----------------------#2024-11-08)
- en: Frugal RLHF with multi-adapter PPO on Amazon SageMaker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊SageMaker上的节俭RLHF与多适配器PPO
- en: '[](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[![Aris
    Tsakpinis](../Images/2cc1101aed68e1f71a0026bfdec28f58.png)](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    [Aris Tsakpinis](https://medium.com/@aris.tsakpinis?source=post_page---byline--2563cec4d10e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    ·26 min read·Nov 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2563cec4d10e--------------------------------)
    ·阅读时间：26分钟·2024年11月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/77e9a1dd091dae519aca481072d8bae4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e9a1dd091dae519aca481072d8bae4.png)'
- en: Photo by StableDiffusionXL on Amazon Web Services
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由StableDiffusionXL在亚马逊Web服务上提供
- en: 'Note: All images, unless otherwise noted, are by the author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：除非另有注明，所有图片均为作者提供。
- en: What is this about and why is it important?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这是什么？为什么它很重要？
- en: Over the last 2 years, research and practice have delivered plenty of proof
    that preference alignment (PA) is a game changer for boosting Large Language Models
    (LLMs) performance, especially (but not exclusively) for models directly exposed
    to humans. PA uses (human) feedback to align model behavior to what is preferred
    in the environment a model is actually living in, instead of relying solely on
    proxy datasets like other fine-tuning approaches do (as I explain in detailed
    in [this blog post](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)
    on fine-tuning variations). This improvement in model performance, as perceived
    by human users, has been a key factor in making LLMs and other Foundation Models
    (FMs) more accessible and popular, contributing significantly to the current excitement
    around Generative AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两年里，研究和实践提供了大量证据，表明偏好对齐（PA）是提升大语言模型（LLM）性能的游戏规则改变者，尤其是（但不限于）直接暴露于人类的模型。PA利用（人类）反馈来使模型行为与模型实际所处环境中的偏好保持一致，而不是像其他微调方法那样仅仅依赖代理数据集（正如我在[这篇博文](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)中详细解释的微调变体）。这种在模型性能上的提升，正如人类用户所感知的，是使LLM和其他基础模型（FM）变得更加可访问和流行的关键因素，显著推动了当前生成式AI的兴奋热潮。
- en: Over time various approaches to PA have been proposed by research and quickly
    adapted by some practitioners. Amongst them, RLHF is (as of Autumn 2024) by far
    the most popular and proven approach.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，研究提出了各种PA方法，并迅速被一些实践者采纳。在这些方法中，RLHF（截至2024年秋季）无疑是最受欢迎且被验证有效的方法。
- en: However, due to challenges around implementation complexity, compute requirements
    or training orchestration, so far the adaptation of PA approaches like RLHF in
    practice is limited to mainly high-skill profile individuals and organizations
    like FM producers. Also, most practical examples and tutorials I found showcasing
    how to master an approach like RLHF are limited or incomplete.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于实施复杂性、计算需求或训练协调等挑战，迄今为止，像RLHF这样的PA方法的适应主要限于高技能个人和组织，如FM生产商。此外，我找到的大多数实际示例和教程，展示了如何掌握像RLHF这样的方式，往往是有限或不完整的。
- en: This blog post provides you with a comprehensive introduction into RLHF, discusses
    challenges around the implementation, and suggests RLHF with multi-adapter PPO,
    a light-weight implementation approach tackling some key ones of these challenges.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文为您提供了 RLHF 的全面介绍，讨论了实施中的挑战，并建议使用多适配器 PPO 的 RLHF，这是一种轻量级的实现方法，解决了这些挑战中的一些关键问题。
- en: Next, we present an end-to-end (E2E) implementation of this approach in a Jupyter
    notebook, covering data collection, preparation, model training, and deployment.
    We leverage HuggingFace frameworks and Amazon SageMaker to provide a user-friendly
    interface for implementation, orchestration, and compute resources. The blog post
    then guides you through the key sections of this notebook, explaining implementation
    details and the rationale behind each step. This hands-on approach allows readers
    to understand the practical aspects of the process and easily replicate the results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在 Jupyter notebook 中展示了该方法的端到端（E2E）实现，涵盖了数据收集、准备、模型训练和部署。我们利用 HuggingFace
    框架和 Amazon SageMaker 提供了一个用户友好的界面，用于实施、编排和计算资源。接下来的博客文章将引导您浏览这个 notebook 的关键部分，解释实现细节以及每个步骤背后的理论依据。这种实践方式让读者能够理解过程中的实际方面，并轻松复现结果。
- en: The principles of RLHF
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习从人类反馈（RLHF）的原理
- en: Reinforcement learning from human feedback was one of the major hidden technical
    backbones of the early Generative AI hype, giving the breakthrough achieved with
    great large decoder models like Anthropic Claude or OpenAI’s GPT models an additional
    boost into the direction of user alignment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈的强化学习是早期生成性 AI 热潮的一个重要技术支撑，它为使用大型解码器模型（如 Anthropic Claude 或 OpenAI 的 GPT
    模型）所取得的突破提供了额外的推动，使其更加贴近用户需求。
- en: The great success of PA for FMs perfectly aligns with the concept of user-centric
    product development, a core and well-established principle of agile product development.
    Iteratively incorporating feedback from actual target users has proven highly
    effective in developing outstanding products. This approach allows developers
    to continually refine and improve their offerings based on real-world user preferences
    and needs, ultimately leading to more successful and user-friendly products.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PA 对 FMs 的巨大成功与以用户为中心的产品开发理念完美契合，这是敏捷产品开发的核心且已确立的原则。通过不断地结合实际目标用户的反馈，已被证明在开发卓越产品方面非常有效。这种方法使开发人员能够基于现实世界的用户偏好和需求不断地改进和完善他们的产品，最终带来更成功且用户友好的产品。
- en: 'Other fine-tuning approaches like continued pre-training (CPT) or supervised
    fine-tuning (SFT) don’t cover this aspect since:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像持续预训练（CPT）或监督微调（SFT）等其他微调方法并没有涵盖这一方面，因为：
- en: the datasets used for these approaches are (labelled or unlabelled) proxies
    for what we think our users like or need (i.e. knowledge or information, language
    style, acronyms or task-specific behaviour like instruction-following, chattiness
    or others), crafted by a few in charge of model training or fine-tuning data.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法所使用的数据集（标注或未标注）是我们认为用户喜欢或需要的内容（即知识或信息、语言风格、缩写或任务特定的行为，如遵循指令、健谈等）的代理，这些数据集由负责模型训练或微调数据的少数人创建。
- en: the algorithm(s), training objective(s) and loss function(s) used for these
    approaches (i.e. causal language modeling) are using next-token prediction as
    proxy for higher level metrics (e.g. accuracy, perplexity, …).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法中使用的算法、训练目标和损失函数（即因果语言建模）将下一个词预测作为更高层次指标（例如准确率、困惑度等）的代理。
- en: Therefore, PA is undoubtedly a technique we should employ when aiming to create
    an exceptional experience for our users. This approach can significantly enhance
    the quality, safety and relevance of AI-generated responses, leading to more satisfying
    interactions and improved overall user satisfaction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PA 无疑是我们在致力于为用户创造卓越体验时应该采用的技术。这种方法可以显著提高 AI 生成响应的质量、安全性和相关性，从而带来更令人满意的互动并提高整体用户满意度。
- en: How does RLHF work?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF 是如何工作的？
- en: '*Note: This section is an adapted version of the RLHF section in my* [*blog
    post about different fine-tuning variations*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)*.
    For a comprehensive overview about fine-tuning you might want to check it out
    as well.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：本节内容是我在我的* [*关于不同微调变体的博客文章*](/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)*中的
    RLHF 部分的改编版本。如果你想要了解微调的全面概述，可能还需要查看它。*'
- en: '![](../Images/0c206faae0d0b57cb16f4bcbf0ba5631.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c206faae0d0b57cb16f4bcbf0ba5631.png)'
- en: 'Figure 1: Reward model training for RLHF (Source: Lambert et al, 2022)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：RLHF 的奖励模型训练（来源：Lambert 等，2022）
- en: 'RLHF works in a two-step process and is illustrated in Figures 13 and 14:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF 采用两步过程，具体过程如图 13 和图 14 所示：
- en: 'Step 1 (Figure 1): First, a reward model needs to be trained for later usage
    in the actual RL-powered training approach. Therefore, a prompt dataset aligned
    with the objective (e.g. chat/instruct model or domain-specific task objective)
    to optimize is being fed to the model to be fine-tuned, while requesting not only
    one but two or more inference results. These results will be presented to human
    labelers for scoring (1st, 2nd, 3rd, …) based on the optimization objective. There
    are also a few open-sourced preference ranking datasets, among them [“Anthropic/hh-rlhf”](https://huggingface.co/datasets/Anthropic/hh-rlhf)
    (we will use this dataset in the practical part of this blog) which is tailored
    towards red-teaming and the objectives of honesty and harmlessness. After normalizing
    and converting the scores into reward values, a reward model is trained using
    individual sample-reward pairs, where each sample is a single model response.
    The reward model architecture is usually similar to the model to be fine-tuned,
    adapted with a small head eventually projecting the latent space into a reward
    value instead of a probability distribution over tokens. However, the ideal sizing
    of this model in parameters is still subject to research, and different approaches
    have been chosen by model providers in the past. In the practical part of this
    blog, for the reward model we will use the same model architecture compared to
    the model to be fine-tuned.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步（图 1）：首先，需要训练一个奖励模型，以便在实际的强化学习驱动的训练方法中使用。因此，模型将接收一个与目标对齐的提示数据集（例如，聊天/指令模型或特定领域任务目标），该数据集用于优化，并要求模型生成不止一个而是两个或更多推理结果。这些结果将提交给人工标注员进行评分（第一、第二、第三等），评分标准基于优化目标。也有一些开源的偏好排序数据集，其中包括[“Anthropic/hh-rlhf”](https://huggingface.co/datasets/Anthropic/hh-rlhf)（我们将在本博客的实践部分中使用该数据集），该数据集专门针对红队测试以及诚实性和无害性目标。经过归一化并将得分转换为奖励值后，使用每个样本-奖励对来训练奖励模型，其中每个样本是单个模型响应。奖励模型的架构通常与待微调的模型相似，只是在最后适配了一个小型头部，将潜在空间投射为奖励值，而不是标记的概率分布。然而，该模型的理想参数规模仍在研究中，过去不同的模型提供者采取了不同的方式。在本博客的实践部分中，奖励模型我们将使用与待微调模型相同的架构。
- en: '![](../Images/762043a6b8c3fb52e183b1886888c937.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/762043a6b8c3fb52e183b1886888c937.png)'
- en: 'Figure 2: Reinforcement learning based model tuning with PPO for RLHF (Source:
    Lambert et al, 2022)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于 PPO 的强化学习模型调优用于 RLHF（来源：Lambert 等，2022）
- en: 'Step 2 (Figure 2): Our new reward model is now used for training the actual
    model. Therefore, another set of prompts is fed through the model to be tuned
    (grey box in illustration), resulting in one response each. Subsequently, these
    responses are fed into the reward model for retrieval of the individual reward.
    Then, Proximal Policy Optimization (PPO), a policy-based RL algorithm, is used
    to gradually adjust the model’s weights in order to maximize the reward allocated
    to the model’s answers. As opposed to Causal Language Modeling (CLM — you can
    find a detailed explanation [here](https://medium.com/towards-data-science/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)),
    instead of gradient descent, this approach leverages gradient ascent (or gradient
    descent over *1 — reward*) since we are now trying to maximize an objective (reward).
    For increased algorithmic stability to prevent too heavy drifts in model behavior
    during training, which can be caused by RL-based approaches like PPO, a prediction
    shift penalty is being added to the reward term, penalizing answers diverging
    too much from the initial language model’s predicted probability distribution
    on the same input prompt.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步（图2）：我们的新奖励模型现在用于训练实际的模型。因此，另一组提示词被输入到需要微调的模型中（图示中的灰色框），每次得到一个响应。随后，这些响应被输入到奖励模型中，以检索各自的奖励。接着，使用基于策略的强化学习算法——近端策略优化（PPO），逐步调整模型的权重，以最大化分配给模型回答的奖励。与因果语言建模（CLM——详细解释见[这里](https://medium.com/towards-data-science/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224)）不同，PPO方法不是采用梯度下降，而是利用梯度上升（或者说是对*1
    — 奖励*进行梯度下降），因为我们现在试图最大化一个目标（奖励）。为了提高算法的稳定性，防止在训练过程中因RL方法（如PPO）引发的模型行为过度漂移，奖励项中加入了预测偏移惩罚，对偏离初始语言模型预测概率分布过多的回答进行惩罚。
- en: Challenges with RLHF
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF面临的挑战
- en: 'The way how RLHF is working poses some core challenges to implementing and
    running it at scale, amongst them the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF的工作方式本身就带来了一些核心挑战，特别是在大规模实现和运行方面，以下是其中的一些挑战：
- en: '- **Cost of training the reward model:** Picking the right model architecture
    and size for the reward model is still current state of research. These models
    are usually transformer models similar to the model to be fine-tuned, equipped
    with a modified head delivering reward scores instead of a vocabular probability
    distribution. This means, that independent from the actual choice, most reward
    models are in the billions of parameters. Full parameter training of such a reward
    model is data and compute expensive.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '- **奖励模型训练的成本：** 选择合适的奖励模型架构和大小仍然是当前研究的热点。这些模型通常是类似于需要微调的模型的变换器模型，并配备一个修改过的头部，输出奖励分数而非词汇概率分布。这意味着，不论实际选择怎样，大多数奖励模型的参数数量都在数十亿级别。对这种奖励模型进行完整的参数训练需要大量的数据和计算资源。'
- en: '- **Cost of training cluster:** With the reward model (for the reward values),
    the base model (for the KL prediction shift penalty) and the model actually being
    fine-tuned three models need to be hosted in parallel in the training cluster.
    This leads to massive compute requirements usually only being satisfied by a multi-node
    cluster of multi-GPU instances (in the cloud), leading to hardware and operational
    cost.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '- **训练集群的成本：** 在训练集群中，需要同时托管奖励模型（用于奖励值）、基础模型（用于KL预测偏移惩罚）和实际被微调的模型三个模型。这会导致巨大的计算需求，通常只有通过多节点的多GPU实例集群（通常部署在云端）才能满足，从而带来硬件和运营成本。'
- en: '- **Orchestration of training cluster:** The RLHF algorithm requires a combination
    of inference- and training-related operations in every training loop. This needs
    to be orchestrated in a multi-node multi-GPU cluster while keeping communication
    overhead minimal for optimal training throughput.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '- **训练集群的协调：** RLHF算法需要在每次训练循环中同时进行推理和训练相关的操作。这要求在多节点多GPU集群中进行协调，并尽量减少通信开销，以达到最佳的训练吞吐量。'
- en: '- **Training/inference cost in highly specialized setups:** PA shines through
    aligning model performance towards a user group or target domain. Since most professional
    use cases are characterized by specialized domains with heterogenous user groups,
    this leads to an interesting tradeoff: Optimizing for performance will lead in
    training and hosting many specialized models excelling in performance. However,
    optimizing for resource consumption (i.e. cost) will lead to overgeneralization
    of models and decreasing performance.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '- **在高度专业化设置中的训练/推理成本：** PA通过将模型性能对齐到用户群体或目标领域而表现突出。由于大多数专业应用场景具有特定的领域和异质的用户群体，这导致了一个有趣的权衡：优化性能将导致训练和托管许多在性能上表现出色的专业化模型。然而，优化资源消耗（即成本）将导致模型的过度泛化，从而降低性能。'
- en: RLHF with multi-adapter PPO
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多适配器 PPO 的 RLHF
- en: '![](../Images/43d376a2044c3fc5511ddd397cca4912.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43d376a2044c3fc5511ddd397cca4912.png)'
- en: 'Figure 3: Minimizing GPU footprint of PPO through dynamic multi-adapter loading'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：通过动态多适配器加载最小化PPO的GPU占用
- en: Multi-adapter PPO is a particularly GPU-frugal approach to the second step of
    the RLHF training process. Instead of using full-parameter fine-tuning, it leverages
    parameter-efficient fine-tuning (PEFT) techniques to reduce the infrastructure
    and orchestration footprint drastically. Instead of hosting three distinct models
    (model being fine-tuned, reward model, reference model for KL prediction shift
    penalty) in parallel in the training cluster this approach leverages Low Rank
    Adaptation (LoRA) adapters during the fine-tuning which are dynamically loaded
    and unloaded into the accelerators of the training cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多适配器PPO是RLHF训练过程第二步中一种特别节省GPU资源的方法。它不是使用全参数微调，而是利用参数高效微调（PEFT）技术，显著减少基础设施和协调占用。与其在训练集群中并行托管三个独立的模型（微调模型、奖励模型、用于KL预测偏移惩罚的参考模型），这种方法在微调过程中使用低秩适配（LoRA）适配器，这些适配器会动态加载和卸载到训练集群的加速器中。
- en: '![](../Images/3fd297299c11b108133ded22488394b9.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fd297299c11b108133ded22488394b9.png)'
- en: 'Figure 4: E2E RLHF with multi-adapter PPO for a harmless Q&A bot'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用多适配器PPO进行端到端RLHF训练，用于无害的问答机器人
- en: 'While this approach’s goal is ultimately a resource and orchestration frugal
    approach to the second step of RLHF, it has implications on the first step:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法的最终目标是对RLHF第二步采取资源和协调节约的方法，但它对第一步也有影响：
- en: '**Reward model choice:** A reward model with the same model architecture as
    the model to be fine-tuned is picked and equipped with a reward classification
    head.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励模型选择：** 选择与要微调的模型具有相同模型架构的奖励模型，并为其配备奖励分类头。'
- en: '**Reward model training approach:** As illustrated in figure 4(2), instead
    of full-parameter reward model training, a reward model LoRA adapter is being
    trained, leading to a much leaner training footprint.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励模型训练方法：** 如图4(2)所示，与全参数奖励模型训练不同，训练的是奖励模型的LoRA适配器，从而减少了训练的占用资源。'
- en: Similarly to the this, the RLHF fine-tuning of the model being performed in
    the second step is not done in a full-parameter fine-tuning manner. Instead, a
    LoRA adapter is trained. As depicted in figure 4, during a training iteration,
    first the RLHF model adapter is being loaded to generate model responses to the
    prompts of the current training batch (4a). Then, the reward model adapter is
    loaded to calculate the corresponding raw reward values (4b). To complete the
    reward term, the input prompt is fed through the base model for calculation of
    the KL prediction shift penalty. Therefor, all adapters need to be unloaded (4c,
    4d). Finally, the RLHF model adapter is loaded again to perform the weight updates
    for this iteration step (4e).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与此类似，第二步中进行的RLHF微调并不是采用全参数微调的方式。相反，训练的是LoRA适配器。如图4所示，在一次训练迭代中，首先加载RLHF模型适配器，以便生成当前训练批次提示的模型响应（4a）。然后，加载奖励模型适配器以计算相应的原始奖励值（4b）。为了完成奖励项，输入提示通过基础模型进行计算，以获得KL预测偏移惩罚。因此，所有适配器需要被卸载（4c，4d）。最后，RLHF模型适配器重新加载，以执行此次迭代步骤的权重更新（4e）。
- en: This approach to RLHF reduces the memory footprint as well as orchestration
    complexity significantly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种RLHF方法显著减少了内存占用和协调复杂性。
- en: Running RLHF with multi-adapter PPO with HuggingFace and Amazon SageMaker
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在HuggingFace和Amazon SageMaker上运行使用多适配器 PPO 的RLHF
- en: In what follows we will go through a notebook showcasing RLHF with multi-adapter
    PPO in an E2E fashion. Thereby we use HuggingFace and Amazon SageMaker for an
    especially user-friendly interface towards the implementation, orchestration and
    compute layers. The entire notebook can be found [here](https://github.com/aws-samples/build-language-models-on-aws/tree/main/align-models-with-amazon-sagemaker/rlhf-with-multi-adapter-ppo).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将通过一个展示多适配器 PPO 在端到端 (E2E) 方式下使用强化学习与人类反馈（RLHF）的笔记本来进行讲解。因此，我们将使用
    HuggingFace 和 Amazon SageMaker 提供一个特别用户友好的接口，用于实现、编排和计算层的操作。整个笔记本可以在[这里](https://github.com/aws-samples/build-language-models-on-aws/tree/main/align-models-with-amazon-sagemaker/rlhf-with-multi-adapter-ppo)找到。
- en: Scenario
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景
- en: The pace model producers nowadays are releasing new models is impressive. Hence,
    I want to keep the scenario we are looking into as generic as possible.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，模型发布的速度令人印象深刻。因此，我希望将我们所讨论的场景保持尽可能通用。
- en: While most of the models published these days have already gone through multiple
    fine-tuning steps like SFT or even PA, since these models are general purpose
    ones they where certainly not performed tailored to your target users or target
    domain. This means that even though we are using a pre-aligned model (e.g. an
    instruction fine-tuned model), for optimising model performance in your domain
    further alignment steps are required.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现如今发布的大多数模型都已经经过多次微调步骤，如 SFT 或甚至 PA，但由于这些模型是通用型的，因此它们显然并未针对您的目标用户或目标领域进行定制。这意味着，即使我们使用一个已调优的预对齐模型（例如指令调优模型），为了在您的领域中优化模型性能，仍然需要进一步的对齐步骤。
- en: For this blog we will assume the model should be optimised towards maximising
    the helpfulness while carrying out user-facing single- and multi-turn conversations
    in a Q&A style in the scientific domain. Thus, we will start from a general-purpose
    instruct / Q&A pre-trained FM.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们假设模型应该优化以最大化在科学领域中进行单轮和多轮用户面对面的问答对话时的有用性。因此，我们将从一个通用的指令/Q&A 预训练 FM
    开始。
- en: Model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: Despite of being generic we need to choose a model for our endeavour. For this
    blog we will be working with Meta Llama3.1–8b-instruct. This model is the smallest
    fashion of a new collection of multilingual pre-trained and instruction-tuned
    decoder models Meta released in Summer 2024\. More details can be found in the
    [documentation](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1#llama-3.1-instruct)
    in the Meta homepage and in the [model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
    provided by HuggingFace.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一选择较为通用，但我们仍需要为我们的任务选择一个模型。在这篇博客中，我们将使用 Meta 的 Llama3.1–8b-instruct 模型。这个模型是
    Meta 在 2024 年夏季发布的一系列多语言预训练和指令调优解码器模型中最小的一个版本。更多细节可以在 Meta 官方网站的[文档](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1#llama-3.1-instruct)以及
    HuggingFace 提供的[模型卡](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)中找到。
- en: '![](../Images/92f35b3b8bd00507db12d2a4a26d3815.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92f35b3b8bd00507db12d2a4a26d3815.png)'
- en: 'Figure 5: Llama-3.1–8b-instruct model card on HuggingFace hub'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：HuggingFace Hub 上的 Llama-3.1–8b-instruct 模型卡
- en: Prerequisites
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 必备条件
- en: We start our notebook walkthrough with some prerequisite preparation steps.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在笔记本的演示中开始一些必要的准备步骤。
- en: '![](../Images/448c07c74376c62d180935b6d48a716b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/448c07c74376c62d180935b6d48a716b.png)'
- en: 'Figure 6: Accepting Meta’s licensing agreement through HuggingFace hub'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：通过 HuggingFace Hub 接受 Meta 的许可协议
- en: We will be retrieving the model’s weights from the HuggingFace model hub. To
    be able to do so we need to accept Meta‘s licensing agreement and provide some
    information. This can be submitted directly through the HuggingFace model hub.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 HuggingFace 模型 Hub 获取模型的权重。为了能够执行此操作，我们需要接受 Meta 的许可协议并提供一些信息。此操作可以通过 HuggingFace
    模型 Hub 直接提交。
- en: Further, for storage of the adapter weights of both the reward model as well
    as the preference-aligned model we will be using private model repositories on
    the HuggingFace model hub. This requires a HuggingFace account. Once logged into
    the HuggingFace platform we need to create two model repositories. For this click
    on the account icon on the top right of the HuggingFace landing page and pick
    “+ New Model” in the menu.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了存储奖励模型和偏好对齐模型的适配器权重，我们将使用 HuggingFace 模型 Hub 上的私人模型仓库。这需要一个 HuggingFace
    账户。一旦登录到 HuggingFace 平台，我们需要创建两个模型仓库。为此，请点击 HuggingFace 登录页右上角的账户图标，并在菜单中选择“+
    新建模型”。
- en: '![](../Images/913dabd67e327e505f481f44bc411501.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/913dabd67e327e505f481f44bc411501.png)'
- en: 'Figure 7: Creating model repositories on HuggingFace model hub'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在 HuggingFace 模型 Hub 上创建模型仓库
- en: We can then create two private model repositories. Feel free to stick to my
    naming convention or pick a name of choice. If you name your repositories differently
    make sure to also adjust the code in the notebook.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建两个私有模型库。您可以遵循我的命名约定，也可以选择自己喜欢的名字。如果您使用不同的名字，请确保在笔记本中的代码也进行相应调整。
- en: Once created, we can see the model repositories in our HuggingFace profile.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，我们可以在 HuggingFace 个人资料中看到模型库。
- en: To authenticate against the HuggingFace model hub when pulling or pushing models
    we need to create an access token, which we will use later in the notebook. For
    this click on the account icon on the top right of the HuggingFace landing page
    and pick „Settings“ in the menu.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 HuggingFace 模型中心进行身份验证以拉取或推送模型，我们需要创建一个访问令牌，稍后将在笔记本中使用。为此，请点击 HuggingFace
    登录页面右上角的帐户图标，然后在菜单中选择“设置”。
- en: In the settings we select the menu item “Access Tokens” and then “+ Create new
    token.”
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置中，我们选择菜单项“访问令牌”，然后点击“+ 创建新令牌”。
- en: '![](../Images/6444c90a1d12546dd6a9699087935638.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6444c90a1d12546dd6a9699087935638.png)'
- en: 'Figure 8: Creating access tokens on HuggingFace hub'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在 HuggingFace hub 上创建访问令牌
- en: According to the principle of least privileges we want to create a token with
    fine-grained permission configurability. For our purpose read and write access
    to repositories is sufficient — this is why we check all three boxes in this section.
    Then we scroll down and create the token.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最小权限原则，我们希望创建一个具有细粒度权限配置的令牌。对于我们的目的，读写访问权限就足够了——因此我们在此部分勾选了所有三个选项。然后我们向下滚动并创建令牌。
- en: Once created the access token appears in plain text. Since the token will only
    be displayed once it makes sense to store it in encrypted format for example in
    a password manager.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，访问令牌会以明文形式显示。由于令牌只会显示一次，因此最好将其以加密格式存储，例如使用密码管理器。
- en: Datasets
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Now that we are finished with the prerequisites we can move on to the datasets
    we will be using for our endeavor.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完成了先决条件，可以继续使用我们在这个任务中将要使用的数据集。
- en: '![](../Images/91ccb2b30be64e7b3eb90d0bb193d5cf.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91ccb2b30be64e7b3eb90d0bb193d5cf.png)'
- en: 'Figure 9: Anthropic hh-rlhf dataset on HuggingFace hub'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：HuggingFace hub 上的 Anthropic hh-rlhf 数据集
- en: For training our reward model we will be using the Anthropic/hh-rlhf dataset,
    which is distributed under [MIT license](https://opensource.org/license/mit).
    This is a handcrafted preference dataset Anthropic has open-sourced. It consists
    of chosen and rejected model completions to one and the same prompt input. Further,
    it comes in different fashions, targeting alignment areas like harmlessness, helpfulness
    and more. For our demonstration we will use the ”helpful” subset to preference
    align our Llama model towards helpful answers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的奖励模型时，我们将使用 Anthropic/hh-rlhf 数据集，该数据集根据 [MIT 许可协议](https://opensource.org/license/mit)
    分发。这是一个由 Anthropic 开源的手工偏好数据集。它包含对同一提示输入的模型完成的选择和拒绝。除此之外，数据集还以不同的形式出现，针对无害性、有用性等对齐领域。为了演示，我们将使用“有用”子集，将我们的
    Llama 模型对齐到更有用的答案上。
- en: For the actual PA step with PPO and the previously trained reward model we need
    an additional dataset representing the target domain of our model. Since we are
    fine-tuning an instruct model towards helpfulness we need a set of instruction-style
    prompts. [The Stanford Question&Answering dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/),
    distributed under the [CC BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/deed.en)**,**
    provides us with question — context — answer pairs across a broad range of different
    areas of expertise. For our experiment we will aim for single-turn open Question&Answering.
    Hence we will use only the “question” feature of the dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际的 PA 步骤，我们需要一个额外的数据集，代表我们模型的目标领域，来配合 PPO 和之前训练的奖励模型。由于我们正在对指令模型进行微调以提高有用性，因此需要一组指令风格的提示。[斯坦福问答数据集（SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)，根据
    [CC BY-SA 4.0 许可协议](https://creativecommons.org/licenses/by-sa/4.0/deed.en)**，**
    提供了涵盖各个领域的问答对。对于我们的实验，我们将专注于单轮开放式问答。因此，我们只会使用数据集中的“问题”特性。
- en: Code repository
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码库
- en: '![](../Images/8558c4acfd289147c8bac0697f5e8d7e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8558c4acfd289147c8bac0697f5e8d7e.png)'
- en: 'Figure 10: Code repository'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：代码库
- en: 'After having looked into the datasets we will use let‘s take a look into the
    directory structure and the files we will use in this demonstration. The directory
    consists of 3 files: config.yaml, a configuration file for running SageMaker jobs
    through the remote decorator and requirements.txt for extending the dependencies
    installed in the training container. Finally, there is the rlhf-multi-adapter-ppo.ipynb
    notebook containing the code for our E2E PA.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了我们将使用的数据集后，接下来让我们看看目录结构以及在本演示中将使用的文件。该目录包含3个文件：config.yaml，一个用于通过远程装饰器运行
    SageMaker 作业的配置文件；requirements.txt，用于扩展训练容器中已安装的依赖项；最后是包含我们 E2E PA 代码的 `rlhf-multi-adapter-ppo.ipynb`
    笔记本。
- en: The previously mentioned config.yaml file holds important configurations for
    the training jobs triggered by the remote decorator, e.g. training instance type
    or training image.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的 config.yaml 文件包含了通过远程装饰器触发的训练作业的重要配置，例如训练实例类型或训练镜像。
- en: Notebook
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 笔记本
- en: Now, let’s open the rlhf-multi-adapter-ppo.ipynb notebook. First, we install
    and import the required dependencies.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开 `rlhf-multi-adapter-ppo.ipynb` 笔记本。首先，我们安装并导入所需的依赖项。
- en: Data preprocessing reward model training dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理奖励模型训练数据集
- en: As previously discussed, we will be using the Anthropic/hh-rlhf dataset for
    training our reward model. Therefore, we need to convert the raw dataset into
    the above specified structure, where “input_ids” and “attention_mask” are the
    outputs of input tokenization. This format is specified as interface definition
    by the HuggingFace trl RewardTrainer class and makes the accepted and rejected
    answers easily accessible during reward model training.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用“Anthropic/hh-rlhf”数据集来训练我们的奖励模型。因此，我们需要将原始数据集转换为上述指定的结构，其中“input_ids”和“attention_mask”是输入标记化的输出。此格式被
    HuggingFace trl RewardTrainer 类指定为接口定义，并使得在奖励模型训练过程中，接受的和拒绝的答案易于访问。
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We login to the HuggingFace hub. Then, we retrieve the “helpful-base” of the
    „Anthropic/hh-rlhf“ dataset. The raw dataset structure looks as follows, we also
    take a look into an example dataset item.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们登录 HuggingFace hub。然后，我们获取“Anthropic/hh-rlhf”数据集中的“helpful-base”。原始数据集结构如下所示，我们还查看了一个示例数据集项。
- en: Next, we parse the conversations into an array seperated by conversation turn
    and role.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对话解析为按对话轮次和角色分隔的数组。
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Based on it’s pre-training process, every model has a specific set of syntax
    and special tokens prompts should be optimized towards — this is the essence of
    prompt engineering and needs to be considered when fine-tuning. For the Meta Llama
    models this can be found in the llama-recipes [GitHub repository](https://github.com/meta-llama/llama-recipes).
    To follow these prompting guidelines for an ideal result we are encoding our dataset
    accordingly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于其预训练过程，每个模型都有一套特定的语法和特殊标记，提示应针对这些语法和标记进行优化——这就是提示工程的核心，在微调时需要考虑。对于 Meta Llama
    模型，相关信息可以在 llama-recipes [GitHub 仓库](https://github.com/meta-llama/llama-recipes)中找到。为了遵循这些提示指南并获得理想结果，我们将相应地编码我们的数据集。
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then we are tokenizing the “chosen” and “rejected” columns. Subsequently we
    remove the plain text columns as we don’t need them any more. The dataset is now
    in the format we were aiming for.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对“chosen”（选择的）和“rejected”（拒绝的）列进行分词。接着我们移除文本列，因为这些列不再需要。现在数据集已经转换为我们所期望的格式。
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, we are uploading the dataset to Amazon S3\. Please adjust the bucket
    path to a path pointing to a bucket in your account.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据集上传到 Amazon S3。请调整存储桶路径，指向您账户中的存储桶路径。
- en: Data preprocessing PPO dataset
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理 PPO 数据集
- en: As previously discussed, we will be using the Stanford Question&Answering Dataset
    (SQuAD) for the actual PA step with PPO. Therefore we need to convert the raw
    dataset into a pre-define structure, where “input_ids“ is the vectorized format
    of the “query“” a padded version of a question.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用斯坦福问题与回答数据集（SQuAD）进行实际的 PA 步骤与 PPO 结合。因此，我们需要将原始数据集转换为预定义的结构，其中“input_ids”是“query”（查询）问题的向量化格式，且该格式为问题的填充版本。
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This time we are not pulling the datasets from the HuggingFace hub — instead
    we are cloning them from a GitHub repository.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们不从 HuggingFace hub 拉取数据集，而是从 GitHub 仓库克隆数据集。
- en: Next, we parse the conversations into an array separated by conversation turn
    and role. Then we are encoding our dataset according to the Meta Llama prompting
    guidelines for an ideal result.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对话解析为按对话轮次和角色分隔的数组。然后，我们根据 Meta Llama 提示指南对数据集进行编码，以获得理想的结果。
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are padding our training examples to a maximum of 2048 tokens to reduce our
    training memory footprint. This can be adjusted to up to a model’s maximum context
    window. The threshold should be a good compromise between adhering to prompt length
    required by a specific use case or domain and keeping the training memory footprint
    small. Note, that larger input token sizes might require scaling out your compute
    infrastructure.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练样本填充到最多2048个标记，以减少训练时的内存占用。这可以调整到模型的最大上下文窗口。阈值应在特定用例或领域所需的提示长度和保持小的训练内存占用之间找到一个良好的折中。请注意，较大的输入标记大小可能需要扩展计算基础设施。
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, we are uploading the dataset to s3\. Please adjust the bucket path
    to a path pointing to a bucket in your account.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据集上传到s3。请将存储桶路径调整为指向您帐户中的存储桶的路径。
- en: Reward model training
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励模型训练
- en: 'For the training of the reward model we are defining two helper functions:
    One function counting the trainable parameters of a model to showcase how LoRA
    impacts the trainable parameters and another function to identify all linear modules
    in a model since they will be targeted by LoRA.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在奖励模型的训练中，我们定义了两个辅助函数：一个计算模型可训练参数的函数，用于展示LoRA如何影响可训练参数；另一个函数用于识别模型中的所有线性模块，因为这些模块将被LoRA所作用。
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The training fuction “train_fn“ is decorated with the remote decorator. This
    allows us to execute it as SageMaker training job. In the decorator we define
    a couple of parameters alongside the ones specified in the config.yaml. These
    parameters can be overwritten by the actual function call when triggering the
    training job.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 训练函数“train_fn”被装饰器remote装饰。这使我们能够将其作为SageMaker训练任务执行。在装饰器中，我们定义了一些参数，并与config.yaml中指定的参数一起使用。这些参数可以在实际调用训练任务时被覆盖。
- en: 'In the training function we first set a seed for determinism. Then we initialize
    an Accelerator object for handling distributed training. This object will orchestrate
    our distributed training in a data parallel manner across 4 ranks (note *nproc_per_node=4*
    in decorator parameters) on a ml.g5.12xlarge instance (note *InstanceType: ml.g5.12xlarge*
    in *config.yaml*).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '在训练函数中，我们首先设置一个种子以确保可重复性。然后，我们初始化一个Accelerator对象来处理分布式训练。该对象将在4个rank（请注意装饰器参数中的*nproc_per_node=4*）的ml.g5.12xlarge实例上以数据并行方式协调我们的分布式训练（请注意*config.yaml*中的*InstanceType:
    ml.g5.12xlarge*）。'
- en: We then log into the HuggingFace hub and load and configure the tokenizer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们登录HuggingFace Hub并加载并配置分词器。
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the next step we are loading the training data from S3 and load them into
    a HuggingFace DatasetDict object. Since this is a demonstration we want to be
    able training with only a subset of the data to save time and resources. For this
    we can configure the range of dataset items to be used.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们从S3加载训练数据，并将其加载到HuggingFace的DatasetDict对象中。由于这是一个演示，我们希望能够仅使用数据的一个子集进行训练，以节省时间和资源。为此，我们可以配置要使用的数据集项的范围。
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We are using the HuggingFace bitsandbytes library for quantization. In this
    configuration, bitsandbytes will replace all linear layers of the model with NF4
    layers and the computation as well as storage data type to bfloat16\. Then, the
    model is being loaded from HuggingFace hub in this quantization configuration
    using the flash attention 2 attention implementation for the attention heads for
    further improved memory usage and computational efficiency. We also print out
    all trainable parameters of the model in this state. Then, the model is prepared
    for quantized training.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用HuggingFace的bitsandbytes库进行量化。在此配置中，bitsandbytes将用NF4层替换模型的所有线性层，并将计算及存储的数据类型设置为bfloat16。然后，模型使用这种量化配置从HuggingFace
    Hub加载，使用Flash Attention 2实现注意力机制，以进一步提高内存使用效率和计算效率。我们还会打印出此状态下模型的所有可训练参数。接着，模型为量化训练做好准备。
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we discover all linear layers of the model to pass them into a LoraConfig
    which specifies some LoRA hyperparameters. Please note, that unlike for traditional
    LLM training the task_type is not “CAUSAL_LM” but ”SEQ_CLS” since we are training
    a reward model and not a text completion model. The configuration is applied to
    the model and the training parameters are printed out again. Please note the difference
    in trainable and total parameters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们发现模型中的所有线性层，并将它们传递到一个LoraConfig中，该配置指定了一些LoRA超参数。请注意，与传统的大型语言模型（LLM）训练不同，task_type不是“CAUSAL_LM”，而是“SEQ_CLS”，因为我们训练的是奖励模型，而不是文本生成模型。该配置应用于模型，训练参数再次被打印出来。请注意可训练参数和总参数的区别。
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We define the RewardConfig holding important training hyperparameters like training
    batch size, training epochs, learning rate and more. We also define a *max_length=512\.*
    Thiswill be the maximum length of prompt+response pairs being used for reward
    adapter training and will be enforced through left-side padding to preserve the
    last conversation turn which marks the key difference between chosen and rejected
    sample. Again, this can be adjusted to up to a model’s maximum context window
    while finding a good compromise between adhering to prompt length required by
    a specific use case or domain and keeping the training memory footprint small.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了 RewardConfig，其中包含了重要的训练超参数，如训练批量大小、训练周期、学习率等。我们还定义了一个*max_length=512*。这将是用于奖励适配器训练的
    prompt+response 对的最大长度，并将通过左侧填充来强制执行，以保持最后一次对话回合，从而标记所选样本与被拒绝样本之间的关键差异。同样，这个值可以调整到模型的最大上下文窗口，同时在遵循特定用例或领域所需的
    prompt 长度与保持训练内存占用较小之间找到良好的折衷。
- en: Further, we initialize the RewardTraining object orchestrating the training
    with this configuration and further training inputs like model, tokenizer and
    datasets. Then we kick off the training. Once the training has finished we push
    the reward model adapter weights to the reward model model repository we have
    created in the beginning.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们初始化了 RewardTraining 对象，通过此配置管理训练，并进一步输入训练数据，如模型、分词器和数据集。然后我们启动训练。一旦训练完成，我们将奖励模型适配器的权重推送到我们一开始创建的奖励模型存储库。
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can now kickoff the training itself. Therefor we call the training function
    which kicks off an ephemeral training job in Amazon SageMaker. For this we need
    to pass some parameters to the training function, e.g. the model id, training
    dataset path and some hyperparameters. Note that the hyperparameters used for
    this demonstration can be adjusted as per requirement. For this demonstration
    we work with 100 training and 10 evaluation examples to keep the resource and
    time footprint low. For a real-world use case a full dataset training should be
    considered. Once the training has started the training logs are streamed to the
    notebook.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练本身了。因此，我们调用训练函数，启动一个临时训练作业在 Amazon SageMaker 上运行。为此，我们需要将一些参数传递给训练函数，例如模型
    ID、训练数据集路径和一些超参数。请注意，此演示中使用的超参数可以根据需要进行调整。在这个演示中，我们使用 100 个训练样本和 10 个评估样本，以减少资源和时间占用。对于实际的应用场景，应该考虑使用完整数据集进行训练。训练开始后，训练日志会被流式传输到笔记本中。
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Multi-adapter PPO
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多适配器 PPO
- en: For the actual PA step with PPO we are reusing function counting the trainable
    parameters of a model to showcase how LoRA impacts the trainable parameters. Sililarily
    to the reward model training step, the training fuction “train_fn“ is decorated
    with the remote decorator allowing us to execute it as SageMaker training job.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际的 PA 步骤与 PPO，我们复用了计算模型可训练参数的函数，以展示 LoRA 如何影响可训练参数。与奖励模型训练步骤类似，训练函数“train_fn”被远程装饰器装饰，使我们能够将其作为
    SageMaker 训练作业执行。
- en: In the training function we first set a seed for determinism. Then we initialize
    an Accelerator object for handling distributed training. As with the reward adapter
    training, this object will handle our distributed training in a data parallel
    manner across 4 ranks on a ml.g5.12xlarge instance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练函数中，我们首先设置了一个种子以确保结果的可重复性。然后，我们初始化了一个 Accelerator 对象，用于处理分布式训练。与奖励适配器训练类似，这个对象将通过数据并行方式在
    ml.g5.12xlarge 实例上的 4 个 rank 进行分布式训练。
- en: We then log into the HuggingFace hub and load and configure the tokenizer. In
    the next step we are loading the training data from S3 and load them into a HuggingFace
    DatasetDict object. Since this is a demonstration we want to be able training
    with only a subset of the data to save time and resources. For this we can configure
    the range of dataset items to be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们登录 HuggingFace hub，并加载和配置分词器。在下一步中，我们从 S3 加载训练数据，并将其加载到一个 HuggingFace
    的 DatasetDict 对象中。由于这是一个演示，我们希望只用数据的一个子集进行训练，以节省时间和资源。为此，我们可以配置数据集项的使用范围。
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we define a LoraConfig which specifies the LoRA hyperparameters. Please
    note, that this time the task_type is “CAUSAL_LM” since we are aiming to fine-tune
    a text completion model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个 LoraConfig，指定 LoRA 的超参数。请注意，这次的 task_type 是“CAUSAL_LM”，因为我们旨在微调一个文本生成模型。
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We are using the HuggingFace bitsandbytes library for quantization. In this
    configuration, bitsandbytes will replace all linear layers of the model with NF4
    layers and the computation to bfloat16.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 HuggingFace 的 bitsandbytes 库进行量化。在这种配置中，bitsandbytes 会将模型的所有线性层替换为 NF4
    层，并将计算改为 bfloat16。
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Then, the model is being loaded from HuggingFace hub in this quantization using
    both the specified LoraConfig and BitsAndBytesConfig. Note that this model is
    not wrapped into a simple AutoModelForCausalLM class, instead we are using a AutoModelForCausalLMWithValueHead
    class taking our reward model adapter as input. This is a model class purposely
    built for multi-adapter PPO, orchestrating adapter loading and plugins during
    the actual training loop we will discuss subsequently.For the sake of completeness
    we also print out all trainable parameters of the model in this state.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，模型将在 HuggingFace Hub 上加载，并使用指定的 LoraConfig 和 BitsAndBytesConfig 进行量化。请注意，这个模型并没有包装成一个简单的
    AutoModelForCausalLM 类，而是使用了一个 AutoModelForCausalLMWithValueHead 类，它将我们的奖励模型适配器作为输入。这个模型类是专门为多适配器
    PPO 构建的，在实际的训练循环中协调适配器加载和插件操作，稍后我们会讨论。为了完整性，我们还会打印出该状态下模型的所有可训练参数。
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We define the PPOConfig holding important training hyperparameters like training
    batch size, learning rate and more. Further, we initialize the PPOTrainer object
    orchestrating the training with this configuration and further training inputs
    like model, tokenizer and datasets. Note, that the ref_model for the computation
    of the KL divergence is not specified. As previously discussed, in this configuration
    the PPOTrainer uses a reference model with the same architecture as the model
    to be optimized with shared layers. Further, the inference parameters for inference
    to retrieve the text completion based on the query from the training dataset are
    defined.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了包含重要训练超参数的 PPOConfig，如训练批次大小、学习率等。此外，我们初始化了 PPOTrainer 对象，它会使用该配置协调训练，并接受其他训练输入，如模型、分词器和数据集。需要注意的是，计算
    KL 散度的 ref_model 并未指定。如前所述，在此配置中，PPOTrainer 使用与要优化的模型具有相同架构且共享层的参考模型。此外，定义了推理参数，以便根据查询从训练数据集中检索文本补全。
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we execute the actual multi-adapter PPO training loop as follows on a
    batch of training data: First, the LoRA adapters we are RLHF fine-tuning are applied
    for inference to retrieve a text completion based on the query from the training
    dataset. The response is decoded into plain text and combined with the query.
    Then, the reward adapters are applied to compute the reward of the the query —
    completion pair in tokenized form. Subsequently, the reward value is used alongside
    the question and response tensors for the optimization step. Note, that in the
    background the Kullback–Leibler-divergence (KL-divergence) between the inference
    logits of the fine-tuned model and base model (prediction shift penalty) is computed
    and included as additional reward signal integrated term used during the optimization
    step. Since this is based on the same input prompt, the KL-divergence acts as
    a measure of how these two probability distributions and hence the models themselves
    differ from each other over training time. This divergence is subtracted from
    the reward term, penalizing divergence from the base model to assure algorithmic
    stability and linguistic consistency. Finally, the adapters we are RLHF fine-tuning
    are applied again for the back propagation.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在一批训练数据上执行实际的多适配器 PPO 训练循环：首先，应用我们正在进行 RLHF 微调的 LoRA 适配器进行推理，从训练数据集中基于查询检索文本补全。响应被解码为纯文本并与查询合并。接着，应用奖励适配器计算查询-补全对在标记化形式下的奖励。随后，奖励值与问题和响应张量一起用于优化步骤。需要注意的是，在后台计算了经过微调的模型与基础模型之间的
    Kullback–Leibler 散度（KL 散度）（预测偏移惩罚），并将其作为额外的奖励信号集成项，在优化步骤中使用。由于这是基于相同的输入提示，KL 散度作为度量标准，用于评估这两个概率分布以及因此模型本身在训练过程中如何相互偏离。这个散度会从奖励项中扣除，惩罚与基础模型的偏离，以确保算法稳定性和语言一致性。最后，我们正在进行
    RLHF 微调的适配器再次应用于反向传播。
- en: Then we kick off the training. Once the training has finished we push the preference-alignged
    model adapter weights to the rlhf model model repository we have created in the
    beginning.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们启动训练。一旦训练完成，我们会将对齐的模型适配器权重推送到我们最初创建的 rlhf 模型仓库中。
- en: '[PRE19]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can now kickoff the training itself. Therefore we call the training function
    which kicks off an ephemeral training job in Amazon SageMaker. For this we need
    to pass some parameters to the training function, e.g. the model id, training
    dataset path, reward model path and some hyperparameters. Note that the hyperparameters
    used for this demonstration can be adjusted as per requirement. For this demonstration
    we work with 100 training examples to keep the resource and time footprint low.
    For a real-world use case a full dataset training should be considered. Once the
    training has started the training logs are streamed to the notebook.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始训练本身。因此，我们调用训练函数，在 Amazon SageMaker 中启动一个短暂的训练作业。为此，我们需要传递一些参数给训练函数，例如模型
    ID、训练数据集路径、奖励模型路径以及一些超参数。请注意，演示中使用的超参数可以根据需要进行调整。为了保持资源和时间消耗较低，我们使用了 100 个训练样本。对于实际使用案例，应考虑使用完整数据集进行训练。一旦训练开始，训练日志将流式传输到笔记本中。
- en: '[PRE20]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Deployment
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署
- en: Finally, we want to test the tuned model. Therefore we will deploy it to a SageMaker
    endpoint. We start with importing required dependencies as well as setting up
    the SageMaker session and IAM.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望测试已调优的模型。因此，我们将其部署到 SageMaker 端点。我们首先导入所需的依赖项，并设置 SageMaker 会话和 IAM。
- en: For the deployment we are using the [SageMaker — Huggingface integration](https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/)
    with the [TGI containers](https://huggingface.co/docs/text-generation-inference/en/index).
    We define the instance type, image as well as model-related parameters like the
    base model, LoRA adapter, quantization and others.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于部署，我们使用了 [SageMaker — Huggingface 集成](https://aws.amazon.com/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/)
    和 [TGI 容器](https://huggingface.co/docs/text-generation-inference/en/index)。我们定义了实例类型、镜像以及与模型相关的参数，如基础模型、LoRA
    适配器、量化等。
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Then we deploy the model. Once the model has been deployed we can test the model
    inference with a prompt of our choice. Note that we are using the encode_dialogue
    function defined during data preprocessing to optimize the prompt for the Llama
    model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们部署模型。模型部署后，我们可以使用我们选择的提示进行模型推理测试。请注意，我们使用了在数据预处理过程中定义的 encode_dialogue 函数，以优化
    Llama 模型的提示。
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Cleanup
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理
- en: Finally, we cleanup the deployed endpoint and model entity to be responsible
    in resource usage.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们清理已部署的端点和模型实体，以便在资源使用上负责任。
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Cost
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: Both reward model adapter training and multi-adapter PPO training were executed
    on an *ml.g5.12xlarge* instance using a dataset of 100 randomly sampled rows from
    the respective training datasets. The average training time was approximately
    400 seconds for each step. As of November 2024, this instance type is priced at
    **$7.09/hour** in the us-east-1 region.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励模型适配器训练和多适配器 PPO 训练都在一个 *ml.g5.12xlarge* 实例上执行，使用了从各自训练数据集中随机抽取的 100 行数据。每个步骤的平均训练时间大约为
    400 秒。截至 2024 年 11 月，这种实例类型在 us-east-1 区域的价格为 **$7.09/小时**。
- en: Consequently, the end-to-end training cost for this RLHF implementation with
    multi-adapter PPO amounts to less than *($7.09 * 400s)/(3600s * 100)* **~ $0.0079
    per individual training sample** for each of the two training steps. This translates
    to **less than $0.015 per 1000 training tokens for the reward model training**
    and **less than $0.0039 per 1000 training tokens** for the multi-adapter PPO step.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，采用多适配器 PPO 的 RLHF 实现的端到端训练成本不到 *($7.09 * 400s)/(3600s * 100)* **~ 每个训练样本
    $0.0079**，每个训练步骤均适用。这意味着 **奖励模型训练的每千个训练令牌的成本不到 $0.015**，并且 **多适配器 PPO 步骤的每千个训练令牌的成本不到
    $0.0039**。
- en: For inference, the model is hosted on an *ml.g5.4xlarge* instance. As of November
    2024, this instance type is priced at **$2.03/hour** in the us-east-1 region.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，模型托管在一个 *ml.g5.4xlarge* 实例上。截至 2024 年 11 月，这种实例类型在 us-east-1 区域的价格为 **$2.03/小时**。
- en: Conclusion
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this blog post, we explored RLHF with multi-adapter PPO, a frugal approach
    to preference alignment for large language models. We covered the following key
    points:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客中，我们探讨了多适配器 PPO 与 RLHF 的结合，这是一种节俭的、大型语言模型偏好对齐方法。我们涵盖了以下关键点：
- en: The importance of preference alignment in boosting LLM performance and its role
    in the democratization of AI.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提升 LLM 性能中，偏好对齐的重要性及其在人工智能民主化中的作用。
- en: The principles of RLHF and its two-step process involving reward model training
    and PPO-based fine-tuning.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RLHF 的原理及其涉及奖励模型训练和基于 PPO 的微调的两步过程。
- en: Challenges associated with implementing RLHF, including computational resources
    and orchestration complexity.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现RLHF过程中面临的挑战，包括计算资源和协调复杂性。
- en: The multi-adapter PPO approach as a solution to reduce infrastructure and orchestration
    footprint.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多适配器PPO方法作为减少基础设施和协调开销的解决方案。
- en: A detailed, end-to-end implementation using HuggingFace frameworks and Amazon
    SageMaker, covering data preprocessing, reward model training, multi-adapter PPO
    training, and model deployment.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用HuggingFace框架和Amazon SageMaker的详细端到端实现，包括数据预处理、奖励模型训练、多适配器PPO训练和模型部署。
- en: This frugal approach to RLHF makes preference alignment more accessible to a
    broader range of practitioners, potentially accelerating the development and deployment
    of aligned AI systems.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种节俭的强化学习与人类反馈（RLHF）方法使得偏好对齐变得更加易于被更广泛的从业者所接受，可能加速对齐AI系统的开发和部署。
- en: By reducing computational requirements and simplifying the implementation process,
    multi-adapter PPO opens up new possibilities for fine-tuning language models to
    specific domains or user preferences.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少计算需求并简化实现过程，多适配器PPO为根据特定领域或用户偏好微调语言模型开辟了新的可能性。
- en: As the field of AI continues to evolve, techniques like this will play a crucial
    role in creating more efficient, effective, and aligned language models. I’d like
    to encourage readers to experiment with this approach, adapt it to their specific
    use cases, and share their success stories in building responsible and user-centric
    LLMs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI领域的不断发展，像这样的技术将在创建更高效、更有效且更对齐的语言模型方面发挥至关重要的作用。我鼓励读者尝试这种方法，将其适应到具体的使用场景中，并分享他们在构建负责任且以用户为中心的大型语言模型（LLMs）方面的成功经验。
- en: '***If you’re interested in learning more about LLM pre-training and alignment,
    I recommend checking out the*** [***AWS SkillBuilder course***](https://explore.skillbuilder.aws/learn/course/external/view/elearning/17556/building-language-models-on-aws)
    ***I recently published with my esteemed colleagues*** [***Anastasia***](https://anastasia-tzeveleka.medium.com/)
    ***and*** [***Gili***](https://medium.com/@gilinachum)***.***'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你有兴趣深入了解LLM的预训练和对齐，我推荐查看我和我的尊敬的同事们最近发布的*** [***AWS SkillBuilder课程***](https://explore.skillbuilder.aws/learn/course/external/view/elearning/17556/building-language-models-on-aws)
    ***，包括*** [***Anastasia***](https://anastasia-tzeveleka.medium.com/) ***和*** [***Gili***](https://medium.com/@gilinachum)***。***'
