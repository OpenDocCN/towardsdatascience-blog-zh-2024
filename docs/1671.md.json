["```py\n! pip install tfx\n```", "```py\n# Importing Libraries\n\nimport tensorflow as tf\n\nfrom tfx.components import CsvExampleGen\nfrom tfx.components import ExampleValidator\nfrom tfx.components import SchemaGen\nfrom tfx.v1.components import ImportSchemaGen\nfrom tfx.components import StatisticsGen\nfrom tfx.components import Transform\n\nfrom tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\nfrom google.protobuf.json_format import MessageToDict\n\nimport os\n```", "```py\n# Path to pipeline folder\n# All the generated components will be stored here\n\n_pipeline_root = '/content/tfx/pipeline/'\n\n# Path to training data\n# It can even contain multiple training data files\n_data_root = '/content/tfx/data/'\n```", "```py\n# Initializing the InteractiveContext \n# This will create an sqlite db for storing the metadata\n\ncontext = InteractiveContext(pipeline_root=_pipeline_root)\n```", "```py\n# Input CSV files \nexample_gen = CsvExampleGen(input_base=_data_root)\n```", "```py\n# Execute the component\n\ncontext.run(example_gen)\n```", "```py\n# View the generated artifacts\nartifact = example_gen.outputs['examples'].get()[0]\n\n# Display split names and uri\nprint(f'split names: {artifact.split_names}')\nprint(f'artifact uri: {artifact.uri}')\n```", "```py\n# Get the URI of the output artifact representing the training examples\ntrain_uri = os.path.join(artifact.uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n```", "```py\n# Helper function to get individual examples\ndef get_records(dataset, num_records):\n    '''Extracts records from the given dataset.\n    Args:\n        dataset (TFRecordDataset): dataset saved by ExampleGen\n        num_records (int): number of records to preview\n    '''\n\n    # initialize an empty list\n    records = []\n\n    # Use the `take()` method to specify how many records to get\n    for tfrecord in dataset.take(num_records):\n\n        # Get the numpy property of the tensor\n        serialized_example = tfrecord.numpy()\n\n        # Initialize a `tf.train.Example()` to read the serialized data\n        example = tf.train.Example()\n\n        # Read the example data (output is a protocol buffer message)\n        example.ParseFromString(serialized_example)\n\n        # convert the protocol bufffer message to a Python dictionary\n        example_dict = (MessageToDict(example))\n\n        # append to the records list\n        records.append(example_dict)\n\n    return records\n```", "```py\n# Get 3 records from the dataset\nsample_records = get_records(dataset, 3)\n\n# Print the output\npp.pprint(sample_records)\n```", "```py\n# Generate dataset statistics with StatisticsGen using the example_gen object\n\nstatistics_gen = StatisticsGen(\n    examples=example_gen.outputs['examples'])\n\n# Execute the component\ncontext.run(statistics_gen)\n```", "```py\n# Show the output statistics\n\ncontext.show(statistics_gen.outputs['statistics'])\n```", "```py\n# Generate schema using SchemaGen with the statistics_gen object\n\nschema_gen = SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    )\n\n# Run the component\ncontext.run(schema_gen)\n\n# Visualize the schema\n\ncontext.show(schema_gen.outputs['schema'])\n```", "```py\n# Adding a schema file manually \nschema_gen = ImportSchemaGen(schema_file=\"path_to_schema_file/schema.pbtxt\")\n```", "```py\n# Validate the examples using the ExampleValidator\n# Pass statistics_gen and schema_gen objects\n\nexample_validator = ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\n\n# Run the component.\ncontext.run(example_validator)\n```", "```py\n# Creating the file containing all constants that are to be used for this project\n\n_constants_module_file = 'constants.py'\n```", "```py\n%%writefile {_constants_module_file}\n\n# Features with string data types that will be converted to indices\nCATEGORICAL_FEATURE_KEYS = [ 'CryoSleep','Destination','HomePlanet','VIP']\n\n# Numerical features that are marked as continuous\nNUMERIC_FEATURE_KEYS = ['Age','FoodCourt','RoomService', 'ShoppingMall','Spa','VRDeck']\n\n# Feature that can be grouped into buckets\nBUCKET_FEATURE_KEYS = ['Age']\n\n# Number of buckets used by tf.transform for encoding each bucket feature.\nFEATURE_BUCKET_COUNT = {'Age': 4}\n\n# Feature that the model will predict\nLABEL_KEY = 'Transported'\n\n# Utility function for renaming the feature\ndef transformed_name(key):\n    return key + '_xf'\n```", "```py\n# Creating a file that contains all preprocessing code for the project\n\n_transform_module_file = 'transform.py'\n```", "```py\n%%writefile {_transform_module_file}\n\nimport tensorflow as tf\nimport tensorflow_transform as tft\n\nimport constants\n\n# Unpack the contents of the constants module\n_NUMERIC_FEATURE_KEYS = constants.NUMERIC_FEATURE_KEYS\n_CATEGORICAL_FEATURE_KEYS = constants.CATEGORICAL_FEATURE_KEYS\n_BUCKET_FEATURE_KEYS = constants.BUCKET_FEATURE_KEYS\n_FEATURE_BUCKET_COUNT = constants.FEATURE_BUCKET_COUNT\n_LABEL_KEY = constants.LABEL_KEY\n_transformed_name = constants.transformed_name\n\n# Define the transformations\ndef preprocessing_fn(inputs):\n\n    outputs = {}\n\n    # Scale these features to the range [0,1]\n    for key in _NUMERIC_FEATURE_KEYS:\n        outputs[_transformed_name(key)] = tft.scale_to_0_1(\n            inputs[key])\n\n    # Bucketize these features\n    for key in _BUCKET_FEATURE_KEYS:\n        outputs[_transformed_name(key)] = tft.bucketize(\n            inputs[key], _FEATURE_BUCKET_COUNT[key])\n\n    # Convert strings to indices in a vocabulary\n    for key in _CATEGORICAL_FEATURE_KEYS:\n        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n\n    # Convert the label strings to an index\n    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(inputs[_LABEL_KEY])\n\n    return outputs\n```", "```py\n# Ignore TF warning messages\ntf.get_logger().setLevel('ERROR')\n\n# Instantiate the Transform component with example_gen and schema_gen objects\n# Pass the path for transform file\n\ntransform = Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_transform_module_file))\n\n# Run the component\ncontext.run(transform)\n```"]