["```py\nimport numpy as np\n\nembeddings = {}\n\nwith open(f\"glove.6B/glove.6B.300d.txt\", \"r\") as f:\n    glove_content = f.read().split('\\n')\n\nfor i in range(len(glove_content)//10):\n    line = glove_content[i].strip().split(' ')\n    if line[0] == '':\n        continue\n    word = line[0]\n    embedding = np.array(list(map(float, line[1:])))\n    embeddings[word] = embedding\n\nprint(len(embeddings))\n```", "```py\ndef cos_sim(a, b):\n    return np.dot(a,b)/(np.linalg.norm(a) * np.linalg.norm(b))\ndef euc_dist(a, b):\n    return np.sum(np.square(a - b)) # no need for square root since we are just ranking distances\n```", "```py\ndef get_sims(to_word=None, to_e=None, metric=cos_sim):\n    # list all similarities to the word to_word, OR the embedding vector to_e\n    assert (to_word is not None) ^ (to_e is not None) # find similarity to a word or a vector, not both\n    sims = []\n    if to_e is None:\n        to_e = embeddings[to_word] # get the embedding for the word we are looking at\n    for word in embeddings:\n        if word == to_word:\n            continue\n        word_e = embeddings[word]\n        sim = metric(word_e, to_e)\n        sims.append((sim, word))\n    sims.sort()\n    return sims\n```", "```py\ndef display_sims(to_word=None, to_e=None, n=10, metric=cos_sim, reverse=False, label=None):\n    assert (to_word is not None) ^ (to_e is not None)\n    sims = get_sims(to_word=to_word, to_e=to_e, metric=metric)\n    display = lambda sim: f'{sim[1]}: {sim[0]:.5f}'\n    if label is None:\n        label = to_word.upper() if to_word is not None else ''\n    print(label) # a heading so we know what these similarities are for\n    if reverse:\n        sims.reverse()\n    for i, sim in enumerate(reversed(sims[-n:])):\n        print(i+1, display(sim))\n    return sims\n```", "```py\ndisplay_sims(to_word='red')\n# yellow, blue, pink, green, white, purple, black, colored, sox, bright\n```", "```py\ndisplay_sims(to_word='share')\n# shares, stock, profit, percent, shared, earnings, profits, price, gain, cents\n```", "```py\ndisplay_sims(to_word='cat')\n# dog, cats, pet, dogs, feline, monkey, horse, pets, rabbit, leopard\ndisplay_sims(to_word='frog')\n# toad, frogs, snake, monkey, squirrel, species, rodent, parrot, spider, rat\ndisplay_sims(to_word='queen')\n# elizabeth, princess, king, monarch, royal, majesty, victoria, throne, lady, crown\n```", "```py\ndisplay_sims(to_e=embeddings['man'] - embeddings['woman'] + embeddings['queen'], label='king-queen analogy')\n# queen, king, ii, majesty, monarch, prince...\n```", "```py\ndisplay_sims(to_e=embeddings['aunt'] - embeddings['woman'] + embeddings['man'], label='aunt-uncle analogy')\n# aunt, uncle, brother, grandfather, grandmother, cousin, uncles, grandpa, dad, father\n```", "```py\ndisplay_sims(to_e=embeddings['aunt'] - embeddings['woman'] + embeddings['man'], metric=euc_dist, reverse=True, label='aunt-uncle analogy')\n# uncle, aunt, grandfather, brother, cousin, grandmother, newphew, dad, grandpa, cousins\n```", "```py\nzero_vec = np.zeros_like(embeddings['the'])\ndisplay_sims(to_e=zero_vec, metric=euc_dist, label='largest magnitude')\n# republish, nonsubscribers, hushen, tael, www.star, stoxx, 202-383-7824, resend, non-families, 225-issue\ndisplay_sims(to_e=zero_vec, metric=euc_dist, reverse=True, label='smallest magnitude')\n# likewise, lastly, interestingly, ironically, incidentally, moreover, conversely, furthermore, aforementioned, wherein\n```", "```py\nimport matplotlib.pyplot as plt\n\ndef plot_magnitudes():\n    words = [w for w in embeddings]\n    magnitude = lambda word: np.linalg.norm(embeddings[word])\n    magnitudes = list(map(magnitude, words))\n    plt.hist(magnitudes, bins=40)\n    plt.show()\n\nplot_magnitudes()\n```", "```py\ngender_pairs = [('man', 'woman'), ('men', 'women'), ('brother', 'sister'), ('he', 'she'),\n                    ('uncle', 'aunt'), ('grandfather', 'grandmother'), ('boy', 'girl'),\n                    ('son', 'daughter')]\nmasc_v = zero_vec\nfor pair in gender_pairs:\n    masc_v += embeddings[pair[0]]\n    masc_v -= embeddings[pair[1]]\n```", "```py\ndisplay_sims(to_e=masc_v, metric=cos_sim, label='masculine vecs')\n# brother, colonel, himself, uncle, gen., nephew, brig., brothers, son, sir\ndisplay_sims(to_e=masc_v, metric=cos_sim, reverse=True, label='feminine vecs')\n# actress, herself, businesswoman, chairwoman, pregnant, she, her, sister, actresses, woman\n```", "```py\nprint(\"nurse - man\", cos_sim(embeddings['nurse'], embeddings['man'])) # 0.24\nprint(\"nurse - woman\", cos_sim(embeddings['nurse'], embeddings['woman'])) # 0.45\n```", "```py\nfrom sklearn.cluster import KMeans\n\ndef get_kmeans(n=300):\n    kmeans = KMeans(n_clusters=n, n_init=1)\n    X = np.array([embeddings[w] for w in embeddings])\n    kmeans.fit(X)\n    return kmeans\n\ndef display_kmeans(kmeans):\n    # print all clusters and 5 associated words for each\n    words = np.array([w for w in embeddings])\n    X = np.array([embeddings[w] for w in embeddings])\n    y = kmeans.predict(X) # get the cluster for each word\n    for cluster in range(kmeans.cluster_centers_.shape[0]):\n        print(f'KMeans {cluster}')\n        cluster_words = words[y == cluster] # get all words in each cluster\n        for i, w in enumerate(cluster_words[:5]):\n            print(i+1, w)\n\nkmeans = get_kmeans()\ndisplay_kmeans(kmeans)\n```", "```py\ndef get_kmeans_cluster(kmeans, word=None, cluster=None):\n    # given a word, find the cluster of that word. (or start with a cluster index.)\n    # then, get all words of that cluster.\n    assert (word is None) ^ (cluster is None)\n    if cluster is None:\n        cluster = kmeans.predict([embeddings[word]])[0]\n    words = np.array([w for w in embeddings])\n    X = np.array([embeddings[w] for w in embeddings])\n    y = kmeans.predict(X)\n    cluster_words = words[y == cluster]\n    return cluster, cluster_words\n\ndef display_cluster(kmeans, word):\n    cluster, cluster_words = get_kmeans_cluster(kmeans, word=word)\n    # print all words in the cluster\n    print(f\"Full KMeans ({word}, cluster {cluster})\")\n    for i, w in enumerate(cluster_words):\n        print(i+1, w)\n    # rank all clusters (excluding this one) by Euclidean distance of their centers from this cluster's center\n    distances = np.concatenate([kmeans.cluster_centers_[:cluster], kmeans.cluster_centers_[cluster+1:]], axis=0)\n    distances = np.sum(np.square(distances - kmeans.cluster_centers_[cluster]), axis=1)\n    nearest = np.argmin(distances, axis=0)\n    _, nearest_words = get_kmeans_cluster(kmeans, cluster=nearest)\n    print(f\"Nearest cluster: {nearest}\")\n    for i, w in enumerate(nearest_words[:5]):\n        print(i+1, w)\n    farthest = np.argmax(distances, axis=0)\n    print(f\"Farthest cluster: {farthest}\")\n    _, farthest_words = get_kmeans_cluster(kmeans, cluster=farthest)\n    for i, w in enumerate(farthest_words[:5]):\n        print(i+1, w)\n```", "```py\ndisplay_cluster(kmeans, 'animal')\n# species, fish, wild, dog, bear, males, birds...\ndisplay_cluster(kmeans, 'dog')\n# same as 'animal'\ndisplay_cluster(kmeans, 'birds')\n# same again\ndisplay_cluster(kmeans, 'bird')\n# spread, bird, flu, virus, tested, humans, outbreak, infected, sars....?\n```", "```py\nfrom sklearn.decomposition import PCA\n\ndef get_pca_vecs(n=10): # get the first 10 principal components\n    pca = PCA()\n    X = np.array([embeddings[w] for w in embeddings])\n    pca.fit(X)\n    principal_components = list(pca.components_[:n, :])\n    return pca, principal_components\n\npca, pca_vecs = get_pca_vecs()\nfor i, vec in enumerate(pca_vecs):\n    # display the words with the highest and lowest values for each principal component\n    display_sims(to_e=vec, metric=cos_sim, label=f'PCA {i+1}')\n    display_sims(to_e=vec, metric=cos_sim, label=f'PCA {i+1} negative', reverse=True)\n```", "```py\n PCA 9\n1 featuring: 0.38193\n2 hindi: 0.37217\n3 arabic: 0.36029\n4 sung: 0.35130\n5 che: 0.34819\n6 malaysian: 0.34474\n7 ka: 0.33820\n8 video: 0.33549\n9 bollywood: 0.33347\n10 counterpart: 0.33343\n    PCA 9 negative\n1 suffolk: -0.31999\n2 cumberland: -0.31697\n3 northumberland: -0.31449\n4 hampshire: -0.30857\n5 missouri: -0.30771\n6 calhoun: -0.30749\n7 erie: -0.30345\n8 massachusetts: -0.30133\n9 counties: -0.29710\n10 wyoming: -0.29613\n```", "```py\n PCA 3\n1 1.8: 0.57993\n2 1.6: 0.57851\n3 1.2: 0.57841\n4 1.4: 0.57294\n5 2.3: 0.57019\n6 2.6: 0.56993\n7 2.8: 0.56966\n8 3.7: 0.56660\n9 1.9: 0.56424\n10 2.2: 0.56063\n```", "```py\ndef plot_pca(pca_vecs, kmeans):\n    words = [w for w in embeddings]\n    x_vec = pca_vecs[0]\n    y_vec = pca_vecs[1]\n    X = np.array([np.dot(x_vec, embeddings[w]) for w in words])\n    Y = np.array([np.dot(y_vec, embeddings[w]) for w in words])\n    colors =  kmeans.predict([embeddings[w] for w in words])\n    plt.scatter(X, Y, c=colors, cmap='spring') # color by cluster\n    for i in np.random.choice(len(words), size=100, replace=False):\n        # annotate 100 randomly selected words on the graph\n        plt.annotate(words[i], (X[i], Y[i]), weight='bold')\n    plt.show()\n\nplot_pca(pca_vecs, kmeans)\n```", "```py\ndef display_covariance():\n    X = np.array([embeddings[w] for w in embeddings]).T # rows are variables (components), columns are observations (words)\n    cov = np.cov(X)\n    cov_range = np.maximum(np.max(cov), np.abs(np.min(cov))) # make sure the colorbar is balanced, with 0 in the middle\n    plt.imshow(cov, cmap='bwr', interpolation='nearest', vmin=-cov_range, vmax=cov_range)\n    plt.colorbar()\n    plt.show()\n\ndisplay_covariance()\n```", "```py\ne9 = np.zeros_like(zero_vec)\ne9[9] = 1.0\ne276 = np.zeros_like(zero_vec)\ne276[276] = 1.0\ndisplay_sims(to_e=e9, metric=cos_sim, label='e9')\n# grizzlies, supersonics, notables, posey, bobcats, wannabe, hoosiers...\ndisplay_sims(to_e=e276, metric=cos_sim, label='e276')\n# pehr, zetsche, steadied, 202-887-8307, bernice, goldie, edelman, kr...\n```", "```py\ndisplay_sims(to_e=e9, metric=cos_sim, label='e9', reverse=True)\n# therefore, that, it, which, government, because, moreover, fact, thus, very\ndisplay_sims(to_e=e276, metric=cos_sim, label='e276', reverse=True)\n# they, instead, those, hundreds, addition, dozens, others, dozen, only, outside\n```"]