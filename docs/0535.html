<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Attention for Vision Transformers, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Attention for Vision Transformers, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=collection_archive---------3-----------------------#2024-02-27">https://towardsdatascience.com/attention-for-vision-transformers-explained-70f83984c673?source=collection_archive---------3-----------------------#2024-02-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5db7" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Vision Transformers Explained Series</h2><div/><div><h2 id="2330" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">The Math and the Code Behind Attention Layers in Computer Vision</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sjcallis?source=post_page---byline--70f83984c673--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Skylar Jean Callis" class="l ep by dd de cx" src="../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--70f83984c673--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@sjcallis?source=post_page---byline--70f83984c673--------------------------------" rel="noopener follow">Skylar Jean Callis</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--70f83984c673--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">4</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1b53" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="nk">Since their introduction in 2017 with </em>Attention is All You Need<em class="nk">¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, </em>An Image is Worth 16x16 Words<em class="nk">² successfully adapted transformers for computer vision tasks. Since then, numerous transformer-based architectures have been proposed for computer vision.</em></p><p id="d1a5" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">This article takes an in-depth look to how an attention layer works in the context of computer vision. We’ll cover both single-headed and multi-headed attention. It includes open-source code for the attention layers, as well as conceptual explanations of underlying mathematics. The code uses the PyTorch Python package.</strong></p></div></div><div class="nl"><div class="ab cb"><div class="lr nm ls nn lt no cf np cg nq ci bh"><figure class="nu nv nw nx ny nl nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns nt"><img src="../Images/9fbe658f52c837298c19cc8f31b3f122.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*XLYx5OALyd914wu7"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Photo by <a class="af ol" href="https://unsplash.com/@mitchel3uo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mitchell Luo</a> on <a class="af ol" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="89c7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article is part of a collection examining the internal workings of Vision Transformers in depth. Each of these articles is also available as a Jupyter Notebook with executable code. The other articles in the series are:</p><ul class=""><li id="c8f3" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers, Explained</a><strong class="mq ga"><br/> </strong>→<a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook</a></li><li id="58d1" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673"><strong class="mq ga">Attention for Vision Transformers, Explained</strong></a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="e346" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/position-embeddings-for-vision-transformers-explained-a6f9add341d5">Position Embeddings for Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="9495" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa">Tokens-to-Token Vision Transformers, Explained</a><br/> → <a class="af ol" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="b96a" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub Repository for Vision Transformers, Explained Series</a></li></ul><h2 id="9841" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Table of Contents</h2><ul class=""><li id="4a2f" class="mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj om on oo bk"><a class="af ol" href="#6c66" rel="noopener ugc nofollow">Attention in General</a></li><li id="e934" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#1dee" rel="noopener ugc nofollow">Single Headed Attention</a></li><li id="0338" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#5945" rel="noopener ugc nofollow">Multi-Headed Attention</a></li><li id="614b" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk"><a class="af ol" href="#5276" rel="noopener ugc nofollow">Conclusion</a><br/> — <a class="af ol" href="#f44b" rel="noopener ugc nofollow">Further Reading</a><br/> — <a class="af ol" href="#8da6" rel="noopener ugc nofollow">Citations</a></li></ul><h1 id="6c66" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Attention in General</h1><p id="fc5c" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">For NLP applications, attention is often described as the relationship between words (tokens) in a sentence. In a computer vision application, attention looks at the relationships between patches (tokens) in an image.</p><p id="d6b2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">There are multiple ways to break an image down into a series of tokens. The original ViT² segments an image into <em class="nk">patches </em>that are then flattened into <em class="nk">tokens</em>; for a more in-depth explanation of this <em class="nk">patch tokenization</em> see the <a class="af ol" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8">Vision Transformers article</a>. The <em class="nk">Tokens-to-Token ViT³ </em>develops a more complicated method of creating tokens from an image; more about that methodology can be found in the <a class="af ol" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa">Tokens-To-Token ViT article</a>.</p><p id="197e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article will proceed though an attention layer assuming tokens as input. At the beginning of a transformer, the tokens will be representative of patches in the input image. However, deeper attention layers will compute attention on tokens that have been modified by preceding layers, removing the directness of the representation.</p><p id="932d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article examines dot-product (equivalently multiplicative) attention as defined in <em class="nk">Attention is All You Need</em>¹. This is the same attention mechanism used in derivative works such as <em class="nk">An Image is Worth 16x16 Words² </em>and <em class="nk">Tokens-to-Token ViT³. </em>The code is based on the publicly available GitHub code for <em class="nk">Tokens-to-Token ViT³ </em>with some modifications. Changes to the source code include, but are not limited to, consolidating the two attention modules into one and implementing multi-headed attention.</p><p id="5a61" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The attention module in full is shown below:</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="460b" class="qp ov fq qm b bg qq qr l qs qt">class Attention(nn.Module):<br/>    def __init__(self, <br/>                dim: int,<br/>                chan: int,<br/>                num_heads: int=1,<br/>                qkv_bias: bool=False,<br/>                qk_scale: NoneFloat=None):<br/><br/>        """ Attention Module<br/><br/>            Args:<br/>                dim (int): input size of a single token<br/>                chan (int): resulting size of a single token (channels)<br/>                num_heads(int): number of attention heads in MSA<br/>                qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                                    if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Define Constants<br/>        self.num_heads = num_heads<br/>        self.chan = chan<br/>        self.head_dim = self.chan // self.num_heads<br/>        self.scale = qk_scale or self.head_dim ** -0.5<br/>        assert self.chan % self.num_heads == 0, '"Chan" must be evenly divisible by "num_heads".'<br/><br/>        ## Define Layers<br/>        self.qkv = nn.Linear(dim, chan * 3, bias=qkv_bias)<br/>        #### Each token gets projected from starting length (dim) to channel length (chan) 3 times (for each Q, K, V)<br/>        self.proj = nn.Linear(chan, chan)<br/><br/>    def forward(self, x):<br/>        B, N, C = x.shape<br/>        ## Dimensions: (batch, num_tokens, token_len)<br/><br/>        ## Calcuate QKVs<br/>        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)<br/>        #### Dimensions: (3, batch, heads, num_tokens, chan/num_heads = head_dim)<br/>        q, k, v = qkv[0], qkv[1], qkv[2]<br/><br/>        ## Calculate Attention<br/>        attn = (q * self.scale) @ k.transpose(-2, -1)<br/>        attn = attn.softmax(dim=-1)<br/>        #### Dimensions: (batch, heads, num_tokens, num_tokens)<br/><br/>        ## Attention Layer<br/>        x = (attn @ v).transpose(1, 2).reshape(B, N, self.chan)<br/>        #### Dimensions: (batch, heads, num_tokens, chan)<br/><br/>        ## Projection Layers<br/>        x = self.proj(x)<br/><br/>        ## Skip Connection Layer<br/>        v = v.transpose(1, 2).reshape(B, N, self.chan)<br/>        x = v + x     <br/>        #### Because the original x has different size with current x, use v to do skip connection<br/><br/>        return x</span></pre><h1 id="1dee" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Single-Headed Attention</h1><p id="7885" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Starting with only one attention head, let’s step through each line of the forward pass, and look at some matrix diagrams as we go. We’re using 7∗7=49 as our starting token size, since that’s the starting token size in the T2T-ViT models.³ We’re using 64 channels because that’s also the T2T-ViT default³. We’re using 100 tokens because it’s a nice number. We’re using a batch size of 13 because it’s prime and won’t be confused for any of the other parameters.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="e705" class="qp ov fq qm b bg qq qr l qs qt"># Define an Input<br/>token_len = 7*7<br/>channels = 64<br/>num_tokens = 100<br/>batch = 13<br/>x = torch.rand(batch, num_tokens, token_len)<br/>B, N, C = x.shape<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/><br/># Define the Module<br/>A = Attention(dim=token_len, chan=channels, num_heads=1, qkv_bias=False, qk_scale=None)<br/>A.eval();</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="a512" class="qp ov fq qm b bg qq qr l qs qt">Input dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 49</span></pre><p id="b746" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">From <em class="nk">Attention is All You Need</em>¹, attention is defined in terms of <strong class="mq ga">Q</strong>ueries, <strong class="mq ga">K</strong>eys, and <strong class="mq ga">V</strong>alues matrices. Th first step is to calculate these through a learnable linear layer. The boolean <em class="nk">qkv_bias</em> term indicates if these linear layers have a bias term or not. This step also changes the length of the tokens from the input 49 to the <em class="nk">chan</em> parameter, which we set as 64.</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns qv"><img src="../Images/07584b12695625a9a4d15b294a069fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-ZAI_q5Rgk1wyRS5n_qNQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Generation of Queries, Keys, and Values for Single Headed Attention (image by author)</figcaption></figure><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="7a90" class="qp ov fq qm b bg qq qr l qs qt">qkv = A.qkv(x).reshape(B, N, 3, A.num_heads, A.head_dim).permute(2, 0, 3, 1, 4)<br/>q, k, v = qkv[0], qkv[1], qkv[2]<br/>print('Dimensions for Queries are\n\tbatchsize:', q.shape[0], '\n\tattention heads:', q.shape[1], '\n\tnumber of tokens:', q.shape[2], '\n\tnew length of tokens:', q.shape[3])<br/>print('See that the dimensions for queries, keys, and values are all the same:')<br/>print('\tShape of Q:', q.shape, '\n\tShape of K:', k.shape, '\n\tShape of V:', v.shape)</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="9478" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for Queries are<br/>   batchsize: 13 <br/>   attention heads: 1 <br/>   number of tokens: 100 <br/>   new length of tokens: 64<br/>See that the dimensions for queries, keys, and values are all the same:<br/>   Shape of Q: torch.Size([13, 1, 100, 64]) <br/>   Shape of K: torch.Size([13, 1, 100, 64]) <br/>   Shape of V: torch.Size([13, 1, 100, 64])</span></pre><p id="a9ff" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now, we can start to compute attention, which is defined in as:</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qx qy l"/></div></figure><p id="0425" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">where <em class="nk">Q, K, V</em>, are the queries, keys, and values, respectively; and dₖ is the dimension of the keys, which is equal to the length of the key tokens and equal to the <em class="nk">chan</em> length.</p><p id="de0f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We’re going to go through this equation as it is implemented in the code. We’ll call the intermediate matrices <strong class="mq ga">A</strong>ttn.</p><p id="d035" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The first step is to compute:</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qx qy l"/></div></figure><p id="d360" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In the code, we set</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qz qy l"/></div></figure><p id="a47f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">By default,</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="ra qy l"/></div></figure><p id="492e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, the user can specify an alternative scale value as a hyperparameter.</p><p id="4a4c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The matrix multiplication <em class="nk">Q·Kᵀ</em> in the numerator looks like this:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rb"><img src="../Images/2326047b0af71d16bbe03baff4720488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYXlW2QMU1zh6OA-b-ueZQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx"><em class="rc">Q·Kᵀ Matrix Multiplication (image by author)</em></figcaption></figure><p id="da83" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">All of that together in code looks like:</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="79d2" class="qp ov fq qm b bg qq qr l qs qt">attn = (q * A.scale) @ k.transpose(-2, -1)<br/>print('Dimensions for Attn are\n\tbatchsize:', attn.shape[0], '\n\tattention heads:', attn.shape[1], '\n\tnumber of tokens:', attn.shape[2], '\n\tnumber of tokens:', attn.shape[3])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="0f9d" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for Attn are<br/>   batchsize: 13 <br/>   attention heads: 1 <br/>   number of tokens: 100 <br/>   number of tokens: 100</span></pre><p id="6c39" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Next, we calculate the softmax of <em class="nk">A</em>, which doesn’t change it’s shape.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="cbfd" class="qp ov fq qm b bg qq qr l qs qt">attn = attn.softmax(dim=-1)<br/>print('Dimensions for Attn are\n\tbatchsize:', attn.shape[0], '\n\tattention heads:', attn.shape[1], '\n\tnumber of tokens:', attn.shape[2], '\n\tnumber of tokens:', attn.shape[3])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="8ce0" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for Attn are<br/>   batchsize: 13 <br/>   attention heads: 1 <br/>   number of tokens: 100 <br/>   number of tokens: 100</span></pre><p id="006a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Finally, we compute <em class="nk">A·V=x, </em>which looks like:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rd"><img src="../Images/05349999ae0551af69e684b862aca2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3SgzSp5BrA60vQ4WpbCqNQ.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx"><em class="rc">A·V Matrix Multiplication (image by author)</em></figcaption></figure><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="3e4a" class="qp ov fq qm b bg qq qr l qs qt">x = attn @ v<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tattention heads:', x.shape[1], '\n\tnumber of tokens:', x.shape[2], '\n\tlength of tokens:', x.shape[3])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="182c" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   attention heads: 1 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="4b6a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The output <em class="nk">x</em> is reshaped to remove the attention head dimension.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="6194" class="qp ov fq qm b bg qq qr l qs qt">x = x.transpose(1, 2).reshape(B, N, A.chan)<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="591c" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="05c0" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We then feed <em class="nk">x</em> through a learnable linear layer that does not change it’s shape.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="419a" class="qp ov fq qm b bg qq qr l qs qt">x = A.proj(x)<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="1230" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="6986" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Lastly, we implement a skip connection. Since the current shape of <em class="nk">x</em> is different from the input shape of <em class="nk">x</em>, we use <em class="nk">V</em> for the skip connection. We do flatten <em class="nk">V</em> in the attention head dimension.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="daf9" class="qp ov fq qm b bg qq qr l qs qt">orig_shape = (batch, num_tokens, token_len)<br/>curr_shape = (x.shape[0], x.shape[1], x.shape[2])<br/>v = v.transpose(1, 2).reshape(B, N, A.chan)<br/>v_shape = (v.shape[0], v.shape[1], v.shape[2])<br/>print('Original shape of input x:', orig_shape)<br/>print('Current shape of x:', curr_shape)<br/>print('Shape of V:', v_shape)<br/>x = v + x     <br/>print('After skip connection, dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="85dd" class="qp ov fq qm b bg qq qr l qs qt">Original shape of input x: (13, 100, 49)<br/>Current shape of x: (13, 100, 64)<br/>Shape of V: (13, 100, 64)<br/>After skip connection, dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="9b65" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">That completes the attention layer!</p><h1 id="5945" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Multi-Headed Attention</h1><p id="f97f" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">Now that we’ve looked at single headed attention, we can expand to multi-headed attention. In the context of computer vision, this is often called <strong class="mq ga">M</strong>ulti-headed <strong class="mq ga">S</strong>elf <strong class="mq ga">A</strong>ttention (MSA). This section isn’t going to go through all the steps in as much detail; instead, we’ll focus on the places where the matrix shapes differ.</p><p id="156b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Same as for a single attention head, we’re using 7∗7=49 as our starting token size and 64 channels because that’s the T2T-ViT default³. We’re using 100 tokens because it’s a nice number. We’re using a batch size of 13 because it’s prime and won’t be confused for any of the other parameters.</p><p id="7e08" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The number of attention heads must evenly divide the number of channels, so for this example we’ll use 4 attention heads.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="5e8b" class="qp ov fq qm b bg qq qr l qs qt"># Define an Input<br/>token_len = 7*7<br/>channels = 64<br/>num_tokens = 100<br/>batch = 13<br/>num_heads = 4<br/>x = torch.rand(batch, num_tokens, token_len)<br/>B, N, C = x.shape<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])<br/><br/># Define the Module<br/>MSA = Attention(dim=token_len, chan=channels, num_heads=num_heads, qkv_bias=False, qk_scale=None)<br/>MSA.eval();</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="b09b" class="qp ov fq qm b bg qq qr l qs qt">Input dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   token size: 49</span></pre><p id="4b21" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The process to computer the <strong class="mq ga">Q</strong>ueries, <strong class="mq ga">K</strong>eys, and <strong class="mq ga">V</strong>alues remains the same as in single-headed attention. However, you can see that the new length of the tokens is <em class="nk">chan</em>/<em class="nk">num_heads</em>. The total size of the <em class="nk">Q</em>, <em class="nk">K</em>, and <em class="nk">V</em> matrices have not changed; their contents are just distributed across the head dimension. You can think abut this as segmenting the single headed matrix for the multiple heads:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns re"><img src="../Images/1f6f3d811b9d760cc180775d581d057e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*02EhPs9iYYgwDaarAkp8KQ.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Multi-Headed Attention Segmentation (image by author)</figcaption></figure><p id="2241" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We’ll denote the submatrices as Qₕᵢ for <strong class="mq ga"><em class="nk">Q</em></strong><em class="nk">uery </em><strong class="mq ga"><em class="nk">h</em></strong><em class="nk">ead </em><strong class="mq ga"><em class="nk">i</em></strong>.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="6b56" class="qp ov fq qm b bg qq qr l qs qt">qkv = MSA.qkv(x).reshape(B, N, 3, MSA.num_heads, MSA.head_dim).permute(2, 0, 3, 1, 4)<br/>q, k, v = qkv[0], qkv[1], qkv[2]<br/>print('Head Dimension = chan / num_heads =', MSA.chan, '/', MSA.num_heads, '=', MSA.head_dim)<br/>print('Dimensions for Queries are\n\tbatchsize:', q.shape[0], '\n\tattention heads:', q.shape[1], '\n\tnumber of tokens:', q.shape[2], '\n\tnew length of tokens:', q.shape[3])<br/>print('See that the dimensions for queries, keys, and values are all the same:')<br/>print('\tShape of Q:', q.shape, '\n\tShape of K:', k.shape, '\n\tShape of V:', v.shape)</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="8634" class="qp ov fq qm b bg qq qr l qs qt">Head Dimension = chan / num_heads = 64 / 4 = 16<br/>Dimensions for Queries are<br/>   batchsize: 13 <br/>   attention heads: 4 <br/>   number of tokens: 100 <br/>   new length of tokens: 16<br/>See that the dimensions for queries, keys, and values are all the same:<br/>   Shape of Q: torch.Size([13, 4, 100, 16]) <br/>   Shape of K: torch.Size([13, 4, 100, 16]) <br/>   Shape of V: torch.Size([13, 4, 100, 16])</span></pre><p id="87e9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The next step is to compute</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qx qy l"/></div></figure><p id="08c8" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">for every head <em class="nk">i</em>. In this context, the length of the keys is</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qx qy l"/></div></figure><p id="d800" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">As in single headed attention, we use the default</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qz qy l"/></div></figure><p id="57f3" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">though the user can specify an alternative scale value as a hyperparameter.</p><p id="bc57" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We end this step with <em class="nk">num_heads</em> = 4 different <strong class="mq ga">A</strong>ttn matrices, which looks like:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns rf"><img src="../Images/8338a991901e1c15a1a239544b2fb2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*dWCDA2wSOjGfhq-82gIc3g.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx"><em class="rc">Q·Kᵀ Matrix Multiplication </em>for MSA (image by author)</figcaption></figure><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="bf01" class="qp ov fq qm b bg qq qr l qs qt">attn = (q * MSA.scale) @ k.transpose(-2, -1)<br/>print('Dimensions for Attn are\n\tbatchsize:', attn.shape[0], '\n\tattention heads:', attn.shape[1], '\n\tnumber of tokens:', attn.shape[2], '\n\tnumber of tokens:', attn.shape[3])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="4c85" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for Attn are<br/>   batchsize: 13 <br/>   attention heads: 4 <br/>   number of tokens: 100 <br/>   number of tokens: 100</span></pre><p id="e958" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Next we calculate the softmax of <em class="nk">A</em>, which doesn’t change it’s shape.</p><p id="80d0" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Then, we can compute</p><figure class="nu nv nw nx ny nl"><div class="qw it l ed"><div class="qx qy l"/></div></figure><p id="d588" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This is similarly distributed across the multiple attention heads:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div role="button" tabindex="0" class="ob oc ed od bh oe"><div class="nr ns rg"><img src="../Images/9e3e9159dbd9fd222d4c6292227c929f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNlfj-0DULGZpkjSX9Eyxw.png"/></div></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx"><em class="rc">A·V Matrix Multiplication </em>for MSA (image by author)</figcaption></figure><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="22a0" class="qp ov fq qm b bg qq qr l qs qt">attn = attn.softmax(dim=-1)<br/><br/>x = attn @ v<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tattention heads:', x.shape[1], '\n\tnumber of tokens:', x.shape[2], '\n\tlength of tokens:', x.shape[3])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="a6c2" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   attention heads: 4 <br/>   number of tokens: 100 <br/>   length of tokens: 16</span></pre><p id="854a" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now we concatenate all of the xₕᵢ’s together through some reshaping. This is the inverse operation from the first step:</p><figure class="nu nv nw nx ny nl nr ns paragraph-image"><div class="nr ns re"><img src="../Images/1f6f3d811b9d760cc180775d581d057e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*02EhPs9iYYgwDaarAkp8KQ.png"/></div><figcaption class="og oh oi nr ns oj ok bf b bg z dx">Multi-Headed Attention Segmentation (image by author)</figcaption></figure><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="fe89" class="qp ov fq qm b bg qq qr l qs qt">x = x.transpose(1, 2).reshape(B, N, MSA.chan)<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="919a" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="a072" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Now that we’ve concatenated all of the heads back together, the rest of the Attention module remains unchanged. For the skip connection, we still use <em class="nk">V</em>, but we have to reshape it to remove the head dimension.</p><pre class="nu nv nw nx ny ql qm qn bp qo bb bk"><span id="6964" class="qp ov fq qm b bg qq qr l qs qt">x = MSA.proj(x)<br/>print('Dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])<br/><br/>orig_shape = (batch, num_tokens, token_len)<br/>curr_shape = (x.shape[0], x.shape[1], x.shape[2])<br/>v = v.transpose(1, 2).reshape(B, N, A.chan)<br/>v_shape = (v.shape[0], v.shape[1], v.shape[2])<br/>print('Original shape of input x:', orig_shape)<br/>print('Current shape of x:', curr_shape)<br/>print('Shape of V:', v_shape)<br/>x = v + x     <br/>print('After skip connection, dimensions for x are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\tlength of tokens:', x.shape[2])</span></pre><pre class="qu ql qm qn bp qo bb bk"><span id="ed57" class="qp ov fq qm b bg qq qr l qs qt">Dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64<br/>Original shape of input x: (13, 100, 49)<br/>Current shape of x: (13, 100, 64)<br/>Shape of V: (13, 100, 64)<br/>After skip connection, dimensions for x are<br/>   batchsize: 13 <br/>   number of tokens: 100 <br/>   length of tokens: 64</span></pre><p id="133b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">And that concludes multi-headed attention!</p><h1 id="5276" class="pt ov fq bf ow pu pv gv pa pw px gy pe py pz qa qb qc qd qe qf qg qh qi qj qk bk">Conclusion</h1><p id="2b70" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">We’ve now walked through every step of an attention layer as implemented for vision transformers. The learnable weights in an attention layer are found in the first projection from tokens to queries, keys, and values and in the final projection. The majority of the attention layer is deterministic matrix multiplication. However, the linear layers can contain large numbers of weights when long tokens are used. The number of weights in the QKV projection layer are equal to <em class="nk">input_token_len</em>∗<em class="nk">chan</em>∗<em class="nk">3, </em>and the number of weights in the final projection layer are equal to <em class="nk">chan²</em>.</p><p id="f57e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To use the attention layers, you can create custom attention layers (as done here!), or use attention layers included in machine learning packages. If you want to use attention layers as defined here, they can be found in the <a class="af ol" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> for this article series. PyTorch also has <code class="cx rh ri rj qm b">torch.nn.MultiheadedAttention()</code>⁴ layers, which compute attention as defined above. Happy attending!</p><p id="2fa9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876. The associated code was approved for a BSD-3 open source license under O#4693.</p><h2 id="f44b" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Further Reading</h2><p id="fff5" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">To learn more about attention layers in NLP contexts, see</p><ul class=""><li id="b86f" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk">Transformers Explained Visually Part 1 Overview of Functionality: <a class="af ol" rel="noopener" target="_blank" href="/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452">https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452</a></li><li id="c9f8" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk">Transformers Explained Visually Part 2 How it Works, Step by Step: <a class="af ol" rel="noopener" target="_blank" href="/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34">https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34</a></li><li id="8c51" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk">Transformers Explained Visually Part 3 Multi-Headed Attention Deep Dive: <a class="af ol" rel="noopener" target="_blank" href="/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853</a></li><li id="108a" class="mo mp fq mq b gt op ms mt gw oq mv mw mx or mz na nb os nd ne nf ot nh ni nj om on oo bk">Visual Guide to Transformer Neural Networks Multi-Head &amp; Self Attention Video: <a class="af ol" href="https://www.youtube.com/watch?v=mMa2PmYJlCo" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=mMa2PmYJlCo</a></li></ul><p id="b94c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For a video lecture broadly about vision transformers (with relevant chapters noted), see</p><ul class=""><li id="536f" class="mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj om on oo bk">Vision Transformer and its Applications: <a class="af ol" href="https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP</a> <br/> — Human Visual Attention: 4:31 — 5:18 (<a class="af ol" href="https://youtu.be/hPb6A92LROc?t=271&amp;si=VMx2lM9lvW-oKcW_" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?t=271&amp;si=VMx2lM9lvW-oKcW_</a>)<br/> — Attention as a Dot Product: 5:18–6:14 <a class="af ol" href="https://youtu.be/hPb6A92LROc?t=318&amp;si=pF2SFp2XXjK8AWsL" rel="noopener ugc nofollow" target="_blank">(https://youtu.be/hPb6A92LROc?t=318&amp;si=pF2SFp2XXjK8AWsL</a>)<br/> — Description of Attention formula: 16:13–17:52 (<a class="af ol" href="https://youtu.be/hPb6A92LROc?si=toAgKQCOh9zGCR-c&amp;t=973" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?si=toAgKQCOh9zGCR-c&amp;t=973</a>)<br/> — Why Multi-Head Self-Attention: 19:44–19:58 (<a class="af ol" href="https://youtu.be/hPb6A92LROc?t=1184&amp;si=Sy1e149ukt99DoRf" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?t=1184&amp;si=Sy1e149ukt99DoRf</a>)</li></ul><h2 id="8da6" class="ou ov fq bf ow ox oy oz pa pb pc pd pe mx pf pg ph nb pi pj pk nf pl pm pn fw bk">Citations</h2><p id="17e6" class="pw-post-body-paragraph mo mp fq mq b gt po ms mt gw pp mv mw mx pq mz na nb pr nd ne nf ps nh ni nj fj bk">[1] Vaswani et al (2017).<em class="nk"> Attention Is All You Need. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.1706.03762" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1706.03762</a></p><p id="b4ca" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[2] Dosovitskiy et al (2020). <em class="nk">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. </em><a class="af ol" href="https://doi.org/10.48550/arXiv.2010.11929" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2010.11929</a></p><p id="4b33" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[3] Yuan et al (2021). <em class="nk">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>. <a class="af ol" href="https://doi.org/10.48550/arXiv.2101.11986" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2101.11986</a><br/> → GitHub code: <a class="af ol" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">https://github.com/yitu-opensource/T2T-ViT</a></p><p id="aecc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">[4] PyTorch. <em class="nk">Multiheaded Attention. </em><a class="af ol" href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></p></div></div></div></div>    
</body>
</html>