- en: 'TIME-MOE: Billion-Scale Time Series Foundation Model with Mixture-of-Experts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31](https://towardsdatascience.com/time-moe-billion-scale-time-series-foundation-model-with-mixture-of-experts-7d165028124a?source=collection_archive---------4-----------------------#2024-10-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And open-source as well!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--7d165028124a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7d165028124a--------------------------------)
    ·8 min read·Oct 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0433c89d3015ad05f8689369087cd24d.png)'
  prefs: []
  type: TYPE_IMG
- en: '*A top-level view of**Time-MOE (*[*Image Source*](https://arxiv.org/pdf/2409.16040)*)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Mixture-of-Experts (MOE) architecture has surged in popularity with the
    rise of large language models (LLMs).**'
  prefs: []
  type: TYPE_NORMAL
- en: As time-series models adopt cutting-edge techniques, **Mixture-of-Experts**
    has naturally found its place in the time-series foundation space.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article discusses **Time-MOE**, a time-series foundation model that uses
    MOE to improve forecasting accuracy while reducing computational costs. Key contributions
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Time-300B Dataset**: The largest open time-series dataset, with 300 billion
    time points across 9 domains, and a scalable data-cleaning pipeline.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scaling Laws for Time Series**: Insights into how scaling laws affect large
    time-series models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time-MOE architecture**: A family of open-source time-series models leveraging
    MOE to enhance performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s get started
  prefs: []
  type: TYPE_NORMAL
- en: '*✅* Find the **hands-on project** for ***Time-MOE*** in the [**AI Projects
    folder**](https://aihorizonforecast.substack.com/p/ai-projects), along with other
    cool projects!'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Enter Time-MOE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time-MOE is a 2.4B parameter open-source time-series foundation model using
    **Mixture-of-Experts (MOE)** forzero-shot forecasting
  prefs: []
  type: TYPE_NORMAL
