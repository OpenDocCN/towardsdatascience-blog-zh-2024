<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introduction to the Finite Normal Mixtures in Regression with R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introduction to the Finite Normal Mixtures in Regression with R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-the-finite-normal-mixtures-in-regression-with-6a884810a692?source=collection_archive---------7-----------------------#2024-11-15">https://towardsdatascience.com/introduction-to-the-finite-normal-mixtures-in-regression-with-6a884810a692?source=collection_archive---------7-----------------------#2024-11-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="91f0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to make linear regression flexible enough for non-linear data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lukaszgatarek81?source=post_page---byline--6a884810a692--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lukasz Gatarek" class="l ep by dd de cx" src="../Images/a44ec84d3c30e6dd5ad0735698d46a52.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*pc9cnZr6-yLSKj7nd-u2iw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6a884810a692--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lukaszgatarek81?source=post_page---byline--6a884810a692--------------------------------" rel="noopener follow">Lukasz Gatarek</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6a884810a692--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="07e9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The linear regression is usually considered not flexible enough to tackle the nonlinear data. From theoretical viewpoint it is not capable to dealing with them. However, we can make it work for us with any dataset by using finite normal mixtures in a regression model. This way it becomes a very powerful machine learning tool which can be applied to virtually any dataset, even highly non-normal with non-linear dependencies across the variables.</p><p id="0562" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">What makes this approach particularly interesting comes with interpretability. Despite an extremely high level of flexibility all the detected relations can be directly interpreted. The model is as general as neural network, still it does not become a black-box. You can read the relations and understand the impact of individual variables.</p><p id="5059" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this post, we demonstrate how to simulate a finite mixture model for regression using Markov Chain Monte Carlo (MCMC) sampling. We will generate data with multiple components (groups) and fit a mixture model to recover these components using Bayesian inference. This process involves regression models and mixture models, combining them with MCMC techniques for parameter estimation.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf ng"><img src="../Images/36ff117a1f0b195a0c5ab4c72153b45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*bB37tG2PcuEy2rbaQlFUDw.png"/></div><figcaption class="no np nq ne nf nr ns bf b bg z dx">Data simulated as a mixtures of three linear regressions</figcaption></figure><h1 id="42df" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Loading Required Libraries</h1><p id="1585" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">We begin by loading the necessary libraries to work with regression models, MCMC, and multivariate distributions</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="ff52" class="oy nu fq ov b bg oz pa l pb pc"># Loading the required libraries for various functions<br/>library("pscl")         # For pscl specific functions, like regression models<br/>library("MCMCpack")     # For MCMC sampling functions, including posterior distributions<br/>library(mvtnorm)        # For multivariate normal distribution functio</span></pre><ul class=""><li id="aa44" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">pscl</strong>: Used for various statistical functions like regression models.</li><li id="c599" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">MCMCpack</strong>: Contains functions for Bayesian inference, particularly MCMC sampling.</li><li id="7de7" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">mvtnorm</strong>: Provides tools for working with multivariate normal distributions.</li></ul><h1 id="1d2a" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Data Generation</h1><p id="84e6" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">We simulate a dataset where each observation belongs to one of several groups (components of the mixture model), and the response variable is generated using a regression model with random coefficients.</p><p id="31e8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We consider a general setup for a regression model using G Normal mixture components.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf pl"><img src="../Images/3ed53e8272456985255a39c1e2290cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*7EQtGxYfanML9eWZ4j4LsQ.png"/></div></figure><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="b7d0" class="oy nu fq ov b bg oz pa l pb pc">## Generate the observations<br/># Set the length of the time series (number of observations per group)<br/>N &lt;- 1000<br/># Set the number of simulations (iterations of the MCMC process)<br/>nSim &lt;- 200<br/># Set the number of components in the mixture model (G is the number of groups)<br/>G &lt;- 3</span></pre><ul class=""><li id="080e" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">N</strong>: The number of observations per group.</li><li id="b820" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">nSim</strong>: The number of MCMC iterations.</li><li id="94b2" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">G</strong>: The number of components (groups) in our mixture model.</li></ul><h2 id="8787" class="pm nu fq bf nv pn po pp ny pq pr ps ob mr pt pu pv mv pw px py mz pz qa qb qc bk">Simulating Data</h2><p id="2f9b" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">Each group is modeled using a univariate regression model, where the explanatory variables (X) and the response variable (y) are simulated from normal distributions. The <code class="cx qd qe qf ov b">betas</code> represent the regression coefficients for each group, and <code class="cx qd qe qf ov b">sigmas</code> represent the variance for each group.</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="ebc3" class="oy nu fq ov b bg oz pa l pb pc"># Set the values for the regression coefficients (betas) for each group<br/>betas &lt;- 1:sum(dimG) * 2.5  # Generating sequential betas with a multiplier of 2.5<br/># Define the variance (sigma) for each component (group) in the mixture<br/>sigmas &lt;- rep(1, G) / 1  # Set variance to 1 for each component, with a fixed divisor of 1</span></pre><ul class=""><li id="5ef1" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">betas</strong>: These are the regression coefficients. Each group’s coefficient is sequentially assigned.</li><li id="1754" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">sigmas</strong>: Represents the variance for each group in the mixture model.</li></ul><p id="3256" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this model we allow each mixture component to possess its own variance paraameter and set of regression parameters.</p><h2 id="b390" class="pm nu fq bf nv pn po pp ny pq pr ps ob mr pt pu pv mv pw px py mz pz qa qb qc bk">Group Assignment and Mixing</h2><p id="4d49" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">We then simulate the group assignment of each observation using a random assignment and mix the data for all components.</p><p id="0f4b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We augment the model with a set of component label vectors for</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qg"><img src="../Images/eb047a04981f3687cebbe80278b29314.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*0JeagbrBM-cEooxcNku9Kw.png"/></div></figure><p id="6e50" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">where</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qh"><img src="../Images/9756b038c770fa78ab6fd85e875d98dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*NElEwXWC0AE58dLWM1z_-A.png"/></div></figure><p id="24c3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and thus <em class="qi">z_gi=1</em> implies that the <em class="qi">i-</em>th individual is drawn from the <em class="qi">g-</em>th component of the mixture.</p><p id="79ae" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This random assignment forms the <code class="cx qd qe qf ov b">z_original</code> vector, representing the true group each observation belongs to.</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="6253" class="oy nu fq ov b bg oz pa l pb pc"># Initialize the original group assignments (z_original)<br/>z_original &lt;- matrix(NA, N * G, 1)<br/># Repeat each group label N times (assign labels to each observation per group)<br/>z_original &lt;- rep(1:G, rep(N, G))<br/># Resample the data rows by random order<br/>sampled_order &lt;- sample(nrow(data))<br/># Apply the resampled order to the data<br/>data &lt;- data[sampled_order,]</span></pre><h1 id="02cf" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Bayesian Inference: Priors and Initialization</h1><p id="65ed" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">We set prior distributions for the regression coefficients and variances. These priors will guide our Bayesian estimation.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qj"><img src="../Images/66c79c39852ac0490d55a7958de9afa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*-QyRfZLqC4iFSNIvRZRMRQ.png"/></div></figure><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="263b" class="oy nu fq ov b bg oz pa l pb pc">## Define Priors for Bayesian estimation# Define the prior mean (muBeta) for the regression coefficients<br/>muBeta &lt;- matrix(0, G, 1)# Define the prior variance (VBeta) for the regression coefficients<br/>VBeta &lt;- 100 * diag(G)  # Large variance (100) as a prior for the beta coefficients# Prior for the sigma parameters (variance of each component)<br/>ag &lt;- 3  # Shape parameter<br/>bg &lt;- 1/2  # Rate parameter for the prior on sigma<br/>shSigma &lt;- ag<br/>raSigma &lt;- bg^(-1)</span></pre><ul class=""><li id="a3d2" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">muBeta</strong>: The prior mean for the regression coefficients. We set it to 0 for all components.</li><li id="73d4" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">VBeta</strong>: The prior variance, which is large (100) to allow flexibility in the coefficients.</li><li id="5f6b" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">shSigma</strong> and <strong class="mk fr">raSigma</strong>: Shape and rate parameters for the prior on the variance (sigma) of each group.</li></ul><p id="eb37" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For the component indicators and component probabilities we consider following prior assignment</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qk"><img src="../Images/32c364a60b362d26cc314efdca7072ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*DKuTxxVTX7PCGpRQeOHS7w.png"/></div></figure><p id="381d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The multinomial prior M is the multivariate generalizations of the binomial, and the Dirichlet prior D is a multivariate generalization of the beta distribution.</p><h1 id="e6fa" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">MCMC Initialization</h1><p id="30a9" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">In this section, we initialize the MCMC process by setting up matrices to store the samples of the regression coefficients, variances, and mixing proportions.</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="c363" class="oy nu fq ov b bg oz pa l pb pc">## Initialize MCMC sampling# Initialize matrix to store the samples for beta<br/>mBeta &lt;- matrix(NA, nSim, G)# Assign the first value of beta using a random normal distribution<br/>for (g in 1:G) {<br/>  mBeta[1, g] &lt;- rnorm(1, muBeta[g, 1], VBeta[g, g])<br/>}# Initialize the sigma^2 values (variance for each component)<br/>mSigma2 &lt;- matrix(NA, nSim, G)<br/>mSigma2[1, ] &lt;- rigamma(1, shSigma, raSigma)# Initialize the mixing proportions (pi), using a Dirichlet distribution<br/>mPi &lt;- matrix(NA, nSim, G)<br/>alphaPrior &lt;- rep(N/G, G)  # Prior for the mixing proportions, uniform across groups<br/>mPi[1, ] &lt;- rdirichlet(1, alphaPrior)</span></pre><ul class=""><li id="8812" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">mBeta</strong>: Matrix to store samples of the regression coefficients.</li><li id="19a6" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">mSigma2</strong>: Matrix to store the variances (sigma squared) for each component.</li><li id="e12c" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">mPi</strong>: Matrix to store the mixing proportions, initialized using a Dirichlet distribution.</li></ul><h1 id="e841" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">MCMC Sampling: Posterior Updates</h1><p id="2f20" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">If we condition on the values of the component indicator variables z, the conditional likelihood can be expressed as</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="qm qn ed qo bh qp"><div class="ne nf ql"><img src="../Images/a03f350ff3f9294c59770a83f1abc56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xr6VteCFT0FvL-J9EDj--Q.png"/></div></div></figure><p id="8e45" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the MCMC sampling loop, we update the group assignments (<code class="cx qd qe qf ov b">z</code>), regression coefficients (<code class="cx qd qe qf ov b">beta</code>), and variances (<code class="cx qd qe qf ov b">sigma</code>) based on the posterior distributions. The likelihood of each group assignment is calculated, and the group with the highest posterior probability is selected.</p><p id="3566" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The following complete posterior conditionals can be obtained:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qq"><img src="../Images/a7d1f5bcc4952cfa4ab1f0d36e1478a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*7h9QN0edlWOkMQOcji1dAA.png"/></div></figure><p id="06f2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">where</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qr"><img src="../Images/aaf3ceb5f34ba16009e704d76e351267.png" data-original-src="https://miro.medium.com/v2/resize:fit:92/format:webp/1*Fw6jVVJd4hGiTFY25yO7wA.png"/></div></figure><p id="aad7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">denotes all the parameters in our posterior other than <em class="qi">x</em>.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="qm qn ed qo bh qp"><div class="ne nf qs"><img src="../Images/b9f326d43e2c0dc83726a042e1f3adff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5BR0NhR6DD6Yc8p5QbOIg.png"/></div></div></figure><p id="0752" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and where <em class="qi">n_g</em> denotes the number of observations in the <em class="qi">g</em>-th component of the mixture.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="qm qn ed qo bh qp"><div class="ne nf qt"><img src="../Images/220beb5af0688c95c858473ae05d9b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gf48ETMXywEmQ8nu4khbLw.png"/></div></div></figure><p id="f4a6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf qu"><img src="../Images/f9b9da480f7230033270068a2921f3d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*0HFE_qvUgmZeCc3E-isL9g.png"/></div></figure><p id="a2b3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Algorithm below draws from the series of posterior distributions above in a sequential order.</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="0d2a" class="oy nu fq ov b bg oz pa l pb pc">## Start the MCMC iterations for posterior sampling# Loop over the number of simulations<br/>for (i in 2:nSim) {<br/>  print(i)  # Print the current iteration number<br/>  <br/>  # For each observation, update the group assignment (z)<br/>  for (t in 1:(N*G)) {<br/>    fig &lt;- NULL<br/>    for (g in 1:G) {<br/>      # Calculate the likelihood of each group and the corresponding posterior probability<br/>      fig[g] &lt;- dnorm(y[t, 1], X[t, ] %*% mBeta[i-1, g], sqrt(mSigma2[i-1, g])) * mPi[i-1, g]<br/>    }<br/>    # Avoid zero likelihood and adjust it<br/>    if (all(fig) == 0) {<br/>      fig &lt;- fig + 1/G<br/>    }<br/>    <br/>    # Sample a new group assignment based on the posterior probabilities<br/>    z[i, t] &lt;- which(rmultinom(1, 1, fig/sum(fig)) == 1)<br/>  }<br/>  <br/>  # Update the regression coefficients for each group<br/>  for (g in 1:G) {<br/>    # Compute the posterior mean and variance for beta (using the data for group g)<br/>    DBeta &lt;- solve(t(X[z[i, ] == g, ]) %*% X[z[i, ] == g, ] / mSigma2[i-1, g] + solve(VBeta[g, g]))<br/>    dBeta &lt;- t(X[z[i, ] == g, ]) %*% y[z[i, ] == g, 1] / mSigma2[i-1, g] + solve(VBeta[g, g]) %*% muBeta[g, 1]<br/>    <br/>    # Sample a new value for beta from the multivariate normal distribution<br/>    mBeta[i, g] &lt;- rmvnorm(1, DBeta %*% dBeta, DBeta)<br/>    <br/>    # Update the number of observations in group g<br/>    ng[i, g] &lt;- sum(z[i, ] == g)<br/>    <br/>    # Update the variance (sigma^2) for each group<br/>    mSigma2[i, g] &lt;- rigamma(1, ng[i, g]/2 + shSigma, raSigma + 1/2 * sum((y[z[i, ] == g, 1] - (X[z[i, ] == g, ] * mBeta[i, g]))^2))<br/>  }<br/>  <br/>  # Reorder the group labels to maintain consistency<br/>  reorderWay &lt;- order(mBeta[i, ])<br/>  mBeta[i, ] &lt;- mBeta[i, reorderWay]<br/>  ng[i, ] &lt;- ng[i, reorderWay]<br/>  mSigma2[i, ] &lt;- mSigma2[i, reorderWay]<br/>  <br/>  # Update the mixing proportions (pi) based on the number of observations in each group<br/>  mPi[i, ] &lt;- rdirichlet(1, alphaPrior + ng[i, ])<br/>}</span></pre><p id="0a18" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This block of code performs the key steps in MCMC:</p><ul class=""><li id="b77c" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pd pe pf bk"><strong class="mk fr">Group Assignment Update</strong>: For each observation, we calculate the likelihood of the data belonging to each group and update the group assignment accordingly.</li><li id="8ce6" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">Regression Coefficient Update</strong>: The regression coefficients for each group are updated using the posterior mean and variance, which are calculated based on the observed data.</li><li id="b71f" class="mi mj fq mk b go pg mm mn gr ph mp mq mr pi mt mu mv pj mx my mz pk nb nc nd pd pe pf bk"><strong class="mk fr">Variance Update</strong>: The variance of the response variable for each group is updated using the inverse gamma distribution.</li></ul><h1 id="4a73" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Visualizing the Results</h1><p id="e8a0" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">Finally, we visualize the results of the MCMC sampling. We plot the posterior distributions for each regression coefficient, compare them to the true values, and plot the most likely group assignments.</p><pre class="nh ni nj nk nl ou ov ow bp ox bb bk"><span id="d982" class="oy nu fq ov b bg oz pa l pb pc"># Plot the posterior distributions for each beta coefficient<br/>par(mfrow=c(G,1))<br/>for (g in 1:G) {<br/>  plot(density(mBeta[5:nSim, g]), main = 'True parameter (vertical) and the distribution of the samples')  # Plot the density for the beta estimates<br/>  abline(v = betas[g])  # Add a vertical line at the true value of beta for comparison<br/>}</span></pre><p id="dcba" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This plot shows how the MCMC samples (posterior distribution) for the regression coefficients converge to the true values (<code class="cx qd qe qf ov b">betas</code>).</p><h1 id="869c" class="nt nu fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Conclusion</h1><p id="9c28" class="pw-post-body-paragraph mi mj fq mk b go op mm mn gr oq mp mq mr or mt mu mv os mx my mz ot nb nc nd fj bk">Through this process, we demonstrated how finite normal mixtures can be used in a regression context, combined with MCMC for parameter estimation. By simulating data with known groupings and recovering the parameters through Bayesian inference, we can assess how well our model captures the underlying structure of the data.</p><figure class="nh ni nj nk nl nm"><div class="qv io l ed"><div class="qw qx l"/></div></figure><p id="3bad" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="qi">Unless otherwise noted, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>