<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Top Evaluation Metrics for RAG Failures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Top Evaluation Metrics for RAG Failures</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02">https://towardsdatascience.com/top-evaluation-metrics-for-rag-failures-acb27d2a5485?source=collection_archive---------3-----------------------#2024-02-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/303836d4eb6b35c4a200aa1fd4ef5171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InEnaDmOTiPwqSO515HjIw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image created by author using Dall-E 3</figcaption></figure><div/><div><h2 id="7e2c" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Troubleshoot LLMs and Retrieval Augmented Generation with Retrieval and Response Metrics</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Amber Roberts" class="l ep by dd de cx" src="../Images/ee686891eeedca7f33a63e147cc2c086.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IuZG1F11ODEgDVZOwk4TRw.jpeg"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://medium.com/@amber.roberts?source=post_page---byline--acb27d2a5485--------------------------------" rel="noopener follow">Amber Roberts</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--acb27d2a5485--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">2</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="fw"><div class="ab cb"><div class="mg nc mh nd mi ne cf nf cg ng ci bh"><figure class="ni nj nk nl nm fw nn no paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp nh"><img src="../Images/be678bd9a80b7f2b544ca337c3da88a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FjGsyIVfL025idc8WUSMQg.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 1: Root Cause Workflows for LLM RAG Applications (flowchart created by author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e95a" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">If you have been experimenting with large language models (LLMs) for search and retrieval tasks, you have likely come across retrieval augmented generation (RAG) as a technique to add relevant contextual information to LLM generated responses. By connecting an LLM to private data, RAG can enable a better response by feeding relevant data in the context window.</p><p id="4afc" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">RAG has been shown to be highly effective for complex query answering, knowledge-intensive tasks, and enhancing the precision and relevance of responses for AI models, especially in situations where standalone training data may fall short.</p><p id="6e57" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">However, these benefits from RAG can only be reaped if you are continuously monitoring your LLM system at common failure points — most notably with response and retrieval evaluation metrics. In this piece we will go through the best workflows for troubleshooting poor retrieval and response metrics.</p><h1 id="b03f" class="ol om gk bf on oo op hk oq or os hn ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Troubleshooting Retrieval and Responses</h1><p id="82fb" class="pw-post-body-paragraph np nq gk nr b hi ph nt nu hl pi nw nx ny pj oa ob oc pk oe of og pl oi oj ok fj bk">It’s worth remembering that <strong class="nr gl">RAG works best when required information is readily available</strong>. Whether relevant documents are available<strong class="nr gl"> </strong>focuses RAG system evaluations on two critical aspects:</p><ul class=""><li id="fa97" class="np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok pm pn po bk"><strong class="nr gl"><em class="pp">Retrieval Evaluation:</em></strong> To assess the accuracy and relevance of the documents that were retrieved</li><li id="e15f" class="np nq gk nr b hi pq nt nu hl pr nw nx ny ps oa ob oc pt oe of og pu oi oj ok pm pn po bk"><strong class="nr gl"><em class="pp">Response Evaluation: </em></strong>Measure the appropriateness of the response generated by the system when the context was provided</li></ul><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pv"><img src="../Images/0a43ebe03eca8b3e0fdb478c80064940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gSi0BCyx5r_CicTo2WpPeg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 2: Response Evals and Retrieval Evals in an LLM Application (image by author)</figcaption></figure><h2 id="b9cf" class="pw om gk bf on px py pz oq qa qb qc ot ny qd qe qf oc qg qh qi og qj qk ql qm bk">Table 1: Response Evaluation Metrics</h2><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qn"><img src="../Images/c69acf88763ec390f93559601d082b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bY0ENu8BeVlCHOeWlpylsw.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Table 1 by author</figcaption></figure><h2 id="f84a" class="pw om gk bf on px py pz oq qa qb qc ot ny qd qe qf oc qg qh qi og qj qk ql qm bk">Table 2: Retrieval Evaluation Metrics</h2><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qn"><img src="../Images/907ac76c31da0365dbf548f1907aa30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpI74eFDJXvH_lQDyLPk_g.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Table 2 by author</figcaption></figure><h1 id="e362" class="ol om gk bf on oo op hk oq or os hn ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Troubleshooting RAG Workflows</h1><p id="f054" class="pw-post-body-paragraph np nq gk nr b hi ph nt nu hl pi nw nx ny pj oa ob oc pk oe of og pl oi oj ok fj bk">Let’s review three potential scenarios to troubleshoot poor LLM performance based on the flow diagram.</p><h2 id="fd46" class="pw om gk bf on px py pz oq qa qb qc ot ny qd qe qf oc qg qh qi og qj qk ql qm bk">Scenario 1: Good Response, Good Retrieval</h2><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qo"><img src="../Images/28c5088db40d603793b39beac7b19aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-QsgSFv_-LVsKUYLDwyYKw.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><p id="9170" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">In this scenario everything in the LLM application is acting as expected and we have a good response with a good retrieval. We find our response evaluation is “correct” and our “Hit = True.” Hit is a binary metric, where “True” means the relevant document was retrieved and “False” would mean the relevant document was not retrieved. Note that the aggregate statistic for Hit is the Hit rate (percent of queries that have relevant context).</p><p id="013f" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">For our response evaluations, correctness is an evaluation metric that can be done simply with a combination of the <strong class="nr gl">input</strong> (query), <strong class="nr gl">output</strong> (response), and <strong class="nr gl">context </strong>as can be seen in <em class="pp">Table 1</em>. Several of these evaluation criteria do not require user labeled ground-truth labels since LLMs can also be used to generate labels, scores, and explanations with tools like the <a class="af qp" href="https://arize.com/blog/calling-all-functions-benchmarking-openai-function-calling-and-explanations/" rel="noopener ugc nofollow" target="_blank">OpenAI function calling</a>, below is an example prompt template.</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qq"><img src="../Images/1776a24f280136742b49ffb25f859d1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1PLnC-XTw2fDjfCM1O3FwA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image by author</figcaption></figure><p id="2b9f" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">These <a class="af qp" href="https://arize.com/blog-course/llm-evaluation-the-definitive-guide/" rel="noopener ugc nofollow" target="_blank">LLM evals</a> can be formatted as numeric, categorical (binary and multi-class) and multi-output (multiple scores or labels) — with categorical-binary being the most commonly used and numeric being the least commonly used.</p><h2 id="de4c" class="pw om gk bf on px py pz oq qa qb qc ot ny qd qe qf oc qg qh qi og qj qk ql qm bk">Scenario 2: Bad Response, Bad Retrieval</h2><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qo"><img src="../Images/478d15be23dedf0f5031c07c5a2edf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lfZUoqFZGODHvXIg7IH7Cw.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><p id="c3ad" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">In this scenario we find that the response is incorrect and the relevant content was not received. Based on the query we see that the content wasn’t received because there is no solution to the query. The LLM cannot predict future purchases no matter what documents it is supplied. However, the LLM can generate a better response than to hallucinate an answer. Here it would be to experiment with the prompt that is generating the response by simply adding a line to the LLM prompt template of “<em class="pp">if relevant content is not provided and no conclusive solution is found, respond that the answer is unknown.” </em>In some cases the correct answer is that the answer does not exist.</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qo"><img src="../Images/124665fbc597674323ef4459f78ff639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmvt8JQYAKjiIcBYVAhBBQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><h2 id="fde9" class="pw om gk bf on px py pz oq qa qb qc ot ny qd qe qf oc qg qh qi og qj qk ql qm bk">Scenario 3: Bad Response, Mixed Retrieval Metrics</h2><p id="34b6" class="pw-post-body-paragraph np nq gk nr b hi ph nt nu hl pi nw nx ny pj oa ob oc pk oe of og pl oi oj ok fj bk">In this third scenario, we see an incorrect response with mixed retrieval metrics (the relevant document was retrieved, but the LLM hallucinated an answer due to being given too much information).</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/09c89963a3710d41d9c93d128d5c8899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mru4BPUmdCoxyk46ct0a5w.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><p id="68a6" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">To evaluate an LLM RAG system, you need to both fetch the right context and then generate an appropriate answer. Typically, developers will embed a user query and use it to search a vector database for relevant chunks (see Figure 3). Retrieval performance hinges not only on the returned chunks being semantically similar to the query, but on whether those chunks provide enough relevant information to generate the correct response to the query. Now, you must configure the parameters around your RAG system (type of retrieval, chunk size, and K).</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qs"><img src="../Images/a37b959baffef3f83e5a1bd9958b4b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqUfSvFclIoyzgbOPYT_FQ.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 3: RAG Framework (by author)</figcaption></figure><p id="82ae" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">Similarly with our last scenario, we can try editing the prompt template or change out the LLM being used to generate responses. Since the relevant content is retrieved during the document retrieval process but isn’t being surfaced by the LLM, this could be a quick solution. Below is an example of a correct response generated from running a revised prompt template (after iterating on prompt variables, LLM parameters, and the prompt template itself).</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/52689ef6524a0041ad99e1a1373f7907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMn3BjVTr89eK6uU-6lalw.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><p id="e799" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">When troubleshooting bad responses with mixed performance metrics, we need to first figure out which retrieval metrics are underperforming. The easiest way of doing this is to implement thresholds and monitors. Once you are alerted to a particular underperforming metric you can resolve with specific workflows. Let’s take nDCG for example. nDCG is used to measure the effectiveness of your top ranked documents and takes into account the position of relevant docs, so if you retrieve your relevant document (Hit = ‘True’), you will want to consider implementing a reranking technique to get the relevant documents closer to the top ranked search results.</p><p id="21cf" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">For our current scenario we retrieved a relevant document (Hit = ‘True’), and that document is in the first position, so let’s try and improve the precision (percent relevant documents) up to ‘K’ retrieved documents. Currently our Precision@4 is 25%, but if we used only the first two relevant documents then Precision@2 = 50% since half of the documents are relevant. This change leads to the correct response from the LLM since it is given less information, but more relevant information proportionally.</p><figure class="ni nj nk nl nm fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qr"><img src="../Images/df1e002e326d5b7f14e8881ce5e0f6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxzPKA5NK81_IS-h0WrkrQ.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Diagram by author</figcaption></figure><p id="2a5c" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">Essentially what we were seeing here is a common problem in RAG known as <a class="af qp" href="https://arxiv.org/abs/2307.03172" rel="noopener ugc nofollow" target="_blank">lost in the middle</a>, when your LLM is overwhelmed with too much information that is not always relevant and then is unable to give the best answer possible. From our diagram, we see that adjusting your chunk size is one of the first things many teams do to improve RAG applications but it’s not always intuitive. With context overflow and lost in the middle problems, more documents isn’t always better, and reranking won’t necessarily improve performance. To evaluate which chunk size works best, you need to define an eval benchmark and do a sweep over chunk sizes and top-k values. In addition to experimenting with chunking strategies, testing out different text extraction techniques and embedding methods will also improve overall RAG performance.</p><h1 id="f67a" class="ol om gk bf on oo op hk oq or os hn ot ou ov ow ox oy oz pa pb pc pd pe pf pg bk">Response and Retrieval Evaluation Metrics Summary</h1><p id="4915" class="pw-post-body-paragraph np nq gk nr b hi ph nt nu hl pi nw nx ny pj oa ob oc pk oe of og pl oi oj ok fj bk">The response and retrieval evaluation metrics and approaches in <a class="af qp" href="https://arize.com/blog-course/rag-evaluation/" rel="noopener ugc nofollow" target="_blank">this piece</a> offer a comprehensive way to view an LLM RAG system’s performance, guiding developers and users in understanding its strengths and limitations. By continually evaluating these systems against these metrics, improvements can be made to enhance RAG’s ability to provide accurate, relevant, and timely information.</p><p id="f333" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk">Additional advanced methods for improving RAG include <a class="af qp" href="https://blog.llamaindex.ai/a-cheat-sheet-and-some-recipes-for-building-advanced-rag-803a9d94c41b" rel="noopener ugc nofollow" target="_blank">re-ranking</a>, metadata attachments, testing out different embedding models, testing out different indexing methods, implementing <a class="af qp" href="https://arize.com/blog/hyde-paper-reading-and-discussion/" rel="noopener ugc nofollow" target="_blank">HyDE</a>, implementing keyword search methods, or implementing Cohere document mode (similar to HyDE). Note that while these more advanced methods — like chunking, text extraction, embedding model experimentation — may produce more contextually coherent chunks, these methods are more resource-intensive. Using RAG along with advanced methods can make performance improvements to your LLM system and will continue to do so as long as your retrieval and response metrics are properly monitored and maintained.</p><p id="1363" class="pw-post-body-paragraph np nq gk nr b hi ns nt nu hl nv nw nx ny nz oa ob oc od oe of og oh oi oj ok fj bk"><em class="pp">Questions? Please reach out to me here or on </em><a class="af qp" href="https://www.linkedin.com/in/amber-roberts42/" rel="noopener ugc nofollow" target="_blank"><em class="pp">LinkedIn</em></a><em class="pp">, </em><a class="af qp" href="https://twitter.com/astronomeramber" rel="noopener ugc nofollow" target="_blank"><em class="pp">X</em></a><em class="pp">, or </em><a class="af qp" href="https://join.slack.com/t/arize-ai/shared_invite/zt-26zg4u3lw-OjUNoLvKQ2Yv53EfvxW6Kg" rel="noopener ugc nofollow" target="_blank"><em class="pp">Slack</em></a><em class="pp">!</em></p></div></div></div></div>    
</body>
</html>