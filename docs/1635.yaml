- en: Continual Learning — A Deep Dive Into Elastic Weight Consolidation Loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/continual-learning-a-deep-dive-into-elastic-weight-consolidation-loss-7cda4a2d058c?source=collection_archive---------5-----------------------#2024-07-02](https://towardsdatascience.com/continual-learning-a-deep-dive-into-elastic-weight-consolidation-loss-7cda4a2d058c?source=collection_archive---------5-----------------------#2024-07-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With PyTorch Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page---byline--7cda4a2d058c--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page---byline--7cda4a2d058c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7cda4a2d058c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7cda4a2d058c--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page---byline--7cda4a2d058c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7cda4a2d058c--------------------------------)
    ·9 min read·Jul 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most significant challenges in training artificial neural networks
    is catastrophic forgetting. This problem arises when a neural network trained
    on one task (Task A) subsequently learns a new task (Task B) and, in the process,
    forgets how to perform the original task. In this article, we will explore a method
    to address this issue known as Elastic Weight Consolidation (EWC). EWC offers
    a promising approach to mitigate catastrophic forgetting enabling neural networks
    to retain knowledge of previously learned tasks while acquiring new skills.
  prefs: []
  type: TYPE_NORMAL
- en: '*All figures in this article are by author unless otherwise specified*'
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bff3bb9ff2449a1ee52fb527cc065d20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: intuition on EWC, figure from the [paper](https://arxiv.org/pdf/1612.00796)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been shown that there exist many configurations of optimal parameters
    with a desired low error on a task — gray and yellow regions for tasks A and B
    respectively in the above figure. Assuming we found one such configuration *θꭺ**
    for task A, when continuing to train the model from such configuration to a new
    task B we have three different scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Simply continuing to train on task B with no penalty we will end up in low level
    region for task B but performing below the desired accuracy on task…
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
