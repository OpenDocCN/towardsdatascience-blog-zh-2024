["```py\nfrom scipy.stats import boxcox\ny, lambda_ = boxcox(x - (-250))\n```", "```py\nimport tensorflow as tf\nimport numpy as np\n\n@tf.function\ndef log_prob(x, params):\n    # extract parameters\n    alpha, log_beta = params[0], params[1]  # rescaling params\n    beta = tf.math.exp(log_beta)\n    epsilon, log_delta = params[2], params[3]  # transformation params\n    delta = tf.math.exp(log_delta)\n    mu, log_sigma = params[4], params[5]  # normal dist params\n    sigma = tf.math.exp(log_sigma)\n    # rescale\n    x_scaled = (x - alpha)/beta\n    # transformation\n    sinh_arg = epsilon + delta * tf.math.asinh(x_scaled)\n    x_transformed = (1/delta) * tf.math.sinh(sinh_arg)\n    # log jacobian of transformation\n    d_sinh = tf.math.log(tf.math.cosh(sinh_arg))\n    d_arcsinh = - 0.5*tf.math.log(x_scaled**2 + 1)\n    d_rescaling = - log_beta\n    jacobian =  d_sinh + d_arcsinh + d_rescaling  # chain rule\n    # normal likelihood\n    z = (x_transformed - mu)/sigma  # standardized\n    normal_prob = -0.5*tf.math.log(2*np.pi) - log_sigma -0.5*z**2\n    return normal_prob + jacobian\n\n# Learning rate and number of epochs\nlearning_rate = 0.1\nepochs = 1000\n\n# Initialize variables\ntf.random.set_seed(892)\nparams = tf.Variable(tf.random.normal(shape=(6,), mean=0.0, stddev=1.0), dtype=tf.float32)\n\n# Use the Adam optimizer\noptimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n\n# Perform gradient descent\nfor epoch in range(epochs):\n    with tf.GradientTape() as tape:\n        loss = - tf.reduce_mean(log_prob(x_tf, params))\n\n    # Compute gradients\n    gradients = tape.gradient(loss, [params])\n\n    # Apply gradients to variables\n    optimizer.apply_gradients(zip(gradients, [params]))\n\n    if (epoch % 100) == 0:\n        print(-loss.numpy())\n\nprint(f\"Optimal vals: {params}\")\n```"]