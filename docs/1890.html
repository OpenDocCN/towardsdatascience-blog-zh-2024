<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Train/Fine-Tune Segment Anything 2 (SAM 2) in 60 Lines of Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Train/Fine-Tune Segment Anything 2 (SAM 2) in 60 Lines of Code</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03">https://towardsdatascience.com/train-fine-tune-segment-anything-2-sam-2-in-60-lines-of-code-928dd29a63b3?source=collection_archive---------0-----------------------#2024-08-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2883" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A step-by-step tutorial for fine-tuning SAM2 for custom segmentation tasks</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sagi eppel" class="l ep by dd de cx" src="../Images/7f02c03dbfb21891b95ddb8c52cb5fff.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*rlrthvP128DI4GS5mgNnag.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sagieppel?source=post_page---byline--928dd29a63b3--------------------------------" rel="noopener follow">Sagi eppel</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--928dd29a63b3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">11</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="93d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://ai.meta.com/blog/segment-anything-2/" rel="noopener ugc nofollow" target="_blank">SAM2 (Segment Anything 2)</a> is a new model by Meta aiming to segment anything in an image without being limited to specific classes or domains. What makes this model unique is the scale of data on which it was trained: 11 million images, and 11 billion masks. This extensive training makes SAM2 a powerful starting point for training on new image segmentation tasks.</p><p id="47d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The question you might ask is if SAM can segment anything why do we even need to retrain it? The answer is that SAM is very good at common objects but can perform rather poorly on rare or domain-specific tasks.<br/>However, even in cases where SAM gives insufficient results, it is still possible to significantly improve the model’s ability by fine-tuning it on new data. In many cases, this will take less training data and give better results then training a model from scratch.</p><p id="3a3a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This tutorial demonstrates how to fine-tune SAM2 on new data in just 60 lines of code (excluding comments and imports).</p><p id="7837" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The full training script of the can be found in:</strong></p><div class="ng nh ni nj nk nl"><a href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for training/fine tune the Meta Segment Anything Model 2 (SAM 2) …</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz lr nl"/></div></div></a></div><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oc"><img src="../Images/1bd74d767271c35cc608b7b62e43607d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFVDKpN0JF-N8kRtnF2aJg.png"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">SAM2 net diagram taken from <a class="af nf" href="https://github.com/facebookresearch/segment-anything-2" rel="noopener ugc nofollow" target="_blank">SAM2 GIT page</a></figcaption></figure><h1 id="5cce" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk"><strong class="al">How Segment Anything works</strong></h1><p id="79a7" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">The main way SAM works is by taking an image and a point in the image and predicting the mask of the segment that contains the point. This approach enables full image segmentation without human intervention and with no limits on the classes or types of segments (as discussed in <a class="af nf" href="https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2" rel="noopener ugc nofollow" target="_blank">a previous post</a>).</p><p id="f615" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The procedure for using SAM for full image segmentation:</p><ol class=""><li id="029a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pu pv pw bk">Select a set of points in the image</li><li id="8b54" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">Use SAM to predict the segment containing each point</li><li id="02eb" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">Combine the resulting segments into a single map</li></ol><p id="3f4d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While SAM can also utilize other inputs like masks or bounding boxes, these are mainly relevant for interactive segmentation involving human input. For this tutorial, we’ll focus on fully automatic segmentation and will only consider single points input.</p><p id="b2cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More details on the model are available on the <a class="af nf" href="https://ai.meta.com/blog/segment-anything-2/" rel="noopener ugc nofollow" target="_blank">project website.</a></p><h1 id="cf67" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk"><strong class="al">Downloading SAM2 and setting environment</strong></h1><p id="ebab" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">The SAM2 can be downloaded from:</p><div class="ng nh ni nj nk nl"><a href="https://github.com/facebookresearch/segment-anything-2?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">GitHub - facebookresearch/segment-anything-2: The repository provides code for running inference…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for…</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="qc l nw nx ny nu nz lr nl"/></div></div></a></div><p id="d57d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you don’t want to copy the training code, you can also download my forked version that already contains the TRAIN.py script.</p><div class="ng nh ni nj nk nl"><a href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">GitHub - sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code: The repository provides…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for training/fine tune the Meta Segment Anything Model 2 (SAM 2) …</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="qd l nw nx ny nu nz lr nl"/></div></div></a></div><p id="fa7c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Follow the installation instructions on the github repository.</p><p id="443b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In general, you need Python &gt;=3.11 and <a class="af nf" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch.</a></p><p id="1c22" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In addition, we will use OpenCV this can be installed using:</p><p id="103a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qe">pip install opencv-python</em></p><h2 id="dc94" class="qf ou fq bf ov qg qh qi oy qj qk ql pb ms qm qn qo mw qp qq qr na qs qt qu qv bk">Downloading pre-trained model</h2><p id="721a" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">You also need to download the pre-trained model from:</p><p id="f1d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr">https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints</strong></a></p><p id="ece7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are several models you can choose from all compatible with this tutorial. I recommend using the <a class="af nf" href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt" rel="noopener ugc nofollow" target="_blank">small model</a> which is the fastest to train.</p><h1 id="bec9" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Downloading training data</h1><p id="41b4" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">For this tutorial, we will use the<a class="af nf" href="https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1" rel="noopener ugc nofollow" target="_blank"> LabPics1 dataset</a> for segmenting materials and liquids. You can download the dataset from this URL:</p><p id="6c3a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1" rel="noopener ugc nofollow" target="_blank">https://zenodo.org/records/3697452/files/LabPicsV1.zip?download=1</a></p><h1 id="61d9" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk"><strong class="al">Preparing the data reader</strong></h1><p id="d8d8" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">The first thing we need to write is the data reader. This will read and prepare the data for the net.</p><p id="7545" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The data reader needs to produce:</p><ol class=""><li id="3302" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pu pv pw bk">An image</li><li id="7bb6" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">Masks of all the segments in the image.</li><li id="052a" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">And a <a class="af nf" href="https://faun.pub/train-pointer-net-for-segmenting-objects-parts-and-materials-in-60-lines-of-code-ca328be8cef2" rel="noopener ugc nofollow" target="_blank">random point inside each mask</a></li></ol><p id="d54d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Lets start by loading dependencies:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="5b17" class="ra ou fq qx b bg rb rc l rd re">import numpy as np<br/>import torch<br/>import cv2<br/>import os<br/>from sam2.build_sam import build_sam2<br/>from sam2.sam2_image_predictor import SAM2ImagePredictor</span></pre><p id="762f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next we list all the images in the dataset:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="c26f" class="ra ou fq qx b bg rb rc l rd re">data_dir=r"LabPicsV1//" # Path to LabPics1 dataset folder<br/>data=[] # list of files in dataset<br/>for ff, name in enumerate(os.listdir(data_dir+"Simple/Train/Image/")):  # go over all folder annotation<br/>    data.append({"image":data_dir+"Simple/Train/Image/"+name,"annotation":data_dir+"Simple/Train/Instance/"+name[:-4]+".png"})</span></pre><p id="492b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now for the main function that will load the training batch. The training batch includes: One random image, all the segmentation masks belong to this image, and a random point in each mask:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="efbf" class="ra ou fq qx b bg rb rc l rd re">def read_batch(data): # read random image and its annotaion from  the dataset (LabPics)<br/><br/>   #  select image<br/><br/>        ent  = data[np.random.randint(len(data))] # choose random entry<br/>        Img = cv2.imread(ent["image"])[...,::-1]  # read image<br/>        ann_map = cv2.imread(ent["annotation"]) # read annotation<br/><br/>   # resize image<br/><br/>        r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor<br/>        Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))<br/>        ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)<br/><br/>   # merge vessels and materials annotations<br/><br/>        mat_map = ann_map[:,:,0] # material annotation map<br/>        ves_map = ann_map[:,:,2] # vessel  annotaion map<br/>        mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merged map<br/><br/>   # Get binary masks and points<br/><br/>        inds = np.unique(mat_map)[1:] # load all indices<br/>        points= []<br/>        masks = [] <br/>        for ind in inds:<br/>            mask=(mat_map == ind).astype(np.uint8) # make binary mask<br/>            masks.append(mask)<br/>            coords = np.argwhere(mask &gt; 0) # get all coordinates in mask<br/>            yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate<br/>            points.append([[yx[1], yx[0]]])<br/>        return Img,np.array(masks),np.array(points), np.ones([len(masks),1])</span></pre><p id="327f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first part of this function is choosing a random image and loading it:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="758f" class="ra ou fq qx b bg rb rc l rd re">ent  = data[np.random.randint(len(data))] # choose random entry<br/>Img = cv2.imread(ent["image"])[...,::-1]  # read image<br/>ann_map = cv2.imread(ent["annotation"]) # read annotation</span></pre><p id="37fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that OpenCV reads images as BGR while SAM expects RGB images. By using <em class="qe">[…,::-1]</em> we change the image from BGR to RGB.</p><p id="5046" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">SAM expects the image size to not exceed 1024, so we are going to resize the image and the annotation map to this size.</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="adb4" class="ra ou fq qx b bg rb rc l rd re">r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor<br/>Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))<br/>ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)</span></pre><p id="9f18" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An important point here is that when resizing the annotation map (<em class="qe">ann_map</em>) we use <em class="qe">INTER_NEAREST</em> mode (nearest neighbors). In the annotation map, each pixel value is the index of the segment it belongs to. As a result, it’s important to use resizing methods that do not introduce new values to the map.</p><p id="89cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next block is specific to the format of the LabPics1 dataset. The annotation map (<em class="qe">ann_map</em>) contains a segmentation map for the vessels in the image in one channel, and another map for the materials annotation in a different channel. We going to merge them into a single map.</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="4211" class="ra ou fq qx b bg rb rc l rd re">  mat_map = ann_map[:,:,0] # material annotation map<br/>  ves_map = ann_map[:,:,2] # vessel  annotaion map<br/>  mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merged map</span></pre><p id="056e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What this gives us is a a map (<em class="qe">mat_map</em>) in which the value of each pixel is the index of the segment to which it belongs (for example: all cells with value 3 belong to segment 3). We want to transform this into a set of binary masks (0/1) where each mask corresponds to a different segment. In addition, from each mask, we want to extract a single point.</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="754b" class="ra ou fq qx b bg rb rc l rd re">inds = np.unique(mat_map)[1:] # list of all indices in map<br/>points= [] # list of all points (one for each mask)<br/>masks = [] # list of all masks<br/>for ind in inds:<br/>            mask = (mat_map == ind).astype(np.uint8) # make binary mask for index ind<br/>            masks.append(mask)<br/>            coords = np.argwhere(mask &gt; 0) # get all coordinates in mask<br/>            yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate<br/>            points.append([[yx[1], yx[0]]])<br/>return Img,np.array(masks),np.array(points), np.ones([len(masks),1])</span></pre><p id="09cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We got the image (<em class="qe">Img</em>), a list of binary masks corresponding to segments in the image (<em class="qe">masks</em>), and for each mask the coordinate of a single point inside the mask (<em class="qe">points</em>).</p><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob rf"><img src="../Images/29c072b8bda93b285bb87851fd68ae8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fPMCXxvBulUltXd00ngwg.jpeg"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Example for a batch of training data: 1) An Image. 2) List of segments masks. 3) For each mask a single point inside the mask (marked red). Taken from the LabPics dataset.</figcaption></figure><h1 id="9302" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Loading the SAM model</h1><p id="cb69" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Now lets load the net:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="c1cc" class="ra ou fq qx b bg rb rc l rd re">sam2_checkpoint = "sam2_hiera_small.pt" # path to model weight<br/>model_cfg = "sam2_hiera_s.yaml" # model config<br/>sam2_model = build_sam2(model_cfg, sam2_checkpoint, device="cuda") # load model<br/>predictor = SAM2ImagePredictor(sam2_model) # load net</span></pre><p id="894b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we set the path to the model weights in: <em class="qe">sam2_checkpoint </em>parameter.<em class="qe"> </em>We downloaded the weights earlier from<a class="af nf" href="https://github.com/facebookresearch/segment-anything-2?tab=readme-ov-file#download-checkpoints" rel="noopener ugc nofollow" target="_blank"> here</a>. <strong class="ml fr">“sam2_hiera_small.pt”</strong> refer to the<a class="af nf" href="https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt" rel="noopener ugc nofollow" target="_blank"> small model</a> but the code will work for any model. Whichever model you choose you need to set the corresponding config file in the <em class="qe">model_cfg </em>parameter.<em class="qe"> </em>The config files are located in the sub folder<em class="qe"> </em><strong class="ml fr"><em class="qe">“</em>sam2_configs/” </strong>of the main repository.</p><h1 id="d543" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Segment Anything General structure</h1><p id="0375" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Before we start training we need to understand the structure of the model.</p><p id="b9b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">SAM is composed of three parts:<br/>1) Image encoder, 2) Prompt encoder, 3) Mask decoder.</p><p id="69cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The image encoder is responsible for processing the image and creating the image embedding. This is the largest component and training it will demand strong GPU.</p><p id="e922" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The prompt encoder processes input prompt, in our case the input point.</p><p id="346d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The mask decoder takes the output of the image encoder and prompt encoder and produces the final segmentation masks.</p><h1 id="3d76" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Setting training parameters:</h1><p id="afe5" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">We can enable the training of the mask decoder and prompt encoder by setting:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="b4b9" class="ra ou fq qx b bg rb rc l rd re">predictor.model.sam_mask_decoder.train(True) # enable training of mask decoder <br/>predictor.model.sam_prompt_encoder.train(True) # enable training of prompt encoder</span></pre><p id="92da" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can enable training of the image encoder by using: “<strong class="ml fr"><em class="qe">predictor.model.image_encoder.train(True)”</em></strong></p><p id="8d94" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This will take stronger GPU but will give the net more room to improve. If you choose to train the image encoder, you must scan the SAM2 code for “<strong class="ml fr"><em class="qe">no_grad”</em></strong> commands and remove them. (<strong class="ml fr"><em class="qe">no_grad</em></strong> blocks the gradient collection, which saves memory but prevents training).</p><p id="4d9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we define the standard adamW optimizer:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="a3aa" class="ra ou fq qx b bg rb rc l rd re">optimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)</span></pre><p id="f4d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also going to use mixed precision training which is just a more memory-efficient training strategy:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="16e7" class="ra ou fq qx b bg rb rc l rd re">scaler = torch.cuda.amp.GradScaler() # set mixed precision</span></pre><h1 id="87b1" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk"><strong class="al">Main training loop</strong></h1><p id="473b" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Now lets build the main training loop. The first part is reading and preparing the data:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="fe43" class="ra ou fq qx b bg rb rc l rd re">for itr in range(100000):<br/>    with torch.cuda.amp.autocast(): # cast to mix precision<br/>            image,mask,input_point, input_label = read_batch(data) # load data batch<br/>            if mask.shape[0]==0: continue # ignore empty batches<br/>            predictor.set_image(image) # apply SAM image encoder to the image</span></pre><p id="ed4e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First we cast the data to mix precision for efficient training:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="c613" class="ra ou fq qx b bg rb rc l rd re">with torch.cuda.amp.autocast():</span></pre><p id="3623" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we use the reader function we created earlier to read training data:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="3fe7" class="ra ou fq qx b bg rb rc l rd re">image,mask,input_point, input_label = read_batch(data)</span></pre><p id="2e93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We take the image we loaded and pass it through the image encoder (the first part of the net):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="dbd3" class="ra ou fq qx b bg rb rc l rd re">predictor.set_image(image)</span></pre><p id="9ae4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we process the input points using the net prompt encoder:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="c5ed" class="ra ou fq qx b bg rb rc l rd re">  mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)<br/>  sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels),boxes=None,masks=None,)</span></pre><p id="af2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that in this part we can also input boxes or masks but we are not going to use these options.</p><p id="7138" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we encoded both the prompt (points) and the image we can finally predict the segmentation masks:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="c633" class="ra ou fq qx b bg rb rc l rd re">batched_mode = unnorm_coords.shape[0] &gt; 1 # multi mask prediction<br/>high_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features["high_res_feats"]]<br/>low_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(image_embeddings=predictor._features["image_embed"][-1].unsqueeze(0),image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),sparse_prompt_embeddings=sparse_embeddings,dense_prompt_embeddings=dense_embeddings,multimask_output=True,repeat_image=batched_mode,high_res_features=high_res_features,)<br/>prd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])# Upscale the masks to the original image resolution</span></pre><p id="0e76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The main part in this code is the<em class="qe"> </em><strong class="ml fr"><em class="qe">model.sam_mask_decoder</em></strong><em class="qe"> </em>which runs the mask_decoder part of the net and generates the segmentation masks (<em class="qe">low_res_masks</em>) and their scores (<em class="qe">prd_scores</em>).</p><p id="b469" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These masks are in lower resolution than the original input image and are resized to the original input size in the <strong class="ml fr"><em class="qe">postprocess_masks</em></strong><em class="qe"> </em>function.</p><p id="f32b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This gives us the final prediction of the net: 3 segmentation masks (<em class="qe">prd_masks</em>) for each input point we used and the masks scores (<em class="qe">prd_scores</em>). <em class="qe">prd_masks</em> contains 3 predicted masks for each input point but we only going to use the first mask for each point. <em class="qe">prd_scores</em> contains a score of how good the net thinks each mask is (or how sure it is in the prediction).</p><h1 id="43c6" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Loss functions</h1><h2 id="d561" class="qf ou fq bf ov qg qh qi oy qj qk ql pb ms qm qn qo mw qp qq qr na qs qt qu qv bk">Segmentation loss</h2><p id="4e3e" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Now we have the net predictions we can calculate the loss. First, we calculate the segmentation loss, which means how good the predicted mask is compared to the ground true mask. For this, we use the standard cross entropy loss.</p><p id="56bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First we need to convert prediction masks (<em class="qe">prd_mask</em>) from logits into probabilities using the sigmoid function:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="0a84" class="ra ou fq qx b bg rb rc l rd re">prd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map</span></pre><p id="e606" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next we convert the ground truth mask into a torch tensor:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="b916" class="ra ou fq qx b bg rb rc l rd re">prd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map</span></pre><p id="b1ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we calculate the cross entropy loss (<em class="qe">seg_loss</em>) manually using the ground truth (<em class="qe">gt_mask</em>) and predicted probability maps (<em class="qe">prd_mask</em>):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="0d5c" class="ra ou fq qx b bg rb rc l rd re">seg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean() # cross entropy loss </span></pre><p id="97ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(we add 0.0001 to prevent the log function from exploding for zero values).</p><h2 id="accd" class="qf ou fq bf ov qg qh qi oy qj qk ql pb ms qm qn qo mw qp qq qr na qs qt qu qv bk">Score loss (optional)</h2><p id="d775" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">In addition to the masks, the net also predicts the score for how good each predicted mask is. Training this part is less important but can be useful . To train this part we need to first know what is the true score of each predicted mask. Meaning, how good the predicted mask actually is. We are going to do it by comparing the GT mask and the corresponding predicted mask using intersection over union (IOU) metrics. IOU is simply the overlap between the two masks, divided by the combined area of the two masks. First, we calculate the intersection between the predicted and GT mask (the area in which they overlap):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="b90f" class="ra ou fq qx b bg rb rc l rd re">inter = (gt_mask * (prd_mask &gt; 0.5)).sum(1).sum(1)</span></pre><p id="91d1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We use threshold <em class="qe">(prd_mask &gt; 0.5)</em> to turn the prediction mask from probability to binary mask.</p><p id="bdde" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we get the IOU by dividing the intersection by the combined area (union) of the predicted and gt masks:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="0edd" class="ra ou fq qx b bg rb rc l rd re">iou = inter / (gt_mask.sum(1).sum(1) + (prd_mask &gt; 0.5).sum(1).sum(1) - inter)</span></pre><p id="5344" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We going to use the IOU as the true score for each mask, and get the score loss as the absolute difference between the predicted scores and the IOU we just calculated.</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="3f4a" class="ra ou fq qx b bg rb rc l rd re">score_loss = torch.abs(prd_scores[:, 0] - iou).mean()</span></pre><p id="6b4c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, we merge the segmentation loss and score loss (giving much higher weight to the first):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="0fb0" class="ra ou fq qx b bg rb rc l rd re">loss = seg_loss+score_loss*0.05  # mix losses</span></pre><h1 id="d8ee" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Final step: Backpropogation and saving model</h1><p id="88f7" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Once we get the loss everything is completely standard. We calculate backpropogation and update weights using the optimizer we made earlier:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="9f41" class="ra ou fq qx b bg rb rc l rd re">predictor.model.zero_grad() # empty gradient<br/>scaler.scale(loss).backward()  # Backpropogate<br/>scaler.step(optimizer)<br/>scaler.update() # Mix precision</span></pre><p id="2182" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also want to save the trained model once every 1000 steps:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="410c" class="ra ou fq qx b bg rb rc l rd re">if itr%1000==0: torch.save(predictor.model.state_dict(), "model.torch") # save model </span></pre><p id="28a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since we already calculated the IOU we can display it as a moving average to see how well the model prediction are improving over time:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="d37a" class="ra ou fq qx b bg rb rc l rd re"><br/>if itr==0: mean_iou=0<br/>mean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())<br/>print("step)",itr, "Accuracy(IOU)=",mean_iou)</span></pre><p id="dd0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And that it, we have trained/ fine-tuned the Segment-Anything 2 in less than 60 lines of code (not including comments and imports). After about 25,000 steps you should see major improvement .</p><p id="9458" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The model will be saved to “model.torch”.</p><p id="a68b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can find the full training code at:</p><div class="ng nh ni nj nk nl"><a href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN.py?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">fine-tune-train_segment_anything_2_in_60_lines_of_code/TRAIN.py at main ·…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for training/fine tune the Meta Segment Anything Model 2 (SAM 2) …</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="rg l nw nx ny nu nz lr nl"/></div></div></a></div><p id="954e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This tutorial used a single image per batch, a more efficient way is to use several different images per batch, The code for doing this is available at:</p><p id="e2be" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nf" href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py" rel="noopener ugc nofollow" target="_blank"><strong class="ml fr"><em class="qe">https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TRAIN_multi_image_batch.py</em></strong></a></p><h1 id="2acf" class="ot ou fq bf ov ow ox gq oy oz pa gt pb pc pd pe pf pg ph pi pj pk pl pm pn po bk">Inference: Loading and using the trained model:</h1><p id="7577" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">Now that the model as been fine-tuned, let’s use it to segment an image.</p><p id="6b84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We going to do this using the following steps:</p><ol class=""><li id="d2c7" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pu pv pw bk">Load the model we just trained.</li><li id="6c44" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">Give the model an image and a bunch of random points. For each point the net will predict the segment mask that contain this point and a score.</li><li id="60ef" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">Take these masks and stitch them together into one segmentation map.</li></ol><p id="3b21" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The full code for doing that is available at:</p><div class="ng nh ni nj nk nl"><a href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main ·…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for training/fine tune the Meta Segment Anything Model 2 (SAM 2) …</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="rh l nw nx ny nu nz lr nl"/></div></div></a></div><p id="c970" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we load the dependencies and cast the weights to float16 this makes the model much faster to run (only possible for inference).</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="a024" class="ra ou fq qx b bg rb rc l rd re">import numpy as np<br/>import torch<br/>import cv2<br/>from sam2.build_sam import build_sam2<br/>from sam2.sam2_image_predictor import SAM2ImagePredictor<br/><br/># use bfloat16 for the entire script (memory efficient)<br/>torch.autocast(device_type="cuda", dtype=torch.bfloat16).__enter__()</span></pre><p id="39a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we load a sample <a class="af nf" href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg" rel="noopener ugc nofollow" target="_blank">image</a> and a mask of the image region we want to segment (download <a class="af nf" href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_image.jpg" rel="noopener ugc nofollow" target="_blank">image</a>/<a class="af nf" href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/sample_mask.png" rel="noopener ugc nofollow" target="_blank">mask</a>):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="0121" class="ra ou fq qx b bg rb rc l rd re">image_path = r"sample_image.jpg" # path to image<br/>mask_path = r"sample_mask.png" # path to mask, the mask will define the image region to segment<br/>def read_image(image_path, mask_path): # read and resize image and mask<br/>        img = cv2.imread(image_path)[...,::-1]  # read image as rgb<br/>        mask = cv2.imread(mask_path,0) # mask of the region we want to segment<br/>        <br/>        # Resize image to maximum size of 1024<br/><br/>        r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])<br/>        img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))<br/>        mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)),interpolation=cv2.INTER_NEAREST)<br/>        return img, mask<br/>image,mask = read_image(image_path, mask_path)</span></pre><p id="5357" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Sample 30 random points inside the region we want to segment:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="d2e2" class="ra ou fq qx b bg rb rc l rd re">num_samples = 30 # number of points/segment to sample<br/>def get_points(mask,num_points): # Sample points inside the input mask<br/>        points=[]<br/>        for i in range(num_points):<br/>            coords = np.argwhere(mask &gt; 0)<br/>            yx = np.array(coords[np.random.randint(len(coords))])<br/>            points.append([[yx[1], yx[0]]])<br/>        return np.array(points)<br/>input_points = get_points(mask,num_samples)</span></pre><p id="ac7d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Load the standard SAM model (same as in training)</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="8b96" class="ra ou fq qx b bg rb rc l rd re"># Load model you need to have pretrained model already made<br/>sam2_checkpoint = "sam2_hiera_small.pt" <br/>model_cfg = "sam2_hiera_s.yaml" <br/>sam2_model = build_sam2(model_cfg, sam2_checkpoint, device="cuda")<br/>predictor = SAM2ImagePredictor(sam2_model)</span></pre><p id="70d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, Load the weights of the model we just trained (model.torch):</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="5303" class="ra ou fq qx b bg rb rc l rd re">predictor.model.load_state_dict(torch.load("model.torch"))</span></pre><p id="f793" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Run the fine-tuned model to predict a segmentation mask for every point we selected earlier:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="333e" class="ra ou fq qx b bg rb rc l rd re">with torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)<br/>        predictor.set_image(image) # image encoder<br/>        masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder<br/>            point_coords=input_points,<br/>            point_labels=np.ones([input_points.shape[0],1])<br/>        )</span></pre><p id="0b35" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we have a list of predicted masks and their scores. We want to somehow stitch them into a single consistent segmentation map. However, many of the masks overlap and might be inconsistent with each other. <br/>The approach to stitching is simple:</p><p id="9a71" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First we will sort the predicted masks according to their predicted scores:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="422a" class="ra ou fq qx b bg rb rc l rd re">masks=masks[:,0].astype(bool)<br/>shorted_masks = masks[np.argsort(scores[:,0])][::-1].astype(bool)</span></pre><p id="5903" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now lets create an empty segmentation map and occupancy map:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="bd7d" class="ra ou fq qx b bg rb rc l rd re">seg_map = np.zeros_like(shorted_masks[0],dtype=np.uint8)<br/>occupancy_mask = np.zeros_like(shorted_masks[0],dtype=bool)</span></pre><p id="33ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we add the masks one by one (from high to low score) to the segmentation map. We only add a mask if it’s consistent with the masks that were previously added, which means only if the mask we want to add has less than 15% overlap with already occupied areas.</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="6008" class="ra ou fq qx b bg rb rc l rd re">for i in range(shorted_masks.shape[0]):<br/>    mask = shorted_masks[i]<br/>    if (mask*occupancy_mask).sum()/mask.sum()&gt;0.15: continue <br/>    mask[occupancy_mask]=0<br/>    seg_map[mask]=i+1<br/>    occupancy_mask[mask]=1</span></pre><p id="5994" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And this is it.</p><p id="15a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qe">seg_mask </em>now contains the predicted segmentation map with different values for each segment and 0 for the background.</p><p id="fba8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can turn this into a color map using:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="5c85" class="ra ou fq qx b bg rb rc l rd re">rgb_image = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint8)<br/>for id_class in range(1,seg_map.max()+1):<br/>    rgb_image[seg_map == id_class] = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]</span></pre><p id="84ea" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And display:</p><pre class="od oe of og oh qw qx qy bp qz bb bk"><span id="22d5" class="ra ou fq qx b bg rb rc l rd re">cv2.imshow("annotation",rgb_image)<br/>cv2.imshow("mix",(rgb_image/2+image/2).astype(np.uint8))<br/>cv2.imshow("image",image)<br/>cv2.waitKey()</span></pre><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob ri"><img src="../Images/9e5af0c935665898307a0372f325bb5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dP6bvtSAs1Gcq59-b5FNUQ.jpeg"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Example for segmentation results using fine-tuned SAM2. Image from the LabPics dataset.</figcaption></figure><p id="9f21" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The full inference code is available at:</p><div class="ng nh ni nj nk nl"><a href="https://github.com/sagieppel/fine-tune-train_segment_anything_2_in_60_lines_of_code/blob/main/TEST_Net.py?source=post_page-----928dd29a63b3--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="nm ab ig"><div class="nn ab co cb no np"><h2 class="bf fr hw z io nq iq ir nr it iv fp bk">fine-tune-train_segment_anything_2_in_60_lines_of_code/TEST_Net.py at main ·…</h2><div class="ns l"><h3 class="bf b hw z io nq iq ir nr it iv dx">The repository provides code for training/fine tune the Meta Segment Anything Model 2 (SAM 2) …</h3></div><div class="nt l"><p class="bf b dy z io nq iq ir nr it iv dx">github.com</p></div></div><div class="nu l"><div class="rh l nw nx ny nu nz lr nl"/></div></div></a></div></div></div></div><div class="ab cb rj rk rl rm" role="separator"><span class="rn by bm ro rp rq"/><span class="rn by bm ro rp rq"/><span class="rn by bm ro rp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0c98" class="ot ou fq bf ov ow rr gq oy oz rs gt pb pc rt pe pf pg ru pi pj pk rv pm pn po bk">Conclusion:</h1><p id="4e52" class="pw-post-body-paragraph mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne fj bk">That’s it, we have trained and tested SAM2 on a custom dataset. Other than changing the data-reader, this should work for any dataset. In many cases, this should be enough to give a significant improvement in performance.</p><p id="1a4a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, SAM2 can also segment and track objects in videos, but fine-tuning this part is for another time.</p></div></div></div><div class="ab cb rj rk rl rm" role="separator"><span class="rn by bm ro rp rq"/><span class="rn by bm ro rp rq"/><span class="rn by bm ro rp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="bd03" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Copyright: </strong>All images for the post are taken from the <a class="af nf" href="https://github.com/facebookresearch/segment-anything-2" rel="noopener ugc nofollow" target="_blank">SAM2 GIT</a> repository (under Apache license), and <a class="af nf" href="https://zenodo.org/records/3697452" rel="noopener ugc nofollow" target="_blank">LabPics </a>dataset (under MIT license). This tutorial code and nets are available under the Apache license.</p></div></div></div></div>    
</body>
</html>