- en: LoRA Fine-Tuning On Your Apple Silicon MacBook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20](https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s Go Step-By-Step Fine-Tuning On Your MacBook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    ·10 min read·Nov 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b79c5fc5ce91eab4ddd3add262ff070.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Flux.1
  prefs: []
  type: TYPE_NORMAL
- en: As models become smaller, we are seeing more and more consumer computers capable
    of running LLMs locally. This both dramatically reduces the barriers for people
    training their own models and allows for more training techniques to be tried.
  prefs: []
  type: TYPE_NORMAL
- en: One consumer computer that can run LLMs locally quite well is an Apple Mac.
    Apple took advantage of its custom silicon and created an array processing library
    called MLX. By using MLX, Apple can run LLMs better than many other consumer computers.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I’ll explain at a high-level how MLX works, then show you
    how to fine-tune your own LLM locally using MLX. Finally, we’ll speed up our fine-tuned
    model using quantization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: MLX Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is MLX (and who can use it?)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLX is an open-source library from Apple that lets Mac users more efficiently
    run programs with large tensors in them. Naturally, when we want to train or fine-tune
    a model, this library comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: The way MLX works is by being very efficient with memory transfers between your
    Central Processing Unit (CPU), Graphics Processing Unit (GPU), and…
  prefs: []
  type: TYPE_NORMAL
