- en: LoRA Fine-Tuning On Your Apple Silicon MacBook
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你的苹果硅芯片MacBook上进行LoRA微调
- en: 原文：[https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20](https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20](https://towardsdatascience.com/lora-fine-tuning-on-your-apple-silicon-macbook-432c7dab614a?source=collection_archive---------5-----------------------#2024-11-20)
- en: Let’s Go Step-By-Step Fine-Tuning On Your MacBook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们一步步在你的MacBook上进行微调
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--432c7dab614a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    ·10 min read·Nov 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--432c7dab614a--------------------------------)
    ·阅读时间：10分钟·2024年11月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1b79c5fc5ce91eab4ddd3add262ff070.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b79c5fc5ce91eab4ddd3add262ff070.png)'
- en: Image by Author — Flux.1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片 — Flux.1
- en: As models become smaller, we are seeing more and more consumer computers capable
    of running LLMs locally. This both dramatically reduces the barriers for people
    training their own models and allows for more training techniques to be tried.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型变得越来越小，我们看到越来越多的消费级计算机能够在本地运行LLMs。这大大降低了人们训练自己模型的门槛，并且可以尝试更多的训练技术。
- en: One consumer computer that can run LLMs locally quite well is an Apple Mac.
    Apple took advantage of its custom silicon and created an array processing library
    called MLX. By using MLX, Apple can run LLMs better than many other consumer computers.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一款能够在本地良好运行大型语言模型（LLMs）的消费级计算机是苹果的Mac。苹果利用其定制的硅片，创建了一个名为MLX的阵列处理库。通过使用MLX，苹果能够比许多其他消费级计算机更好地运行LLMs。
- en: In this blog post, I’ll explain at a high-level how MLX works, then show you
    how to fine-tune your own LLM locally using MLX. Finally, we’ll speed up our fine-tuned
    model using quantization.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将高层次地解释MLX是如何工作的，然后展示如何使用MLX在本地微调你自己的LLM。最后，我们将使用量化技术加速我们微调后的模型。
- en: Let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解吧！
- en: MLX Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLX背景
- en: What is MLX (and who can use it?)
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是MLX（谁可以使用它？）
- en: MLX is an open-source library from Apple that lets Mac users more efficiently
    run programs with large tensors in them. Naturally, when we want to train or fine-tune
    a model, this library comes in handy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MLX是苹果推出的一个开源库，允许Mac用户更高效地运行包含大量张量的程序。自然地，当我们想要训练或微调模型时，这个库就非常有用。
- en: The way MLX works is by being very efficient with memory transfers between your
    Central Processing Unit (CPU), Graphics Processing Unit (GPU), and…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MLX的工作原理是通过在中央处理单元（CPU）、图形处理单元（GPU）之间非常高效地进行内存传输来实现的……
