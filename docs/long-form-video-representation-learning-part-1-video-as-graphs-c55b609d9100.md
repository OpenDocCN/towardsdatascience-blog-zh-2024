# 长格式视频表示学习（第1部分：视频作为图）

> 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14)

## 我们探索了具备长格式推理能力的新型视频表示方法。第1部分着重讨论视频作为图的表示，以及如何为多个下游应用学习轻量级的图神经网络。[第2部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)聚焦于稀疏视频-文本变换器。而[第3部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e)则展示了我们最新最前沿的探索。

[](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[![Subarna Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------) [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------) ·10分钟阅读·2024年5月14日

--

现有的视频架构在处理视频内容的几秒钟后往往会遇到计算或内存瓶颈。那么，我们如何实现准确高效的长格式视觉理解呢？一个重要的第一步是拥有一个能在长视频上实际运行的模型。为此，我们探索了具备长格式推理能力的新型视频表示方法。

## **什么是长格式推理，为什么要进行长格式推理？**

正如我们看到的，基于图像的理解任务随着深度学习模型（如卷积神经网络或变换器）取得了巨大成功，下一步自然就是超越静态图像，探索视频理解。开发视频理解模型需要两个同样重要的关注领域。首先是大规模的视频数据集，其次是用于高效提取视频特征的可学习骨干网络。即使设计者和标注者都有最好的意图，为动态信号（如视频）创建更细粒度且一致的注释也并非易事。自然，创建的大型视频数据集采用了相对更简单的方法，即在整个视频层面进行标注。至于第二个关注点，再次自然地将基于图像的模型（如CNN或变换器）扩展到视频理解中，因为视频被视为一系列视频帧的集合，每一帧的大小和形状都与图像相同。研究人员制作了使用采样帧作为输入的模型，而不是使用所有视频帧，这显然是为了节省内存。在具体应用中，当分析一个5分钟的视频片段，且视频帧率为30帧/秒时，我们需要处理9,000帧视频。若涉及到对每个视频帧提取16x16矩形块的密集计算，CNN或变换器都无法处理9,000帧的序列。因此，大多数模型的操作方式是：它们将一个短视频片段作为输入，进行预测，然后进行时间平滑，而不是理想情况下我们希望模型能够完整地观看视频。

现在提出这个问题。如果我们需要知道一个视频是“游泳”类型还是“网球”类型，是否真的需要分析一分钟的内容？答案显然是否定的。换句话说，优化用于视频识别的模型，很可能学会了查看背景和其他空间上下文信息，而不是学习推理一个“长”视频中实际发生的事情。我们可以将这种现象称为学习空间捷径。这些模型在视频识别任务中表现良好。那么你能猜到这些模型在需要实际时间推理的其他任务（如动作预测、视频问答和最近提出的情节记忆任务）中如何表现吗？由于它们没有经过时间推理训练，因此它们在这些应用中的表现并不好。

所以我们理解到，数据集/注释使得大多数视频模型无法学习如何在时间和动作序列上进行推理。随着时间的推移，研究人员意识到了这个问题，并开始提出不同的基准来解决长时间推理问题。然而，仍然存在一个问题，主要是由内存限制引起的，即我们如何迈出第一步，让模型能够接受一段长视频作为输入，而不是将多个短片段依次处理。为了解决这个问题，我们提出了一种基于时空图学习（Spatio-Temporal Graphs Learning，简称SPELL）的视频表示方法，以赋予模型长时间推理的能力。

# 视频作为时间图

设G = (V, E)为一个图，其中V为节点集，E为边集。对于社交网络、引用网络和分子结构等领域，V和E是系统所提供的，我们称图作为输入提供给可学习的模型。现在，考虑视频中的最简单情况，其中每一帧视频被视为一个节点，形成V。然而，目前尚不清楚节点t1（时间=t1时的帧）和节点t2（时间=t2时的帧）是如何连接的。因此，边集E没有提供。没有E，图的拓扑结构就不完整，导致“真实值”图的不可用。一个重要的挑战依然是如何将视频转换为图。由于数据集中没有这样的标注（或“真实值”）图，所以该图可以被视为潜在图。

当视频被建模为时间图时，许多视频理解问题可以被表述为节点分类或图分类问题。我们利用SPELL框架来处理诸如动作边界检测、时间动作分割、视频摘要/高亮回放检测等任务。

## 视频摘要：表述为节点分类问题

在这里，我们提出了这样一个框架，称为VideoSAGE，代表视频摘要与图表示学习。我们利用视频作为时间图的方法，通过该框架生成视频亮点集锦。首先，我们将输入的视频转换为图，其中每个节点对应视频的每一帧。然后，我们通过仅连接那些在指定时间距离内的节点对来对图施加稀疏性。接着，我们将视频摘要任务表述为一个二元节点分类问题，精确地对视频帧进行分类，判断它们是否应属于输出的摘要视频。以这种方式构建的图（如图1所示）旨在捕捉视频帧之间的长程交互，而稀疏性确保了模型在训练时不会遇到内存和计算瓶颈。对两个数据集（SumMe和TVSum）进行的实验表明，与现有的最先进的摘要方法相比，所提出的高效模型在计算时间和内存使用上提高了一个数量级的效率。

![](../Images/46bd7806ce3bad4dd5aef3c2feab206c.png)

(图片来源：作者) 图1：*VideoSAGE从输入视频构建图，每个节点编码一帧。我们将视频摘要问题表述为一个二元节点分类问题。*

下表展示了我们方法（即VideoSAGE）在性能和客观评分上的对比结果。这篇论文最近已被CVPR 2024工作坊接受。论文详细信息和更多结果请见[此处](https://arxiv.org/pdf/2404.10539)。

![](../Images/7eb6d1cadf078899548713e0d781918a.png)

*(图片来源：作者) 表1：（左）与SOTA方法在SumMe和TVSum数据集上的比较，（右）使用A2Summ、PGL-SUM和VideoSAGE进行推理分析。*

## 动作分割：作为一个节点分类问题进行表述

同样地，我们也将动作分割问题作为在这种从输入视频构建的稀疏图中的节点分类问题。GNN结构与上面类似，唯一的区别是最后一层GNN使用的是图注意力网络（GAT），而不是视频摘要中使用的SageConv。我们在50-Salads数据集上进行了实验。我们利用MSTCN或ASFormer作为第一阶段的初始特征提取器。接下来，我们利用我们的稀疏双向GNN模型，该模型利用并行的时间“前向”和“后向”局部消息传递操作。GNN模型进一步优化了我们系统的最终细粒度每帧动作预测。有关结果，请参阅表2。

![](../Images/d311245330649d21ebfc5e28487a4029.png)

*(图片来源：作者) 表2：在50-Salads数据集上进行的动作分割结果，评估指标为F1@0.1和准确率。*

# 视频作为“面向对象”的时空图

在这一部分，我们将描述如何采用类似的基于图的方式，其中节点表示“对象”而不是整个视频帧。我们将从一个具体的示例开始，来描述时空图方法。

![](../Images/e875282b21bc287d0b5ec792a2f090fd.png)

*(图由作者提供) 图 2：我们将视频从视听输入数据转换为标准图，其中每个节点对应于帧中的一个人，边表示节点之间的空间或时间交互。构建的图足够稠密，可以通过跨越时间上远离但相关的节点进行信息传递，来建模长期依赖关系，但又足够稀疏，可以在低内存和计算预算下处理。ASD 任务被设定为这个长范围时空图中的二元节点分类任务。*

## 主动说话人检测：任务被表述为节点分类

图 2 展示了我们为主动说话人检测（ASD）任务设计的框架概览。通过将视听数据作为输入，我们构建了一个多模态图，并将 ASD 转化为一个图节点分类任务。图 3 展示了图的构建过程。首先，我们创建一个图，其中每个节点对应于每帧中的一个人，边表示它们之间的空间或时间关系。初始节点特征是通过简单且轻量级的二维卷积神经网络（CNN）构建的，而不是复杂的 3D CNN 或 Transformer。接下来，我们在这个图的每个节点上执行二分类节点分类，即活跃或非活跃说话人——通过学习一个轻量级的三层图神经网络（GNN）。图是专门为编码不同面部身份之间的空间和时间依赖关系而构建的。因此，GNN 可以利用这一图结构来建模语音中的时间连续性，以及长期的时空上下文，同时需要较低的内存和计算。

你可能会问，为什么图的构建方式是这样的？这就是领域知识的影响所在。之所以在同一时间距离内，具有相同面部 ID 的节点会相互连接，是为了模拟现实世界中的场景：如果一个人在 t=1 时在拍摄，而同一个人在 t=5 时在讲话，那么很有可能他在 t=2、t=3、t=4 时也在讲话。为什么我们要将不同的面部 ID 连接在一起，如果它们共享相同的时间戳？这是因为一般来说，如果一个人在讲话，其他人更可能在听。如果我们将所有节点彼此连接，使得图变得稠密，模型不仅需要巨大的内存和计算资源，而且还会变得嘈杂。

我们在AVA-ActiveSpeaker数据集上进行了广泛的实验。结果表明，SPELL优于所有先前的最先进（SOTA）方法。得益于所构建图谱的稀疏性（约95%），与当时领先的SOTA方法之一ASDNet（48.6M #Params）相比，SPELL在视觉特征编码时需要显著更少的硬件资源（11.2M #Params）。

![](../Images/5cdd5baf9097f49bfd4a92b2a7144280.png)

*(作者提供的图片) 图 3：（a）：我们图谱构建过程的示意图。上面的框架按时间顺序从左到右排列。蓝色、红色和黄色三种颜色表示框架中存在的三种身份。图中的每个节点对应框架中的每个面孔。SPELL通过无向边连接同一框架中不同身份的面孔。SPELL还通过前向/后向/无向边跨框架连接相同身份的面孔（由超参数τ控制）。在这个示例中，相同的身份通过前向边跨框架连接，前向边是有向的，仅朝时间前进的方向延伸。（b）：创建后向图和无向图的过程是相同的，唯一的不同是在前者的情况下，相同身份的边缘朝相反方向延伸，而在后者的情况下则没有有向边。每个节点还包含音频信息，但此处未显示。*

## 时间上下文的长度是多少？

请参阅下方的图 4，展示了我们的方法在两个不同应用中的时间上下文。

SPELL中的超参数τ（在我们的实验中为0.9秒）对跨时间距离较远的节点之间的直接连接施加了额外的约束。连续时间戳之间的面孔身份始终是相互连接的。以下是SPELL的有效时间上下文大小估算。AVA-ActiveSpeaker数据集包含365万帧和530万标注面孔，导致每帧1.45个面孔。平均每帧1.45个面孔，一个按时间顺序排序的包含500到2000个面孔的图谱可以跨越345到1379帧，相当于25帧/秒视频中的13到55秒。换句话说，图中的节点可能存在大约1分钟的时间差，而SPELL能够在有限的内存和计算预算下有效地推理这一长期时间窗口。值得注意的是，[MAAS](https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf)中的时间窗口大小为1.9秒，TalkNet使用最多4秒的长期序列级时间上下文。

关于用于活跃说话者检测的时空图的工作已在 ECCV 2022 上发表。手稿可以在[这里](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22)找到。我们在之前的[博客](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Spatio-Temporal-Graphs-for-Long-Term-Video-Understanding/post/1425258#.Y1oG7jhUOBs.linkedin)中提供了更多细节。

![](../Images/951c6765f91d02fe1eb515f2fc3b7dc2.png)

*(作者提供的图像) 图 4：左图和右图分别展示了我们方法在活跃说话者检测和动作检测应用中的时间支持对比。*

## 动作检测：任务形式为节点分类

Ava 活跃说话者数据集中的 ASD 问题设置可以访问标注的面孔和标注的面部轨迹，作为问题设置的输入。这在很大程度上简化了图的构建，特别是在节点和边的识别方面。对于其他问题，例如动作检测（Action Detection），由于没有提供地面真相的物体（人）位置和轨迹，我们使用预处理来检测物体和物体轨迹，然后利用 SPELL 来进行节点分类问题。类似于之前的情况，我们利用领域知识构建稀疏图。首先构建“面向物体”的图，始终保持底层应用的目标。

平均而言，我们实现了约 90% 的稀疏图；这一点与依赖于密集通用矩阵乘法（GEMM）操作的视觉 transformer 方法有显著区别。我们的稀疏 GNN 使我们能够 (1) 实现比 transformer 方法更好的性能；(2) 在与 transformer 方法相比的 10 倍更长的时间窗口（100 秒 vs 10 秒）中聚合时间上下文；以及 (3) 相比于 transformer 方法节省 2 到 5 倍的计算资源。

# GraVi-T：开源软件库

我们已经开源了我们的软件库 GraVi-T。目前，GraVi-T 支持多种视频理解应用，包括活跃说话者检测、动作检测、时间分割、视频摘要等。请查看我们的开源软件库[GraVi-T](https://github.com/IntelLabs/GraVi-T)了解更多应用。

# 亮点

与 transformers 相比，我们的图方法可以在 10 倍更长的视频上聚合上下文，消耗约 10 倍更少的内存和 5 倍更低的 FLOPs。我们在这个主题（活跃说话者检测）上的首个主要工作已发表于 [ECCV’22](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22)。请关注我们即将在即将召开的 CVPR 2024 上发布的关于视频摘要（即视频高亮集锦创建）的最新成果。

我们将视频建模为稀疏图的方法在多个应用中超越了复杂的SOTA方法，并在多个排行榜中名列前茅。包括2022年ActivityNet，2022年ECCV的Ego4D音频-视频分割挑战赛，2023年CVPR等赛事。用于训练过去挑战赛获胜模型的[源代码](https://intelailabpage.github.io/2023/03/29/gravi-t.html)也包含在我们的开源软件库[GraVi-T](https://intelailabpage.github.io/2023/03/29/gravi-t.html)中。

我们对这个通用、轻量级且高效的框架充满期待，并正在朝着其他新应用方向努力。更多令人兴奋的消息即将发布！！！
