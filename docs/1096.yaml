- en: Large Language Model Performance in Time Series Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01](https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dd978bb41f6e9848459ee4ba10e124fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: How do major LLMs stack up at detecting anomalies or movements in the data when
    given a large set of time series data within the context window?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    ·5 min read·May 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*My thanks to Evan Jolley for his contributions to this research and piece*'
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs clearly excel in natural language processing tasks, their ability
    to analyze patterns in non-textual data, such as time series data, remains less
    explored. As more teams rush to deploy LLM-powered solutions without thoroughly
    testing their capabilities in basic pattern analysis, the task of evaluating the
    performance of these models in this context takes on elevated importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this research, we set out to investigate the following question: given a
    large set of time series data within the context window, how well can LLMs detect
    anomalies or movements in the data? In other words, should you trust your money
    with a stock-picking OpenAI GPT-4 or Anthropic Claude 3 agent? To answer this
    question, we conducted a series of experiments comparing the performance of LLMs
    in detecting anomalous time series patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: All code needed to reproduce these results can be found in this [GitHub repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack/blob/main/LLMTimeseriesTester.py).
  prefs: []
  type: TYPE_NORMAL
- en: Methodology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/68c83586b2ed3d913b5b890c955f72c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A rough sketch of the time series data (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We tasked GPT-4 and Claude 3 with analyzing changes in data points across time.
    The data we used represented specific metrics for different world cities over
    time and was formatted in JSON before input into the models. We introduced random
    noise, ranging from 20–30% of the data range, to simulate real-world scenarios.
    The LLMs were tasked with detecting these movements above a specific percentage
    threshold and identifying the city and date where the anomaly was detected. The
    data was included in this prompt template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 2: The basic prompt template used in our tests*'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing patterns throughout the context window, detecting anomalies across
    a large set of time series simultaneously, synthesizing the results, and grouping
    them by date is no simple task for an LLM; we really wanted to push the limits
    of these models in this test. Additionally, the models were required to perform
    mathematical calculations on the time series, a task that language models generally
    struggle with.
  prefs: []
  type: TYPE_NORMAL
- en: We also evaluated the models’ performance under different conditions, such as
    extending the duration of the anomaly, increasing the percentage of the anomaly,
    and varying the number of anomaly events within the dataset. We should note that
    during our initial tests, we encountered an issue where synchronizing the anomalies,
    having them all occur on the same date, allowed the LLMs to perform better by
    recognizing the pattern based on the date rather than the data movement. When
    [evaluating LLMs](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/),
    careful test setup is extremely important to prevent the models from picking up
    on unintended patterns that could skew results.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/753bd7bf849d92663b828e44eb588a13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Claude 3 significantly outperforms GPT-4 in time series analysis
    (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: In testing, Claude 3 Opus significantly outperformed GPT-4 in detecting time
    series anomalies. Given the nature of the testing, it’s unlikely that this specific
    evaluation was included in the training set of Claude 3 — making its strong performance
    even more impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Results with 50% Spike
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first set of results is based on data where each anomaly was a 50% spike
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10934110b434fcecac3aaec7c8460f7c.png)![](../Images/27e66bed8cd4b26c6c6418cd4c8061d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figures 4a and 4b: GPT-4 and Claude 3 results with 50% spikes (images by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Claude 3 outperformed GPT-4 on the majority of the 50% spike tests, achieving
    accuracies of 50%, 75%, 70%, and 60% across different test scenarios. In contrast,
    GPT-4 Turbo, which we used due to the limited context window of the original GPT-4,
    struggled with the task, producing results of 30%, 30%, 55%, and 70% across the
    same tests.
  prefs: []
  type: TYPE_NORMAL
- en: Results With 90% Spike
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Claude 3’s also led where each anomaly was a 90% spike in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bdf17ca1eb4d8af9f95e6b6353cbce0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: ChatGPT-4 and Claude 3 results with 90% spikes'
  prefs: []
  type: TYPE_NORMAL
- en: Claude 3 Opus consistently picked up the time series anomalies better than GPT-4,
    achieving accuracies of 85%, 70%, 90%, and 85% across different test scenarios.
    If we were actually trusting a language model to analyze data and pick stocks
    to invest in, we would of course want close to 100% accuracy. However, these results
    are impressive. GPT-4 Turbo’s performance ranged from 40–50% accuracy in detecting
    anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: Results With Standard Deviation Pre-Calculated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To assess the impact of mathematical complexity on the models’ performance,
    we did additional tests where the standard deviation was pre-calculated and included
    in the data like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1e107ad2a6bc203f88548a249ab521e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Standard deviation included in prompt'
  prefs: []
  type: TYPE_NORMAL
- en: Since math isn’t a strong suit of large language models at this point, we wanted
    to see if helping the LLM complete a step of the process would help increase accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a1c26926ee386d9948c161f859e7b8f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Standard deviation included in our prompt versus not (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: The change did in fact increase accuracy across three of the four Claude 3 runs.
    Seemingly minor changes like this can help LLMs play to their strengths and greatly
    improve results.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This evaluation provides concrete evidence of Claude’s capabilities in a domain
    that requires a complex combination of retrieval, analysis, and synthesis — though
    the delta between model performance underscores the need for comprehensive evaluations
    before deploying LLMs in high-stakes applications like finance.
  prefs: []
  type: TYPE_NORMAL
- en: While this research demonstrates the [potential of LLMs in time series analysis](https://arize.com/blog-course/large-language-model-performance-in-time-series-analysis/)
    and data analysis tasks, the findings also point to the importance of careful
    test design to ensure accurate and reliable results — particularly since data
    leaks can lead to misleading conclusions about an LLM’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: As always, understanding the strengths and limitations of these models is pivotal
    for harnessing their full potential while mitigating the risks associated with
    their deployment.
  prefs: []
  type: TYPE_NORMAL
