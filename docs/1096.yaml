- en: Large Language Model Performance in Time Series Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型在时间序列分析中的表现
- en: 原文：[https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01](https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01](https://towardsdatascience.com/large-language-model-performance-in-time-series-analysis-4d274b480e24?source=collection_archive---------3-----------------------#2024-05-01)
- en: '![](../Images/dd978bb41f6e9848459ee4ba10e124fe.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd978bb41f6e9848459ee4ba10e124fe.png)'
- en: Image created by author using Dall-E 3
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 Dall-E 3 创建
- en: How do major LLMs stack up at detecting anomalies or movements in the data when
    given a large set of time series data within the context window?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当给定大量时间序列数据并处于上下文窗口中时，主要的 LLM 在检测数据中的异常或变化方面如何表现？
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--4d274b480e24--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    ·5 min read·May 1, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4d274b480e24--------------------------------)
    ·阅读时间：5 分钟·2024年5月1日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*My thanks to Evan Jolley for his contributions to this research and piece*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢 Evan Jolley 对本研究和文章的贡献*'
- en: While LLMs clearly excel in natural language processing tasks, their ability
    to analyze patterns in non-textual data, such as time series data, remains less
    explored. As more teams rush to deploy LLM-powered solutions without thoroughly
    testing their capabilities in basic pattern analysis, the task of evaluating the
    performance of these models in this context takes on elevated importance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LLM 在自然语言处理任务中明显表现优异，但它们在分析非文本数据（如时间序列数据）中的模式的能力仍然不够探索。随着越来越多的团队匆忙部署 LLM
    驱动的解决方案，而没有彻底测试其在基本模式分析中的能力，评估这些模型在此情境下的表现变得更加重要。
- en: 'In this research, we set out to investigate the following question: given a
    large set of time series data within the context window, how well can LLMs detect
    anomalies or movements in the data? In other words, should you trust your money
    with a stock-picking OpenAI GPT-4 or Anthropic Claude 3 agent? To answer this
    question, we conducted a series of experiments comparing the performance of LLMs
    in detecting anomalous time series patterns.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究旨在探讨以下问题：在给定上下文窗口内的大量时间序列数据的情况下，LLM 能多好地检测数据中的异常或变化？换句话说，你是否可以信任一个选股的 OpenAI
    GPT-4 或 Anthropic Claude 3 代理来处理你的资金？为了回答这个问题，我们进行了系列实验，比较了 LLM 在检测异常时间序列模式中的表现。
- en: All code needed to reproduce these results can be found in this [GitHub repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack/blob/main/LLMTimeseriesTester.py).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有重现这些结果所需的代码可以在这个 [GitHub 仓库](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack/blob/main/LLMTimeseriesTester.py)中找到。
- en: Methodology
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: '![](../Images/68c83586b2ed3d913b5b890c955f72c4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68c83586b2ed3d913b5b890c955f72c4.png)'
- en: 'Figure 1: A rough sketch of the time series data (image by author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：时间序列数据的粗略示意图（图像由作者提供）
- en: 'We tasked GPT-4 and Claude 3 with analyzing changes in data points across time.
    The data we used represented specific metrics for different world cities over
    time and was formatted in JSON before input into the models. We introduced random
    noise, ranging from 20–30% of the data range, to simulate real-world scenarios.
    The LLMs were tasked with detecting these movements above a specific percentage
    threshold and identifying the city and date where the anomaly was detected. The
    data was included in this prompt template:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让GPT-4和Claude 3分析数据点随时间的变化。我们使用的数据代表了不同世界城市随时间变化的特定指标，并在输入模型之前以JSON格式整理。我们引入了从数据范围的20%到30%之间的随机噪声，以模拟真实世界的场景。大语言模型的任务是检测这些变化是否超过特定的百分比阈值，并识别异常发生的城市和日期。数据被包含在以下提示模板中：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Figure 2: The basic prompt template used in our tests*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2：我们测试中使用的基本提示模板*'
- en: Analyzing patterns throughout the context window, detecting anomalies across
    a large set of time series simultaneously, synthesizing the results, and grouping
    them by date is no simple task for an LLM; we really wanted to push the limits
    of these models in this test. Additionally, the models were required to perform
    mathematical calculations on the time series, a task that language models generally
    struggle with.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分析整个上下文窗口中的模式、同时检测大量时间序列中的异常、合成结果并按日期进行分组，对于大语言模型来说并非易事；我们在这次测试中真正想要挑战这些模型的极限。此外，模型还需要对时间序列进行数学计算，这是语言模型通常难以处理的任务。
- en: We also evaluated the models’ performance under different conditions, such as
    extending the duration of the anomaly, increasing the percentage of the anomaly,
    and varying the number of anomaly events within the dataset. We should note that
    during our initial tests, we encountered an issue where synchronizing the anomalies,
    having them all occur on the same date, allowed the LLMs to perform better by
    recognizing the pattern based on the date rather than the data movement. When
    [evaluating LLMs](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/),
    careful test setup is extremely important to prevent the models from picking up
    on unintended patterns that could skew results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了模型在不同条件下的表现，如延长异常持续时间、增加异常百分比，以及改变数据集中的异常事件数量。需要指出的是，在我们初步测试中，遇到了一个问题，即将所有异常同步，使它们都发生在同一天，能够让大语言模型通过识别基于日期的模式来表现得更好，而不是通过数据变化。当[评估LLM](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)时，仔细的测试设置非常重要，以防止模型捕捉到可能扭曲结果的意外模式。
- en: Results
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/753bd7bf849d92663b828e44eb588a13.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/753bd7bf849d92663b828e44eb588a13.png)'
- en: 'Figure 3: Claude 3 significantly outperforms GPT-4 in time series analysis
    (image by author)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Claude 3在时间序列分析中显著超越GPT-4（图片由作者提供）
- en: In testing, Claude 3 Opus significantly outperformed GPT-4 in detecting time
    series anomalies. Given the nature of the testing, it’s unlikely that this specific
    evaluation was included in the training set of Claude 3 — making its strong performance
    even more impressive.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试中，Claude 3 Opus在检测时间序列异常方面显著优于GPT-4。鉴于测试的性质，这个特定的评估不太可能包含在Claude 3的训练集中——这使得它的强大表现更加令人印象深刻。
- en: Results with 50% Spike
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 50%波动结果
- en: Our first set of results is based on data where each anomaly was a 50% spike
    in the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一组结果基于每个异常为数据中50%的波动的情况。
- en: '![](../Images/10934110b434fcecac3aaec7c8460f7c.png)![](../Images/27e66bed8cd4b26c6c6418cd4c8061d9.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10934110b434fcecac3aaec7c8460f7c.png)![](../Images/27e66bed8cd4b26c6c6418cd4c8061d9.png)'
- en: 'Figures 4a and 4b: GPT-4 and Claude 3 results with 50% spikes (images by author)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4a和4b：GPT-4和Claude 3在50%波动情况下的结果（图片由作者提供）
- en: Claude 3 outperformed GPT-4 on the majority of the 50% spike tests, achieving
    accuracies of 50%, 75%, 70%, and 60% across different test scenarios. In contrast,
    GPT-4 Turbo, which we used due to the limited context window of the original GPT-4,
    struggled with the task, producing results of 30%, 30%, 55%, and 70% across the
    same tests.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 3在大多数50%波动的测试中超越了GPT-4，分别在不同测试场景下达到了50%、75%、70%和60%的准确率。相比之下，我们使用的GPT-4
    Turbo（因为原始GPT-4的上下文窗口有限）在这一任务上表现不佳，在相同测试中得到了30%、30%、55%和70%的结果。
- en: Results With 90% Spike
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 90%波动结果
- en: Claude 3’s also led where each anomaly was a 90% spike in the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 3在每个异常为数据中90%波动的情况下也表现优异。
- en: '![](../Images/2bdf17ca1eb4d8af9f95e6b6353cbce0.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bdf17ca1eb4d8af9f95e6b6353cbce0.png)'
- en: 'Figure 5: ChatGPT-4 and Claude 3 results with 90% spikes'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：ChatGPT-4和Claude 3在90%波动情况下的结果
- en: Claude 3 Opus consistently picked up the time series anomalies better than GPT-4,
    achieving accuracies of 85%, 70%, 90%, and 85% across different test scenarios.
    If we were actually trusting a language model to analyze data and pick stocks
    to invest in, we would of course want close to 100% accuracy. However, these results
    are impressive. GPT-4 Turbo’s performance ranged from 40–50% accuracy in detecting
    anomalies.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Claude 3 Opus在检测时间序列异常方面始终优于GPT-4，在不同测试场景中达到了85%、70%、90%和85%的准确率。如果我们真要依赖语言模型来分析数据并挑选投资股票，我们当然希望准确率接近100%。然而，这些结果仍然令人印象深刻。GPT-4
    Turbo在检测异常时的准确率介于40%到50%之间。
- en: Results With Standard Deviation Pre-Calculated
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准偏差预先计算的结果
- en: 'To assess the impact of mathematical complexity on the models’ performance,
    we did additional tests where the standard deviation was pre-calculated and included
    in the data like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估数学复杂度对模型性能的影响，我们进行了额外的测试，其中标准偏差被预先计算并包含在数据中，如下所示：
- en: '![](../Images/c1e107ad2a6bc203f88548a249ab521e.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1e107ad2a6bc203f88548a249ab521e.png)'
- en: 'Figure 6: Standard deviation included in prompt'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：标准偏差包含在提示中
- en: Since math isn’t a strong suit of large language models at this point, we wanted
    to see if helping the LLM complete a step of the process would help increase accuracy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数学目前并不是大语言模型的强项，我们想看看是否通过帮助LLM完成过程中的某个步骤能提高准确性。
- en: '![](../Images/a1c26926ee386d9948c161f859e7b8f7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1c26926ee386d9948c161f859e7b8f7.png)'
- en: 'Figure 7: Standard deviation included in our prompt versus not (image by author)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：我们的提示中包含标准偏差与不包含标准偏差的对比（图像来自作者）
- en: The change did in fact increase accuracy across three of the four Claude 3 runs.
    Seemingly minor changes like this can help LLMs play to their strengths and greatly
    improve results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一变化确实提高了四次Claude 3运行中的三次准确率。看似微小的变化可以帮助大语言模型（LLMs）发挥其优势，并大大改善结果。
- en: Takeaways
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结论
- en: This evaluation provides concrete evidence of Claude’s capabilities in a domain
    that requires a complex combination of retrieval, analysis, and synthesis — though
    the delta between model performance underscores the need for comprehensive evaluations
    before deploying LLMs in high-stakes applications like finance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这项评估提供了关于Claude在一个需要复杂组合的检索、分析和综合领域中的能力的具体证据——尽管模型性能的差距突显了在将LLM部署到金融等高风险应用之前进行全面评估的必要性。
- en: While this research demonstrates the [potential of LLMs in time series analysis](https://arize.com/blog-course/large-language-model-performance-in-time-series-analysis/)
    and data analysis tasks, the findings also point to the importance of careful
    test design to ensure accurate and reliable results — particularly since data
    leaks can lead to misleading conclusions about an LLM’s performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项研究展示了[大语言模型在时间序列分析](https://arize.com/blog-course/large-language-model-performance-in-time-series-analysis/)和数据分析任务中的潜力，但研究结果也指出了精心设计测试的重要性，以确保准确和可靠的结果——特别是因为数据泄露可能导致关于LLM性能的误导性结论。
- en: As always, understanding the strengths and limitations of these models is pivotal
    for harnessing their full potential while mitigating the risks associated with
    their deployment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常一样，理解这些模型的优势和局限性对于充分利用它们的潜力，同时减轻其部署所带来的风险至关重要。
