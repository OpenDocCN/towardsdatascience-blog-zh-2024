<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Build a General-Purpose LLM Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Build a General-Purpose LLM Agent</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-general-purpose-ai-agent-c40be49e7400?source=collection_archive---------0-----------------------#2024-12-05">https://towardsdatascience.com/build-a-general-purpose-ai-agent-c40be49e7400?source=collection_archive---------0-----------------------#2024-12-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8bd4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Step-by-Step Guide</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mayamurad?source=post_page---byline--c40be49e7400--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Maya Murad" class="l ep by dd de cx" src="../Images/ef2b6ee189faf7cf50a9ed738d837c4b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*pN0lE7dRou_YrhQv7Mzjiw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c40be49e7400--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mayamurad?source=post_page---byline--c40be49e7400--------------------------------" rel="noopener follow">Maya Murad</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c40be49e7400--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">23</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/42cfd9265f8bf7647a3faccc2fbe4a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*KKKdVTkdU_39D7lrlw4Ugw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">High-level Overview of an LLM Agent.</strong> (Image by author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="da28" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Why build a general-purpose agent? </strong>Because it’s an excellent tool to prototype your use cases and lays the groundwork for designing your own custom agentic architecture.</p><p id="bcbe" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Before we dive in, let’s quickly introduce LLM agents. <em class="og">Feel free to skip ahead.</em></p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0a8a" class="op oq fq bf nj or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">What is an LLM agent?</h1><blockquote class="pk pl pm"><p id="5631" class="nk nl og nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">An LLM agent is a program whose execution logic is controlled by its underlying model.</p></blockquote><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pn"><img src="../Images/96012c20a3e723166e01d95cc009bda4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3B6dghRN0Ut1TCqbvIRSUA.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">From Standalone LLMs to Agentic Systems.</strong> (<em class="po">Image by author)</em></figcaption></figure><p id="9184" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">What sets an LLM agent apart from approaches like few-shot prompting or fixed workflows is its ability to define and adapt the steps required to execute a user’s query. Given access to a set of tools (like code execution or web search), the agent can decide which tool to use, how to use it, and iterate on results based on the output. This adaptability enables the system to handle diverse use cases with minimal configuration.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pp"><img src="../Images/8d08ae4583742848adec12ed3bc2a7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qc9SCzDgBjV7qQPH4zHhxA.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">A Spectrum of Agentic Architectures. </strong>(Image by author)</figcaption></figure><p id="92df" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Agentic architectures exist on a spectrum, ranging from the reliability of fixed workflows to the flexibility of autonomous agents. For instance, a fixed flow like Retrieval-Augmented Generation (<a class="af pq" href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG" rel="noopener ugc nofollow" target="_blank">RAG</a>) can be enhanced with a self-reflection loop, enabling the program to iterate when the initial response falls short. Alternatively, a <a class="af pq" href="https://www.promptingguide.ai/techniques/react" rel="noopener ugc nofollow" target="_blank">ReAct</a> agent can be equipped with fixed flows as tools, offering a flexible yet structured approach. The choice of architecture ultimately depends on the use case and the desired trade-off between reliability and flexibility.</p><p id="7ac9" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">For a deeper overview, check out <a class="af pq" href="https://www.youtube.com/watch?v=F8NKVhkZZWI&amp;t=1s" rel="noopener ugc nofollow" target="_blank">this video</a>.</p><h1 id="fff1" class="op oq fq bf nj or pr gq ot ou ps gt ow ox pt oz pa pb pu pd pe pf pv ph pi pj bk">Let’s build a general-purpose LLM agent from scratch!</h1><h2 id="56f0" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 1. Select the right LLM</h2><p id="22bf" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">Choosing the right model is critical to achieving your desired performance. There are several factors to consider, like licensing, cost, and language support. The most important consideration for building an LLM agent is the model’s performance on key tasks like coding, tool calling, and reasoning. Benchmarks to evaluate include:</p><ul class=""><li id="54b6" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><a class="af pq" href="https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu" rel="noopener ugc nofollow" target="_blank">Massive Multitask Language Understanding (MMLU)</a> (reasoning)</li><li id="2986" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><a class="af pq" href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="noopener ugc nofollow" target="_blank">Berkeley’s Function Calling Leaderboard</a> (tool selection &amp; tool calling)</li><li id="fc49" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><a class="af pq" href="https://evalplus.github.io/leaderboard.html" rel="noopener ugc nofollow" target="_blank">HumanEval</a> and <a class="af pq" href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard" rel="noopener ugc nofollow" target="_blank">BigCodeBench</a> (coding)</li></ul><p id="4e69" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Another crucial factor is the model’s context window. Agentic workflows can eat up a lot of tokens — sometimes 100K or more — a larger context window is really helpful.</p><p id="c6e6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Models to Consider</strong> (at the time of writing)</p><ul class=""><li id="1bd0" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk">Frontier models: <a class="af pq" href="https://platform.openai.com/docs/models#gpt-4o" rel="noopener ugc nofollow" target="_blank">GPT4-o</a>, <a class="af pq" href="https://www.anthropic.com/news/claude-3-5-sonnet" rel="noopener ugc nofollow" target="_blank">Claude 3.5</a></li><li id="2051" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk">Open-source models: <a class="af pq" href="https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf" rel="noopener ugc nofollow" target="_blank">Llama3.2</a>, <a class="af pq" href="https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e" rel="noopener ugc nofollow" target="_blank">Qwen2.5</a>.</li></ul><p id="bd02" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In general, larger models tend to offer better performance, but smaller models that can run locally are still a solid option. With smaller models, you’ll be limited to simpler use cases and might only be able to connect your agent to one or two basic tools.</p><h2 id="99d8" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 2. Define the agent’s control logic (aka communication structure)</h2><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/fbcc79f1334b27e03e7a7701fb7780c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nV1TFBv7PTg4GlimjpkXHg.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">Single Agent Architecture</strong>. (Image by author)</figcaption></figure><p id="70b4" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The main difference between a simple LLM and an agent comes down to the <strong class="nm fr">system prompt</strong>.</p><blockquote class="pk pl pm"><p id="6e64" class="nk nl og nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The <a class="af pq" href="https://promptengineering.org/system-prompts-in-large-language-models/" rel="noopener ugc nofollow" target="_blank">system prompt</a>, in the context of an LLM, is a set of instructions and contextual information provided to the model before it engages with user queries.</p></blockquote><p id="c6cb" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The agentic behavior expected of the LLM can be codified within the system prompt.</p><p id="5244" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Here are some common agentic patterns, which can be customized to fit your needs:</p><ul class=""><li id="968c" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><strong class="nm fr">Tool Use:</strong> The agent determines when to route queries to the appropriate tool or rely on its own knowledge.</li><li id="5b5c" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Reflection:</strong> The agent reviews and corrects its answers before responding to the user. A reflection step can also be added to most LLM systems.</li><li id="3b14" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Reason-then-Act (</strong><a class="af pq" href="https://www.promptingguide.ai/techniques/react" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">ReAct</strong></a><strong class="nm fr">):</strong> The agent iteratively reasons through how to solve the query, performs an action, observes the outcome, and determines whether to take another action or provide a response.</li><li id="a6d2" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Plan-then-Execute:</strong> The agent plans upfront by breaking the task into sub-steps (if needed) and then executes each step.</li></ul><p id="7733" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The last two patterns — <strong class="nm fr">ReAct</strong> and <strong class="nm fr">Plan-then-Execute</strong> — are often the best starting point for building a general-purpose single agent.</p></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/fcc5f1655367edc786adf8f32957c5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*1F63COXi6OHYRiWhN85XFA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">Overview of Common Agentic Patterns.</strong> (Image by author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="aa88" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To implement these behaviors effectively, you’ll need to do some prompt engineering. You might also want to use a <a class="af pq" href="https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">structured generation</strong></a><strong class="nm fr"> </strong>technique. This basically means shaping the LLM’s output to match a specific format or schema, so the agent’s responses stay consistent with the communication style you’re aiming for.</p><p id="59ec" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Example:</strong> Below is a system prompt excerpt for a ReAct style agent from the <a class="af pq" href="https://github.com/i-am-bee/bee-agent-framework/blob/main/src/agents/bee/prompts.ts" rel="noopener ugc nofollow" target="_blank">Bee Agent Framework</a>.</p><pre class="ms mt mu mv mw rc rd re bp rf bb bk"><span id="1a2e" class="rg oq fq rd b bg rh ri l rj rk"># Communication structure<br/>You communicate only in instruction lines. The format is: "Instruction: expected output". You must only use these instruction lines and must not enter empty lines or anything else between instruction lines.<br/>You must skip the instruction lines Function Name, Function Input and Function Output if no function calling is required.<br/><br/>Message: User's message. You never use this instruction line.<br/>Thought: A single-line plan of how to answer the user's message. It must be immediately followed by Final Answer.<br/>Thought: A single-line step-by-step plan of how to answer the user's message. You can use the available functions defined above. This instruction line must be immediately followed by Function Name if one of the available functions defined above needs to be called, or by Final Answer. Do not provide the answer here.<br/>Function Name: Name of the function. This instruction line must be immediately followed by Function Input.<br/>Function Input: Function parameters. Empty object is a valid parameter.<br/>Function Output: Output of the function in JSON format.<br/>Thought: Continue your thinking process.<br/>Final Answer: Answer the user or ask for more information or clarification. It must always be preceded by Thought.<br/><br/>## Examples<br/>Message: Can you translate "How are you" into French?<br/>Thought: The user wants to translate a text into French. I can do that.<br/>Final Answer: Comment vas-tu?</span></pre><h2 id="3fe8" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 3. Define the agent’s core instructions</h2><p id="ff1b" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">We tend to take for granted that LLMs come with a bunch of features right out of the box. Some of these are great, but others might not be exactly what you need. To get the performance you’re after, it’s important to spell out all the features you want — and don’t want — in the system prompt.</p><p id="d1d3" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This could include instructions like:</p><ul class=""><li id="cc52" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><strong class="nm fr">Agent Name and Role:</strong> What the agent is called and what it’s meant to do.</li><li id="165b" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Tone and Conciseness:</strong> How formal or casual it should sound, and how brief it should be.</li><li id="fe75" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">When to Use Tools:</strong> Deciding when to rely on external tools versus the model’s own knowledge.</li><li id="dd52" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Handling Errors:</strong> What the agent should do when something goes wrong with a tool or process.</li></ul><p id="2f36" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Example:</strong> Below is a snippet of the instructions section from the <a class="af pq" href="https://github.com/i-am-bee/bee-agent-framework/blob/main/src/agents/bee/prompts.ts" rel="noopener ugc nofollow" target="_blank">Bee Agent Framework</a>.</p><pre class="ms mt mu mv mw rc rd re bp rf bb bk"><span id="3067" class="rg oq fq rd b bg rh ri l rj rk"># Instructions<br/>User can only see the Final Answer, all answers must be provided there.<br/>You must always use the communication structure and instructions defined above. Do not forget that Thought must be a single-line immediately followed by Final Answer.<br/>You must always use the communication structure and instructions defined above. Do not forget that Thought must be a single-line immediately followed by either Function Name or Final Answer.<br/>Functions must be used to retrieve factual or historical information to answer the message.<br/>If the user suggests using a function that is not available, answer that the function is not available. You can suggest alternatives if appropriate.<br/>When the message is unclear or you need more information from the user, ask in Final Answer.<br/><br/># Your capabilities<br/>Prefer to use these capabilities over functions.<br/>- You understand these languages: English, Spanish, French.<br/>- You can translate and summarize, even long documents.<br/><br/># Notes<br/>- If you don't know the answer, say that you don't know.<br/>- The current time and date in ISO format can be found in the last message.<br/>- When answering the user, use friendly formats for time and date.<br/>- Use markdown syntax for formatting code snippets, links, JSON, tables, images, files.<br/>- Sometimes, things don't go as planned. Functions may not provide useful information on the first few tries. You should always try a few different approaches before declaring the problem unsolvable.<br/>- When the function doesn't give you what you were asking for, you must either use another function or a different function input.<br/>  - When using search engines, you try different formulations of the query, possibly even in a different language.<br/>- You cannot do complex calculations, computations, or data manipulations without using functions.m</span></pre><h2 id="e284" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 4. Define and optimize your core tools</h2><p id="c11c" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">Tools are what give your agents their superpowers. With a narrow set of well-defined tools, you can achieve broad functionality. Key tools to include are code execution, web search, file reading, and data analysis.</p><p id="7e6d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">For each tool, you’ll need to define the following and include it as part of the system prompt:</p><ul class=""><li id="c0d1" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><strong class="nm fr">Tool Name:</strong> A unique, descriptive name for the capability.</li><li id="a006" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Tool Description:</strong> A clear explanation of what the tool does and when to use it. This helps the agent determine when to pick the right tool.</li><li id="3b4e" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Tool Input Schema:</strong> A schema that outlines required and optional parameters, their types, and any constraints. The agent uses this to fill in the inputs it needs based on the user’s query..</li><li id="b3b2" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk">A pointer to where/how to run the tool.</li></ul><p id="e3d1" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Example:</strong> Below is an excerpt of an Arxiv tool implementation from <a class="af pq" href="https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/tools/arxiv/tool.py" rel="noopener ugc nofollow" target="_blank">Langchain Community</a>. This implementation requires an <a class="af pq" href="https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/utilities/arxiv.py" rel="noopener ugc nofollow" target="_blank">ArxivAPIWrapper</a> implementation.</p><pre class="ms mt mu mv mw rc rd re bp rf bb bk"><span id="b7a7" class="rg oq fq rd b bg rh ri l rj rk">class ArxivInput(BaseModel):<br/>    """Input for the Arxiv tool."""<br/><br/>    query: str = Field(description="search query to look up")<br/><br/><br/>class ArxivQueryRun(BaseTool):  # type: ignore[override, override]<br/>    """Tool that searches the Arxiv API."""<br/><br/>    name: str = "arxiv"<br/>    description: str = (<br/>        "A wrapper around Arxiv.org "<br/>        "Useful for when you need to answer questions about Physics, Mathematics, "<br/>        "Computer Science, Quantitative Biology, Quantitative Finance, Statistics, "<br/>        "Electrical Engineering, and Economics "<br/>        "from scientific articles on arxiv.org. "<br/>        "Input should be a search query."<br/>    )<br/>    api_wrapper: ArxivAPIWrapper = Field(default_factory=ArxivAPIWrapper)  # type: ignore[arg-type]<br/>    args_schema: Type[BaseModel] = ArxivInput<br/><br/>    def _run(<br/>        self,<br/>        query: str,<br/>        run_manager: Optional[CallbackManagerForToolRun] = None,<br/>    ) -&gt; str:<br/>        """Use the Arxiv tool."""<br/>        return self.api_wrapper.run(query)p</span></pre><p id="3020" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In certain cases, you’ll need to optimize tools to get the performance you’re looking for. This might involve tweaking the tool name or description with some prompt engineering, setting up advanced configurations to handle common errors, or filtering the tool’s output.</p><h2 id="d324" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 5. Decide on a memory handling strategy</h2><p id="c28b" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">LLMs are limited by their context window — the number of tokens they can “remember” at a time. This memory can fill up fast with things like past interactions in multi-turn conversations, lengthy tool outputs, or extra context the agent is grounded on. That’s why having a solid memory handling strategy is crucial.</p><blockquote class="pk pl pm"><p id="f75c" class="nk nl og nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Memory,</strong> in the context of an agent, refers to the system’s capability to store, recall, and utilize information from past interactions. This enables the agent to maintain context over time, improve its responses based on previous exchanges, and provide a more personalized experience.</p></blockquote><p id="8d84" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">Common Memory Handling Strategies:</strong></p><ul class=""><li id="afae" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><strong class="nm fr">Sliding Memory:</strong> Keep the last <em class="og">k</em> conversation turns in memory and drop the older ones.</li><li id="59f3" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Token Memory:</strong> Keep the last <em class="og">n</em> tokens and forget the rest.</li><li id="af75" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Summarized Memory:</strong> Use the LLM to summarize the conversation at each turn and drop the individual messages.</li></ul><p id="cfbd" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Additionally, you can also have an LLM detect key moments to store in long-term memory. This allows the agent to “remember” important facts about the user, making the experience even more personalized.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="262c" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The five steps we’ve covered so far lay the foundation for setting up an agent. But what happens if we run a user query through our LLM at this stage?</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/c03aedb4ced401c01f08dee317cf332f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gwtct9L3Y3g61LdlLdl9A.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">Answer: you get a raw text output.</strong> (Image by author)</figcaption></figure><p id="5d7d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Here’s an example of what that might look like:</p><pre class="ms mt mu mv mw rc rd re bp rf bb bk"><span id="c825" class="rg oq fq rd b bg rh ri l rj rk">User Message: Extract key insighs from this dataset<br/>Files: bill-of-materials.csv<br/>Thought: First, I need to inspect the columns of the dataset and provide basic data statistics.<br/>Function Name: Python<br/>Function Input: {"language":"python","code":"import pandas as pd\n\ndataset = pd.read_csv('bill-of-materials.csv')\n\nprint(dataset.columns)\nprint(dataset.describe())","inputFiles":["bill-of-materials.csv"]}<br/>Function Output:</span></pre><p id="7776" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">At this point, the agent produces raw text output. So how do we get it to actually execute the next step? That’s where parsing and orchestration come in.</p><h2 id="bce1" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk"><strong class="al">Step 6. Parse the agent’s raw output</strong></h2><blockquote class="pk pl pm"><p id="45f2" class="nk nl og nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">A <strong class="nm fr">parser</strong> is a function that converts raw data into a format your application can understand and work with (like an object with properties)</p></blockquote><p id="fdca" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">For the agent we’re building, the parser needs to recognize the communication structure we defined in <strong class="nm fr">Step 2</strong> and return a structured output, like JSON. This makes it easier for the application to process and execute the agent’s next steps.</p><p id="9827" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><em class="og">Note: some model providers like </em><a class="af pq" href="https://openai.com/index/introducing-structured-outputs-in-the-api/" rel="noopener ugc nofollow" target="_blank"><em class="og">OpenAI</em></a>,<em class="og"> can return parsable outputs by default. For other models, especially open-source ones, this would need to be configured.</em></p><h2 id="1609" class="pw oq fq bf nj px py pz ot qa qb qc ow nt qd qe qf nx qg qh qi ob qj qk ql qm bk">Step 7. Orchestrate the agent’s next step</h2><p id="164e" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">The final step is setting up the orchestration logic. This determines what happens after the LLM outputs a result. Depending on the output, you’ll either:</p><ol class=""><li id="4f91" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rl qt qu bk"><strong class="nm fr">Execute a tool call</strong>, or</li><li id="4a65" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of rl qt qu bk"><strong class="nm fr">Return an answer</strong> — either the final response to the user’s query or a follow-up request for more information.</li></ol><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/2cf3fbfd9faabab0ab8405345f963ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBpr4BfqAgpyQ57GkP3Ubw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx"><strong class="bf nj">Extended single agent architecture.</strong> (Image by author)</figcaption></figure><p id="9955" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">If a tool call is triggered, the tool’s output is sent back to the LLM (as part of its working memory). The LLM would then determine what to do with this new information: either perform another tool call or return an answer to the user.</p><p id="971b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Here’s an example of how this orchestration logic might look in code:</p><pre class="ms mt mu mv mw rc rd re bp rf bb bk"><span id="6981" class="rg oq fq rd b bg rh ri l rj rk">def orchestrator(llm_agent, llm_output, tools, user_query):<br/>    """<br/>    Orchestrates the response based on LLM output and iterates if necessary.<br/><br/>    Parameters:<br/>    - llm_agent (callable): The LLM agent function for processing tool outputs.<br/>    - llm_output (dict): Initial output from the LLM, specifying the next action.<br/>    - tools (dict): Dictionary of available tools with their execution methods.<br/>    - user_query (str): The original user query.<br/><br/>    Returns:<br/>    - str: The final response to the user.<br/>    """<br/>    while True:<br/>        action = llm_output.get("action")<br/><br/>        if action == "tool_call":<br/>            # Extract tool name and parameters<br/>            tool_name = llm_output.get("tool_name")<br/>            tool_params = llm_output.get("tool_params", {})<br/><br/>            if tool_name in tools:<br/>                try:<br/>                    # Execute the tool<br/>                    tool_result = tools[tool_name](**tool_params)<br/>                    # Send tool output back to the LLM agent for further processing<br/>                    llm_output = llm_agent({"tool_output": tool_result})<br/>                except Exception as e:<br/>                    return f"Error executing tool '{tool_name}': {str(e)}"<br/>            else:<br/>                return f"Error: Tool '{tool_name}' not found."<br/><br/>        elif action == "return_answer":<br/>            # Return the final answer to the user<br/>            return llm_output.get("answer", "No answer provided.")<br/><br/>        else:<br/>            return "Error: Unrecognized action type from LLM output."</span></pre><p id="efd4" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><strong class="nm fr">And voilà! </strong>You now have a system capable of handling a wide variety of use cases — from competitive analysis and advanced research to automating complex workflows.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2fbe" class="op oq fq bf nj or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">Where do multi-agent systems come in?</h1><p id="9b73" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">While this generation of LLMs is incredibly powerful, they have a key limitation: <a class="af pq" href="https://arxiv.org/html/2410.18745v1" rel="noopener ugc nofollow" target="_blank">they struggle with information overload</a>. Too much context or too many tools can overwhelm the model, leading to performance issues. A general-purpose single agent will eventually hit this ceiling, especially since agents are notoriously token-hungry.</p><p id="0115" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">For certain use cases, a multi-agent setup might make more sense. By dividing responsibilities across multiple agents, you can avoid overloading the context of a single LLM agent and improve overall efficiency.</p><p id="6c74" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">That said, <strong class="nm fr">a general-purpose single-agent setup is a fantastic starting point for prototyping</strong>. It can help you quickly test your use case and identify where things start to break down. Through this process, you can:</p><ol class=""><li id="848c" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of rl qt qu bk">Understand which parts of the task truly benefit from an agentic approach.</li><li id="4ab5" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of rl qt qu bk">Identify components that can be spun off as standalone processes in a larger workflow.</li></ol><p id="81be" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Starting with a single agent gives you valuable insights to refine your approach as you scale to more complex systems.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3bdb" class="op oq fq bf nj or os gq ot ou ov gt ow ox oy oz pa pb pc pd pe pf pg ph pi pj bk">What is the best way to get started?</h1><p id="cb15" class="pw-post-body-paragraph nk nl fq nm b go qn no np gr qo nr ns nt qp nv nw nx qq nz oa ob qr od oe of fj bk">Ready to dive in and start building? Using a framework can be a great way to quickly test and iterate on your agent configuration.</p><ul class=""><li id="5e7b" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qs qt qu bk"><strong class="nm fr">Planning on using open-source models like Llama 3?</strong> Try <a class="af pq" href="https://github.com/i-am-bee/bee-agent-framework-starter" rel="noopener ugc nofollow" target="_blank">this starter template</a> from the <a class="af pq" href="https://github.com/i-am-bee/bee-agent-framework" rel="noopener ugc nofollow" target="_blank">Bee Agent Framework</a>.</li><li id="c234" class="nk nl fq nm b go qv no np gr qw nr ns nt qx nv nw nx qy nz oa ob qz od oe of qs qt qu bk"><strong class="nm fr">Planning on using frontier models like OpenAI?</strong> Try <a class="af pq" href="https://langchain-ai.github.io/langgraph/how-tos/react-agent-from-scratch/" rel="noopener ugc nofollow" target="_blank">this tutorial</a> from <a class="af pq" href="https://langchain-ai.github.io/langgraph/" rel="noopener ugc nofollow" target="_blank">LangGraph</a>.</li></ul><p id="5678" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk"><em class="og">What’s your experience building general-purpose agents? <br/>Share your in the comments!</em></p></div></div></div></div>    
</body>
</html>