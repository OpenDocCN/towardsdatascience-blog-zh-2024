- en: A High Level Guide to LLM Evaluation Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM评估指标的高级指南
- en: 原文：[https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27](https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27](https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27)
- en: Developing an understanding of a variety of LLM benchmarks & scores, including
    an intuition of when they may be of value for your purpose
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发展对各种LLM基准和评分的理解，包括何时它们可能对你的目的有价值的直觉
- en: '[](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    ·17 min read·Feb 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    ·阅读时间17分钟·2024年2月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/545d60b8b6ed5fb82e706c0b7eb16bdf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/545d60b8b6ed5fb82e706c0b7eb16bdf.png)'
- en: Title card created by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 标题卡由作者创建
- en: It seems that almost on a weekly basis, a new large language model (LLM) is
    launched to the public. With each announcement of an LLM, these providers will
    tout performance numbers that can sound pretty impressive. The challenge that
    I’ve found is that there is a wide breadth of performance metrics that are referenced
    across these press releases. While there are a few that show up more often than
    the others, there unfortunately is not simply one or two “go to” metrics. If you
    want to see a tangible example of this, [check out the page for GPT-4’s performance](https://openai.com/research/gpt-4).
    It references many different benchmarks and scores!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每周都会有一个新的大型语言模型（LLM）向公众发布。每次发布LLM时，这些提供商都会宣扬一些看起来相当令人印象深刻的性能数据。我发现的挑战是，这些新闻稿中引用的性能指标种类繁多。虽然有一些指标比其他指标出现得更频繁，但遗憾的是，并没有简单的“一两个”标准指标。如果你想看到一个实际的例子，[查看GPT-4性能的页面](https://openai.com/research/gpt-4)。它引用了许多不同的基准和评分！
- en: The first natural question one might have is, “Why can’t we simply agree to
    use a single metric?” **In short, there is no clean way to assess LLM performance,
    so each performance metric seeks to provide a quantitative assessment for one
    focused domain**. Additionally, many of these performance metrics have “sub-metrics”
    that calculate the metric slightly differently than the original metric. When
    I originally started performing research for this blog post, my intention was
    to cover every single one of these benchmarks…
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个自然的问题是：“为什么我们不能简单地达成共识，使用一个单一的指标？”**简而言之，没有一种简单的方法可以评估LLM的性能，因此每个性能指标都试图为某个特定领域提供定量评估**。此外，许多这些性能指标还有“子指标”，这些子指标计算方式与原始指标略有不同。当我最初开始为这篇博客文章进行研究时，我的初衷是覆盖所有这些基准……
