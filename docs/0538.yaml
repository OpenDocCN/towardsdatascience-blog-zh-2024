- en: A High Level Guide to LLM Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27](https://towardsdatascience.com/a-high-level-guide-to-llm-evaluation-metrics-fbecd08f725c?source=collection_archive---------6-----------------------#2024-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Developing an understanding of a variety of LLM benchmarks & scores, including
    an intuition of when they may be of value for your purpose
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page---byline--fbecd08f725c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbecd08f725c--------------------------------)
    ·17 min read·Feb 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/545d60b8b6ed5fb82e706c0b7eb16bdf.png)'
  prefs: []
  type: TYPE_IMG
- en: Title card created by the author
  prefs: []
  type: TYPE_NORMAL
- en: It seems that almost on a weekly basis, a new large language model (LLM) is
    launched to the public. With each announcement of an LLM, these providers will
    tout performance numbers that can sound pretty impressive. The challenge that
    I’ve found is that there is a wide breadth of performance metrics that are referenced
    across these press releases. While there are a few that show up more often than
    the others, there unfortunately is not simply one or two “go to” metrics. If you
    want to see a tangible example of this, [check out the page for GPT-4’s performance](https://openai.com/research/gpt-4).
    It references many different benchmarks and scores!
  prefs: []
  type: TYPE_NORMAL
- en: The first natural question one might have is, “Why can’t we simply agree to
    use a single metric?” **In short, there is no clean way to assess LLM performance,
    so each performance metric seeks to provide a quantitative assessment for one
    focused domain**. Additionally, many of these performance metrics have “sub-metrics”
    that calculate the metric slightly differently than the original metric. When
    I originally started performing research for this blog post, my intention was
    to cover every single one of these benchmarks…
  prefs: []
  type: TYPE_NORMAL
