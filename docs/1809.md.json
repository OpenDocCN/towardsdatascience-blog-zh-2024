["```py\n@action(reads=[\"count\"], writes=[\"count\"])\ndef counter(state: State) -> State:\n    return state.update(counter=state.get(\"count\", 0) +1) \n```", "```py\nfrom burr.core import ApplicationBuilder, default, expr\napp = (\n    ApplicationBuilder()\n    .with_actions(\n        count=count, \n        done=done # implementation left out above\n    ).with_transitions(\n        (\"counter\", \"counter\", expr(\"count < 10\")), # Keep counting if the counter is < 10\n        (\"counter\", \"done\", default) # Otherwise, we're done\n    ).with_state(count=0)\n    .with_entrypoint(\"counter\") # we have to start somewhere\n    .build()\n)\n```", "```py\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n```", "```py\n@streaming_action(reads=[\"prompt\", \"chat_history\", \"mode\"], writes=[\"response\"])\nasync def chat_response(\n    state: State, prepend_prompt: str, model: str = \"gpt-3.5-turbo\"\n) -> AsyncGenerator[Tuple[dict, Optional[State]], None]:\n    \"\"\"A simple proxy.\n\n    This massages the chat history to pass the context to OpenAI, \n    streams the result back, and finally yields the completed result \n    with the state update.\n    \"\"\"\n    client = _get_openai_client()\n    # code skipped that prepends a custom prompt and formats chat history\n    chat_history_for_openai = _format_chat_history(\n        state[\"chat_history\"], \n        prepend_final_promprt=prepend_prompt)\n    result = await client.chat.completions.create(\n        model=model, messages=chat_history_api_format, stream=True\n    )\n    buffer = []\n\n    async for chunk in result:\n        chunk_str = chunk.choices[0].delta.content\n        if chunk_str is None:\n            continue\n        buffer.append(chunk_str)\n        yield {\"delta\": chunk_str}, None\n\n    result = {\n        \"response\": {\"content\": \"\".join(buffer), \"type\": \"text\", \"role\": \"assistant\"},\n    }\n    yield result, state.update(**result).append(chat_history=result[\"response\"])\n```", "```py\n# Constructing a graph from actions (labeled by kwargs) and \n# transitions (conditional or default).\ngraph = (\n    GraphBuilder()\n    .with_actions(\n        prompt=process_prompt,\n        check_safety=check_safety,\n        decide_mode=choose_mode,\n        generate_code=chat_response.bind(\n            prepend_prompt=\"Please respond with *only* code and no other text\" \n                \"(at all) to the following\",\n        ),\n        # more left out for brevity\n    )\n    .with_transitions(\n        (\"prompt\", \"check_safety\", default),\n        (\"check_safety\", \"decide_mode\", when(safe=True)),\n        (\"check_safety\", \"unsafe_response\", default),\n        (\"decide_mode\", \"generate_code\", when(mode=\"generate_code\")),\n        # more left out for brevity\n    )\n    .build()\n)\n```", "```py\n# Here we couple more application concerns (telemetry, tracking, etcâ€¦).\napp = ApplicationBuilder()\n  .with_entrypoint(\"prompt\")\n  .with_state(chat_history=[])\n  .with_graph(graph)\n  .with_tracker(project=\"demo_chatbot_streaming\")\n  .with_identifiers(app_id=app_id)\n  .build()\n)\n```", "```py\n# Running the application as you would to test, \n# (in a jupyter notebook, for instance).\naction, streaming_container = await app.astream_result(\n    halt_after=[\"generate_code\", \"unsafe_response\", ...], # terminal actions\n    inputs={\n      \"prompt\": \"Please generate a limerick about Alexander Hamilton and Aaron Burr\"\n    }\n)\n\nasync for item in streaming_container:\n    print(item['delta'], end=\"\")\n```", "```py\n@app.post(\"/response/{project_id}/{app_id}\", response_class=StreamingResponse)\nasync def chat_response(project_id: str, app_id: str, prompt: PromptInput) -> StreamingResponse:\n    \"\"\"A simple API that wraps our Burr application.\"\"\"\n    burr_app = _get_application(project_id, app_id)\n    chat_history = burr_app.state.get(\"chat_history\", [])\n    action, streaming_container = await burr_app.astream_result(\n        halt_after=chat_application.TERMINAL_ACTIONS, inputs=dict(prompt=prompt.prompt)\n    )\n\n    async def sse_generator():\n        yield f\"data: {json.dumps({'type': 'chat_history', 'value': chat_history})}\\n\\n\"\n\n        async for item in streaming_container:\n            yield f\"data: {json.dumps({'type': 'delta', 'value': item['delta']})} \\n\\n\"\n\n    return StreamingResponse(sse_generator())\n```", "```py\n// A simple fetch call with getReader()\nconst response = await fetch(\n      `/api/v0/streaming_chatbot/response/${props.projectId}/${props.appId}`,\n      {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt: currentPrompt })\n      }\n    );\nconst reader = response.body?.getReader();\n```", "```py\n// Datatypes on the frontend. \n// The contract is loose, as nothing in the framework encodes it\ntype Event = {\n  type: 'delta' | 'chat_history';\n};\n\ntype ChatMessageEvent = Event & {\n  value: string;\n};\n\ntype ChatHistoryEvent = Event & {\n  value: ChatItem[];\n};\n```", "```py\n// Loop through, continually getting the stream. \n// For each item, parse it as our desired datatype and react appropriately.\nwhile (true) {\n    const result = await reader.read();\n    if (result.done) {\n      break;\n    }\n    const message = decoder.decode(result.value, { stream: true });\n    message\n      .split('data: ')\n      .slice(1)\n      .forEach((item) => {\n        const event: Event = JSON.parse(item);\n        if (event.type === 'chat_history') {\n          const chatMessageEvent = event as ChatHistoryEvent;\n          setDisplayedChatHistory(chatMessageEvent.value);\n        }\n        if (event.type === 'delta') {\n          const chatMessageEvent = event as ChatMessageEvent;\n          chatResponse += chatMessageEvent.value;\n          setCurrentResponse(chatResponse);\n        }\n      });\n}\n```", "```py\n<!-- More to illustrates the example -->\n<div className=\"flex-1 overflow-y-auto p-4 hide-scrollbar\" id={VIEW_END_ID}>\n  {displayedChatHistory.map((message, i) => (\n    <ChatMessage\n      message={message}\n      key={i}\n    />\n  ))}\n  {isChatWaiting && (\n    <ChatMessage\n      message={{\n        role: ChatItem.role.USER,\n        content: currentPrompt,\n        type: ChatItem.type.TEXT\n      }}\n    />\n  )}\n  {isChatWaiting && (\n    <ChatMessage\n      message={{\n        content: currentResponse,\n        type: ChatItem.type.TEXT,\n        role: ChatItem.role.ASSISTANT\n      }}\n    />\n  )}\n</div>\n<!-- Note: We've left out the isChatWaiting and currentPrompt state fields above, \n see StreamingChatbot.tsx for the full implementation. --> \n```", "```py\npip install \"burr[start]\"\nburr # will open up in a new window\n```"]