<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Data Valuation — A Concise Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Data Valuation — A Concise Overview</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-valuation-a-concise-overview-72e8cf12c755?source=collection_archive---------9-----------------------#2024-12-12">https://towardsdatascience.com/data-valuation-a-concise-overview-72e8cf12c755?source=collection_archive---------9-----------------------#2024-12-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e87b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding the Value of Your Data: Challenges, Methods, and Applications</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tim.wibiral?source=post_page---byline--72e8cf12c755--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tim Wibiral" class="l ep by dd de cx" src="../Images/f086d8efa0d38b798fa3a79e84ca9faa.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*UakiWcsr2yWP6hXHO1kwSw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--72e8cf12c755--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tim.wibiral?source=post_page---byline--72e8cf12c755--------------------------------" rel="noopener follow">Tim Wibiral</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--72e8cf12c755--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3257" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">ChatGPT and similar LLMs were trained on insane amounts of data. OpenAI and Co. scraped the internet, collecting books, articles, and social media posts to train their models. It’s easy to imagine that some of the texts (like scientific or news articles) were more important than others (such as random Tweets). This is true for almost any dataset used to train machine learning models; they contain almost always noisy samples, have wrong labels, or have misleading information.</p><p id="d2e6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The process that tries to understand how important different training samples are for the training process of a machine learning model is called Data Valuation. Data Valuation is also known as Data Attribution, Data Influence Analysis, and Representer Points. There are many different approaches and applications, some of which I will discuss in this article.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ng"><img src="../Images/f4c2402ea83dd0e0b4f3ec08d2d71df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*568ZGYS5o3ZmKSTgOB1FPg.jpeg"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Data Valuation visualized. An importance score is assigned to each training sample. (Image by author.)</figcaption></figure><h1 id="3ed8" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Why do we need Data Valuation?</h1><h2 id="6683" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Data Markets</h2><p id="fbcc" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">AI will become an important economic factor in the coming years, but they are hungry for data. High-quality data is indispensable for training AI models, making it a valuable commodity. This leads to the concept of data markets, where buyers and sellers can trade data for money. Data Valuation is the basis for pricing the data, but there’s a catch: Sellers don’t want to keep their data private until someone buys it, but for buyers, it is hard to understand how important the data of that seller will be without having seen it. To dive deeper into this topic, consider having a look at the papers <a class="af pp" href="https://arxiv.org/abs/1805.08125" rel="noopener ugc nofollow" target="_blank">“A Marketplace for Data: An Algorithmic Solution”</a> and <a class="af pp" href="https://arxiv.org/abs/1208.5258" rel="noopener ugc nofollow" target="_blank">“A Theory of Pricing Private Data”</a>.</p><h2 id="f7d2" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Data Poisoning</h2><p id="15b0" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Data poisoning poses a threat to AI models: Bad actors could try to corrupt training data in a way to harm the machine learning training process. This can be done by subtly changing training samples in a way that’s invisible for humans, but very harmful for AI models. Data Valuation methods can counter this because they naturally assign a very low importance score to harmful samples (no matter if they occur naturally, or by malice).</p><h2 id="94e8" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Explainability</h2><p id="3af8" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">In recent years, explainable AI has gained a lot of traction. The High-Level Experts Group on AI of the EU even <a class="af pp" href="https://data.europa.eu/doi/10.2759/346720" rel="noopener ugc nofollow" target="_blank">calls for the explainability of AI as foundational for creating trustworthy AI</a>. Understanding how important different training samples are for an AI system or a specific prediction of an AI system is important for explaining its behavior.</p><h2 id="90d0" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Active Learning</h2><p id="55ca" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">If we can better understand how important which training samples of a machine learning model are, then we can use this method to acquire new training samples that are more informative for our model. Say, you are training a new large language model and find out that articles from the Portuguese Wikipedia page are super important for your LLM. Then it’s a natural next step to try to acquire more of those articles for your model. In a similar fashion, we used Data Valuation in our paper on <a class="af pp" href="https://arxiv.org/abs/2412.04158" rel="noopener ugc nofollow" target="_blank">“LossVal”</a> to acquire new vehicle crash tests to improve the passive safety systems of cars.</p><h1 id="2937" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Overview of Data Valuation Methods</h1><p id="0c26" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Now we know how useful Data Valuation is for different applications. Next, we will have a look at understanding how Data Valuation works. As described in <a class="af pp" href="https://arxiv.org/abs/2412.04158" rel="noopener ugc nofollow" target="_blank">our paper</a>, Data Valuation methods can be roughly divided into four branches:</p><ul class=""><li id="48ab" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pq pr ps bk">Retraining-Based Approaches</li><li id="b961" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk">Gradient-Based Approaches</li><li id="7262" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk">Data-Based Approaches</li><li id="a285" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk">“Others”</li></ul><h2 id="b395" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Retraining-Based Approaches</h2><p id="479e" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">The common scheme of retraining-based approaches is that they train a machine learning model multiple times to gain insight into the training dynamics of the model, and ultimately, into the importance of each training sample. The most basic approach (<a class="af pp" href="https://www.jstor.org/stable/1268249" rel="noopener ugc nofollow" target="_blank">which was introduced in 1977 by Dennis Cook</a>) simply retrains the machine learning model without a data point to determine the importance of that point. If removing the data point decreases the performance of the machine learning model on a validation dataset, then we know that the data point was bad for the model. Reversely, we know that the data point was good (or informative) for the model if the model’s performance on the validation set increases. Repeat the retraining for each data point, and you have valuable importance scores for your complete dataset. This kind of score is called the Leave-One-Out error (LOO). Completely retraining your machine learning model for every single data point is very inefficient, but viable for simple models and small datasets.</p><p id="51a7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Data Shapley extends this idea using the <a class="af pp" href="https://medium.com/datalab-log/understanding-the-impact-of-features-and-data-through-shapley-values-f235489b0b3e" rel="noopener">Shapley value</a>. The idea was published concurrently by both <a class="af pp" href="https://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf" rel="noopener ugc nofollow" target="_blank">Ghorbani &amp; Zou</a> and by <a class="af pp" href="https://proceedings.mlr.press/v89/jia19a.html" rel="noopener ugc nofollow" target="_blank">Jia et al</a>. in 2019. The Shapley value is a construct from game theory that tells you how much each player of a coalition contributed to the payout. A closer-to-life example is the following: Imagine you share a Taxi with your friends Bob and Alice on the way home from a party. Alice lives very close to your starting destination, but Bob lives much farther away, and you're somewhere in between. Of course, it wouldn’t be fair if each of you pays an equal share of the final price, even though you and Bob drive a longer distance than Alice. The Shapley value solves this, by looking at all the sub-coalitions: What if only you and Alice shared the taxi? What if Bob drove alone? And so on. This way, the Shapley value can help you all three pay a fair share towards the final taxi price. This can also be applied to data: Retrain a machine learning model on different subsets of the training data to fairly assign an “importance” to each of the training samples. Unfortunately, this is extremely inefficient: calculating the exact Shapley values would need more than the O(2ⁿ) retrainings of your machine learning model. However, Data Shapley can be approximated much more efficiently using Monte Carlo methods.</p><p id="1bac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Many alternative methods have been proposed, for example, <a class="af pp" href="https://arxiv.org/abs/2304.07718" rel="noopener ugc nofollow" target="_blank">Data-OOB</a> and <a class="af pp" href="https://arxiv.org/abs/2206.10013" rel="noopener ugc nofollow" target="_blank">Average Marginal Effect (AME)</a>. Retraining-based approaches struggle with large training sets, because of the repeated retraining. Importance scores calculated using retraining can be imprecise because of the effect of randomness in neural networks.</p><h2 id="ac2f" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Gradient-Based Approaches</h2><p id="cf28" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Gradient-based approaches only work for machine learning algorithms based on gradient, such as Artificial Neural Networks or linear and logistic regression.</p><p id="6c0a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Influence functions are a staple in statistics and were <a class="af pp" href="https://www.jstor.org/stable/1268187" rel="noopener ugc nofollow" target="_blank">proposed by Dennis Cook</a>, who was mentioned already above. Influence functions use the Hessian matrix (or an approximation of it) to understand how the model’s performance would change if a certain training sample was left out. Using Influence Functions, there is no need to retrain the model. This works for simple regression models, but also for <a class="af pp" href="https://arxiv.org/abs/1703.04730" rel="noopener ugc nofollow" target="_blank">neural networks</a>. Calculating influence functions is quite inefficient, but approximations have been proposed.</p><p id="d4da" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Alternative approaches, like <a class="af pp" href="https://papers.nips.cc/paper/2020/file/e6385d39ec9394f2f3a354d9d2b88eec-Paper.pdf" rel="noopener ugc nofollow" target="_blank">TraceIn</a> and <a class="af pp" href="https://proceedings.mlr.press/v202/park23c.html" rel="noopener ugc nofollow" target="_blank">TRAK</a> track the gradient updates during the training of the machine learning model. They can successfully use this information to understand how important a data point is for the training without needing to retrain the model. <a class="af pp" href="https://arxiv.org/abs/2405.08217" rel="noopener ugc nofollow" target="_blank">Gradient Similarity</a> is another method that tracks the gradients but uses them to compare the similarity of training and validation gradients.</p><p id="aa2d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For my master’s thesis, I worked on a new gradient-based Data Valuation method that exploits gradient information in the loss function, called <a class="af pp" href="https://arxiv.org/abs/2412.04158" rel="noopener ugc nofollow" target="_blank">LossVal</a>. We introduced a self-weighting mechanism into standard loss functions like mean squared error and cross-entropy loss. This allows to assign importance scores to training samples during the first training run, making gradient tracking, Hessian matrix calculation, and retraining unnecessary, while still achieving state-of-the-art results.</p><h2 id="fe08" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">Data-Based Approaches</h2><p id="7db6" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">All methods we touched on above are centered around a machine learning model. This has the advantage, that they tell you how important training samples are for your specific use case and your specific machine learning model. However, some applications (like Data Markets) can profit from <a class="af pp" href="https://proceedings.neurips.cc/paper/2021/hash/59a3adea76fadcb6dd9e54c96fc155d1-Abstract.html" rel="noopener ugc nofollow" target="_blank">“model-agnostic”</a> importance scores that are not based on a specific machine learning model, but instead solely build upon the data.</p><p id="ed0c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This can be done in different ways. For example, one can analyze the <a class="af pp" href="https://arxiv.org/abs/2305.00054" rel="noopener ugc nofollow" target="_blank">distance</a> between the training set and a clean validation set or use a <a class="af pp" href="https://proceedings.neurips.cc/paper/2021/hash/59a3adea76fadcb6dd9e54c96fc155d1-Abstract.html" rel="noopener ugc nofollow" target="_blank">volume measure</a> to quantify the diversity of the data.</p><h2 id="3670" class="ot ny fq bf nz ou ov ow oc ox oy oz of mr pa pb pc mv pd pe pf mz pg ph pi pj bk">“Others”</h2><p id="4a26" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Under this category, I subsume all methods that do not fit into the other categories. For example, using K-nearest neighbors (KNN) allows a much <a class="af pp" href="https://arxiv.org/abs/2304.01224" rel="noopener ugc nofollow" target="_blank">more efficient computation</a> of Shapley values without retraining. <a class="af pp" href="https://aclanthology.org/2020.sustainlp-1.6/" rel="noopener ugc nofollow" target="_blank">Sub-networks that result from zero-masking</a> can be analyzed to understand the importance of different data points. <a class="af pp" href="https://proceedings.mlr.press/v162/wu22j.html" rel="noopener ugc nofollow" target="_blank">DAVINZ</a> analyzes the change in performance when the training data changes by looking at the generalization boundary. <a class="af pp" href="https://arxiv.org/abs/2303.08114" rel="noopener ugc nofollow" target="_blank">Simfluence</a> runs simulated training runs and can estimate how important each training sample is based on that. <a class="af pp" href="https://arxiv.org/abs/1909.11671" rel="noopener ugc nofollow" target="_blank">Reinforcement learning</a> and <a class="af pp" href="https://dl.acm.org/doi/abs/10.1145/3535508.3545522" rel="noopener ugc nofollow" target="_blank">evolutionary algorithms</a> can also be used for Data Valuation.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf py"><img src="../Images/de366ad6372ef050f2278d6dbde7f94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUAEYSrg7y-VI6ijLHcfYA.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Overview of some more Data Valuation methods. (Screenshot from <a class="af pp" href="https://arxiv.org/abs/2412.04158" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2412.04158</a>)</figcaption></figure><h1 id="f039" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Which Method Should You Choose for Your Application?</h1><p id="75ce" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">As for most machine learning problems, there is no free lunch in Data Valuation. To understand which method is the best for your problem, you need to pay attention to what dataset you use, what model you use, and what kind of noise you find in your dataset. A frequent application of Data Valuation is to identify noisy samples in your dataset. The plots below show how well different methods work for this. First, noise was added to a dataset, then different methods were used to identify the noisy samples. The x-axis shows how many of the samples are noisy. The y-axis shows how well each method was able to detect noisy samples (higher is better).</p><p id="44eb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The first and second plots are taken from the <a class="af pp" href="https://openreview.net/forum?id=eEK99egXeB" rel="noopener ugc nofollow" target="_blank">OpenDataVal paper</a>. They used logistic regression as the base model for the classification task in both plots. In the first plot, the authors shuffled the labels of some of the training samples. Here, Data-OOB outperforms all of the other methods tested.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf pz"><img src="../Images/7a3f97994d875b2a10d30fecc4481cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvJscm5NpehYJPVj5ER02w.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Comparison over multiple datasets after <strong class="bf nz">label noise</strong> was applied, using a logistic regression as the base model. Higher is better. (Screenshot from <a class="af pp" href="https://openreview.net/forum?id=eEK99egXeB" rel="noopener ugc nofollow" target="_blank">OpenDataVal</a>.)</figcaption></figure><p id="d9cf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The next plot paints a very different picture: Datasets and Data Valuation methods stayed the same, but the noise is different! Here, the authors added Gaussian noise to some of the training samples. Now LAVA and KNN Shapley show the best performance.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf qa"><img src="../Images/01e330eefa97488ce358f8bbb128dd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWE676jYviNA7IF2EQQeuQ.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Comparison over multiple datasets after <strong class="bf nz">feature noise </strong>was applied, using a logistic regression as the base model. Higher is better. (Screenshot from <a class="af pp" href="https://openreview.net/forum?id=eEK99egXeB" rel="noopener ugc nofollow" target="_blank">OpenDataVal</a>.)</figcaption></figure><p id="088f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If we use an MLP instead of the logistic regression model, we can observe a similar change. Data-OOB seems to perform much worse now. The plots above only considered classification tasks, but as you can see in the plot below, many methods perform very different when applied to a regression task.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf qb"><img src="../Images/96d64dd63cbbd4aada15f104efa58e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkm3dJv9-m4G4t0qnlc-Tg.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Comparison averaged over multiple datasets, using an MLP as the base model. Higher is better. (Screenshot from <a class="af pp" href="https://arxiv.org/abs/2412.04158" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2412.04158</a>)</figcaption></figure><p id="75ff" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For most practical problems, you don’t know exactly how noisy your samples and features are. This makes it hard to decide what Data Valuation approach you should use. In the best case, you can just try out all of them. Otherwise, I can give you some guidelines:</p><ul class=""><li id="ceae" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pq pr ps bk">If you don’t know what machine learning model you will use: Choose a model-agnostic approach, such as KNN Shapley, LAVA, or SAVA.</li><li id="4241" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk">If you use a simple model, like linear or logistic regression: Data-OOB will be very good. However, those simple models are very efficient and you can try to use computationally expensive but theoretically beneficial methods like Data Shapley.</li><li id="ecf0" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk">If you use an MLP: LossVal probably performs best. However, Data Valuation for deep learning is computationally quite expensive. If it’s not feasible in your case, you can try to use a model-agnostic method.</li></ul><p id="32a4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Keep in mind that this guide is based on a limited selection of Data Valuation methods that were evaluated on noisy sample detection only. If possible, try to test which approach works best for your specific problem, dataset, and model.</p><h1 id="5225" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Current Research Directions</h1><p id="356f" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Currently, there are some research trends in different directions. Some research is being conducted to bring other game theoretic concepts, like the <a class="af pp" href="https://proceedings.mlr.press/v206/wang23e/wang23e.pdf" rel="noopener ugc nofollow" target="_blank">Banzhaf Value</a> or the <a class="af pp" href="https://arxiv.org/abs/2402.01943" rel="noopener ugc nofollow" target="_blank">Winter value</a>, to Data Valuation. Other approaches try to create joint importance scores that include other aspects of the learning process in the valuation, such as the <a class="af pp" href="https://dl.acm.org/doi/abs/10.1145/3461702.3462574" rel="noopener ugc nofollow" target="_blank">learning algorithm</a>. Further approaches work on <a class="af pp" href="https://arxiv.org/abs/2210.08723" rel="noopener ugc nofollow" target="_blank">private</a> (where the data does not have to be disclosed) and <a class="af pp" href="https://arxiv.org/abs/2407.15546" rel="noopener ugc nofollow" target="_blank">personalized</a> Data Valuation (where metadata is used to enrich the data).</p><h1 id="6955" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Conclusion</h1><p id="45bf" class="pw-post-body-paragraph mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd fj bk">Data Valuation is a growing topic, lots of other Data Valuation methods were not mentioned in this article. Data Valuation is a valuable tool for better understanding and interpreting machine learning models. If you want to learn more about Data Valuation, I can recommend the following articles:</p><ul class=""><li id="7d5f" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd pq pr ps bk"><a class="af pp" href="https://link.springer.com/article/10.1007/s10994-023-06495-7" rel="noopener ugc nofollow" target="_blank">Training data influence analysis and estimation: a survey</a> (Hammoudeh &amp; Lowd, 2024)</li><li id="a9cd" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk"><a class="af pp" href="https://www.ijcai.org/proceedings/2022/0782.pdf" rel="noopener ugc nofollow" target="_blank">Data Valuation in Machine Learning: “Ingredients”, Strategies, and Open Challenges</a> (Sim &amp; Xu et al., 2022)</li><li id="9898" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk"><a class="af pp" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/5b047c7d862059a5df623c1ce2982fca-Abstract-Datasets_and_Benchmarks.html" rel="noopener ugc nofollow" target="_blank">OpenDataVal: a Unified Benchmark for Data Valuation</a> (Jian &amp; Liang et al., 2023)</li><li id="72b2" class="mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd pq pr ps bk"><a class="af pp" href="https://github.com/daviddao/awesome-data-valuation" rel="noopener ugc nofollow" target="_blank">Awesome Data Valuation Repository</a></li></ul></div></div></div><div class="ab cb qc qd qe qf" role="separator"><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e295" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="qk">Feel free to get in touch via </em><a class="af pp" href="https://www.linkedin.com/in/tim-wibiral/" rel="noopener ugc nofollow" target="_blank"><em class="qk">LinkedIn</em></a></p></div></div></div></div>    
</body>
</html>