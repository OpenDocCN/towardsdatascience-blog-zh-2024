# å°‘å³æ˜¯å¤šï¼Ÿæ·±åº¦å­¦ä¹ é¢„æµ‹æ¨¡å‹éœ€è¦ç‰¹å¾å‡å°‘å—ï¼Ÿ

> åŸæ–‡ï¼š[`towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30`](https://towardsdatascience.com/is-less-more-do-deep-learning-forecasting-models-need-feature-reduction-25d8968ac15c?source=collection_archive---------7-----------------------#2024-09-30)

## æ˜¯å¦éœ€è¦ç­›é€‰ç‰¹å¾ï¼Œè¿™æ˜¯ä¸€ä¸ªé—®é¢˜

[](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)![Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------) [Philippe Ostiguy, M. Sc.](https://ostiguyphilippe.medium.com/?source=post_page---byline--25d8968ac15c--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--25d8968ac15c--------------------------------) Â·12 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 9 æœˆ 30 æ—¥

--

![](img/d06437fd932d3128b235a26000f217c7.png)

AI å›¾åƒç”±ä½œè€…åœ¨ MidJourney V6.1 ä¸Šåˆ›å»ºã€‚

æ—¶é—´åºåˆ—é¢„æµ‹æ˜¯æ•°æ®ç§‘å­¦ä¸­çš„ä¸€é¡¹å¼ºå¤§å·¥å…·ï¼Œé€šè¿‡å†å²æ¨¡å¼æä¾›æœªæ¥è¶‹åŠ¿çš„æ´å¯Ÿã€‚åœ¨æˆ‘ä»¬ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº† [å¦‚ä½•ä½¿ä½ çš„æ—¶é—´åºåˆ—è‡ªåŠ¨å¹³ç¨³åŒ–](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)ï¼Œè¿™èƒ½æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ã€‚ä½†å¹³ç¨³æ€§åªæ˜¯è§£å†³æ–¹æ¡ˆçš„ä¸€éƒ¨åˆ†ã€‚éšç€æˆ‘ä»¬ç»§ç»­ä¼˜åŒ–é¢„æµ‹æ¨¡å‹ï¼Œå¦ä¸€ä¸ªå…³é”®é—®é¢˜å‡ºç°äº†ï¼šå¦‚ä½•å¤„ç†æ•°æ®å¯èƒ½å‘ˆç°çš„ä¼—å¤šç‰¹å¾ï¼Ÿ

å½“ä½ å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ—¶ï¼Œä½ ç»å¸¸ä¼šå‘ç°è‡ªå·±é¢ä¸´ç€è®¸å¤šå¯èƒ½çš„ç‰¹å¾ï¼Œéœ€è¦å°†å…¶åŒ…å«åœ¨æ¨¡å‹ä¸­ã€‚è™½ç„¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®å¾ˆæœ‰è¯±æƒ‘åŠ›ï¼Œä½†æ·»åŠ æ›´å¤šç‰¹å¾å¹¶ä¸æ€»æ˜¯æ›´å¥½çš„é€‰æ‹©ã€‚è¿™å¯èƒ½ä¼šä½¿ä½ çš„æ¨¡å‹å˜å¾—æ›´åŠ å¤æ‚ï¼Œè®­ç»ƒé€Ÿåº¦å˜æ…¢ï¼Œè€Œä¸ä¸€å®šä¼šæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚

ä½ å¯èƒ½åœ¨æƒ³ï¼šç®€åŒ–ç‰¹å¾é›†æ˜¯å¦é‡è¦ï¼Œç°æœ‰çš„æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿè¿™æ­£æ˜¯æˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­è®¨è®ºçš„å†…å®¹ã€‚

è¿™é‡Œæ˜¯æˆ‘ä»¬å°†è¦è®¨è®ºçš„å†…å®¹çš„ç®€è¦æ€»ç»“ï¼š

+   **æ—¶é—´åºåˆ—ä¸­çš„ç‰¹å¾å‡å°‘** â€” *æˆ‘ä»¬å°†è§£é‡Šæ—¶é—´åºåˆ—åˆ†æä¸­ç‰¹å¾å‡å°‘çš„æ¦‚å¿µä»¥åŠå®ƒçš„é‡è¦æ€§ã€‚*

+   **å®é™…å®æ–½æŒ‡å—** â€” *é€šè¿‡ä½¿ç”¨ Pythonï¼Œæˆ‘ä»¬å°†é€æ­¥è®²è§£å¦‚ä½•è¯„ä¼°å’Œé€‰æ‹©æ—¶é—´åºåˆ—æ¨¡å‹çš„ç‰¹å¾ï¼Œæä¾›ä¼˜åŒ–ä½ æ–¹æ³•çš„å®ç”¨å·¥å…·ã€‚æˆ‘ä»¬è¿˜å°†è¯„ä¼°æ˜¯å¦éœ€è¦å‰Šå‡ç‰¹å¾ä»¥ä¼˜åŒ–æˆ‘ä»¬çš„é¢„æµ‹æ¨¡å‹ã€‚*

ä¸€æ—¦ä½ ç†Ÿæ‚‰äº†åƒå¹³ç¨³æ€§å’Œç‰¹å¾å‡å°‘è¿™æ ·çš„æŠ€æœ¯ï¼Œå¹¶ä¸”æƒ³è¿›ä¸€æ­¥æå‡ä½ çš„æ¨¡å‹ï¼Ÿå¯ä»¥æŸ¥çœ‹è¿™ç¯‡å…³äº[åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ä½¿ç”¨è‡ªå®šä¹‰éªŒè¯æŸå¤±](https://medium.com/@ostiguyphilippe/enhancing-deep-learning-model-evaluation-for-stock-market-forecasting-bea30b905b80)çš„æ–‡ç« ï¼Œä»¥è·å¾—æ›´å¥½çš„è‚¡ç¥¨é¢„æµ‹â€”â€”è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¸‹ä¸€æ­¥ï¼

# æ—¶é—´åºåˆ—ä¸­çš„ç‰¹å¾å‡å°‘ï¼šä¸€ä¸ªç®€å•çš„è§£é‡Š

ç‰¹å¾å‡å°‘å°±åƒæ˜¯æ•´ç†å·¥ä½œåŒºï¼Œä½¿å¾—ä½ æ›´å®¹æ˜“æ‰¾åˆ°æ‰€éœ€çš„ä¸œè¥¿ã€‚åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­ï¼Œè¿™æ„å‘³ç€å‡å°‘æ¨¡å‹ç”¨äºé¢„æµ‹çš„è¾“å…¥å˜é‡ï¼ˆç‰¹å¾ï¼‰çš„æ•°é‡ã€‚ç›®æ ‡æ˜¯ç®€åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶é¢„æµ‹èƒ½åŠ›ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºè¿‡å¤šçš„ç‰¹å¾å’Œç›¸å…³ç‰¹å¾ä¼šä½¿æ¨¡å‹å˜å¾—å¤æ‚ã€ç¼“æ…¢å¹¶é™ä½å‡†ç¡®æ€§ã€‚

å…·ä½“æ¥è¯´ï¼Œç®€åŒ–ç‰¹å¾é›†å¯ä»¥ï¼š

+   **é™ä½å¤æ‚åº¦**ï¼šç‰¹å¾è¾ƒå°‘æ„å‘³ç€æ¨¡å‹æ›´ç®€å•ï¼Œè¿™é€šå¸¸ä½¿å¾—è®­ç»ƒå’Œä½¿ç”¨æ›´å¿«ã€‚

+   **æé«˜æ³›åŒ–èƒ½åŠ›**ï¼šé€šè¿‡å»é™¤å™ªå£°ã€æ¶ˆé™¤ç›¸å…³ç‰¹å¾å¹¶èšç„¦äºå…³é”®ä¿¡æ¯ï¼Œå®ƒå¸®åŠ©æ¨¡å‹å­¦ä¹ çœŸå®çš„æ½œåœ¨æ¨¡å¼ï¼Œè€Œä¸æ˜¯è®°ä½å†—ä½™ä¿¡æ¯ã€‚è¿™å¢å¼ºäº†æ¨¡å‹å°†é¢„æµ‹æ¨å¹¿åˆ°ä¸åŒæ•°æ®é›†çš„èƒ½åŠ›ã€‚

+   **æ›´æ˜“è§£é‡Š**ï¼šç‰¹å¾è¾ƒå°‘çš„æ¨¡å‹é€šå¸¸æ›´å®¹æ˜“è®©äººç±»ç†è§£å’Œè§£é‡Šã€‚

+   **è®¡ç®—æ•ˆç‡**ï¼šè¾ƒå°‘çš„ç‰¹å¾éœ€è¦æ›´å°‘çš„å†…å­˜å’Œå¤„ç†èƒ½åŠ›ï¼Œè¿™å¯¹äºå¤§æ•°æ®é›†æˆ–å®æ—¶åº”ç”¨è‡³å…³é‡è¦ã€‚

è¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„ Python åŒ…å¹¶ä¸ä¼šè‡ªåŠ¨è¿›è¡Œç‰¹å¾å‡å°‘ã€‚è¿™æ˜¯ä½ é€šå¸¸éœ€è¦åœ¨ä½¿ç”¨è¿™äº›åŒ…ä¹‹å‰è‡ªè¡Œå¤„ç†çš„ä¸€ä¸ªæ­¥éª¤ã€‚

ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ¦‚å¿µï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®é™…çš„ä¾‹å­æ¥ä½¿ç”¨æ¥è‡ªç¾å›½è”é‚¦å‚¨å¤‡ç»æµæ•°æ®ï¼ˆFREDï¼‰æ•°æ®åº“çš„çœŸå®ä¸–ç•Œæ—¥æ•°æ®ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œè·³è¿‡æ•°æ®è·å–çš„è¿‡ç¨‹ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)ä¸­è¯¦ç»†ä»‹ç»äº†å¦‚ä½•é€šè¿‡ FRED API è·å–å…è´¹çš„å¯é æ•°æ®ã€‚ä½ å¯ä»¥é€šè¿‡[è¿™ä¸ªè„šæœ¬](https://github.com/philippe-ostiguy/free-fin-data)è·å–æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®ã€‚ä¸€æ—¦ä½ è·å–äº†æ•°æ®ï¼š

+   åœ¨å½“å‰ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª`data`ç›®å½•

```py
mkdir -p /path/to/current_directory/data
```

+   å°†æ•°æ®å¤åˆ¶åˆ°ä½ çš„ç›®å½•ä¸­

```py
cp -R /path/to/fetcher_directory /path/to/current_directory/data
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œè®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ç‰¹å¾å‡å°‘çš„ç¤ºä¾‹ã€‚

æˆ‘ä»¬ä¹‹å‰åœ¨[å¦ä¸€ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/get-free-and-reliable-financial-market-data-machine-learning-ready-246e59b00cea)ä¸­å±•ç¤ºäº†å¦‚ä½•æ¸…ç†ä» FRED API è·å–çš„æ¯æ—¥æ•°æ®ï¼Œå› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œè·³è¿‡è¯¥è¿‡ç¨‹ï¼Œç›´æ¥ä½¿ç”¨æ–‡ç« ä¸­æ­¥éª¤å¾—åˆ°çš„`processed_dataframes`ï¼ˆæ•°æ®æ¡†åˆ—è¡¨ï¼‰ã€‚

```py
import pandas as pd
import os
import warnings
warnings.filterwarnings("ignore")

def is_sp500(df):
    last_date = df['ds'].max()
    last_value = float(df.loc[df['ds'] == last_date, 'value'].iloc[0])
    return 5400 <= last_value <= 5650

initial_model_train = None
for i, df in enumerate(processed_dataframes):
    if df['value'].isna().any():
        continue
    if is_sp500(df):
        initial_model_train = df
        break

TRAIN_SIZE = .90
START_DATE = '2018-10-01'
END_DATE = '2024-09-05'
initial_model_train = initial_model_train.rename(columns={'value': 'price'}).reset_index(drop=True)
initial_model_train['unique_id'] = 'SPY'
initial_model_train['price'] = initial_model_train['price'].astype(float)
initial_model_train['y'] = initial_model_train['price'].pct_change()

initial_model_train = initial_model_train[initial_model_train['ds'] > START_DATE].reset_index(drop=True)
combined_df_all = pd.concat([df.drop(columns=['ds']) for df in processed_dataframes], axis=1)
combined_df_all.columns = [f'value_{i}' for i in range(len(processed_dataframes))]
rows_to_keep = len(initial_model_train)
combined_df_all = combined_df_all.iloc[-rows_to_keep:].reset_index(drop=True)

train_size = int(len(initial_model_train)*TRAIN_SIZE)
initial_model_test = initial_model_train[train_size:]
initial_model_train = initial_model_train[:train_size]
combined_df_test = combined_df_all[train_size:]
combined_df_train = combined_df_all[:train_size]
```

ä½ å¯èƒ½ä¼šé—®ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ŸåŸå› æ˜¯ä¸ºäº†ç¡®ä¿åœ¨åº”ç”¨ä»»ä½•è½¬æ¢æˆ–é™ç»´æŠ€æœ¯ä¹‹å‰ä¸ä¼šå‘ç”Ÿæ•°æ®æ³„æ¼ã€‚

`initial_model_data`åŒ…å«æ ‡å‡†æ™®å°” 500 çš„æ¯æ—¥ä»·æ ¼ï¼ˆæœ€åˆå­˜å‚¨åœ¨`processed_dataframes`ä¸­ï¼‰ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬å°è¯•é¢„æµ‹çš„æ•°æ®ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„æ•°æ®æ˜¯å¹³ç¨³çš„ã€‚å…³äºå¦‚ä½•è‡ªåŠ¨ä½¿æ•°æ®å¹³ç¨³å¹¶æé«˜ 20%æ¨¡å‹è¡¨ç°çš„è¯¦ç»†è§£é‡Šï¼Œè¯·å‚é˜…[è¿™ç¯‡æ–‡ç« ](https://levelup.gitconnected.com/want-to-decrease-your-models-prediction-errors-by-20-follow-this-simple-trick-97354102098e)ã€‚

```py
import numpy as np
from statsmodels.tsa.stattools import adfuller
P_VALUE = 0.05

def replace_inf_nan(series):
    if np.isnan(series.iloc[0]) or np.isinf(series.iloc[0]):
        series.iloc[0] = 0
    mask = np.isinf(series) | np.isnan(series)
    series = series.copy()
    series[mask] = np.nan
    series = series.ffill()
    return series

def safe_convert_to_numeric(series):
    return pd.to_numeric(series, errors='coerce')

tempo_df = pd.DataFrame()
stationary_df_train = pd.DataFrame()
stationary_df_test = pd.DataFrame()

value_columns = [col for col in combined_df_all.columns if col.startswith('value_')]

transformations = ['first_diff', 'pct_change', 'log', 'identity']

def get_first_diff(numeric_series):
    return replace_inf_nan(numeric_series.diff())

def get_pct_change(numeric_series):
    return replace_inf_nan(numeric_series.pct_change())

def get_log_transform(numeric_series):
    return replace_inf_nan(np.log(numeric_series.replace(0, np.nan)))

def get_identity(numeric_series):
    return numeric_series

for index, val_col in enumerate(value_columns):
    numeric_series = safe_convert_to_numeric(combined_df_train[val_col])
    numeric_series_all = safe_convert_to_numeric(combined_df_all[val_col])

    if numeric_series.isna().all():
        continue

    valid_transformations = []

    tempo_df['first_diff'] = get_first_diff(numeric_series)
    tempo_df['pct_change'] = get_pct_change(numeric_series)
    tempo_df['log'] = get_log_transform(numeric_series)
    tempo_df['identity'] = get_identity(numeric_series)

    for transfo in transformations:
        tempo_df[transfo] = replace_inf_nan(tempo_df[transfo])
        series = tempo_df[transfo].dropna()

        if len(series) > 1 and not (series == series.iloc[0]).all():
            result = adfuller(series)
            if result[1] < P_VALUE:
                valid_transformations.append((transfo, result[0], result[1]))

    if valid_transformations:
        if any(transfo == 'identity' for transfo, _, _ in valid_transformations):
            chosen_transfo = 'identity'
        else:
            chosen_transfo = min(valid_transformations, key=lambda x: x[1])[0]

        if chosen_transfo == 'first_diff':
            stationary_df_train[val_col] = get_first_diff(numeric_series_all)
        elif chosen_transfo == 'pct_change':
            stationary_df_train[val_col] = get_pct_change(numeric_series_all)
        elif chosen_transfo == 'log':
            stationary_df_train[val_col] = get_log_transform(numeric_series_all)
        else:
            stationary_df_train[val_col] = get_identity(numeric_series_all)

    else:
        print(f"No valid transformation found for {val_col}")

stationary_df_test = stationary_df_train[train_size:]
stationary_df_train = stationary_df_train[:train_size]

initial_model_train = initial_model_train.iloc[1:].reset_index(drop=True)
stationary_df_train = stationary_df_train.iloc[1:].reset_index(drop=True)
last_train_index = stationary_df_train.index[-1]
stationary_df_test = stationary_df_test.loc[last_train_index + 1:].reset_index(drop=True)
initial_model_test = initial_model_test.loc[last_train_index + 1:].reset_index(drop=True)
```

ç„¶åï¼Œæˆ‘ä»¬å°†ç»Ÿè®¡è‡³å°‘ä¸å¦ä¸€ä¸ªå˜é‡æœ‰ 95%ç›¸å…³ç³»æ•°çš„å˜é‡æ•°é‡ã€‚

```py
CORR_COFF = .95
corr_matrix = stationary_df_train.corr().abs()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)
high_corr = corr_matrix.where(mask).stack()
high_corr = high_corr[high_corr >= CORR_COFF]
unique_cols = set(high_corr.index.get_level_values(0)) | set(high_corr.index.get_level_values(1))
num_high_corr_cols = len(unique_cols)

print(f"\n{num_high_corr_cols}/{stationary_df_train.shape[1]} variables have â‰¥{int(CORR_COFF*100)}% "
      f"correlation with another variable.\n")
```

![](img/3e88d562d5f155546e490822fecc2ca8.png)

é«˜åº¦ç›¸å…³çš„å˜é‡æ•°é‡ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

å¦‚æœ 438 ä¸ªå˜é‡ä¸­æœ‰ 260 ä¸ªå˜é‡ä¸è‡³å°‘å¦ä¸€ä¸ªå˜é‡çš„ç›¸å…³æ€§è¾¾åˆ° 95%æˆ–æ›´é«˜ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚è¿™è¡¨æ˜æ•°æ®é›†ä¸­å­˜åœ¨æ˜¾è‘—çš„å¤šé‡å…±çº¿æ€§ã€‚è¿™ç§å†—ä½™å¯èƒ½ä¼šå¯¼è‡´ä»¥ä¸‹å‡ ä¸ªé—®é¢˜ï¼š

+   ä½¿æ¨¡å‹å˜å¾—å¤æ‚ï¼Œä½†å¹¶æœªå¢åŠ å®è´¨æ€§çš„æ–°ä¿¡æ¯

+   å¯èƒ½å¯¼è‡´å›å½’ç³»æ•°ä¼°è®¡çš„ä¸ç¨³å®š

+   å¢åŠ è¿‡æ‹Ÿåˆçš„é£é™©

+   ä½¿å¾—è§£é‡Šå•ä¸ªå˜é‡çš„å½±å“å˜å¾—å›°éš¾

# ç‰¹å¾è¯„ä¼°ä¸é€‰æ‹©

æˆ‘ä»¬ç†è§£ç‰¹å¾é™ç»´å¯èƒ½å¾ˆé‡è¦ï¼Œä½†æˆ‘ä»¬è¯¥å¦‚ä½•è¿›è¡Œå‘¢ï¼Ÿæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªäº›æŠ€æœ¯ï¼Ÿè¿™äº›æ˜¯æˆ‘ä»¬ç°åœ¨è¦æ¢è®¨çš„é—®é¢˜ã€‚

æˆ‘ä»¬å°†è¦æ¢è®¨çš„ç¬¬ä¸€ç§æŠ€æœ¯æ˜¯ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ã€‚è¿™æ˜¯ä¸€ç§å¸¸è§ä¸”æœ‰æ•ˆçš„é™ç»´æŠ€æœ¯ã€‚PCA è¯†åˆ«ç‰¹å¾ä¹‹é—´çš„çº¿æ€§å…³ç³»ï¼Œå¹¶ä¿ç•™è§£é‡ŠåŸå§‹æ•°æ®é›†ä¸­é¢„å®šç™¾åˆ†æ¯”æ–¹å·®çš„ä¸»æˆåˆ†ã€‚åœ¨æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†`EXPLAINED_VARIANCE`é˜ˆå€¼è®¾ç½®ä¸º 90%ã€‚

```py
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
EXPLAINED_VARIANCE = .9
MIN_VARIANCE = 1e-10

X_train = stationary_df_train.values
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
pca = PCA(n_components=EXPLAINED_VARIANCE, svd_solver='full')
X_train_pca = pca.fit_transform(X_train_scaled)

components_to_keep = pca.explained_variance_ > MIN_VARIANCE
X_train_pca = X_train_pca[:, components_to_keep]

pca_df_train = pd.DataFrame(
    X_train_pca,
    columns=[f'PC{i+1}' for i in range(X_train_pca.shape[1])]
)

X_test = stationary_df_test.values
X_test_scaled = scaler.transform(X_test)
X_test_pca = pca.transform(X_test_scaled)

X_test_pca = X_test_pca[:, components_to_keep]

pca_df_test = pd.DataFrame(
    X_test_pca,
    columns=[f'PC{i+1}' for i in range(X_test_pca.shape[1])]
)

print(f"\nOriginal number of features: {stationary_df_train.shape[1]}")
print(f"Number of components after PCA: {pca_df_train.shape[1]}\n")
```

![](img/5544416f6efe78886454deea283a782d.png)

ä½¿ç”¨ä¸»æˆåˆ†åˆ†æè¿›è¡Œç‰¹å¾é™ç»´ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚

ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼šåœ¨é™ç»´åï¼Œ438 ä¸ªç‰¹å¾ä¸­åªå‰©ä¸‹ 76 ä¸ªæˆåˆ†ï¼ŒåŒæ—¶ä¿æŒäº† 90%çš„æ–¹å·®è§£é‡Šï¼æ¥ä¸‹æ¥æˆ‘ä»¬å°†è½¬å‘ä¸€ç§éçº¿æ€§é™ç»´æŠ€æœ¯ã€‚

[æ—¶é—´åºåˆ—èåˆå˜æ¢å™¨ï¼ˆTFTï¼‰](https://arxiv.org/pdf/1912.09363.pdf)æ˜¯ä¸€ç§ç”¨äºæ—¶é—´åºåˆ—é¢„æµ‹çš„å…ˆè¿›æ¨¡å‹ã€‚å®ƒåŒ…å«å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ï¼Œè¿™æ˜¯æ¨¡å‹çš„ä¸€ä¸ªå…³é”®ç»„ä»¶ã€‚å®ƒä¸“é—¨è®¾è®¡ç”¨äºè‡ªåŠ¨è¯†åˆ«å’Œå…³æ³¨æ•°æ®é›†ä¸­çš„æœ€ç›¸å…³ç‰¹å¾ã€‚é€šè¿‡ä¸ºæ¯ä¸ªè¾“å…¥å˜é‡åˆ†é…å­¦ä¹ åˆ°çš„æƒé‡ï¼Œå®ƒæœ‰æ•ˆåœ°çªå‡ºå“ªäº›ç‰¹å¾å¯¹é¢„æµ‹ä»»åŠ¡çš„è´¡çŒ®æœ€å¤§ã€‚

åŸºäº VSN çš„æ–¹æ³•å°†æ˜¯æˆ‘ä»¬ç¬¬äºŒç§é™ç»´æŠ€æœ¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[PyTorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/)æ¥å®ç°ï¼Œå®ƒå…è®¸æˆ‘ä»¬åˆ©ç”¨ TFT æ¨¡å‹ä¸­çš„å˜é‡é€‰æ‹©ç½‘ç»œã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåŸºç¡€é…ç½®ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä¸æ˜¯åˆ›å»ºæ€§èƒ½æœ€å¼ºçš„æ¨¡å‹ï¼Œè€Œæ˜¯è¯†åˆ«æœ€ç›¸å…³çš„ç‰¹å¾ï¼ŒåŒæ—¶å°½é‡å‡å°‘èµ„æºä½¿ç”¨ã€‚

```py
from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet
from pytorch_forecasting.metrics import QuantileLoss
from lightning.pytorch.callbacks import EarlyStopping
import lightning.pytorch as pl
import torch

pl.seed_everything(42)
max_encoder_length = 32
max_prediction_length = 1
VAL_SIZE = .2
VARIABLES_IMPORTANCE = .8
model_data_feature_sel = initial_model_train.join(stationary_df_train)
model_data_feature_sel = model_data_feature_sel.join(pca_df_train)
model_data_feature_sel['price'] = model_data_feature_sel['price'].astype(float)
model_data_feature_sel['y'] = model_data_feature_sel['price'].pct_change()
model_data_feature_sel = model_data_feature_sel.iloc[1:].reset_index(drop=True)

model_data_feature_sel['group'] = 'spy'
model_data_feature_sel['time_idx'] = range(len(model_data_feature_sel))

train_size_vsn = int((1-VAL_SIZE)*len(model_data_feature_sel))
train_data_feature = model_data_feature_sel[:train_size_vsn]
val_data_feature = model_data_feature_sel[train_size_vsn:]
unknown_reals_origin = [col for col in model_data_feature_sel.columns if col.startswith('value_')] + ['y']

timeseries_config = {
    "time_idx": "time_idx",
    "target": "y",
    "group_ids": ["group"],
    "max_encoder_length": max_encoder_length,
    "max_prediction_length": max_prediction_length,
    "time_varying_unknown_reals": unknown_reals_origin,
    "add_relative_time_idx": True,
    "add_target_scales": True,
    "add_encoder_length": True
}

training_ts = TimeSeriesDataSet(
    train_data_feature,
    **timeseries_config
)
```

`VARIABLES_IMPORTANCE`é˜ˆå€¼è®¾ç½®ä¸º 0.8ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å°†ä¿ç•™ç”±å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ç¡®å®šçš„æ’åå‰ 80%çš„é‡è¦ç‰¹å¾ã€‚æœ‰å…³æ—¶é—´åºåˆ—èåˆå˜æ¢å™¨ï¼ˆTFTï¼‰åŠå…¶å‚æ•°çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[æ–‡æ¡£](https://pytorch-forecasting.readthedocs.io/en/stable/api/pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer.html#pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer)ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è®­ç»ƒ TFT æ¨¡å‹ã€‚

```py
if torch.cuda.is_available():
    accelerator = 'gpu'
    num_workers = 2
else :
    accelerator = 'auto'
    num_workers = 0

validation = TimeSeriesDataSet.from_dataset(training_ts, val_data_feature, predict=True, stop_randomization=True)
train_dataloader = training_ts.to_dataloader(train=True, batch_size=64, num_workers=num_workers)
val_dataloader = validation.to_dataloader(train=False, batch_size=64*5, num_workers=num_workers)

tft = TemporalFusionTransformer.from_dataset(
    training_ts,
    learning_rate=0.03,
    hidden_size=16,
    attention_head_size=2,
    dropout=0.1,
    loss=QuantileLoss()
)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=1e-5, patience=5, verbose=False, mode="min")

trainer = pl.Trainer(max_epochs=20,  accelerator=accelerator, gradient_clip_val=.5, callbacks=[early_stop_callback])
trainer.fit(
    tft,
    train_dataloaders=train_dataloader,
    val_dataloaders=val_dataloader

)
```

æˆ‘ä»¬æ•…æ„è®¾ç½®äº†`max_epochs=20`ï¼Œä»¥é¿å…æ¨¡å‹è®­ç»ƒæ—¶é—´è¿‡é•¿ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å®ç°äº†`early_stop_callback`ï¼Œå¦‚æœæ¨¡å‹åœ¨è¿ç»­ 5 ä¸ª epoch ä¸­æ²¡æœ‰æ”¹å–„ï¼Œå°†åœæ­¢è®­ç»ƒï¼ˆ`patience=5`ï¼‰ã€‚

æœ€åï¼Œä½¿ç”¨è·å¾—çš„æœ€ä½³æ¨¡å‹ï¼Œæˆ‘ä»¬é€‰æ‹©ç”± VSN ç¡®å®šçš„æœ€é‡è¦ç‰¹å¾çš„ 80 ç™¾åˆ†ä½ã€‚

```py
best_model_path = trainer.checkpoint_callback.best_model_path
best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)

raw_predictions = best_tft.predict(val_dataloader, mode="raw", return_x=True)

def get_top_encoder_variables(best_tft,interpretation):
    encoder_importances = interpretation["encoder_variables"]
    sorted_importances, indices = torch.sort(encoder_importances, descending=True)
    cumulative_importances = torch.cumsum(sorted_importances, dim=0)
    threshold_index = torch.where(cumulative_importances > VARIABLES_IMPORTANCE)[0][0]
    top_variables = [best_tft.encoder_variables[i] for i in indices[:threshold_index+1]]
    if 'relative_time_idx' in top_variables:
        top_variables.remove('relative_time_idx')
    return top_variables

interpretation= best_tft.interpret_output(raw_predictions.output, reduction="sum")
top_encoder_vars = get_top_encoder_variables(best_tft,interpretation)

print(f"\nOriginal number of features: {stationary_df_train.shape[1]}")
print(f"Number of features after Variable Selection Network (VSN): {len(top_encoder_vars)}\n")
```

![](img/752a7a4f43f1726c1de6b2236a68e5c7.png)

ä½¿ç”¨å˜é‡é€‰æ‹©ç½‘ç»œè¿›è¡Œç‰¹å¾é™ç»´ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

åŸå§‹æ•°æ®é›†åŒ…å« 438 ä¸ªç‰¹å¾ï¼Œåœ¨åº”ç”¨ VSN æ–¹æ³•åï¼Œä»…å‰©ä¸‹ 1 ä¸ªç‰¹å¾ï¼è¿™ç§å‰§çƒˆçš„é™ç»´æš—ç¤ºäº†å‡ ç§å¯èƒ½æ€§ï¼š

1.  è®¸å¤šåŸå§‹ç‰¹å¾å¯èƒ½æ˜¯å†—ä½™çš„ã€‚

1.  ç‰¹å¾é€‰æ‹©è¿‡ç¨‹å¯èƒ½å·²è¿‡åº¦ç®€åŒ–æ•°æ®ã€‚

1.  ä»…ä½¿ç”¨ç›®æ ‡å˜é‡çš„å†å²å€¼ï¼ˆè‡ªå›å½’æ–¹æ³•ï¼‰å¯èƒ½ä¸åŒ…å«å¤–ç”Ÿå˜é‡çš„æ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œç”šè‡³å¯èƒ½æ›´å¥½ã€‚

# è¯„ä¼°ç‰¹å¾é™ç»´æŠ€æœ¯

åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒåº”ç”¨äºæ¨¡å‹çš„é™ç»´æŠ€æœ¯ã€‚æ¯ç§æ–¹æ³•åœ¨ä¿æŒç›¸åŒæ¨¡å‹é…ç½®çš„åŒæ—¶è¿›è¡Œæµ‹è¯•ï¼Œå”¯ä¸€åŒºåˆ«æ˜¯æ‰€é€‰ç‰¹å¾çš„é™ç»´å¤„ç†ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨[TiDE](https://arxiv.org/pdf/2304.08424.pdf)ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº Transformer çš„å°å‹æœ€å…ˆè¿›æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[NeuralForecast](https://nixtlaverse.nixtla.io/neuralforecast/models.tide.html)æä¾›çš„å®ç°ã€‚åªè¦å®ƒå…è®¸å¤–ç”Ÿå†å²å˜é‡ï¼ŒNeuralForecast çš„ä»»ä½•æ¨¡å‹[è¿™é‡Œ](https://nixtlaverse.nixtla.io/neuralforecast/docs/capabilities/overview.html)éƒ½å¯ä»¥ä½¿ç”¨ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨æ¯æ—¥ SPYï¼ˆæ ‡æ™® 500ETFï¼‰æ•°æ®è®­ç»ƒå’Œæµ‹è¯•ä¸¤ä¸ªæ¨¡å‹ã€‚ä¸¤ä¸ªæ¨¡å‹å°†å…·æœ‰ç›¸åŒçš„ï¼š

1.  è®­ç»ƒ-æµ‹è¯•åˆ†å‰²æ¯”ä¾‹

1.  è¶…å‚æ•°

1.  å•ä¸€æ—¶é—´åºåˆ—ï¼ˆSPYï¼‰

1.  é¢„æµ‹èŒƒå›´ä¸ºä¸€æ­¥ ahead

è¿™ä¸¤ä¸ªæ¨¡å‹ä¹‹é—´å”¯ä¸€çš„åŒºåˆ«æ˜¯ç‰¹å¾å‡å°‘æŠ€æœ¯ã€‚å°±è¿™æ ·ï¼

1.  ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼šåŸå§‹ç‰¹å¾ï¼ˆæ²¡æœ‰ç‰¹å¾å‡å°‘ï¼‰

1.  ç¬¬äºŒä¸ªæ¨¡å‹ï¼šä½¿ç”¨ PCA è¿›è¡Œç‰¹å¾å‡å°‘

1.  ç¬¬ä¸‰ä¸ªæ¨¡å‹ï¼šä½¿ç”¨ VSN è¿›è¡Œç‰¹å¾å‡å°‘

è¯¥è®¾ç½®ä½¿æˆ‘ä»¬èƒ½å¤Ÿéš”ç¦»æ¯ç§ç‰¹å¾å‡å°‘æŠ€æœ¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„é…ç½®è®­ç»ƒä¸‰ä¸ªæ¨¡å‹ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ç‰¹å¾ã€‚

```py
from neuralforecast.models import TiDE
from neuralforecast import NeuralForecast

train_data = initial_model_train.join(stationary_df_train)
train_data = train_data.join(pca_df_train)
test_data = initial_model_test.join(stationary_df_test)
test_data = test_data.join(pca_df_test)

hist_exog_list_origin = [col for col in train_data.columns if col.startswith('value_')] + ['y']
hist_exog_list_pca = [col for col in train_data.columns if col.startswith('PC')] + ['y']
hist_exog_list_vsn = top_encoder_vars

tide_params = {
    "h": 1,
    "input_size": 32,
    "scaler_type": "robust",
    "max_steps": 500,
    "val_check_steps": 20,
    "early_stop_patience_steps": 5
}

model_original = TiDE(
    **tide_params,
    hist_exog_list=hist_exog_list_origin,
)

model_pca = TiDE(
    **tide_params,
    hist_exog_list=hist_exog_list_pca,
)

model_vsn = TiDE(
    **tide_params,
    hist_exog_list=hist_exog_list_vsn,
)

nf = NeuralForecast(
    models=[model_original, model_pca, model_vsn],
    freq='D'
)

val_size = int(train_size*VAL_SIZE)
nf.fit(df=train_data,val_size=val_size,use_init_models=True)
```

ç„¶åï¼Œæˆ‘ä»¬è¿›è¡Œé¢„æµ‹ã€‚

```py
from tabulate import tabulate
y_hat_test_ret = pd.DataFrame()
current_train_data = train_data.copy()

y_hat_ret = nf.predict(current_train_data)
y_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])

for i in range(len(test_data) - 1):
    combined_data = pd.concat([current_train_data, test_data.iloc[[i]]])
    y_hat_ret = nf.predict(combined_data)
    y_hat_test_ret = pd.concat([y_hat_test_ret, y_hat_ret.iloc[[-1]]])
    current_train_data = combined_data

predicted_returns_original = y_hat_test_ret['TiDE'].values
predicted_returns_pca = y_hat_test_ret['TiDE1'].values
predicted_returns_vsn = y_hat_test_ret['TiDE2'].values

predicted_prices_original = []
predicted_prices_pca = []
predicted_prices_vsn = []

for i in range(len(predicted_returns_pca)):
    if i == 0:
        last_true_price = train_data['price'].iloc[-1]
    else:
        last_true_price = test_data['price'].iloc[i-1]
    predicted_prices_original.append(last_true_price * (1 + predicted_returns_original[i]))
    predicted_prices_pca.append(last_true_price * (1 + predicted_returns_pca[i]))
    predicted_prices_vsn.append(last_true_price * (1 + predicted_returns_vsn[i]))

true_values = test_data['price']
methods = ['Original','PCA', 'VSN']
predicted_prices = [predicted_prices_original,predicted_prices_pca, predicted_prices_vsn]

results = []

for method, prices in zip(methods, predicted_prices):
    mse = np.mean((np.array(prices) - true_values)**2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(np.array(prices) - true_values))

    results.append([method, mse, rmse, mae])

headers = ["Method", "MSE", "RMSE", "MAE"]
table = tabulate(results, headers=headers, floatfmt=".4f", tablefmt="grid")

print("\nPrediction Errors Comparison:")
print(table)

with open("prediction_errors_comparison.txt", "w") as f:
    f.write("Prediction Errors Comparison:\n")
    f.write(table)
```

æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ¯æ—¥å›æŠ¥ï¼Œç„¶åå°†è¿™äº›å›æŠ¥è½¬æ¢ä¸ºä»·æ ¼ã€‚è¿™ç§æ–¹æ³•ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ä»·æ ¼è®¡ç®—é¢„æµ‹è¯¯å·®ï¼Œå¹¶å°†å®é™…ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼è¿›è¡Œæ¯”è¾ƒï¼Œå‘ˆç°åœ¨å›¾è¡¨ä¸­ã€‚

![](img/269abd534c5abdfe8194df39136fd090.png)

ä½¿ç”¨ä¸åŒç‰¹å¾å‡å°‘æŠ€æœ¯çš„é¢„æµ‹è¯¯å·®æ¯”è¾ƒã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

TiDE æ¨¡å‹åœ¨åŸå§‹å’Œå‡å°‘ç‰¹å¾é›†ä¸Šè¡¨ç°ç›¸ä¼¼ï¼Œæ­ç¤ºäº†ä¸€ä¸ªå…³é”®çš„è§è§£ï¼šç‰¹å¾å‡å°‘å¹¶æ²¡æœ‰åƒé¢„æœŸé‚£æ ·æé«˜é¢„æµ‹ç²¾åº¦ã€‚è¿™è¡¨æ˜å¯èƒ½å­˜åœ¨ä¸€äº›å…³é”®é—®é¢˜ï¼š

+   ä¿¡æ¯æŸå¤±ï¼šå°½ç®¡ç›®çš„æ˜¯ä¿ç•™é‡è¦æ•°æ®ï¼Œä½†é™ç»´æŠ€æœ¯ä¸¢å¼ƒäº†ä¸é¢„æµ‹ä»»åŠ¡ç›¸å…³çš„ä¿¡æ¯ï¼Œè¿™è§£é‡Šäº†åœ¨å‡å°‘ç‰¹å¾æ—¶é¢„æµ‹æ²¡æœ‰æ”¹è¿›ã€‚

+   æ³›åŒ–å›°éš¾ï¼šä¸åŒç‰¹å¾é›†ä¹‹é—´çš„ä¸€è‡´è¡¨ç°è¡¨æ˜æ¨¡å‹åœ¨æ•æ‰æ½œåœ¨æ¨¡å¼æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œæ— è®ºç‰¹å¾æ•°é‡å¤šå°‘ã€‚

+   å¤æ‚æ€§è¿‡åº¦ï¼šä½¿ç”¨è¾ƒå°‘ç‰¹å¾è·å¾—ç±»ä¼¼ç»“æœè¡¨æ˜ï¼ŒTiDE çš„å¤æ‚æ¶æ„å¯èƒ½è¿‡äºå¤æ‚ã€‚ä¸€ç§æ›´ç®€å•çš„æ¨¡å‹ï¼Œä¾‹å¦‚ ARIMAï¼Œå¯èƒ½è¡¨ç°å¾—åŒæ ·å¥½ã€‚

ç„¶åï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹å›¾è¡¨ï¼Œçœ‹çœ‹æ˜¯å¦èƒ½è§‚å¯Ÿåˆ°ä¸‰ç§é¢„æµ‹æ–¹æ³•ä¸å®é™…ä»·æ ¼ä¹‹é—´æœ‰ä»»ä½•æ˜¾è‘—å·®å¼‚ã€‚

```py
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')
plt.plot(test_data['ds'], true_values, label='True Prices', color='green')
plt.plot(test_data['ds'], predicted_prices_original, label='Predicted Prices', color='red')
plt.legend()
plt.title('SPY Price Forecast Using All Original Feature')
plt.xlabel('Date')
plt.ylabel('SPY Price')
plt.savefig('spy_forecast_chart_original.png', dpi=300, bbox_inches='tight')
plt.close()

plt.figure(figsize=(12, 6))
plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')
plt.plot(test_data['ds'], true_values, label='True Prices', color='green')
plt.plot(test_data['ds'], predicted_prices_pca, label='Predicted Prices', color='red')
plt.legend()
plt.title('SPY Price Forecast Using PCA Dimensionality Reduction')
plt.xlabel('Date')
plt.ylabel('SPY Price')
plt.savefig('spy_forecast_chart_pca.png', dpi=300, bbox_inches='tight')
plt.close()

plt.figure(figsize=(12, 6))
plt.plot(train_data['ds'], train_data['price'], label='Training Data', color='blue')
plt.plot(test_data['ds'], true_values, label='True Prices', color='green')
plt.plot(test_data['ds'], predicted_prices_vsn, label='Predicted Prices', color='red')
plt.legend()
plt.title('SPY Price Forecast Using VSN')
plt.xlabel('Date')
plt.ylabel('SPY Price')
plt.savefig('spy_forecast_chart_vsn.png', dpi=300, bbox_inches='tight')
plt.close()
```

![](img/198959fd7771ea78751d923086b0820d.png)

ä½¿ç”¨æ‰€æœ‰åŸå§‹ç‰¹å¾çš„ SPY ä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

![](img/12c100de5f3352051765fbd2db5be6e0.png)

ä½¿ç”¨ PCA çš„ SPY ä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

![](img/ba26bb6bbb1727805ee38cd8788db4cd.png)

ä½¿ç”¨ VSN çš„ SPY ä»·æ ¼é¢„æµ‹ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚

çœŸå®ä»·æ ¼ä¸é¢„æµ‹ä»·æ ¼ä¹‹é—´çš„å·®å¼‚åœ¨ä¸‰ä¸ªæ¨¡å‹ä¸­çœ‹èµ·æ¥ä¸€è‡´ï¼Œæ€§èƒ½ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„å·®å¼‚ã€‚

# ç»“è®º

æˆ‘ä»¬åšåˆ°äº†ï¼æˆ‘ä»¬æ¢è®¨äº†ç‰¹å¾å‡å°‘åœ¨æ—¶é—´åºåˆ—åˆ†æä¸­çš„é‡è¦æ€§ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªå®ç”¨çš„å®ç°æŒ‡å—ï¼š

+   ç‰¹å¾é™ç»´çš„ç›®æ ‡æ˜¯ç®€åŒ–æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒé¢„æµ‹èƒ½åŠ›ã€‚å…¶å¥½å¤„åŒ…æ‹¬å‡å°‘å¤æ‚æ€§ã€æé«˜æ³›åŒ–èƒ½åŠ›ã€ç®€åŒ–è§£é‡Šå’Œè®¡ç®—æ•ˆç‡ã€‚

+   æˆ‘ä»¬ä½¿ç”¨ FRED æ•°æ®æ¼”ç¤ºäº†ä¸¤ç§é™ç»´æŠ€æœ¯ï¼š

1.  ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰ï¼Œä¸€ç§çº¿æ€§é™ç»´æ–¹æ³•ï¼Œå°†ç‰¹å¾ä» 438 ä¸ªå‡å°‘åˆ° 76 ä¸ªï¼ŒåŒæ—¶ä¿ç•™äº† 90%çš„è§£é‡Šæ–¹å·®ã€‚

1.  æ¥è‡ªæ—¶åºèåˆå˜æ¢å™¨ï¼ˆTemporal Fusion Transformersï¼‰çš„å˜é‡é€‰æ‹©ç½‘ç»œï¼ˆVSNï¼‰ï¼Œä¸€ç§éçº¿æ€§æ–¹æ³•ï¼Œé€šè¿‡è®¾ç½® 80 ç™¾åˆ†ä½é‡è¦æ€§é˜ˆå€¼ï¼Œå°†ç‰¹å¾å¤§å¹…å‡å°‘åˆ°ä»…å‰© 1 ä¸ªã€‚

+   ä½¿ç”¨ TiDE æ¨¡å‹è¿›è¡Œçš„è¯„ä¼°è¡¨æ˜ï¼ŒåŸå§‹ç‰¹å¾é›†å’Œé™ç»´åçš„ç‰¹å¾é›†åœ¨æ€§èƒ½ä¸Šç›¸ä¼¼ï¼Œè¿™è¡¨æ˜ç‰¹å¾é™ç»´å¹¶ä¸æ€»æ˜¯èƒ½æå‡é¢„æµ‹æ€§èƒ½ã€‚è¿™å¯èƒ½æ˜¯ç”±äºé™ç»´è¿‡ç¨‹ä¸­ä¿¡æ¯ä¸¢å¤±ã€æ¨¡å‹éš¾ä»¥æ•æ‰æ½œåœ¨æ¨¡å¼ï¼Œæˆ–å¯èƒ½æ˜¯å¯¹äºè¿™ä¸ªç‰¹å®šçš„é¢„æµ‹ä»»åŠ¡ï¼Œæ›´ç®€å•çš„æ¨¡å‹åŒæ ·æœ‰æ•ˆã€‚

æœ€åä¸€ç‚¹ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰æ¢ç´¢æ‰€æœ‰çš„ç‰¹å¾é™ç»´æŠ€æœ¯ï¼Œä¾‹å¦‚ SHAPï¼ˆSHapley Additive exPlanationsï¼‰ï¼Œå®ƒæä¾›äº†è·¨å¤šç§æ¨¡å‹ç±»å‹çš„ç»Ÿä¸€ç‰¹å¾é‡è¦æ€§åº¦é‡ã€‚å³ä½¿æˆ‘ä»¬æ²¡æœ‰æ”¹è¿›æ¨¡å‹ï¼Œè¿›è¡Œç‰¹å¾ç­›é€‰å¹¶æ¯”è¾ƒä¸åŒé™ç»´æ–¹æ³•çš„æ€§èƒ½ä»ç„¶æ˜¯å€¼å¾—çš„ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºç¡®ä¿åœ¨ä¼˜åŒ–æ¨¡å‹æ•ˆç‡å’Œå¯è§£é‡Šæ€§çš„åŒæ—¶ï¼Œä¸ä¸¢å¤±æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚

åœ¨æœªæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›ç‰¹å¾é™ç»´æŠ€æœ¯åº”ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹ï¼Œæ¯”è¾ƒå®ƒä»¬å¯¹æ€§èƒ½å’Œå¯è§£é‡Šæ€§çš„å½±å“ã€‚æ•¬è¯·å…³æ³¨ï¼

å‡†å¤‡å°†è¿™äº›æ¦‚å¿µä»˜è¯¸å®è·µäº†å—ï¼Ÿä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/philippe-ostiguy/feature-reduction-ts)æ‰¾åˆ°å®Œæ•´çš„ä»£ç å®ç°ã€‚

# å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿè¡¨ç¤ºä½ çš„æ”¯æŒï¼

ğŸ‘ é¼“æŒ 50 æ¬¡

ğŸ¤ é€šè¿‡[LinkedIn](https://www.linkedin.com/in/philippe-ostiguy/)å‘æˆ‘å‘é€è¿æ¥è¯·æ±‚ï¼Œä¿æŒè”ç³»

*ä½ çš„æ”¯æŒæ„ä¹‰é‡å¤§ï¼* ğŸ™
