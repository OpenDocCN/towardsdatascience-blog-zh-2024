- en: How to Cut RAG Costs by 80% Using Prompt Compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04](https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb?source=collection_archive---------0-----------------------#2024-01-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Accelerating Inference With Prompt Compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[![Iulia
    Brezeanu](../Images/f108eeec620ec9be40778dfaceca4e6c.png)](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    [Iulia Brezeanu](https://medium.com/@brezeanu.iulia?source=post_page---byline--877a07c6bedb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--877a07c6bedb--------------------------------)
    ·11 min read·Jan 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc087a93d65e728e01c7bf6895e134c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author. AI Generated.
  prefs: []
  type: TYPE_NORMAL
- en: The inference process is one of the things that greatly increases the money
    and time costs of using large language models. This problem augments considerably
    for longer inputs. Below, you can see the relationship between model performance
    and inference time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66a23213977cf798c55cfa2556c09cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance score vs inference throughput [1]
  prefs: []
  type: TYPE_NORMAL
- en: Fast models, which generate more tokens per second, tend to score lower in the
    Open LLM Leaderboard. Scaling up the model size enables better performance but
    comes at the cost of lower inference throughput. This makes it difficult to deploy
    them in real-life applications [1].
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing LLMs’ speed and reducing resource requirements would allow them to
    be more widely used by individuals or small organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Different solutions are proposed for increasing LLM efficiency; some focus on
    the model architecture or system. However, proprietary models like ChatGPT or
    Claude can be accessed only via APIs, so we cannot change their inner algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss a simple and inexpensive method that relies only on changing
    the input given to the model — prompt…
  prefs: []
  type: TYPE_NORMAL
