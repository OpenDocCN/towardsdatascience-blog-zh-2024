# 使用生成式AI从杂乱数据中获取见解

> 原文：[https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03](https://towardsdatascience.com/using-generative-ai-to-get-insights-from-disorderly-data-af056e5910eb?source=collection_archive---------0-----------------------#2024-09-03)

## 使用大型语言模型提取可操作的见解，即使元数据不准确。

[](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[![Omer Ansari](../Images/4e1ff96eb856567b55f8e7ef7e0278a1.png)](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------) [Omer Ansari](https://medium.com/@oansari?source=post_page---byline--af056e5910eb--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--af056e5910eb--------------------------------) ·32分钟阅读·2024年9月3日

--

![](../Images/4a79fc1bee50dacb05b4ccaac1c06b99.png)

使用生成式AI分析杂乱数据的最佳实践清单

本文分享了我们如何在公司使用生成式AI分析数据以更有效地运营的最佳实践。虽然花了一些时间，但我最终获得了Salesforce的营销、法律、安全和公关团队的批准，才能发布这篇文章。希望这能帮助你加速你的数据分析。

*本文中的所有图表和图形都具有方向性并准确传达概念，但数据已被匿名化。*

# 见解盒子

+   **使用LLM进行数据过滤：** 无需在源头清理数据；可以使用LLM在数据流中进行清洗。

+   **使用GPT进行Python自动化：** 提取、修改和可视化数据通常需要中级的Python技能，但GPT可以自动化并加速这些任务。

+   **领域特定的工单过滤：** 当元数据不可靠时，通过处理这些工单的支持工程师来过滤工单。

+   **可靠的数据提取：** 专注于提取像描述和时间戳这样的可靠字段，因为这些字段不容易出错。

+   **使用GPT进行数据匿名化：** 在将数据发送到公共API之前，使用GPT与开源匿名化库对数据进行匿名化。

+   **精心选择分隔符：** 选择输出分隔符时要小心，确保它们不会干扰语言模型的处理，并通过去除所选的分隔符来清理输入数据。

+   **微调GPT提示以提高准确性：** 在进行全面分析之前，先评估并微调提示在已知工单描述上的表现。

+   **上下文数据限制：** 需要注意 GPT 在处理无关数据块时的上限；保持在识别的限制下方 10% 以内，以避免数据丢失。

+   **与 GPT 头脑风暴 KPI：** 在提取元数据后，使用 GPT 进行头脑风暴并创建有意义的 KPI 以供可视化。

+   **简化数据可视化：** 利用 GPT 编写 Python 代码来创建图表，将分析过程简化并保持版本控制，在一个环境中完成，而不是使用单独的可视化工具。

# 总结

你是否曾经面对大量由人类输入的凌乱和自由格式数据，试图理清其中的含义？这是一项极其令人头痛且耗时的工作，除非你有专门的时间来仔细处理，否则很可能最终只是对数据进行抽样，得到一些表面的洞察，且这些洞察可能是基于不可靠的元数据。通常来说，效果不大。

不难看出，专门用于理清混乱数据的大型语言模型（LLM）可以在这里提供帮助。本文分享了从这种实现中总结的最佳实践，涵盖了多种概念，如使用 GPT 清洗数据、进行分析并创建有用图表的最有效方法，管理个人可识别信息（PII）的方法、生产级提示设计、绕过 GPT 的“前额皮质”瓶颈等！

但在此之前，我想先分享一下这个经历如何完全改变了我原本对数据质量的坚定看法：

我曾经认为，为了提高数据质量，必须从源头解决问题，也就是从参与系统（Systems of Engagement）入手。例如，我曾认为，对于销售 CRM，我们必须确保销售和市场团队一开始就录入高质量的数据和元数据。同样，对于客户支持，我们必须确保客户支持工程师在工单的创建、持续和关闭过程中，选择所有正确的元数据（如工单原因代码、客户影响等）。

在我最近的经历之后，这些信念已经被彻底打破。你完全可以在源头拥有杂乱无章的数据，只要有正确的引导，大型语言模型（LLMs）仍然能够理清数据，得出有意义的洞察！

***无需在源头清洗数据：就像水过滤器一样，你只需将一个 LLM 插入数据流中，进行净化！***

![](../Images/e7da3944a3aa757e1835c3aa5724cf07.png)

GPT 可以像水过滤器一样，接收包含脏元数据的信息并加以净化，从中提取出有意义的洞察。

从长远来看，在源头处填充准确的元数据确实有帮助，尽管要注意，这些过程是需要协调和审核的，且非常耗时。

# 操作原则

为了进行这项分析，我遵循了两个简单的原则：

1.  避免打乱我团队的当前交付：虽然让我团队中的某个成员做这项分析会更容易，但那会打乱团队在其他正在进行项目上的进度。我必须在做产品开发主管的日常工作同时，自己搞定所有的分析工作。

1.  用生成式AI做一切：大语言模型在数据处理方面非常强大，特别是在这个使用场景下，它能够从杂乱的数据中提取价值。它们在编程方面也比我强得多。告诉别人做事并检查结果比进入状态亲自做要容易得多。这样，你即使是兼职工作，也能有所成就。

# 也用GPT来编写分析代码！

**结论：** *提取、修改和可视化数据需要中级的Python编程技能，但现在，GPT可以帮你更快地完成所有这些，甚至可以做到更高质量。快用吧！*

在下面的图片中，我展示了所有需要编写代码来转换数据并调用GPT API以从工单详情中提取洞察的步骤（以绿色字体标出）。最棒的是，我不需要从零开始编写这些代码。我使用GPT帮我编写了它！

![](../Images/adeb7281b8e58274d3dda1229ac9eb41.png)

*基于LLM的工单分析的所有步骤*

# 我是如何进行编码的

虽然我对Python有一定的了解，但使用GPT编写代码让我至少提高了3倍的效率。我通过GPT编写代码的方法非常简单：我并没有使用它来执行任何代码，而是直接告诉GPT数据的样子，让它帮我写代码。我还要求GPT在代码的不同位置自由地插入打印语句，打印出变量的值。然后我将代码复制到我笔记本上的Jupyter Notebook中并执行。例如，我的提示语句大致如下：

**我：** *这是我在分析中会使用的所有文件。我将列举它们，并在提示语中用编号来称呼它们。*

*1\. “All Interacted Tickets.xlsx”*

*2\. “Copy of Ticket Dump — Received from Ops.xlsx”*

*3\. “verifying_accurate_list_of_ops_people_supporting_my_space.xlsx”*

*它们都在../data/目录下。*

*编写Python代码，将文件1、2和3导入到pandas数据框中。忽略所有包含“pivot”字样的工作表，但将其余数据提取出来。用每个Excel文件中工作表本身的文本命名数据框，使用蛇形命名法....*

然后，GPT会输出代码，例如：

```py
import pandas as pd
import os

# Define the file paths
files = {
1\. "All Interacted Tickets.xlsx",
2\. "Copy of Ticket Dump - Received from Ops.xlsx",
3\. "Verifying_accurate_list_of_ops_people_supporting_my_space.xlsx"
}

def snake_case(s):
    return s.lower().replace(" ", "_")

def read_excel_sheets(file_path, ignore_pivot=True):
    xl = pd.ExcelFile(file_path)
    sheet_names = xl.sheet_names
    ….<code clipped>
```

然后，我会将这些代码拿到本地运行。如果出现错误，我会把输出（包括打印语句的输出）粘贴到*“相同的”*聊天中，因为它保留了记忆，GPT‘大多数’情况下能够修复我的问题。不过，在某些情况下，GPT会卡住（你可以通过它不断推荐相同的解决方案来识别这一点），这时我会开始用更多问题来询问它：

**我：** *df = df[1:] 是做什么的*

**GPT：** *df = df[1:] 用于删除数据框的第一行，通常当读取Excel文件时，这一行是表头……*

**我：** *df = df[1:] 是不正确的，我不希望你删除第一行。事实上，这一行应该作为每个数据框的表头。*

因此，如果像我一样使用GPT开发代码，并且是通过带外方式进行的，那么对Python的适度了解有助于解决一些GPT无法理解的代码问题，因为它几乎无法理解上下文。

*请注意，如果你使用多代理框架，代理之间可能会相互传递代码并自动解决这些缺陷。在未来的文章中，我将展示我的本地数据工程和分析环境设置，展示如何在你的笔记本电脑上设置这个多代理框架。如果你对此感兴趣，请在评论中告诉我。*

# 运营票据分析的逐步方法

在经过多次迭代和“失误”后，我提出了以下步骤！换句话说，如果我需要重新进行这项分析，我会遵循以下结构来简化过程。因此，我将这个结构呈现给你，以便你能从中受益。别客气！

# 第一步：筛选相关票据

**核心观点：** *如果元数据不可靠，那么根据处理过票据的支持工程师筛选与你领域相关的票据是最好的选择。*

![](../Images/7f3078578aa1e40a041c4f224d7add9b.png)

*为你的团队筛选票据*

（只有在你在中型或大型组织工作，并且是多个团队之一，使用共享运营团队时，才需要此步骤）

将票据的工作集减少到仅与你的部门或团队相关的部分，是在你的公司中有大量运营票据时必须采取的重要筛选步骤。你将通过LLMs发送这些票据，如果你使用的是像GPT4这样的付费服务，你只想发送与你相关的部分！

然而，当你拥有不良元数据时，推导出有效的票据工作集是一个问题。支持工程师可能没有被指示标明这些票据属于哪些团队，或者没有很好的票据分类供选择，所以你所能使用的仅是一些自由格式的数据和一些自动收集的基本“事实”。这些事实包括谁创建了票据，谁负责处理，票据创建时的时间戳，状态变化（如果幸运的话），以及票据关闭。可能还有其他“主观”数据，例如票据的优先级。收集这些信息是可以的，但它们可能不准确，因为票据创建者往往会将自己打开的每个票据标记为“紧急”或“高优先级”。根据我的经验，通过LLMs推导出实际优先级往往更为中立，尽管这仍然可能出错，后面会进一步讲解。

**换句话说，坚持“事实”。**

在通常有助于你减少工作集的“事实”中，支持工程师的姓名通常是其中之一，支持工程师创建或处理工单的情况也包含在内。由于支持工程师通常专注于特定领域（如数据技术、CRM、Workday 等），第一步是与支持经理合作，识别所有与您领域相关的工单的支持工程师姓名。

然后，使用一个可以识别的键（例如他们的工作邮箱地址），你可以将一堆工单筛选成与你部门相关的子集，并拉取与这些工单相关的“事实”元数据。

完成这一步骤后，你还会得到第一个统计数据：在一段时间内，我所在领域的工单被打开的数量。

# 第 2 步：提取“描述”字段及其他元数据

**关键要点：** *虽然工单创建者可能会搞错很多元数据，但她不能搞错描述字段，因为这是她与支持团队沟通问题及其业务影响的唯一途径。这一点非常重要，因为理解自由流动数据正是 GPT 的强项。因此，专注于提取描述字段以及其他“难以出错”的事实数据，如工单的开始和结束时间等。*

![](../Images/025260409e2575cca5cc8790ed05b061.png)

*通过元数据丰富筛选过的工单，特别是描述字段*

大多数工单系统，如 Jira 服务管理、Zendesk、Service Now 等，允许你下载工单元数据，包括长的、多行的描述字段。（在我工作的公司，我们使用的定制系统不太幸运。）然而，几乎所有这些系统都有一个一次可以下载的最大工单数量。一个更自动化的方式，也是我采取的方式，是通过 API 提取这些数据。在这种情况下，你需要获取 Step1 中由支持工程师处理的已整理好的工单集合，然后循环遍历每个工单，调用 API 来获取其元数据。

其他一些系统允许你通过类似 ODBC 的接口发出 SQL（或者 Salesforce 产品中的 SOQL）查询，这很酷，因为你可以使用 WHERE 子句将第 1 步和第 2 步合并为一步。以下是一个伪代码示例：

```py
SELECT ticket_number, ticket_description, ticket_creation_date, blah blah 
FROM ticket_table 
WHERE ticket_owners include "johndoe@company.com, janedang@company.com" ...
```

你明白了……

将这些数据保存为 MS-Excel 格式，并存储在磁盘上。

***为什么选择 MS-Excel****？我喜欢将表格数据“序列化”成 MS-Excel 格式，因为这样可以避免在将数据导入 Python 代码时遇到的转义或重复分隔符问题。Excel 格式将每个数据点编码到自己的“单元格”中，这样就不会因为文本中包含的特殊字符/分隔符而导致解析错误或列错位。此外，当将这些数据导入 Python 时，我可以使用 Pandas（一种流行的表格数据处理库）通过简单的 Excel 导入选项将 Excel 数据导入到数据框中。*

# 第 3 步：将数据转换为 GPT 友好的格式（JSON）

**总结：** *JSON 既易于人类阅读，又便于机器解析，安全无误，易于排查问题，而且 GPT 可以最少出错地进行操作。此外，当你丰富数据时，你可以继续为相同的 JSON 结构添加新字段。这是非常美妙的！*

```py
"16220417": {
        "description": "Hi Team, \nThe FACT_XYZ_TABLE has not refreshed in time. Typically the data is available at 10am PST, but when I see the job, it has been completing several hours late consistently for the last few weeks. Also, it is 11am, and I see that it is still in running state!\n\n It is critical that this table is completed in time, so we have enough time to prepare an important sales executive report by 8am every morning. Please treat this with urgency.",
        "opened_priority": "P2 - Urgent",
        "origin": "Helpdesk ticket portal",
        "closedDate": "2024-02-26T07:04:54.000Z",
        "createdDate": "2024-02-01T09:03:43.000Z",
        "ownerName": "Tom Cruise (Support Engineer)",
        "ownerEmail": "tcruise@shoddy.com",
        "contactName": "Shoddy Joe (Stakeholder)",
        "contactEmail": "sjoe@shoddy.com",
        "createdByName": "Shoddy Joe (Stakeholder)",
        "createdByEmail": "sjoe@shoddy.com",
        "ticket_status": "closed"
    },
```

*上面的片段展示了一个示例的 JSON 格式化票务元数据，其中票号作为键，指向一个包含进一步键值对元数据的对象。文件中会有很多这种类型的 JSON 块，每个票据一个。*

经过多次试验和调整，我意识到，让 GPT 为我编写数据处理代码的最有效方法是将数据转换为 JSON 格式，并与 GPT 分享这种格式进行操作。将这些数据塞入 pandas 数据框并没有错，甚至可能更容易做到这一点，以高效地处理、清理和转换数据。我最终决定将最终数据集转换为 JSON 的一个重要原因是，因为将表格数据传递给 GPT 提示符是非常繁琐的。对于人类来说，它难以阅读；同时也会为大语言模型（LLM）引入错误，正如下面所述。

当你将表格引入提示符时，必须通过逗号分隔值（CSV）格式来完成。这有两个问题：

1.  由于文本中也可能包含逗号，因此你必须通过将文本放入双引号中来进一步转义这些逗号（例如，“text one”，“text, two”，“test \“hi!\””）。这引入了另一个问题：

1.  如果文本块中包含双引号（“），那么你就必须进一步转义这些双引号。将这些值分隔成单独的列往往会引发问题。

是的，尽管在 JSON 中也需要转义双引号（例如 “key”: “value has \”quotes\””），但是将这个值对齐到列中完全没有问题，因为“key”是唯一标识符。CSV 格式中列对齐有时会出现问题，届时很难排查出错的原因。

使用 JSON 的另一个原因是，你可以清晰地看到并区分当你通过 GPT 在未来的步骤中增强元数据时；它只是将更多的键值对水平添加。你也可以在表格中做到这一点，但这通常需要在你的 IDE 或笔记本中向右滚动。

***小贴士：*** *在未来的某个步骤中，你将把这些数据传入 GPT，并要求它返回由分隔符（例如“|”）分隔的多个字段。因此，现在是删除你传入 JSON 格式的自由格式字段中任何此类分隔符的好时机。你不希望 GPT 在字段中传送“|”字符。*

# 步骤 4：使用简单技巧增强数据（即基本特征工程）

**总结：** *简单的数学分析，如时间差、平均值、标准差，可以使用基础编码轻松且更便宜地完成，因此让 GPT 编写代码来完成这些任务，并在本地运行该代码，而不是将数据发送给 GPT 进行计算。语言模型已经被证明容易犯数学错误，所以最好把它们用于它们擅长的事情。*

首先，我们可以通过聚合其中一些基本信息来增强工单的元数据。这是一个前期步骤，最好用一些简单的代码来完成，而不是浪费 GPT 的积分来处理。

在这种情况下，我们通过将 CreatedTime 从 ClosedTime 中减去来计算工单持续时间。

![](../Images/aeb4719e96d239cba4db56479bca4677.png)

*从左到右，JSON 显示通过基本数据聚合/增强进行水化处理*

# 第 6 步：主菜：GPT 驱动的数据增强（增强的特征工程）

现在我们进入主菜部分。如何使用 GPT 转换原始数据，并从中派生出复杂且结构化的元数据，从这些元数据中可以提取出洞察。在数据科学领域，这一步叫做特征工程。

## 6.1：预处理：模糊化敏感信息（可选）

**总结：** *让 GPT 使用开源匿名化库并开发代码，在将数据发送到公共 API 服务之前进行匿名化处理。*

![](../Images/ac37bfa07b5d49b68c3675ad9d33a925.png)

图片来源：[Kyle Glenn](https://unsplash.com/@kylejglenn?utm_source=medium&utm_medium=referral) 通过 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

这一步适用于你在使用 OpenAI 而不是本地开源 LLM 的情况，因为数据会保留在你的笔记本电脑上。在未来的文章中，我会展示我的数据工程和分析本地环境设置，其中展示了一种开源 LLM 选项。

在我工作的公司，我们有一个安全的代理网关，既可以连接 OpenAI，也可以连接内部训练的 LLM，并且它能够掩盖个人身份信息（PII），并在可信边界内操作 OpenAI。这很方便，因为我可以将所有内部信息发送到这个代理，并以安全的方式享受 OpenAI 模型的优势。

然而，我意识到并非所有公司都有这个奢侈的条件。因此，我在这里增加了一个可选步骤来模糊化个人身份信息（PII）或其他敏感数据。所有这一切的美妙之处在于，GPT 知道这些库并可以用来编写代码，来模糊化数据！

我评估了五个库用于此目的，但我寻找的关键特性是能够将敏感信息转换为匿名数据，然后还能将其重新转换回来。我只找到了以下具有此功能的库。

+   Microsoft Presidio [[link](https://github.com/microsoft/presidio)]（使用实体映射的概念）

+   Gretel synthetics [[link](https://github.com/gretelai/gretel-synthetics)]（使用“分词器”概念）

在这两者中，Presidio是我最喜欢的。我一直对微软在过去十年中做出的“高质量”开源贡献印象深刻。这套Python库也不例外。它具备开箱即用的能力，能够识别PII（个人身份信息）类型的数据，并可以定制和指定需要被匿名化的其他数据。

这是一个例子：

原始文本：

```py
('Peter gave his book to Heidi which later gave it to Nicole. 
Peter lives in London and Nicole lives in Tashkent.')
```

匿名化测试：

```py
'<PERSON_1> gave his book to <PERSON_2> which later gave it to <PERSON_0>. 
<PERSON_1> lives in <LOCATION_1> and <PERSON_0> lives in <LOCATION_0>.`
```

这些数据可以发送给GPT进行分析。当它返回结果时，你可以通过映射进行去匿名化：

*实体映射*

```py
{ 'LOCATION': {'London': '<LOCATION_1>', 'Tashkent': '<LOCATION_0>'},
 'PERSON': { 'Heidi': '<PERSON_2>',
 'Nicole': '<PERSON_0>',
 'Peter': '<PERSON_1>'}
}
```

使用实体映射，文本可以被去匿名化：

*去匿名化的文本：*

```py
('Peter gave his book to Heidi which later gave it to Nicole. 
Peter lives in London and Nicole lives in Tashkent.')
```

我推荐查看这个[笔记本](https://github.com/microsoft/presidio/blob/main/docs/samples/python/pseudonomyzation.ipynb)，它会引导你如何实现这种方法。

请注意，除了PII，可能需要模糊处理的其他信息包括系统信息（IP地址、DNS名称等）和数据库细节（如名称、模式等）

现在我们有了一个机制来匿名化敏感数据，下一步是创建一个高质量的提示来处理这些数据。

## 6.2 预处理：清理输入数据

**总结：** *在选择输出分隔符时要考虑周到，因为某些特殊字符在语言模型中有“意义”。然后，你可以放心地通过去除你选择的分隔符来清理原始输入。*

**问题：** 当请求基于文本的接口（如LLM）返回表格数据时，必须告诉它使用分隔符输出数据（例如csv或tsv格式）。假设你要求GPT输出汇总数据（即“特征”）并使用逗号分隔值。问题在于，输入的票据数据是原始且不可预测的，有人可能在描述中使用了逗号。从技术上讲，这不应该是问题，因为GPT会处理这些数据并去除进入的数据中的逗号，但仍然存在风险，GPT可能会在输出中使用部分原始数据（包括逗号），例如在简短的总结中。经验丰富的数据工程师们可能现在已经意识到这个问题。当你的数据值本身包含应该用来分隔它们的分隔符时，就会出现各种处理问题。

有人可能会问：为什么不通过将值用双引号括起来来转义这些字符呢？例如：

*“key” : “this, is the value, with all these characters !#@$| escaped” .*

这里有一个问题。用户可能也在他们的数据中输入了双引号！

*“key” : “this is a ”value” with double quotes, and it is a problem!”*

是的，确实有解决这个问题的方法，比如使用多行正则表达式，但它们会让代码变得复杂，也使得GPT修复缺陷变得更加困难。因此，处理这个问题最简单的方法是选择一个输出分隔符，这样即使从输入中去除它，也不会对数据上下文产生太大影响，然后再将其从输入数据中去除！

我还尝试了一些分隔符，这些分隔符在输入数据中肯定不会出现，如|%|，但我很快意识到这些分隔符会快速消耗输出的标记限制，因此最终放弃了这种方法。

这里是我测试过的一些分隔符

![](../Images/aca1199ad8e405e8afb806caefa6ca38.png)

最终，我选择了管道“|”分隔符，因为这是大多数利益相关者在表述问题时不会在工单描述中使用的符号。

之后，我让GPT编写了一些额外的代码，清理每个工单描述中的“|”符号。

# 6.3 — 提示性能调优

**概述：** *在运行GPT数据分析提示之前，先评估其在已知输出的工单描述集上的表现，微调提示并反复迭代，直到获得最佳性能评分。*

![](../Images/af718fc5daecca0a49290beed5fec94c.png)

通过措施迭代改进提示

**目标：** 让GPT读取客户编写的工单描述，并仅通过该描述提取以下元数据，之后可以对其进行汇总和可视化：

1.  描述性标题概述问题

1.  商业影响*

1.  工单严重性*

1.  工单复杂度

1.  受影响的利益相关者群体

1.  所属团队

1.  工单类别

**基于客户提供的影响和紧急程度*

**方法：** 我通过以下方式优化了主提示：

1.  选取一些控制工单样本，

1.  手动对每个工单进行分类，按照我希望GPT完成的方式（按类别、复杂度、利益相关者（客户）群体等），

1.  将这些控制工单输入设计好的GPT提示中，

1.  将GPT的结果与我自己的手动分类进行交叉比较，

1.  评估GPT分类在各个维度上的表现，

1.  根据哪个维度得分较低，改进GPT提示以提升其效果

这给了我重要的反馈，帮助我不断优化GPT提示，提升每个维度的得分。最终的提示，请参见[附录：处理工单描述的GPT提示](https://docs.google.com/document/d/1-GEMcOa0OF3-rLsVP6F1ZgVfMYcEjsltmbGIIjxbEC4/edit#heading=h.4k28p1gr2x2r)。

**结果：**

这里是从原始工单描述中提取的元数据的详细信息，以及经过多次微调提示后整体性能评分：

![](../Images/ceb381d6fdc2ae2439dc700557c5238d.png)

LLM在元数据创建上的表现

这是我对某些维度得分较低的原因的推理，尽管经过了多次迭代：

+   **复杂性：** 在为每个工单评分“复杂性”时，我遇到了一个挑战，GPT基于工单描述给出的复杂性评分远高于实际情况。当我要求它更加激进地评分时，评分开始向另一个方向倾斜，就像一只试图取悦主人的狗，它开始给复杂性评分过低，因此变得不可靠。我怀疑这种出厂默认的高评分行为是由于当前GPT技术的状态。我使用的是GPT-4，它被认为相当于一名聪明的高中生，因此高中生会给出较高的复杂性评分。我猜测未来这些前沿模型的版本会达到大学甚至博士水平，我们将能够更准确地衡量此类任务的复杂性。或者，为了改善GPT-4的复杂性评分分析，我本可以使用“少量示例”学习技术，提供一些复杂性的示例，这可能会提升该维度的表现评分。

+   **严重性：** 当我要求GPT使用影响与紧急矩阵来评分严重性时，GPT不得不依赖利益相关者在工单描述中提供的内容，而这些内容可能具有误导性。我们都曾在与IT部门开具内部工单时，使用一些旨在促使更快行动的词语。此外，利益相关者在工单描述中根本没有提供任何影响细节，且这种情况在不少情况下存在，这导致GPT选择了一个错误的严重性评分。

尽管某些元数据维度的得分较低，但我对整体输出感到满意。GPT在一些关键元数据（如标题和类别）上的得分很高，我可以根据这些继续进行。

提示词本身没有问题，但我即将遇到一个有趣的GPT限制，它的“健忘症”。

## 6.4 — 探索GPT健忘症的极限

**总结：** *当向GPT提示中发送上下文上* **无关** *的数据块（如多个工单描述）时，处理的上限可能远低于通过填充输入令牌限制所允许的最大数据块数量。（在我的案例中，这个上限在20到30之间）。观察到GPT在超过此限制后会 consistently 忘记或忽略处理。可以通过反复试探来识别这个限制，保持在这个限制以下的10%范围内，以避免数据丢失。*

![](../Images/4904abf7add8a68f0b719cd9d581f78d.png)

[照片由Pierre Bamin](https://unsplash.com/@bamin?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

人类大脑可以在前额皮质中保持5到7个不相关的事物，而事实证明GPT可以保持30到40个不相关的事物，无论它的上下文窗口多大。我实际上只是发送了工单号和描述，其他数据并不需要复杂的推理。

由于我有将近3000个工单需要GPT审核，我最初的想法是尽量优化每次调用的数量，“打包”尽可能多的工单描述到每个提示中。我想出了一种详细的方法，根据单词数量来确定平均token大小（因为token是子词，基于变换器架构），发现每个提示中可以容纳大约130个工单描述。

但随后我开始看到一个奇怪的现象。无论我发送多少个工单描述到GPT进行处理，它总是只处理前20到30个工单！GPT似乎无法处理超过这个神秘数字的内容。

![](../Images/a8e6ee9426c5b24d82e83bc9a321cd36.png)

这使我改变了策略，我决定将每次API调用的工单批量大小减少到最多10到12个工单，根据该批量的字数，略低于20到30的上限。虽然这种方法确实增加了调用次数，因此延长了分析时间，但它确保了没有工单被遗漏处理。

```py
*Total tickets chunked: 3012*

*The full ticket data has been chunked into 309 chunks:*
*Chunk 0: Number of words = 674*
*Chunk 0: Number of tickets = 12*
*Chunk 1: Number of words = 661*
*Chunk 1: Number of tickets = 12*
*Chunk 2: Number of words = 652*
*Chunk 2: Number of tickets = 12*
*Chunk 3: Number of words = 638*
*Chunk 3: Number of tickets = 7*
*Chunk 4: Number of words = 654*
*….*
```

在与我公司的一位AI架构师讨论时，他确实提到这是GPT中最近观察到的现象。当你输入的上下文数据是相关时，大量输入才能表现良好。但当你将不相关的零散信息输入到GPT中，并要求它一次性处理完全无关的数据时，系统会崩溃。正是我观察到的现象。

在确定了最佳工单批量大小为10到12个，并创建了高效的提示后，是时候将所有批量通过提示进行处理了。

## 6.5 展示时间！将所有工单通过GPT进行处理

**结论提前告诉你：** *GPT可以在几小时内分析工单，而相同数量的工单可能需要数周或数月才能由人类完成。并且它便宜得多，尽管GPT有一定的错误率。*

我向GPT提供了JSON格式，让它为我编写执行以下操作的代码：

+   将JSON数据加载到字典中

+   每次处理10到12个工单，将这些工单分析提示与它们一起合并成完整的GPT提示，使用###分隔每个工单/描述元组

+   将完整的提示发送到GPT API（为了工作，我调用了我公司内部构建的、更安全的包装API，它内嵌了安全性和隐私保护功能，但如果之前使用过混淆步骤，使用外部GPT API同样是安全的。）

+   将输出保存为管道分隔格式，并将其连接到磁盘上的文件中。

+   运行去匿名化工具，如果之前进行了混淆处理。（由于我公司内部已经构建了GPT包装API，我不需要写这一步骤）

+   将输出也转换为原始的JSON文件。

+   在完整运行结束后，将JSON文件保存到磁盘。

+   打印出一些可见的队列，显示处理了多少个工单

+   为每次API调用的文本处理、工单数量、开始和结束时间进行计时。

**为什么在成功运行后保存到磁盘是务实的：** *这些运行非常昂贵，从时间的角度来看比从金钱的角度更为昂贵。因此，在成功运行完成后，明智之举是将数据序列化（保存）到磁盘上，这样以后就可以在保存的数据上进行分析，而无需再次执行这个代码块。事实上，在成功运行后，我在我的笔记本中注释掉了整个代码块，这样如果我从头到尾运行整个笔记本，它就会跳过这个昂贵的步骤，直接将JSON数据从磁盘加载到内存中，并继续进行分析。*

这是完整填充的JSON输出示例。蓝色条目是GPT从描述字段中提取的元数据

![](../Images/12724fc1e5db3af567ce6f798edb3710.png)

GPT从原始信息中提取并重新生成的结构化元数据，格式为JSON

我通过这个流程运行了大约3000个票据，并在大约2.95小时内完成了处理。👏

![](../Images/14959c841756e19355fd7e58c3c3ea63.png)

图片来源：[Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral) 来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

为了给你一个比较点，告诉你如果我雇佣人工进行这项工作大概要花多久时间：我有一个类似的经历，可以追溯到两年前（也就是GenAI之前的时代）。我接管了一个运营团队，票据中的元数据也很差。我需要获得情境意识，所以我必须全职派出两名资深工程师，一个月时间，仔细而艰难地处理1500个票据描述并进行分类。

与GPT进行对比。即使每次API调用的批次较小，每次只有10-15个票据，GPT仍在2.95小时内处理了2800个票据！是的，尽管GPT的错误率为10%，它在3小时内分析的票据数量仍然是两个人在1个月内的**两倍**。

所以，节省的时间显然是惊人的，但成本比较如何呢？

## GPT-4与人工分析成本比较

我请GPT帮助我进行成本比较。请注意，我将人力劳动的2个月成本与GPT分析的票据数量进行比较。

**提示：**如果我使用其API运行2.95小时，GPT-4的成本是多少？与两位资深IT软件工程师（分别来自美国中西部和印度，且全职工作2个月，周末除外）在同一项目上的工作时间成本相比，如何？

**薪资估算：**

美国中西部资深IT软件工程师：

+   平均年薪：$120,000（这可能会根据具体位置和公司有所不同，但通常低于硅谷的水平）。

+   月薪：$120,000 / 12 = $10,000。

印度资深IT软件工程师：

+   平均年薪：$30,000（这可能会有所不同，但通常远低于美国的水平）。

+   月薪：$30,000 / 12 = $2,500。

**2个月的成本计算：**

+   中西部工程师：2个月薪水：$20,000。

+   印度工程师：2个月薪水：$5,000。

+   两位工程师的总成本：$25,000

**GPT-4 API费用（2.95小时）：** ~$6.64，假设GPT-4–8k定价和给定的token使用率。（GPT4-o或GPT4-o mini的费用会更便宜。）

即使你把我在这个项目上花的20小时的成本也算上，整体成本比较仍然要好得多。而且更重要的是，这项工作现在是可复制的。

***所以基本上，使用$7和3小时，GPT完成了与人类需要1个月和$25,000才能完成的相同分析***

🎤 麦克风掉落！

![](../Images/ad6f00a9f6688f5826dc72d7c74a0fda.png)

照片由[Andrew Gaines](https://unsplash.com/@shotbygaines?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

## 第7步：从GPT派生的元数据中提取见解

**总结：** *一旦你使用GPT提取了有用的元数据，转身与GPT进行头脑风暴，看看你能从中绘制出哪些KPIs。*

尽管我已经有些好奇想要了解的内容，但我也和GPT进行了头脑风暴，要求它给我更多的想法。同样，使用JSON格式非常方便，我只需将一个票据的匿名样本传递给GPT，并问它，*“根据你看到的内容，给我一些关于我如何绘制图表以便从我的运营中获取见解的想法”*

最终，这里是我们两个人想出来的思路。我采纳了一些，忽略了其他的。

![](../Images/8a9ccaee2d28fe3df5b62eba6bbd0fc2.png)

与GPT进行头脑风暴，讨论要可视化的KPIs……

## 第8步：可视化

**总结：** *得益于GPT，你可以编写Python代码来创建图表，而不需要在Python中转换数据并将这些数据导入可视化工具。这有助于保持所有分析的简洁性、版本控制，并且将所有内容集中在一个地方。*

从历史上看，探索性数据分析（EDA）中的典型模式是：在Python中提取并转换数据，然后将其存储在文件或数据库中，再将Tableau、Power BI或Looker与这些数据连接，利用它们来创建图表。虽然在这些可视化产品中保持长期有效的仪表盘绝对是正确的做法，但将这些产品用于进行早期阶段的EDA可能是一个高摩擦过程，会导致延迟。而且，管理和匹配不同版本的图表与不同版本的数据转换变得困难。然而，历史上遵循这种两步模式是必要的恶习，原因有二：

1.  （拉取）这些可视化工具直观且具有拖放界面，意味着你可以快速进行实验并创建图表。

1.  （推动）生成图表的事实标准Python库是matplotlib。我不知道你是怎么想的，但我发现matplotlib是一个非常不友好的库（与R中直观的ggplot库不同，ggplot使用起来非常愉快）。Seaborn稍好一些，但它仍然比可视化工具需要更多的工作。

然而，现在 GPT 可以为你编写所有的 matplotlib（或 seaborn、或 plotly）代码，你就不必将工作转移到可视化工具中了。你可以从头到尾都停留在同一个 Python Notebook 中，这正是我所做的！

我确实查看了 Tableau，以验证一些中等复杂度的聚合逻辑是否在 Python 中正确计算（事实上这帮我找到了一个 bug），但总体来说，我所需要的所有图形都是通过 Python 中的散点图、条形图、折线图、直方图和饼图来构建的。

这里有一些这些图表和表格的示例。文本和数字当然已经匿名化，但这里的目的是展示你可以开始提取的那种洞察。

***任何洞察的目标都是引发更深层次的问题，最终采取基于数据的有意义行动，进而创造价值。***

洞察力就是让你对系统的行为产生好奇，从而你可以尝试改进它。

![](../Images/3b78ee7b1787c70a828823463ed9fd3a.png)

票据的复杂度如何准确地影响该票据的时长，以及应该聚焦哪些票据类别，以减少它们的时长并提高客户满意度。

![](../Images/3d310bc3496219a6fb55a123c3d0f816.png)

确定票据时长是否与处理该票据的支持工程师相关联，以便你可以找出行为或培训方面的问题。

![](../Images/b9ed493745f54112106775e817ac0d06.png)

哪个工程团队接收的票据数量最多，且这一趋势如何发展。

处理服务请求（纯运维工作）是一个隐藏的成本，需要量化，因为它必须从工程团队的冲刺速度中扣除。在没有这些数据的情况下，工程团队通常会分配一部分时间来处理运维工作，这一比例通常是‘凭经验猜测’的，我们都知道这是笨拙的，并且因团队而异。通过这种分析，你可以更准确地为这些运维工作划分容量，同时不会妥协团队的产品承诺或让个人过度疲劳。

![](../Images/0ae3125d560c0e72596b89927b47ad90.png)

这些票据按类别的趋势是什么？我们是看到更多的及时性问题，还是准确性问题？

![](../Images/e81cba556a2f44af96f59fbf5107e277.png)

我们的客户最常使用哪些接口来提交票据，以便我们能够精简和优化这些区域，也许可以插入一些有帮助的自助服务文章。

![](../Images/e584b62b59d366fe872b2c231094be9e.png)

有多少票据已经有1天的历史，是否存在某些运维人员选择处理大量简单案例的模式？这有助于平衡资源管理。

![](../Images/b5570ade930bda38719a96ed1984c9ce.png)

有多少票据确实是低复杂度问题，比如数据访问或系统访问，这些问题可以通过自动化和自助服务选项来解决。

# 未来的改进

1.  **利用 GPT 的数据科学能力进行更深层次的分析：** 然而，这个分析虽然非常有洞察力，但只是停留在表面，仅仅是数据的可视化。还可以做更复杂的工作，使用线性回归或逻辑回归、ANOVA 进行预测分析，或使用聚类方法（如 KNN）来发掘数据中的其他模式。

1.  **多智能体框架以加速和提高质量：**

+   我必须与 GPT 进行多轮反复的沟通才能编写实际代码。虽然这比我从头开始编写代码的时间（20-30 天全职工作，这意味着“永远不可能”！）要快得多（7 天兼职），但我确实认为使用 LLM 支持的 AI 代理可以相互批评对方的输出，并找到更好的方法。（这是我目前在工作中积极实验的方向，初步实验非常令人鼓舞。以后会写更多相关内容）

+   在推荐代码方面，GPT 真的很盲目。我从它那儿复制了代码并在我的 Jupyter Notebook 中运行。更好的方法是使用 MAF 设置一个环境代理（或许通过[容器](https://microsoft.github.io/autogen/docs/tutorial/code-executors)完美配置我的所需库等），然后让 AI 编码代理编写代码、执行、发现缺陷、迭代并修复。我想，这样能节省我超过 50% 的开发时间。

+   分解分析提示：虽然我最终使用了一个大型提示来进行分析，但如果我使用一些链式 AI 代理机制，我本可以将分析任务分配给不同的代理，为它们提供不同的 LLM 端点，并且每个代理使用不同的[温度](https://www.hopsworks.ai/dictionary/llm-temperature)设置。温度越低，LLM 越精准，创造性越差。例如，我通过艰难的方式学到，如果使用默认的温度设置，GPT 会对每个票据的类别（如“数据完整性”或“完整性”）进行细微修改，结果导致了更多的后期处理清理工作，给我带来了困扰。

+   哎，我甚至可以让我的多智能体团队中的创意 AI 代理帮我写出这份文档的大部分内容！

# 来自产品负责人关于操作票据的总结想法

我们的客户通过他们日常与我们产品的互动来体验我们的产品。通过工单和服务请求，他们不断向我们传递哪些工作正常、哪些工作不正常的信号，并通过观察我们如何响应这些信号形成对我们的印象。然而，我们常常把注意力集中在产品开发、正在进行的重大转型计划以及跟踪和监控下一个炫目的项目上，而忽视了这些运营信号，这可能会带来风险。当然，响应重大事件是每个负责领导者的工作，行动计划通常通过根本原因分析（RCA）会议得以形成并取得成果。然而，我认为有大量中等严重性的问题和服务请求，客户常常因为其数量庞大而被忽视。当您真诚地开始挖掘这些宝贵的工单数据时，它们往往会让人感到不知所措且未经过整理，您的大脑可能会开始晕旋！您有可能因此走向一个简化且不完整的思维模型，这个模型可能是由别人创建的总结报告所导致的。

我的理念是，作为领导者，您必须创造时间和能力，亲自深入实际操作。这是您真正了解自己业务如何运作的唯一方式。在genAI时代之前，这曾是非常困难的。即使是能够进行数据分析的领导者，也无法从日常工作中抽出时间来做这件事。而现在，情况不再如此！

虽然本文试图为您提供一些能力，以便启动基于genAI的运营工单分析之旅，但只有您，我亲爱的读者，才能创造时间和空间来付诸实践。更重要的是，我希望您在有效使用LLMs来加速分析的过程中获得的某些见解，能在运营工单分析之外的许多其他领域得到应用。

# 附录：经过微调的GPT提示版本

以下是我用来进行工单分析的最有效提示的清理版。我用数据管理协会（DAMA）发布的标准替代了我们内部的数据质量维度。如果您的公司有数据质量政策，我建议您在这里使用这些标准。

*以下是一些案例及其描述，每个案例之间以###分隔。这些案例与数据相关技术相关。您的任务是仔细审查每个案例，提取必要的7个数据点，并以指定格式展示结果。详细说明如下：*

1.  ***标题:*** *根据描述内容，为每个案例创建一个简洁且具描述性的标题，确保字符数不超过300个。*

1.  ***影响:*** *从描述中简要总结影响，写成一句话。如果影响未直接陈述或暗示，则写“未提供影响”。*

1.  ***严重性:*** *使用紧急性与影响矩阵方法，为每个案例分配严重性级别，考虑问题的紧急性及其对系统的影响：*

+   ***S1:*** *高紧急性和高影响，可能导致系统故障或使应用程序无法使用。*

+   ***S2:*** *高紧急性但影响中等，或中等紧急性但影响较大，影响多个用户。*

+   ***S3:*** *低紧急性，影响中等或较低，用户中断最小。*

+   ***S4:*** *低紧急性和低影响，通常与一般请求相关（注意：访问问题通常不属于S4）。*

+   *每个案例应仅分配一个严重性级别。*

*4\. 复杂性: 根据你在数据领域的专业知识评估案件的复杂性：*

+   ***高复杂性***

+   ***中等复杂性***

+   ***低复杂性***

+   *通常，访问相关的案例是低复杂性，但可以根据描述做出判断。*

***5\. 业务线（LOB）:*** *根据描述确定相关的业务线。选项包括：*

+   ***财务***

+   ***市场营销***

+   ***销售***

+   ***客户支持***

+   ***人力资源***

+   ***杂项:*** *如果无法明确识别业务线。*

+   *每个案例只选择一个业务线。如果提到多个，请选择最重要的。*

***6\. 团队:*** *根据描述分配适当的团队。选项包括：*

+   ***CDI（中央数据摄取）:*** *任何提到CDI或“中央数据摄取团队”的案例应仅归类于该团队。*

+   ***数据工程:*** *与数据管道相关的案例，如提取、转换或加载。*

+   ***数据平台:*** *与数据平台相关的任何问题，包括数据可视化或DEP。*

+   *每个案例应仅分配一个团队。*

***7\. 工单分类:*** *最后，根据描述将工单分类，使用简单的1-2个词的标签。使用DAMA数据质量维度进行分类。类别应包括但不限于：*

+   ***完整性:*** *确保所有必要的数据都包含在内。*

+   ***独特性:*** *验证数据条目是唯一的，不重复。*

+   ***及时性:*** *确保数据是最新的并按预期可用。*

+   ***准确性:*** *确认数据是正确的并符合其真实值。*

+   ***一致性:*** *确保不同数据集中的数据保持一致。*

+   ***有效性:*** *确保数据符合所需的格式或值。*

+   ***访问:*** *与请求访问数据或系统相关。*

+   *如有需要，可以创建2-3个其他类别，但请保持简洁一致。*

*这是输出格式的一个示例。应为一个列表，每个项之间用管道符（|）分隔：*

*16477679|描述性标题（不超过300个字符）|简短的影响描述|S2|高复杂性|财务|数据工程|及时性*

16377679|另一个描述性标题|另一个简短的影响描述|S1|高复杂性|销售|数据平台|准确性*

除非另有说明，所有图片均由作者提供
