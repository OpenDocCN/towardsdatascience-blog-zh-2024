- en: Gated Recurrent Units (GRU) — Improving RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15](https://towardsdatascience.com/gated-recurrent-units-gru-improving-rnns-4467b66c5150?source=collection_archive---------7-----------------------#2024-06-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explaining how Gated Recurrent Neural Networks work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--4467b66c5150--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4467b66c5150--------------------------------)
    ·10 min read·Jun 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c751c90c4f7c68dff3f32198562febf8.png)'
  prefs: []
  type: TYPE_IMG
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>Neural network icons created by juicy_fish — Flaticon.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will explore a standard implementation of recurrent neural
    networks (RNNs): gated recurrent units (GRUs).'
  prefs: []
  type: TYPE_NORMAL
- en: GRUs were introduced in 2014 by [Kyunghyun Cho et al.](https://arxiv.org/abs/1406.1078)
    and are an improvement from vanilla RNNs as they suffer less from the vanishing
    gradient problem, allowing them to have more extended memory.
  prefs: []
  type: TYPE_NORMAL
- en: They are similar to [Long-Short-Term Memory (LSTM)](/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?sk=03c1ce36eba874e0e0bc0d0267eb5314)
    networks but have fewer operations, making them more memory efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vanishing and Exploding Gradient Problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How GRUs Work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurrent Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks are a type of neural network that is especially adept
    at sequence-based data, such as natural language and time series.
  prefs: []
  type: TYPE_NORMAL
- en: They achieve this by adding a “recurrent” neuron that allows information to
    be fed through from past inputs and outputs to the next step.
  prefs: []
  type: TYPE_NORMAL
