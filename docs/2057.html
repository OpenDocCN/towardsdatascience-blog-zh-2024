<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building an Image Similarity Search Engine with FAISS and CLIP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building an Image Similarity Search Engine with FAISS and CLIP</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23">https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9071" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A guided tutorial explaining how to search your image dataset with text or photo queries, using CLIP embedding and FAISS indexing.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lihi Gur Arie, PhD" class="l ep by dd de cx" src="../Images/7a1eb30725a95159401c3672fa5f43ab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M0YTyQAxsWWqI8MLBi2xrA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------" rel="noopener follow">Lihi Gur Arie, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxdBeffQpqMcDMewzD58pA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image was generated by author on Flux-Pro platform</figcaption></figure><h1 id="ed28" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="5c07" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Have you ever wanted to find an image among your never-ending image dataset, but found it too tedious? In this tutorial we’ll build an image similarity search engine to easily find images using either a text query or a reference image. For your convenience, the complete code for this tutorial is provided at the bottom of the article as a <strong class="oa fr">Colab notebook</strong>.</p><blockquote class="ou ov ow"><p id="85fd" class="ny nz ox oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">If you don’t have a paid Medium account, you can read for free<em class="fq"> </em><a class="af pd" rel="noopener" target="_blank" href="/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad">here</a>.</p></blockquote><h2 id="132c" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk">Pipeline Overview</h2><p id="1994" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The semantic meaning of an image can be represented by a numerical vector called an embedding. Comparing these low-dimensional embedding vectors, rather than the raw images, allows for efficient similarity searches. For each image in the dataset, we’ll create an embedding vector and store it in an index. When a text query or a reference image is provided, its embedding is generated and compared against the indexed embeddings to retrieve the most similar images.</p><p id="bb11" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">Here’s a brief overview:</p><ol class=""><li id="ae50" class="ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot pv pw px bk"><strong class="oa fr">Embedding: </strong>The embeddings of the images are extracted using the CLIP model.</li><li id="604d" class="ny nz fq oa b go py oc od gr pz of og oh qa oj ok ol qb on oo op qc or os ot pv pw px bk"><strong class="oa fr">Indexing</strong>: The embeddings are stored as a FAISS index.</li><li id="acc5" class="ny nz fq oa b go py oc od gr pz of og oh qa oj ok ol qb on oo op qc or os ot pv pw px bk"><strong class="oa fr">Retrieval</strong>: With FAISS, The embedding of the query is compared against the indexed embeddings to retrieve the most similar images.</li></ol><h2 id="f46d" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk">CLIP Model</h2><p id="64c8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI, is a multi-modal vision and language model that maps images and text to the same latent space. Since we will use both image and text queries to search for images, we will use the CLIP model to embed our data. For further reading about CLIP, you can check out my previous article <a class="af pd" rel="noopener" target="_blank" href="/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1">here</a>.</p><h2 id="306b" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk">FAISS Index</h2><p id="ded2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">FAISS (Facebook AI Similarity Search) is an open-source library developed by Meta. It is built around the Index object that stores the database embedding vectors. FAISS enables efficient similarity search and clustering of dense vectors, and we will use it to index our dataset and retrieve the photos that resemble to the query.</p><h1 id="aebc" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Code Implementation</h1><h2 id="e897" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk"><strong class="al">Step 1 — Dataset Exploration</strong></h2><p id="d989" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To create the image dataset for this tutorial I collected 52 images of varied topics from <a class="af pd" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">Pexels</a>. To get the feeling, lets observe 10 random images:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/5783da36e449663c611c2569c8921c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*coBvgqyD6lSjRIeT2xPq0g.png"/></div></div></figure><h2 id="f18b" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk"><strong class="al">Step 2 — Extract CLIP Embeddings from the Image Dataset</strong></h2><p id="531b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To extract CLIP embeddings, we‘ll first load the CLIP model using the HuggingFace SentenceTransformer library:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="d4e9" class="qi nd fq qf b bg qj qk l ql qm">model = SentenceTransformer('clip-ViT-B-32')</span></pre><p id="82c3" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">Next, we’ll create a function that iterates through our dataset directory with <code class="cx qn qo qp qf b">glob</code>, opens each image with <code class="cx qn qo qp qf b">PIL Image.open</code>, and generates an embedding vector for each image with <code class="cx qn qo qp qf b">CLIP model.encode</code>. It returns a list of the embedding vectors and a list of the paths of our images dataset:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="1392" class="qi nd fq qf b bg qj qk l ql qm">def generate_clip_embeddings(images_path, model):<br/><br/>    image_paths = glob(os.path.join(images_path, '**/*.jpg'), recursive=True)<br/>    <br/>    embeddings = []<br/>    for img_path in image_paths:<br/>        image = Image.open(img_path)<br/>        embedding = model.encode(image)<br/>        embeddings.append(embedding)<br/>    <br/>    return embeddings, image_paths<br/><br/><br/><br/>IMAGES_PATH = '/path/to/images/dataset'<br/><br/>embeddings, image_paths = generate_clip_embeddings(IMAGES_PATH, model)</span></pre><h2 id="2643" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk"><strong class="al">Step 3 — Generate FAISS Index</strong></h2><p id="6ae8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The next step is to create a FAISS index from the embedding vectors list. FAISS offers various distance metrics for similarity search, including Inner Product (IP) and L2 (Euclidean) distance.</p><p id="29f1" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">FAISS also offers various indexing options. It can use approximation or compression technique to handle large datasets efficiently while balancing search speed and accuracy. In this tutorial we will use a ‘Flat’ index, which performs a brute-force search by comparing the query vector against every single vector in the dataset, ensuring exact results at the cost of higher computational complexity.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="7fb0" class="qi nd fq qf b bg qj qk l ql qm">def create_faiss_index(embeddings, image_paths, output_path):<br/><br/>    dimension = len(embeddings[0])<br/>    index = faiss.IndexFlatIP(dimension)<br/>    index = faiss.IndexIDMap(index)<br/>    <br/>    vectors = np.array(embeddings).astype(np.float32)<br/><br/>    # Add vectors to the index with IDs<br/>    index.add_with_ids(vectors, np.array(range(len(embeddings))))<br/>    <br/>    # Save the index<br/>    faiss.write_index(index, output_path)<br/>    print(f"Index created and saved to {output_path}")<br/>    <br/>    # Save image paths<br/>    with open(output_path + '.paths', 'w') as f:<br/>        for img_path in image_paths:<br/>            f.write(img_path + '\n')<br/>    <br/>    return index<br/><br/><br/>OUTPUT_INDEX_PATH = "/content/vector.index"<br/>index = create_faiss_index(embeddings, image_paths, OUTPUT_INDEX_PATH)</span></pre><p id="e56e" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">The <code class="cx qn qo qp qf b">faiss.IndexFlatIP</code> initializes an Index for Inner Product similarity, wrapped in an <code class="cx qn qo qp qf b">faiss.IndexIDMap</code> to associate each vector with an ID. Next, the <code class="cx qn qo qp qf b">index.add_with_ids</code> adds the vectors to the index with sequential ID’s, and the index is saved to disk along with the image paths.</p><p id="46e4" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">The index can be used immediately or saved to disk for future use .To load the FAISS index we will use this function:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="b971" class="qi nd fq qf b bg qj qk l ql qm">def load_faiss_index(index_path):<br/>    index = faiss.read_index(index_path)<br/>    with open(index_path + '.paths', 'r') as f:<br/>        image_paths = [line.strip() for line in f]<br/>    print(f"Index loaded from {index_path}")<br/>    return index, image_paths<br/><br/>index, image_paths = load_faiss_index(OUTPUT_INDEX_PATH)</span></pre><h2 id="f02d" class="pe nd fq bf ne pf pg ph nh pi pj pk nk oh pl pm pn ol po pp pq op pr ps pt pu bk"><strong class="al">Step 4 — Retrieve Images by a Text Query or a Reference Image</strong></h2><p id="95ff" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">With our FAISS index built, we can now retrieve images using either text queries or reference images. If the query is an image path, the query is opened with <code class="cx qn qo qp qf b">PIL Image.open</code>. Next, the query embedding vector is extracted with <code class="cx qn qo qp qf b">CLIP model.encode</code>.</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="7ba3" class="qi nd fq qf b bg qj qk l ql qm">def retrieve_similar_images(query, model, index, image_paths, top_k=3):<br/>    <br/>    # query preprocess:<br/>    if query.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):<br/>        query = Image.open(query)<br/><br/>    query_features = model.encode(query)<br/>    query_features = query_features.astype(np.float32).reshape(1, -1)<br/><br/>    distances, indices = index.search(query_features, top_k)<br/><br/>    retrieved_images = [image_paths[int(idx)] for idx in indices[0]]<br/><br/>    return query, retrieved_images</span></pre><p id="66de" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">The Retrieval is happening on the <code class="cx qn qo qp qf b">index.search</code> method. It implements a k-Nearest Neighbors (kNN) search to find the <code class="cx qn qo qp qf b">k</code> most similar vectors to the query vector. We can adjust the value of k by changing the <code class="cx qn qo qp qf b">top_k</code> parameter. The distance metric used in the kNN search in our implementation is the cosine similarity. The function returns the query and a list of retrieve images paths.</p><p id="3809" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Search with a Text Query:</strong></p><p id="0b48" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">Now we are ready to examine the search results. The helper function <code class="cx qn qo qp qf b">visualize_results</code> displays the results. You can fined it in the associated Colab notebook. Lets explore the retrieved most similar 3 images for the text query “ball” for example:</p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="6ff6" class="qi nd fq qf b bg qj qk l ql qm">query = 'ball'<br/>query, retrieved_images = retrieve_similar_images(query, model, index, image_paths, top_k=3)<br/>visualize_results(query, retrieved_images)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/0982e9cc9e59715d51147782f62d60f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cJojIzAHaEk-vsOhBrcEw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Retrieved images with the query: ‘a ball’</figcaption></figure><p id="3d3d" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">For the query ‘animal’ we get:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/6beffd16ea96ed9928100a9d117bcb0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iErhNVaRUsLg30nMMFPKrQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Retrieved images with the query: ‘animal’</figcaption></figure><p id="ec29" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Search with a Reference Image:</strong></p><pre class="mm mn mo mp mq qe qf qg bp qh bb bk"><span id="22cd" class="qi nd fq qf b bg qj qk l ql qm">query ='/content/drive/MyDrive/Colab Notebooks/my_medium_projects/Image_similarity_search/image_dataset/pexels-w-w-299285-889839.jpg'<br/>query, retrieved_images = retrieve_similar_images(query, model, index, image_paths, top_k=3)<br/>visualize_results(query, retrieved_images)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/015aeddcd340d75270dd41904b56d11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xxqSsyAdRe8Nqhe8cbs56g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Query and Retrieved images</figcaption></figure><p id="9517" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">As we can see, we get pretty cool results for an off-the-shelf pre-trained model. When we searched by a reference image of an eye painting, besides finding the original image, it found one match of eyeglass and one of a different painting. This demonstrates different aspects of the semantic meaning of the query image.</p><p id="6cc8" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">You can try other queries on the provided Colab notebook to see how the model performs with different text and image inputs.</p><h1 id="7688" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Concluding Remarks</h1><p id="1ffb" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In this tutorial we built a basic image similarity search engine using CLIP and FAISS. The retrieved images shared similar semantic meaning with the query, indicating the effectiveness of the approach. Though CLIP shows nice results for a Zero Shot model, it might exhibit low performance on Out-of-Distribution data, Fine-Grained tasks and inherit the natural bias of the data it was trained on. To overcome these limitations you can try other CLIP-like pre-trained models as in <a class="af pd" href="https://github.com/mlfoundations/open_clip/tree/main" rel="noopener ugc nofollow" target="_blank">OpenClip</a>, or fine-tune CLIP on your own custom dataset.</p><h1 id="de7c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Thank you for reading!</h1><p id="1182" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Congratulations on making it all the way here. Click 👍 to show your appreciation and raise the algorithm self esteem 🤓</p><p id="b8d0" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Want to learn more?</strong></p><ul class=""><li id="58e7" class="ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot qr pw px bk"><a class="af pd" href="https://medium.com/@lihigurarie" rel="noopener"><strong class="oa fr">Explore</strong></a> additional articles I’ve written</li><li id="f16b" class="ny nz fq oa b go py oc od gr pz of og oh qa oj ok ol qb on oo op qc or os ot qr pw px bk"><a class="af pd" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"><strong class="oa fr">Subscribe</strong></a><strong class="oa fr"> </strong>to get notified when I publish articles</li><li id="a96a" class="ny nz fq oa b go py oc od gr pz of og oh qa oj ok ol qb on oo op qc or os ot qr pw px bk">Follow me on <a class="af pd" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">Linkedin</strong></a></li></ul><h1 id="a80d" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Full Code as Colab notebook:</h1><figure class="mm mn mo mp mq mr"><div class="qs io l ed"><div class="qt qu l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Colab Notebook <a class="af pd" href="https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc" rel="noopener ugc nofollow" target="_blank">Link</a></figcaption></figure></div></div></div></div>    
</body>
</html>