- en: 'Maixtchup: Make Your Own Mixture of Experts with Mergekit'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29](https://towardsdatascience.com/maixtchup-make-your-own-mixture-of-experts-with-mergekit-87cc46401587?source=collection_archive---------11-----------------------#2024-01-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The rise of the MoEs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--87cc46401587--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--87cc46401587--------------------------------)
    ·8 min read·Jan 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce341668f026e81e27282ec3085bba1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author — Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Since the release of Mixtral-8x7B by Mistral AI, there has been a renewed interest
    in the [mixture of expert (MoE) models](https://medium.com/p/0e3fc7fde818). This
    architecture exploits expert sub-networks among which only some of them are selected
    and activated by a router network during inference.
  prefs: []
  type: TYPE_NORMAL
- en: MoEs are so simple and flexible that it is easy to make a custom MoE. On the
    Hugging Face Hub, we can now find several trending LLMs that are custom MoEs,
    such as [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8).
  prefs: []
  type: TYPE_NORMAL
- en: However, most of them are not traditional MoEs made from scratch, they simply
    use a combination of already fine-tuned LLMs as experts. Their creation was made
    easy with [mergekit](https://github.com/cg123/mergekit) ([LGPL-3.0 license](https://github.com/cg123/mergekit#LGPL-3.0-1-ov-file)).
    For instance, Phixtral LLMs have been made with mergekit by combining several
    [Phi-2 models](https://medium.com/p/06db49949ff1).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will see how Phixtral was created. We will apply the same
    process to create our own mixture of experts, Maixtchup, using several Mistral
    7B models.
  prefs: []
  type: TYPE_NORMAL
- en: What is Phixtral?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To quickly understand the high-level architecture of a model, I like to print
    it. For instance, for [mlabonne/phixtral-4x2_8](https://huggingface.co/mlabonne/phixtral-4x2_8)
    (MIT license):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
