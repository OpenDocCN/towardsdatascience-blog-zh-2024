- en: '(Un)Objective Machines: A Look at Historical Bias in Machine Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/un-objective-machines-a-look-at-historical-bias-in-machine-learning-da5101d46169?source=collection_archive---------6-----------------------#2024-04-17](https://towardsdatascience.com/un-objective-machines-a-look-at-historical-bias-in-machine-learning-da5101d46169?source=collection_archive---------6-----------------------#2024-04-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into biases in machine learning, with a focus on historical (or
    social) biases.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@greteltan21?source=post_page---byline--da5101d46169--------------------------------)[![Gretel
    Tan](../Images/dba8c83e3c13f94a99d9a8c33688c153.png)](https://medium.com/@greteltan21?source=post_page---byline--da5101d46169--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--da5101d46169--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--da5101d46169--------------------------------)
    [Gretel Tan](https://medium.com/@greteltan21?source=post_page---byline--da5101d46169--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--da5101d46169--------------------------------)
    ·10 min read·Apr 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Humans are biased. To anyone who has had to deal with bigoted individuals, unfair
    bosses, or oppressive systems — in other words, all of us — this is no surprise.
    We should thus welcome machine learning models which can help us to make more
    objective decisions, especially in crucial fields like healthcare, policing, or
    employment, where prejudiced humans can make life-changing judgements which severely
    affect the lives of others… right? Well, no. Although we might be forgiven for
    thinking that machine learning models are objective and rational, biases can be
    in-built into models in a myraid of ways. In this blog post, we will be focusing
    on historical biases in machine learning (ML).
  prefs: []
  type: TYPE_NORMAL
- en: What is a Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our daily lives, when we invoke bias, we often mean “[judgement based on
    preconceived notions or prejudices, as opposed to the impartial evaluation of
    facts](https://ainowinstitute.org/publication/ai-now-2017-report-2)”. Statisticians
    also use “bias” to describe pretty much anything which may lead to a systematic
    disparity between the ‘true’ parameters and what is estimated by the model.
  prefs: []
  type: TYPE_NORMAL
- en: ML models suffer from statistical biases since statistics play a big role in
    how they work. However, these models are also designed by humans, and use data
    generated by humans for training, making them vulnerable to learning and perpetuating
    human biases. Thus, perhaps counterintuitively, ML models are arguably *more*
    susceptible to biases than humans, not less.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experts disagree on the exact number of algorithmic biases, but there are at
    least 7 potential sources of harmful bias ([Suresh & Guttag, 2021](https://dl.acm.org/doi/fullHtml/10.1145/3465416.3483305)),
    each generated at a different point in the data analysis pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Historical bias, which arises from the world, in the data generation phase;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Representation bias, which comes about when we take samples of data from the
    world;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measurement bias, where the metrics we use or the data we collect might not
    reflect what we actually want to measure;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregation bias, where we apply the same approach to our whole data set, even
    though there are subsets which need to be treated differently;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learning bias, where the ways we have defined our models cause systematic errors;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluation bias, where we ‘grade’ our models’ performances on data which does
    not actually reflect the population we want to use the models on, and finally;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deployment bias, where the model is not used in the way the developers intended
    for it to be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/d0e09d1aefa3353ee704e3e612c63403.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hunter Harritt](https://unsplash.com/@hharritt?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/red-and-blue-lights-from-tower-steel-wool-photography-Ype9sdOPdYc?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: While all of these are important biases, which any budding data scientist should
    consider, today I will be focusing on historical bias, which occurs at the first
    stage of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Psst! Interested in learning more about other types of biases? Watch this helpful
    video:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Historical Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike the other types of biases, historical bias does not originate from ML
    processes, but from our world. Our world has historically been, and still is peppered
    with prejudices, so even when the data we use to train our models perfectly reflects
    the world we live in, our data might capture these discriminatory patterns. This
    is where historical bias arises. Historical bias may also manifest in instances
    where our world has made strides towards equality, but our data does not adequately
    capture these changes, reflecting past inequalities instead.
  prefs: []
  type: TYPE_NORMAL
- en: Why Should We Care?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most societies have anti-discrimination laws, which aim to protect the rights
    of vulnerable groups in society, who have been historically oppressed. If we are
    not careful, previous acts of discrimination might be learned and perpetuated
    by our ML models due to historical bias. With the rising prevalence of ML models
    in practically every area of our lives, from the mundane to the life-changing,
    this poses a particularly insidious threat — historically biased ML models have
    the potential to perpetuate inequality on a never-before-seen scale. Data scientist
    and mathematician Cathy O’Neil calls such models ‘weapons of math destruction’
    or WMDs for short — models whose workings are a mystery, generate harmful outcomes
    which victims cannot dispute, and which often penalise the poor and oppressed
    in our society, while benefiting those who are already well off (O’Neil, 2017).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3097e830cf16e7342353e058b0142e69.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [engin akyurt](https://unsplash.com/@enginakyurt?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/woman-with-hands-tied-l1clu1ZKjSw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: 'Such WMDs are already impacting vulnerable groups worldwide. Although we would
    think that Amazon, which profits from recommending us items we have never heard
    of, yet suddenly desperately want, would have mastered machine learning, [it was
    found](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8830968/) that an algorithm
    they used to scan CVs had learned a gender bias, due to the historically low number
    of women in tech. Perhaps more chillingly, [predictive policing tools](https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect.)
    have also been shown to have racial biases, as have algorithms used in [healthcare](https://www.science.org/doi/10.1126/science.aax2342),
    and even the [courtroom](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/).
    The mass proliferation of such tools obviously has great impacts, particularly
    since they may serve as a way to entrench the already deep-rooted inequalities
    in our society. I would argue that these WMDs are a far greater hindrance in our
    collective efforts to stamp out inequality compared to biased humans, for two
    main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, it is hard to get insight into why ML models make certain predictions.
    Deep learning seems to be the buzzword of the season, with complicated neural
    networks taking the world by storm. While these models are exciting since they
    have the potential to model very complex phenomena which humans cannot understand,
    they are considered black-box models, since their workings are often opaque, even
    to their creators. Without concerted efforts to test for historical (and other)
    biases, it is difficult to tell if they are inadvertently discriminating against
    protected groups.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the scale of damage which might be done by a historically biased model
    is, in my opinion, unprecedented and overlooked. Since humans have to rest, and
    need time to process information effectively, the damage a single prejudiced person
    might do is limited. However, just one biased ML model can pass thousands of discriminatory
    judgements in a matter of minutes, without resting. Dangerously, many also believe
    that machines are more objective than humans, leading to reduced oversight over
    potentially rogue models. This is especially concerning to me, since with the
    massive success of large language models like ChatGPT, more and more people are
    developing an interest in implementing ML models into their workflows, potentially
    automating the rise of WMDs in our society, with devastating consequences.
  prefs: []
  type: TYPE_NORMAL
- en: What Can We Do About It?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the impacts of biased models might be scary, this does not mean that we
    have to abandon ML models entirely. Artificial Intelligence (AI) ethics is a growing
    field, and researchers and activists alike are working towards solutions to get
    rid of, or at least reduce the biases in models. Notably, there has been a recent
    push for [FAT](https://ainowinstitute.org/publication/ai-now-2017-report-2) or
    [FATE](https://www.sciencedirect.com/science/article/pii/S2666920X23000310) AI
    — fair, accountable, transparent and ethical AI, which might help in the detection
    and correction of biases (among other ethical issues). While it is not a comprehensive
    list, I will provide a brief overview of some ways to mitigate historical biases
    in models, which will hopefully help you on your own data science journey.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the problem arises from disproportionate outcomes in the real world’s
    data, why not fix it by making our collected data more proportional? This is one
    statistical approach of dealing with historical bias, suggested by Suresh, H.,
    & Guttag, J. (2021). Put simply, it comprises collecting more data from some groups
    and less from others (systematic over- or under- sampling), resulting in a more
    balanced distribution of outcomes in our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In line with the goals of FATE AI, interpretability can be built into models,
    making their decision-making processes more transparent. Interpretability allows
    data scientists to see why models make the decisions they do, providing opportunities
    to spot and mitigate potential instances of historical biases in their models.
    In the real world, this also means that victims of machine-based discrimination
    can challenge decisions made by previously inscrutable models, and hopefully cause
    them to be reconsidered. This will hopefully increase trust in our models.
  prefs: []
  type: TYPE_NORMAL
- en: 'More technically, algorithms and models to address biases in ML models are
    also being developed. [Adversarial debiasing](https://dl.acm.org/doi/pdf/10.1145/3278721.3278779)
    is one interesting solution. Such models essentially consist of two parts: a predictor,
    which aims to predict an outcome, like hireability, and an adversary, which tries
    to predict protected attributes based on the predicted outcomes. Like boxers in
    a ring, these two components go back and forth, fighting to perform better than
    the other, and when the adversary can no longer detect protected attributes based
    on the predicted outcomes, the model is considered to have been debiased. Such
    models have performed quite well compared to models which have not been debiased,
    showing that we need not compromise on performance while prioritising fairness.
    [Algorithms](https://dl.acm.org/doi/10.1145/3468264.3468537) have also been developed
    to reduce bias in ML models, while retaining good performances.'
  prefs: []
  type: TYPE_NORMAL
- en: Human-based Solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, and perhaps most crucially, it is critical to remember that while our
    machines are doing the work for us, **we** are their creators. Data science starts
    and ends with us — humans who are aware of historical biases, decide to prioritise
    fairness, and take steps to mitigate the effects of historical biases. We should
    not cede power to our creations, and should remain in the loop at all stages of
    data analysis. To this end, I would like to add my voice to the chorus calling
    for the creation of transnational third party organisations to audit ML processes,
    and to enforce best practices. While it is no silver bullet, it is a good way
    to check if our ML models are fair and unbiased, and to concretise our commitment
    to the cause. On an organisational level, I am also heartened by the calls for
    increased diversity in data science and ML teams, as I believe that this will
    help to identify and correct existing blind spots in our data analysis processes.
    It is also necessary for business leaders to be aware of the limits of AI, and
    to use it wisely, instead of abusing it in the name of productivity or profit.
  prefs: []
  type: TYPE_NORMAL
- en: As data scientists, we should also take responsibility for our models, and remember
    the power they wield. As much as historical biases arise from the real world,
    I believe that ML tools also have the potential to help us correct present injustices.
    For example, while in the past, racist or sexist recruiters might filter out capable
    applicants because of their prejudices before handing the candidate list to the
    hiring manager, a fair ML model may be able to efficiently find capable candidates,
    disregarding their protected attributes, which might lead to valuable opportunities
    being provided to previously ignored applicants. Of course, this is not an easy
    task, and is itself fraught with ethical questions. However, if our tools can
    indeed shape the world we live in, why not make them reflect the world we want
    to live in, not just the world as it is?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether you are a budding data scientist, a machine learning engineer, or just
    someone who is interested in using ML tools, I hope this blog post has shed some
    light on the ways historical biases can amplify and automate inequality, with
    disastrous impacts. Though ML models and other AI tools have made our lives a
    lot easier, and are becoming inseparable from modern living, we must remember
    that they are not infallible, and that thorough oversight is needed to make sure
    that our tools stay helpful, and not harmful.
  prefs: []
  type: TYPE_NORMAL
- en: Interested in Learning More?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some resources I found useful in learning more about biases and ethics
    in machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Videos**'
  prefs: []
  type: TYPE_NORMAL
- en: '[3 types of bias in AI | Machine learning](https://www.youtube.com/watch?v=59bMh59JQDo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Algorithmic Bias and Fairness: Crash Course AI #18](https://www.youtube.com/watch?v=gV0_raKR2UQ)
    (also linked above)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[How I’m fighting bias in algorithms | Joy Buolamwini](https://www.youtube.com/watch?v=UG_X_7g63rY)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Weapons of Math Destruction | Cathy O’Neil | Talks at Google](https://www.youtube.com/watch?v=TQHs8SA1qpk&t=41s)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Books**'
  prefs: []
  type: TYPE_NORMAL
- en: Weapons of Math Destruction by Cathy O’Neil (highly recommended!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Invisible Women: Data Bias in a World Designed for Men by Caroline Criado-Perez'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Atlas of AI by Kate Crawford
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI Ethics by Mark Coeckelbergh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Feminism by Catherine D’Ignazio and Lauren F. Klein
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Papers**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Overcoming the pitfalls and perils of algorithms: A classification of machine
    learning biases and mitigation methods](https://www.sciencedirect.com/science/article/pii/S0148296322000881)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Bias in Machine Learning — What is it Good for?](https://arxiv.org/pdf/2004.00686.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI Now Institute. (2024, January 10). *Ai now 2017 report*. [https://ainowinstitute.org/publication/ai-now-2017-report-2](https://ainowinstitute.org/publication/ai-now-2017-report-2)
  prefs: []
  type: TYPE_NORMAL
- en: 'Belenguer, L. (2022). AI Bias: Exploring discriminatory algorithmic decision-making
    models and the application of possible machine-centric solutions adapted from
    the pharmaceutical industry. *AI and Ethics*, *2*(4), 771–787\. [https://doi.org/10.1007/s43681-022-00138-8](https://doi.org/10.1007/s43681-022-00138-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016, July
    21). *Man is to computer programmer as woman is to homemaker? Debiasing word embeddings*.
    arXiv.org. [https://doi.org/10.48550/arXiv.1607.06520](https://doi.org/10.48550/arXiv.1607.06520)
  prefs: []
  type: TYPE_NORMAL
- en: 'Chakraborty, J., Majumder, S., & Menzies, T. (2021). Bias in machine learning
    software: Why? how? what to do? *Proceedings of the 29th ACM Joint Meeting on
    European Software Engineering Conference and Symposium on the Foundations of Software
    Engineering*. [https://doi.org/10.1145/3468264.3468537](https://doi.org/10.1145/3468264.3468537)'
  prefs: []
  type: TYPE_NORMAL
- en: Gutbezahl, J. (2017, June 13). *5 types of statistical biases to avoid in your
    analyses*. Business Insights Blog. [https://online.hbs.edu/blog/post/types-of-statistical-bias](https://online.hbs.edu/blog/post/types-of-statistical-bias)
  prefs: []
  type: TYPE_NORMAL
- en: Heaven, W. D. (2023a, June 21). *Predictive policing algorithms are racist.
    they need to be dismantled.* MIT Technology Review. [https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/)
  prefs: []
  type: TYPE_NORMAL
- en: Heaven, W. D. (2023b, June 21). *Predictive policing is still racist-whatever
    data it uses*. MIT Technology Review. [https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect.](https://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect.)
  prefs: []
  type: TYPE_NORMAL
- en: Hellström, T., Dignum, V., & Bensch, S. (2020, September 20). *Bias in machine
    learning — what is it good for?*. arXiv.org. [https://arxiv.org/abs/2004.00686](https://arxiv.org/abs/2004.00686)
  prefs: []
  type: TYPE_NORMAL
- en: '*Historical bias in AI systems*. The Australian Human Rights Commission. (2020,
    November 24). [https://humanrights.gov.au/about/news/media-releases/historical-bias-ai-systems#:~:text=Historical%20bias%20arises%20when%20the,by%20women%20was%20even%20worse.](https://humanrights.gov.au/about/news/media-releases/historical-bias-ai-systems#:~:text=Historical%20bias%20arises%20when%20the,by%20women%20was%20even%20worse.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memarian, B., & Doleck, T. (2023). Fairness, accountability, transparency,
    and ethics (fate) in Artificial Intelligence (AI) and Higher Education: A systematic
    review. *Computers and Education: Artificial Intelligence*, *5*, 100152\. [https://doi.org/10.1016/j.caeai.2023.100152](https://doi.org/10.1016/j.caeai.2023.100152)'
  prefs: []
  type: TYPE_NORMAL
- en: Obermeyer, Z., Powers, B., Vogeli, C., & Mullainathan, S. (2019). Dissecting
    racial bias in an algorithm used to manage the health of populations. *Science*,
    *366*(6464), 447–453\. [https://doi.org/10.1126/science.aax2342](https://doi.org/10.1126/science.aax2342)
  prefs: []
  type: TYPE_NORMAL
- en: 'O’Neil, C. (2017). *Weapons of math destruction: How big data increases inequality
    and threatens democracy*. Penguin Random House.'
  prefs: []
  type: TYPE_NORMAL
- en: Roselli, D., Matthews, J., & Talagala, N. (2019). Managing bias in AI. *Companion
    Proceedings of The 2019 World Wide Web Conference*. [https://doi.org/10.1145/3308560.3317590](https://doi.org/10.1145/3308560.3317590)
  prefs: []
  type: TYPE_NORMAL
- en: Suresh, H., & Guttag, J. (2021). A framework for understanding sources of harm
    throughout the machine learning life cycle. *Equity and Access in Algorithms,
    Mechanisms, and Optimization*. [https://doi.org/10.1145/3465416.3483305](https://doi.org/10.1145/3465416.3483305)
  prefs: []
  type: TYPE_NORMAL
- en: 'van Giffen, B., Herhausen, D., & Fahse, T. (2022). Overcoming the pitfalls
    and perils of algorithms: A classification of machine learning biases and mitigation
    methods. *Journal of Business Research*, *144*, 93–106\. [https://doi.org/10.1016/j.jbusres.2022.01.076](https://doi.org/10.1016/j.jbusres.2022.01.076)'
  prefs: []
  type: TYPE_NORMAL
- en: Zhang, B. H., Lemoine, B., & Mitchell, M. (2018). Mitigating unwanted biases
    with adversarial learning. *Proceedings of the 2018 AAAI/ACM Conference on AI,
    Ethics, and Society*. [https://doi.org/10.1145/3278721.3278779](https://doi.org/10.1145/3278721.3278779)
  prefs: []
  type: TYPE_NORMAL
