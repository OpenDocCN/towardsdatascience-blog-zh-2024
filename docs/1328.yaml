- en: Simple Ways to Speed Up Your PyTorch Model Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28](https://towardsdatascience.com/simple-ways-to-speed-up-your-pytorch-model-training-9c9d4899313d?source=collection_archive---------3-----------------------#2024-05-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If all machine learning engineers want one thing, **it‚Äôs faster model training**
    ‚Äî maybe after good test metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Alex
    Dremov](../Images/8afeaa6bae03d3b6c436d81127c75a0c.png)](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    [Alex Dremov](https://alexdremov.medium.com/?source=post_page---byline--9c9d4899313d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c9d4899313d--------------------------------)
    ¬∑11 min read¬∑May 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb75109bbcb6622409f76f7e507791cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Julian Hochgesang](https://unsplash.com/@julianhochgesang?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
    / [Unsplash](https://unsplash.com/?utm_source=ghost&utm_medium=referral&utm_campaign=api-credit)
  prefs: []
  type: TYPE_NORMAL
- en: Does this topic even need an introduction?
  prefs: []
  type: TYPE_NORMAL
- en: Speeding up machine learning model training is one thing that all machine learning
    engineers want. Faster training equals faster experiments equals faster iterations
    for your product. Also, it means that one model training will require fewer resources.
    So, straight to the point
  prefs: []
  type: TYPE_NORMAL
- en: Containerization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yes, this will not speed up your training on its own. But this targets another
    important aspect ‚Äî reproducibility. Sometimes virtualenv with fixed library versions
    is enough, but I encourage you to take one step further and build an all-in-one
    docker container for your model training.
  prefs: []
  type: TYPE_NORMAL
- en: This ensures that the environment is fully consistent during debugging, profiling,
    and final training. The last thing you want is to optimize a part of code that
    is no longer a bottleneck due to python12 speed up, for example. Or even a bug
    that is not reproducible on different CUDA versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a starting point, you can use pre-built images from NVIDIA. They already
    have CUDA, PyTorch, and other popular libs installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## PyTorch | NVIDIA NGC'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is a GPU accelerated tensor computational framework. Functionality can
    be extended with common Python libraries‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: catalog.ngc.nvidia.com](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: üí° A Docker container is the ultimate solution for problems like
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚ÄúHey, it works on my machine. I have no idea why it doesn‚Äôt on yours.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Get comfortable with PyTorch profiler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before optimizing anything, you have to understand how long some parts of your
    code run. Pytorch profiler is *almost* an all-in-one tool for profiling training.
    It‚Äôs able to record:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU operations timings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUDA kernels timings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory consumption history
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That‚Äôs all you need. And it‚Äôs easy to enable!
  prefs: []
  type: TYPE_NORMAL
- en: 'To record events, all you need is to embed training into a profiler context
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After that, you can launch the tensorboard and view profiling traces. Do not
    forget to install [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/?ref=alexdremov.me).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## PyTorch Profiler With TensorBoard - PyTorch Tutorials 2.3.0+cu121 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare the data and model Use profiler to record execution events Run the profiler
    Use TensorBoard to view results and‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Profiler has a lot of different options, but the most important are `activities`
    and `profile_memory`. You can experiment with other options, but keep in mind
    a simple rule: **the fewer options you''ve enabled, the less overhead you have**.'
  prefs: []
  type: TYPE_NORMAL
- en: So, if you want to profile CUDA kernel execution timings, it is a good idea
    to turn off CPU profiling and all other features. In this mode, profiling will
    be as close to the real execution as possible.
  prefs: []
  type: TYPE_NORMAL
- en: To make traces easier to understand, consider adding profiling contexts that
    describe core parts of your code. If profiling is not enabled, those are no-op.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, the labels that you use will be visible in traces. So, it will be
    easier to identify code blocks. Or even more granular inside mode‚Äôs forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Understanding PyTorch traces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After you gather traces, open them in the tensorboard. That‚Äôs what the CPU
    + CUDA profile looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b207f5f91e4560da06402145f3cfb486.png)'
  prefs: []
  type: TYPE_IMG
- en: ¬© Copyright 2024, PyTorch | [https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html?ref=alexdremov.me)
  prefs: []
  type: TYPE_NORMAL
- en: 'Straight away, find the core parts of any training:'
  prefs: []
  type: TYPE_NORMAL
- en: data loading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: forward pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: backward pass
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backward pass is handled by PyTorch in a separate thread (thread 16893 on the
    image above), so it is easy to identify.
  prefs: []
  type: TYPE_NORMAL
- en: Data loading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For data loading, we want near-zero timings.
  prefs: []
  type: TYPE_NORMAL
- en: No compromises.
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôs because during data loading GPU does nothing, which under-utilizes available
    resources. However, data processing can be overlapped with GPU computing as those
    are independent parts.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily identify areas where GPU is idle ‚Äî just look at *GPU Est. SM
    Efficiency* and *GPU Utilization* figures in the profiler‚Äôs trace. Areas with
    zero activity are our patients. That‚Äôs where GPU does nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple solution for that is:'
  prefs: []
  type: TYPE_NORMAL
- en: process data in the background process (no GIL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: process data augmentations and transforms in parallel processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you use PyTorch DataLoader, then it can be easily achieved by specifying
    `num_workers`. It's more complicated if you use `IterableDataset`, as then data
    will be duplicated. However, this issue still can be solved by using [get_worker_info()](https://pytorch.org/docs/stable/data.html?ref=alexdremov.me#torch.utils.data.IterableDataset)
    - you need to adjust iteration in a way so that each worker receives different,
    non-intersecting rows.
  prefs: []
  type: TYPE_NORMAL
- en: For more configurable processing, you may consider implementing multi-process
    transforms yourself with `multiprocessing`
  prefs: []
  type: TYPE_NORMAL
- en: üí° If you never checked your code‚Äôs data processing speed, then this slight modification
    can yield **dramatic speedups**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making friends with memory allocator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You want to be friends with PyTorch‚Äôs CUDA caching allocator.
  prefs: []
  type: TYPE_NORMAL
- en: When you allocate tensors with PyTorch on a CUDA device, PyTorch will use a
    caching allocator. That‚Äôs because `cudaMalloc`/ `cudaFree` are expensive operations
    that we want to avoid, so PyTorch has its allocator that will try to reuse previously
    allocated through `cudaMalloc` blocks. That is, if PyTorch's allocator has an
    appropriate block available, it will give it straight away without calling `cudaMalloc`.
    That way, `cudaMalloc` is called only at the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you‚Äôre dealing with data of variable length, different forward passes
    will require intermediate tensors of different sizes. So, PyTorch‚Äôs allocator
    may not have an appropriate block of data available. In this case, the allocator
    panics and releases allocated previously bocks by calling `cudaFree` to free up
    space for new allocations.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the allocator starts building its cache again, doing tons of `cudaMalloc`,
    which is an expensive operation. You can spot this problem by looking at the memory
    profiler section of the tensorboard profiler viewer.
  prefs: []
  type: TYPE_NORMAL
- en: üí° You also can spot this problem in the traces. It will be visible as calls
    to `cudaMalloc`and `cudaFree`
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/414070253ac9c94f59f8f522399d7fb4.png)'
  prefs: []
  type: TYPE_IMG
- en: PyTorch allocator freaks out | Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: As you see, a red line that corresponds to the allocator‚Äôs reserved memory constantly
    changes. That means that PyTorch allocator is not able to efficiently handle allocation
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: When allocations are handled without the allocator panicking, the red line is
    completely straight
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8e801553645665ba7dff138173a00cd.png)'
  prefs: []
  type: TYPE_IMG
- en: PyTorch allocator works as expected | Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: As I said, that is usually due to variable shapes of tensors. How to fix that?
  prefs: []
  type: TYPE_NORMAL
- en: '**Expandable Segments**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing that is worth trying is to set PyTorch‚Äôs relatively new allocator
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*If set to* `*True*`*, this setting instructs the allocator to create CUDA
    allocations that can later be expanded to better handle cases where a job changes
    allocation sizes frequently, such as having a changing batch size.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, this tells PyTorch allocator to allocate blocks that could be expanded in
    the future, which is exactly our case. Though, if size variations are too big,
    it still may fail to solve the issue. In this case, move to the next option.
  prefs: []
  type: TYPE_NORMAL
- en: '**Make allocations variate less**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another possible solution is to make data shapes consistent. That way it will
    be easier for the allocator to find an appropriate data block to reuse.
  prefs: []
  type: TYPE_NORMAL
- en: To accomplish that, you may pad data to the same sizes. Or you can preheat the
    allocator by running a model with maximum input sizes.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about PyTorch allocator modification in the following article
  prefs: []
  type: TYPE_NORMAL
- en: '[## CUDA semantics - PyTorch 2.3 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: A guide to torch.cuda, a PyTorch module to run CUDA operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/notes/cuda.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Tidy up allocations history
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We want to use all available GPU memory ‚Äî that allows us to run big batches
    and process data faster. However, at some point, you will encounter a *CUDA out-of-memory*
    error when increasing batch size. What causes this error?
  prefs: []
  type: TYPE_NORMAL
- en: To debug this, we can view the allocator‚Äôs memory history. It can be recorded
    through PyTorch and then visualized at [https://pytorch.org/memory_viz](https://pytorch.org/memory_viz?ref=alexdremov.me)
  prefs: []
  type: TYPE_NORMAL
- en: '**Start:** `torch.cuda.memory._record_memory_history(max_entries=100000)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Save:** `torch.cuda.memory._dump_snapshot(file_name)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stop:** `torch.cuda.memory._record_memory_history(enabled=None)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visualization will draw something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/270f72428251b625a672af134cf2f454.png)'
  prefs: []
  type: TYPE_IMG
- en: ¬© Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  prefs: []
  type: TYPE_NORMAL
- en: The x-axis represents time, the y-axis represents total used memory, and colourful
    blocks represent tensors. So, it shows when tensors were allocated and when it
    was released.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice narrow spikes ‚Äî those are short-lasting tensors that take up
    a lot of space. By clicking on a tensor, you can get information on where this
    tensor was allocated. We want to minimize those spikes as they limit efficient
    memory usage. Check out what caused this spike and consider other ways of computing
    what you intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from spikes, it‚Äôs easy to detect memory leaks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d257a3b5af410d7d0bb7b3e46ecda12d.png)'
  prefs: []
  type: TYPE_IMG
- en: ¬© Copyright 2024, PyTorch | [https://pytorch.org/blog/understanding-gpu-memory-1/](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me)
  prefs: []
  type: TYPE_NORMAL
- en: As you see, some data after the first forward is not cleared. By clicking on
    blocks you can get the idea where these tensors come from. In the image is the
    case when gradients are not cleared after the training step, so they lay dead
    during the forward pass, limiting the ability to increase the batch size to fit
    more data.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Understanding GPU Memory 1: Visualizing All Allocations over Time'
  prefs: []
  type: TYPE_NORMAL
- en: 'During your time with PyTorch on GPUs, you may be familiar with this common
    error message:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/blog/understanding-gpu-memory-1/?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Speed up the model and use less memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What can be better than this? We can achieve so by using the **FlashAttention**
    kernel for calculating dot-product attention.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - Dao-AILab/flash-attention: Fast and memory-efficient exact attention'
  prefs: []
  type: TYPE_NORMAL
- en: Fast and memory-efficient exact attention. Contribute to Dao-AILab/flash-attention
    development by creating an account‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Dao-AILab/flash-attention?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: If you haven‚Äôt heard about it, it is a way of calculating precise dot product
    attention without constructing the attention matrix explicitly. That optimizes
    GPU‚Äôs io operations which improves speed and also **dramatically** minimizes memory
    consumption. There‚Äôs simply no reason not to use it.
  prefs: []
  type: TYPE_NORMAL
- en: üò° Unfortunately, there‚Äôs one reason not to use it ‚Äî hardware.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Flash attention only works with `fp16` and `bf16` precision on compatible hardware.
    That is NVIDIA Ampere, Hooper, etc
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Other libraries use flash attention under the hood, so you may consider using
    other variants that better fit your codebase.
  prefs: []
  type: TYPE_NORMAL
- en: '**XFormers**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - facebookresearch/xformers: Hackable and optimized Transformers building
    blocks, supporting‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Hackable and optimized Transformers building blocks, supporting a composable
    construction. - facebookresearch/xformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/facebookresearch/xformers?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformer Engine**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## GitHub - NVIDIA/TransformerEngine: A library for accelerating Transformer
    models on NVIDIA GPUs‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: A library for accelerating Transformer models on NVIDIA GPUs, including using
    8-bit floating point (FP8) precision on‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/NVIDIA/TransformerEngine?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch itself!**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'That is true, new versions of PyTorch may use flash attention when applicable.
    To activate this mode, you need to execute attention blocks in the context manager
    that specify which attention strategy to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## torch.nn.functional.scaled_dot_product_attention - PyTorch 2.3 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: Read the PyTorch Domains documentation to learn more about domain-specific libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch-nn-functional-scaled-dot-product-attention)
  prefs: []
  type: TYPE_NORMAL
- en: Optimize multi-GPU data redundancy ‚Äî FSDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you use multiple GPUs to run your training, the basic solution is to use
    the `DistributedDataParallel` class. This way, several identical processes are
    spawned, and gradients are aggregated during the backward step.
  prefs: []
  type: TYPE_NORMAL
- en: However, that is sub-optimal!
  prefs: []
  type: TYPE_NORMAL
- en: The problem is as we spawned identical processes, then we have identical models
    and optimiser states on each GPU, which is redundant. The solution is to shard
    data across. We can do so using the Fully Sharded Data Parallel PyTorch wrapper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b542604a9a173f89aed4bc0375e2e515.png)'
  prefs: []
  type: TYPE_IMG
- en: ¬© Copyright 2024, PyTorch | https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs: []
  type: TYPE_NORMAL
- en: 'As I said, when training on several GPUs, each process has exact copies of
    the same data when training with DDP. We can optimize it, by implementing several
    enhancements:'
  prefs: []
  type: TYPE_NORMAL
- en: Shard optimizer state (ZeRO 1)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training with DDP, each process holds a complete copy of the optimizer
    states. With ZeRO1, we shard these optimizer states across all ranks such that
    each rank holds only a portion of the optimizer states. During the backward pass,
    each rank only needs to gather the optimizer states relevant to its parameters
    to make an optimization step. This reduction in redundancy helps conserve memory.
  prefs: []
  type: TYPE_NORMAL
- en: üí° In case of the Adam, which holds parameters at roughly twice the model size,
    sharding the optimizer state among 8 ranks means each rank **stores only one quarter
    (2/8) of the total state size.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shard gradients (ZeRO 2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We shard optimizer states. Now, we will modify the optimizer step to shard
    gradients too. If one rank has optimizer states for a portion of parameters, then
    we will:'
  prefs: []
  type: TYPE_NORMAL
- en: aggregate all gradients relevant to the states the rank holds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: calculate optimization step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: send optimization step for a portion of parameters to all other ranks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you noticed, now each rank does not need to hold a full replica of gradients.
    We can send gradients to a relevant rank as soon as they are available. So, we
    can reduce peak memory consumption even further.
  prefs: []
  type: TYPE_NORMAL
- en: Shard model parameters (ZeRO 3)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is about to be epic.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to store a full copy of the model on each rank? Let‚Äôs shard model
    parameters between all ranks. Then, we‚Äôre going to fetch the required parameters
    just in time during forward and backward.
  prefs: []
  type: TYPE_NORMAL
- en: üí° In case of large models, these optimisations can drammaticaly decrease memory
    consumption
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to use FSDP?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quite simple actually. All we need is to wrap the model with FSDP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also specify the sharding strategy of FSDP. For example, we can select
    the `SHARD_GRAD_OP` strategy to achieve behaviour similar to that of ZeRO2\. You
    can learn about other strategies here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## FullyShardedDataParallel - PyTorch 2.3 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: A wrapper for sharding module parameters across data parallel workers. This
    is inspired by Xu et al. as well as the‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/fsdp.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.distributed.fsdp.ShardingStrategy)
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can wrap with FSDP submodules. In the example above, only one FSDP
    module is used, which will reduce computation efficiency and memory efficiency.
    The way it works is that, suppose your model contains 100 Linear layers. If you
    do FSDP(model), there will only be one FSDP unit which wraps the entire model.
    In that case, the allgather would collect the full parameters for all 100 linear
    layers, and hence won‚Äôt save CUDA memory for parameter sharding.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can wrap submodules explicitly or define an auto-wrap policy. To learn
    more about FSDP, read the PyTorch guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
    [## Getting Started with Fully Sharded Data Parallel(FSDP) - PyTorch Tutorials
    2.3.0+cu121‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Note View and edit this tutorial in github. Training AI models at a large scale
    is a challenging task that requires a‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Magic speedup with `torch.compile`
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That is, torch compile can speed up your code by several percent by just enabling
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Torch traces your execution graph and tries to compile it into an efficient
    format so that the model can be executed almost without Python invocation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic usage is to wrap the model with compile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will execute almost instantly. The actual tracing will happen only during
    the first forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also has a lot of options that are worth to try:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## torch.compile - PyTorch 2.3 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizes given model/function using TorchDynamo and specified backend. Concretely,
    for every frame executed within the‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/docs/stable/generated/torch.compile.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------#torch.compile)
  prefs: []
  type: TYPE_NORMAL
- en: üí° Torch compiler is a big feature that will be covered in the next posts!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Stay tuned
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Learn more about torch compile here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## Introduction to torch.compile - PyTorch Tutorials 2.3.0+cu121 documentation'
  prefs: []
  type: TYPE_NORMAL
- en: torch.compile is included in the latest PyTorch. Running TorchInductor on GPU
    requires Triton, which is included with‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pytorch.org](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html?ref=alexdremov.me&source=post_page-----9c9d4899313d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This post is in no way complete with explanations. Rather, that is a list of
    speed-ups that are worth trying straight away. Hope that it was helpful. Feel
    free to leave a comment!
  prefs: []
  type: TYPE_NORMAL
- en: Consider subscribing
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://alexdremov.me*](https://alexdremov.me/simple-ways-to-speedup-your-pytorch-model-training/)
    *on May 28, 2024.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Images from PyTorch Blog were used, which is a project of the Linux Foundation
    and is subject to the Linux Foundation* [*policies*](https://www.linuxfoundation.org/legal/terms)*.
    So, all images may be used as allowed by* [*Creative Commons Attribution 3.0 License*](http://creativecommons.org/licenses/by/3.0/)'
  prefs: []
  type: TYPE_NORMAL
