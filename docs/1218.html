<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Long-form video representation learning (Part 3: Long-form egocentric video representation learning)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Long-form video representation learning (Part 3: Long-form egocentric video representation learning)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14">https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="17e6" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">We explore novel video representation learning methods that are equipped with long-form reasoning capability. This is Part III providing a sneak peek into our latest and greatest explorations for “long-form” egocentric video representation learning. See <a class="af hd" href="https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100" rel="noopener">Part I</a> on video as a graph and is <a class="af hd" href="https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71" rel="noopener">Part II</a> on sparse video-text transformers.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Subarna Tripathi" class="l ep by dd de cx" src="../Images/0a949764464eeef40a6d3ae0d183873f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IiET-CtXqIrFAtaoWbjf1Q.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------" rel="noopener follow">Subarna Tripathi</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="75b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first two blogs in this series described how different architectural motifs ranging from graph neural networks to sparse transformers addressed the challenges of “long-form” video representation learning. We showed how explicit graph based methods can aggregate 5-10X larger temporal context, but they were two-stage methods. Next, we explored how we can make memory and compute efficient end-to-end learnable models based on transformers and aggregate over 2X larger temporal context.</p><p id="1471" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this blog, I’ll take you to our latest and greatest explorations, especially for egocentric video understanding. As you can imagine, an egocentric or first-person video (captured usually by head-mounted cameras) is most likely coming from an always-ON camera, meaning the videos are really really long, with a lot of irrelevant visual information, specially when the camera wearer move their heads. And, this happens a lot of times with head mounted cameras. A proper analysis of such first-person videos can enable a detailed understanding of how humans interact with the environment, how they manipulate objects, and, ultimately, what are their goals and intentions. Typical applications of egocentric vision systems require algorithms able to represent and process video over temporal spans that last in the order of minutes or hours. Examples of such applications are action anticipation, video summarization, and episodic memory retrieval.</p><h1 id="10bf" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Egocentric action scene graphs:</h1><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc od"><img src="../Images/23d0558b28f8b7f83369cb2f3083eeb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lMZNEYch4MHZMGZiift9FQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Figure 1: (Image by author) <em class="ou">Egocentric Action Scene Graphs are temporal dynamic graphs (G(t)) capturing the action verbs (nodes in blue), direct or active objects (nodes in green), and other objects (nodes in yellow) involved in the activity performed by a camera wearer (the orange CW node). Edges between nodes represent relationship between the verb and the objects or between object pairs. The graph evolves through time providing a long-from representation of the egocentric video (dashed lines). Objects of interaction are grounded with bounding boxes</em></figcaption></figure><p id="c011" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this joint work with University of Catania, we present Egocentric Action Scene Graphs (EASGs), a new representation for long-form understanding of egocentric videos. EASGs extend standard manually-annotated representations of egocentric videos, such as verb-noun action labels, by providing a temporally evolving graph-based description of the actions performed by the camera wearer. The description also includes interacted objects, their relationships, and how actions unfold in time. Through a novel annotation procedure, we extend the Ego4D dataset adding manually labeled Egocentric Action Scene Graphs which offer a rich set of annotations for long-from egocentric video understanding.</p><p id="2b89" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">EASGs provide annotations for a video clip in the form of a dynamic graph. We formalize an EASG as a time-varying directed graph G(t) = (V (t), E(t)), where V (t) is the set of nodes at time t and E(t) is the set of edges between such nodes (Figure 2). Each temporal realization of the graph G(t) corresponds to an egocentric action spanning over a set of three frames defined as in [<a class="af hd" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf" rel="noopener ugc nofollow" target="_blank">Ego4D</a>]: the precondition (PRE), the point of no return (PNR) and the postcondition (POST) frames. The graph G(t) is hence effectively associated to three frames: F(t) = {PREₜ, PNRₜ, POSTₜ}, as shown in figure 1 below.</p><h2 id="5710" class="ov ng fq bf nh ow ox oy nk oz pa pb nn ms pc pd pe mw pf pg ph na pi pj pk pl bk">Egocentric scene graph generation:</h2><p id="0c8e" class="pw-post-body-paragraph mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne fj bk">Figure 2 shows an example of an annotated graph in details.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pr"><img src="../Images/f86b56508dd81639cb80b238588b53f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5HvS40yHbIp-mjS_N30oA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author</figcaption></figure><p id="0c1f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We obtain an initial EASG leveraging existing annotations from Ego4D, with initialization and refinement procedure. e.g. we begin with adding the camera wearer node, verb node and and the default action edge from camera wearer node to the verb node. The annotation pipeline is shown in figure 3 below.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pr"><img src="../Images/741dc3085e12cc53e6f1cffdc88c7b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-djVawZoslQmxP3kYu_VKQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">image by author</figcaption></figure><p id="9694" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, we do the graph refinement via inputs from 3 annotators. The validation stage aggregates the data received from three annotators and ensures the quality of the final annotations as shown below.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ps"><img src="../Images/85a017a1199a4556f35822bb4c5c2eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEuFwzDwpJaGNIZrbsl0dQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Figure 4 (Image by author): Examples of questions (with correct answers in red) asked to the annotators in the validation stage to resolve ambiguities between<br/>the labels provided in the annotation stage.</figcaption></figure><p id="2a6a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As it can be noted, the EASG dataset is unique in its labels. And, in the table below you can see how this new dataset compares with other video datasets with visual relations, in terms of labels and size.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ps"><img src="../Images/81de1def11cca0f0c1ba817ee558221d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbwrJppbOB3LQBT1Yr-tEA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author: Comparison with existing video scene graph datasets. Our Ego4D-EASG dataset is the only one explicitly designed for long-form egocentric video understanding, featuring egocentric videos, dynamic graphs, an average sequence length of 3.1 minutes and an average number of 28.3 graphs per sequence. *measured in object-relation-object triplets. **intransitive + transitive verb predicates</figcaption></figure><figure class="oe of og oh oi oj"><div class="pt ip l ed"><div class="pu pv l"/></div></figure><p id="f414" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The above video visually demonstrates one example EASG for annotation as it dynamically changes as the content of the video changes.</p><p id="dfcf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After the creation of this unique dataset, we will now describe different tasks that are evaluated on this dataset. The first set of tasks is about generating action scene graphs which stems from the image scene graph generation literature. In other words we aim to learn EASG representations in a supervised way and measure its performance in standard Recall metrics used in scene graph literature. We devise baselines and compare the EASG generation performance of different baselines on this dataset.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ps"><img src="../Images/ea0522254893e4e567c963e15017e4a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHonBD3xEsDm_2r-oNB_IA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">(image by author) Baseline results for three EASG generation tasks (i.e. Edge Cls, SG Cls, and EASG Cls) in terms of Recall@K</figcaption></figure><h2 id="9106" class="ov ng fq bf nh ow ox oy nk oz pa pb nn ms pc pd pe mw pf pg ph na pi pj pk pl bk"><strong class="al">Long-from understanding tasks with EASG:</strong></h2><p id="857c" class="pw-post-body-paragraph mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne fj bk">We show the potential of the EASG representation in the downstream tasks of action anticipation and activity summarization. Both tasks require to perform long-form reasoning over the egocentric video, processing long video sequences spanning over different time-steps. Following recent results showing the flexibility of Large Language Models (LLMs) as symbolic reasoning machines, we perform these experiments with LLMs accessed via the OpenAI API. The experiments aim to examine the expressive power of the EASG representation and its usefulness for downstream applications. We show that EASG offers an expressive way of modeling long-form activities, in comparison with the gold-standard verb-noun action encoding, extensively adopted in egocentric video community.</p><p id="e686" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Action anticipation with EASGs:</strong></p><p id="10e7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the action anticipation task, we use the GPT3 text-davinci-003 model. We prompt the model to predict the future action from a sequence of length T ∈ {5, 20}. We compare two types of representations — EASG and sequences of verb-noun pairs. Below table shows the results of this experiment.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pw"><img src="../Images/4c45c4973e60d12c8a2fa4e038e9f73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30G23pB1qWlTQ-P-9pLL1A.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author: Performance Comparison for the Action anticipation task</figcaption></figure><p id="c772" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even short EASG sequences (T =5) tend to outperform long V-N sequences (T = 20), highlighting the higher representation power of EASG, when compared to standard verb-noun representations. EASG representations achieve the best results for long sequences (T = 20).</p><p id="8ca2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Long-form activity summarization with EASGs:</strong></p><p id="3546" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We select a subset of 147 Ego4D-EASG clips containing human-annotated summaries describing the activities performed within the clip in 1–2 sentences from Ego4D. We construct three types of input sequences: sequences of graphs S-EASG = [G(1), G(2), …, G(Tmax)], sequences of verb-noun pairs svn = [s-vn(1), s-vn(2), …, s-vn(Tmax)], and sequences of original Ego4D narrations, matched with the EASG sequence. This last input is reported for reference, as we expect summarization from narrations to bring the best performance, given the natural bias of language models towards this representation.</p><p id="fff4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Results reported in the below table indicate strong improvement in CIDEr score over the sequence of verb-noun inputs, showing that models which process EASG inputs capturing detailed object action relationships, will generate more specific, informative sentences that align well with reference descriptions.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pw"><img src="../Images/0a1c6d4d80fc0fb45487d2a43b89d225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oJ0UuVVToEmiP2eGPCMig.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author: Results of activity summarization with EASGs and verb-noun representations</figcaption></figure><p id="19c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We believe that these contributions mark a step forward in long-form egocentric video understanding.</p><p id="7f67" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Highlights:</strong></p><ul class=""><li id="33b1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne px py pz bk">We introduce Egocentric Action Scene Graphs, a novel representation for long-form understanding of egocentric videos;</li><li id="92eb" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">We extend Ego4D with manually annotated EASG labels, which are gathered through a novel annotation procedure;</li><li id="2910" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">We propose a EASG generation baseline and provide initial baseline results;</li><li id="533f" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">We present experiments that highlight the effectiveness of the EASG representation for long-form egocentric video understanding. We will release the dataset and the code to replicate data annotation and the<br/>experiments;</li><li id="d17d" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">We will present this work at <strong class="ml fr">CVPR 2024</strong>, next month.</li><li id="12b6" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">Paper: <a class="af hd" href="https://arxiv.org/abs/2312.03391" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2312.03391</a> and Code: <a class="af hd" href="https://github.com/fpv-iplab/EASG" rel="noopener ugc nofollow" target="_blank">https://github.com/fpv-iplab/EASG</a></li></ul><h1 id="d807" class="nf ng fq bf nh ni nj gq nk nl nm gt nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Temporal grounding for episodic tasks:</h1><p id="8431" class="pw-post-body-paragraph mj mk fq ml b go pm mn mo gr pn mq mr ms po mu mv mw pp my mz na pq nc nd ne fj bk">In recent years, egocentric video-language pre-training (VLP) has been adopted significantly in academia and in industry. A line of works such as <a class="af hd" href="https://github.com/showlab/EgoVLP/blob/main/README.md" rel="noopener ugc nofollow" target="_blank">EgoVLP</a>, <a class="af hd" href="https://shramanpramanick.github.io/EgoVLPv2/" rel="noopener ugc nofollow" target="_blank">EgoVLPv2</a> learn transferable spatial-temporal representations from large-scale video-text datasets. Recently, <a class="af hd" href="https://facebookresearch.github.io/LaViLa/" rel="noopener ugc nofollow" target="_blank">LaViLa</a> showed that VLP can benefit from the dense narrations generated by Large Language Models (LLMs). However, all such methods do hit the memory and compute bottleneck while processing video sequences, each consisting of a small number of frames (e.g. 8 or 16 frame models), leading to limited temporal context aggregation capability. On the contrary, our model, called <strong class="ml fr">LAVITI</strong>, is equipped with long-form reasoning capability (<strong class="ml fr">1,000</strong> frames vs 16 frames) and is not limited to a small number of input frames.</p><p id="709b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this ongoing work, we devised a novel approach to learning language, video, and <strong class="ml fr"><em class="qf">temporal representations</em></strong> in long-form videos via contrastive learning. Unlike existing methods, this new approach aims to align language, video, and temporal features by extracting meaningful moments in <strong class="ml fr">untrimmed</strong> videos by formulating it as a direct set prediction problem. LAVITI outperforms existing state-of-the-art methods by a significant margin on egocentric action recognition, yet is trainable on memory and compute-bound systems. Our method can be trained on the Ego4D dataset with only 8 NVIDIA RTX-3090 GPUs in a day.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pw"><img src="../Images/cafb9f150d4a0d6617a9f5261cc8c744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Htoifux3n-t8PFzsyfg1hA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author: Performance on CharadesEgo. Our approach achieves significant gains in both zero-shot and fine-tuned settings. ZS and FT stand for zero-shot and finetuning, respectively.</figcaption></figure><p id="0977" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As our model is capable of long-form video understanding with explicit temporal alignment, the Ego4D Natural Language Query (NLQ) task is a natural fit with the pre-training targets. We can directly predict intervals which are aligned with language query given a video; therefore, LAVITI can<br/>perform the NLQ task under the zero-shot setting (without modifications of the architecture and re-training on NLQ annotations).</p><p id="a03a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the near future, we plan on assessing its potential to learn improved representations for episodic memory tasks including NLQ and Moment Query (MQ). To summarize, we are leveraging existing foundation models (essentially “short-term”) for creating “long-form” reasoning module aiming at 20X-50X larger context aggregation.</p><p id="808a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Highlights:</strong></p><p id="c1a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We devised exciting new ways for egocentric video understanding. Our contributions are manifold.</p><ul class=""><li id="0e75" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne px py pz bk">Pre-training objective aligns language, video, and <em class="qf">temporal features</em> jointly by extracting meaningful moments in <strong class="ml fr">untrimmed</strong> videos;</li><li id="13c4" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">formulating the video, language and temporal alignment as a direct set prediction problem;</li><li id="6fb1" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">enabling long-form reasoning over potentially <strong class="ml fr"><em class="qf">thousands of frames</em></strong> of a video in a memory-compute efficient way;</li><li id="c191" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">demonstrating the efficacy of LAVITI by its superior performance on CharadesEgo action recognition;</li><li id="16d6" class="mj mk fq ml b go qa mn mo gr qb mq mr ms qc mu mv mw qd my mz na qe nc nd ne px py pz bk">Enabling zero-shot natural language query (NLQ) task without needing to train additional subnetworks or NLQ annotations.</li></ul><p id="1f02" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Watch out for more exciting results with this new paradigm of “long-form” video representation learning!</p></div></div></div></div>    
</body>
</html>