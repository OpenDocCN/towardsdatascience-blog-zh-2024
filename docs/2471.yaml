- en: 'Decision Tree Regressor, Explained: A Visual Guide with Code Examples'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树回归器解释：带有代码示例的可视化指南
- en: 原文：[https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10](https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10](https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10)
- en: REGRESSION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: Trimming branches smartly with cost-complexity pruning
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明智地修剪分支，通过成本复杂度修剪
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    ·11 min read·Oct 10, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    ·阅读时间：11分钟·2024年10月10日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
    [## 决策树分类器解释：带有代码示例的初学者可视化指南'
- en: A fresh look on our favorite upside-down tree
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对我们喜爱的倒立树的全新视角
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
- en: Decision Trees aren’t limited to categorizing data — they’re equally good at
    predicting numerical values! Classification trees often steal the spotlight, but
    Decision Tree Regressors (or Regression Trees) are powerful and versatile tools
    in the world of continuous variable prediction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树不仅仅限于数据分类——它们同样擅长预测数值！分类树经常占据焦点，但决策树回归器（或回归树）在连续变量预测的世界中是强大且多功能的工具。
- en: While we’ll discuss the mechanics of regression tree construction (which are
    mostly similar to the classification tree), here, we’ll also advance beyond the
    *pre*-pruning methods like “minimal sample leaf" and "max tree depth” introduced
    in the classifier article. We’ll explore the most common *post*-pruning method
    which is **cost complexity pruning**, that introduces a complexity parameter to
    the decision tree’s cost function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将讨论回归树构建的机制（这些机制与分类树大致相同），但在这里，我们还将超越分类器文章中介绍的*前*修剪方法，如“最小样本叶节点”和“最大树深度”。我们将探索最常见的*后*修剪方法——**成本复杂度修剪**，它为决策树的成本函数引入了一个复杂度参数。
- en: '![](../Images/af2717ef7b5f2f1e541ab5ba0909511d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af2717ef7b5f2f1e541ab5ba0909511d.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可视化图：作者使用Canva Pro创作。优化为移动端显示；在桌面端可能显得过大。
- en: Definition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义
- en: A Decision Tree for regression is a model that predicts numerical values using
    a tree-like structure. It splits data based on key features, starting from a root
    question and branching out. Each node asks about a feature, dividing data further
    until reaching leaf nodes with final predictions. To get a result, you follow
    the path matching your data’s features from root to leaf.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回归决策树是一种使用树状结构预测数值的模型。它根据关键特征拆分数据，从根问题开始并分支。每个节点询问一个特征，继续拆分数据，直到达到叶节点并作出最终预测。要获得结果，你需要从根节点到叶节点，沿着匹配数据特征的路径进行跟踪。
- en: '![](../Images/4d58bb0bb413cc741a3d2babe2bb0a24.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d58bb0bb413cc741a3d2babe2bb0a24.png)'
- en: Decision Trees for regression predict numerical outcomes by following a series
    of data-driven questions, narrowing down to a final value.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 回归决策树通过一系列基于数据的问题来预测数值结果，逐步缩小范围直到最终结果。
- en: 📊 Dataset Used
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 📊 使用的数据集
- en: To demonstrate our concepts, we’ll work with [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).
    This dataset is used to predict the number of golfers visiting on a given day
    and includes variables like weather outlook, temperature, humidity, and wind conditions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们的概念，我们将使用[我们的标准数据集](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)。该数据集用于预测某一天访客高尔夫球场的人数，包含天气展望、温度、湿度和风力等变量。
- en: '![](../Images/be5edbc6b8231b1605ca2447d8c33657.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be5edbc6b8231b1605ca2447d8c33657.png)'
- en: 'Columns: ‘Outlook’ (one-hot encoded to sunny, overcast, rain), ‘Temperature’
    (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (Yes/No) and ‘Number of Players’ (numerical,
    target feature)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列：‘天气展望’（通过独热编码转为晴天、阴天、雨天），‘温度’（以华氏度表示），‘湿度’（以百分比表示），‘风力’（是/否）和‘玩家数量’（数值型，目标特征）
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Main Mechanism
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要机制
- en: 'The Decision Tree for regression operates by recursively dividing the data
    based on features that best reduce prediction error. Here’s the general process:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回归决策树通过递归地根据最能减少预测误差的特征来拆分数据。以下是一般过程：
- en: Begin with the entire dataset at the root node.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根节点开始，使用整个数据集。
- en: Choose the feature that minimizes a specific error metric (such as mean squared
    error or variance) to split the data.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最小化特定误差指标（如均方误差或方差）的特征来拆分数据。
- en: Create child nodes based on the split, where each child represents a subset
    of the data aligned with the corresponding feature values.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据拆分创建子节点，每个子节点表示与相应特征值对齐的数据子集。
- en: Repeat steps 2–3 for each child node, continuing to split the data until a stopping
    condition is reached.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个子节点重复步骤2–3，继续拆分数据直到达到停止条件。
- en: Assign a final predicted value to each leaf node, typically **the average of
    the target values** in that node.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个叶节点分配一个最终的预测值，通常是该节点中目标值的**平均值**。
- en: '![](../Images/48ae23f237c31ae05ba52c569f790fb8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48ae23f237c31ae05ba52c569f790fb8.png)'
- en: Training Steps
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练步骤
- en: 'We will explore the regression part in the decision tree algorithm CART (Classification
    and Regression Trees). It builds binary trees and typically follows these steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索决策树算法CART（分类与回归树）中的回归部分。它构建二叉树，通常遵循以下步骤：
- en: 1.Begin with all training samples in the root node.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 从根节点开始，使用所有训练样本。
- en: '![](../Images/62033724ca4273f8d77b2f9184658c43.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62033724ca4273f8d77b2f9184658c43.png)'
- en: '2.For each feature in the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 对于数据集中的每个特征：
- en: a. Sort the feature values in ascending order.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: a. 按升序排序特征值。
- en: b. Consider all midpoints between adjacent values as potential split points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: b. 将相邻值之间的所有中点视为潜在的拆分点。
- en: '![](../Images/b37c7a5dead079a3548bbc02b7d9d800.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b37c7a5dead079a3548bbc02b7d9d800.png)'
- en: In total, there are 23 split points to check.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总共需要检查23个拆分点。
- en: '3\. For each potential split point:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 对每个潜在拆分点：
- en: a. Calculate the mean squared error (MSE) of the current node.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: a. 计算当前节点的均方误差（MSE）。
- en: b. Compute the weighted average of errors for the resulting split.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: b. 计算结果拆分的加权平均误差。
- en: '![](../Images/edce98c77731a3a1fb3e871e75d9b0f2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edce98c77731a3a1fb3e871e75d9b0f2.png)'
- en: As an example, here, we calculated the weighted average of MSE for split point
    “Temperature” with value 73.5
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这里，我们计算了拆分点“温度”值为73.5时的均方误差（MSE）的加权平均值。
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/8302971702ad2e5e18eba4e710464c8a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8302971702ad2e5e18eba4e710464c8a.png)'
- en: 4\. After evaluating all features and split points, select the one with lowest
    weighted average of MSE.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 在评估所有特征和拆分点后，选择均方误差（MSE）加权平均值最低的一个。
- en: '![](../Images/2bcbe92a762d4f11ca62b9548587d9d7.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bcbe92a762d4f11ca62b9548587d9d7.png)'
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/1ce048d6bbf06583edef5c90b22e9d5e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ce048d6bbf06583edef5c90b22e9d5e.png)'
- en: '5\. Create two child nodes based on the chosen feature and split point:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 根据选择的特征和切分点创建两个子节点：
- en: '- Left child: samples with feature value <= split point'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '- 左子节点：特征值 <= 切分点的样本'
- en: '- Right child: samples with feature value > split point'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '- 右子节点：特征值 > 切分点的样本'
- en: '![](../Images/87c47d48c4a8dd061010b27bc42362f0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87c47d48c4a8dd061010b27bc42362f0.png)'
- en: 6\. Recursively repeat steps 2–5 for each child node. (Continue until a stopping
    criterion is met.)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 对每个子节点递归重复步骤2到5。（继续直到满足停止准则。）
- en: '![](../Images/60ff1e00eadc6faef39b8fb6a57fe968.png)![](../Images/3ad498b5148c0d1e27ed7cf1a7f885e7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60ff1e00eadc6faef39b8fb6a57fe968.png)![](../Images/3ad498b5148c0d1e27ed7cf1a7f885e7.png)'
- en: 7\. At each leaf node, assign the average target value of the samples in that
    node as the prediction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 在每个叶节点，分配该节点中样本的平均目标值作为预测值。
- en: '![](../Images/aab4fe4bdcce592f0c4e10224a18ad10.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aab4fe4bdcce592f0c4e10224a18ad10.png)'
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4c99833d95cd4cda3734cf643d7c2b9d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c99833d95cd4cda3734cf643d7c2b9d.png)'
- en: In this scikit-learn output, the samples and values are shown for the leaf nodes
    and interim nodes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个scikit-learn的输出中，展示了叶节点和中间节点的样本及其值。
- en: Regression/Prediction Step
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归/预测步骤
- en: 'Here’s how a regression tree makes predictions for new data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是回归树如何对新数据进行预测的过程：
- en: 1\. Start at the top (root) of the tree.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 从树的顶部（根节点）开始。
- en: '2\. At each decision point (node):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在每个决策点（节点）：
- en: '- Look at the feature and split value.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '- 查看特征和切分值。'
- en: '- If the data point’s feature value is smaller or equal, go left.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果数据点的特征值较小或相等，向左走。'
- en: '- If it’s larger, go right.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果它更大，向右走。'
- en: 3\. Keep moving down the tree until you reach the end (a leaf).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 一直向下移动直到到达树的末端（叶节点）。
- en: 4\. The prediction is the average value stored in that leaf.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 预测值是该叶节点中存储的平均值。
- en: '![](../Images/ce9e66e2984c1dd8926cee0a62199806.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce9e66e2984c1dd8926cee0a62199806.png)'
- en: Evaluation Step
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估步骤
- en: '![](../Images/b1973f3c301a44f070306fc8def936ce.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1973f3c301a44f070306fc8def936ce.png)'
- en: This value of RMSE is so much better than [the result of the dummy regressor](https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个RMSE值比[虚拟回归器的结果](https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)要好得多。
- en: Pre-pruning vs Post-pruning
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预剪枝与后剪枝
- en: 'After building the tree, the only thing we need to worry about is the method
    to make the tree smaller to prevent overfitting. In general, the method of pruning
    can be categorized as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完树之后，我们唯一需要担心的就是如何使树变小，以防止过拟合。通常，修剪的方法可以分为以下几类：
- en: Pre-pruning
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预剪枝
- en: 'Pre-pruning, also known as early stopping, involves halting the growth of a
    decision tree during the training process based on certain predefined criteria.
    This approach aims to prevent the tree from becoming too complex and overfitting
    the training data. Common pre-pruning techniques include:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 预剪枝，也称为早期停止，是指在训练过程中基于某些预定义的标准，停止决策树的生长。此方法旨在防止树变得过于复杂并导致过拟合。常见的预剪枝技术包括：
- en: '**Maximum depth**: Limiting how deep the tree can grow.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大深度**：限制树的最大深度。'
- en: '**Minimum samples for split**: Requiring a minimum number of samples to justify
    splitting a node.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每次切分的最小样本数**：要求切分一个节点时，必须满足最小样本数的条件。'
- en: '**Minimum samples per leaf**: Ensuring each leaf node has at least a certain
    number of samples.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每个叶节点的最小样本数**：确保每个叶节点至少包含一定数量的样本。'
- en: '**Maximum number of leaf nodes**: Restricting the total number of leaf nodes
    in the tree.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大叶节点数**：限制树中叶节点的总数。'
- en: '**Minimum impurity decrease**: Only allowing splits that decrease impurity
    by a specified amount.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最小纯度减少**：仅允许那些减少纯度达到指定值的切分。'
- en: These methods stop the tree’s growth when the specified conditions are met,
    effectively “pruning” the tree during its construction phase.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在满足指定条件时停止树的生长，有效地在树的构建阶段进行“修剪”。
- en: ([We have discussed these methods before, which is exactly the same in regression
    case.](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ([我们之前已经讨论过这些方法，它们在回归问题中完全相同。](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e))
- en: Post-pruning
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后剪枝
- en: Post-pruning, on the other hand, allows the decision tree to grow to its full
    extent and then prunes it back to reduce complexity. This approach first builds
    a complete tree and then removes or collapses branches that don’t significantly
    contribute to the model’s performance. One common post-pruning technique is called
    **Cost-Complexity Pruning.**
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 后修剪方法允许决策树生长到最大，然后修剪回去以减少复杂性。这种方法首先构建完整的树，然后移除或合并那些对模型表现贡献不大的分支。一种常见的后修剪技术被称为**成本复杂度修剪**。
- en: Cost Complexity Pruning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本复杂度修剪
- en: 'Step 1: Calculate the Impurity for Each Node'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：计算每个节点的不纯度
- en: For each interim node, calculate the impurity (MSE for regression case). We
    then sorted this value from the lowest to highest.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个中间节点，计算不纯度（回归问题中的MSE）。然后我们将这个值从小到大排序。
- en: '![](../Images/57e29c193ac2b684411d3da6c0176e9c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57e29c193ac2b684411d3da6c0176e9c.png)'
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/d4244591a1bc3d372a2b2429fc6e24b9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4244591a1bc3d372a2b2429fc6e24b9.png)'
- en: In this scikit learn output, the impurity are shown as “squared_error” for each
    nodes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个scikit-learn的输出中，节点的不纯度显示为每个节点的“squared_error”。
- en: '![](../Images/fbbc7f5114b6b7959849be45f651aec6.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbbc7f5114b6b7959849be45f651aec6.png)'
- en: Let‘s give name to these interim nodes (from A-J). We then sort it based on
    their MSE, from lowest to highest
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给这些中间节点（从A到J）命名。然后我们根据它们的MSE从小到大进行排序。
- en: 'Step 2: Create Subtrees by Trimming The Weakest Link'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：通过修剪最弱的链接来创建子树
- en: The goal is to gradually turn the interim nodes into leaves starting from the
    **node with the lowest MSE** (= weakest link). We can create a path of pruning
    based on that.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是从**均方误差（MSE）最小的节点**开始，逐渐将中间节点转化为叶子节点。我们可以基于此创建一个修剪路径。
- en: '![](../Images/bd666447562c4b89a0452d15a3a697b9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd666447562c4b89a0452d15a3a697b9.png)'
- en: Let’s name them “Subtree *i*” based on how many times (*i*) it is being pruned.
    Starting from the original tree, the tree will be pruned on the node with lowest
    MSE (starting from node J, M (already got cut by J), L, K, and so on)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们命名为“子树*i*”，基于它被修剪的次数(*i*)。从原始树开始，树会在具有最低MSE的节点上修剪（从节点J开始，M（已经被J修剪掉），L，K，依此类推）。
- en: 'Step 3: Calculate Total Leaf Impurities for Each Subtree'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：计算每棵子树的总叶子不纯度
- en: 'For each subtree *T*, total leaf impurities (*R*(*T*)) can be calculated as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一棵子树*T*，可以计算总叶子不纯度（*R*(*T*)）：
- en: '*R*(*T*) = (1/*N*) Σ *I*(*L*) * *n*_*L*'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*(*T*) = (1/*N*) Σ *I*(*L*) * *n*_*L*'
- en: 'where:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '**·** *L* ranges over all leaf nodes'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** *L* 遍历所有叶子节点'
- en: '**·** *n_L* is the number of samples in leaf *L* **·** *N* is the total number
    of samples in the tree'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** *n_L* 是叶子*L*中样本的数量 **·** *N* 是树中样本的总数'
- en: '**·** *I*(*L*) is the impurity (MSE)of leaf *L*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** *I*(*L*) 是叶子*L*的不纯度（MSE）'
- en: '![](../Images/db7b97078d3ca41116ba6586413f17a6.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db7b97078d3ca41116ba6586413f17a6.png)'
- en: The more we prune, the higher the total leaf impurities.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修剪得越多，总的叶子不纯度就越高。
- en: 'Step 4: Compute the Cost Function'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：计算成本函数
- en: 'To control when to stop turning the interim nodes into leaves, we check the
    cost complexity first for each subtree *T* using the following formula:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制何时停止将中间节点转化为叶子节点，我们首先使用以下公式检查每棵子树*T*的成本复杂度：
- en: Cost(*T*) = *R*(*T*) + *α* * |*T*|
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Cost(*T*) = *R*(*T*) + *α* * |*T*|
- en: 'where:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '**·** *R*(*T*) is the total leaf impurities'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** *R*(*T*) 是总叶子不纯度'
- en: '**·** |*T*| is the number of leaf nodes in the subtree**·** *α* is the complexity
    parameter'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**·** |*T*| 是子树中的叶子节点数**·** *α* 是复杂度参数'
- en: '![](../Images/d1e444f5b41346a1628e95d70944d312.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1e444f5b41346a1628e95d70944d312.png)'
- en: 'Step 5: Select the Alpha'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤5：选择Alpha
- en: The value of alpha control which subtree we will end up with. The **subtree
    with the lowest cost will be the final tree**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*的值控制我们最终得到哪一棵子树。**具有最低成本的子树将是最终的树**。'
- en: '![](../Images/7f89a62ec5564f5169596ae9b6d2d1d6.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f89a62ec5564f5169596ae9b6d2d1d6.png)'
- en: When *α* is small, we care more about accuracy (bigger trees). When *α* is large,
    we care more about simplicity (smaller trees)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当*α*较小时，我们更关心准确性（较大的树）。当*α*较大时，我们更关心简洁性（较小的树）。
- en: While we can freely set the *α*, in scikit-learn, you can also get the smallest
    value of *α* to obtain a particular subtree. This is called **effective *α****.*
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以自由设定*α*，在scikit-learn中，你也可以获取最小的*α*值来获得特定的子树。这被称为**有效的*α***。
- en: '![](../Images/d70f61e11dcfad94df3cbd4294949d3d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d70f61e11dcfad94df3cbd4294949d3d.png)'
- en: This effective ***α*** *can also be computed.*
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有效的***α*** *也可以计算出来*。
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/970ac8efb0ce4a38d8e6f98f7d4a57ee.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/970ac8efb0ce4a38d8e6f98f7d4a57ee.png)'
- en: Final Remarks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终备注
- en: Pre-pruning methods are generally faster and more memory-efficient, as they
    prevent the tree from growing too large in the first place.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 预剪枝方法通常更快且更节省内存，因为它们从一开始就防止了树形过大。
- en: Post-pruning can potentially create more optimal trees, as it considers the
    entire tree structure before making pruning decisions. However, it can be more
    computationally expensive.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 后剪枝可能创建出更优的树形结构，因为它会在做出剪枝决定之前考虑整个树的结构。然而，这可能会消耗更多的计算资源。
- en: Both approaches aim to find a balance between model complexity and performance,
    with the goal of creating a model that generalizes well to unseen data. The choice
    between pre-pruning and post-pruning (or a combination of both) often depends
    on the specific dataset, the problem at hand, and of course, computational resources
    available.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法的目标都是在模型复杂度和性能之间找到平衡，目的是创建一个能很好地对未见数据进行泛化的模型。选择预剪枝还是后剪枝（或两者结合）通常取决于具体的数据集、当前问题以及可用的计算资源。
- en: In practice, it’s common to use a combination of these methods, like applying
    some pre-pruning criteria to prevent excessively large trees, and then using post-pruning
    for fine-tuning the model’s complexity.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，通常会结合使用这些方法，比如先应用一些预剪枝标准来防止树形过大，再使用后剪枝对模型的复杂度进行微调。
- en: 🌟 Decision Tree Regressor (with Cost Complexity Pruning) Code Summarized
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🌟 决策树回归器（带成本复杂度剪枝）代码总结
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Further Reading
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: For a detailed explanation of the [Decision Tree Regressor](https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html),
    [Cost Complexity Pruning](https://scikit-learn.org/1.5/auto_examples/tree/plot_cost_complexity_pruning.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[决策树回归器](https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html)、[成本复杂度剪枝](https://scikit-learn.org/1.5/auto_examples/tree/plot_cost_complexity_pruning.html)及其在scikit-learn中的实现，读者可以参考其官方文档。该文档提供了关于使用方法和参数的全面信息。
- en: Technical Environment
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术环境
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用Python 3.7和scikit-learn 1.5。虽然讨论的概念通常适用，但具体的代码实现可能会因版本不同而略有差异。
- en: About the Illustrations
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于插图
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者创作，并结合了Canva Pro授权的设计元素。
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
- en: Regression Algorithms
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归算法
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----fbd2836c3bef--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----fbd2836c3bef--------------------------------)5个故事![一个戴着粉红色帽子、扎着辫子的卡通玩偶。这只“虚拟”玩偶，凭借其基础的设计和心形图案的衬衫，形象地表现了机器学习中的虚拟回归器概念。就像这个玩具般的形象是一个简化、静态的人的表现，虚拟回归器则是作为更复杂分析基线的基本模型。](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
- en: Classification Algorithms
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类算法
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fbd2836c3bef--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fbd2836c3bef--------------------------------)
    8 个故事！[](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
