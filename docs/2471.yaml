- en: 'Decision Tree Regressor, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10](https://towardsdatascience.com/decision-tree-regressor-explained-a-visual-guide-with-code-examples-fbd2836c3bef?source=collection_archive---------1-----------------------#2024-10-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: REGRESSION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Trimming branches smartly with cost-complexity pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--fbd2836c3bef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbd2836c3bef--------------------------------)
    ¬∑11 min read¬∑Oct 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: A fresh look on our favorite upside-down tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----fbd2836c3bef--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees aren‚Äôt limited to categorizing data ‚Äî they‚Äôre equally good at
    predicting numerical values! Classification trees often steal the spotlight, but
    Decision Tree Regressors (or Regression Trees) are powerful and versatile tools
    in the world of continuous variable prediction.
  prefs: []
  type: TYPE_NORMAL
- en: While we‚Äôll discuss the mechanics of regression tree construction (which are
    mostly similar to the classification tree), here, we‚Äôll also advance beyond the
    *pre*-pruning methods like ‚Äúminimal sample leaf" and "max tree depth‚Äù introduced
    in the classifier article. We‚Äôll explore the most common *post*-pruning method
    which is **cost complexity pruning**, that introduces a complexity parameter to
    the decision tree‚Äôs cost function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af2717ef7b5f2f1e541ab5ba0909511d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Decision Tree for regression is a model that predicts numerical values using
    a tree-like structure. It splits data based on key features, starting from a root
    question and branching out. Each node asks about a feature, dividing data further
    until reaching leaf nodes with final predictions. To get a result, you follow
    the path matching your data‚Äôs features from root to leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d58bb0bb413cc741a3d2babe2bb0a24.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision Trees for regression predict numerical outcomes by following a series
    of data-driven questions, narrowing down to a final value.
  prefs: []
  type: TYPE_NORMAL
- en: üìä Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate our concepts, we‚Äôll work with [our standard dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).
    This dataset is used to predict the number of golfers visiting on a given day
    and includes variables like weather outlook, temperature, humidity, and wind conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be5edbc6b8231b1605ca2447d8c33657.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‚ÄòOutlook‚Äô (one-hot encoded to sunny, overcast, rain), ‚ÄòTemperature‚Äô
    (in Fahrenheit), ‚ÄòHumidity‚Äô (in %), ‚ÄòWind‚Äô (Yes/No) and ‚ÄòNumber of Players‚Äô (numerical,
    target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Decision Tree for regression operates by recursively dividing the data
    based on features that best reduce prediction error. Here‚Äôs the general process:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin with the entire dataset at the root node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the feature that minimizes a specific error metric (such as mean squared
    error or variance) to split the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create child nodes based on the split, where each child represents a subset
    of the data aligned with the corresponding feature values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2‚Äì3 for each child node, continuing to split the data until a stopping
    condition is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign a final predicted value to each leaf node, typically **the average of
    the target values** in that node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/48ae23f237c31ae05ba52c569f790fb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will explore the regression part in the decision tree algorithm CART (Classification
    and Regression Trees). It builds binary trees and typically follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1.Begin with all training samples in the root node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62033724ca4273f8d77b2f9184658c43.png)'
  prefs: []
  type: TYPE_IMG
- en: '2.For each feature in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Sort the feature values in ascending order.
  prefs: []
  type: TYPE_NORMAL
- en: b. Consider all midpoints between adjacent values as potential split points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b37c7a5dead079a3548bbc02b7d9d800.png)'
  prefs: []
  type: TYPE_IMG
- en: In total, there are 23 split points to check.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. For each potential split point:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate the mean squared error (MSE) of the current node.
  prefs: []
  type: TYPE_NORMAL
- en: b. Compute the weighted average of errors for the resulting split.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edce98c77731a3a1fb3e871e75d9b0f2.png)'
  prefs: []
  type: TYPE_IMG
- en: As an example, here, we calculated the weighted average of MSE for split point
    ‚ÄúTemperature‚Äù with value 73.5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8302971702ad2e5e18eba4e710464c8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 4\. After evaluating all features and split points, select the one with lowest
    weighted average of MSE.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bcbe92a762d4f11ca62b9548587d9d7.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1ce048d6bbf06583edef5c90b22e9d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: '5\. Create two child nodes based on the chosen feature and split point:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Left child: samples with feature value <= split point'
  prefs: []
  type: TYPE_NORMAL
- en: '- Right child: samples with feature value > split point'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87c47d48c4a8dd061010b27bc42362f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 6\. Recursively repeat steps 2‚Äì5 for each child node. (Continue until a stopping
    criterion is met.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/60ff1e00eadc6faef39b8fb6a57fe968.png)![](../Images/3ad498b5148c0d1e27ed7cf1a7f885e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 7\. At each leaf node, assign the average target value of the samples in that
    node as the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aab4fe4bdcce592f0c4e10224a18ad10.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4c99833d95cd4cda3734cf643d7c2b9d.png)'
  prefs: []
  type: TYPE_IMG
- en: In this scikit-learn output, the samples and values are shown for the leaf nodes
    and interim nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Regression/Prediction Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here‚Äôs how a regression tree makes predictions for new data:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Start at the top (root) of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. At each decision point (node):'
  prefs: []
  type: TYPE_NORMAL
- en: '- Look at the feature and split value.'
  prefs: []
  type: TYPE_NORMAL
- en: '- If the data point‚Äôs feature value is smaller or equal, go left.'
  prefs: []
  type: TYPE_NORMAL
- en: '- If it‚Äôs larger, go right.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Keep moving down the tree until you reach the end (a leaf).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. The prediction is the average value stored in that leaf.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce9e66e2984c1dd8926cee0a62199806.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/b1973f3c301a44f070306fc8def936ce.png)'
  prefs: []
  type: TYPE_IMG
- en: This value of RMSE is so much better than [the result of the dummy regressor](https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).
  prefs: []
  type: TYPE_NORMAL
- en: Pre-pruning vs Post-pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After building the tree, the only thing we need to worry about is the method
    to make the tree smaller to prevent overfitting. In general, the method of pruning
    can be categorized as:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pre-pruning, also known as early stopping, involves halting the growth of a
    decision tree during the training process based on certain predefined criteria.
    This approach aims to prevent the tree from becoming too complex and overfitting
    the training data. Common pre-pruning techniques include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Maximum depth**: Limiting how deep the tree can grow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minimum samples for split**: Requiring a minimum number of samples to justify
    splitting a node.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minimum samples per leaf**: Ensuring each leaf node has at least a certain
    number of samples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Maximum number of leaf nodes**: Restricting the total number of leaf nodes
    in the tree.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minimum impurity decrease**: Only allowing splits that decrease impurity
    by a specified amount.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These methods stop the tree‚Äôs growth when the specified conditions are met,
    effectively ‚Äúpruning‚Äù the tree during its construction phase.
  prefs: []
  type: TYPE_NORMAL
- en: ([We have discussed these methods before, which is exactly the same in regression
    case.](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e))
  prefs: []
  type: TYPE_NORMAL
- en: Post-pruning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Post-pruning, on the other hand, allows the decision tree to grow to its full
    extent and then prunes it back to reduce complexity. This approach first builds
    a complete tree and then removes or collapses branches that don‚Äôt significantly
    contribute to the model‚Äôs performance. One common post-pruning technique is called
    **Cost-Complexity Pruning.**
  prefs: []
  type: TYPE_NORMAL
- en: Cost Complexity Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Step 1: Calculate the Impurity for Each Node'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For each interim node, calculate the impurity (MSE for regression case). We
    then sorted this value from the lowest to highest.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57e29c193ac2b684411d3da6c0176e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d4244591a1bc3d372a2b2429fc6e24b9.png)'
  prefs: []
  type: TYPE_IMG
- en: In this scikit learn output, the impurity are shown as ‚Äúsquared_error‚Äù for each
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbbc7f5114b6b7959849be45f651aec6.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äòs give name to these interim nodes (from A-J). We then sort it based on
    their MSE, from lowest to highest
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Create Subtrees by Trimming The Weakest Link'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal is to gradually turn the interim nodes into leaves starting from the
    **node with the lowest MSE** (= weakest link). We can create a path of pruning
    based on that.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd666447562c4b89a0452d15a3a697b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs name them ‚ÄúSubtree *i*‚Äù based on how many times (*i*) it is being pruned.
    Starting from the original tree, the tree will be pruned on the node with lowest
    MSE (starting from node J, M (already got cut by J), L, K, and so on)
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Calculate Total Leaf Impurities for Each Subtree'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For each subtree *T*, total leaf impurities (*R*(*T*)) can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*R*(*T*) = (1/*N*) Œ£ *I*(*L*) * *n*_*L*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**¬∑** *L* ranges over all leaf nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '**¬∑** *n_L* is the number of samples in leaf *L* **¬∑** *N* is the total number
    of samples in the tree'
  prefs: []
  type: TYPE_NORMAL
- en: '**¬∑** *I*(*L*) is the impurity (MSE)of leaf *L*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db7b97078d3ca41116ba6586413f17a6.png)'
  prefs: []
  type: TYPE_IMG
- en: The more we prune, the higher the total leaf impurities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Compute the Cost Function'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To control when to stop turning the interim nodes into leaves, we check the
    cost complexity first for each subtree *T* using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost(*T*) = *R*(*T*) + *Œ±* * |*T*|
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**¬∑** *R*(*T*) is the total leaf impurities'
  prefs: []
  type: TYPE_NORMAL
- en: '**¬∑** |*T*| is the number of leaf nodes in the subtree**¬∑** *Œ±* is the complexity
    parameter'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1e444f5b41346a1628e95d70944d312.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Step 5: Select the Alpha'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The value of alpha control which subtree we will end up with. The **subtree
    with the lowest cost will be the final tree**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f89a62ec5564f5169596ae9b6d2d1d6.png)'
  prefs: []
  type: TYPE_IMG
- en: When *Œ±* is small, we care more about accuracy (bigger trees). When *Œ±* is large,
    we care more about simplicity (smaller trees)
  prefs: []
  type: TYPE_NORMAL
- en: While we can freely set the *Œ±*, in scikit-learn, you can also get the smallest
    value of *Œ±* to obtain a particular subtree. This is called **effective *Œ±****.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d70f61e11dcfad94df3cbd4294949d3d.png)'
  prefs: []
  type: TYPE_IMG
- en: This effective ***Œ±*** *can also be computed.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/970ac8efb0ce4a38d8e6f98f7d4a57ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Final Remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-pruning methods are generally faster and more memory-efficient, as they
    prevent the tree from growing too large in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Post-pruning can potentially create more optimal trees, as it considers the
    entire tree structure before making pruning decisions. However, it can be more
    computationally expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Both approaches aim to find a balance between model complexity and performance,
    with the goal of creating a model that generalizes well to unseen data. The choice
    between pre-pruning and post-pruning (or a combination of both) often depends
    on the specific dataset, the problem at hand, and of course, computational resources
    available.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it‚Äôs common to use a combination of these methods, like applying
    some pre-pruning criteria to prevent excessively large trees, and then using post-pruning
    for fine-tuning the model‚Äôs complexity.
  prefs: []
  type: TYPE_NORMAL
- en: üåü Decision Tree Regressor (with Cost Complexity Pruning) Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [Decision Tree Regressor](https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeRegressor.html),
    [Cost Complexity Pruning](https://scikit-learn.org/1.5/auto_examples/tree/plot_cost_complexity_pruning.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôéùôöùôö ùô¢ùô§ùôßùôö ùôçùôöùôúùôßùôöùô®ùô®ùôûùô§ùô£ ùòºùô°ùôúùô§ùôßùôûùô©ùôùùô¢ùô® ùôùùôöùôßùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----fbd2836c3bef--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This ‚Äúdummy‚Äù doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----fbd2836c3bef--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----fbd2836c3bef--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
