- en: 'How to Choose the Best ML Deployment Strategy: Cloud vs. Edge'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-choose-the-best-ml-deployment-strategy-cloud-vs-edge-7b62d9db9b20?source=collection_archive---------3-----------------------#2024-10-14](https://towardsdatascience.com/how-to-choose-the-best-ml-deployment-strategy-cloud-vs-edge-7b62d9db9b20?source=collection_archive---------3-----------------------#2024-10-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The choice between cloud and edge deployment could make or break your project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--7b62d9db9b20--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--7b62d9db9b20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7b62d9db9b20--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7b62d9db9b20--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--7b62d9db9b20--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7b62d9db9b20--------------------------------)
    ·14 min read·Oct 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/670dca37200cce7a7304957e014758b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jakob Owens](https://unsplash.com/@jakobowens1?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: As a machine learning engineer, I frequently see discussions on social media
    emphasizing the importance of deploying ML models. I completely agree — model
    deployment is a critical component of MLOps. As ML adoption grows, there’s a rising
    demand for scalable and efficient deployment methods, yet specifics often remain
    unclear.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, does that mean model deployment is always the same, no matter the context?
    In fact, quite the opposite: I’ve been deploying ML models for about a decade
    now, and it can be quite different from one project to another. There are many
    ways to deploy a ML model, and having experience with one method doesn’t necessarily
    make you proficient with others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The remaining question is: **what are the methods to deploy a ML model**, and
    **how do we choose the right method**?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models can be deployed in various ways, but they typically fall into two main
    categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It may sound easy, but there’s a catch. For both categories, there are actually
    many subcategories. Here is a non-exhaustive diagram of deployments that we will
    explore in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70f0d9d6cbcae7b724cde834f082013a.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the explored subcategories of deployment in this article. Image by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before talking about how to choose the right method, **let’s explore each category:
    what it is, the pros, the cons, the typical tech stack, and I will also share
    some personal examples** of deployments I did in that context. Let’s dig in!'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From what I can see, it seems cloud deployment is by far **the most popular
    choice** when it comes to ML deployment. This is what is usually expected to master
    for model deployment. But cloud deployment usually means one of these, depending
    on the context:'
  prefs: []
  type: TYPE_NORMAL
- en: API deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even in those sub-categories, one could have another level of categorization
    but we won’t go that far in that post. Let’s have a look at what they mean, their
    pros and cons and a typical associated tech stack.
  prefs: []
  type: TYPE_NORMAL
- en: API Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'API stands for Application Programming Interface. This is a very popular way
    to deploy a model on the cloud. Some of the most popular ML models are deployed
    as APIs: Google Maps and OpenAI’s ChatGPT can be queried through their APIs for
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re not familiar with APIs, know that it’s usually called with a simple
    query. For example, type the following command in your terminal to get the 20
    first Pokémon names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, what happens when calling an API might be a bit more complex.
    API deployments usually involve a standard tech stack including load balancers,
    autoscalers and interactions with a database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6dd59aa3082e1c70e16efd4f871e817.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical example of an API deployment within a cloud infrastructure. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: APIs may have different needs and infrastructure, this example is simplified
    for clarity.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'API deployments are popular for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to implement and to integrate into various tech stacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s easy to scale: using horizontal scaling in clouds allow to scale efficiently;
    moreover managed services of cloud providers may reduce the need for manual intervention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows centralized management of model versions and logging, thus efficient
    tracking and reproducibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While APIs are a really popular option, there are some cons too:'
  prefs: []
  type: TYPE_NORMAL
- en: There might be latency challenges with potential network overhead or geographical
    distance; and of course it requires a good internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost can climb up pretty quickly with high traffic (assuming automatic scaling)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance overhead can get expensive, either with managed services cost of
    infra team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To sum up, **API deployment is largely used** in many startups and tech companies
    **because of its flexibility** and a rather short time to market. But the **cost
    can climb up quite fast for high traffic**, and the maintenance cost can also
    be significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'About the tech stack: there are many ways to develop APIs, but the most common
    ones in Machine Learning are probably [FastAPI](https://fastapi.tiangolo.com/)
    and [Flask](https://flask.palletsprojects.com/en/3.0.x/). They can then be deployed
    quite easily on the main cloud providers (AWS, GCP, Azure…), preferably through
    docker images. The orchestration can be done through managed services or with
    Kubernetes, depending on the team’s choice, its size, and skills.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of API cloud deployment, I once deployed a ML solution to automate
    the pricing of an electric vehicle charging station for a customer-facing web
    app. You can have a look at this project here if you want to know more about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/how-renault-leveraged-machine-learning-to-scale-electric-vehicles-sales-4f42bee34a12?source=post_page-----7b62d9db9b20--------------------------------)
    [## How Renault Leveraged Machine Learning to Scale Electric Vehicle Sales'
  prefs: []
  type: TYPE_NORMAL
- en: How I built and deployed a machine learning-based solution in a few months that
    allowed Renault to scale the sale of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/how-renault-leveraged-machine-learning-to-scale-electric-vehicles-sales-4f42bee34a12?source=post_page-----7b62d9db9b20--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Even if this post does not get into the code, it can give you a good idea of
    what can be done with API deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'API deployment is very popular for its simplicity to integrate to any project.
    But some projects may need even more flexibility and less maintenance cost: this
    is where serverless deployment may be a solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another popular, but probably less frequently used option is serverless deployment.
    Serverless computing means that **you run your model** (or any code actually)
    **without owning nor provisioning any server**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Serverless deployment offers several significant advantages and is quite easy
    to set up:'
  prefs: []
  type: TYPE_NORMAL
- en: No need to manage nor to maintain servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No need to handle scaling in case of higher traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You only pay for what you use: no traffic means virtually no cost, so no overhead
    cost at all'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But it has some limitations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: It is usually not cost effective for large number of queries compared to managed
    APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold start latency is a potential issue, as a server might need to be spawned,
    leading to delays
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The memory footprint is usually limited by design: you can’t always run large
    models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The execution time is limited too: it’s not possible to run jobs for more than
    a few minutes (15 minutes for AWS Lambda for example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a nutshell, I would say that serverless deployment is a **good option when
    you’re launching something new, don’t expect large traffic and don’t want to spend
    much on infra management**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Serverless computing is proposed by all major cloud providers under different
    names: [AWS Lambda](https://aws.amazon.com/lambda/), [Azure Functions](https://azure.microsoft.com/en-us/products/functions/)
    and [Google Cloud Functions](https://cloud.google.com/functions) for the most
    popular ones.'
  prefs: []
  type: TYPE_NORMAL
- en: I personally have never deployed a serverless solution (working mostly with
    deep learning, I usually found myself limited by the serverless constraints mentioned
    above), but there is lots of documentation about how to do it properly, such as
    [this one from AWS](https://aws.amazon.com/blogs/compute/deploying-machine-learning-models-with-serverless-templates/).
  prefs: []
  type: TYPE_NORMAL
- en: While serverless deployment offers a flexible, on-demand solution, some applications
    may require a more scheduled approach, like batch processing.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way to deploy on the cloud is through scheduled batch processing. While
    serverless and APIs are mostly used for live predictions, in some cases batch
    predictions makes more sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whether it be database updates, dashboard updates, caching predictions… as
    soon as there is **no need to have a real-time prediction, batch processing is
    usually the best option**:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing large batches of data is more resource-efficient and reduce overhead
    compared to live processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing can be scheduled during off-peak hours, allowing to reduce the overall
    charge and thus the cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Of course, it comes with associated drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing creates a spike in resource usage, which can lead to system
    overload if not properly planned
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling errors is critical in batch processing, as you need to process a full
    batch gracefully at once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch processing should be considered for any task that does not required
    real-time results**: it is usually more cost effective. But of course, for any
    real-time application, it is not a viable option.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is used widely in many companies, mostly within ETL (Extract, Transform,
    Load) pipelines that may or may not contain ML. Some of the most popular tools
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Airflow for workflow orchestration and task scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark for fast, massive data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As an example of batch processing, I used to work on a YouTube video revenue
    forecasting. Based on the first data points of the video revenue, we would forecast
    the revenue over up to 5 years, using a multi-target regression and curve fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e891c0b723fb52a7b5e9269012dfedd.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot representing the initial data, multi-target regression predictions and
    curve fitting. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this project, we had to re-forecast on a monthly basis all our data to
    ensure there was no drifting between our initial forecasting and the most recent
    ones. For that, we used a managed Airflow, so that every month it would automatically
    trigger a new forecasting based on the most recent data, and store those into
    our databases. If you want to know more about this project, you can have a look
    at this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.datadriveninvestor.com/how-to-forecast-youtube-video-revenue-e35c60bd1105?source=post_page-----7b62d9db9b20--------------------------------)
    [## How to Forecast YouTube Video Revenue'
  prefs: []
  type: TYPE_NORMAL
- en: This method achieved an error rate of less than 6% in revenue forecasting over
    a portfolio of dozens of YouTubers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/how-to-forecast-youtube-video-revenue-e35c60bd1105?source=post_page-----7b62d9db9b20--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: After exploring the various strategies and tools available for cloud deployment,
    it’s clear that this approach offers significant flexibility and scalability.
    However, cloud deployment is not always the best fit for every ML application,
    particularly when real-time processing, privacy concerns, or financial resource
    constraints come into play.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/578ee44791128a7decddea53afe8da55.png)'
  prefs: []
  type: TYPE_IMG
- en: A list of pros and cons for cloud deployment. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This is where edge deployment comes into focus as a viable option. Let’s now
    delve into edge deployment to understand when it might be the best option.
  prefs: []
  type: TYPE_NORMAL
- en: Edge Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From my own experience, edge deployment is rarely considered as the main way
    of deployment. A few years ago, even I thought it was not really an interesting
    option for deployment. With more perspective and experience now, I think **it
    must be considered as the first option** for deployment anytime you can.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like cloud deployment, edge deployment covers a wide range of cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Native phone applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge server and specific devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While they all share some similar properties, such as limited resources and
    horizontal scaling limitations, each deployment choice may have their own characteristics.
    Let’s have a look.
  prefs: []
  type: TYPE_NORMAL
- en: Native Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We see more and more smartphone apps with integrated AI nowadays, and it will
    probably keep growing even more in the future. While some Big Tech companies such
    as OpenAI or Google have chosen the API deployment approach for their LLMs, Apple
    is currently working on the iOS app deployment model with solutions such as [OpenELM](https://machinelearning.apple.com/research/openelm),
    a tini LLM. Indeed, this option has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The infra cost if virtually zero: no cloud to maintain, it all runs on the
    device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Better privacy: you don’t have to send any data to an API, it can all run locally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your model is directly integrated to your app, no need to maintain several codebases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Moreover, Apple has built a fantastic ecosystem for model deployment in iOS:
    you can run very efficiently ML models with Core ML on their Apple chips (M1,
    M2, etc…) and take advantage of the neural engine for really fast inferences.
    To my knowledge, Android is slightly lagging behind, but also has a great ecosystem.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this can be a really beneficial approach in many cases, there are still
    some limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Phone resources limit model size and performance, and are shared with other
    apps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heavy models may drain the battery pretty fast, which can be deceptive for the
    user experience overall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Device fragmentation, as well as iOS and Android apps make it hard to cover
    the whole market
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decentralized model updates can be challenging compared to cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite its drawbacks, native app deployment is often a strong choice for ML
    solutions that run in an app. It **may seem more complex during the development
    phase**, but it will turn out to be **much cheaper** as soon as it’s deployed
    compared to a cloud deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to the tech stack, there are actually two main ways to deploy:
    iOS and Android. They both have their own stacks, but they share the same properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'App development: Swift for iOS, Kotlin for Android'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model format: Core ML for iOS, TensorFlow Lite for Android'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hardware accelerator: Apple Neural Engine for iOS, Neural Network API for Android'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: This is a mere simplification of the tech stack. This non-exhaustive
    overview only aims to cover the essentials and let you dig in from there if interested.*'
  prefs: []
  type: TYPE_NORMAL
- en: As a personal example of such deployment, I once worked on a book reading app
    for Android, in which they wanted to let the user navigate through the book with
    phone movements. For example, shake left to go to the previous page, shake right
    for the next page, and a few more movements for specific commands. For that, I
    trained a model on accelerometer’s features from the phone for movement recognition
    with a rather small model. It was then deployed directly in the app as a TensorFlow
    Lite model.
  prefs: []
  type: TYPE_NORMAL
- en: Native application has strong advantages but is limited to one type of device,
    and would not work on laptops for example. A web application could overcome those
    limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Web Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Web application deployment means running the model on the client side. Basically,
    it means **running the model inference on the device** used by that browser, whether
    it be a tablet, a smartphone or a laptop (and the list goes on…). This kind of
    deployment can be really convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: Your deployment is working on any device that can run a web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The inference cost is virtually zero: no server, no infra to maintain… Just
    the customer’s device'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Only one codebase for all possible devices: no need to maintain an iOS app
    and an Android app simultaneously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note: Running the model on the server side would be equivalent to one of the
    cloud deployment options above.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'While web deployment offers appealing benefits, it also has significant limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Proper resource utilization, especially GPU inference, can be challenging with
    TensorFlow.js
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Your web app must work with all devices and browsers: whether is has a GPU
    or not, Safari or Chrome, a Apple M1 chip or not, etc… This can be a heavy burden
    with a high maintenance cost'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may need a backup plan for slower and older devices: what if the device
    can’t handle your model because it’s too slow?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unlike for a native app, there is no official size limitation for a model.
    However, a small model will be downloaded faster, making it overall experience
    smoother and must be a priority. And a very large model may just not work at all
    anyway.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, while web deployment is powerful, it comes with significant limitations
    and must be used cautiously. One more advantage is that it might be a door to
    another kind of deployment that I did not mention: WeChat Mini Programs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The tech stack is usually the same as for web development: HTML, CSS, JavaScript
    (and any frameworks you want), and of course TensorFlow Lite for model deployment.
    If you’re curious about an example of how to deploy ML in the browser, you can
    have a look at this post where I run a real time face recognition model in the
    browser from scratch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=post_page-----7b62d9db9b20--------------------------------)
    [## BlazeFace: How to Run Real-time Object Detection in the Browser'
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step guide to training a BlazeFace model, from the Python training
    pipeline to the JavaScript demo through…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/blazeface-how-to-run-real-time-object-detection-in-the-browser-66c2ac9acd75?source=post_page-----7b62d9db9b20--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This article goes from a model training in PyTorch to up to a working web app
    and might be informative about this specific kind of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cases, native and web apps are not a viable option: we may have no
    such device, no connectivity, or some other constraints. This is where edge servers
    and specific devices come into play.'
  prefs: []
  type: TYPE_NORMAL
- en: Edge Servers and Specific Devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides native and web apps, edge deployment also includes other cases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deployment on edge servers: in some cases, there are local servers running
    models, such as in some factory production lines, CCTVs, etc…Mostly because of
    privacy requirements, this solution is sometimes the only available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment on specific device: either a sensor, a microcontroller, a smartwatch,
    earplugs, autonomous vehicle, etc… may run ML models internally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment on edge servers can be really close to a deployment on cloud with
    API, and the tech stack may be quite close.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: It is also possible to run batch processing on an edge server, as well
    as just having a monolithic script that does it all.*'
  prefs: []
  type: TYPE_NORMAL
- en: But deployment on specific devices may involve using [FPGA](https://en.wikipedia.org/wiki/Field-programmable_gate_array)s
    or low-level languages. This is another, very different skillset, that may differ
    for each type of device. It is sometimes referred to as TinyML and is a very interesting,
    growing topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'On both cases, they share some challenges with other edge deployment methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Resources are limited, and horizontal scaling is usually not an option
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The battery may be a limitation, as well as the model size and memory footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even with these limitations and challenges, in some cases it’s the only viable
    solution, or the most cost effective one.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an edge server deployment I did was for a company that wanted
    to automatically check whether the orders were valid in fast food restaurants.
    A camera with a top down view would look at the plateau, compare what is sees
    on it (with computer vision and object detection) with the actual order and raise
    an alert in case of mismatch. For some reason, the company wanted to make that
    on edge servers, that were within the fast food restaurant.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, here is a big picture of what are the main types of deployment and
    their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03465e3c6a42058a4aafcc2c9d2269fc.png)'
  prefs: []
  type: TYPE_IMG
- en: A list of pros and cons for cloud deployment. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: With that in mind, **how to actually choose the right deployment method?** There’s
    no single answer to that question, but let’s try to give some rules in the next
    section to make it easier.
  prefs: []
  type: TYPE_NORMAL
- en: How to Choose the Right Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before jumping to the conclusion, let’s make a decision tree to help you choose
    the solution that fits your needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right deployment requires understanding specific needs and constraints,
    often through discussions with stakeholders. Remember that each case is specific
    and might be a edge case. But in the diagram below I tried to outline the most
    common cases to help you out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f7ccb5214d4bfecc3380b38a08cf0ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Deployment decision diagram. Note that each use case is specific. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram, while being quite simplistic, can be reduced to a few questions
    that would allow you go in the right direction:'
  prefs: []
  type: TYPE_NORMAL
- en: Do you need real-time? If no, look for batch processing first; if yes, think
    about edge deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is your solution running on a phone or in the web? Explore these deployments
    method whenever possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the processing quite complex and heavy? If yes, consider cloud deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Again, that’s quite simplistic but helpful in many cases. Also, note that a
    few questions were omitted for clarity but are actually more than important in
    some context: Do you have privacy constraints? Do you have connectivity constraints?
    What is the skillset of your team?'
  prefs: []
  type: TYPE_NORMAL
- en: Other questions may arise depending on the use case; with experience and knowledge
    of your ecosystem, they will come more and more naturally. But hopefully this
    may help you navigate more easily in deployment of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion and Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While cloud deployment is often the default for ML models, edge deployment
    can offer significant advantages: cost-effectiveness and better privacy control.
    Despite challenges such as processing power, memory, and energy constraints, I
    believe edge deployment is a compelling option for many cases. Ultimately, the
    best deployment strategy aligns with your business goals, resource constraints
    and specific needs.'
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve made it this far, I’d love to hear your thoughts on the deployment
    approaches you used for your projects.
  prefs: []
  type: TYPE_NORMAL
