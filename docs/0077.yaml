- en: 'Courage to Learn ML: A Detailed Exploration of Gradient Descent and Popular
    Optimizers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09](https://towardsdatascience.com/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d?source=collection_archive---------4-----------------------#2024-01-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are You Truly Mastering Gradient Descent? Use This Post as Your Ultimate Checkpoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--022ecf97be7d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--022ecf97be7d--------------------------------)
    ·22 min read·Jan 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdbe85b9e568d392e02e271ea983476d.png)'
  prefs: []
  type: TYPE_IMG
- en: We will use an RPG game as an analogy today. Created By ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: Welcome back to a new chapter of ‘[Courage to Learn ML](https://towardsdatascience.com/tagged/courage-to-learn-ml).
    For those new to this series, this series aims to make these complex topics accessible
    and engaging, much like a casual conversation between a mentor and a learner,
    inspired by the writing style of “[The Courage to Be Disliked](https://www.goodreads.com/book/show/43306206-the-courage-to-be-disliked),”
    with a specific focus on machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our previous discussions, our mentor and learner discussed about some common
    loss functions and the three fundamental principles of designing loss functions.
    Today, they’ll explore another key concept: gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, here’s a list of the topics we’ll be exploring today:'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly is a gradient, and why is the technique called ‘gradient descent’?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why doesn’t vanilla gradient descent perform well in Deep Neural Networks (DNNs),
    and what are the improvements?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A review of various optimizers and their relationships (Newton’s method, Adagrad,
    Momentum, RMSprop, and Adam)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical insights on selecting the right optimizer based on my personal experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
