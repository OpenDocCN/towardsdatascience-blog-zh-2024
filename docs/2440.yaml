- en: 'K Nearest Neighbor Regressor, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/k-nearest-neighbor-regressor-explained-a-visual-guide-with-code-examples-df5052c8c889?source=collection_archive---------1-----------------------#2024-10-07](https://towardsdatascience.com/k-nearest-neighbor-regressor-explained-a-visual-guide-with-code-examples-df5052c8c889?source=collection_archive---------1-----------------------#2024-10-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: REGRESSION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the neighbors FAST with KD Trees and Ball Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--df5052c8c889--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--df5052c8c889--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--df5052c8c889--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--df5052c8c889--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--df5052c8c889--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--df5052c8c889--------------------------------)
    ¬∑11 min read¬∑Oct 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----df5052c8c889--------------------------------)
    [## K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples
    for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: The friendly neighbor approach to machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----df5052c8c889--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Building on our exploration of the [Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1),
    let‚Äôs turn to its sibling in the regression world. The Nearest Neighbor Regressor
    applies the same intuitive concept to predicting continuous values. But as our
    datasets get bigger, finding these neighbors efficiently becomes a real pain.
    That‚Äôs where KD Trees and Ball Trees come in.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs super frustrating that there‚Äôs no clear guide out there that really explains
    what‚Äôs going on with these algorithms. Sure, there are some 2D visualizations,
    but they often don‚Äôt make it clear how the trees work in multidimensional setting.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will explain what‚Äôs **actually** going on in these algorithms without
    using the oversimplified 2D representation. We‚Äôll be focusing on the construction
    of the trees itself and see which computation (and numbers) actually matters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00c7b6d82e926d5ce08ebc77977eab63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Nearest Neighbor Regressor is a straightforward predictive model that estimates
    values by averaging the outcomes of nearby data points. This method builds on
    the idea that similar inputs likely yield similar outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aba12c7b4e3b510a0d187544f9510e12.png)'
  prefs: []
  type: TYPE_IMG
- en: Nearest Neighbor approaches are among the most basic yet powerful techniques
    in the machine learning toolkit. Their simplicity belies their effectiveness in
    many real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: üìä Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate our concepts, we‚Äôll use [our usual dataset](/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629).
    This dataset helps predict the number of golfers visiting on any given day. It
    includes factors such as weather outlook, temperature, humidity, and wind conditions.
    Our goal is to estimate the daily golfer turnout based on these variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8f70777fcdcfbe631c352c17a649d816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‚ÄòOutlook‚Äô, ‚ÄòTemperature‚Äô (in Fahrenheit), ‚ÄòHumidity‚Äô (in %), ‚ÄòWind‚Äô
    (Yes/No) and ‚ÄòNumber of Players‚Äô (numerical, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: To use Nearest Neighbor Regression effectively, we need to preprocess our data.
    This involves [converting categorical variables into numerical format](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    and [scaling numerical features](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/856bb5024939a0acbe57928f62346faa.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard scaling is applied to ‚ÄòTemperature‚Äô and ‚ÄòHumidity‚Äô while the one-hot
    encoding is applied to ‚ÄòOutlook‚Äô and ‚ÄòWind‚Äô
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Nearest Neighbor Regressor works similarly to its classifier counterpart,
    but instead of voting on a class, it averages the target values. Here‚Äôs the basic
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the distance between the new data point and all points in the training
    set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the K nearest neighbors based on these distances.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the average of the target values of these K neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign this average as the predicted value for the new data point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/250524333fd9efd70666c9f0c773da69.png)'
  prefs: []
  type: TYPE_IMG
- en: The approach above, using all data points to find neighbors, is known as the
    **Brute Force** method. It‚Äôs straightforward but can be slow for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we‚Äôll explore two more efficient algorithms for finding nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**KD Tree for KNN Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KD Tree (K-Dimensional Tree) is a binary tree structure used for organizing
    points in a *k*-dimensional space. It‚Äôs particularly useful for tasks like nearest
    neighbor searches and range searches in multidimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Steps:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. Build the KD Tree:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Start with a root node that contains all the points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9eb3bb6d235cd561bf67f214fa3586f.png)'
  prefs: []
  type: TYPE_IMG
- en: b. Choose a feature to split on. Any random feature should be ok actually, but
    another good way to choose this is by looking which feature has median value closest
    to the midpoint between max and min value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79a398331b63176a9e9084603cb200e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Temperature has the midpoint line closest to the median line. We can start splitting
    from that dimension.
  prefs: []
  type: TYPE_NORMAL
- en: c. Split the tree in the chosen feature and midpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1a195c9003c18519a00a688f662db95.png)'
  prefs: []
  type: TYPE_IMG
- en: d. Recursively, do step a-c until the stopping criterion, usually minimal leaf
    size (see [‚Äúmin samples leaf‚Äù here](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/))
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfad5e8688c34aba173a931f66dbd614.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Store the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: Along with each point in the KD Tree, store its corresponding target value.
    The minimum and maximum value for each node are also stored.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3cf4c4165760cabc63946fa1328de2c8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Regression/Prediction Steps:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. Traverse the KD Tree:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Start at the root node.
  prefs: []
  type: TYPE_NORMAL
- en: b. Compare the query point (test point) with the splitting dimension and value
    at each node.
  prefs: []
  type: TYPE_NORMAL
- en: c. Recursively traverse left or right based on this comparison.
  prefs: []
  type: TYPE_NORMAL
- en: d. When reaching a leaf node, add its points to the candidate set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6ac7ebb18046966c3e7886af47a307e.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Refine the search:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Backtrack through the tree, checking if there could be closer points in other
    branches.
  prefs: []
  type: TYPE_NORMAL
- en: b. Use distance calculations to the maximum and minimum of the unexplored branches
    to determine if exploring is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7b70deaeed99a10b64698e7be1c231e.png)'
  prefs: []
  type: TYPE_IMG
- en: We backtrack to the branches that has not been visited and check the distance
    to the minimum and maximum of those node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfb1b7b1b636bee05aa2c2ce53805785.png)'
  prefs: []
  type: TYPE_IMG
- en: As both the minimum and maximum of those nodes are further than kth distance,
    no need to check the distance to the data points in those nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Find K nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Among the candidate points found, select the K points closest to the query
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Perform regression:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate the average of the target values of the K nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: b. This average is the predicted value for the query point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73271b1d2aa3434168ed7b3c7b85b162.png)'
  prefs: []
  type: TYPE_IMG
- en: By using a KD Tree, the average time complexity for finding nearest neighbors
    can be reduced from *O*(*n*) in the brute force method to *O*(log *n*) in many
    cases, where *n* is the number of points in the dataset. This makes KNN Regression
    much more efficient for large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ball Tree for KNN Regression**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ball Tree is another space-partitioning data structure that organizes points
    in a series of nested hyperspheres. It‚Äôs particularly effective for high-dimensional
    data where KD Trees may become less efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training Steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Build the Ball Tree:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate the centroid of all points in the node (the mean). This becomes
    the **pivot point**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4e68143b7ec7469a2c23cc8f59c3d50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'b. Make the first branch:'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Finding the first center:** Choose the furthest point from the pivot point
    as the first center with its distance as the **radius**.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Finding the second center:** From the first center, select the furthest
    point as the second center.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Partitioning:** Divide the remaining points into two child nodes based
    on which center they are closer to.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f318c1ad3e5909f1299034607cbb9d7.png)'
  prefs: []
  type: TYPE_IMG
- en: d. Recursively apply steps a-b to each child node until a stopping criterion
    is met, usually minimal leaf size (see [‚Äúmin samples leaf‚Äù here](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e/)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a2c7c38531c763643bd4d7f179f03e4.png)![](../Images/466452b8a11ac4c4308f57d663d4c728.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Store the target values:'
  prefs: []
  type: TYPE_NORMAL
- en: Along with each point in the Ball Tree, store its corresponding target value.
    The radius and centroid for each node are also stored.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9883a4e7158dce6fa0413c88d60c6432.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Regression/Prediction Steps:**'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Traverse the Ball Tree:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Start at the root node.
  prefs: []
  type: TYPE_NORMAL
- en: b. At each node, calculate the distance between the unseen data and the center
    of each child hypersphere.
  prefs: []
  type: TYPE_NORMAL
- en: c. Traverse into the closest hypersphere first.
  prefs: []
  type: TYPE_NORMAL
- en: d. When reaching a leaf node, add its points to the candidate set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85e4feaae0d4811c3cc6f20839783835.png)'
  prefs: []
  type: TYPE_IMG
- en: '2\. Refine the search:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Determine if other branches need to be explored.
  prefs: []
  type: TYPE_NORMAL
- en: b. If the distance to a hypersphere plus its radius is greater than the current
    Kth nearest distance, that branch can be safely ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a55da95adf34346e7d47fb4f5f316f47.png)'
  prefs: []
  type: TYPE_IMG
- en: For those branches that we considered before, add the radius to the distance.
    If it is greater than the kth distance, no need to explore those balls.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b1dfe1b420ff207edb0a413d4f3f131.png)'
  prefs: []
  type: TYPE_IMG
- en: '3\. Find K nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Among the candidate points found, select the K points closest to the query
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Perform regression:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate the average of the target values of the K nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: b. This average is the predicted value for the query point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2b2f49da2d913d67e259e3e5fa857fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Ball Trees can be more efficient than KD Trees for high-dimensional data or
    when the dimensionality is greater than the log of the number of samples. They
    maintain good performance even when the number of dimensions increases, making
    them suitable for a wide range of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The time complexity for querying in a Ball Tree is O(log *n*) on average, similar
    to KD Trees, but Ball Trees often perform better in higher dimensions where KD
    Trees may degrade to linear time complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Step (Brute Force, KD Tree, Ball Tree)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regardless of the algorithm we choose, all of them give the same following
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac80599b3efec37df8746639217d7aea.png)'
  prefs: []
  type: TYPE_IMG
- en: Compared to [the result of the dummy regressor,](https://medium.com/towards-data-science/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629)
    there is a major improvement for the value of RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: Which Algorithm to Choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can follow this simple rule for choosing the best one:'
  prefs: []
  type: TYPE_NORMAL
- en: '- For small datasets (< 1000 samples), ‚Äòbrute‚Äô might be fast enough and guarantees
    finding the exact nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: '- For larger datasets with few features (< 20), ‚Äòkd_tree‚Äô is usually the fastest.'
  prefs: []
  type: TYPE_NORMAL
- en: '- For larger datasets with many features, ‚Äòball_tree‚Äô often performs best.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ‚Äòauto‚Äô option in scikit-learn typically follow the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdea807e4c32673c58a2daa66594870a.png)'
  prefs: []
  type: TYPE_IMG
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While KNN regression has many other parameter, other than the algorithm we just
    discussed (brute force, kd tree, ball tree), you mainly need to consider
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of Neighbors (K).**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '- Smaller K: More sensitive to local patterns, but may lead to overfitting.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '- Larger K: Smoother predictions, but might miss important local variations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unlike classification, **even numbers are fine** in regression as we‚Äôre averaging
    values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Leaf Size**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the stopping condition for building kd tree or ball tree. Generally,
    It affects the speed of construction and query, as well as the memory required
    to store the tree.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Simplicity and Versatility**: Easy to understand and implement; can be used
    for both classification and regression tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Assumptions**: Doesn‚Äôt assume anything about the data distribution, making
    it suitable for complex datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**No Training Phase**: Can quickly incorporate new data without retraining.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpretability**: Predictions can be explained by examining nearest neighbors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cons:**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Computational Complexity**: Prediction time can be slow, especially with
    large datasets, though optimized algorithms (KD-Tree, Ball Tree) can help for
    lower dimensions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Curse of Dimensionality**: Performance degrades in high-dimensional spaces,
    affecting both accuracy and efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Memory Intensive**: Requires storing all training data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensitive to Feature Scaling and Irrelevant Features**: Can be biased by
    features on larger scales or those unimportant to the prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The K-Nearest Neighbors (KNN) regressor is a basic yet powerful tool in machine
    learning. Its straightforward approach makes it great for beginners, and its flexibility
    ensures it‚Äôs useful for experts too.
  prefs: []
  type: TYPE_NORMAL
- en: As you learn more about data analysis, use KNN to understand the basics of regression
    before exploring more advanced methods. By mastering KNN and how to compute the
    nearest neighbors, you‚Äôll build a strong foundation for tackling more complex
    challenges in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: üåü k Nearest Neighbor Regressor Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html),
    [KDTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html),
    [BallTree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html),
    and its implementation in scikit-learn, readers can refer to their official documentation.
    It provides comprehensive information on their usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôéùôöùôö ùô¢ùô§ùôßùôö ùôçùôöùôúùôßùôöùô®ùô®ùôûùô§ùô£ ùòºùô°ùôúùô§ùôßùôûùô©ùôùùô¢ùô® ùôùùôöùôßùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----df5052c8c889--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----df5052c8c889--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This ‚Äúdummy‚Äù doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----df5052c8c889--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----df5052c8c889--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
