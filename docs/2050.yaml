- en: Why Does Position-Based Chunking Lead to Poor Performance in RAGs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/semantic-chunking-for-rag-35b7675ffafd?source=collection_archive---------7-----------------------#2024-08-22](https://towardsdatascience.com/semantic-chunking-for-rag-35b7675ffafd?source=collection_archive---------7-----------------------#2024-08-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to implement semantic chunking and gain better results.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page---byline--35b7675ffafd--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page---byline--35b7675ffafd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--35b7675ffafd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--35b7675ffafd--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page---byline--35b7675ffafd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--35b7675ffafd--------------------------------)
    ·11 min read·Aug 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cab396b365bca88a9b5329d247790974.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [vackground.com](https://unsplash.com/@vackground?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Neighbors could still be different.
  prefs: []
  type: TYPE_NORMAL
- en: Language models come with a context limit. For newer OpenAI models, this is
    around 128k tokens, roughly 80k English words. This may sound big enough for most
    use cases. Still, large production-grade applications often need to refer to more
    than 80k words, not to mention images, tables, and other unstructured information.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we pack everything within the context window with more irrelevant information,
    LLM performance drops significantly.
  prefs: []
  type: TYPE_NORMAL
- en: This is where RAG helps. RAG retrieves the relevant information from an embedded
    source and passes it as context to the LLM. To retrieve the ‘relevant information,’
    we should have divided the documents into chunks. Thus, chunking plays a vital
    role in a RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking helps the RAG retrieve specific pieces of a large document. However,
    small changes in the chunking strategy can significantly impact the responses
    LLM makes.
  prefs: []
  type: TYPE_NORMAL
