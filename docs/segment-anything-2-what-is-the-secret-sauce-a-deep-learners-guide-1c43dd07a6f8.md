# Segment Anything 2：秘密武器是什么？（深度学习者指南）

> 原文：[https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06](https://towardsdatascience.com/segment-anything-2-what-is-the-secret-sauce-a-deep-learners-guide-1c43dd07a6f8?source=collection_archive---------2-----------------------#2024-08-06)

## 基础模型 + 可提示 + 互动 + 视频。如何实现？

[](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Avishek Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------) [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1c43dd07a6f8--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1c43dd07a6f8--------------------------------) ·阅读时长 10 分钟·2024年8月6日

--

![](../Images/903c3e88dd79d171b8844c61bba59c13.png)

Segment Anything 2 流程（图由作者提供）

Meta 最近发布了 Segment Anything 2 模型，简称 SAM 2——这是一种神经网络，不仅能够分割图像，还能分割整个视频。SAM 2 是一个可提示的互动基础分割模型。**可提示**意味着你可以点击或拖动一个或多个你想要分割的对象的边界框，SAM2 就能预测一个遮罩，将目标对象单独标出并在输入片段中追踪它。**互动**意味着你可以随时编辑提示，比如在不同的帧中添加新的提示——分割结果会相应地调整！最后，作为一个**基础**分割模型，它是在庞大的数据集上训练的，可以应用于多种不同的使用场景。

**请注意，本文是“深度学习指南”，因此我们将主要关注 SAM-2 后面的网络架构。** [**如果你是视觉学习者，可能想查看本文所依据的 YouTube 视频。**](https://youtu.be/wMGb97EZkVU)

# **可提示视觉分割（PVS）**

SAM-2 侧重于 PVS 或可提示视觉分割任务。给定一个输入视频和用户提示——如点击、框选或遮罩——网络必须预测一个 masklet，这也是时空遮罩的另一种说法。一旦预测出 masklet，就可以通过在额外的帧中提供更多的提示——通过正负点击——来迭代地精细化它，以交互方式更新分割后的遮罩。

# 原始 Segment Anything 模型

SAM-2是在原始SAM架构的基础上发展而来，原始SAM是一个图像分割模型。让我们快速回顾一下原始SAM模型的基本架构。

![](../Images/b7b60f389aac038f79e037627b14323f.png)

SAM-1架构 — 图像编码器对输入图像进行编码。提示编码器对输入提示进行编码，掩码解码器将提示和图像嵌入结合起来，预测查询对象的分割掩码。掩码解码器还输出IOU得分，为简化起见，以上未显示这些得分。（插图由作者提供）

1.  **图像编码器**处理输入图像，生成通用的图像嵌入。这些嵌入与提示无关。

1.  **提示编码器**处理用户输入的提示，生成提示嵌入。提示嵌入与输入图像无关。

1.  **掩码解码器**输入无条件的图像和提示嵌入，并应用交叉注意力和自注意力模块将它们进行上下文化处理。通过生成的上下文化嵌入，生成多个分割掩码

1.  **多个输出分割掩码**由掩码解码器预测。这些掩码通常表示查询对象的整体、部分或子部分，并有助于解决由于用户提示而可能出现的歧义（下图）。

1.  **交并比（Intersection-Over-Union）** **得分**会为每个输出分割掩码进行预测。这些IoU得分表示SAM对每个预测掩码“正确”的信心分数。也就是说，如果SAM为掩码1预测了较高的IoU得分，那么掩码1很可能是正确的掩码。

![](../Images/bd73d4e972661b0b50d755bf9a604df8.png)

SAM预测3个分割掩码，通常表示查询对象的“整体”、“部分”和“子部分”。对于每个预测的掩码，SAM还会预测一个IOU得分，以估计生成的掩码与感兴趣对象的重叠程度（来源：图像由作者提供）

那么，SAM-2在采用上述架构处理视频时做了什么不同的改进呢？让我们讨论一下。

# **帧编码器**

输入视频首先被分割成多个帧，每个帧都使用基于视觉变换器（Vision Transformer）的掩码自编码器（Masked Auto-encoder）计算机视觉模型独立编码，称为[Heira架构](https://arxiv.org/abs/2306.00989)。我们稍后再讨论这个变换器的具体架构，现在只需将其视为一个黑盒，它将单个帧作为图像输入，输出一个形状为256x64x64的多通道特征图。视频的所有帧都采用相同的编码器进行处理。

![](../Images/a49f79e9bdba6c49fafeb11ad150f185.png)

输入视频的帧通过预训练的视觉变换器进行嵌入（图像由作者提供）

请注意，这些嵌入并不考虑视频序列——它们只是独立的帧嵌入，意味着它们无法访问视频中的其他帧。其次，就像 SAM-1 一样，它们完全不考虑输入提示——这意味着结果输出被视为输入帧的通用表示，并且完全不依赖于输入提示。

> 这样做的好处是，如果用户在未来的帧中添加新的提示，我们无需再次通过图像编码器处理这些帧。每个帧只需对图像编码器运行一次，结果会被缓存并重用于所有类型的输入提示。这一设计决策使得 SAM-2 可以以交互速度运行——因为图像编码的重工作只需要在每个视频输入中进行一次。

## 关于 Heira 架构的简要说明

图像编码器的确切性质是一个实现细节——只要编码器足够好并且在大量图像语料库上进行训练，就可以。Heira 架构是一个层次化的视觉变换器，这意味着随着网络的加深，空间分辨率被降低，特征维度被增加。这些模型是在掩膜自编码的任务上进行训练的，其中输入网络的图像被分成多个块，有些块会被随机灰化——然后 Heira 模型从剩余的可以看到的块中学习重建原始图像。

![](../Images/b5906b22e25bd869ec70282798dabd8e.png)

Heira 模型返回通用的图像嵌入，可以用于各种下游计算机视觉任务！（来源：[这里](https://arxiv.org/abs/2306.00989)）

由于掩膜自编码是自监督的，意味着所有标签都是从源图像本身生成的，我们可以轻松地在大量图像数据集上训练这些大型编码器，而不需要手动标注它们。掩膜自编码器倾向于学习关于图像的通用嵌入，这些嵌入可以用于许多下游任务。它的通用嵌入能力使其成为 SAM 帧编码器的首选架构。

# **提示编码器**

就像原始的 SAM 模型一样，输入提示可以来自点击点、框或分割掩膜。提示编码器的工作是通过将这些提示转换为代表性向量表示来编码它们。这里有一段关于原始 SAM 架构的视频，详细讲解了提示编码器是如何工作的。

原始的《Segment Anything》论文解释（作者的视频）

提示编码器将其转换为 N_tokens x 256 的形状。例如，

+   **编码点击** — 点击的 x 和 y 坐标的位置信息被用作提示序列中的一个 token。点击的“类型”（前景/正面或背景/负面）也包含在表示中。

+   **编码边界框** — 使用左上角和右下角点的位置信息进行编码。

![](../Images/1f213877f686559e3908dc264a96c792.png)

编码提示（作者插图）

## 关于密集提示编码的说明

点点击和边界框是**稀疏提示编码**，但我们也可以输入整个分割掩码，这是一种**密集提示编码**。密集提示编码在推理过程中很少使用，但在训练期间，它们用于迭代训练SAM模型。SAM的训练方法超出了本文的范围，但对于那些感兴趣的人，下面是我试图在一段话中解释整个文章的尝试。

> SAM是通过迭代分割训练的。在训练过程中，当SAM输出分割掩码时，我们将其作为密集提示与来自真实标签和预测的细化点击（稀疏提示）一起输入回SAM。掩码解码器使用这些稀疏和密集提示，并学习输出一个新的细化分割掩码。在推理过程中，仅使用稀疏提示，分割掩码通过一次推理预测出来（不反馈密集掩码）。

也许有一天我会写一篇关于迭代分割训练的文章，但现在，让我们继续探索SAM的网络架构。

在SAM-2中，提示编码基本保持不变。唯一的区别是，它们必须分别对用户提示的所有帧运行。

# 掩码解码器（预告）

嗯…在我们讨论掩码解码器之前，先来聊聊SAM-2中的*内存*概念。目前，我们假设掩码解码器输入一堆*东西*（相信我，我们会在稍后讨论这些东西是什么），然后输出分割掩码。

# **内存编码器与内存库**

**在掩码解码器生成输出掩码后，输出掩码会经过内存编码器，得到一个内存嵌入。**每处理一帧都会创建一个新的内存。这些内存嵌入会被追加到**内存库**中，内存库是一个先进先出（FIFO）的队列，用于存储视频解码过程中生成的最新内存。

![](../Images/68fcfff8b576d705204230c7bef185d0.png)

对于每一帧，内存编码器将掩码解码器的输出掩码输入，并将其转换为内存。然后，这个内存会被插入到名为内存库的FIFO队列中。请注意，这张图没有显示内存注意模块——我们稍后会在文章中讨论它。（来源：作者插图）

## 内存编码器

输出的掩码首先通过卷积层进行下采样，然后将无条件的图像编码添加到此输出中，经过轻量级的卷积层融合信息，得到的空间特征图称为内存。你可以把内存想象成原始输入帧和给定时间帧生成的掩码的表示。

![](../Images/6c09fd1398bf2decbcd19132e6ef00a5.png)

内存编码器（作者插图）

## 内存库

内存库包含以下内容：

+   最近的N个记忆存储在队列中

+   用户输入的最后M个提示，用于跟踪多个先前的提示。

+   每个帧的掩码解码器输出标记也会被存储——这些标记像对象指针，捕捉到关于待分割对象的高级语义信息。

![](../Images/4b3bb6a345a4066cb18a4130b6156ec3.png)

内存库（图片来自作者）

# 内存注意力

我们现在有了一种保存历史信息到内存库中的方式。接下来，我们需要在生成未来帧的分割掩码时使用这些信息。这是通过使用**内存注意力**来实现的。内存注意力的作用是将当前帧特征与内存库特征进行条件化，然后再输入到掩码解码器中。

![](../Images/903c3e88dd79d171b8844c61bba59c13.png)

内存注意力模块将图像编码与内存库进行上下文化处理。然后，这些上下文化的图像嵌入被输入到掩码解码器中生成分割掩码。（图片来自作者）

内存注意力模块首先对帧嵌入执行自注意力，然后在图像嵌入和内存库内容之间执行交叉注意力。因此，无条件的图像嵌入会与先前的输出掩码、先前的输入提示和对象指针进行上下文化处理。

![](../Images/c1c865a12ddbd6da4bdc6efba6d6280a.png)

内存注意力模块（图片来自作者）

在自注意力和交叉注意力层中，除了常规的正弦位置嵌入外，还使用了二维旋转位置嵌入。简单来说——旋转位置嵌入能够捕捉帧之间的相对关系，二维旋转位置嵌入在图像中效果良好，因为它们有助于建模帧之间的空间关系，包括水平和垂直方向。

# 掩码解码器（这次是真的）

掩码解码器输入由内存注意力模块输出的内存上下文化的图像编码，以及提示编码，并输出分割掩码、IOU分数和（全新的）遮挡分数。

![](../Images/75323ed13ac8d94dd234ad70a184ec0d.png)

SAM-2掩码解码器（来源：SAM-2论文 [这里](https://arxiv.org/abs/2408.00714)）

掩码解码器使用自注意力和交叉注意力机制，将提示标记与（内存条件化的）图像嵌入进行上下文化处理。这使得图像和提示能够“结合”成一个上下文感知的序列。然后，这些嵌入被用来生成分割掩码、IOU分数和遮挡分数。基本上，在视频过程中，被查询的对象可能会被遮挡，因为它被场景中的另一个对象挡住——遮挡分数预测查询的对象是否出现在当前帧中。遮挡分数是SAM-2的新功能，用于预测查询的对象是否存在于当前场景中。

回顾一下，和IOU分数一样，SAM会为每个预测的三种掩码生成一个遮挡分数。这三个IOU分数告诉我们SAM对每个预测掩码的信心有多大，而这三个遮挡分数则告诉我们SAM认为相应物体出现在场景中的可能性有多大。

![](../Images/c8151a6a0bd269c1550c5470e9d5258d.png)

SAM-2 Mask解码器的输出（图片由作者提供）

# 最终思考

作者关于SAM-2的视频

所以…这就是SAM-2背后的网络架构概述。对于从视频中提取的帧（通常为6帧每秒），这些帧会使用帧/图像编码器进行编码，记忆注意力会对图像编码进行上下文处理，如果有提示词，提示词也会被编码，之后Mask解码器将图像和提示词的嵌入结合，生成输出掩码、IOU分数和遮挡分数，记忆通过记忆编码器生成，并被追加到记忆库中，整个过程会重复进行。

![](../Images/7b4500639eff2ecec696df16741e614e.png)

SAM-2算法（图片由作者提供）

关于SAM-2仍然有许多内容值得讨论，比如它是如何通过互动训练成为可提示模型的，以及他们的数据引擎是如何工作来创建训练数据的。我希望在另一篇文章中详细讨论这些话题！你可以在我上面的YouTube频道观看SAM-2视频，了解更多信息，并观看支持SAM-2的系统的视觉展示。

***感谢阅读！请鼓掌并关注！***

# 参考文献/接下来该去哪儿

+   [作者的YouTube频道](https://www.youtube.com/@avb_fj)

+   [我对SAM的讲解视频](https://youtu.be/OhxJkqD1vuE)

+   [我对SAM-2的讲解视频](https://youtu.be/wMGb97EZkVU)

+   你可以在[Meta的网站](https://ai.meta.com/blog/segment-anything-2/)上使用Segment Anything 2模型。

+   [OG SAM论文](https://arxiv.org/abs/2304.02643)

+   [SAM-2论文](https://arxiv.org/pdf/2408.00714)
