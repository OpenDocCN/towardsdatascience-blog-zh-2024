- en: 'One Year of Consistent Kaggling: What Did It Teach Me?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/1-year-of-continuous-kaggling-what-did-it-taught-me-d267c222cfa3?source=collection_archive---------1-----------------------#2024-05-11](https://towardsdatascience.com/1-year-of-continuous-kaggling-what-did-it-taught-me-d267c222cfa3?source=collection_archive---------1-----------------------#2024-05-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Competitions are more valuable than other components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@geremieyeo?source=post_page---byline--d267c222cfa3--------------------------------)[![Geremie
    Yeo](../Images/d218b1be15725937f7e70b6db49cb184.png)](https://medium.com/@geremieyeo?source=post_page---byline--d267c222cfa3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d267c222cfa3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d267c222cfa3--------------------------------)
    [Geremie Yeo](https://medium.com/@geremieyeo?source=post_page---byline--d267c222cfa3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d267c222cfa3--------------------------------)
    ·5 min read·May 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[Kaggle](https://www.kaggle.com/) is a platform for users to gain hands-on
    experience on practical data science and machine learning. It has 4 different
    progression components, namely Competitions, Datasets, Notebooks and Discussions.
    No prior experience in data science is necessary to get yourself started in using
    this platform and learn'
  prefs: []
  type: TYPE_NORMAL
- en: 'My background: I did my first project on Kaggle as part of a Machine Learning
    course in my Bachelor’s curriculum (Math + Comp Sci) in early 2023\. Since then
    I have been hooked to this platform as a **favorite pastime**. I have taken part
    in 20 competitions to date. I had no work/internship experience as a data scientist
    prior to starting Kaggle.'
  prefs: []
  type: TYPE_NORMAL
- en: In one year, I have made (I believe) significant progress in my Kaggle journey,
    including winning 2 gold competition medals, one of which I won 1st place and
    rising to the top 116 in the Competitions category, while barely missing a day
    of activity.
  prefs: []
  type: TYPE_NORMAL
- en: '[My Kaggle Profile](https://www.kaggle.com/yeoyunsianggeremie)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e919c5318c1a0d336047ed28f21b0232.png)![](../Images/6d05e765021b51e73713d913d93bc87c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s dive into 3 key learnings from my Kaggle journey to date.
  prefs: []
  type: TYPE_NORMAL
- en: '**Your team cannot solely rely on public notebooks to succeed in Competitions**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A standard Kaggle competition only awards Gold medals to the **top 10 + floor(NumTeams
    / 500) teams**! For example, in a competition with 2500 teams, only 15 teams win
    gold. This is **mandatory** for one to progress to the **Master** tier in competitions,
    and you need **five (including one solo)** to progress to the **Grandmaster**
    tier**.**
  prefs: []
  type: TYPE_NORMAL
- en: It is very unlikely that your team could just briefly modify public work (such
    as ensembling public notebooks) and earn a spot in the gold zone. Your team will
    be competing against top-notch data scientists and grandmasters who have lots
    of creative ideas to approach the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly modifying public work is something even beginners to ML can do and it
    is unlikely your team’s solution stands out using this. Most likely, a small enhancement
    of a public notebook gets a bronze medal, or if lucky, a silver medal.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the 2 competitions which my team won gold:'
  prefs: []
  type: TYPE_NORMAL
- en: '1/2048 (Champion) [PII Detection](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/overview):
    We used a wide variety of Deberta architectures and postprocessing strategies,
    most of which are not shared in the public forums. No public models were used
    in our final ensemble'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '14/4436 [Optiver — Trading At The Close](https://www.kaggle.com/competitions/optiver-trading-at-the-close/overview):
    We used online training to make sure the model is fitted with the latest data
    before making the prediction. It was not easy to write an online training pipeline
    that worked on the private LB, and such an idea was not shared in the public forums,
    as far as I know. We did not use the [popular public training approach](https://www.kaggle.com/code/verracodeguacas/fold-cv)
    as we felt it was overfitting to the train data, despite its great public LB score'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In contrast, here is a competition in which my team won bronze:'
  prefs: []
  type: TYPE_NORMAL
- en: '185/2664 [LLM Science Exam](https://www.kaggle.com/competitions/kaggle-llm-science-exam):
    We briefly modified a [public training pipeline](https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-1)
    and a [public inference pipeline](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles),
    such as changing the embedding model, hyperparameters and ensembling. There was
    no creative ideas in our final solution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Summary: In my opinion, it is better to spend more time analyzing the baseline,
    and research to think of enhancements. It may be a good idea to start with a small
    model (deberta-v3-xsmall for example) to evaluate ideas quickly. Aim to establish
    a robust cross-validation strategy from the very beginning.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **You learn much more from the Competitions category compared to Datasets/Notebooks/Discussions**
  prefs: []
  type: TYPE_NORMAL
- en: Some of the **real-world** skills I learnt
  prefs: []
  type: TYPE_NORMAL
- en: I was the team leader for most of the competitions I participated in, including
    both of them which my team won the gold medal. It has drastically improved my
    communication and leadership skills.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborating with other data scientists/engineers from different countries
    and timezones, and learning good practices from them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using [Wandb](https://wandb.ai/site) to track and log experiments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing architectures of transformer models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating use-case specific synthetic datasets using LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to model a real-world use case in a data science perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing clean code that is easily understandable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to utilize multi-GPU training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better time management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating and mitigating model mistakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast, it is much easier to progress in datasets/notebooks/discussions
    without learning much about data science. In discussions, a user can earn gold
    discussion medals by posting his/her accomplishments on the Kaggle forum. I doubt
    I would learn most of the skills above without doing competitions. In my opinion,
    progress on datasets/notebooks/discussions does not necessarily tell that one
    is passionate about data science.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. **Playground Competitions is a great way to start for beginners**
  prefs: []
  type: TYPE_NORMAL
- en: The playground series simulate the featured competitions, except that it is
    more beginner-friendly and do not award medals/prizes. In playgrounds, you make
    predictions on a tabular dataset, which allows you to learn the basics of coding
    an ML pipeline. Plenty of notebooks are shared in playgrounds, both tabular and
    NN (neural network) approaches, so if you are stuck, those public notebooks are
    a good reference.
  prefs: []
  type: TYPE_NORMAL
- en: Each playground series competition is about 1 month long.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on my experience, the playground competitions taught me:'
  prefs: []
  type: TYPE_NORMAL
- en: How to build a robust cross-validation strategy and not overfit the public LB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to select submissions for evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to perform feature engineering and feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to style a Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (More on the data engineering side of things) How to use [Polars](https://pola.rs/).
    This is a much faster dataframe library than Pandas and is better suited for big
    data use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, I feel the most rewarding part from doing Kaggle is the hands-on
    experience in competitions and the opportunity to collaborate with data professionals
    from around the globe. I get to solve a wide variety of problems ranging from
    tabular to more advanced NLP tasks. Looking forward to more as I continue to improve
    myself in the field of data science!
  prefs: []
  type: TYPE_NORMAL
