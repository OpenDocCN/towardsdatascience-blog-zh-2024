<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Machine Learning Algorithms as a Mapping Between Spaces: From SVMs to Manifold Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Machine Learning Algorithms as a Mapping Between Spaces: From SVMs to Manifold Learning</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07">https://towardsdatascience.com/machine-learning-algorithms-as-a-mapping-between-spaces-from-svms-to-manifold-learning-b1dfe1046e4f?source=collection_archive---------3-----------------------#2024-02-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="10aa" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring the beauty of mapping between spaces in SVMs, autoencoders, and manifold learning (isomaps) algorithms</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Salih Salih" class="l ep by dd de cx" src="../Images/220f3c5363989d94c5593eca7ff72c67.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*RZlJ5vTLIQxUcd95ZE2eHQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@salih.salih?source=post_page---byline--b1dfe1046e4f--------------------------------" rel="noopener follow">Salih Salih</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b1dfe1046e4f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/85f97ec0087c788a144bd88bd57fa8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVhUflIpoCFfelz2RKMdPA.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by Evgeni Tcherkasski on Unsplash</figcaption></figure><h1 id="db93" class="nb nc fq bf nd ne nf gq ng nh ni gt nj nk nl nm nn no np nq nr ns nt nu nv nw bk">Introduction</h1><p id="7753" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">In machine learning, understanding how algorithms process, interpret, and classify data relies heavily on the concept of “spaces.” In this context, a space is a mathematical construct where data points are positioned based on their features. Each dimension in the space represents a specific attribute or feature of the data, allowing algorithms to navigate a structured representation.</p><h2 id="1286" class="ot nc fq bf nd ou ov ow ng ox oy oz nj og pa pb pc ok pd pe pf oo pg ph pi pj bk">Feature Space and Input Space</h2><p id="d775" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The journey begins in the feature or input space, where each data point is a vector representing an instance in the dataset. To simplify, imagine an image where each pixel is a dimension in this space. The complexity and dimensionality of the space depend on the number and nature of the features. Working with high-dimensional spaces can be either enjoyable or frustrating for data practitioners.</p><h2 id="ceac" class="ot nc fq bf nd ou ov ow ng ox oy oz nj og pa pb pc ok pd pe pf oo pg ph pi pj bk">Challenges in Low-dimensional Spaces</h2><p id="b3dc" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">In low-dimensional spaces, not all relationships or patterns in the data are easily identifiable. Linear separability, which is the ability to divide classes with a simple linear boundary, is often unachievable. This limitation becomes more apparent in complex datasets where the interaction of features creates non-linear patterns that cannot be captured by simple linear models.</p><p id="6371" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">In this article, we will explore machine learning algorithms in the perspective of mapping and interaction between different spaces. We will start with support vector machines (SVMs) as an example of simplicity, then move on to autoencoders, and finally, we will discuss manifold learning and Isomaps.</p><p id="83b9" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk"><strong class="nz fr">Please note that the code examples in this article are for demonstration and may not be optimized. I encourage you to modify, improve and try the code with different datasets to deepen your understanding and gain further insights.</strong></p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1e16" class="nb nc fq bf nd ne px gq ng nh py gt nj nk pz nm nn no qa nq nr ns qb nu nv nw bk">Support Vector Machines</h1><p id="1597" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Support Vector Machines (SVMs) are known machine learning algorithms that excel at classifying data. As we mentioned at the beginning:</p><blockquote class="qc qd qe"><p id="9430" class="nx ny qf nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">In lower dimensions, linear separability is often impossible, which means it’s difficult to divide classes with a simple linear boundary.</p></blockquote><p id="c024" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">SVMs overcomes this difficulty by transforming data into a higher-dimensional space, making it easier to separate and classify. To illustrate this, let’s look at an example. The code below generates synthetic data that is clearly not linearly separable in its original space.</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="2dd8" class="qk nc fq qh b bg ql qm l qn qo">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import make_circles<br/><br/># Generate synthetic data that is not linearly separable<br/>X, y = make_circles(n_samples=100, factor=0.5, noise=0.05)<br/><br/># Visualize the data<br/>plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')<br/>plt.xlabel('Feature 1')<br/>plt.ylabel('Feature 2')<br/>plt.title('Original 2D Data')<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qp"><img src="../Images/b7e601d9fede398564838a0b0ec32f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*amE3_Ad-T5yvns4__k8jLg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="d063" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">SVMs use a mapping between spaces to separate different classes. They lift the data from a lower dimensional space to a higher dimensional one. In this new space, SVMs find the optimal hyperplane, which is a decision boundary that separates the classes. It’s like finding the perfect line that divides groups in a two-dimensional graph, but in a more complex, multidimensional universe.</p><p id="b854" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">In the provided data, one class is close to the origin and another class is far from the origin. Let’s look at a typical example to understand how this data becomes separable when transformed into higher dimensions.</p><p id="a4f5" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">We will transform each 2D point (x, y) to a 3D point (x, y, z), where z = x² + y². The transformation adds a new third dimension based on the squared distance from the origin in the 2D space. Points that are farther from the origin in the 2D space will be higher in the 3D space because their squared distance is larger.</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="60e8" class="qk nc fq qh b bg ql qm l qn qo">from mpl_toolkits.mplot3d import Axes3D<br/><br/># Transform the 2D data to 3D for visualization<br/>Z = X[:, 0]**2 + X[:, 1]**2  # Use squared distance from the origin as the third dimension<br/><br/># Visualize the 3D data<br/>fig = plt.figure()<br/>ax = fig.add_subplot(111, projection='3d')<br/>ax.scatter(X[:, 0], X[:, 1], Z, c=y, cmap=plt.cm.coolwarm)<br/><br/># Set labels<br/>ax.set_xlabel('Feature 1')<br/>ax.set_ylabel('Feature 2')<br/>ax.set_zlabel('Transformed Feature')<br/><br/># Set the viewpoint<br/>elevation_angle = 15  # Adjust this to change the up/down angle<br/>azimuth_angle = 45  # Adjust this to rotate the plot<br/>ax.view_init(elev=elevation_angle, azim=azimuth_angle)<br/><br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/2a47086d619d1e375b7a139452c47163.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*CoTkYZy5H2Jy2ciCPzZovQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="1a97" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">You can notice from the output above that after this transformation, our data becomes linearly separable by a 2D hyperplane.</p><p id="5e74" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Another example is the effectiveness of drug dosages. A patient is only cured if the dosage falls within a certain range. Dosages that are too low or too high are ineffective. This scenario naturally creates a dataset that is not linearly separable, making it a good candidate for demonstrating how a polynomial kernel can help.</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="554e" class="qk nc fq qh b bg ql qm l qn qo"># Train the SVM model on the 2D data<br/>svc = SVC(kernel='linear', C=1.0)<br/>svc.fit(X, y)<br/><br/># Create a function to plot decision boundary<br/>def plot_svc_decision_function(model, plot_support=True):<br/>    """Plot the decision function for a 2D SVC"""<br/>    ax = plt.gca()<br/>    xlim = ax.get_xlim()<br/>    ylim = ax.get_ylim()<br/><br/>    # create grid to evaluate model<br/>    xx = np.linspace(xlim[0], xlim[1], 30)<br/>    yy = np.linspace(ylim[0], ylim[1], 30)<br/>    YY, XX = np.meshgrid(yy, xx)<br/>    xy = np.vstack([XX.ravel(), YY.ravel()]).T<br/>    Z = model.decision_function(xy).reshape(XX.shape)<br/><br/>    # plot decision boundary and margins<br/>    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,<br/>               linestyles=['--', '-', '--'])<br/>    # plot support vectors<br/>    if plot_support:<br/>        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],<br/>                   s=300, linewidth=1, facecolors='none', edgecolors='k')<br/><br/># Adjust the figure size for better visualization<br/>plt.figure(figsize=(8, 5))<br/><br/># Scatter plot for original dosage points<br/>plt.scatter(dosages, np.zeros_like(dosages), c=y, cmap='bwr', marker='s', s=50, label='Original Dosages')<br/><br/># Scatter plot for dosage squared points<br/>plt.scatter(dosages, squared_dosages, c=y, cmap='bwr', marker='^', s=50, label='Squared Dosages')<br/><br/># Calling the function to plot the SVM decision boundary<br/>plot_svc_decision_function(svc)<br/><br/># Expanding the limits to ensure all points are visible<br/>plt.xlim(min(dosages) - 1, max(dosages) + 1)<br/>plt.ylim(min(squared_dosages) - 10, max(squared_dosages) + 10)<br/><br/># Adding labels, title and legend<br/>plt.xlabel('Dosage')<br/>plt.ylabel('Dosage Squared')<br/>plt.title('SVM Decision Boundary with Original and Squared Dosages')<br/>plt.legend()<br/><br/># Display the plot<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qr"><img src="../Images/5f9d86f94306290f327605ff250d64af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esmoNshbLTeToOCFx_fEcw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><blockquote class="qc qd qe"><p id="6aa6" class="nx ny qf nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">In the two examples above, we take advantage of our knowledge about the data. For instance, in first example we know that we have two classes: one close to the origin and another far from the origin. This is what the algorithm does through training and fine-tuning — it finds a suitable space where the data can be linearly separated.</p></blockquote><p id="4e98" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">The great thing here is that SVMs don’t map data into higher dimensions as this would be very complex computationally. Instead, they compute the relationship between the data as if it were in higher dimensions using the dot product. This is called the “Kernel trick.” I will explain SVM kernels in another article.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="df55" class="nb nc fq bf nd ne px gq ng nh py gt nj nk pz nm nn no qa nq nr ns qb nu nv nw bk">Autoencoders</h1><p id="9bac" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">Autoencoders are truly amazing and beautiful architectures that capture my imagination. They have a wide range of applications across various domains, utilizing diverse types of autoencoders.</p><p id="eef6" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">They basically consist of an <strong class="nz fr">encoder </strong>and a <strong class="nz fr">decoder</strong>, the encoder takes your input and encode/compress it, a process in which we move from a high-dimensional space to a more compact, lower-dimensional one. What’s truly interesting is how the decoder then takes this condensed representation and reconstructs the original data in the higher-dimensional space. The natural question is: how is it possible to go back to the original space from a significantly reduced dimension?</p><p id="0409" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Let’s consider an HD image with a resolution of 720x720 pixels. Storing and transmitting this image requires a lot of memory and bandwidth. Autoencoders solve this problem by compressing the image into a lower-dimensional space, like a 32x32 representation called the ‘bottleneck’. The encoder’s job is done at this point. The decoder takes over, trying to rebuild the original image from this compressed form.</p><p id="a64f" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">This process is similar to sharing images on platforms like WhatsApp. The image is encoded to a lower quality for transmission and then decoded on the receiver’s end. The difference in quality between the original and received image is called ‘reconstruction error’, which is common in autoencoders.</p><p id="caa4" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">In autoencoders, we can think of it as an interaction between 3 spaces:</p><ol class=""><li id="c04f" class="nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os qs qt qu bk">The input space.</li><li id="6b45" class="nx ny fq nz b go qv ob oc gr qw oe of og qx oi oj ok qy om on oo qz oq or os qs qt qu bk">The latent representation space (the bottleneck).</li><li id="b07f" class="nx ny fq nz b go qv ob oc gr qw oe of og qx oi oj ok qy om on oo qz oq or os qs qt qu bk">The output space.</li></ol><p id="4377" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">The beauty here is that we can see the autoencoder as something that operates in these 3 spaces. It takes advantage of the latent spaces to remove any noisy or unnecessary information from the input space, resulting in a very compact representation with core information about the input space. It does this by trying to mirror the input space in the output space, reducing the difference between the two spaces or the reconstruction error.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="c7c7" class="ot nc fq bf nd ou ov ow ng ox oy oz nj og pa pb pc ok pd pe pf oo pg ph pi pj bk">Convolutional Autoencoders: Encoding Complexity into Simplicity</h2><p id="8595" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">The code below shows an example of a convolutional autoencoder, which is a type of autoencoders that works well with images. We will use the popular MNIST dataset[LeCun, Y., Cortes, C., &amp; Burges, C.J. (1998). The MNIST Database of Handwritten Digits. <a class="af ra" href="https://www.tensorflow.org/datasets/catalog/mnist" rel="noopener ugc nofollow" target="_blank">Retrieved from TensorFlow</a>, CC BY 4.0], which contains 28x28 pixel grayscale images of handwritten digits. The encoder plays a crucial role by reducing the dimensionality of the data from 784 elements to a smaller, more condensed form. The decoder then aims to reconstruct the original high-dimensional data from this lower-dimensional representation. However, this reconstruction is not perfect and some information is lost. The autoencoder overcomes this challenge by learning to prioritize the most important features of the data.</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="ea47" class="qk nc fq qh b bg ql qm l qn qo">import tensorflow as tf<br/>from tensorflow.keras import layers<br/>from tensorflow.keras.datasets import mnist<br/>import matplotlib.pyplot as plt<br/><br/># Load MNIST dataset<br/>(x_train, _), (x_test, _) = mnist.load_data()<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = x_train.reshape((len(x_train), 28, 28, 1))<br/>x_test = x_test.reshape((len(x_test), 28, 28, 1))<br/><br/># Define the convolutional autoencoder architecture<br/>input_img = layers.Input(shape=(28, 28, 1))<br/><br/># Encoder<br/>x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)<br/>x = layers.MaxPooling2D((2, 2), padding='same')(x)<br/>x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)<br/>encoded = layers.MaxPooling2D((2, 2), padding='same')(x)<br/><br/># Decoder<br/>x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)<br/>x = layers.UpSampling2D((2, 2))(x)<br/>x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)<br/>x = layers.UpSampling2D((2, 2))(x)<br/>decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)<br/><br/># Autoencoder model<br/>autoencoder = tf.keras.Model(input_img, decoded)<br/>autoencoder.compile(optimizer='adam', loss='binary_crossentropy')<br/>autoencoder.fit(x_train, x_train, epochs=10, batch_size=64, validation_data=(x_test, x_test))<br/><br/># Visualization<br/># Sample images<br/>sample_images = x_test[:8]<br/># Reconstruct images<br/>reconstructed_images = autoencoder.predict(sample_images)<br/><br/># Plot original images and reconstructed images<br/>fig, axes = plt.subplots(nrows=2, ncols=8, figsize=(14, 4))<br/>for i in range(8):<br/>    axes[0, i].imshow(sample_images[i].squeeze(), cmap='gray')<br/>    axes[0, i].set_title("Original")<br/>    axes[0, i].axis('off')<br/>    axes[1, i].imshow(reconstructed_images[i].squeeze(), cmap='gray')<br/>    axes[1, i].set_title("Reconstructed")<br/>    axes[1, i].axis('off')<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/3e836d1a5deab08a354758f18b8d0b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zaMjtvR1oqoEIlNnbg0GOw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="b0bb" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">The output above shows how well the autoencoder works. It displays pairs of images: the original digit images and their reconstructions after encoding and decoding. This example proves that the encoder captures the essence of the data in a smaller form and the decoder can approximate the original image, even though some information is lost during compression.</p><p id="7679" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Now, let’s go further and visualize the learned latent space (the bottleneck). We will use PCA and t-SNE, two techniques to reduce dimensions, to show the compressed data points on a 2D plane. This step is important because it helps us see how the autoencoder organizes the data in the latent space and shows any natural clusters of similar digits. We used PCA and t-SNE together just to compare how well they work.</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="95cf" class="qk nc fq qh b bg ql qm l qn qo"># Encode all the test data<br/>encoded_imgs = encoder.predict(x_test)<br/><br/># Reduce dimensionality using PCA<br/>pca = PCA(n_components=2)<br/>pca_result = pca.fit_transform(encoded_imgs)<br/><br/># Reduce dimensionality using t-SNE<br/>tsne = TSNE(n_components=2, perplexity=30, n_iter=300)<br/>tsne_result = tsne.fit_transform(encoded_imgs)<br/><br/><br/><br/># Visualization using PCA<br/>plt.figure(figsize=(20, 10))<br/>plt.subplot(1, 2, 1)<br/>plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y_test, cmap=plt.cm.get_cmap("jet", 10))<br/>plt.colorbar(ticks=range(10))<br/>plt.title('PCA Visualization of Latent Space')<br/><br/># Visualization using t-SNE<br/>plt.subplot(1, 2, 2)<br/>plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=y_test, cmap=plt.cm.get_cmap("jet", 10))<br/>plt.colorbar(ticks=range(10))<br/>plt.title('t-SNE Visualization of Latent Space')<br/><br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rc"><img src="../Images/849a0320906aa2409e41565a0f82a497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KScDWhVN1uRYCN9bFtw9Lw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="7816" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Comparing the two resulted graphs, t-SNE is better than PCA at separating different classes of digits in the latent space visualization(it captures non-linearity). It creates distinct clusters with minimal overlap between classes. The autoencoder compresses images into a lower dimensional space but still captures enough information to distinguish between different digits, as shown in the t-SNE graph.</p><blockquote class="qc qd qe"><p id="7260" class="nx ny qf nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">An important note here is that t-SNE is a non-linear technique used for visualizing high-dimensional data. It preserves local data structures, making it useful for identifying clusters and patterns visually. However, it is not typically used for feature reduction in machine learning.</p></blockquote><p id="7d07" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk"><strong class="nz fr">But what does this autoencoder probably learn?</strong></p><p id="2e55" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Generally speaking, one can say that an autoencoder like this learns the basic and simple edges and textures, moving to parts of the digits like loops and lines and how they are arranged, and finally understanding whole digits(hierarchical characteristics), all this while capturing the unique essence of each digit in a compact form. It can guess missing parts of an image and recognizes common patterns in how digits are written.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="30be" class="nb nc fq bf nd ne px gq ng nh py gt nj nk pz nm nn no qa nq nr ns qb nu nv nw bk">Manifold learning: The Blessing of Non-Uniformity</h1><p id="8cd6" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">In a previous article titled <a class="af ra" rel="noopener" target="_blank" href="/curse-of-dimensionality-an-intuitive-exploration-1fbf155e1411">Curse of Dimensionality: An Intuitive Exploration</a>, I explored the concept of the “Curse of dimensionality”, which refers to the problems and challenges that arises when working with data in higher dimensions, making the job of ML algorithms harder in many ways.</p><p id="cf94" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Here come the manifold learning algorithms, driven by the blessing of non-uniformity, the uneven distribution or variation of data points within a given space or dataset.</p><blockquote class="qc qd qe"><p id="4f35" class="nx ny qf nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">The fundamental assumption underlying manifold learning is that high-dimensional data actually lies on or near a lower-dimensional manifold within the high-dimensional space. This concept is based on the idea that although the data might exist in a high-dimensional space due to the way it’s measured or recorded, the intrinsic dimensions that effectively describe the data and its structure are much lower.</p></blockquote><p id="d703" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Let’s generate the famous Swiss roll dataset and use it as an example of non-uniformity in higher-dimensional spaces. In its original form, this dataset looks like a chaotic mess of data points. But beneath this chaos, there is hidden order — a low-dimensional structure that includes the important features of the data. Manifold learning techniques, like Isomaps, take advantage of this non-uniformity. By mapping data points from the high-dimensional space to a lower-dimensional one, Isomap shows us the intrinsic shape of the Swiss roll. It keeps the richness of the original data while revealing the underlying structure — a 2D projection that captures the non-uniformity of the high-dimensional space:</p><pre class="ml mm mn mo mp qg qh qi bp qj bb bk"><span id="0ba0" class="qk nc fq qh b bg ql qm l qn qo">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn import manifold, datasets<br/><br/># Generate a Swiss Roll dataset<br/>X, color = datasets.make_swiss_roll(n_samples=1500)<br/><br/># Apply Isomap for dimensionality reduction<br/>iso = manifold.Isomap(n_neighbors=10, n_components=2)<br/>X_iso = iso.fit_transform(X)<br/><br/># Plot the 3D Swiss Roll<br/>fig = plt.figure(figsize=(15, 8))<br/><br/># Create a 3D subplot<br/>ax = fig.add_subplot(121, projection='3d')<br/>ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)<br/><br/># Set the viewing angle<br/>elevation_angle = 30  # adjust this for elevation<br/>azimuthal_angle = 45  # adjust this for azimuthal angle<br/>ax.view_init(elev=elevation_angle, azim=azimuthal_angle)<br/><br/>ax.set_title("Original Swiss Roll")<br/><br/># Plot the 2D projection after Isomap<br/>ax = fig.add_subplot(122)<br/>ax.scatter(X_iso[:, 0], X_iso[:, 1], c=color, cmap=plt.cm.Spectral)<br/>plt.axis('tight')<br/>ax.set_title("2D projection by Isomap")<br/><br/># Show the plots<br/>plt.show()</span></pre><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rd"><img src="../Images/3a4a17ae7e0217abc2d3e0597d1dec59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIlllbVUQJAHJFhd9OSpnw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author</figcaption></figure><p id="3456" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Let’s look at the output above:</p><p id="d459" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">We have two colorful illustrations. On the left, there’s a 3D Swiss roll with a rainbow of colors that spiral together. It shows how each shade transitions into the next, marking a path through the roll.</p><p id="b431" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Now, on the right. There’s a 2D spread of the same colors. Even though the shape has changed the order and flow of colors still tell the same story of the original data. The order and connections between points are preserved, as if the Swiss roll was carefully unrolled onto a flat surface so we can see the entire pattern at once.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1aeb" class="nb nc fq bf nd ne px gq ng nh py gt nj nk pz nm nn no qa nq nr ns qb nu nv nw bk"><strong class="al">Conclusion</strong></h1><p id="02f2" class="pw-post-body-paragraph nx ny fq nz b go oa ob oc gr od oe of og oh oi oj ok ol om on oo op oq or os fj bk">This article started by exploring the concept of spaces, which are the mathematical constructs where data points are positioned based on their features/attributes. We examined how <strong class="nz fr">Support Vector Machines (SVMs)</strong> leverage the idea of mapping data into higher-dimensional spaces to address the challenge of <strong class="nz fr">non-linear separability</strong> in lower spaces.</p><p id="d6e5" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">Then we moved on to <strong class="nz fr">autoencoders</strong>, an elegant and truly beautiful architecture that maps between <strong class="nz fr">3 spaces</strong>, the <strong class="nz fr">input space</strong>, that gets compressed to a much lower<strong class="nz fr"> latent representation(the bottleneck)</strong>, and then comes the decoder to take the lead aiming to reconstruct the original input from this lower representation<strong class="nz fr"> while minimizing the reconstruction error</strong>.</p><p id="3884" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">We also explored <strong class="nz fr">manifold learning</strong>, and the blessing that we get from non-uniformity as a way to overcome the <strong class="nz fr">curse of dimensionality</strong> by simplifying complex datasets without losing important details.</p><p id="5569" class="pw-post-body-paragraph nx ny fq nz b go pk ob oc gr pl oe of og pm oi oj ok pn om on oo po oq or os fj bk">If you made it this far, I would like to thank you for your time reading this, I hope you found it enjoyable and useful, please feel free to point out any mistakes or misconceptions in my article, your feedback and suggestions are also greatly appreciated.</p></div></div></div></div>    
</body>
</html>