<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Enhancing Cancer Detection with StyleGAN-2 ADA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Enhancing Cancer Detection with StyleGAN-2 ADA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22">https://towardsdatascience.com/enhancing-cancer-detection-with-stylegan-2-ada-aee55ef99c5b?source=collection_archive---------5-----------------------#2024-01-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="39a4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Data augmentation for data-deficient deep neural networks.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ian Stebbins" class="l ep by dd de cx" src="../Images/50ece59dc136f7d41e02c046ea1216e4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*2P18Apr7YmeYsBJdLgTuEQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ianstebbs?source=post_page---byline--aee55ef99c5b--------------------------------" rel="noopener follow">Ian Stebbins</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--aee55ef99c5b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="681a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="ne">By: </em><a class="af nf" href="https://www.linkedin.com/in/ian-stebbins-244a1722b/" rel="noopener ugc nofollow" target="_blank"><em class="ne">Ian Stebbins</em></a><em class="ne">, </em><a class="af nf" href="https://www.linkedin.com/in/benjamin-goldfried/" rel="noopener ugc nofollow" target="_blank"><em class="ne">Benjamin Goldfried</em></a><em class="ne">, </em><a class="af nf" href="https://www.linkedin.com/in/benjamin-maizes/" rel="noopener ugc nofollow" target="_blank"><em class="ne">Ben Maizes</em></a></p><h2 id="492c" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">Intro</strong></h2><p id="4547" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">Often for many domain-specific problems, a lack of data can hinder the effectiveness and even disallow the use of deep neural networks. Recent architectures of Generative Adversarial Networks (GANs), however, allow us to synthetically augment data, by creating new samples that capture intricate details, textures, and variations in the data distribution. This synthetic data can act as additional training input for deep neural networks, thus making domain tasks with limited data more feasible.</p><p id="0118" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this project, we applied NVIDIA StyleGAN-2 with Adaptive Discriminator Augmentation (ADA) to a small <a class="af nf" href="https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&amp;sortBy=voteCount" rel="noopener ugc nofollow" target="_blank">Chest CT-Scan Dataset</a> (Licensed under <a class="af nf" href="http://opendatacommons.org/licenses/odbl/1.0/" rel="noopener ugc nofollow" target="_blank">Database: Open Database, Contents: © Original Authors</a>)[1]. Additionally, we built a CNN classifier to distinguish normal scans from those with tumors. By injecting varying proportions of synthetically generated data into the training of different models, we were able to evaluate the performance differences between models with all real data and those with a real-synthetic mix.</p><h2 id="a9d8" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">StyleGAN-2 ADA</strong></h2><p id="ef24" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">StyleGAN-2 with ADA was first introduced by NVIDIA in the NeurIPS 2020 paper: <a class="af nf" href="https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf" rel="noopener ugc nofollow" target="_blank">“Training Generative Adversarial Networks with Limited Data”</a> [2]. In the past, training GANs on small datasets typically led to the network discriminator overfitting. Thus rather than learning to distinguish between real and generated data, the discriminator tended to memorize the patterns of noise and outliers of the training set, rather than learn the general trends of the data distribution. To combat this, ADA dynamically adjusts the strength of data augmentation based on the degree of overfitting observed during training. This helps the model to generalize better and leads to better GAN performance on smaller datasets.</p><h2 id="83a7" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">Augmenting The Dataset</strong></h2><p id="ab89" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">To use the StyleGAN-2 ADA model, we used the official NVIDIA model implementation from GitHub, which can be found <a class="af nf" href="https://github.com/NVlabs/stylegan3" rel="noopener ugc nofollow" target="_blank">here</a>. <em class="ne">Note that this is the StyleGAN-3 repo but StyleGAN-2 can still be run.</em></p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="c4a9" class="op nh fq om b bg oq or l os ot">!git clone https://github.com/NVlabs/stylegan3</span></pre><p id="49a9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Depending on your setup you may have to install dependencies and do some other preprocessing. For example, we chose to resize and shrink our dataset images to 224x224 since we only had access to a single GPU, and using larger image sizes is much more computationally expensive. We chose to use 224x224 because ResNet, the pre-trained model we chose for the CNN, is optimized to work with this size of image.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="e1ca" class="op nh fq om b bg oq or l os ot">!pip install pillow<br/>from PIL import Image<br/>import os<br/><br/>'''Loops through the files in an input folder (input_folder), resizes them to a<br/>specified new size (new_size), an adds them to an output folder (output_folder).'''<br/>def resize_images_in_folder(input_folder, output_folder, new_size):<br/>    # Loop through all files in the input folder<br/>    for filename in os.listdir(input_folder):<br/>        input_path = os.path.join(input_folder, filename)<br/><br/>        # Check if the file is an image<br/>        if os.path.isfile(input_path) and filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):<br/>            # Open the image file<br/>            image = Image.open(input_path)<br/><br/>            #Convert to RGB<br/>            image = image.convert('RGB')<br/><br/>            # Resize the image<br/>            resized_image = image.resize(new_size)<br/><br/>            # Generate the output file path<br/>            output_path = os.path.join(output_folder, filename)<br/><br/>            # Save the resized image to the output folder<br/>            resized_image.save(output_path)<br/><br/>            print(f"Resized {filename} and saved to {output_path}")</span></pre><p id="3161" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To begin the training process, navigate to the directory where you cloned the repo and then run the following.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="d7e2" class="op nh fq om b bg oq or l os ot">import os<br/><br/>!python dataset_tool.py --source= "Raw Data Directory" --dest="Output Directory" --resolution='256x256'<br/><br/># Training<br/>EXPERIMENTS = "Output directory where the Network Pickle File will be saved""<br/>DATA = "Your Training DataSet Directory"<br/>SNAP = 10<br/>KIMG = 80<br/><br/># Build the command and run it<br/>cmd = f"/usr/bin/python3 /content/stylegan3/train.py --snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}  --kimg {KIMG} --cfg stylegan2 --gpus 1 --batch 8 --gamma 50"<br/>!{cmd}<br/></span></pre><p id="8577" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">SNAP</strong> refers to the number of Ticks (training steps where information is displayed) after which you would like to take a snapshot of your network and save it to a pickle file.</p><p id="59e5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">KIMG</strong> refers to the number of thousands of images you want to feed into your GAN.</p><p id="51be" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">GAMMA d</strong>etermines how strongly the regularization affects the discriminator.</p></div></div><div class="ou"><div class="ab cb"><div class="ll ov lm ow ln ox cf oy cg oz ci bh"><div class="og oh oi oj ok ab ke"><figure class="le ou pa pb pc pd pe paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><img src="../Images/b34210233f78736ace829dafc60eb73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*u8NftPmMpdZnhOxV9rPxkg.jpeg"/></div></figure><figure class="le ou pa pb pc pd pe paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><img src="../Images/6a3268bd615e1c08b05b12fa8d74ea19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*dtAXS8ndxsjRXQ_cOswt6A.jpeg"/></div><figcaption class="pk pl pm pn po pp pq bf b bg z dx pr ed ps pt">Initial Generated Images</figcaption></figure></div><div class="ab ke"><figure class="le ou pa pb pc pd pe paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><img src="../Images/86495efb421750130e2efe65762c8570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*ywDG7nOywRkmEGWnXqYohg.jpeg"/></div></figure><figure class="le ou pa pb pc pd pe paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><img src="../Images/948a9ccdfd5f21abf3313d44279cfa62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*3Cyzj4zf9YJQw3lwXmx1Cg.jpeg"/></div><figcaption class="pk pl pm pn po pp pq bf b bg z dx pr ed ps pt">Generated Images During Training</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4e38" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Once your model has finished training (this can take multiple hours depending on your compute resources) you can now use your trained network to generate images.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="50c6" class="op nh fq om b bg oq or l os ot">pickle_file = "Network_Snapshot.pkl"<br/>model_path = f'Path to Pickle File/{pickle_file}'<br/>SAMPLES = Number of samples you want to generate<br/>!python /content/stylegan3/gen_images.py --outdir=Output Directory --trunc=1 --seeds {SAMPLES} \<br/>    --network=$model_path</span></pre></div></div><div class="ou"><div class="ab cb"><div class="ll ov lm ow ln ox cf oy cg oz ci bh"><div class="og oh oi oj ok ab ke"><figure class="le ou pa pb pc pd pe paragraph-image"><img src="../Images/43a52ee7c15a0a27c8aee5c3c86606fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*mTLEWJcX6ugPzrWzBAiCHg.png"/></figure><figure class="le ou pa pb pc pd pe paragraph-image"><img src="../Images/7496b11702ef63c7c146d11d852d35d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*RWLInTNowf6U3SJXctJhQw.png"/><figcaption class="pk pl pm pn po pp pq bf b bg z dx pr ed ps pt">Normal Real Image (Left) vs Normal Generated Image (Right)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="555d" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">Transfer Learning &amp; Convolutional Neural Network</strong></h2><p id="fe7c" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">To benchmark the effectiveness of our synthetically generated data, we first trained a CNN model on our original data. Once we had a benchmark accuracy on the test set, we re-trained the model with increasing amounts of synthetic data in the training mix.</p><p id="a978" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To feed our data into the model we used Keras data generators which flow the samples directly from a specified directory into the model. The original dataset has 4 classes for different types of cancer, however, for simplicity, we turned this into a binary classification problem. The two classes we decided to work with from the original Kaggle dataset were the normal and squamous classes.</p><pre class="og oh oi oj ok ol om on bp oo bb bk"><span id="ca4d" class="op nh fq om b bg oq or l os ot"># Define directories for training, validation, and test datasets<br/>train_dir = 'Your training data directory'<br/>test_dir = 'Your testing data directory'<br/>val_dir = 'Your validation data directory'<br/><br/># Utilize data genarators to flow directly from directories<br/>train_generator = train_datagen.flow_from_directory(<br/>    train_dir,<br/>    target_size=(224, 224),<br/>    batch_size=20,<br/>    class_mode='binary',  #Use 'categorical' for multi-class classification<br/>    shuffle=True,<br/>    seed=42 )<br/><br/>val_generator = val_datagen.flow_from_directory(<br/>    val_dir,<br/>    target_size=(224, 224),<br/>    batch_size=20,<br/>    class_mode='binary',<br/>    shuffle=True )<br/><br/>test_generator = test_datagen.flow_from_directory(<br/>    test_dir,<br/>    target_size=(224, 224),<br/>    batch_size=20,<br/>    class_mode='binary',<br/>    shuffle=True )</span></pre><p id="c594" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To build our model, we began by using the ResNet50 base architecture and model weights. We chose to use ResNet50 due to its moderate-size architecture, good documentation, and ease of use through Keras. After importing ResNet50 with the Imagenet model weights, we then froze the ResNet50 layers and added trainable dense layers on top to help the network learn our specific classification task.</p><p id="0088" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We also chose to incorporate batch normalization, which can lead to faster convergence and more stable training by normalizing layer inputs and reducing internal covariate shift [3]. Additionally, it can provide a regularization effect that can help prevent overfitting in our added trainable dense layers.</p><figure class="og oh oi oj ok ou pn po paragraph-image"><div class="pn po pu"><img src="../Images/563757ec5c2f1f0f7c1b6a808ac3f16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*bBiVQtlaVj2DTXNobn--8w.png"/></div><figcaption class="pk pl pm pn po pp pq bf b bg z dx">Our Model Architecture</figcaption></figure><p id="8c54" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Originally, our model was not performing well. We solved this issue by switching our activation function from ReLU to leaky ReLU. This suggested that our network may have been facing the dying ReLU or dead neuron problem. In short, since the gradient of ReLU will always be zero for negative numbers, this can lead to neurons “dying” and not contributing to the network [4][5]. Since leaky ReLU is nonzero for negative values, using it as an activation function can help combat this issue.</p><h2 id="0167" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">Results</strong></h2><p id="c649" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">To test our synthetic data, we trained the above CNN on 5 separate instances with 0%, 25%, 50%, 75%, and 100% additional synthetic samples. For example, 0% synthetic samples meant that the data was all original, while 100% meant the training set contained equal amounts of original and synthetic data. For each network, we then evaluated the performance using an accuracy metric on a real set of unseen test data. The plot below visualizes how different proportions of synthetic data affect the testing accuracy.</p><figure class="og oh oi oj ok ou pn po paragraph-image"><div role="button" tabindex="0" class="pf pg ed ph bh pi"><div class="pn po pv"><img src="../Images/9a3fef522d3001511884da078b5b2dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4pVBso20A2G82Xi1B8xIw.jpeg"/></div></div><figcaption class="pk pl pm pn po pp pq bf b bg z dx">Test Accuracy on Binary (Normal vs Squamous Tumor) Classification</figcaption></figure><p id="e2af" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Training the model was unstable, thus we ruled out iterations where the accuracy was 1.0 or extremely low. This helped us avoid training iterations that were under or over fit.</p><p id="dea0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can see that from 0 to 25% we see a sharp increase in the testing accuracy, suggesting that even augmenting the dataset by a small amount can have a large impact on problems where the data is initially minimal.</p><p id="6b5b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since we only trained our GAN on 80 KIMG (due to compute limitations) the quality of our synthetic data could have potentially been better, given more GAN training iterations. Notably, an increase in synthetic data quality could also influence the graph above. We hypothesize that an increase in synthetic quality will also lead to an increase in the optimal proportion of synthetic data used in training. Further, if the synthetic images were better able to fit the real distribution of our training data, we could incorporate more of them in model training without overfitting.</p><h2 id="38d2" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk"><strong class="al">Conclusion</strong></h2><p id="c092" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">In this project, using GANs for the augmentation of limited data has shown to be an effective technique for expanding training sets and more importantly, improving classification accuracy. While we opted for a small and basic problem, this could easily be upscaled in a few ways. Future work may include using more computational resources to get better synthetic samples, introducing more classes into the classification task (making it a multi-class problem), and experimenting with newer GAN architectures. Regardless, using GANs to augment small datasets can now bring many previously data-limited problems into the scope of deep neural networks.</p><h2 id="9964" class="ng nh fq bf ni nj nk nl nm nn no np nq mr nr ns nt mv nu nv nw mz nx ny nz oa bk">Kaggle Dataset</h2><p id="2637" class="pw-post-body-paragraph mi mj fq mk b go ob mm mn gr oc mp mq mr od mt mu mv oe mx my mz of nb nc nd fj bk">We compiled our augmented and resized images into the following <a class="af nf" href="https://www.kaggle.com/datasets/benjaminmaizes/formatted-and-augmented-chest-ct-scan-images" rel="noopener ugc nofollow" target="_blank">Kaggle dataset</a>. This contains 501 normal and 501 squamous 224x224 synthetic images which can be used for further experimentation.</p><p id="963f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af nf" href="https://github.com/istebbins/Enhancing-Cancer-Detection-with-StyleGAN-2-ADA" rel="noopener ugc nofollow" target="_blank">Our <em class="ne">GitHub Repo</em></a></p><p id="5c90" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Citations</strong></p><p id="5e77" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[1] Hany, Mohamed, <a class="af nf" href="https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images/code?datasetId=839140&amp;sortBy=voteCount" rel="noopener ugc nofollow" target="_blank">Chest CT-Scan images Dataset</a>, Kaggle (2020).</p><p id="046e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[2] Karras, Tero, et al, <a class="af nf" href="https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/ada-paper.pdf" rel="noopener ugc nofollow" target="_blank">Training Generative Adversarial Networks with Limited Data</a> (2020), Advances in neural information processing systems 2020.</p><p id="13e8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[3] Ioffe, Sergey, and Christian Szegedy, <a class="af nf" href="http://proceedings.mlr.press/v37/ioffe15.pdf" rel="noopener ugc nofollow" target="_blank">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>, (2015), <em class="ne">International conference on machine learning</em>. pmlr, 2015.</p><p id="eb6d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[4] He, Kaiming, et al, <a class="af nf" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</a>, (2015), <em class="ne">Proceedings of the IEEE international conference on computer vision</em>. 2015.</p><p id="31ca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[5]Bai, Yuhan, <a class="af nf" href="https://www.shs-conferences.org/articles/shsconf/pdf/2022/14/shsconf_stehf2022_02006.pdf" rel="noopener ugc nofollow" target="_blank">RELU-function and derived function review</a>, (2022), <em class="ne">SHS Web of Conferences</em>. Vol. 144. EDP Sciences, 2022.</p></div></div></div></div>    
</body>
</html>