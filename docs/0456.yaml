- en: Exploring Object Detection with R-CNN Models — A Comprehensive Beginner’s Guide
    (Part 2)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索使用R-CNN模型进行目标检测——全面的初学者指南（第2部分）
- en: 原文：[https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17](https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17](https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17)
- en: Object detection models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测模型
- en: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)[![Raghav
    Bali](../Images/49fea68f38f59d0bc39dab484b55684f.png)](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)
    [Raghav Bali](https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)
    ·7 min read·Feb 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------)
    ·阅读时间：7分钟·2024年2月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/77bfdeb62b64a50add93691e90b907d5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77bfdeb62b64a50add93691e90b907d5.png)'
- en: Photo by [liam siegel](https://unsplash.com/@datalore?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影： [liam siegel](https://unsplash.com/@datalore?utm_source=medium&utm_medium=referral)
    来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Object Detection Models
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测模型
- en: Object detection is an involved process which helps in localization and classification
    of objects in a given image. In [part 1](/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78),
    we developed an understanding of the basic concepts and the general framework
    for object detection. In this article, we will briefly cover a number of important
    object detection models with a focus on understanding their key contributions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是一个复杂的过程，帮助在给定图像中进行目标的定位和分类。在[第1部分](/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78)中，我们理解了目标检测的基本概念和一般框架。在本文中，我们将简要介绍一些重要的目标检测模型，重点理解它们的关键贡献。
- en: The general object detection framework highlights the fact that there are a
    few interim steps to perform object detection. Building on the same thought process,
    researchers have come up with a number of innovative architectures which solve
    this task of object detection. One of the ways of segregating such models is in
    the way they tackle the given task. Object detection models which leverage multiple
    models and/or steps to solve this task as called as multi-stage object detectors.
    The Region based CNN (RCNN) family of models are a prime example of **multi-stage
    object detectors**. Subsequently, a number of improvements led to model architectures
    that solve this task using a single model itself. Such models are called as **single-stage
    object detectors**. We will cover single-stage models in a subsequent article.
    For now, let us now have a look under the hood for some of these multi-stage object
    detectors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的目标检测框架突出了目标检测过程中需要执行的一些中间步骤。在这个思维框架的基础上，研究人员提出了许多创新的架构来解决目标检测任务。将这些模型进行分类的一种方式是根据它们处理任务的方式。利用多个模型和/或步骤来解决此任务的目标检测模型被称为多阶段目标检测器。基于区域的CNN（RCNN）模型家族是**多阶段目标检测器**的典型例子。随后，许多改进导致了使用单一模型本身来解决此任务的模型架构。这些模型被称为**单阶段目标检测器**。我们将在后续的文章中讨论单阶段模型。现在，让我们来看看这些多阶段目标检测器的一些内部工作原理。
- en: Region Based Convolutional Neural Networks
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于区域的卷积神经网络
- en: Region based Convolutional Neural Networks (R-CNNs) were initially presented
    by Girshick et. al. in their paper titled “[Rich feature hierarchies for accurate
    object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)”
    in 2013\. R-CNN is a multi-stage object detection models which became the starting
    point for faster and more sophisticated variants in following years. Let’s get
    started with this base idea before we understand the improvements achieved through
    **Fast R-CNN** and **Faster R-CNN** models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于区域的卷积神经网络（R-CNN）最初由 Girshick 等人于 2013 年在他们的论文 “[Rich feature hierarchies for
    accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)”
    中提出。R-CNN 是一种多阶段物体检测模型，成为后续更快、更复杂的变种的起点。在理解通过 **Fast R-CNN** 和 **Faster R-CNN**
    模型取得的改进之前，让我们先从这个基础思想开始。
- en: 'The R-CNN model is made up of four main components:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 模型由四个主要组成部分构成：
- en: '**Region Proposal**: The extraction of regions of interest is the first and
    foremost step in this pipeline. The R-CNN model makes use of an algorithm called
    Selective Search for region proposal. Selective Search is a greedy search algorithm
    proposed by [Uijlings et. al](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf).
    in 2012\. Without going into too many details, selective search makes use of a
    bottoms-up multi-scale iterative approach to identify ROIs. In every iteration
    the algorithm groups similar regions until the whole image is a single region.
    Similarity between regions is calculated based on color, texture, brightness etc.
    Selective search generates a lot of false positive (background) ROIs but has a
    high recall. The list of ROIs is passed onto the next step for processing.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域提议**：提取感兴趣区域是该流程中的第一步也是最重要的一步。R-CNN 模型使用名为选择性搜索（Selective Search）的算法进行区域提议。选择性搜索是由
    [Uijlings 等人](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf) 于
    2012 年提出的一种贪心搜索算法。简单来说，选择性搜索利用自底向上的多尺度迭代方法来识别 ROI。在每次迭代中，算法将相似的区域进行分组，直到整张图像被归为一个区域。区域之间的相似性是基于颜色、纹理、亮度等计算的。选择性搜索会生成大量的假阳性（背景）ROI，但具有较高的召回率。ROI
    列表将传递到下一步进行处理。'
- en: '**Feature Extraction**: The R-CNN network makes use of pre-trained CNNs such
    as VGGs or ResNets for extracting features from each of the ROIs identified in
    the previous step. Before the regions/crops are passed as inputs to the pre-trained
    network these are reshaped or warped to the required dimensions (each pretrained
    network requires inputs in specific dimensions only). The pre-trained network
    is used without the final classification layer. The output of this stage is a
    long list of tensors, one for each ROI from the previous stage.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：R-CNN 网络使用预训练的 CNN（如 VGG 或 ResNet）从前一步中识别的每个 ROI 中提取特征。在将区域/裁剪传递到预训练网络作为输入之前，这些区域会被重新调整或扭曲为所需的尺寸（每个预训练网络仅要求特定尺寸的输入）。预训练网络在没有最终分类层的情况下使用。此阶段的输出是一长串张量，每个张量对应前一阶段的一个
    ROI。'
- en: '**Classification Head**: The original R-CNN paper made use of Support Vector
    Machines (SVMs) as the classifier to identify the class of object in the ROI.
    SVM is a traditional supervised algorithm widely used for classification purposes.
    The output from this step is a classification label for every ROI.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类头**：原始的 R-CNN 论文使用支持向量机（SVM）作为分类器来识别 ROI 中物体的类别。SVM 是一种传统的监督学习算法，广泛用于分类任务。此步骤的输出是每个
    ROI 的分类标签。'
- en: '**Regression Head**: This module takes care of the localization aspect of the
    object detection task. As discussed in the previous section, bounding boxes can
    be uniquely identified using 4 coordinates (top-left (x, y) coordinates along
    with width and height of the box). The regressor outputs these 4 values for every
    ROI.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归头**：此模块处理物体检测任务中的定位部分。如前一节所述，边界框可以通过 4 个坐标唯一确定（左上角（x，y）坐标以及框的宽度和高度）。回归器为每个
    ROI 输出这 4 个值。'
- en: This pipeline is visually depicted in figure 1 for reference. As shown in the
    figure, the network requires multiple independent forward passes (one of each
    ROI) using the pretrained network. This is one of the primary reasons which slows
    down the R-CNN model, both for training as well as inference. The authors of the
    paper mention that it requires 80+ hours to train the network and an immense amount
    of disk space. The second bottleneck is the selective search algorithm itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该流程在图1中进行了视觉展示，供参考。如图所示，网络需要使用预训练网络对每个ROI进行多次独立的前向传播。这是R-CNN模型在训练和推理过程中变慢的主要原因之一。论文的作者提到，训练该网络需要超过80小时，并且需要大量的磁盘空间。第二个瓶颈是选择性搜索算法本身。
- en: '![](../Images/bc1576d845aac3e157ff3d86511e95fc.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc1576d845aac3e157ff3d86511e95fc.png)'
- en: 'Figure 1: Components of the R-CNN model. Region proposal component is based
    on selective search followed by a pre-trained network such as VGG for feature
    extraction. Classification head makes use of SVMs and a separate regression head.
    Source: Author'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：R-CNN模型的组成部分。区域提议组件基于选择性搜索，随后通过预训练的网络（如VGG）进行特征提取。分类头部使用支持向量机（SVM）和一个单独的回归头部。来源：作者
- en: The R-CNN model is a good example of how different ideas can be leveraged as
    building blocks to solve a complex problem. While we will have a detailed hands-on
    exercise to see object detection in context of transfer learning, in its original
    setup itself R-CNN makes use of transfer learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN模型是一个很好的例子，展示了如何将不同的思想作为构建块来解决复杂的问题。虽然我们将在实践中详细演示如何在迁移学习的背景下进行目标检测，但在其原始设置中，R-CNN已经利用了迁移学习。
- en: The R-CNN model was slow, but it provided a good base for object detection models
    to come down the line. The computationally expensive and slow feature extraction
    step was mainly addressed in the **Fast R-CNN** implementation. The [Fast R-CNN](https://arxiv.org/abs/1504.08083)
    was presented by Ross Grishick in 2015\. This implementation boasts of not just
    faster training and inference but also improved mAP on [PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)
    dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN模型虽然较慢，但为后来的目标检测模型提供了良好的基础。计算上昂贵且缓慢的特征提取步骤在**Fast R-CNN**实现中得到了主要解决。[Fast
    R-CNN](https://arxiv.org/abs/1504.08083)由Ross Grishick于2015年提出。该实现不仅在训练和推理速度上更快，还在[PASCAL
    VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)数据集上提高了mAP。
- en: 'The key contributions from the **Fast R-CNN** paper can be summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**Fast R-CNN**论文的主要贡献可以总结如下：'
- en: '**Region Proposal**: For the base R-CNN model, we discussed how selective search
    algorithm is applied on the input image to generate thousands of ROIs upon which
    a pretrained network works to extract features. The Fast R-CNN changes this step
    to derive maximum impact. Instead of applying the feature extraction step using
    the pretrained network thousands of times, the Fast R-CNN network does it only
    once. In other words, we first process the whole input image through the pretrained
    network just once. The output features are then used as input for the selective
    search algorithm for identification of ROIs. This change in order of components
    reduces the computation requirements and performance bottleneck to a good extent.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区域提议**：对于基础的R-CNN模型，我们讨论了如何将选择性搜索算法应用于输入图像，生成数千个ROI，并通过预训练网络提取特征。Fast R-CNN改变了这一过程，以获得更大的影响。它不再使用预训练网络对特征提取步骤进行数千次处理，而是仅执行一次。换句话说，我们首先通过预训练网络处理整个输入图像，仅进行一次。然后，将输出特征作为输入，供选择性搜索算法识别ROI。这一组件顺序的改变在很大程度上减少了计算需求和性能瓶颈。'
- en: '**ROI Pooling Layer**: The ROIs identified in the previous step can be arbitrary
    size (as identified by the selective search algorithm). But the fully connected
    layers after the ROIs have been extracted take only fixed size feature maps as
    inputs. The ROI pooling layer is thus a fixed size filter (the paper mentions
    a size of 7x7) which helps transform these arbitrary sized ROIs into fixed size
    output vectors. This layer works by first dividing the ROI into equal sized sections.
    It then finds the largest value in each section (similar to Max-Pooling operation).
    The output is just the max values from each of equal sized sections. The ROI pooling
    layer speeds up the inference and training times considerably.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROI池化层**：在前一步中识别出的ROI可以是任意大小（由选择性搜索算法确定）。但是，ROI提取后的全连接层只能接受固定大小的特征图作为输入。因此，ROI池化层是一个固定大小的滤波器（论文中提到大小为7x7），它帮助将这些任意大小的ROI转化为固定大小的输出向量。该层的工作方式是首先将ROI划分为大小相等的区域，然后在每个区域中找到最大值（类似于最大池化操作）。输出是来自每个等大小区域的最大值。ROI池化层显著加快了推理和训练时间。'
- en: '**Multi-task Loss**: As opposed to two different components (SVM and bounding
    box regressor) in R-CNN implementation, Faster R-CNN makes use of a multi-headed
    network. This setup enables the network to be trained jointly for both the tasks
    using a multi-task loss function. The multi-task loss is a weighted sum of classification
    and regression losses for object classification and bounding box regression tasks
    respectively. The loss function is given as:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多任务损失**：与R-CNN实现中的两个不同组件（SVM和边界框回归器）不同，Faster R-CNN使用了一个多头网络。这种设置使得网络可以通过多任务损失函数同时训练两个任务。多任务损失是分类损失和回归损失的加权和，分别用于物体分类和边界框回归任务。损失函数表示为：'
- en: Lₘₜ = Lₒ + 𝛾Lᵣ
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Lₘₜ = Lₒ + 𝛾Lᵣ
- en: where 𝛾 ≥ 1 if the ROI contains an object (objectness score), 0 otherwise. Classification
    loss is simply a negative log loss while the regression loss used in the original
    implementation is the smooth L1 loss.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，𝛾 ≥ 1如果ROI包含物体（物体得分），否则为0。分类损失是一个简单的负对数损失，而原始实现中使用的回归损失是平滑L1损失。
- en: The original paper details a number of experiments which highlight performance
    improvements based on various combinations of hyper-parameters and layers fine-tuned
    in the pre-trained network. The original implementation made use of pretrained
    VGG-16 as the feature extraction network. A number of faster and improved implementation
    such as MobileNet, ResNet, etc. have come up since the Fast R-CNN’s original implementation.
    These networks can also be swapped in place of VGG-16 to improve the performance
    further.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文详细描述了多项实验，突出基于不同超参数组合和在预训练网络中微调的层次的性能提升。原始实现使用了预训练的VGG-16作为特征提取网络。从Fast
    R-CNN的原始实现以来，出现了多个更快且改进的实现，如MobileNet、ResNet等。这些网络也可以替换VGG-16，进一步提升性能。
- en: '**Faster R-CNN** is the final member of this family of multi-stage object detectors.
    This is by far the most complex and fastest variant of them all. While Fast R-CNN
    improved training and inference times considerably it was still getting penalized
    due to the selective search algorithm. The Faster R-CNN model presented in 2016
    by Ren et. al. in their paper titled “[Faster R-CNN: Towards Real-Time Object
    Detection with Region Proposal Networks](https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)”
    addresses the regional proposal aspect primarily. This network builds on top of
    Fast R-CNN network by introducing a novel component called **Region Proposal Network
    (RPN)**. The overall Faster R-CNN network is depicted in figure 2 for reference.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**Faster R-CNN**是这一系列多阶段物体检测器中的最终成员。这是迄今为止最复杂且最快的变体。虽然Fast R-CNN显著改善了训练和推理时间，但由于选择性搜索算法，它仍然受到了一定的惩罚。2016年，Ren等人在其论文《[Faster
    R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)》中提出的Faster
    R-CNN模型主要解决了区域提议的问题。该网络在Fast R-CNN的基础上引入了一个新的组件，称为**区域提议网络（RPN）**。整体Faster R-CNN网络如图2所示，供参考。'
- en: '![](../Images/317b48c82e252b5db04dbe89c0ba9aca.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/317b48c82e252b5db04dbe89c0ba9aca.png)'
- en: 'Figure 2: Faster R-CNN is composed of two main components: 1) a Region Proposal
    Network (RPN) to identify ROIs and 2) a Fast R-CNN like multi-headed network with
    ROI pooling layer. Source: Author'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Faster R-CNN由两个主要组件组成：1) 区域提议网络（RPN）用于识别ROI，2) 类似Fast R-CNN的多头网络，包含ROI池化层。来源：作者
- en: RPN is a fully convolutional network (FCN) that helps in generating ROIs. As
    shown in figure 3.12, RPN consists of two layers only. The first being a 3x3 convolutional
    layer with 512 filters followed by two parallel 1x1 convolutional layers (one
    each for classification and regression respectively). The 3x3 convolutional filter
    is applied onto the feature map output of the pre-trained network (the input to
    which is the original image). Please note that the classification layer in RPN
    is a binary classification layer for determination of objectness score (not the
    object class). The bounding box regression is performed using 1x1 convolutional
    filters on anchor boxes. The proposed setup in the paper uses 9 anchor boxes per
    window, thus the RPN generates 18 objectness scores (2xK) and 36 location coordinates
    (4xK), where K=9 is the number of anchor boxes. The use of RPN (instead of selective
    search) improves the training and inference times by orders of magnitudes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RPN 是一个完全卷积网络（FCN），用于生成 ROI。如图 3.12 所示，RPN 仅由两层组成。第一层是一个 3x3 的卷积层，具有 512 个过滤器，后面是两个并行的
    1x1 卷积层（分别用于分类和回归）。3x3 的卷积过滤器应用于预训练网络的特征图输出（其输入为原始图像）。请注意，RPN 中的分类层是一个二分类层，用于确定物体性得分（而不是物体类别）。边界框回归是通过在锚框上使用
    1x1 的卷积过滤器来进行的。论文中提议的设置在每个窗口使用 9 个锚框，因此 RPN 生成 18 个物体性得分（2xK）和 36 个位置坐标（4xK），其中
    K=9 是锚框的数量。使用 RPN（而不是选择性搜索）可以将训练和推理时间提高数量级。
- en: The Faster R-CNN network is an end-to-end object detection network. Unlike the
    base R-CNN and Fast R-CNN models which made use of a number of independent components
    for training, Faster R-CNN can be trained as a whole.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 网络是一个端到端的目标检测网络。与基础的 R-CNN 和 Fast R-CNN 模型不同，后者使用了多个独立组件进行训练，而
    Faster R-CNN 可以作为一个整体进行训练。
- en: This concludes our discussion on the R-CNN family of object detectors. We discussed
    key contributions to better understand how these networks work.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于 R-CNN 家族目标检测器的讨论总结。我们讨论了关键的贡献，以更好地理解这些网络是如何工作的。
