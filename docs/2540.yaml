- en: 'Temporal-Difference Learning: Combining Dynamic Programming and Monte Carlo
    Methods for Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17](https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Milestones of RL: Q-Learning and Double Q-Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    ·15 min read·Oct 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'We continue our deep dive of Sutton’s book “Reinforcement Learning: An Introduction”
    [1], and in this post introduce Temporal-Difference (TD) Learning, which is Chapter
    6 of said work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TD learning can be viewed as a combination of [Dynamic Programming (DP)](/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f)
    and [Monte Carlo (MC) methods](/monte-carlo-methods-for-solving-reinforcement-learning-problems-ff8389d46a3e),
    which we introduced in the previous two posts, and marks an important milestone
    in the field of Reinforcement Learning (RL) — combining the strength of aforementioned
    methods: TD learning does not need a model and learns from experience alone, similar
    to MC, but also “bootstraps” — uses previously established estimates — similar
    to DP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18ffe8d06e7bcab87fb8b4c749756123.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brooke Campbell](https://unsplash.com/@bcampbell?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brass-colored-analog-clock-Rw2-Y0nSIKQ?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will introduce this family of methods, both from a theoretical standpoint
    but also showing relevant practical algorithms, such as Q-learning — accompanied
    with Python code. As usual, all code can be found on [GitHub](https://github.com/hermanmichaels/rl_book).
  prefs: []
  type: TYPE_NORMAL
- en: We begin with an introduction and motivation, and then start with the prediction
    problem — similar to the previous posts. Then, we dive deeper in the theory and
    discuss which solution TD learning finds. Following that, we move to the control
    problem, and present a…
  prefs: []
  type: TYPE_NORMAL
