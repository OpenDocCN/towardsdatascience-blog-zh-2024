- en: 'Temporal-Difference Learning: Combining Dynamic Programming and Monte Carlo
    Methods for Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差分学习：将动态规划与蒙特卡罗方法结合用于强化学习
- en: 原文：[https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17](https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17](https://towardsdatascience.com/temporal-difference-learning-combining-dynamic-programming-and-monte-carlo-methods-for-e0c2f0829a51?source=collection_archive---------10-----------------------#2024-10-17)
- en: 'Milestones of RL: Q-Learning and Double Q-Learning'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习的里程碑：Q学习与双重Q学习
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--e0c2f0829a51--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    ·15 min read·Oct 17, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e0c2f0829a51--------------------------------)
    ·15分钟阅读·2024年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'We continue our deep dive of Sutton’s book “Reinforcement Learning: An Introduction”
    [1], and in this post introduce Temporal-Difference (TD) Learning, which is Chapter
    6 of said work.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续深入研究Sutton的书《强化学习：导论》[1]，在这篇文章中介绍时间差分（TD）学习，这是该书的第六章内容。
- en: 'TD learning can be viewed as a combination of [Dynamic Programming (DP)](/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f)
    and [Monte Carlo (MC) methods](/monte-carlo-methods-for-solving-reinforcement-learning-problems-ff8389d46a3e),
    which we introduced in the previous two posts, and marks an important milestone
    in the field of Reinforcement Learning (RL) — combining the strength of aforementioned
    methods: TD learning does not need a model and learns from experience alone, similar
    to MC, but also “bootstraps” — uses previously established estimates — similar
    to DP.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TD学习可以看作是[动态规划（DP）](/introducing-markov-decision-processes-setting-up-gymnasium-environments-and-solving-them-via-e806c36dc04f)与[蒙特卡罗（MC）方法](/monte-carlo-methods-for-solving-reinforcement-learning-problems-ff8389d46a3e)的结合，我们在前两篇文章中介绍了这两种方法，并且标志着强化学习（RL）领域的一个重要里程碑——结合了上述方法的优点：TD学习不需要模型，仅从经验中学习，类似于MC，但也具有“自举”——使用先前的估计值，类似于DP。
- en: '![](../Images/18ffe8d06e7bcab87fb8b4c749756123.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18ffe8d06e7bcab87fb8b4c749756123.png)'
- en: Photo by [Brooke Campbell](https://unsplash.com/@bcampbell?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/brass-colored-analog-clock-Rw2-Y0nSIKQ?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Brooke Campbell](https://unsplash.com/@bcampbell?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在[Unsplash](https://unsplash.com/photos/brass-colored-analog-clock-Rw2-Y0nSIKQ?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: Here, we will introduce this family of methods, both from a theoretical standpoint
    but also showing relevant practical algorithms, such as Q-learning — accompanied
    with Python code. As usual, all code can be found on [GitHub](https://github.com/hermanmichaels/rl_book).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍这类方法的理论基础，并展示相关的实用算法，如Q学习——并附上Python代码。与往常一样，所有代码都可以在[GitHub](https://github.com/hermanmichaels/rl_book)上找到。
- en: We begin with an introduction and motivation, and then start with the prediction
    problem — similar to the previous posts. Then, we dive deeper in the theory and
    discuss which solution TD learning finds. Following that, we move to the control
    problem, and present a…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍背景和动机，然后从预测问题开始——与之前的帖子类似。接着，我们深入探讨理论，并讨论TD学习找到的解决方案。随后，我们转向控制问题，并提出…
