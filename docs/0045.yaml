- en: 'Learning Discrete Data with Harmoniums: Part I, The Essentials'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05](https://towardsdatascience.com/learning-discrete-data-with-harmoniums-part-i-the-essentials-be54e0e293b4?source=collection_archive---------13-----------------------#2024-01-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'From the Archives: Generative AI in the ‘00s'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[![Hylke
    C. Donker](../Images/bed587d1bb305ded80f7ce21bc4f4856.png)](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    [Hylke C. Donker](https://medium.com/@hylke.donker?source=post_page---byline--be54e0e293b4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be54e0e293b4--------------------------------)
    ·7 min read·Jan 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'I want to take you back to the last generative AI episode, in the early ’00s.
    During this time, Geoff Hinton, one of the founding fathers of deep learning,
    published an influential paper detailing the contrastive divergence algorithm
    [1]. This discovery allowed Smolensky’s harmonium [2] — which Hinton called the
    restricted Boltzmann machine — to be trained efficiently. It was soon realised
    that this model could be used for all sorts of purposes: initialising a feed-forward
    neural net [3], used as part of a deep belief net [4], etc. For at least a decade,
    the harmonium remained one of the pillars in AI until we discovered better optimisers
    for training feed-forward networks. Although the harmonium has now gone out of
    fashion, the model remains useful for modelling discrete data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first article of a two-part series, we’ll focus on the essentials:
    what harmoniums are, when they are useful, and how to get started with *scikit-learn*.
    In a follow-up, we’ll take a closer look at the technicalities.'
  prefs: []
  type: TYPE_NORMAL
- en: What are Harmoniums?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0ca9ec5daae7f78bca9886032b72a05f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: **Graphical representation of a harmonium.** Receptive fields are edges
    connecting the visible units, ***x***, with the hidden units, **h**, so as to
    form a bipartite network. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The vanilla harmonium — or, restricted Boltzmann machine — is a neural network
    operating on binary data [2]. These networks are composed of two types of variables:
    the input, ***x,*** and the hidden states, ***h*** (Fig. 1). The input consists
    of zeroes and ones, *xᵢ* ∈ {0, 1}, and together we call these observed values—***x***
    — the *visible states* or *units* of the network. Conversely, the hidden units
    ***h*** are latent, not directly observed; they are internal to the network. Like
    the visible units, the hidden units ***h*** are either zero or one, *hᵢ* ∈ {0,
    1}.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard feed-forward neural networks process data sequentially, by directing
    the layer’s output to the input of the next layer. In harmoniums, this is different.
    Instead, the model is an *un*directed network. The network structure dictates
    how the probability distribution factorises over the graph. In turn, the network
    topology follows from the **energy function** *E*(***x***, ***h***) that quantifies
    the preferences for specific configurations of the visible units ***x*** and the
    hidden units ***h***. Because the harmonium is defined in terms of an energy function,
    we call it an **energy-based model**.
  prefs: []
  type: TYPE_NORMAL
- en: The Energy Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The simplest network directly connects the observations, ***x***, with the hidden
    states, ***h***, through *E*(***x***, ***h***) = ***x***ᵀ***Wh*** where ***W***
    is a receptive field. Favourable configurations of ***x*** and ***h*** have a
    low energy *E*(***x***, ***h***) while unlikely combinations have a high energy.
    In turn, the energy function controls the probability distribution over the visible
    units
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(***x***,***h***) = exp[-*E*(***x***, ***h***)] / *Z,*'
  prefs: []
  type: TYPE_NORMAL
- en: 'where the factor *Z* is a constant called the partition function. The partition
    function ensures that *p*(***x***,***h***) is normalised (sums to one). Usually,
    we include additional bias terms for the visible states, ***a***, and hidden states,
    ***b*** in the energy function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(***x***, ***h***) = ***x***ᵀ***a*** + ***x***ᵀ***Wh*** + ***b***ᵀ***h.***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Structurally, *E*(***x***, ***h***) forms a bipartition in ***x*** and ***h***
    (Fig. 1). As a result, we can easily transform observations ***x*** to hidden
    states ***h*** by sampling the distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*hᵢ*=1|***x***) = *σ*[-(***W***ᵀ***x***+***b***)],'
  prefs: []
  type: TYPE_NORMAL
- en: 'where *σ*(*x*) = 1/[1 + exp(-*x*)] is the sigmoid activation function. As you
    see, the probability distribution for ***h*** | ***x*** is structurally akin to
    a one-layer feed-forward neural network. A similar relation holds for the visible
    states given the latent observation: *p*(*xᵢ*=1|***h***) = *σ*[-(***Wh***+***a***)].'
  prefs: []
  type: TYPE_NORMAL
- en: This identity can be used to **impute** (generate new) input variables based
    on the latent state ***h***. The trick is to Gibbs sample by alternating between
    *p*(***x***|***h***) and *p*(***h***|***x***). More on that in the second part
    of this series.
  prefs: []
  type: TYPE_NORMAL
- en: When to use harmoniums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In practice, consider using harmoniums when:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Your data is discrete (binary-valued).**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Harmoniums have a strong theoretical foundation: it turns out that the model
    is powerful enough to describe *any* discrete distribution. That is, harmoniums
    are universal approximators [5]. So in theory, harmoniums are a one-size-fits-all
    when your dataset is discrete. In practice, harmoniums also work well on data
    that naturally lies in the unit [0, 1] interval.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. For representation learning.**'
  prefs: []
  type: TYPE_NORMAL
- en: The hidden states, ***h***,that are internal to the network can be used in itself.
    For example, ***h*** can be used as a dimension reduction technique to learn a
    compressed representation of ***x***. Think of it as principal components analysis,
    but for discrete data. Another application of the latent representation ***h***
    is for a downstream task by using it as the features for a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. To elicit latent structure in your variables.**'
  prefs: []
  type: TYPE_NORMAL
- en: Harmoniums are neural networks with receptive fields that describe how an example,
    ***x***, relates to its latent state ***h:*** neurons that wire together, fire
    together. We can use the receptive fields as a read-out to identify input variables
    that naturally go together (cluster). In other words, the model describes different
    modules of associations (or, correlations) between the visible units.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. To impute your data.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since harmoniums are generative models, they can be used to complete missing
    data (i.e., imputation) or generate completely new (synthetic) examples. Traditionally,
    they have been used for in-painting: completing part of an image that is masked
    out. Another example is recommender systems: harmoniums featured in the Netflix
    competition to improve movie recommendations for users.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know the essentials, let’s show how to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: As our running example, we’ll use the [UCI MLR handwritten digits database](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits)
    (CC BY 4.0) that is part of *scikit-learn*. While technically the harmonium requires
    binary data as input, using binary probabilities (instead of samples thereof)
    works fine in practice. We therefore normalise the pixel values to the unit interval
    [0, 1] prior to training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Conveniently, *scikit-learn* comes with an off-the-shelf implementation: [BernoulliRBM](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Under the hood, the model relies on the persistent contrastive divergence algorithm
    to fit the parameters of the model [6]. (To learn more about the algorithmic details,
    stay tuned.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b69e638d7b20e9283b2599d72118ad54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 2: Receptive fields **W** of each harmonium’s hidden unit. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To interpret the associations in the data — which input pixels fire together
    — you can inspect the receptive fields ***W.*** In *scikit-learn*, a NumPy array
    of ***W*** can be accessed by the `BernoulliRBM.components_` attribute after fitting
    the `BernoulliRBM` model (Fig. 2). [Beware: *scikit-learn* uses a different sign
    convention in the energy function: *E*(***x***,***h***) -> -*E*(***x***,***h***).]'
  prefs: []
  type: TYPE_NORMAL
- en: For **representation learning**, it is customary to use a deterministic value
    *p*(*hᵢ*=1|***x***) as a representation instead of stochastic sample *hᵢ ~ p*(*hᵢ*|***x***).
    Since *p*(*hᵢ*=1|***x***) equals the expected hidden state <*hᵢ>* given ***x***,
    it is a convenient measure to use during inference where we prefer determinism
    (over randomness). In *scikit-learn*, the latent representation, *p*(*hᵢ*=1|***x***),
    can be directly obtained through
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to demonstrate **imputation** or in-painting, let’s take an image containing
    the digit six and erase 25% of the pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now use the harmonium to impute the erased variables. The trick is
    to do Markov chain Monte Carlo (MCMC): simulate the missing pixel values using
    the pixel values that we do observe. It turns out that Gibbs sampling — a specific
    MCMC approach — is particularly easy in harmoniums.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/290ea16c6c9431f49e89c7c10e34cf37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 3: Pixel values in the red square are missing (left), and imputated with
    a harmonium (middle). For comparison, the original image (UCI MLR handwritten
    digits database, CC BY 4.0) is shown on the right. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how yo do it: first, initialise multiple Markov chains (e.g., 100)
    using the sample you want to impute. Then, Gibbs sample the chain for several
    iterations (e.g., 1000) while clamping the observed values. Finally, aggregate
    the samples from the chains to obtain a distribution over the missing values.
    In code, this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The result is shown in Fig. 3\. As you can see, the harmonium does a pretty
    decent job reconstructing the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI is not new, it goes back a long way. We’ve looked at harmoniums,
    an energy-based unsupervised neural network model that was popular two decades
    ago. While no longer at the centre of attention, harmoniums remain useful today
    for a specific niche: learning from discrete data. Because it is a generative
    model, harmoniums can be used to impute (or, complete) variable values or generate
    completely new examples.'
  prefs: []
  type: TYPE_NORMAL
- en: In this first article of a two-part harmonium series, we’ve looked at the essentials.
    Just enough to get you started. Stay tuned for part two, where we’ll take a closer
    look at the technicalities behind training these models.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I would like to thank [Rik Huijzer](https://huijzer.xyz/) and Dina Boer for
    proofreading.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Hinton “[Training products of experts by minimizing contrastive divergence.](https://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)”
    *Neural computation* 14.8, 1771–1800 (2002).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Smolensky “[Information processing in dynamical systems: Foundations of
    harmony theory.](https://apps.dtic.mil/sti/pdfs/ADA620727.pdf)” 194–281 (1986).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Hinton-Salakhutdinov, “[Reducing the dimensionality of data with neural
    networks.](https://doi.org/10.1126/science.1127647)” *Science* 313.5786, 504–507
    (2006).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Hinton-Osindero-Teh. “[A fast learning algorithm for deep belief nets.](https://doi.org/10.1162/neco.2006.18.7.1527)”
    *Neural computation* 18.7, 1527–1554 (2006).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Le Roux-Bengio, “[Representational power of restricted Boltzmann machines
    and deep belief networks.](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf)”
    Neural computation 20.6, 1631–1649 (2008).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Tieleman, “[Training restricted Boltzmann machines using approximations
    to the likelihood gradient.](https://dl.acm.org/doi/pdf/10.1145/1390156.1390290?casa_token=KA8SOPhKmvIAAAAA%3AulezajFrxWkXlhByFI-M_T8BhZBe7snX8eaFql0D0IMDw0igH710rVMYtCmK-r4Vz2VcjMPXGysT)”
    *Proceedings of the 25th international conference on Machine learning*. 2008.'
  prefs: []
  type: TYPE_NORMAL
