<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>TPUs Are Not for Sale, But Why?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>TPUs Are Not for Sale, But Why?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30">https://towardsdatascience.com/tpus-are-not-for-sale-but-why-5964f87f7a15?source=collection_archive---------4-----------------------#2024-04-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="5f54" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Opinion</h2><div/><div><h2 id="282f" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">An analysis of Google’s unique approach to AI hardware</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Haifeng Jin" class="l ep by dd de cx" src="../Images/705d6ecaed975b6376fac19087f2c02c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*RMJMO_yegbLzax5fZyaPBw.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://haifeng-jin.medium.com/?source=post_page---byline--5964f87f7a15--------------------------------" rel="noopener follow">Haifeng Jin</a></p></div></div></div><div class="ie if l"><div class="ab ig"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ij ik ah ai aj ak al am an ao ap aq ar il im in" disabled="">Follow</button></p></div></div></span></div></div><div class="l io"><span class="bf b bg z dx"><div class="ab cn ip iq ir"><div class="is it ab"><div class="bf b bg z dx ab iu"><span class="iv l io">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5964f87f7a15--------------------------------" rel="noopener follow"><p class="bf b bg z iw ix iy iz ja jb jc jd bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ih ii" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="je jf l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv"><div class="h k w ea eb q"><div class="kl l"><div class="ab q km kn"><div class="pw-multi-vote-icon ed iv ko kp kq"><div class=""><div class="kr ks kt ku kv kw kx am ky kz la kq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lb lc ld le lf lg lh"><p class="bf b dy z dx"><span class="ks">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kr lk ll ab q ee lm ln" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count li lj">1</span></p></button></div></div></div><div class="ab q jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk"><div class="lo k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lp an ao ap il lq lr ls" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lt cn"><div class="l ae"><div class="ab cb"><div class="lu lv lw lx ly lz ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lp an ao ap il ma mb ln mc md me mf mg s mh mi mj mk ml mm mn u mo mp mq"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mu mv mw mx my mz mr ms paragraph-image"><div role="button" tabindex="0" class="na nb ed nc bh nd"><div class="mr ms mt"><img src="../Images/26ebc18d628a6a347ee233a287cca807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JAM_sLQmJr7xR6hK"/></div></div><figcaption class="nf ng nh mr ms ni nj bf b bg z dx">Photo by <a class="af nk" href="https://unsplash.com/@dollargill?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dollar Gill</a> on <a class="af nk" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ac71" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Nvidia’s stock price has skyrocketed because of its GPU’s dominance in the AI hardware market. However, at the same time, TPUs, well-known AI hardware from Google, are not for sale. You can only rent virtual machines on Google Cloud to use them. Why did Google not join the game of selling AI hardware?</p><p id="8b2c" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">DISCLAIMER: The views expressed in this article are solely those of the author and do not necessarily reflect the opinions or viewpoints of Google or its affiliates. The entirety of the information presented within this article is sourced exclusively from publicly available materials.</p><h2 id="5f8b" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">A popular theory</h2><p id="2dba" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">One popular theory I heard is that Google wants to attract more customers to its cloud services. If they sell it to other cloud service providers, they are less competitive in the cloud service market.</p><p id="0249" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">According to cloud service customers, this theory does not make much sense. No corporate-level customer wants to be locked to one specific cloud service provider. They want to be flexible enough to move to another whenever needed. Otherwise, if the provider increases the price, they can do nothing about it.</p><p id="e537" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">If they are locked to Google Cloud for using TPUs, they would rather not use it. This is why many customers don’t want to use TPUs. They only started to feel less locked in recently when OpenXLA, an intermediate software to access TPUs, supported more frameworks like PyTorch.</p><p id="3823" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">So, using TPUs to attract customers to Google Cloud is not a valid reason for not selling them. Then, what is the real reason? To answer this question better, we must look into how Google started the TPU project.</p><h2 id="8c92" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">Why did Google start the TPU project?</h2><p id="e7b4" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">The short answer is for proprietary usage. There was a time when GPUs could not meet the computing requirements for AI hardware.</p><p id="b1a5" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Let’s try to estimate when the TPU project was started. Given it was <a class="af nk" href="https://cloud.google.com/blog/products/ai-machine-learning/google-supercharges-machine-learning-tasks-with-custom-chip" rel="noopener ugc nofollow" target="_blank">first announced to the public in 2016</a>, it would be a fair guess that it started around 2011. If that is true, they started the project pretty early since we did not see a significant improvement in computer vision until 2012 by AlexNet.</p><p id="c014" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">With this timeline, we know GPUs were less potent than today when the project started. Google saw this AI revolution early and wanted faster hardware for large-scale computing. Their only choice is to build a new solution for it.</p><p id="ca0f" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">That was why Google started this project, but there are more questions. Why were GPUs not good enough back in the day? What potential improvements did Google see that are significant enough to start their new hardware project?</p><p id="8cb4" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">The answer lies in the microarchitecture of GPUs and TPUs. Let’s examine the design of the cores on GPUs and TPUs.</p><h2 id="c919" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">The design idea of GPUs</h2><p id="af71" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">First, let’s do a quick recap of the background knowledge of CPUs. When an instruction comes, it is decoded by the instruction decoder and fed into the arithmetic logic unit (ALU) together with data from the registers. The ALU does all the computing and returns the results to one of the registers. If you have multiple cores in the CPU, they can work in parallel.</p><p id="ebfb" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">What is a GPU? It is short for the graphics processing unit. It was designed for graphics computing and later discovered suitable for machine learning. Most of the operations in a GPU are matrix operations, which could run in parallel. This also means there are not many operations they need to support compared with a CPU.</p><p id="a6ed" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">The more specialized the chip is for a given task, the faster it is on the task.</p><p id="0ff1" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">The key idea of the GPU’s initial design was to have a feature-reduced CPU with smaller but more cores for faster parallel computing. The number of instructions supported on a GPU is much less than on a CPU, which makes the area taken by a single core on a chip much smaller. This way, they can pack more cores onto the chip for large-scale parallel computing.</p><p id="e279" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Why do fewer features mean a smaller area on the chip? In software, more features mean more code. In hardware, all features are implemented using logical circuits instead of code. More features mean the circuit is more complex. For example, a CPU must implement more instructions on the chip.</p><p id="1f97" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Smaller also means faster. A simpler design of the logic gates leads to a shorter cycle time.</p><h2 id="3e26" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">The design idea of TPUs</h2><p id="484a" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">TPU further developed this idea of specialized chips for deep learning. The defining feature of a TPU is its matrix-multiply unit (MXU). Since matrix multiplication is the most frequent operation in deep learning, TPU builds a specialized core for it, the MXU.</p><p id="63d7" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">This is even more specialized than a GPU core, capable of many matrix operations, while the MXU only does one thing: matrix multiplication.</p><p id="4311" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">It works quite differently from a traditional CPU/GPU core. All the dynamics and generality are removed. It has a grid of nodes all connected together. Each node only does multiplication and addition in a predefined manner. The results are directly pushed to the next node for the next multiplication and addition. So, everything is predefined and fixed.</p><p id="1794" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">This way, we save time by removing the need for instruction decoding since it just multiplies and adds whatever it receives. There is no register for writing and reading since we already know where the results should go, and there is no need to store it for arbitrary operations that come next.</p><p id="d8f4" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Besides the MXU, the TPU has also been designed for better scalability. It has dedicated ports for high-bandwidth inter-chip interconnection (ICI). It is designed to sit on the racks in Google’s data centers and to be used in clusters. Since it is for proprietary usage only, they don’t need to worry about selling single chips or the complexity of installing the chips on the racks.</p><h2 id="1c6a" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">Are TPUs still faster today?</h2><p id="40e3" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">It doesn’t make sense others didn’t come up with the same simple idea of building dedicated cores for tensor operations (matrix multiplication). Even if they did not, it doesn’t make sense that they don’t copy.</p><p id="e726" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">From the timeline, it seems Nvidia came up with the same idea at about the same time. A similar product from Nvidia, the Tensor Cores, was <a class="af nk" href="https://developer.nvidia.com/blog/inside-volta/" rel="noopener ugc nofollow" target="_blank">first announced to the public in 2017</a>, one year after Google’s TPU announcement.</p><p id="a290" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">It is unclear whether TPUs are still faster than GPUs today. I cannot find public benchmarks of the latest generations of TPUs and GPUs, and it is unclear to me which generation and metrics should be used for benchmarking.</p><p id="0d40" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">However, we can use one universal application-oriented metric: dollars per epoch. I found <a class="af nk" href="https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/03efaadb875e3ba0ccbf5047c308f20dcd4f93d2/community-content/vertex_model_garden/benchmarking_reports/jax_vit_benchmarking_report.md" rel="noopener ugc nofollow" target="_blank">one interesting benchmark</a> from Google Cloud that aligns different hardware to the same axis: money. TPUs appear cheaper on Google Cloud if you have the same model, data, and number of epochs.</p><p id="dbda" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Large models, like Midjourney, Claude, and Gemini, are all very sensitive to the training cost because they consume too much computing power. As a result, many of them use TPUs on Google Cloud.</p><h2 id="397b" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">Why are TPUs cheaper?</h2><p id="984e" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">One important reason is the software stack. You are using not only the hardware but also the software stack associated with it. Google has better vertical integration for its software stack and AI hardware than GPUs.</p><p id="6f47" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Google has dedicated engineering teams to build a whole software stack for it with strong vertical integration, from the model implementation (Vertex Model Garden) to the deep learning frameworks (Keras, JAX, and TensorFlow) to a compiler well-optimized for the TPUs (XLA).</p><p id="8cc5" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">The software stack for GPUs is very different. PyTorch is the most popular deep learning framework used with Nvidia GPUs, and it was mainly developed by Meta. The most widely used model pools with PyTorch are <code class="cx pg ph pi pj b">transformers</code> and <code class="cx pg ph pi pj b">diffusers</code> developed by HuggingFace. It is much harder to do perfect vertical integration for the software stack across all these companies.</p><p id="b470" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">One caveat is that fewer models are implemented with JAX and TensorFlow. Sometimes, you may need to implement the model yourself or use it from PyTorch on TPUs. Depending on the implementation, you may experience some friction when using PyTorch on TPUs. So, there might be extra engineering costs besides the hardware cost itself.</p><h2 id="5654" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">Why not start selling TPUs?</h2><p id="8c9f" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">We understand the project was started for proprietary usage and acquired a pretty good user base on Google Cloud because of its lower price. Why did not Google just start to sell it to customers directly, just like Nvidia’s GPUs?</p><p id="bc0c" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">The short answer is to stay focused. Google is in fierce competition with OpenAI for generative AI. At the same time, it is in the middle of multiple waves of tech layoffs to lower its cost. A wise strategy now would be to focus its limited resources on the most important projects.</p><p id="1a37" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">If Google ever wants to start selling its TPUs, it will be competing with two strong opponents, Nvidia and OpenAI, at the same time, which may not be a wise move at the moment.</p><h2 id="91f7" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">The huge overhead of selling hardware</h2><p id="76c7" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">Selling hardware directly to customers creates huge overheads for the company. Conversely, renting TPUs on their cloud services is much more manageable.</p><p id="5b83" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">When TPUs are only served on the cloud, they can have a centralized way to install all the TPUs and related software. There is no need to deal with various installation environments or the difficulty of deploying a TPU cluster.</p><p id="0dc3" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">They know exactly how many TPUs to make. The demands are all internal, so there is no uncertainty. Thus, managing the supply chain is much easier.</p><p id="526b" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Sales also become much easier since it is just selling the cloud service. There is no need to build a new team experienced in selling hardware.</p><h2 id="bf57" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">The advantages of the TPU approach</h2><p id="d253" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">Without all the overhead of selling hardware directly to the customers, Google got a few advantages in return.</p><p id="01fc" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">First, they can have a more aggressive TPU architecture design. The TPUs have a unique way of connecting the chips. Unlike multiple GPUs that connect to the same board, TPUs are organized in cubes. They arranged 64 TPUs in a 4 by 4 by 4 cube to interconnect them with each other for faster inter-chip communication. There are 8960 chips in a single v5p Pod. They can be easily used together. This is the advantage of fully control your hardware installation environment.</p><p id="3699" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Second, they can iterate faster to push out new generations. Since they only need to support a small set of use cases for proprietary usages, it drastically reduces their research and development cycle for every generation of the chips. I wonder if Nvidia came up with the TensorCore idea earlier than Google, but because of the overhead of selling hardware to external customers, they could only announce it one year later than Google.</p><p id="5596" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">From the perspective of serving its most important purpose, competing in GenAI, these advantages put Google in a very good position. Most importantly, with this in-house hardware solution, Google saved huge money by not buying GPUs from Nvidia at a monopoly price.</p><h2 id="f2b1" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">The downside of the TPU approach</h2><p id="c981" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">So far, we have discussed many advantages of Google’s AI hardware approach, but is there any downside? Indeed, there is a big one. Google became a tech island.</p><p id="486b" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Every pioneer in tech will become an island isolated from the rest of the world, at least for a while. This is because they started early when the corresponding infrastructure was not ready. They need to build everything from scratch. Due to the migration cost, they will stick with their solution even if everyone else uses something else.</p><p id="5a72" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">This is exactly what Google is experiencing right now. The rest of the world is innovating with models from HuggingFace and PyTorch. Everyone is quickly tweaking each other’s models to develop better ones. However, Google cannot join this process easily since its infra is largely built around TensorFlow and JAX. When putting a model from external into production, it must be re-implemented with Google’s framework.</p><p id="3f7c" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">This “tech island” problem slows Google down in taking good solutions from the external world and further isolates it from others. Google will either start bringing more external solutions like HuggingFace, PyTorch, and GPUs or always ensure its in-house solutions are the best in the world.</p><h2 id="f369" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">What does the future of AI hardware look like?</h2><p id="9399" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">Finally, let’s peek into the future of AI hardware. What would the future AI hardware look like? The short answer is mode collapse as the hardware becomes more specialized.</p><p id="ff9c" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Hardware will be further coupled with the applications. For example, support more precision formats for better language model serving. Like with <code class="cx pg ph pi pj b">bfloat16</code>, <code class="cx pg ph pi pj b">TF32</code>, they may better support <code class="cx pg ph pi pj b">int8</code> and <code class="cx pg ph pi pj b">int4</code>. Nvidia announced their second generation of the <a class="af nk" href="https://docs.nvidia.com/deeplearning/transformer-engine/index.html" rel="noopener ugc nofollow" target="_blank">Transformer Engine</a>, which works with Blackwell GPU. This made optimizing their hardware for transformer models easier without changing the user code. A lot of codesign is happening.</p><p id="9e79" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">On the other hand, software cannot easily jump out of the transformer realm. If they do, they will be slow due to a lack of hardware support. On the contrary, they implement their models with the hardware in mind. For example, the <a class="af nk" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">FlashAttention</a> algorithm is designed to leverage the memory hierarchy of GPUs for better performance.</p><p id="4cd2" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">We see a big mode collapse coming soon. The hardware and software are so well optimized for each other for the current models. Neither of them can easily leave the current design or algorithm. If there is a new model completely different from the transformers, it needs to be 10x better to get widely adopted. It must incentivize people to make new hardware as fast and cheap as transformers.</p><h2 id="e5db" class="oh oi fq bf oj ok ol om on oo op oq or nu os ot ou ny ov ow ox oc oy oz pa fw bk">Summary</h2><p id="e1d9" class="pw-post-body-paragraph nl nm fq nn b gt pb np nq gw pc ns nt nu pd nw nx ny pe oa ob oc pf oe of og fj bk">In conclusion, the TPU project started for proprietary usage when the GPU’s computing power was insufficient. Google wants to focus on GenAI instead of competing in the AI hardware market to avoid slowing the iteration speed and sacrificing its innovative design. Faster computing at a lower cost helped Google significantly in doing AI research and developing AI applications. However, it also made Google a tech island.</p><p id="beb9" class="pw-post-body-paragraph nl nm fq nn b gt no np nq gw nr ns nt nu nv nw nx ny nz oa ob oc od oe of og fj bk">Looking into the future, AI hardware will be even more optimized for certain applications, like the transformer models. Neither the hardware nor the models could easily jump out of this mode collapse.</p></div></div></div></div>    
</body>
</html>